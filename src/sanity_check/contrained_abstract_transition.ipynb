{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "X_MIN, X_MAX, Y_MIN, Y_MAX = 0, 10, 0, 10\n",
    "obs_dim = 10\n",
    "state_dim = 2\n",
    "T = np.random.randn(obs_dim, state_dim)\n",
    "\n",
    "# Centroids\n",
    "centroids = []\n",
    "for x in (X_MAX//4, X_MAX * 3//4):\n",
    "    for y in (Y_MAX//4, Y_MAX * 3//4):\n",
    "        centroids.append(np.array((x, y)))\n",
    "\n",
    "rooms = []\n",
    "for x in (X_MAX//2, X_MAX):\n",
    "    for y in (Y_MAX//2, Y_MAX):\n",
    "        rooms.append((x-X_MAX//2, y-Y_MAX//2, x, y))\n",
    "def initiation_set(room_limits):\n",
    "    def _init(state):\n",
    "        low_limit = room_limits[0:2]\n",
    "        high_limit = room_limits[2:]\n",
    "        return not (state[0] > low_limit[0] and state[0] < high_limit[0] and state[1] > low_limit[1] and state[1] < high_limit[1])\n",
    "    return _init\n",
    "# Synthetic option definition\n",
    "\n",
    "effect_dists_params = []\n",
    "initiation_sets = []\n",
    "std_dev = 1\n",
    "for i, room in enumerate(rooms):\n",
    "    initiation_sets.append(initiation_set(room))\n",
    "    effect_dists_params.append((centroids[i], std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "\n",
    "N = 5000\n",
    "\n",
    "x = np.random.uniform(0, X_MAX, N)\n",
    "y = np.random.uniform(0, Y_MAX, N)\n",
    "\n",
    "s = np.array((x,y)).T\n",
    "\n",
    "masks = []\n",
    "for i, init_set in enumerate(initiation_sets):\n",
    "    mask = np.zeros((N,1))\n",
    "    for j in range(N):\n",
    "        mask[j] = float(init_set(s[j]))\n",
    "    masks.append(mask)\n",
    "\n",
    "masks = np.array(masks)\n",
    "I_s = np.array(masks).transpose((1,0,2))[...,0] # initiation vector at s\n",
    "\n",
    "# generate next states\n",
    "s_prime = []\n",
    "for i, effect_dist in enumerate(effect_dists_params):\n",
    "    s_prime.append(np.random.multivariate_normal(effect_dist[0], effect_dist[1] * np.eye(2), size=N))\n",
    "\n",
    "\n",
    "s_prime = np.array(s_prime) * masks  + s[np.newaxis] * (1-masks)\n",
    "masks = masks.reshape((-1,))\n",
    "\n",
    "I_s_prime = []\n",
    "for action in range(len(initiation_sets)):\n",
    "    _m = np.zeros((N,len(initiation_sets)))\n",
    "    for i, init_set in enumerate(initiation_sets):\n",
    "        for j in range(N):\n",
    "            _m[j, i] = float(init_set(s_prime[action][j]))\n",
    "    I_s_prime.append(_m)\n",
    "\n",
    "# Random affine transformation\n",
    "\n",
    "\n",
    "x = np.einsum('ij, kj->ki', T, s)\n",
    "x_prime = np.einsum('ij, lkj->lki', T, s_prime)\n",
    "\n",
    "data = []\n",
    "for i in range(len(effect_dists_params)):\n",
    "    data.append((x, x_prime[i], I_s, I_s_prime[i], s, s_prime[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "X_MIN, X_MAX, Y_MIN, Y_MAX = 0, 10, 0, 10\n",
    "obs_dim = 10\n",
    "state_dim = 2\n",
    "\n",
    "# Centroids\n",
    "centroids = []\n",
    "for x in (X_MAX//4, X_MAX * 3//4):\n",
    "    for y in (Y_MAX//4, Y_MAX * 3//4):\n",
    "        centroids.append(np.array((x, y)))\n",
    "\n",
    "rooms = []\n",
    "for x in (X_MAX//2, X_MAX):\n",
    "    for y in (Y_MAX//2, Y_MAX):\n",
    "        rooms.append((x-X_MAX//2, y-Y_MAX//2, x, y))\n",
    "def initiation_set(room_limits):\n",
    "    def _init(state):\n",
    "        low_limit = room_limits[0:2]\n",
    "        high_limit = room_limits[2:]\n",
    "        return not (state[0] > low_limit[0] and state[0] < high_limit[0] and state[1] > low_limit[1] and state[1] < high_limit[1])\n",
    "    return _init\n",
    "# Synthetic option definition\n",
    "\n",
    "effect_dists_params = []\n",
    "initiation_sets = []\n",
    "std_dev = 1\n",
    "for i, room in enumerate(rooms):\n",
    "    initiation_sets.append(initiation_set(room))\n",
    "    effect_dists_params.append((centroids[i], std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "X_MIN, X_MAX, Y_MIN, Y_MAX = 0, 10, 0, 10\n",
    "obs_dim = 10\n",
    "state_dim = 2\n",
    "\n",
    "# Centroids\n",
    "centroids = []\n",
    "for x in (X_MAX//4, X_MAX * 3//4):\n",
    "    for y in (Y_MAX//4, Y_MAX * 3//4):\n",
    "        centroids.append(np.array((x, y)))\n",
    "\n",
    "rooms = []\n",
    "for x in (X_MAX//2, X_MAX):\n",
    "    for y in (Y_MAX//2, Y_MAX):\n",
    "        rooms.append((x-X_MAX//2, y-Y_MAX//2, x, y))\n",
    "def initiation_set(room_limits):\n",
    "    def _init(state):\n",
    "        low_limit = room_limits[0:2]\n",
    "        high_limit = room_limits[2:]\n",
    "        return not (state[0] > low_limit[0] and state[0] < high_limit[0] and state[1] > low_limit[1] and state[1] < high_limit[1])\n",
    "    return _init\n",
    "# Synthetic option definition\n",
    "\n",
    "effect_dists_params = []\n",
    "initiation_sets = []\n",
    "std_dev = 1\n",
    "for i, room in enumerate(rooms):\n",
    "    initiation_sets.append(initiation_set(room))\n",
    "    effect_dists_params.append((centroids[i], std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAADdS0lEQVR4nOydeVwTZ/7HP5OQgwQIl5JgVRCxiqiIR0GsrYqrVau9tuvVe23X6s/W325X7dZV17a2e6mtra1uaw+r7v5qrVYtrldrRRArXohWRbwQVK5wh5CZ3x9hQhJyzCQzyQTm/Xq5WyCZeTKZeZ7v8z0+X4KiKAoiIiIiIiIiIj5C4u8BiIiIiIiIiHQuRONDRERERERExKeIxoeIiIiIiIiITxGNDxERERERERGfIhofIiIiIiIiIj5FND5EREREREREfIpofIiIiIiIiIj4FNH4EBEREREREfEpQf4egD0tLS04efIkYmJiIJGItpGIiIiIiEggQJIkbt++jcGDByMoyLV5ITjj4+TJkxg+fLi/hyEiIiIiIiLiAXl5eRg2bJjL1wjO+IiJiQEA3LhxA2FhYX4ejYiIiIiIiAgTampq0L17d8s67grBGR90qCUsLEw0PkRERERERAIMJikTYlKFiIiIiIiIiE8RjQ8RERERERERnyIaHyIiIiIiIiI+RTQ+RERERERERHyKaHyIiIiIiIiI+BTR+BARERERERHxKaLxISIiIiIiIuJTRONDRERERERExKcITmRMRESk82EiKeQVV+JObRO6hioxPD4SUgnh72GJiIjwBGvj4/Dhw/jb3/6GEydOoLS0FNu3b8cjjzxi+TtFUVi6dCk2bNiA6upqZGRkYN26dUhMTORy3LzibiI0kRRyiyqQc6UcAIH0hCik9YqyvEaoE6m/xtVRr4f9+4f0jMCJa1WMj2d/H90XHwkJQaC83iCo62QP199nVkEpln9XiFJ9k+V3Oo0SSx9OwoRknddjYjNe69dGqxUAAZTXCev78Mfz5Ok1FOL4+BgrF8cR6jzJF6yNj/r6egwaNAjPP/88HnvssXZ//+tf/4r33nsPn3/+OeLj47FkyRKMHz8ehYWFUCqVnAyaT9xNhFkFpVj0zVlUNxgtf1976DLCVTK889gAAOB1IuXyc0Wq5XgkJRbjkrSMzuHJuLhYWPggq6AUy3YWoqymbVzaMCWWTXE+LuvPf7W8HlvyrqOsxmD5u4QASKrt9Y4+J32MfYVl+M/PN1FnaLH8be0h2/NxfZ24MLbWHryEjdlXUd3Ydv97M86sglLM2ZQPyu73ZfomzNmUj3WzUjEhWedy7K7uMYD58+joONb4+77l4/ozgc0z7I/n3dNz7jlzC2/sKEBlfdu1jFTL8ObUZEwcGMvZ+Zk8d47G4u/7jW8IiqLsn3vmbyYIG88HRVGIjY3F73//e/zhD38AAOj1esTExOCzzz7DtGnT3B7z5s2b6N69O/R6vU96uzS3kPgy5yquVTagwdCCbfkl7SZC+jZ5cVQ8Pj5czPoc9PvpidQVfDy8ziZ4a9ydw5NxOTsvm+vBB1kFpfjdpnynf//IwbjcLUyOsP+cbI/B5XXy9r5yZHRbj5MCsCAzEXHRasaGjYmkMPLdg06vBwFAq1FiyaQkrNjt3Lhwdo85u98dXVcmzwj9Xvvvg4mnlK3RZ/+eqvpmvP6t8+tv/3m4gs0z7Oq1FIDnM+IYb3TcQV+f/YVl+CT7aru/u7smK/cUupzLXxoVj8UTk9yOw931eXFUPHaeLnX53Lkai6P7TcjU1NRAo9Hgxo0buOeee1y+llPj48qVK0hISMDJkyeRkpJied0DDzyAlJQUrFmzpt0xDAYDDIa23WNJSQmSkpJ8Ynys3FOIDT8V2+xWnUEAIOx2tmygJ9IjC8c4ffD4WKzdTfD251k3KxXjkrR2E58BczefZDUupguLq+vBByaSwpA39zmcxGkiVDL8/MY4m501k4XJGdowBf48uT/mbmZ/DE+uk6OFy9G5HX1/jhbKfYVlrD8/E8Mmp6gC0zfksjhq+7FrVDKX36Wr92s1Svz9iUG4U9uEFbvPo7K+mdF7dVbfBxNPqSdGO1tDFwDCVTJ8MD0VaQlRnCzuuUUVmLs538bLYk+UWo6cxWMhlRCM5xkuNlNMro+zZ2fPmVK8vNn55oPmwxmpmDjQ+RjZzK324wLMzx1Jwu1Y6PsNgODDMmyMD04TTsvKygCgXTvdmJgYy9/sWblyJZYvX87lMBjhzvK1hwLguZlmfn+pvgl5xZVIT4iy/J6e7Mv0jVix+7zDCZ7+3aJvziJUIWM1ueQVVzJ+OKjWc9iHIySE410kBfODtPy7QoxL0tqMyd15nV0Pvsm9UuF2sapqMCL3SgUyekfDRFJY/l2hx4YHAJTVGPDHbWc8Ogab6+TMLc/0+9tXWNZuUteGKdHUYmI99jJ9E363Kd9mt2siKYuXsWekCuEqOcuj2o4dgEeGB/3+Un0TZn5yjPV76e9D39jsMmTkzFNaahdSssYbQ7e6wYiZnxzz2eIOABX1zUhbuR/PpMcxnmfsQ2psx8b0+jh6dkwkhTd2FDA615IdBchMinGax8VmbrUfFwFg2c5zaDa5/ySl+iasPXgJW4/fEFz42hv8Xu2yePFi/O///q/lZ9rzwSfNLSTWexA+4YI7tW03D9sdjieTi/X5mJ4DsJ3QXXl7nC2OTM/Ldnw0nuYvbMq9xuj4n2UX40JpDSrrmz2aYOyxzu3whOzLd11+RldhESbf3/sHLmH1gUvt/m5thLKBPuWn2VfxafZVqORSNBpNNga8sPZs7CiracJfsy643Cy4mmMotDfauTB0Ad8t7jSV9Uas2t/+3nEGfWxHmxZ3icOeXB/rOSavuJKxh4s2rJzlYXg6dwHma2CdL+YOR9e3tNXA/3DGYFY5KkKBU+NDq9UCAG7fvg2dru2mv337tk0YxhqFQgGFQmH5uaamhsshOeT1bzzbhXJB11Bz0q03Oxw2kwt9Pr6xfxCZnteT8Xmav5BVUIrvCxx74OzZd/4O9p2/w3psfLH2UBG25Zc4TfLzJiwEAGscGB5c0tBsavc7fz2DXFBZZ3BrlLr7fPZGu6c7aUfndeaRdAVXxg9T7D+/u+fa0+tjPcewNaatDQ/Adu711dzqjnlbTmItCJchIiHCqchYfHw8tFotDhw4YPldTU0Njh07hvT0dC5P5TEmksIehguQPd6E1wiYH6QhPSOQfbkci7ad9fght945mEgKJpJCTlEFdpwqQU5RBUxWW93h8ZHQaZS87zLtH0Qm5w1XyUC2jt8aE0kh+1I5/r73Av6+9xdkXy63vIZeaO0nIXpS2HPmlsNrQU+sgQz9GbMKSi2/42rBCGRDwNdEqeWIVHseMrLmVlWD5b+92UnbY+2RdEVzC4lPfrqCP+8owLKdBZwYP2zYV2iei90911kFpayvDwFzvhVJUdhxqgRr9l/EG9vPeDVe67l3SM8I6DT+N0BIypw3Yj0vBAKsPR9ZWVl4++23cf78eQDAzp07ERcXh8jISPTo0QOvvPIK3njjDbz55ptoaGhAWFgYunTpYqMF4k/yiisd7sKYoJRJGb3XPtOeXoCnDNLhgb8d4mx3Y44FXsbW49ddegGeHHoP1hy47PU5HUEndQ2Pj7T5vVRCYOnDSZizKd9p5YGjMJKrUua3HxmAFbsdL7T07+ZtOemw3FUTLPf5xMo11jvaMX3Nsejsy3cD/nMFGiumJiOCI+Nj2a5CqJVBmJCs42UnnX35rtPQJJuEe77YceoWFj3Uz6kBbX3P//3Xg1gdmwLQ1EJi5r/Y5/W4O26pvgknrlVh6cNJLivnfAlbT5e/YW18nD59Gj/99JPl540bN2Ljxo145pln8NlnnwEwl9wKFW92F42thodK3t4IiVDJsNKJzodWo8SUQTqsP1zM+Q5z1f6L7X5nnfD2759vepyU5w76Fl/6cJLDG35Csg7rZqVi2c5zLuOb7hL0ALOhwiRD3X4ipY/99Iiebt8bCNATn30sWsQ3qORSjE/W4p3vz3NyvNqmFosbf1ySFjqNEmX6Js7mibWHiiz/bW3ks02454uK+mZ8mXOVUXI6aaJcllBbowySoKmF5G3uA8xrydSUbliQ2cfhPOxr/JHA7w2swy4LFy4ERVEWA2P79u2gKAqfffYZKIrCmjVr8Oabb6KqqgoGgwGXL1/G3bt38e2333I9dtaYSArltcyTfOyhrXBNsAyfPzMMj6TE4ldJMfjTxH449nomJiTrMCFZhyMLx2DL7DSsmZaCLbPT8ONro7HzdKnPXNtU67+PDxfz+vBpVDKnZbZ06OOXslq3VUL0ePlIAqaP/cVRZommfEHAbKBGqLhJs/LE8AiQDZGgaWg24ejlcnxyhNt7lQ4J0tolfEAb4t+dvoUNP/nf8KA5fKmc0es2H7/GeA5VyfmvpaA9VfPG9IY2TOHm1b6By9Ad33D6DRUXF6OsrAyZmZmW32k0Gtx3333IyclxKDJmr/NRW1vL5ZAseFo7bw9thS/4+rQla/q/hbfxaXax04TH41e5SSQTIqEKGXacKrHRhLAv1WUKn8aZP31x9Jq/8rEBaDSSWPDvU34Zx9rpgxGhVuD7glJ8keNfYyyQ2ZZ/k9NQhXV+xoRkncdihkzOQwD40/azfg212PPjxbuMXvd9wW1Gr1MEEahsYFbR4gn2oWaphMCyKf29TvrmAqEkwTKhU+h8cFENYI99uZZ16MBe0U4lk3J4ZuFA52zQhHso+BRIqBVSyKUSVLH4nFord3dOUQWPo3MMQQAv3h9vU44nGh+ew7Riii3Zl8sxpGcEdpziL3GQAlDTxL7s2751AFcQhHf6SY5IuSccx65WcXtQO+xDzXSImYsNrqfoHOTeCRlOq108YfHixdDr9ZZ/hYXcViT4qnzMOtRhf/M1GD1LcA00OrrhAQAyqQRHF411W+0QogjCqicHYcvsNBxZOMbiEfNV9ZENlDmkRWfDV9UbQARoCCY8WAaN0r/yRIYWkpfjrj10GWkrD3jkNZw8QIfwYBmn43kqrQeea82V4stTwkd6IJ+Gh4QAPpgx2KGH2zrk/s9fD4JS5rvllYDz3DuhwunVsdb5sOb27duWv9mjUCgQFhZm+RcaGsrlkDirnRcRAcwG1uZj1/Dm1GSXr6sztODLnKsg7WZtugrIl1iXB+46VYKXN5/kZdL3BdWNRug92LkHCkwFsOzJuVLhUgbdHndLlIQAUntE4nOBesj8tcSSFBChdp7fIZWYu5w/NuQerP5NCqtjEzCXcbNFQph7yASa0mmH1/kQegJO4NipIjQrdp/HX3adg0ruOpyWf0OPmZ8cw5AV+7Bm/yWL9si4JC3WzUr1aZIanVcwb+spn51TxHdUsDBadBolfnt/nMvXkBSw4D+nBJUbQiOTEFAr/BfKdrem0An3hhYSCzITGT3n9DqwYmoyaw8WaefZDBRY+y/r6upw+XKbZkRxcTFOnTpl0fl49dVX8eabbyIxMRHx8fFYsmQJYmNj/abzIfQEnEi1nNXEISIM2EgjVzcabUrxItUyPJrSDf/4dQqOX610KGseSESq5Zg6SIeNfq4oEmEGnX8kIQif6HzMG90bAGVT9usNRpKC0WCCUiZBk5GfEJgrymsNMJGUwxCHo8KGSLUcr45NRHwXNaLVChy/WonPjtr2X6LzwgCw8mBZE2g6H6y72v7www8YPXp0u9/TOh8URWHp0qVYv349qqurMXLkSHz44Yfo06cPo+PfvHkT3bt356yrLd15kMvaeS54Or0nHkrWoUzfiAX/Oe3v4Yj4CV2rBox9knIgJe9+9cJ9kEgIjzvUinBDqFKK2ibn+WUSAlg73bZTa2OzCcPf3ufyfd6yZXYa7tQ24ZUO5HXThimwbEp/m1CHu8KGcUldUVBSY/ucB8vwXEYc5o1JBABkvHPQ435KgPla+1Png9eutg8++KBLETGCIPCXv/wFf/nLX9gemhdcKW0yFazhg4eSdUhPiMKqfd6J09DZ4t58FrVcinoPVV+FjCJIwltyIFeU6Zuw/nAxPphhLoOl1ShJkvKo46o/yLlSjoQuIYhUy1FV3ywoI59P/Dl/OMKdAfE/Y3rDSJLIKaqwqJ2eulHNq+FBV2CsPciPwrK/KKsx4Heb8vFRq84Rk8KGfYXte0XpG41Yvf8S7tWGQhMs98rwAID9hWUdV2QsEKHLoLR2OvxajRILMpl5ZLiC7vEyPD4SWQWlXjf0oiiY44oe9Bigx/Lb+3t5NQahInTDA2irknpjRwGG9IzA1JRuSE+IQlpClCD6RjBh7aEiLPiPWfdGSIuxNUN6hnN+TF99VrmUwIBYcyK+Jw51TXAQ1Aop1hy4jFe2nsL0DbkY+e5BZBWUokzf6NXYxiV1dfo3ugIDALbkXffqPEJl0TdnLZ14PSlssE4G/+8570u4t58qadcrS6j4t2bNh0xI1mFckrZdu2YA7Xqj8IW1HDlgvnG5IC5ajSMLxyCvuJK1gNTSh5NAktzu4hRSwBCAjhR/7mQr640Y8uY+vPvYAEwcGGvx2Amlb0Qgk9I9DPnXqv09DI9pNlE4e6tVfJHBTUrAnGfwxqR+OHDhDnadaZ+ISLdjD1F4vgQ8N6Inlk5Jxp4zpXhjR4FNpY7OTtvG2x29UKluMCK3qALl9d4pZ5fqm/B1/k2vx1NZbwwYifVOY3wAbWVQ9tBhGYDfxcdabGrN/oucxfS7hiptPhsT4yNKLcdbj5rLRedu5laA7clhPREdohBEvwM2+Hu/UNvUgpc3n8RLN6uxeKLZQA2k3A+hcupGjc/OFaIIQp2Bv1JgJhl6FMzVL9cqGhwaHtZ4M9bMJLN8wsSBOoxPbr+xoxMfhV5x6C05V8qR0buL18ep5aiEPFCud6cyPmhoNxn9oNClj3yo09GVDZlJWssD2dxCYv1PV7w+tqOOsrSIlasE2zBlEI4sHAN5kAQj3z3I+aIbF6XCC/f3wr3aECzadtbj7O3OyseHzRUI//qJ+0aEIvxicCMomNmvCwpKan3iCfjXEe/nGFf8779PYsZ9PREXrUbXUCVSuodj87FrOHGtCoW39HgqPQ5SCeFVP61A4GZVI+56Gb7iEqFXeNKwrnbhG66rXawxkRTWHryEjdm2ZU60i3BckhbvH7jkdenjvNEJSIwJddjGOqugFK9vL/BYTMgaAnDY2I3Ougac7+Z1GiWmDeuOVfu5LfOUEMDnzw1HZUMzuoYq0dJC4qmNeZyeozPAh+y0iP+JUMlAURSqGzuuUBoNQQDBsvYdwEX4gd6MHlk4xm/ltmyqXTqN8ZFVUIpF35x16MKmv6YPZgzGGzvOeW0YOCt34rLHTLhKhnceG+BU1c5dIz2+8htUctvJJlIlB0lR0DcaxV08x3TUKiUR76A7b4sex84DvYY52oz6EjbGR6eodqEXfWexc3pRtE+a8gRnzX247jHzwfS2m8y6hX1OUQVMJIUJyTr8+NpoRKodq+VxbQjQvULsdzmVDc2objU8AkP6JnDI7Bvj/kVO8Oa7IGD2cIkIEwqAkRR+pZc3RKpleH/64ICpCOObmDCF3w0PtnT4nA8TSWHRN2fdLrYUzJnC3mLd3Mc6t6S81sBZPkmIQoq0Vs+KIw8HHUbSBMs5+UyuCJZJMHGAFvvP34HejSs5LDjI5jVhyiAM7hGO2/omXLhd59U4nLmzmYQvCAAqhRT1gVaiQzAzIQd2C8ONqkabTrwalQyZfWM8yrCnYO6Su/6wmJMiNMKDg1Dd2BJ49zJLKuuNiA5RYMmkfnh580nezxehkrHqZM33cez5x5MpyOgdzflx+aTDGx9rD17yWbXAcyN6WgRn3j9wCf86cgV1PEwC9ydGQyohnIZxyvRNmLMpH89nxHF+bnuajCS25d9i9FoJgK9+ex8Onr+N7adKUFlvxI8XyzkZx5ND78H6w8Xtfs8kqPjBjFQAlE8mMS7pFqFi9LozJeZqjxBFEEiKQkOzCdUNRnydf9OjVunhKhn+OKEfBveIwLKd51hJzYvwx6tjE7H1+HUgQPJJns+Iw/cFZR5vym5VN+Lv//2F41G1J0Ilw7HXM/HegUtYe8g7sTS+tIfK6wLvGezQYRcTSWFj9lWfne9X/XXIKijFgGV7sfrAJV4MDwCYdV+cyzAO/bvtp0oYHS/dQZiIKWzWrarGFvx8tQqfZl/l1CPz+OBY7DhV6nQsBMwTiH2DJ51GiY9mmeWmJw6MxeSBgeOyBICM3tGs3M51hpZ2YTFP9IiqG8xaAhOSdfjz5P5eaUWIOCZUyfya0vfxfb2iAsoQHJektbSgnzc6gfX7T92o8ok+EwWzTIM3ngU6SslX8i3dbyaQ6NCzRl5xpc+SrnQaJarqDV7tniPVcphMpMuW4eEqGdISotwq6tFhJFeS13R2dGGp73QQNmZz76rfdtK154UCUNVgxFe/vQ8SgnCoRQAAa6YNxk8X7wZEy/YQRRBIksKSSUmc67Qw4U5tE7IKSv1y7s7A1JRYbMp1rwo6b3QCFoy7F1IJgR0MNxtCIFItw5CeEZBKCAyPj0T2ZW48oHxAG9vD4yM91t3h+xlZsfs8/nWk2KIjFQh0aOODS7EVWjzIUX8YAFgyKQl/2XXOq3P8ekg3DO4R4VLV8rHB3ZBXXMlYJ2Bwdw0OXLjr9O/ThvXwqRiYPzPwy+sMmJrSzfIznahLGyNV9c0gnGRS0t+7WiC5IXWGFjz1aR7CVTIMuCcMZ256b0A+ntoNJElh+yn3YbTiu/X49883RMODJ+Kj1Ixel9G7i8WADhR9B8C8MXrgb4ccNlVkShzDa8QFd2qbsK+wTNCCf3S4PVASTzt02IWLh/Hp9J7YMjsNp5f+Ch/OSEWEWm7zd61GiVcz+6CwtMZrlyeds/DRrNR2IQJ6Sfw0+yqmb8jFCoaGjivD48VR8YiLZpY34E8ImLs/eov1/bDnzC0Me2sfpm/ItfS7eHmz84qoYLkUCzITBWF4WFPdYOTE8ACAbfklGNMvBhEq93uSL3Kv+cTlLXSI1n8vjYp3GQKjXzf7/niXlUJ0v6WuYUpGFUVVVtV5tMBgoBQileqb8PHhYo/uI51GiRn39fRZ1VV0iALLdnq3ueQb6z4xgRCC6dCeDyZqn+6gu89mFZRixe5Cm1JctUKKhuYWTj0Hy78rxJGFYyx9aPYXluGT7Kvtxs8kZ8JdpcfO06UYyYEsMN98MGMwxifr8Fl2MVbsPu/RMSRE20S9ck8hPnaQnOoKc5Km98JwQmf+lpOMnhUuRPICEfsEXeuWCX+c0M/yzNIJ1Y5eN7h7uMPwLL2OThmkY/w9rNhdCE2wDOX1BnQNVVrCcL7u4D2uX1fsO9++aysf0A3rTt2o9ihniS2hSim+zLkaEPk0dJ+YQOjv0qGND7o515xN7R9GJlh3n3VUVcL1Lpi+cT7LLkZ0qALRIQrsPuu6NwPgfGJxV+lRqm/ChbJaT4bqEyLVMrw5NRnjk3XIK67EtcoGj49FUuYeNr+9EY8NP7EzPGi4aPwkdIS/X/IvJAUsmdQP0aGKdnlDdH+l9IQovD4pyWmvk4kDY/GRhGhXIq/VKLFkUj/8ZRdzPaBSfRNmfnLM8rNOo8SLo+LbhTK0GiWmDNLh3z/fbOfd40JN15Ou2p5g3bDOVzkutU0mZJ277ZNzcUUg9Hfp0MYHYO5m62nfloeStci9UoFlO8/5dFJmu7uPUMs93okev1rp0fv4QhkkQVNrOVplvRGvf1uA178t4CzW+q8jnhkegHkSivTiWot0DKJDFZbcIfu8IdrIcNbEksZZl+21By97tcMu0zdh/eFifDBjMDTBcuRcKQdAIEhCYM2BSx5tUpjQyFMVx9zRCRjRK9ri2bE24gIpx8XXBMK16fDGB2D7oGdfLmdcq/1p9lV86sNSXU+ZkByDb/JL0GRkX0P+30JhWfRNdnXwXCZ4UZb/8ZxHUmID4p4Q4Q96Yncl8GetPuzMA2JvoGQVlHodwqWVhF//tgDKICnvDewImCvwvs7nxwuRFh+FC2U1uFbZgJ6RKkuFDAAM6RkhbgbscNRsVKh0CuMDaHvQh8dH4usTNwIifseUzcdu+HsInYaxfWNQ02jkbbIVETbBMglaWkjsOeO4zNi64gCAW+OEhlZi5gIKtNHum8oMPrzCtOrw05/m2Rz/rT3nMfv+eAzuEYHl3xU6NTzoUDRdpdgZoHOGrFW2hUynaSxH46rBnIiIK8JVMp/sJj1hXFJX7C+8E5A5GyEKKUYkROK/hc4rs4SGqxwyAmb5en1D+2aKzhqArdl/kfMO03wTrpLhuRHxnJfqc5EcSxt5Y/rGIG3lft7bTAgBZ4atLxEbyznBXYM5ERFXVDcYBWl4PJ8Rhw1PD8Mvbz6EJ1K7uX+DwKgzmPxqeKjlUtbvcbU40p4HV+rD1uWQJpLCx4evsB6Dv/lgeiovpfpaDsqFX5/YDxOSdZAHSfD2owMspc58wNbJ4KzZp7f8/YlBAaHvQdNpjA+uu8qKdC48WaB8hSbYrD0jlRB4fEh3PJ8Rh0g7PRoudFI6KvIg37qo6aq2Vft+QU5RBd47cJE32W2+IAhgWHwk54mNT6R2w/MZcV7P08t2nrMYd3TRAdcVOdbaLUzuIFrDJXdxJrbMTsOq36RwaojkXCm36WwudDpNzoc7OXIhQycRPTm0Ozb8dEWwE9W80b0hk0qwev/FDmfk1Qv0mgPA1uPXkdg1BCt22+YXRKpleDSlGzKTtCBJyqYkU8T8XEWq5ajwU8Li2kNFWHuoKGBEwayhKODLnKuIDFEgUi3jLKzxdX4JHujjvfZQRX2zjdaFddFBWU0TKusMuFnVgB2nS23yRqyr7dxBEGbDY/HEJEsOirM1xjofQx4ksYwrWCZxKOPgCWsPFVn+WwghGHd0Gs8HF3XPOo0SX71wn6VbrC8njSmDdHjvwCXBGh7aMAUWjOuDVzITsW5WasDstJ9K6+F29yH0xaFU34SXN+e3m/iq6o34NPsq9I3NSEuIYtWErqNDf6dTU2L9Og7Au/wGfz5nK3afx4J/n3JqeHj63Jy8XuX5oKzYX1hm87NUQkDf2Iy/Zl3Ait3nsfHoNVTWNyNSLcMLGXGYNEDL2PAAzJov6w8XI6ugFBOSdZYmeY48j1qN0ibPhy7RNrSQmD+2N+dzDJ34nFXgXifKX3QazwcX7sGlDychIzEaGYnRGB4f6ZN24mahrQFYsVvYIaOmFhL7CsswIVmHCck6hCpkAbHT7hGpxptTkzG3VXHSVZw+0KDLLpd/V4hxSVqL4B79t84MrTiqCZYHdOn0BzNT8c2JG26bK/oD+hqfvF7FSlG4pql9Dy1P+CT7KkKVQTDbExSkEgnec6B1UlVvxCde3AP080U3yQOAAd00ZsMmRAFtmG2JtaMSba6xf/aFWP3SaYyP4fGRCA+WedXY7JeyWssXOSFZh1ClDDP/xd8CG6mSIXdxJk5c4751tExCwOhBXDBEIUWdA2XX6gajTVOjtIQojztAssWbieqtPectqpCO1B8DHWu5ZTr2/fr2Ak60EVRyCRqa2WvL+JNwlQwfTE9FWkIUpBICJpLyel7wFxICOHD+tmAMDzqM9cakftBqgi0L7oRkHUgK+NdPzDtaj+nbxWVfKqasPuBe08kbI8f6+dI3NjstrbY2PLgKs7AZmxCl1jtN2EUqIfBcRrxXx1i1/xIy3jlocWWV1/Hr9Xj7sQGQB0l4kcpla3gQMIdWQhTO3bwUgMXfnEVzC4ncKxUwGH0TIvL2QS5rbXDV0QwPa+h7aEKyDksm9fP6eA8la9EYYIYHYDaSJa0KpDT9dCF+HJHnkBQE5bWhYM610GqCkd5q3AHmBXcDC8MDAH57fwJeGtW+CZ/w9u9m9heWYc6m9qFP6/CHv4oehCq13mk8HwAwb0xvbDzq3SJTVtMmInS13PNeI64IUUjxm6HdoQmWo7mFRHmtfwXRaM/CyN7RbsW1qhqMSF2xL6CEfTpDCMI67KjVBHt9vGPFlQF73ejJWNT84Yfsy+UWrwdb8TRrhc70hCj8/ld98WXOVYvC6Yz7emLMP37wqlkoH2w9fsNpyJYOf4QqZH4pehCq1HqnMj6kEgLvPDYAv2uNe3vDsp3n0NzC3e2vlEnwYJ9oHCuuQlWDOQb5SfbVdl00/YFGZfZ2MFX19NTw0IYpMG1YD6w+EFhiS0LGkdyyt92e1XJJQEtadw1VsnJ/EwBeGZuIo0XlyLvKTTJkR2btocvYln8TSx9OQqiSXeiVAjBtWA/Lz/IgCV64v5fNa6xzl4SCq2o4OvxxpMjzMBKbKhxrwlUywUqtd5qwC82EZB0WZCZ6dQwKQFmNAZUctljP7BeDvefuoMruQfWX4TFvdG+smZaCBZl9oG8w+mR3uGxKf/SMVvN+ns4EhfZyy3S3Z8C1G9vZ3+oDMNxiTXmdgZX7+/HUbojvosYrY/sETBWXv6HDDZtyr7F+76r9FzHy3YOWUEVOUYWNfsW4JC1ezUyEJsC+i49+8FxIzhPDAwCeGxEvyGRToBPKqwPmMqeMdw4KUq1SKCyZ1A9Ppcfhgb8d4t1VSAD4YMZgSCQEXt9+tlNIIfuKcJUMJ94Y53ACctUYDUCHDUlEeaHtoVZIUe8g4VqkPXR/Fk+uFx3qtU9aD2/1wnbE+5JrVHIpzi4b71Pjg428eqc0PoC2rGOg48b8vW2q5KuOkc9lxOG++EifZYF3NrbMTnOa7e6s66qJpDBg2V7B6spY42yh4hOVXOq3a8NFGapI4CCXEmg2efaNf2TXQ4hvxN4uDHAmuRuhkkET3DFSYbxN+vRVXD+zX4xgpO/t9wjCdFiyw1m2u7XhER2iAElS2HXmFnKKKnDk4t2AMDwAc07LR7NSceKNcZg3urdPzunPayOE54QNQnX7BwqeGh50oqtQpdY7xirrIdaSu9Y7PwBYe/Ay590aRdoTqZYBFBiFdiQE8MLIeOw6U8ppKEhCAPPHJKKmyYhvT92yMbq0GiVqm1oCqnrHHkfZ7u6EjpQyYe9LrKXjrQWcMnpHY+0h99oOarlU0JL5HQmhLn4dHaHrfHRq4wMwW+WOvphXMhPRq4sa87eehLACUx2LN6cmo7yeWSkxSQFj+sZg0UP9kFtUgZe/OgF9k/dGQagyyKbCJpB6orgLjensKl0AZkJHTUbhJZVOHqjDuKQYm/CQPe4qedr6JN2DNQwEqIRChErWLhldRIQJQtX5EPb2xo9kFZRi6c4C0fDgkdn3xyNCrcCl23WM33Ontsm86BDgxPAAAH2j7XGse6IwNYz8gU6jxIhernc0UwbpLDkcOUUV2H6yBK9vPxtwrnsCwIlrVZg8MNZGwMoaOoz0ULLWoq9gfwzAXP0zf2wfS/JiIMCl80AMgnQuRJ2PAMKXEridkRBFEH4zrDu25Zdgw0/Mez4AbRoNC78+w9PobIWB/v7rQbydx1samluw66zrxlE7T5di0D3hWLH7fMB2dQbcu5CZ9MvQ2nX6fOexAQHznOs5lH8PhM8r4j2ONH6EhGh82MGlBO6SSf2g0wTjL7sKxbJeKx7oE41PjrAzOgDzTr+q3oC5m0/yPoHSi13hrRrB9v6w99g4wtzx9qQPRuMb9heWtTM+nG0W6J/H9u2C396f0C5UMyFZhxdHxbNqeiYi4oxQZRDqmloEYdxZe/mEmvDbacMu9uI1zS0kcooqsGrfRU52iDqNEs9mxEMiARqNgZusyCUEAE1wEHafLXP7WkfvXTIpCSt2n/fpw/3WnvOCNDx8ibOpyx9z2ifZV23ahDPZLBy4cBdV9c3tJmETSWHnaeG2HBcJHMKDZXj3sQEA/BPWsn8WtRqlpcmnUOmUng9HLlquZcyXPpyEfa3NhoRgCfsbWpugxYOLHB4sw3MZcdAE+6c3gq8JUQSh3iCMHRTg3E3vryIG6zbhecWVjO6JN3YUQBMsQ3m9wZKwyvS9IiLuaCFJSCQE1s1Kbbe2+EJ/hqTMnvboUIXLhGwh0emMD2cuWi4n0gWZfTAuSYuR7x4UzALia2RSwLqpbUyYglFjOpqHkrXIKapAdaMR1Y1GrNp/ySs5ZZVMggYBVnBYY60uOmdTvigm5QTr3A+m4czK+mabqiWdRomJyVq+hijSSme5h+sMJkvD0R9fG23TDK9P11A8tTHP7TEeT+2G7SdLPF6LokMVmJrSzbM3+4FOZXz4oqVxhEqGeWN6C35XFa6S4f3fDMb/bD3pMqxAwNxYjo3lLiGAM0sn4NSNapTpG5F9uQL7zt9mbHgAwPcF7UMz3iTdDYuPxI8Xyz1+P19EqeWYmhKLcXZ6FY52UJ1lImcCXT5YWedZNVKZvgmf+KAdvUomxYZnhkLfYOyUuV90ki9JUnhjR0GHb52w6JuzUAbZfs/aMCXCVTLoG4xOn9/w4CB8k1/i1fMt1KoWZ3BufJhMJixbtgybNm1CWVkZYmNj8eyzz+KNN94AQfjXDeQLg+CZ9DjsOnOLVfkoH7gTUXrnsQEICpK4zWegAHwwPRUSCWERYtt/vhSfHHHeMGr2/fEIlkuhb2zG8l2FrF2OBAHOS5xHJXYRhPERoZLh2RHxiItWuXSPTkjWYUzfGHyZcxWHL5Xjx4t3RcPDiku365BTVOFxozf6Wkpa7zW+rm2D0QQJQWDiQB00wTJBa8ZwyQsZce0E4MYn6yyCjlfLG7C6VcRRqPc1AeDFUfH49883Gc1hFOieM7avvV3TpjnjbAOhb/QuzOpIz0focG58vPvuu1i3bh0+//xz9O/fHz///DOee+45aDQazJ8/n+vTsYJvsRWVXCKYdvAUgA9npOIvu86hrKZtd6gNU+DPk5OgCZZjj5syTZo7dQY8OrjNnZeeEIUgiQQbfiq2cRESAMb07YJRfbpi1b6LWOPhteDa8NBplIgJVXKe18OWJ1K74d0nBjGKxTIpHQ0UItUyTB0UixaSwpe51xm9vqre+S4RMLdtX3voMiLVcq/GRt8PfHqVsi+X405tEy7druXpDMKCALD7bCnG9IvBrjO3bIxs60qle7UhnN7jmf26Yv/5O07/Pi6pKwpKato1UkzuFoYD5+/YzA0SwryJWjwxCYPuicDLm/M9Hhdduh+ukoGkKIdVat7ee0KuanEG543lJk+ejJiYGHzyySeW3z3++OMIDg7Gpk2b3L6fz8ZyOUUVmL4hl9NjCpmvXrgPaQlRNvLxVfXNWLGb3QP/RGo3/P3JlHa/b24hLTvz/OtVqOVA9EseJEGzh+2jnfHSqHisP1zs9x3WmmkpjGKy/tCZ4WPxJQAseqgvuoYpoQ1TYkjPCDzwt0Mu1UdjwhSYNqw7VjNUH+Vi3M9nxOH7grIOYegJFZ2dxgqNiaSQe6VVrZhB6bi7c/SPDXNpgDw3oifuiVAhMkQBbZh5Ppy7uf2zRi/jH8wYzKlGjiJIAgOH81uIIghPDr2nXdjWX7BpLMe552PEiBFYv349Ll68iD59+uD06dM4cuQI/vnPfzp8vcFggMHQtjOvreVvd+BOetkTQpVBkBDMNBd8Tc6VcmQkRlt2G1kFpQ4fNHfsP38HJpKy3Ni0kmSZvhHnS2vx48W7nI2Za8Pj1bG98e+fb/rd8ACA8loDdpwqcRlu8UVekiMieOhgTAFY+f0Fy886jRJTBumw/nBxO6OB/rmphWRseNDn8JZxSVr8aVIS8oorkX25nFFvmM7GvNG9kdE7GlX1Bry+vYB1+XmZvglzNuXj1cw+lpDjkJ4ROHGtCgfO34aJxWPvTGq+VN/k1kjYeNQcLtZplFgyqZ/T0n3aW7Hom7Oo4UhJGQCnhgdgbh76afZVfJp91amBJ1Q4Nz4WLVqEmpoa9O3bF1KpFCaTCW+99RZmzpzp8PUrV67E8uXLuR6GQ6QSAksfTuK0kuCptB748IcrrN4TopAiVCFDKc/JZ0V36y3/7c2iVt1otFQXBFo4oLi8XhBjlRDAit3nLT87myh8magcogjCiqn9odUE46dLd1jfx2wp0zdh/eFivDgqHjtP2zYHpJOavSlJtM8Vimh1c9c4iadbK0DSIYHh8ZHYln9TEPeMkGhsbrFsYjTBcta5K/T1t27WySQMGq6S4e1HkhGhVli6L//+P6dYndsRZQzE9yiAU8ODb2gDT+j6HjSci4z95z//wVdffYXNmzcjPz8fn3/+Of7+97/j888/d/j6xYsXQ6/XW/4VFhZyPSQbJiTrsG5WKjRu+jpEMYwl/3y1itHrHkmJxQOJ0ZAS5rIsTwwPRRC7ryv3SoWlo6S3i9qd2iZLOCCQJuYdXopILchM5KRNu/0kS08U1oJZ5t83en0uptQZWnC9sgHpCVGQEPzrDdKXYMepW/jr4wMxb3RvzBudgC+fGw5lkNT747ee4PmMOGyZnYaf3xiHdx8fCMB1nxdrDxS9QfE1bL3lvu46bC3ulpYQxUlfHCb5V8EyKcYn65CeEIWpKd0gIQibHDZPEYInlGvoz7T8u8KA6CTMuefjtddew6JFizBt2jQAwIABA3Dt2jWsXLkSzzzzTLvXKxQKKBQKy881NTVcD8khrnZYCzITMefB3i7j04B5p5XH0Pj49tQtVuOjd2V/f2IQyusNFoufzYNX1WDEqn0XkdE72utFLVIlxx+3nemQD60jrD0TO04xLxFminX/mHFJZr2JtQcvY/1PRZyfyxUbs68itXsEblY2+OR8FICyGgOe+rRN92Cz+jpnJZgEzGXaix7qh7ziShhaSLyamYgteddtE69d5CD8UuabSrVHU2IRGixDz0gVYkIV+J+tpwAwWxj90XV42c5zGJekxb7CMt5Fs2hK9U34LLsYz2bEQ9pacSfiHHc9kIQE58ZHQ0MDJBJbq1wqlYIkhSHwRIcfXPF5zlXMebC32xANXx1vrXdlGYnRAMzJsp5Y/FxVBVwoqw0oj4c3LMjsg3ljelt2xNFqhZt3eAY9Uaw9eBkbjxb7bEK3prrRyEgAiU+41H6gr+mQN/fZJEBrw5RYYJVv4CjnJqugFMt2nuNkZ82E7VYbkki1DL+9Pw67zgg38bWsxoDcogq38yfXrNh9Hht+Ksb04T1gZJMcIiBkEgJGH3ojAsFI49x39/DDD+Ott97C7t27cfXqVWzfvh3//Oc/8eijj3J9Ko9gEn6orDdiyJv7QJJgFKLhGke6/N7eTFVeJhPeqPLNzpgtMgnBeS+FrcftykF5TiBftf+i14ZHiEIqtkq3wr7yqqymCav3X4QiSIL0hCiHhsecTfk+Mzzsqaw3YsNPVzF5oA5bZqdh3ugEv4zDHTlXyv1iHJXVNGHV/osBmwzsSVsJAFDJPQtHBoLgGOfGx/vvv48nnngCL7/8Mvr164c//OEPeOmll7BixQquT+URTMMPtU0teHlzPrbl34TehzvSoT3DcWThmHbuYG93397a3HVNwlQmNJIU56Eg2m1Jc6fWPwsSG+oMJjye2s0rCXpnsDVqVHJhGkIUHMfD/VVh5IgNPxWjqr4ZiTGh/h6KE4T4zXoGAXN4dfb98byfy9N7i+3Vpj9TIAiOcW58hIaGYvXq1bh27RoaGxtRVFSEN998E3K5d25/LsgqKLWpOGDCvsI7Pp2U7olQOa7V5vCZj7Dz5DBJdvvmJLuclUDH2tPkqYS3r/k6v8QrCXpn0Pe/muEu7KVR5l07X8tUqNLzaLG9YQn4tsKICYu+OYOLDAXJQhRBCFH4pktGTKgcYQyvPdvkeC5gk7RLv3TJpH7YdUa4nY1dqVTb4yyJWqj4/g7xE7RblWstA655PNWxMEs5hwugvQdQKfO+0sBbPHUv8kXXUCVMJIWcogpcrah3/waWCH9qaA+TiVCnUWLOgwl4NbMP514YeldHty73FPsQptDi4zVNLfjgELPE43pDC+oNLQhR8P/81BlMeNtKt8UVhhYS4/vH8DwiW5xFNh4eqIU2zNZzTIe2I9QKQRme3uAoXC9kOkVjOSG5VV2hlktxX68o5BRVWBRJ6cQ4LmN49rvjBhbWNV+8NCpBEL0e6CqjiloDhr21nzdjVa2QYkJ/Latme0KGNqamDNLhgb8dspnQ5VICJpKCycsvlgIwbVgP7w6C9vHwq+XCzGdiAn1JZVIJABMvSrUquRQNzSZWu3DAnCQvBH6+Vo3DfxyDE9eq2s2rfFSy+RpaAE4ICqds6BTGh9Dcqs6Yldaj3cRNl3yOS9Jyrs4qBOjFft6Y3rhXG4JlO/3f+TO5WxjmbXUtQOQtdQYTK8PDXaNAf6O1Ui+1vz+bvbU6rFi1/yJrTQxrwpRBSOkebjHwo9UKbMlz329G6FQ1GPHwQC1+vlbN6VwXHiyFUhbk0QalpqmFUZ8evinVN+HEtSqHpaeBkJjpDHruXDCuT0AZHTSch11MJhOWLFmC+Ph4BAcHIyEhAStWrADHLWRY4Wu36qtje2PNtBRsmZ2GD2cMhk7j+gbXhiks/UfsJw5ajGpfYZlF/CjwbjPX0DHKCck6/OPXg/w6lsE9wrGv0HlvCGeoeU6yTOsVCbUPXOueICHMPVx2nCr1ySLjTcViTVML+i/NwvQNuXhl6ynM/OSY341drvjuTBmWTErCVy/c53G3X3uqG01eVQA9mtKN1T3x8ECtx+dyxZHLd7HjVAlyiipsEo7plhvOnl0C5jJoT+FrTgi0/A5HdIqutr60bgkA//75Jo4sHNOulXRZTRMq6wwIV8lR3dCMSLUcXUOVICkK/7PlpMseA8u/K8SPr43Gq5l9sDG7mHVvBSFCd460jlGW1/s3uTP/ejWr16vkUgzqFo6cYn5czLQb/cAF7vrncA1JAa+0CmQFAp4aL/by7UJkxe5CHFk4Br8Zdg8+Plzs7+FgTL8YbDtZwriUfEw/LX66XMG55o11Do21gKCrlhv0kv5oSjd8kn3Vo/PydbtEquWYmhILTbDcpu9WIMG58XH06FFMnToVkyZNAgDExcVhy5YtyMvzn5CRtw3lhvQMx61q902LgDaRo9yiCkhaFflciRr94evTbo9LHzNt5QGbHARNcBBaSAr1BuG6411BUsD6w8UY3CPCYoAEmhu0odnEm+EB+Cf/hUnPjc4IRQEPJcfg+4Lb/h6KU0r1Tci9UoGdXrYV4IIQhRQFJdWsDInyWgPvYntl+ib8blM+FmQmIi5aja6hSnwwI7Vdt29aBVcTLPfY+OCDEIUUFfXNAdtQjqZTdLW1tm494crdehx7PRMnrlXh+4JSfJFzze175m7Ot/FO2N8gnrRNt09+FGInXU9Y/l0hxvSNwYlrVSiraUKoMqidSJSI75h9vzkECAReDwwCQEyYAi+M7IW39rArq2dCQpdQAMI1PgBzoidXeR/aMAWaWyhUNrBPvK4zmLDy+18Yvz5CJcPag5dYn4ctbU3u2s6lDVNg2rDuMDedpZDeKxpprWJ0JpKCNkwpmPBcnd1mszTAGsrRdJqutuOStHg89R58nX+T9XurGoxY98NlvJLZBwAYGR/2YRHrjoPjkrQBUX3DBk+z7J3JYYv4HgLABzNSMXGgDoN7RARU92KgzU2+bEp/zluX02iCZQgPlgk87MndzLJsSn+QJPDyZs82bmyo8kN7AZqyGgNWH2hTT/36RAmWTWkLzUwf3sOmI6/QoAX0xiVpAyYE0ym62mYVlGLkuwc9MjxoPjh0GdmXyzGkZ4TLBCVnWHcczL3C3c7E3xAw75S1bpJq3cGl4cFUDEvEFo1KBrot04RkHY4sHIMts9PwfEacX8fFlJgwhWX3x1f47q095wVreBAAIlUy5F7hJgxIi5dNHKjziQqokCirMYdm9rQKkMVFq/w8Ivc4EtATMgTFcRlK9+7dsWjRIsydO9fyuzfffBObNm3ChQvuBWpu3ryJ7t27Q6/XIywszOvxeBLecEWkWo7HU7thw0+eJ3PNG907YHsUOOPDGYNRqm9irSArwi0SApiYHINdZz0PDXw4YzAmDoy1+d3ynQXYeNS9x8+f/GliPzw/Mt7iKh/57sEOV5ruS+gN1ouj4rHzdGmH2TCxgSCAD6YPRoRagekbcv09HLesenIQHnUiVOkLampqoNFocOPGDdxzj+txcO75EFJXWz7ExSrrm70yPADg6GXhVi54yord5/FUepxHXiE+CA+WCWIcvoak4JXhAQDztpy07PhoftVf+LHkt/acx8h3DyKroNSS5wV0vNJ0X0G1/vvYgQSAkLk/MRp/mtgPXUO8b+lBUcDLm0+iqr5ZMHObK4Su4G1Nh+5qK1Rxsfwben8PgXNoIR96wvc3z2XEizteDyEpc4x/zf5LFk0EumJM6NC5VVkFpZiQrMO6WalehwQ7MqHKwAlRMu3p89Olcry15zzqjdxVAa7YXYglk/qxmlOG9Az3ubESofJ/DzWmdOiutkLr2dDRuVPbZJnwI9X+eQjo/h+JXUMQruK+w6s7fNFjw1es2n8RGe8csPEkCH3nZ51bZSIpTEjWYcmkJL/dj0JGp1FiRK9ofw+DMWwXKy4lCEr1TYhQK7AgM5HR61UyCU5cq/b5BqjKg6okf9Ghu9oGmmaEryAABPGQEU1f7wnJOuQuHuuVMiCN0oPumFMG6cylzhxkz7O5Smm9ItuVwQU6ZTWGdp4EoXtA6AqqvOJKZBWUYu5m4TeU9AeNzS3YWyjssmFravxcDXentglx0WpGr200+j7NAEBAGdkduqvt8PjIgPoyfAUFoIVjFSmdxiykRiMPkuDtRweAgHcx93sighm/VhumwAczBmPnaW5kvofHReDssvEID3bv7g0PliH3SuBkmrPF2pNAV8G8/GCCv4flkj1nb+H17QVi+M0OuoqlmgedID4rzWi1Z01wECL84NUsrzUgOkTh/oXwvthZESTx6FpqNcznS3/ToY0PqYTAm1OT/T0Mv0L4yE/uqMcAFzF3pjk7w3qGI3vRWE5bZOddrcKovx1iNEkLtfySC6w9CYD5uUpPiMK92lD/DswNX+ZeF4THg4nx6kvqDPx4EJQyCT6cnopXx/bm5fiA+V7UN7bgmfQ4n28sV+w+j//99ymEq/hJZl8yqZ+lJ1jhXybgzLLx+OqF+/BQcgyjvk72G0ChI6ynggcmDtThpZvxjPschKtkeG5EPKobm7FRQJK6nkJRwILMRHSPUOH3/3eal13gS6PinSrrTUjWYUzfGKSt3I/KevYLNNNOrpmt4jpc5/kIYfESCvS1NZEU8oorcek292rEHZF+2jDk36jmTfjMFzAREWwyknjm8+O+GA5WH2CnhBqplnPyLN+u5a/3VHSoAlNTull+ziooZSX0F2hN5jq054Nm8cQkfDhjMKMcBH2DEav3X8R98ZEBIazD5F7bfOwavj1Zwpv7eefpUptOkfacuFblkeHBhg9/uIysgtJOn+dDALwl2nYNVVoE+6ZvyMVaq2ZdIs7JKa4MWMNDp1HiwxmpiAjQ8DWdgJ67eCxefrCXv4fjEuu5i9anYmp4qOVSjEvipyMwX3QK4wMAJg6MxfE/jcOW2Wn4568HOS3bss6WX/RQP2T26+K7QbJAowzCE6ndGDUAu13bjMOXy3kbiztlPV9UHekbWzBnUz6q6puhDWMWl+2IfDAjFe88NoDTY9ITeFW9gdWEKBLYEDDvpicO1GFYXASnx34o2TcLJQXzZ5AHSRClZjYvZPbryu+gHCAhgKpWz4wn+lT1zSbOlG19RacxPoC2WLUuPNilnLd1Z9pzt4TpWtY3teDr/BJ/D8PC/sIyp3/zxhsRHiyDhkXM/E/fnkUjh/X9XDN5ID9iXREqGT6aZe7LMiFZhw9npDLyirmDPsSSSUlYsfu8mLwpICYP1PFaeaRSSEGSFEwkhSMcb176akN9UgqvlEnQ2GxCTlEFwhlqYPgjcEFr6+w5U+qxPlVOkWh8CB6mO/GcK+XiLo8h20+VOA29eCNQNbJ3NOPuvRTMzamcvV4THMRbspg7aO/Bz1f5qYixb8o1caAOa6cP9vq4Wo0S62aZ3e7isyAsjlwux4+vjcZXv70P4cHcL+T1BhNe3nwSM/+Vy6lmhlouxar9lzgphXdHk5HEgv+cxvQNuYy7HKsV/kuFnLclH/89V+r+hQ4JrK1BpzQ+mO/EAyd5x99U1hudhl6spa7Zsuuspw9ie1TyILz9iDkk4etvlgIwbVgPlNXwl7C2aNtZZF8qtxiBEwfG4iMPdTnmje6NLbPTcGThGExI1omCfQKkusGIE9eqICEIXqutuC4h57rMnylVDBJOdRolekT4r4kcScHjHkrpASQYB3RS44PeiTtbgOhdanpClC+HFfCU6Rud/m1Csg4LMvv4cDTtMasUyv0iuf18RhzvnTGrG42Y+ckxS38TwLY77ZppKa27ZNc7u3CVDAvG9UF6QpQle96fibwBlMDvc7Ivl+P7Au4MdF/gr+RbVyYPrUf05NDu+DxX2A0UHRGukiEtwNarDl9q6wh6Jz5nU367EjJ6nlv6cBLSekVBowyC3s/KeoGCu1K2eWN6Y0veNV53/+64U9uEqSndMC5Ji7ziStypbUJ0iAK//88pXsfly0x0ur8J3V6eznUCzMlsRpPrnafR1H5xoA12f3SJ9dNGOSDoaN2xfUWkWmZTgReukoECsIZlCa9QeG5EPHaduYWuoWatj0Aoue2Ung/AuQAWHeOmJ+1AK1+iCZYRPu9tEulG/U8qIbBsSn8fjcYx9A6eXpCnpnRDRu9oLJvSn7dQTJgyCMPjI9163LjCvr+JNblXKtxqp9Qb2mfOi11iuUUuFa+iP1kyub/FG7ggMxFVDUaf5KBwPSer5RKEq2RYtf8iXtl6CtM35Np4PoVMpzU+gPYuaesYt4mkkH2pHPqmwBSZajRSPnmYrNGG2RpyJpJCTlEFdpwqQU5RhWUh5MMo0oYpXKoA0qE0ZwqA45K0eDUzERoeEvceS+0GqYTw6QJur0pKwzQj3tHrxC6x3NHsxvskwi/aMHNYffLAWGw9fsMn5xzbtwueG8GtdlR9M9lunrfu7CxkOmXYxRoTSaHwlh7XKhvQM1KFlO7hWH/4Cj4+XIQGhuqaQiZEIfVJszP7hX3PmVt4Y0dBO9cm1wbR8xlx0ATLsSXvutOMfDq0Nm1Yd4euSbZKgmwZ37+tvJZewO3Px8e1ARxVdjFd9By/bkKyDuOStFh78DJW7b/o1dhEApM/TeyH/OtVOPTLHTSxbKBG92bho68M0/NrW+cqE0nhs+xin1VxHbhwFwcu3EV4sDmUT7l4FCWE5+FGugfO8u8KMa5V+VmIdGrjY+WeQmz4qdjmS16xm1k5VqBQZzAh3AcPu7W078o9hQ7l7LlcXAmYPQoURbldBDWtnpZV+9viuTqN0uKFmLMp36M8BgLm3jmuJglH3hZ6AadzTrqGKkGSFGZ+csyDUbjGOlHURFLQBDPTOrDPnKcl1e/UNiFarcCWvOucjtMf+Mow70joNEp0Cw/G23s803yhADyXEW/zLPqapQ8nYV9hGa8bDlfoG1ucXjvaTJiYrPOq0s/a8ynUwolOa3w4WyA7IukJUfi+wDets/ecKfXJdaUAbGMgshaiCHJo9JTpm/C7TfmWRDNPx+Bq90LALM5lbWTQHhfrJFDAvLjrNErOJkPrHR7AzrtjnznPt2fI1zyaEosnh/VAmb4RC/5z2t/DCSiWTOqHFbvZqW9ao5ZL0eynapfwYBmey4gHSVKYu/mk31QxXJ2Xno+4khgQcol8pzI+6N3brepGrP+JuwWSSdMl/8Kv24128Y3pG4M3dhTwei62OOvgSX9fXHhjXsiIw+6zpTbVMtowBaamxGLFbttFm/a42Dfik0oITBmk48Rws67YkkoIS58Ipvfo248MsAlJeeoZYsrYvl1w8MJdAL55jkbd2xXpCVEeKULSz3qIIsjm3opQydoJvXU0FmQmet01ur7ZhA9+8K4n0CtjE5HWKwrZl++y6i9U3WjEqv0XISHY3We+nN+5voeE3Ouq0xgfWQWlWLazEGU13FqCIYogUBTFuPuqr1HLJbxr/tMuvi9zrvLSBTZEEQQpAcGWPIcFy2Fv4DW1kA4NCfsyWBoTSWHnac92O/aTo9bKwPGkTwTdRMyT93rCb+9PwK+HdveZd4VOjGZSPmwfe6evrX3YbEjPCKz7oQgbs4u9Fvx6+cEESAgIqnGfTqPEvDGJ2HXmlr+Hgl5d1EhPiMLw+Ehsyy9hXf7NNJeCfqLfn5aCP393jvfmmFxi7/kUIp3C+MgqKMXvNuXzcmxnO2uhYDSRqG/2jZvzWmUD58ckAMy+v5dgkxvDVTKs3n+x3eTnzKPiLBnM034O9DEBc/LtuCStTTKtJ8elxeK8GRNT6JwYqYTAmL4xGPyX//JqyIerZJZ+JYA5CdlV/kGoMshGrr/JaMLx4kpoguWWcWcVlOKBvx2yuVaa4CAYWkjWCZkAcH9iF8G5y5dMMnvRhLCTvlpunmfo6jG+5nZrI14iIfDy5pO8nIdr7D2fQqXDGx8mksKib876exh+w5cOmQYPk/fonbt9xQcdohB6O3K2ngFHyWBcLDbbT5bgT5NsJxxPjrti93kEy6U+ue5TBuks4z1xrYp3D2J1g1kFli73dhd2s+8TVNVgxCfZV/FJ9lVowxR4eFAsNjgI4TLtR2QPbYy56hLtD2hvmD/F5mi2Hr+OeWN6QyohMCFZh+cz4vBp9lVOz/FUWg+k9oyEJlhuTtRm2JTOH6gVUptKP62T0K7Q6PDGR+6VCp/rXXRGNMFB+OnSXY/e68yVTe8shdyt0Zt7K/vyXctn5GJHWdVgRO6VCmT0bqtUoXeJrI5T34w5m/Lxqgdy+GxLBD8+XIxmE4V7woNxtaKe9fkcoZZLQRBwWcnCxZxQVmNwaHh4g77RiH2FZRiXpPX7Im/NvsIyi9y+M3VoX0Eb7rSRppJzu4xJCODL3Ov4Mtdc0RWplmF4nDArRgCzKGCkWo5HUmLbeT6FDEFRrvL1fc/NmzfRvXt36PV6hIWFeX28v+/9RZQg9gEyCQGjB4XpSyb1w7MZ8S4fFhNJIeOdg5zn6wgBnZXhNfLdg14vNvNG98Yfxt8LwLvrRseMG5pbXO7iNcFB+HDmEJTXGVBea/Brqfq80b2R0TvaEufOvVKBuV/l89p0jS8+mpUKwFwGDggjof0jqzwlf1dAvZARhz0FZR2mAstb6NnTPpfM19TU1ECj0eDGjRu45557XL62EyicCuGx7fiwNTxoxVF3hgdgju1OH97Di9EJFzoBdV9hmcedf21p+x5yr1R4bLDRoSF3HUglBIG0XmaZ+uhQ1/L6fBKpliO9VxTu1LapuvLd7ZVPXv/mLMb0jcG6WamICfN/ngWdp0TnyjhSh/5wRvsOynztvz/JvioaHla4aqkgVDp82CW9V7SgssZFPEuI4rsjrL+wTkA9snAMXhwV3074jg20OFhWQSkWbfM+18mZaixNVYMRq/b9gozeXRDtprcPn5AkaSPSptMo8VByYPZlAoDKBiPSVu7Hm1OT8cLIOLy15wLj90aqZahvMoJL/TTaGP0su9iyYbDXqgGA8cm2odODF8qw4aer3A1ExCmBICxmTYcPu5hICgOW7e0QUukdBWdaF67IKarA9A25PI6KH9jIpi/I7OOwcoYpGqUUH84aioPnb+MTjhPwmKANU6CphYS+weh3f6NQtHcIAugaosDtWt92ch6VGI3Dl8p5OTbT59cXGjEi7VkzLQVTU7r55dxswi4d3vMBAPIgiV+MD5VcgsZmst3DR+/1NRz28/CmF4CveCEjDpkeJkTRWfaB5GoNV8mQ93om3jtwiVHe0cbsYq8makIiwcx/cSfRHqWWo4KFbsvtGoNl/P5e/GmPkjv5e97HQQH//E0KJASBfYVlnFdlOKNnlBrgyfhwplVjja80YkTac+UuN4nbfNPhcz7yiiv9Vu0yfVgPrJvVPg6q1SixblYq3nlsACfnmftgAl4YGcfJsfhAp1Hio1mpWPJwf4s70FG3W1fQCqCBRHWDEV/mXAXTZdjT/ASVXGI5H5esmJoMnUbJOG5PL/jhKpkg8hQoCMMgL68zYHh8JL4vKPPZOeua+JvzmOQX+EIjRsQx7x+8hD1nhN3RFugEng9/ivWM6ReDtF5RCFXIkHOlHIA5RprWK8qy838hI85rF7k8SIoPXUgWy6QEjH5q4b0gMxHzxiS67CDLxI1rIin8++ebvI+Xa/is/pBLCWyYNRQLt59FQzO39/mCzD6YOFAHiQSsyiopmI2gr15IhURCWGL/VfXN7aTmfcXYvl1w4IJnZeBcEK1W+Hwx3n7KtRKpBCSGSy6gK6pxB+HII/uCZLEXdZdfIDSRNK5QySQgJITbXCh/QlLAy5vz8ZHEv5Uv7ujwxoc/Ffnyiivxh/87bTPpbMu/abPQhgXLvDpHTKgcnx117a4PV0pRXu+8kyJbwoODXHZmtGbr8RuYNyYRgPMYMBM3rqjX0p7VvxkMuVzKeQmyJjgI88b0BmCualg3K5V1WWV5vaFd3JlORtxfWObTnJSTN/Q+O5cjfv9/pzFxgHCSX8dL8rBU9gViiTYhs1tUJJYbn8ZecjirY9H6H/YIQQmVDxqMJAZ3D8fJG9X+Hopb7FWUhUaHD7vQuQL+uPxrDlxqN2HTC21WQSmyCkq9bi1d30y6bUZ0t74Fg3tovDqPNdWNLZg0gJlFTe+OXMWAXblxTSSFnKIK/DWLebZ/Z2DyQB0mDtRhXyH3rnz78lrrssp5o3szOoajxYeujljycH985CAcSaPTKPHhjMH46oX7oJZL2X8AO/joN8SG2zVNPsv1cMd4SR7WyVZDC1sFVS0qsU62GuMleayO92n2VWQVtHfxD+kZASZrnjCXRecQAC7dqeP1HEoZN8syPfcKlQ7v+RCCIp811qWVXBQaMe0tk39dj4QuKhTd5ab/ypHLzJPZaO0FVztnR25cfwsZCRVNcBDWTBsME0nhWzfudU+oN5jwz//+gpGJXSzJwbThYG7mddOpGBrThlYTknUWRduymiZU1hkQqZZDqwm2nNNEUpAFSXzbI4AH6Ovk76RwCUgslX1hGYvN31rHtlT2JfYZhjIOwTjqUwSYpfKZfNYItZyRcajTKDFlkA7rW5s1+usyUuC3n1ewTAqZBOBqxhNy+KvDGx+A565jvqAXWl/DleEBsEuO7BqqZPwQ0K/bc6YUL2/mp2FUoPPu4wMtsvN87eo/+KEIH/xQ1C4fx5Uxz1a/xZFOhDX+TBbnA3oxdnTdHPU24prhkgs2oRZ7JAQQiwoMl1xALslM8M5Z7gfdnNAdUwfp8Kv+OtypbTLrxFDmkF20WgEQ5mRd61YLg3tECGYe54NGownMrhwzhBz+6hTGB2C707pT24SiO7V476AoPsYn1rtgpu6/6BAF9py5hXlbAqODpD2PpMTy4o0A2ifm+mJXU6pvwu825ePDGamYONB8XmfGPNcNrYS8a/MG+xJ7Z72NDl64zWnvmK6o5vR11lh/V1kFpYwTrXecvoXFE5Nw4lqV5XNPHhjr1Hi1n8cjVXLM35qPqgZhdxf3NUw9kP6k0xgfQNtOa8+ZUt5UTyNUMjw7Is7rXI5Ax34XPKRnBCIZuFh/+/lxNHrQhlwo/HpIdxy+VM6pRyJcJcMH01ORZlWmXKZvRP71Ks7O4Y55W/KxFoMxcWAsgPaLgPXu1B0mkmL0PiHv2rxBGSTBV7+9z7KrH9IzAieuVWHXmVs2i296QhRKq5uw6yw3ZZN3EM7odQ2KaBCN7EIb9HfFVlisst6ItJUHbJ4Xd9VvUglh2dBkXy4XvOHxROo9+Drfd5V6nihI+4NOZXwA5ofDW3f+uKSuKCipsdn1hQfL8OyInhgWZ+4vEamWo6q+2e85JlwSLJMwNgw0KhneeWwAJiTrLLkbTBZkTw0Pf5YT0xAAquoNeDqtJ1Yf4M74rG4worBUjy9yr+LHi3fRxOIa2ecZaJRBqG9uQQvLy2wu3zuJj1rbmANtxjxtTNCLpysjhE2ptRDat/NBWY0BEoLA1JRuyCooxQN/O+TwegDgzPAAgDyyL25RkdCi0mEyKEkBZYjCwabeltw0d9fdeoftqbCY/bxAJ+V/MCMVEWp5OyM1UHLB6GsTqvTtMsu1B5IveJFXLykpwcKFC/H999+joaEBvXv3xsaNGzF06FC37+VaXt0aE0lh5LsHvb5pdRolfnxttI2r0J86BkJEp1HiyMIx2FdYJkos+wF6bbGfwL115dPfqye6Lc52xa46cmYVlOJ3m4ST+0MQZtVSb3k+Iw7D4yOdXg++ckDoahfANumUNlDnGF/Ff8nh0KhkUAa5LuO2/964boFgbzhbJ50GwnxCAPhgxmC8saMAlfW+yV2aNzoBC8bd6zePh1/l1auqqpCRkYHRo0fj+++/R5cuXXDp0iVERERwfSrWcCX0U6pvwolrVTZVGXM3iwusNaX6JuReqRAllv2Eo93PnjOlXucQWCcXMtVtMZEUcq9UYNG2sy5Lrf+0vQBj+sZAHmRbacF3IiYbuNqqfXvqFvacLXV5Pbj6zEsm9UN0qAJXy+uxar/ZwFgq+wKxVuW2ZYjCcuNTFp0Pe6G4q+X12JJ3HWU1bT1q7O8xrnN07KtlSvVN+PgwdzkwfDNpoA4aldxnhgcAZPTuIuhQizWcGx/vvvsuunfvjo0bN1p+Fx8fz/VpPILLh4MW1+Gqh8GSSf0QrpLj1I0qXCuvx+HLFZyM0598/fMN0RPkJ/ppQ6AJlsNEUpay1Td2FHBy7Du1TW51W+gSTJIEY49gRX0z0lbux9uPtoXrOqrXzJfaI9GhCovg273aUCzaJsPIxqFuFU7theLmjUl0mavTUXN0PGXXmVL85EF/HU8lIXQCTzC1h3PjY+fOnRg/fjx+/etf48cff0S3bt3w8ssvY/bs2Q5fbzAYYDC0WdO1tbVcD8kClw/Hp9lXMTw+EppgOSdhHJ1GiRW7z7M6lhB0S1zhTuJZhD8O/lKOg7+UW0IgmmBmegpMuFrewFi3hW1+VWW9sTXePxgrdp8X9P0dKFjPe+OStLhR2Yi39px3W05rP186K42mc37KapoQqZYx2ukzfR2f+GL+1LPs1zQxOQYnb+g9WlOEnmBqD+cKp1euXMG6deuQmJiIvXv3Ys6cOZg/fz4+//xzh69fuXIlNBqN5V9SErP6ck8YHh8JrZuGVxICCFMyU1VcuqMAnx/13g04eaAOczefZHzDqeVSzB/TW5yYRdxCh0C4VELdevw6Yx0HT3ljR4EgvWYSIrBUOa13w1kFpRj57kG8tcd9GWykWoYhPd2HyuljTt+QiwX/PuXUoKCv2fMZcdgyOw25izP9pjwNmHsXvT8txU9nd06vLqE4snAMYyVhmhcy4gSfYGoP58YHSZJITU3F22+/jcGDB+PFF1/E7Nmz8dFHHzl8/eLFi6HX6y3/CgsLuR6ShX2FZWhqca2WuHZ6Kl4Y2YvR8W7XNiPr3G2vx7Ut/yYrQ6K+2YRNx657fV6Rjg99X+3g0AtVqm/iNWxAAX7fFTvjwXu7cGb0a4KDnC6+dHdgb5k2rIelQmTOpnzGBl1lvREP/O2QQ+l0GjbH1LZ2tv5za2dreZDEUtHjSwOE7rD9SmYiogQYJkpPMDcdzegdzep9mUnC6R3EFM6ND51O18570a9fP1y/7nixVCgUCAsLs/wLDQ3lekgA2h4UZ0lcBMzdLzXBMjSbfKsz4clE6+9+FZ0ZrnovuEKtkKJriPeLD2BezCvqmznpk0Jzs7rRrztXb6AXdgLsF76DHHbHJQjCMh6b37f+/zuPDcBLo7zLlzOaTGhuIT3KS7PuQ2WPu1w3AkCUWo5VTw7Cltlp+PG10dAEy7HjVAlyiipgIimLWJ3WSY8fPvj7E4M4SZANUTg3HD0lQiVDWi9zHmFLC8l4nolSywMq14OG85yPjIwM/PLLLza/u3jxInr27Mn1qRjDJCmUAnDgwl2/tt4WCQyajCTUcinqeew5Um8wQSbl1sjhcrw7T93Ciqn98fJm/pRoQxRBnPfRsF7YAfhVL6K6wYhRidE4fVNvkxtgXUUyIVmHQfeEtyvXjFTJUMmgGmbtoSJszrvh0WbFOnHYvncLk5yfivpmaDXB0Dc2O9Uyse/xs2LXOV69XuX1bfmFbHMArXNE+OjvsvKxAdhXWIZF35xlVem0bEr/gMr1oOHc+FiwYAFGjBiBt99+G08++STy8vKwfv16rF+/nutTMYarElsR4fNQshZPp8ehqt7AOoGXDVws5K+M7Y1/H7/pVEtBKOWljqiob8alO/W8HV8THAR9I/cTvH156LgkLRZ+fRpf55d4fWxPGscdtqqGCA+W4bmMeMwb09tmMRmfrIMmWI6cK+UAzEmfJEnhqU+ZdaD1xkvqrHcLU6/B/sIyfJp91W05dnpCVGuvIn7v+Wi1AoB5Q0pSFMKDZYz7VPGZY7cgsw8AeKRp8/ae85BJiYDL+eDc+Bg2bBi2b9+OxYsX4y9/+Qvi4+OxevVqzJw5k+tTMaaj9ojoTKjkUjQZTS4nd22YAmtnpFom7vHJOqzadxFrD1320SjZMTw+CnNHJ7aTlw4UNmbzp7nARsWVCXMfTLDp0msNmw7NriCpNk2N8loD4/4mNNWNRqzefxH3akMsC4kjEbdt+TcxMdm3MX77OTQ6RMHofd+cLHGpZWLtVfHFPH38agVqDUbBKaT2iAzG0h3nPHqvvSEXKPCi+zp58mRMnjyZj0N7hFh/Hvg0uPE0EDC7HwFz7xNaiyC9V5RgjY/yOgNOXKsKSMMDYNfZmC0GtvrvbpBJJRZdHuv7gyQpG+EsbympbsQL9/eCiaTwryPFHknD0wuyM3XgMn0TPsm+ytGImUEbG3RZ7VGGBluVG++dtVeF9kqwRUIAz4yIw0YG12T9T8VobDYJrlIw+3IFbtd6dh+6Co8JmU7R24XuESEkS7cjw0es3hUEgPlje4Mk0U4+XxumRLhKBn2DUXATTtdQJee7PV9f+0Bh9YFLOFtSjZM3qm1c++HB3CT10tD6PxOSdVj6cBJrNzod5nClDuyP+/j3/zmFqSmx2Hm6lPN5dF9hGfSNzVi207NKx/d+MxhRoQpGxoe7TYw/CFfJvG485yw8JmQ6hfEhlRCYMkgXUNK8gYyvFz8KwJoDjr0bt2vc7zwHd9fgWkUjKhvaPBB8ChBZN+PKK650+3o21BlasCAzEUYTJViPj79wlEzOtffGegfqDTlFFYLaLJXVGHibP7fk3cCnXnhyokIVGB4fySp/w5eoFVLUG5wbPY0cGkSBlGLQKYyPrIJSrBeA4cGXqp9QHzohwMSAOHlD79H7PMG+3TXXnVsJAFuP38DB3z+ID3+4zDoBUsQ76B3op0eu4JMjV708kmdIQLqVTucbAkBMmAKGFtJt6KXR6N3im335Lu7UNqGfNgw5xf5vSzGkhwbpCdGgk4PTekXhd5t+xr7COw5fz2WIMZBSDDq88cFV7xUuWDK5Pyrr2CeiueO5jDis2s9dC3d77MtKVXIpGo0mmyZbXHX7FBqeVDC4wr7aQiohsPThJMzZlM+Jt4Ve/N7eU+h3w0Mll3rs5qbvOaG3EHDGW3suePxebZgC6b2isfZQEev3jpfkmZvGEW0etVtUJJYbn7Y0jeMb2sBeNqU/jhdX8p6f4sl14pMT1/V4YWQCJg40P+PNLSQOnHdseHCFtTc1UOjwxoeQymy1YUpwmQskIYC10wdjfLIOW4/f4Gz3bM+HM1Ihl0lbu1s2YPX+i+3O0xEND6CtgiFSLceK3edZJ4dGqILw7Ih4xEWrHTbjAmARW7LPwPfG8Pky1/8KuLPv74UwZZBHxvZDyTpogoPwzckStzvnjkadwYT9F26z9pSOl+RhnWx1u99rUYl1stWYY3zVJwaItYGtCZb7PDlWCCzZUYDMpBicuFaFfx+/7pONQKD1dunwxoevYmDuForwYBlIimJcosaEtdNTLdY1vXvmgz9uO4PlU/tj8sBYDHlzX0DuRL0hOlSBh5J12H6yxEaXwR1RajlyFo9t1yLeEdZiS3QlRlW9AXNbRbwC8Zq/d+ASnsuIY/0+goDXCXiBTJ2hhVHypDUSkFgq+8L833brDz03LZV9iX2Goe1CMN6UB1szb3QCMnq3lTObSAokyU5Lo6Ng7tDsmxJ6tVyKF0f18jrPyNd0eOOD7xjY3NEJGNm7i9uForrRiJn/OgZNsPeXXGfnugfMi9eLo+Kx4adizq3s27UG/G5TPh4eqPVK+EobpkBDswk1TYFVjbGv8DYW/PsUq+tKAHjr0WRGhgeNo66h6ySEIDQJPMkjoOBZT5mO6kXjk+GSCzahFnskBBCLCgyXXLB0s6Vd9c9mxFuMBUflwe6+e/o4C8bda9l5O9In6Wz4qoS+vtmEVfsvYevxG+3WBSHT4Y0PPstsI1Qy/K/VA8dkoWCr2kjHvBdkJrp03dNJtXzO29+d8a4z6vThPZFbVI4cjis8+EQmJbDrjPPmWo5QK6R48f4EjEvSWnQRaG+Go+8OgNPXWXtEsi+X+6WCxZs8gor6ZkSqZaiqd1/qzHV+TWeiK6pZv44CMG1Yd+w6c8tyz9nnH7n77u0TqIG2PlriV+lbAk1sjKAoYe0zbt68ie7du0Ov1yMsLIyTY2YVlHokW+uOjxx8ySaSQm5RBeZuzufE1ejIy2GPiaTa6VsIkRcy4jpV/Dc8OAggCBtvkaPv09Eu0dHr6O+Zr9weR1jnEVjbTLSRwCSP4MF7o/HjL+VOx/xCRhxiw4M5S8R+dWwihsVFYv/5Mmw8eo2TYwqdNEkhtsrfdPu6ac1vWDwf9gnB9D0HAIu+OYv7mrLdfvdnQkfZ3KdczkUdNYmdT2gv1JGFY/yS/1FTUwONRoMbN27gnnvucflaXuqvSkpKMGvWLERFRSE4OBgDBgzAzz//zMepGDEuSctpR89Itdyh4QGYXecSCcGJ4RGplmPJJPduNCEl1bpi+ynv+2e4QqdR4sMZg7Fldhpe8CDXgGuqG1vahansO4U6a0te6qCjKF0Z4yvc5REA5jwCCVyXCv7wSzkyk7o6PMZLo+Lx+qQkzpJKw1Uy9NWFIiMxGr/qL/zdH1fkkX1xi4p06jkiKeAWFYU8sq/ld/aVSPS9efJ6FWoaDC6/e4Ig8F74v3HktQds5icu56Jn03sKomuy/efXaZQYl9SV9/N6EqK3FhsTOpwbH1VVVcjIyIBMJsP333+PwsJC/OMf/0BERATXp2JMXnElpx09//Jwf6cGgYmkkM1Rr4iq+mbM3ey4pbU1gSAsEyyT8N406rHB3RChNgsOLXm4Pz6alQqdF+26pTzMfNY9Ldy1OqcALP7mLExWKwpdGcNGmZPw8HPQeQTONlASAoglzHkE7thXeKfdwkhRwMeHizHkzX2chZP0DUaL0SaU50ICEmmSQkyRHEWapNCtseYJJCRYbnza/N9215n+ebnxKZd5OvTbNvxU7Pa7J0BB0VCKC8f2YsepEuQUVcBEUpxe83CVHJk+WOTdsfrXKdgyOw1rppn//8jCMdjw9DB8OCMVESpuFXKteT6jl8fvFcq97wrOcz7effdddO/eHRs3brT8Lj4+nuvTsILrL+Kt78/joYE6h3kXXCZZOdLsp8M61h0uPe2J4EsaOW4U5ogPfijCBz8UtWvXvfbgZazaf5H18VJ7RuD41SrOx0nvTr7Muer2XqlqMGL1/l8wIqGLJR9kXJIWoUoZZv7rGKPzPcuw74U9nuQRsIFe7Ljs3mv9zPz9iUGcHdcZBIBXxiaiT0woVuxu/+z7UndjLzkcc4yvms+HtvOVIQrLjU8xOh8Fs1HYlahmdM6Pdx/FztZHW6dRYtqwHozeF6KQos6F6icAfJpdjBoeOhuzZdnuc3j70QGYmtLN5vcTB+qgUTF/DpkSrpLhnccGgCQpj/OgAkFsjHPjY+fOnRg/fjx+/etf48cff0S3bt3w8ssvY/bs2Q5fbzAYYDC0NdSpra3lekicfxGONPT5SrKydqPpG5ux6JuzNpP12kOXEa6SIVwl470Fu0outUgBCzkUa514NS5Ji63HPdO8mPdgbzz7+XHe4s7XKhsYve79g0V4/2CbkJJOo8SSSf0YKaPOvj8OY/pqPTI+7iCc09f5CvqZAWEOXfJZdUDB3DeG/k4i1ArsKyzDt6duYVjjEZ/rbuwlh2OfYajXCqeefPdl+ias3n/R7VwkIeDW8ACYJ+c/ldYDQ+Mi8dPFu/g6n/vQbmW9Eb/blI8PZwzGxIGxNn8rr/O+KeHcBxNAEOYU3/Re0UhLiMK+wjLM3XzSo3k2Si0PCLExzsMuV65cwbp165CYmIi9e/dizpw5mD9/Pj7//HOHr1+5ciU0Go3lX1IS9zFtuuKFSy867U2hwyyLtp3ldUHeX1iG323Kd/hQVzcYeTc8AOCfTw7Culmp0NqFMnQaJV4aFY9wHl2QbLAObeRe8bxHxsJvzmBs3y7cDcyOnpEqj95Xpm/C3M0nMWWQ+5yGf/10FQcvlHkkbudJHoGQKK8z4NHBse5fyAH0d6JvbMafH+6P44tH473wrSAIZvkyIQopJvSPQYjC+9w0EhLkkknYJx2JXDLJI2l1T757+qVGN3LhXFc0UTBvMDN6RyNSJef24FbM23ISe+wq37jY2I5M7II/jL8XfxjfFxmJ0QDglSr31JTYgBAb47zaRS6XY+jQoTh69Kjld/Pnz8fx48eRk5PT7vX2no+SkhIkJSVxWu0CtHkmAG527Vtmp0Hf2OyzWnaVXIKGZv5DF46ICZVjxn09LaW+Q3pG4MS1KpuyUADIeOcAp+3JueDRlFhs90BrgoYAMOCeMJy5WcPdoGA22A7+/kEMXfFf1HsQkqKz2l+f2A+vbD3JW4nqY8H5+Af1j9Zztp2ETbWLv5g/pjfeP3TZZxUTNpUG144An092+x7r6hMAiAlVoL7ZxElzxucz2sJtbC6BdZmtt5VOvkApk6DJB2FdGutiA28r0HQOKlNyiiowfUOux+PbMjvNb51t/VrtotPp2nkv+vXrh+vXHbu+FQoFwsLCLP9CQ0O5HhKAtkQ9+127J4QHy3DsSjl+56BKwRWKIInHImP+MDyez4jDgsw+IAgJVu2/hFe2nsL0Dbl44G+HUFXfbGkJn1dcidyiCsEZHgC8MjwA8yR8o7IRa36TglCl7XfnTTv2qBA5hr213yPDgx5Xqb4Jd2qaeNXGeHT673DxgQ/QpLRN/CtDlGAWH3sImFUf3zvoO8MDaPtOjl4qx8UiZgm09vkyd2oNFsPD273ruCStR3OeqtX78t/WHJIy2Lrwhfbd+9LwAMxeCToJ3LoCje33RaBNH8VEUsgpqsCOUyX46ZLnfWBCFEEY0tN/xR1s4DznIyMjA7/88ovN7y5evIiePXtyfSrWWAs2/bewzKM4OGBWK13tpIW7KwwtJKcdDPkkRBEEmZRw2MelVN+Elzfb6qZ4sxALnaoGI66U1+HD6anYdvImGppNGBYXib7aUDz1aZ5Hxywo4caTwjRvhC0EzIlvr319GmU14ZDgH37vlOpsnJSDn7msbmPL0xvzcJ+kHFsZRADscyvohFlNawjTm3BqRW0THhoYC7U8CL/94mfGc0+9wYQFmYnYevwG9uq5ySHxhHCVDEN7RmA/j03ZPMkJss/5oze29vl4gPkzPJ7aDf/5+QZqm2w1VZZM6gdNsBx/+e4cvj11i5PcpDpDC4a/vR/vPDZA8EJjnBsfCxYswIgRI/D222/jySefRF5eHtavX4/169dzfSqPkEoI6Bub8VknErvyhDpDCz4+XMz49R29d8MaO2Pzv4W3oZZL23X89TV8aATSC7i19gadRyAkXhmbiP/8fMPG+6jVKFHTZEQ9g4RGvqDQljOhheNyVZIyexAc5ctQ4KYC6H+2noJm5zmPjhUXrcaPr4229Cfxx3f/m6H3YD2LOYgtSyb1w1PpcXjgb4dYh84dVVDqHVxnfYMRnx65ig9mpCJCLbfq29TssDqKC6obzAmyzrSohALn5uuwYcOwfft2bNmyBcnJyVixYgVWr16NmTNncn0qjzCRlFfJPCIiNPXNJr8aHoC5ey3XuWVajdJt8rD9OSPVcswf05vbgTg7d6tuRln2Jvw+8Q6+en6oRYPh708M8qvhQcOF7oa3eGPElNca8N6BS17txr3RN5l9fxx2ni7ldZ6ODlVAHiRhLNxn/XmCrmfD1GIOj7laU6jWf2/sOIshPSMwNaUb9I1m/Sa+cwWtw0NChJfeLpMnT8bkye6TrfxBoKiBiogwxZv5JVwlw/u/GQyJlEB5nQFdQ5UgSQozP3GtXWB/zsr6ZnyecxXhKhn0DbZ9XJg2pWPyuna6GQVAWUEUVOlLMTjlGezgWUWXDWx0Nzxp3McXEgJeS917q2+yLb/EpSghF9eLaaWKBCTmSrfj+aAsRBD15l/mA7fzF+FW+lI09Z7sdk2prDcibeUBvDk1GSt2txkqfH7vjiQhhESHbyxnD1vBsZTuGpy6oedpNCIi/uXtRwbg/ntty4k9XcCtdRncNSZ7h3oWO5uHWibfTOIEHg06giiiTeenilLj05YJ+MD0KABgrnQ7FgRta3ferlQFuh6dj5MAuva23fT4e1FnorvhbqH29WfwdrNsXSVjDRt9E3vDw/oa9CRKMT3oIGKJNgHAW1QkVhifQhVCGV0nncZcpUd7LVx9lpWyfyGSqGv3ty5UBbocnY9tt+sAxLn8PObP1GyTK+cLATohK512OuODbV32yN7RovEh0mGJULfPivRWuyBcJYMySIpBdYedLkKriX9iYsyTGFi932kr+AiiHr+XbcPLQd/BBAIhhONqKklrA7K4nD9BPXqGpYu1q8l94pMvorzW4PEO394g+Jnsg6GSizYLH2CWqI9BJaKIGlRQ7aUD3C3U61smY0rQUZvPYG2UeWuEuDNsrP9+F+bxd0GN08Xd3A/IrOnkSN+EpMz6JvsMQxmP3dH3aJ/qpEMlPpStsWkloKeCccLUB0eoAfjC9Cu0WC13dJVJTpFzHSD6u3EW1aQ/z/1X/gEJ1jj9PI6u8TjJzx4ZaBplEPRNzMuwhax02umMD1pwjGnoJb1XNLbll/i0k6iIiK9wtDMaHh/plTJodYMRXz4zEIn/ng1Qzheh8dX/YfRMBRPux0EQQARqkfPl61j68P9i++aPXE7up2/3AmIyHR7L3YLsaDE0UQSkRNunqaTUkIFEKNHY7vj0Lr0aarwj22A+p4NrRFHAS0G72l0j2ih7LmgvFht/67F3xN3O29Hf7T+H/S59rvRbG4+EPRICiEUFnpVm4TPTBJdjjAqWYEbz1/jfoK/b/c2+X5GjDrgaohFjgk5jDE7jT0FfYUPLJLxjmoEFmYmWRExnngG6qSLh4Fz2n0eLCoxVXcb+hj7tvivH1zgCShgt77c/HkUBb8k+xQFDKqLD1PjHkymWkGhBiR5v7WFmMNPeHaHS6YwPui7bnRQ6LRiUlhBleb19WZ+ICFNSu2uQL0APmqOdkVRC4JGUWHzqRUXYrbMHcD8qnIof0JMu1zqMfW9shnrmCqTIv3Rp+OhylqNh+oR272eyIDsyaiR2M0ME6p0uWo526Y4g3FyjCNQ59Y64C9u423lvaJmE2UG7XY7Pfpc+XpLn0FBwxJ9lm/DboD0OQwwEgEeD8/FH6lNoZcy7s7o0EkDhpaBdkEkJPDvmS8vvnXkG6MZ6THlSlY/aphYbo8+5V6vKZZI4QQDRqEGuYi5+7PE60uIexIVje9F4swShdcFIl9yy8T7R47U3PGnvjlDhXOHUW27evInu3btzrnBqT1ZBqcO6bBoCwDqrUiWum8aJdD5eGhWPfx+/KZiyZEfqijTeqiyuHVCEyZeWeDM8j8lJ/APSL/3d7evOZn6FF38Ktng13Sl6rm55DM8F7YUG9V5XGFGU592G7Y8DmDdFjsa8vmUypgYdhc5qIS2lIqFEMzSoc/o5KKr9MR1BUoAeIZhrnId/yNZDi0rGn8uRUioB4FdOvgdvoSjABAnIxaWQK81GhzOF0imSo3hPvpb1OWijbx85FEcU852WWjOBpMzXo5oIQQTa55wAQCUVAgA2OSm3qEjsiZ2P3760wLMTe4FfFU4DhQnJOrz9yAAoZe0vAQHgxVHxNjXSE5J1OLJwDL564T5O+i/Yn0+kY0MA2Hm6FE+n8SO2R8Bs3OhYqFkumdTP6c6oqr7Z40lTJZciqOqKZ2/mAKKKmTaEofqWpcySdrMDjr0lBID/lX2DCMJ7wwPgxvCgj+OsfwwBc9hGC9sdvBaViCCcGx70cZl8TgkBRBB12Cx/BzqCueFBvxew7XGjCw3CcvmXNn/nCoIAgggSJ7b9zfI7qYTA0sn34j67kmCmjfXst+5aVOIj2Wq8L3sPsYTnhgfQ9vnDKceGB2D2ftkbJlpU4vlby3Byr+N+akKh0xofWQWleHlzvkNpXgrAx4eLkVVQaiN7m1dcCTDsyMiWdA5jcxE8N3ibfX88r8fnA5Xcv7c6Lb393iH2yrju0GmUWDcrFYsnJuHIwjFYMqkfo/dFqBUOf59VUIq5m/M9rnq4vyUHv7q70afS5tZIjPWMXhcc0c2iTjlW+YvLxYIrY8GX0IaJo/wIoSAhgFiiAitTa7Fldho+fsAILSo4NzysUd34AQBgamnB9e1LMWZnOrbK38R78rXYKn8TRxTzEY4alLporAc49l5JWq/3JKlnysf2OPr+3P2dvna6nOUWLRIh0ulyPgCzq23ZTuflVTSLtp3Bsp3nbHqWeCojTseanXHsKvP4ojv40pUhAMwf2xvzx/bBkJ4RLsNWQsNfTfl8wfMZcRiXpAVg3slFhzo2KuzZX1hm0QAwkRTyiitxq7oRy74r8Di3ydqD4I9FjqKAnvrjuI1IdKGcq4veIaLQ977xAICYkv/ib1jl45G6hquwTKCQoKrD0IQoHC+4wfu5EpvO4sKWxbjnl43ogfYJwTqiCuvk7+NK4vPAxU9AOsgdAoT9/dCJsOeO7UX/jEn+Ho5DOqXnI6+4EmU17nM3qhtb2jVL8yReH6qUuragwa3BoPdgjEyeIwpmmfGR7x7EyetVPjc8CJi9OtowZosrlzgKzwmFt/ZcwMh3DyKrwNzum2l53fZTJTCRFLIKSjHy3YOYviEXv/+/0zY9KNhCJ+r5K8+NIIAYVKCo++MAnKuLlqYvhTQoCCf3fo5BR+dD48K17Q+EvLDxgVF/G6aWFrTUlfN+LhUM6PvLhwhxYHgA5u7NBICE21k4nb4adwlbr3Q95fv5x1Maq4QjumePcGdUHvG18Io3k7mv0GqU+GhWKj6ckYpIB9oP1pTpm1j1feGSlY8NQPaisdgyOw1rpqXgidRuPjmvrztnsqVM34Q5m/KRVVBqKZV1R2W9EWsPXsYclt2ZXWHfpdVfDLz+BY7pZuIuYavueIeIwukR72Hw+GdgamlBbM5yAJ1vsRca6Zf+jvI3+wDVjruf+x4KqClBSE0R7LdmBiJwjI/gCN/Mj57QKcMu0SGBc/PwhTZMgenDeyAuWo2uoeZ6cDr5cHyyFrlXKjD3q3yHnh5/lUdZJwHT4YLJA2Ox/8KdgAn/8AXdDXX5d4UYl6RlXCq7MbuY0++TaaIe36hhQFrpV8jtMRtXSBMACiF9xyApfSK0QeZp78KxvejvohxYxLd0pSrQ9c5/BPV99D73vvn5sBpTOFVj6dki1EpWigJuE5GW0KIQ6ZTGB1ezbXiwjJOySVrIhknoJVwlg7GF9Lih2dPpPfFQss7G2LBHKiEgIQjBlIQCbdUif5xgW6EhlRB457EBbnVbXKEMkqCJYbtxIUMntX6WXYyxfWMYGR9cf8fuurn6Clp0Kv3GBsvvbpfsxJmGKgwe/wwAYbukhQJJAfUIRggaeV9sHQmFCQFnQmAE4DQfxN8QBFCiHmgxtIVIpwy7lNc7lmlmy/vTBmPL7DS8kBEHtdzz8lsK5goSAu2Nfvrn5zPisGV2Gk68MQ4n//wrjytaHkrWIT0hyq34jNB6AtALa15x+8RcumKBTZkpTZgyyOeGB99z1Yrd5/H7/zvtcXK0N7jq5upr7EMpXagKDDo631KCyMYlLcRFkW/o7+/yiHdxasR77UJYfFwToYS/6I/mbDyWKhMG4yUpoBKhXA2NMb3qfxZ0tUunND640rt/bdsZ/PDLbXyafdXr1uqDe0Rg3axUaO0WUDoX40+TzHoEu87cwpc5V1HlQZhBG6ZgLLcr1J4AzowiWodlQWYiq+PVsOiTwAULMvu0+475oKymyW+eK7qbaxki/HJ+Z9iXIDbpb8NEMVvthLIo+hI9EWrOjxk3C4P79EKXR1eieMgbOJ66EnlhD6EOHTN8zcamcndb0AbcL10f8nQ4HhOBOlw4ttfn52WKcH0yPMK2v4szbtdwl3i5aNtZfDAzFQd//yA2H7uGa5UN6BmpwlPpcTh44TZGvnvQ6/FOH97DrceDLrn877lSQbpBXRlFJpLC5znXfDgadkSp5Zg3pjfmjemNvOJKfJFzFd8XlPl7WLxAd3NdK3sPEznSPOACugQx771ZGKr/3t/DESwkBWhQC03FaWDVSqC2FBIA8QB6onXX2jqV+LMsmI9zNxPBkJGNjI7r7iXVRCgudp0IhHYD7nAyPOSFjsfQmv9CQrifnIUcWuyUxgfd3+V3m/LdvtaVPgeX63J1oxEz/3Ws3fneP3SZs2TKivpm5BRVOM334EJCPlwlQ3WDsV0fHPpsr2b2QY/IYGRfLse+87dt2rAzoaWFxI5TJe2SZLMKSvH69rPtWnELiRVTky3jTU+IQllNU4c1PgBzCOZLU6agjA+a4TXfm5N0PVi4uF7whKjpQc9D8Rc/sSQzW/5m91p/jp3eIFmPgWr9H7bjoiigMGIsoqtPoouTMly2RKIWaXf+zcrwcHY/0Po0KXO/wM8fPo3hNe69GmK1iwCZkKzDR7NSnQpl0d/9/DGJWH3gklfnkkslaDYxyyuwN3S4rOL4Iucavsi5hki1HI+kxGJcktaygGcVlHqVtEnzwfRU1BqM7YwYrUaJpQ8nYUKyDlkFpdiWX+LRuZ7a2LaQ6VqPCYCTsfPJS6PiMXGgzuZ3lXXc5B4JlfGSPCxrFRwTIp4umlwvtkIzPGiEmEjpCPvr1ywPh9xQzcqo00OFc10mgoiIQ//qA36vuLFPZKXXhWLtBBDvJGE4Kly+n6LMXhex2kWgTEjWgSSBP247gzqD7Q5co5LhnccGwMBBMqJCxtz48AWV9c34NPsqPs2+Cp1GiSWTkrBidyEni3d5vQFTU7phXJIWecWVuFPbZOOlMJEUln/HzblobQuNSiZowyMtPgKLJya1+z0TLY5AxVlHT1cwXSyE6CkQ8T9l0RnQTloMRc8RyNm40KbSyRnnZMnQGa8hErUYUf41wL/GmVty7nkBCSXfIsbKwKgjVLii6I+00q/Mv2B0/wt5Vuzkxgfdw8LRV6Rv9Th4k3hJAIhQywQdCijVN+Hlze7DT0wprzXYhEUAs6LsrjO30DVUCZKkOBO0or83oWt8nCmpgYmk2oW6tJpgP42IW4IkBFqsXHaumrS5ggQBKYMJUzQ8RBwRUXUa6DkCJ/dtQr8bWxi9p7+xwJzXJqB7qm/Jf3Ch6yTI7uxBJGoAAGFoQIrhOONxEoQ54VTI8uqd1vhwtwOnYBZs+vG10dBplO1aLjNl6qBYbDwq3CRILiFgLvOkCW8tB7Y2DrjuCBwINDSbkFdcaRFGoxkeHwltmKKdhH+g0WIXK6Ql1plCUUAVEYbrD6xCyuEXuB6eR7TLIxC9LYJHYarDpa+XYtC5982/YLFQC4lwqhbpt7eaf/BybEJOOO2UpbaAeTfubgdeqm/Cuh8uW/IK2NwHkWoZXhwVjx2nb3kxysDC3jirbjC280rw0RE4EKBLhO27JP95cn8/j4w9EpBIs2tBbg1biXWCACJRA1nh/3E4Su8QcidYXyC0KjemdC3cCCBw8lUcQY+di3tOTDgVIExFtFbtv4SPZqVi3axUxpUgUWo5lk3pj/lbTgo86ibiK7qGKh1WE+k0SkweqMOuM6V+HB1zxkvysFT2hY1n4xYVieXGp7GXHA7Ac4n1/uVZXAyRUzq6x8PZ53NURRIIaFAniBCKt9fO2+tOUYAJEiQOGevdgXik03o+2ORy0P0yjiwcY1E0dcWwuHAs/uaMaHiIADB7wf57rhS/c9DArUzfhN1nSqEJFv4+gE4i1cI2pKJFJdbJVmO8xFyJREuse6JwKrRnJtAWX7a4+nyB9NkpAE0Slb+HYcHfGkkEAQQRJC6dOOC/Qbih0xoftNAYE2hZb6mEQHpCFJY83B8fuZDzzjp3p9OGF0TaU1lvdJr3QzeoMvlbi9wNrpJI6Z+Xyr6EBKRXEusEzJO2q4k7UEMCvqAzXhr6flCSDbydQ+CPp1PEnA8BQguNMSX78l3sOFWCnKIKmEjKYzlvERFH1BlMCFEEQS4V5naTTiJ1FkuXEEAsUYHhkgsAzAqnc42voB7sq8UsfTNc/F3EMZ320vBsHNBN5Fi9p9X7QTKU8OcDMedDoExI1mFBZh+s2n/R7WvXHiqy/DctbjUuSYsteTf4HKJIJ8Jea4YpkT4o52aaREq/brwkD2/JPkEoIawGhUwIxFyHzoyvvivKTreZyX1CEAABCjkJCxB2fR/6Gwv4HaTV2KqJEEGLjHVazwfNvDG9oQ1j1yCJFrdae/AyymoCb3IV8QxFkG8eF7aZ+oO7h/MyDmuYJpHeQbglNyQCdfwOigdIyry81EMRUOEdU9Ij/h4CZwjxuhMEICUo5CT+AT8P/RuOdn+JlbOlX9EGnxkegHm84VQdzhz4ymfnZEunMj6syxzp8IlUQmDZlP4O29k7g77pNh7lpqmciBmhbzYNLSQWZPbBmmkpmDe6N2/nYevePXldz89ArHCXREpSwC0qCj+TfSy5IYHoPWggFLjUdw6CJNKAGT8FAIXf+nkU3CHk607ob2Do5Bcx4oW/Ii/mN4zfF0b5xxCnOzgLkU5jfGQVlGLkuwcxfUMuXtl6CtM35GLkuweRVVCKCck6h+3sXUFB+MqagUR4cBA0raJkNBEqGVRy4YiSEQC2Hr+OyQNjkdE7mtdzje3bhfFrKxuaeRyJGVdJpPTPy41PYajkosvcEKETAgPu/WUdFDwmL3INAUA4T0nHps+d7y2LeeigqYzeQ1HMvJnuEq3ZQrR2cL5wzH0DOn/Au/HxzjvvgCAIvPrqq3yfyil00zRHZY5zNuVbDBC6lJbNzlYTLHP/Ip5Ij49y/6IAQd/YguoGIxZkJmLNtBRsmZ2GY69nIkzp/fVVy6XQhtkaljqNEi+Nimdc8QSYDc5SfRNyr1SAJCle1Vp/e38CnhvRk7fje8JecjjmGF9FGSJtfl+GKMwxvoq95HDWAmMiIkKA6aIfiRrLYt73vvG4jSi372XiyeGzmkaoFS+8JpweP34cH3/8MQYOHMjnaVziSkadlvSndTzoUloAyCmqwNpDl90ef1y/rvg63z9f7plb1X45ryO0YQo0tZAee4Po7+eLnGvIWTwW8iAJcooqOMmpqW824bf3x2N4fBTK6ww2fWdGJXZFzpVyAAQu36lD1jn3Le7nfpWP6kZ+vF4EzB2A6fEJTZp/Lzkc+wxDMVxyAV1RjTsIRx7ZF2TrPsZTgTER/nCWGEkvmkIOc/ANSQF6IgQa1INgmMXRWFUCU0sLLhzbiyZVf8Q0HPZ6HNVEmKWPC9cIteKFN+Ojrq4OM2fOxIYNG/Dmm2/ydRq3uJNRp3ez9r03aB0Qdz1dvs4vQbhKhuYWEg3NvtX2qBeQlshjqd2gCApiVDnkior6ZqSt3I+3H+WmozDNmgOXoQ27iWVTkpCeEOVQbTRSzczLwqfhAQBLH06CVEJgeHykTypZ2EJCglzScZl6OGoEvah1xkoWZ4ZHZ7sOjpAQgJIyQEIwdz0EX/sRdW8uR3+OEqpzEv+AoJBoRJ5cxMnxaEgKuENECbbihbewy9y5czFp0iRkZma6fJ3BYEBNTY3lX21tLafjYCqjfqe2qV3fjSWT+gFwnwhZ3WBEQ7MJj6d2w9Ce4d4NOED58IcrWLX/IiehiMp6I+ZsyseVu9wmaZXVmMNsK/cUOgzD+XuR12qUWDcrFROSdQDMWjSPpghz1+IICUj8WbbJ7FEU6MJGwffVFEKt3uhsOPselGD33Pe/u8ss4+4lJGUOWQ7/zWKoont4fTz7YwNAafpSSIOEqajBy6i2bt2K/Px8HD9+3O1rV65cieXLl/MxDADMZdSvljdg5LsH2/XdeHFUPHacusWo8+g2P4VfhASXyq5f5PATctjwU7FbB6ttRT9/LJnUD9GhCksoSGqXmRYWLPfBKLiBbTdbX0IBaJYEQ25q9Pm5O+NCL1RIB8mf/vp+CAAlw9+ANijInD+yLwpdqApOkrXvEFEoTV+KweOf8f5gPMG55+PGjRt45ZVX8NVXX0GpdL/wL168GHq93vKvsLCQ0/EMj4+0tHZ3hkouxer9Fx0mpH58uBhNRu7c/yLMoABU8VBNRIFZcleE2nbRd3cPsYWA2bh9NiMeU1O6IT0hqp3hYSIpbMm7zul5+USoyaYUZb7eCrLRrXqqSMeFIITV7ZYggHvy3sTJvZ9DGhSEW+lLAXiXfNpIyXF27Jfo8sZFQRseAA+ejxMnTuDOnTtITU21/M5kMuHw4cNYu3YtDAYDpNI217xCoYBC0SbyVVPDfdKN0U3uQKPR5DQhFeAvxi8iXJZM6getJhh3apvQNVQJkqQw85Njbt83MTkGT6XHo6q+Ga9/e9ZhAq59bgdgNjTyiist5xseH4m84sqAErETarKpaGz4FzG/xDldqAp0OTofJwEMHv8MTgKIzVmOGFSwOg4dUjrZ4xnclz5RsKEWazgf4dixY3H27Fmb3z333HPo27cvFi5caGN4+IK1By+h3k0iqBBjst6yILMP7tWG4H//c9rnibAdAa0m2CYB2URS0IYp3RoD+der8f4Mc/hkfLIW7x+4hA1HrtgkB2tb5fnp3A5Hya86jRJJulCOPxW/0EJkWgSuzocI9/jb8BCy8SMhzJ4OXc5ymMbOxODxz8A0dibOHduLmtPf4t47WYyqYOjPN+LGBtx+8xvcEnjIBeAh7BIaGork5GSbf2q1GlFRUUhOTub6dC4xkRQ2Zl/16TmFQly0ChOSddjw1FBOj/urpK54NCWW0WtfGZsouEWIADPXa1V9e+Gukb3d66qU1RiQV2zOe9hXWIZ//3zDxvCIVMuwZFI/G8PDmQbNgQt33Q9UQHjTzVak42Mk5MhXpvn8vP4yPCgKMDFoKiexEwOTBgWhf8YkpL+8AZo3inF27Jc4qnsGdRQzyf8uVAUGHZ2Pk3s/9/Yj8IrwfTNekFdc2WlDJnSibVpCFHQapctyYzb8t/AOgDaL3RG0VsX8sYm4NyYUL2/O5+Tc3kJPAy+MjMeGn1xL46/YXQhNsAzl9QZcLa/HlrzrjJKOAXPlFG1U2F+iqnoj5m4+iXUSAuOStC41aISMBKRDrQ9aiGyp7AvEQpjJpyL+QUY1I7Up19/D8Am0kXBMOx1EkBLpJZ+6fY8jMbAzB75CbM5yDEAF4/4T9t4UoYZgfDKqH374wRenaQfTMlu+IAAEyyVoaHadc6KSSzkNjYQHyywiVVIJgSmDdPj4MLd9aFztbCm05TNMHKjDR5LUdmEFf0CHOzTBcrfGR6m+iVGOhyMiVXL8cdsZt8J2oUqZ36+JJ4yX5JmNC6vKlltUJJYbn8ZecriNENlr0q0YInUv1ici0pEgWg2AXrf34u7Y1QAD48NeDOzk3s8x6Oj81gOyOz/tTTl3bC/6Z0xi92Yf0aF7uzAts+UC+3uD/lke5DrHJUQhxV8fG8iqsZ07MvvFYNeZW8gpqkBzC4mdp0s5OrJnWEvXP53uH8nwMGUQfnxtNCYk63g3Sr89eZORsF1OEbukMiFAd6zV2nk1tKjEOtlqjJfkATCHYDSow2DJ5Q6ZUyUSGHDdL4UNtAGgriyEHiEumzJWIgwN5ddxLns3TC0tMLW0IDZnueU4niJUaXWgg4ddmKqUegMBQKOSQRkktUlG1GqUmDash1vFzzqDCX/+7hwrPRF3fJ1/E1/n3wQAvyhk0jv7MX1jcOJalU0FB8CffocrappacPxqJTJ6R/NulH5fcJvhKwNrVZaAtHSstZ8QaVfvUtmX2Gcw5xmtlP3LbFQ7mTyFnAgo0jEgiDYDxF/3Wq+TK1sH034cdBl4JGosCqe390WhqPsTGMEi1OIMoUqrAx3c+JBKCCx9OAlzNuXzJhpFd7f908QEJMVqbHqH7Dpzi9ExKuubsf5wMf5nTG+8d5BbF7U/VDvpnX3aygOotErc1GmUWDKpH+8GoTNmf/Ez/vnkIIxL0vI6hgYjsxCaVEK4zJ3hC2cGszvciYhJCCAWFRguuYBhxHlEEq5VIEXDQ8QXsL3PeG0P4OCY9ufpQlWg6/WPvTI8hC6tDnTwsAtgdvmvm5UKLYvupZ7w1p4L+MP/nYYiSGIRjGKzw6YAzg0Pf1NpVzFSpm/C3M0nMWWQudLD12tPQ7MJv9uUj32FZVj6sOPeJFyhljsPtxEAIlQyrDlw2a3hoZZLORc4owD8Zug9yF40xtJCgAlMRcRiUInng7I8G5yIiBN8FT6hhei8PZ+j99NzHukiHCQhvNsoB4K0OtAJjA/AbID8+NpoRHA8idtTpjf3DskqMOdY0GEfcYNnhmr9938nbuK96YN5NwidsWznOZCkeffPF660ZejrwISGZhP0DUYsyOyDVU8OwlNp3PSA+PhwMfYWlOLZjHjG9yhTEbEoogYRRL1X4xMRsSfQPGWuxish4FJtlw5reuIVJSHByftWdT6dD6Fy4loVL3Ld1tD3yfLvCmEiKUvYR8SWynojXt9+Fn96qB8mDdD6/PxlNQa8vDnfofqor2B6btpQ2fBTEZZ9dw5f5nIntz5vy0nsLWDuBaJFxFwlzt2iolBJhXA2RhERfyEUY4etARJEkFBqYvgZDId0GuPD0woHwsl/O4POd/gsuxim1ruGzx12oFLb1IJ5W09i99kyfw8lIKgzmKBvbOH0mCQFiwbLulmp0LV6oiQgkSYpxBTJUaRJCiGBuVTclYgY/fPOlnS8IfuK03GKiHRW6qFEDcHemBdylQuNcANCHONphQOtDQEAi75x3KvDESt2n8f7hy7ztrtWy6VuZeNFRJiw/LtCHFk4BuOStDi0/V9IOvO2Sw0PRyJiZYjCzpZ0vBi0yx8fgRVUq9iKQDa2IiJOCUFTmzgQC4Rc5ULTaYyPqvpm1pUFCzITMW9MIqQSAlkFpawNCT7d+qLhIWKNTErAaPIsTa1U34S84kqkG7Ix9uxroAjb49AaHnOMr7YTEaMVTn8m++Cw4lUAwuoc6giCaAtnCXyoIp0cR4mvrqpxAqHKhaZThF2yCkoxd3M+K8ODALD1+A0A5h4xi7456/oNAkEocUoR3+Kp4UFzp6YeyFoIAlS7SYE2JpbKvrQJweSSSdhJjkAumYShkouIJQKnoVyADFOEZwJBacd+TneWqEqvb8XdH+d/UBzQ4Y0PE0k57Z/hCjp3Y9W+X/DpkWK/JicCzHeTFAVEquVQuSj17KyEq2SYfX+cIBZIIYzBmt4NZ4Ea57o0EgKIJcwaHo5gWoYrJAT2FYj4A5k6IAwQJkgI87/0G+tR/mYfsbGcv8krrvSqf8baQ0UcjoY9KrkUCzL74K095xm/x15fozMTogjC0+k9kB4fjRPXK7H6gDC0VITU9TVcJUO/UD2j13ZFtcOmckzLcEVEfIlzZVMCUEWBaCj39ZB8QheqAl2OzsdJQLAltx3e+PB3czlv+eeTg2Bocd2YTsQ5dYYWKIKkeG3bGVZqnnzhD0VTm/M7MByeGxEPKoRZ/5+eRCmOKOa3S0hdYXwKt6hIaBE4oReRjg2dG9E+t6f1pwG/Bo6t4/ycQgh9B0JnW+GNiGN82VyOS3StVTYTknUB2YBMSKzaf8mv549Uy7Bkcn9U1hmwYjdzDxbXOOpGW4YodNGtwgdF9+IJF8YDSQHVCMGCoG3t/qZFJT6QrcH6lsl4MWgXSEp4YSUR4cN1AjBtBBikIVCYrKT+w2KBCe8A+hscns32nFzhjTEj9M62HT7nI5BURkMUQVj15CBsmZ2GIwvHYEKyWYY8kD6DSBt0p+K3Hx2ARwd3Q3Sowm9jcdaNNgYVkPzf0zh3aItbDQ8aR03lAGBKUA5eNs5HGSK5HLpIJ4Gv+c3a8KhEGE4mvQYkTQHUXXg6I7d46ykVquYH78bHO++8A4Ig8Oqrr/J9KocEksponaEFXcOUlt4wNNafoTMbIHzL43uL/Xej1SixblaqxYj0lxfOVTdaAubd1VLZl9hHDsUc46vtjIcyRGFVyxOIJOqcejTohNRqhOF+w3t4UbIMOV2eRCXCuP9AIiIeEk7VYFDOq+ZkzFCdv4fTDgpAsyIKPw95F+fGbUb+fatwl4jy6phC1fzgNexy/PhxfPzxxxg4cCCfp3HLhGQdXhwVj48PF/N6Hm2YAr8Z1h2fH72G6kbPqmNyiiqQ0Tu63e/pBnlshM46Gs0mYee+0BuUUYlR6BmlRkr3CGiC5RapfdqD5euOvmy60TrS8Mgj+2KyJJfRubqiGiQkeOyxaUhP1sHU0oKzOXvQd/+zkBGiNo2I73AUsrDJhRh9HtKwWJdVXv5APnU1hiZNsfxs+tXTOHdsL2pOfoO0u18DYB6KqUSYYDU/ePN81NXVYebMmdiwYQMiIiL4Og0jTCSFnaeZJdR5wrzRCdgyOw3Zi8YirVe0x4YHABTdrXX6t3FJWiiDOnykzCn1hsBYvA5fqsCXudfx+/87jekbcjHy3YPIKij1mweLaRks/Tp7DQ8SEsbVLN0Vtfh21C1MUF8GSBOkQUEYkDEJpFTYXiuRjoerpm1aVODC8f3AhHcBEIIpt61DMEx9Jtr8ThoUhP4Zk5A+7xPk6mYyOg7V2jW3ePhyQSabAjwaH3PnzsWkSZOQmZnp8nUGgwE1NTWWf7W1zhdfT/G23NYVOo0SC8bdawmVeFtd833BbUtXXBNJIaeoAjtOlSCnqAK5VypQVmPgYthesSAzkfM27x2ZUqtux7QHy5cdfZkaDj0J5wb6z2QfmCjCaRtwerJ7jfocg/L+AHw+GVidDBTuBK4dhYL0f6WRiIg1jZU3gOAIIG0OCJV3oQ2uCEUjLhzb6/Tv6b/7EPn3rUYlQt0eK1c3E0MmPs/l8DiFF5No69atyM/Px/Hjx92+duXKlVi+fDkfw7DAZ7lto9GEfYVlmJCsg4mkUF7rvXGw/LtCkCSFFbvP2xhN4cHCWPB7RKqgDJIC6JzhH09ZtvMcQpUyGFpI/P2JQQAB3Kk1oLLOgJwrFdh//g4v5z1O9kVpayWLs90gRQHTgw7hA9OjIB3sSUarrkDqIvPN4XFrSoH/PA2kzfFw5CIi/JFy5k3ghNVmVxWNO2HJkJXlIwI1fhsXnSBqamnBhWN70VhVguCIbkgcMhaXThyA9OwWaKhap+7TSoSh+L6/IH3icz4cNXsIinK2l/GMGzduYOjQodi3b58l1+PBBx9ESkoKVq9e3e71BoMBBkPbgl1SUoKkpCTo9XqEhXGTrJZTVIHpG5jFrNlCf/8vjorHztOlvHlYhMSSSf38WjLaUaC9R3zm8LzUel8+UbcZv5d97fb105rfQC6ZZKMHkhDfC3OHqhG04yXW56cAGKVqyE31HoxeRIR73DUWND3xGS4f+Qb3lu305bAsnB37JVoaqhCbsxwxaJNZMFEEpITj5ZpexfNDHkTKgm1+C7XU1NRAo9Hgxo0buOeee1y+lvMRnjhxAnfu3EFqaqrldyaTCYcPH8batWthMBgglbZJfysUCigUbSWINTXcW5x8JvrRx+M7mZVLVDIJFozrg7f2OJbKdoVaIcXNqgZGr/Wm2VlngEujw5F4WLBchsHdw/DHvuW4cywY+MX9cbqiur0eSAlQfzvco8mCACyGh1AEmEREXCHdOQ99muvcv5Anai/8gPSbn5h/sHpeJC5WL7oB3aC6H2FqaRFsnoc1nHs+amtrce3aNZvfPffcc+jbty8WLlyI5ORkl++/efMmunfvzqnnAzA3l5uzKR9AYDQT4pvB3TU4eYOZpLY3KGUSNBmFXaXClrT4SOQWO68e8TWOxMNuUZHY2TICU4KOuqx0secfxsctQmLWZbUkbTh4YUCIxodIoMBW8IzLe5ukCEiceDiYkKeZgOEL/s3NYFjCxvPBufHhCFdhF3v4Mj4AswGy/LvCThEaEeGP8GCZVxVNbHDk0bDOyaDFwwBbY8FaWtrm93A8qZIUWvU9KGhR5VTllHBwTBGRzo6QDGuKAvLvW40hfsj58GvYRchMSNZhXJIWuUUVePHLn1HfHBilm3xBu+r8jVDGwRRfGR7OPBrLjU9jLznctXhY6zV1JipG2Hk1AGBLyxiXeSH0sSqpUEShLVGvhSIgASUaJCKdFvp58sYIoedAb40YggBSji3ACVCdr9rFnh9++MEXp2GEVEJAIiFYGx6Rahmq6o0dKmTj7wV/7oMJqGky4svc6/4dCEMIABofeT2sPRrWaFGJdbLVmGN8FWlJCYi97Dyk4mwSs/99GaKw3PgUFGhhNLa/GJ/CbURavDHhqMGHsvfEni4inR5PDQeunx0pQWHwsQU4KZUKtqttp1SsKtM3Mn4tAbOWx1NpcR3K8KB5ISMO2jD/9Bz56tj1gDI8AOC5jDjG71FICfzPmARow9j15XHl0aB/Xir7EvFy73J2Pmv5FaY1v4GRhjXYSw5nrAdyG5E2ImRZZJpDWXYRERFmGMCPjIIuZzlMLcw2Fb6mUxkftGjXD7/cZfweCmYtjzUH/NsZlS8yk7T48+T+CFH4PgLHpwfB202EWiG1+Znu0zLnwd6MdygGE4X/+7kEU1N0rMZEy6G766MSSXgnyJdFDrcomAJAHtkXt6hIp42sSAq4RUUhj+zb7m97yeEYaXgP7xunejUmEZHOBkUBcoZeRzZYlFxdiJb5k06T8+FJsqkySIKmFrLD9lLRaZSoqm/G3M35Ae/VeTy1Gw79cheV9c0AvKtoilDJcOz1TJy4VoU7tU3oGqrE8PhISCUEsi+Xs+oyWVbThPWHi1npwDCVQ1eGx+BWq3iYI0PFWfzZnFza3oiQy4Kw3Pg01slWt3MD09Uuf2me6VCEDDDLsmdTA/A/2MFo/CIiIubnSsrjDNxpu9oKAbrMlm2VCx/xa0UQgXCl/20+ArRYWGHAGx4AsC2/xGJ4eAMBYOVjAyAPkiA9IQqTB8YCAHaduYU1+y/i5U0nPDruztOl+NND7T0GjmAa/khISMR7st8CaN9221Lt4uD3BIDlxqfaGRHThnXHXnK4wxCKpFWUaZn8K4yX5Nn8LVwlw4fTB2L3w8Dv00JRiTC/5xOJiHQUvH2WOmVXWyFgIiks/86zBbaBQ30K2o5ZM20wxvSNwZc5V3GsuAL/LeRHUtuaEEUQ6gxtbj2dRomlDydBEywXy47teDy1GxqbTfjkpyu4WdWIHadLUFnvneeLgrm/y+JvCxi9ng5/OPVogAARFgtpXAYefCQRL29uxp9lXyAWbcmnVQhBOOochnqcPQt088W95HBIjBQ+lK0BYOs96UpVWBJe95LDAQBpTdkYnfU/CG4sszmHmIAqIuI93lS/CLmrbYc3PjxtKqeSS9HAYSmutnXBB4AH/nbIZ4v+S6Pi8ccJ/ZBXXNkuhLDjlDDdcfbQbbB9wdf5Jfg6n5/rUtvELK5LQuI0/GHR6ZjwDiCRYkKyDpjxO/x6Zwa6151GV1TjLsLwD9k6AO0nLroEd6nsS+wzDLV4PyLVMovnSAISS2RfOtTzoL8L+v3jJD/jQ9lqwEEOt2h3iIj4l/KwZEQKVO20w4ddPG0q99KoXpyNYcmkfvjxtdH4pawWv/Mg/OMJkWoZ1k5LwYP3xmDXmVsAgMkDYy3ddwGga6jvOqt6g68MDyHhLPxRJ49B3vDVyFFkwNR6YSYk63B40TiMe+hx7CRHgIIEsYRjoTCgLWF1uKRNXv/RlDbXLNOE1/skhW1VOXavsdY9EBER8Q9xTeeBKz8CZ78Gin8CSOFoWwnTJOIQtgtshCoIKx8biHFJWmw9foOTfjAl1Y0Y9ddDKKthb3SEKKSoM7C/YR5PvQdvfX/BxtChwy0Tks3VF3z2vBHxnr3kcOwzDLVVOG3qC/KwBDica/N9SiUEokPNJdNME1a7ohoSAlg7fTAi1Ap8kn2V1ft/Lf3RpXS7UBQfRUQ6K/LmKuCLKW2/CIsFJrwLJE1x/iYf0eE9H/QCy2QeDFFI8d60VIxL0kIqISxhEm/n0E+zr7I2PIjWf7PvT/DonBt+Km7nYSnTN2HOpnxkFZhj+9afUUSYkJDYaGpYJ4naf5+0oc00YfUuwvDAvV0QoVZgSM8Iy3PC9P2PSbPZfBQREREW8OI1rCkF/vM0UOifjr3WdHjjg80CW2cw4alP8zDy3YPIKijFhGQd1s1KRYwfRLhiwhR4NTMRPaJUiFRzI0BD38vLvyu0cdmvm5UKbZhzD1GESoY101IQLPP97UKLvGnDFGIOgR1U67/Xt5/F9pMlIEkK2jAljrvR66BZK3sfGZf/gTWffIoH392PKYPMHjHz+yPEkImIiJ+wNHLknNaHOmuR30MwPmksxwY+u9ou+uYsI80O+jtfNysVE5J1yL5cjpn/OsbZWFwxb3QCZFIptuRd9yhMw5Qts9OQnhAFwHxtlu08h7Iag+XvKpkEo/p0xZCeESipbsC2/BJGCZMRKrOhVMWBNgr9PXwwIxWX7tRh1f6LXh+zoxOukqG6wYgJkjxzIiiYVZzQPWPiRv4GO0+X4om6r/B72TZ+BysiIuIQnzSqe2YXEH8/p4cUG8s5YFySFst2nmP0WtoaW7jtDEIVMtypM7h8PVeEq2RI0ml8Ivp1p7YJJpLC2oOXsGp/e/XWBiOJrHNlyDpX5uDdzpkyKBZKmQQfHy72eoxajRJTBumwYrdzcTi6y6qIGX2r0ZerzMCcJmCp7HPEosrt++ieMQtyJfjTY7Ox7z8xXo2DqyZZIiIi7nHWrdoldbd5GAlzOo3xkVdcabOzZ4K+sQUzPznGWdjDHc+kx/lM9ItWfOVCmMuazXnXYTQ5/wTjkrpinwttkwWZfRAXrULXUCWq6g2Yu/mk0+uRHh+JnGLnCY+dEXoSUgZJ8PTz/4O9OfF4rugVt++jS2gXEp/hb9spLJF96dU4miCDEkZBtRoXEQkUmHbJJSlAT4QgPDQMqL3F7iQh3m0wvKXD53zQeFpyC8BrkSkmhKtkGB4f6TP9j+8LbnNueABwaXgQAApKavDhjMHtmtlpwxT4cMZgDI83l5aSFIW/7HJtiImGh2MoAGU1BkgIAhk65qasuYS2Ev/AKkTCu74xCpifGdHwEBHxDmeJEXRO19X0t0EsKDCHUR7/BHhqh7mqxakvhADCugE9R/AxXMZ0Gs+H0DUt3nlsAMoZhne4FkDzFbTSZ4RagexFY22Ez6rqm12GV0TYc6e2CQQRwfp9BLw3GkRlUxEh4syTIFQPnasx3SGiUJq+FIPHP2P+hXX+xoR3zVUt7QLTrQdsFSn0J53G88Gm5NbXPJ8RhwnJOsYG0oanh2JBZh+EB/smHMQ1d2qbIJUQSE+IwtSUbtA3mpvbdUTDI0IlQ7hK5tV9p5ZLMWmAltGCLgGJNEkhpkiOolftCWw6WoQqKoSVUJsQJ2ERES5wdm8Hwj1PUuZ/OdFP4Ny4zejyxsU2w8OepCnAk18AYTrb34fFmn8vAJ2PTuP5oEtu52zKF1yS4rgkLQCgqt7gUkqcgDkJM61XFDJ6R2PemN4W78Gl27VYe6jId4P2Amsjy5veO77E03tm+vDuaDKS+DT7qsfHePPRAdCGKbH7rOvk3/GSPCyVfdEm/HVgLdYCls2OUHd3IiIdDXfPGtX6P2yeR3ptiC8/hC73fQxpq2y6qaUFF47tRWNVCYIjuqHvfePNf0uaAvSdBFw7ak4uDYkxh1r87PGg6TTGB9CmacG05JZvaGNieHwksgpKXSZX0kwb1gO7ztyy9Gihy2VziioCwviQEMCQnm2hAE977/gKWkWUJCm8vPkk6/d/+MOVth88tD60YUqU6R00T4HZ0zFccgGZkp/xvDSL/cFFREQ4x51RQbT+D9sNgYQAtKjAuWN70T9jEk7u/RyxOcvRHxWW19zeF4VbdDhGIuW8nJYrOpXxQSMEwwMwr0O0AJq73T9BABplkI3WhbW8dqBIpZMUsO6Hy5g3JhF5xZX4vlWdU0gsmdQP0aEKmyZ8APCRhMDy7zzPS/FEUUcbpsCxK+XYcKR96XI7T4cbRK+HiIjA8PCZbKwqwcm9n2PQ0fntjtOFqkCXo/NxEnAelhEAnUZkDDC7+Ee+e1BQO+2PZqVCEyzH9A25rN9rL4aWVVCKOZv41wjhAloMiw+8ScjVaZQ4snCMxeCwx6yNchkbs4tR3ci/EauWS1Hv4LOMl+RhHQsRMb4QQzkiIr7nyuDFiDr5PsKoOofPH0mZE1K7vHHREp7xBWxExjpNwikgTBf/8u8KPVYytZdLn5CswwczBgsyqdYevgyPIT3DvaoEmjJI59TwAIB9hWVYvf+iTwwPAA4NDwnItm6yfv6ySf+eXkSkU0FRgAkS9Dq5Eho4NjyAtvDMhWN7fTtAFnQq44Op1kd8lJrnkbRRqm9CpRcKqnT5al6r5kWEWuFzz8cjKbFY9eQgfPXCfS57xPiCE9eqvXr/+sPFlkZt9gglOdZdy3tfIhXAGEREOgN0jELCwuSvPb2Dp9F4D+fGx8qVKzFs2DCEhoaia9eueOSRR/DLL79wfRqPYFrKOm24a3cR14Sr5NBpvFu0acNqXyE7OXQuGNBNA4mEgERC4OFBOvdvEDjWjfcAs9GRU1SBVft+EYTnjGnLe74QVqBWRKRzQBIEQLBLE+lz53uYWtz35PIHnAeDfvzxR8ydOxfDhg1DS0sLXn/9dfzqV79CYWEh1GrfeRQc4S4pk64+eS6jF9YcuOwzIa/qhmYsmZSElzfne3yMrqFKZBWU4tPsq9wNjAESAlix+7zNz4GMtScpPSHKIkMvBKODhmnLe29xlM8h5niIiPgeCoDUA59rJGoslTFCg3PPR1ZWFp599ln0798fgwYNwmeffYbr16/jxIkTXJ+KNbTWB9DeeqR/XvpwEuRBEvzzyUE+G1ekWo4Itdzj9+s0SgzpGYHl3xVyOCpm2GuSsBGzEjJ3apssCbxCMjwAII/si1tUpF+utWh4iIj4Hm8eu8aqEs7GwSW853zo9XoAQGRkpMO/GwwG1NTUWP7V1nrXU8IdtNaH1i7ModUoLVUjbFDLvRds0WqCveo9s/ThJJy4VuXTRTLQPRzuiAyW4/XtBT7P75AQwNppKS7VeElIsNz4tPm/HRh/FMWNEeitoSGGZ0RE/E9wRDd/D8EhvNbgkCSJV199FRkZGUhOTnb4mpUrV2L58uV8DqMdE5J1GJektektYq3nQCcWukKtkOKjmUMgIQjM/OSYV+Opqm/2uPfME6ndMC5Ji11nWHY09IJRidE4fKncZ+fzB7O//BlNLb6v5SApICpU6VaN97/kcMwxvmrW+UCbzkclwhBN1Aii4kn0koiI+A+KAm4Tkeh733h/D8UhvHo+5s6di4KCAmzdutXpaxYvXgy9Xm/5V1jom9CBdW+R9IQom/JKJiW59QYTgqQSpCVEed0zZsXuQgzpGeHRcb7OL8HIdw/ianmDFyNgBgFAExzkleERKOuRPwwPmi9yrsLQQuLVzD6IsaseClfJ8ETqPaAA7CWHY6ThPUxrfgPzm+dhWvMbWGGc5Z9Bi4iIeA2XzkKCAIq7P+FTnQ828DaqefPmYdeuXTh8+LBLsRGFQgGFoq29ek1NDV9DYgzTEAjdIM3bnjGl+iacuFZlOY4n71+9/yLCVTLoG4y8hQooADVNnmVO09dG9MS75/uCMnxfYK5a0oYpsCAzEXHRaouHbteZW/g6/yYAcwgml0yyvDdN4vu8HxEREe6gwN0mTRaTyNGRuIdzzwdFUZg3bx62b9+OgwcPIj4+nutT8A7TEAj9Omd5JGzczndqmyzHUSvY55HQNyyXN641Ia1jYhrHt88J0WqUeD4jjttBdQJu1xiwev8lKIIkFg+dq/vTXTKqtzkhJAWYqE4lDyQiwjskBZQhCi2PfgqSw2VZqPkeAA+ej7lz52Lz5s3YsWMHQkNDUVZm3sFpNBoEBwdzfTpeYFKSGxOmAElR2HGqBF1DlRiXpLXJI4lWKzB3cz5jJcxLt2uRfakcoUoZnk6Lw7of2TeJq2owYkFmIrYev8F58qlUIgHAvPR4YrIOM9N62uTU5BVX+rwUONChjcnl3xViXJIWUgnhsvsxnYy6TrYaJGVrBNKv/7hlMqYEHbXJFWE0Fso8lg0tD+EQNRgjJAWYH/Sth59MREQEaNvQlaYvRUWdEv050A2mj9mkv+31sfiC894uhJPt/saNG/Hss8+6fT+fvV3YQJdZArahAtq7YN+bxLrJG2DuMutJvxZvWTMtBZMHxiKvuBJlNU1449uzqDc4NxqcLWJc8JFd9RDdW8eT5ndhyiA8ntoNG49eY/W+SJUMlQJpJOgtW2anQd/YzKh/j6Omc7eoKCw3PoW95HBIQOI+yQV0RSWiiWrMC9oBDerdVjFRFFCKKIw0rIEEJH5RPAMJKIdePnpmERNPRUTMONPOKYzMRL8Hn8Sl79fi3qYznJzLH/1d/NrbhaIoh/+YGB5CwlkoRaOSAWjfm6RU34Q5m/It0tzelM56Q9dQpSWZNlgmcWl4AMCqJ12XdRIAwoNlHo3FXinUlc6KO2qaWpCZpEW4it1YSMpsKHaE9e+/50oZy7s7SkYdaViDveRwAIBSLkMY6rBQthVLZJsRQZgND3dbEYIAYokKDJdcwFDJRUgJx4YH/VqCEPN8RERonD0r/av2Q7L9Rc4MD0D4/V2EmQYrEKxLcsv0jSiva8baQ5edvp5Cm3vc09JZb5AQwJCeEQCYlQsDwMrvL2Bqig7rDxe3S5iln5PnMuKwav8l1uMp1Tcht6gCEglhCb+MS9Ji3axUj1RDc4oqWDek81UDOF/A1utjn4xqzf0tOZauuJ7ARuK9Ixh+IiJ8wbdnUKgiY53a+DCRlFOtDxqphIC+sRl/3cusrwe94NIluJ6EGDzl/9s78/goyvuPf2Y3m/u+E85waFyCXJoQolQOBUUQFVsrKlCrFkGhWgVPpKjB2p+iQqFSRRQRRVEiKhRThSKBKHcIkStQjiRIQg4SyLH7/P6YzGZ3s8fM7szObPJ9v14RM5l5nmdmZ+f5zPf5HmYG7Dp5Adm940RX8C2vvYx3tpbioeFpyNtXZnNMcutS0o3GZHxceMqj6rv2fi8pUcF4fpwRf79rAFbtOGmJ6hCH51fS00ikjoirqrhiH4S+SvFOEIR3aNXptNOKD0c1O+z9NoT9xKyxWzP9o13426SrvQ7B9QRhuUeqUMjbV4YtT47ArpMXLA6z4IDzFxux+D9HcKnZsxBbe8tDWc1lj2rYpEQFI7tXPBZ/L90RFyDhYY1QFdcTBK/8QnM6AOAsi0UytFFhlyCINjp1kjGt4qxmR7md34anJdRrL7fgT63Oqo78RuK8qOPiDqHA3IINB0UfIxRTE6wmQQE6PLF2Lyb/aydmrdmLN747gppL6lVG5MCnkBesSYR3iF0ysb/vBfed+c33wQydJbKGA6VSJwitwScZu7PzJRnTKq4EhX1Yo9ilC2fMXXcAu567sV0q9yE9YvCb176XNRxWqMh7ob4RM1bv8ehNv7zmEt787rBH/h1KwQFYcg8fNWMyM1zTIxpf7ZeyVKMOwufxu2u6YVG+dq4nIH7JpIpFII5rq7VUhUh82ZKDGoRDBzPM0CHfPBh1CEYEtFV8jyB8idLVns2tYe6S+zD7pjK7J3Q68eFOUAhWgDc2/wJvXeWqG5qx/ch5XH9lArJ7x1m2m8wMqdHBsooPBuDWq1Ow4OtDHi8xLNhwCFUNTbKNSQ4YgJiwQGwsKsOLecUe+Z34GusKyWMzUpCeEuGRg61SCInI3C2XNCEA/9d8J6K4ekzU/4h4rhZ/NHyLP+JbnGWxyGsZhrsCtiCS08Z5EYRaKO00Ws1FoFyXCqP5F4lHatck2anEh8nM8ONRcXVJPPUtsOfBD3/GorsHWvxINhaV4YlP96G+SX5Fuvy/pV4drzXhIbC5uBwrfjyh4a+RLUmRQfh9Znc0tphRcKzSJgHdmQsN+Mtn8oXTeYKrRGTWJOEC/hzwuUOfpWRU4eGADUoPlSAU4fjgZ6BvuoiUX95HYLP6JT2cwRhwESEIf/ooTD/9G8i/T9rxnPdV15Wi04gPRw6mvuByixl/WrUby+4dDACSnVcJ4Mu9Z/3imkUFB2BaThrW/HTKZunK2pG5QB5N6zWbLFVxVyIVFxzuY533w16gCH+jBGKEP9LLcAGYsBAwv4jSr19H2q6X1B5SO4Tv3oFu9yLwuw/QXHZI8vG9Tq+DqeVVTfp9aG9ECuBJxIrcvJh3EADnF5OoVuAAxIQZUFWvTYuMPTWXWxz6dwiOzEvuGYziMu28ZW0yZ6K2ORQfB77idB9X4oKEB+G37FwKhMQAw/+CSi4KWqxAJny/hp1eDpz27PhkVOHgzk3olzNO3sHJQIcXH55GrMhNeW2jyiPwPxiAoWmx+KZIfH2CUIMeDc3acrIS7r2ZH+9WLJW9pyRAO2KIIHzKD68AO5chRZ+q9khE40nJAkoyphLeRqwQ6iJFeADQnPCwRmvCA6BkYUQn51IVukgssKgmnIgSCPZoNclYh8/zoVaNFYLwB4TIF2fCiDHK4UEQWkKs1UNICEhJxlRCjRor3jL6qgTKGEn4BCHyBWhvmdGipYYgiPbYvyAI392y7HmadDYFOoH4yEyLdVm1VWs8PDwN03J6SX7wp0QF48HreyoyJqLjEBNqwMPD02y+D0LkSzlibfZlrY8HciwlCG1j/x2t4cKxb9hbGDRmijoDEoE2JZGMCGXcfV1jRUBKn4+N6ANjl0jM+Eh87ZPoEAOm5aRh5sg+KCytwvL/nvBkmKIQwiu9vYbj+ifjx6OVfldxNlDPIUDHoaHZrPZQPGJor1h89Meh0Os4hAYa8MZ3hy1/22TOxObGa5CpK0EiqhHPVeMFwyoVR0sQnRtPQ9nNDIjCRfkHJDMd3vIBAGMzUhzWWPEFUibqD3aexPRVuyVNyjWXmrHou8PYXFyuuH/LiCsTZBFvXx8oR7PJ/ybwJhPzW+EBACfO11v+v2d8qMt9o/3g4UUQHR1Plj6FJfuUgvkwtahXk8sdnUJ8ALwA2fLkCDx7SzpCA7WZ9a26oVny5C7sP/+rYr4SrQII4ju/5Feb371BiQyvhGvKaxtRWMp79tv7Qo3RFWJb0GNYE/gS3gpcjMcMX6owQoIgAMAEHY73nYZfObulUJEThI4DklGJkp2bFBidPHT4ZRcBvjbIwQ6Zb0OoRwOO9/0or7ks6/KSfVtq+SFGhxj8bqlGawjWscy0WMSGBaKqvgljdIVYaljUbl9XOQU8LnRFEIRTBEvH/uzXAU6H8KN5XrWn1RwfQCexfGwsKsOfVu3ukMLDmnN1jZg33ghAHuuElvjz6Cuw5J7BovcPNnSKW1sy8eG8dUyv4/DSbRnQwYx5hg8AtE+hLggLR570He3+IggtUM1FYt+wtwBOhwHbH0MCq/SqPa3m+AA6gfgwmRnmrjug9jB8woINBwHAoX+LP4fuJkcGYebIPjhfL148XlbJN0Pzl9lKSNxydQoWDKxBKue8ui3HtbduVCHS4XaCILzj2KBncPWoyUgtmA8Ozl8I3KH1HB9AJ1h22XGsEtUNncNUX1XfjOmrdmPpvYOxbc5IFJZWobz2MqouNiI6NBDVDU34X1UDVhaclK3P0EA9GhTy3xC+Zy9O6Ae9jtN8zpbZo/rgk59Pu82oq0bUlYC9U/LkfsFAifvj3mqZiKPmrjiHaCShCm8G/kOhERJE5yU0vjtKdm5CP1R6/CZjneMjWaM5PoBOID4Kjp9Xewg+hQGYu+4AIoIMqLnUjL9tLLGZDOW2gCy//xqA8de5sLQKhSccV0j1hGSrarAAcEHjBebSEsLx/DgjHlntOlQ6KtTgsSB+8Pqe2LC/3OYzjQk1oKnFhPom99aeF/IOIihAh1uubq1nEZ4kqt/t5gzsMPNLekN1xdIHLgNURZeQGy3dUybG4XJNBcwt3j3nznFxKMuep+kcH4AC4iM3Nxfr1q1DSUkJQkJCMGzYMLz66qu48sor5e5KJBq5s3xIdUMzJr+70+Hf5MpayYEXB0N7xUGv41DX2IzF38tXLz42zIAtT45AYAC/MmgyMyz42neTXkyoARcamiVZKc7VXsa720643Cc2LBAGLxTgut1nsW3OSOw9VY1zdZdR+utFLMo/Kvr4ussteGT1HowvKsNoYzISw67E0MhUcLVnHe4vmG8LzemWbUJK9mSuyqfrtlqZJIiOg5buKQ4Mg3b+GT9H3+zR8btDcxCUMwPpWWM0bfEQkP3ZsWXLFsyYMQM7duzA5s2b0dzcjJtuugn19fXuD1aA7N5xovaLCTUoPJKOg/B9nTfeCL2Os1QOlpOq+mbsOtlmRfFlgcCZI3rj5+duxDKJuWFe/qYE5bWux1hV34SKOs8dnyvrm5Dzaj5qLjXBoNPhrf8chQ5mDNUVY4JuO4bqiqGDewvIV/vLMWvNXvz+3Z+w6mKmQ4HFWh1L81qyYbZ6VLhKyU4QhGcI7yRX1Gz16HhD9p/QL2ecZtOp2yP7KDdu3Gjz+/vvv4/ExETs2rULw4cPl7s7twztFYdoN2busCA9AvUaksAaIibUAAbYXD/75RClhIG1f4IvCwTm9EmAXsdhbEYKbjQmo7C0CpuLy/HejydU9dcQqKpvxp9W8Us7Y3SFmBf4AVK5tsqcZ1ks5jffj03mTLdt6WDGKFPrw86BcxtjwISAAvzNdLeNANlkzsT0ptl40fABUvyoKihBeFKW3lfoOCAa0l7UGQPqwb8kmVpabMWH2QSc3A5crOCXWHsMA3TayHOluESqqakBAMTGxjr8e2NjIxob294E6+rqZO1fr+Ow8I7+loe1I+obTahvVC/p1fC+8bgiMQL/+rFUtTHYExaoxzv3XYOhrZajwtIqnKu7jMSIYGSmxUJvtXSglDCwdjD1lbMpB1vfEr2OQ3bvOGT3jkNmWizmf1XsMwuMO5zl50hGFZYaFmF682y3AiRTV2IjXOzhOCAVlcjUlVh8PgSElOwz9F/gzwGfA/Dep4ihMy6UEr6E47QfLl6NMESyerffJ8FnJRyX0T//PlTkx+Gs4O9RnAdsnANYL6lGpgJjXwWME5Q9AREoumRrNpsxe/Zs5OTkICMjw+E+ubm5iIqKsvwYjUaH+3nD2IwU3oQeqc3w061HzuOzPacRFKCdyOf/++0A5PSNh17HWSbg2wZ2QXbvOBvhAXgmDDg4//Jz4JOlZaa1CVZfFQhkAB5ZvRt//eogCo5VwmS1rjA2IwXb5ozEpMFdFR6Fe1zl5xB+n2f40O0STCKqRfXnaD8dzMjUleAkS8EbLZNQYV+YTqKJSNhf6nEEIRW5n/0MrXWvZLp3Q3SOx+iu/QRWiQHbH8Px1Y8Dn95vKzwAoLaM317sXfIyOVB0tpsxYwaKioqwZs0ap/s8/fTTqKmpsfwUFyvjVHijMRn/99sBmDmiD27O4D38tbReXd3QjMYW9euGpEQFY9m9gy1LKmLwRBg8NDwNQHsBYu9PYs3d13b32ZLHez+ewO+X78CQBZvx5neHbUTId4cqfDQK5wgWC2cPUR0HpHK8xcIV5xAtqj/7/ezTsT9h+AwMDK8334ELLNyrKAItmsMJwjkcOHCoSB0FM2c7pXoiSBiAQLPjpZdqLgzbuz2IaoTx1hsnLx49Dq8Ac+zJxf+zcS6/JKMiii27zJw5Exs2bMDWrVvRtavzN8WgoCAEBbXVJKmtrZV9LBuLyjRlLpeCkMJaaaF0wxXxePg3fdotqYhBSuXguLBALLgtA7dcnYJB3WPafS72/iSAup9f9aVmvPHdEazYfgIL7+iPqJBATaR498ZiYY0lcgWOhYyjaBfnyz0XMDtgncdvlSQ6CL8kNA649Q0kGyfA1NSI49++gZYTP8JQewo9W6QvpTv7GjAA0RFR4NDqF+LixQMuLZ4MqD3D+4KkXS95fHIhu/hgjOHRRx/FF198gR9++AFpaWlydyGJjUVlmL5qt+pOgp4imPOU5sHhvW0ig0xm5tLPwx6hcrA7kVBZ34QFXxdDp4ONQ6ezfrTy+VU38AncpuX0VHkkPJ5aLOwRIleWGhbBzGxNvYLg/WvzZIuzqbvlHi1ZEwnCJ4x5GQiJAQ58huOH9iKh+L02p1EJgtqdtZADgLqzGFq7XB6HlYvqWnA5xuSd2h555BGsXr0a69evt8ntERUVhZCQELfHnz59Gt26dUNNTQ0iIyO9GovJzHDdq//x6RuzQcehWYEnsJKZRAE+hfmLE/phbEaKQ0tDigOLhCNMZobF/zmKN7477HI/DsCSewYhJiwI5+ou8zVHGHC+vpGvzsvxtWoWbDiIqnrplgb7idCg59Bs8v5zCQvUiUrmpTQ6mLEt6DG3FovrGt+0iVJxxhhdIeYZPnDofGodPTNUV4w1gS/JcQoE0TEIjQca2pJZerLkKOUY2RKj3fAMcMMcGRpqo7a2FlFRUTh16pTLFQ9AAfHBObkqK1aswNSpU90eL6f4KDhWid8v3+FVG50F4VN7aHga3tla2s7SIPx9aas/iDPLiMnMMOSlzaIyeCr5pvzg9T0xMj3ZMr4l3x/FtqMdK9ut9fKHI4uFmGgXa27W7cQ/DG8CsH24WbcXhBa8FbjYm2ETRIdAiMzSYoQWax2U23H99kNZI1+kiA9Fll20gi9zQyiJ0lYPoM1PY/l/2wsP4e8cgGe/KMLO45VYv68MVVYhqYJlJCJIfOpwJU30G/aXY+7NbU6r3xw4i23iE4H6BZvMmZjePJu3WFjl2ihHHOY33ydJeOhgxvOGD8HgfDllnuFDPNH8sKj2tJxLgSC8RZjctSg8rHE9Po53PE0fp0ruD/9IheYhWi9EJpZbMpLx2e4zPunLlSBg4H02VmxvX5iuvOYy/rRqNwZ0jVJucBIoq7mMwtIqix/LM7cY8eGO/6k8KvkRcm1k6kqQiGqcQzQKzemillqscZfvQ9ea7wOAKAfVvJYsPBzwjaQxaAkt1fwgtImWbw9x9666jqfaSSyhAJlpsYj287Tp4UEBCAvSvkYUNMu+0zWqjsOazcXllv/fcvicz/r19UPJDB12mI3IMw/DDrNRsvAAxEfPJKDWaWp14ff5zfdhoeleTG9+DOdZuOSxaAEhuytBOKJDCVOVHE87tPjwBwZ05f1anN3LFxtbsLKgvaWBcM/6vWdhMjOYzAxz1x1wu78cz5OYUAOSIoPc76gyoQbbr76U6BlhuafcLqlYOeJs/Ew2mofi0ebHZBmvGnSoCYbwO3wmfkVWtpYb7b9Se0FhaZXHpcvFkNkzFoUnvKtrse90rcU6o+RYOyOV9U0oLK3CjuOVoq6tHN/1Cw3N+OiPWdBxnMXZdefxSizKPyJD69IYf3Uyfj5ZbRO1FB1qwNTsnlhZcAINzW1RO1LzfYhd7kmA/Hl7CKIj4mipz5vlvypEIiYiDFxdORw/3Tg+3XqPYZ514CUdWnwo7XBadFaeJYaahmYwAJMGd0FIYADW7jqFy83qh3MqhS+Ls/37YBned+CjoiTnLzbitoFdLL+r5fg82piMRXcPxo7jlSg4VgmAIbtXPMChnRgSk+9jfvN97arb2td7sUesRYUgOjOCleOI8VH0vWogEJ6Evb8cReqOBUhq9bWSyuHEmzH0hlv5dOrtnrqtX/CxC1UrNNehxYfSDqdyRaAIt4SvnErVIjrEgGk5aeibGI4Zq32TOMyRc6zS2N93J843+LR/oebKVefPYdcPMXhqRyjO1PKWn8XfH0N0iGM/KDmjZwTcWVQIorPhyJpRzUXgRPbLfEG4VgalXQ/T6PtQ8N7jyD67UnI/EQNuA4zjgN9+4KTA3EJVC8x1aPEh1Bwpr7mseobMzsxNxkRMy+llk710qW4wXsw7iPLaRjdHe46vM25y4NPDCwXx+IRrR/DGd94vuQiCYmQXM3b+asD3l/s6dCy1SRa2jd+2lsVivu5+i3hwlR5erugZAWuLCkWQEFrGzDjoOOUfGOe4OBzregc41gKAQ3j6CBizb8GggPbTsT4gAAhLkNQ+Y7yYSc8aw28wTuDDaU9u551Lw5P4pRaVLB4CsicZ8xY5k4wBbem5Ad+Z+glbPnogCzl949ttF5sN1R8Q5tTZo69Az/hQnDjfgI8L/4fyWu+XXBxlH7XOOmq9n5xJx+Tk78bjGHPsJURwHSP3DkFIgTGghgvDqVHLYMy+hRcVIvlp/VJcu2eupL4YgH3D3rKxpFgwmxQTIqpmOPUWucUHwAuQF/OKZZkICGlEhxqw67kbXdaFcZTO3Ru/kLAgPa7tEYMfDvsuo2l4kB5mJt9SnIBYQSF3unU5ebJbCSb9usTjtWuCkIJWLWwF3R5C9gOvST7u4I9fo9/meyQdY2a8hSXhucO2Qqc4z8kSzKuyLMGomuFUu2hKY3nFzRnJuKZHDBZ8fUjtobhl4R39AfCp7oXojyE9YrDr5AXL7zcaky0F5sprLqGqvgmVF5vwjy3HPOqzvtGkqPAINnCYmp2GuPBAnK25jLU/n8bFxhbZ+xFTxG2e4UPLMomYJGGZuhK3TqL2RIcYPK7kO0ZXiOnnFvGWIS8mBMqYSoiBwbt7xNV95q2oMST19ei49KwxqNgci0RWJbp/HQckoxIHd25Cv5xx/MbivFbnU7u5sLaM3/7bD3zqA9LhxYdWqqLKRUyoAYvvGQwA+Ne2Us36s8SEBiD3jqsBoF1xP3tfDCE1OwD8bdMvPi0E6AmXmxmWbT2ueD9SBIXYJGFi9xO4fWAqUmNCsOR76UJQBzNyDf8CB3lEQzP0CISyZQYI/0SI0PL2NhPuU0dCw9t7OCSGj4AztbSgZOcmXLpwBiExXZCeNcblMow+IADHu01C0ql3JPd56UJrEIPZxFs8XBXP8HGq9Q4tPkxmhvlfFWtycvYEDkDuHf0tSxjzxhsxfdVuyUsUclV4dcVfb+MtHo6En70TqJCanbBFiqCQkiRMCl/sPet+JyfM0H+BWO6ix8cLmMChZeI7qF4/BwmMomaI9sh9T8hpYROWQNKzxmDPppVILZiPflZLkBWbY3G82yQYkvq2EyOCUOEuebZkKQgenNxuu9TSDt+nWu/Q4qOwtErzb9FiiQ0z4JXb+9uUtB+bkYKl9w5u5y/hDqWFBwC8tOEgms3iRJEa4jDYoNN8LhUpgkJqkjCl0cGMPwRslKWtloAIVO//Fkmo0nZBDYKwQ3jRKsueh7L8jzBge2vGX6v7OJFV8VaNU/zvFZvjcDZ7HgC0EypS+hUEDwDxKdR9mGq9Q4uP8ppLag9BNp67xYiokECs33vGpoT92IwUi7/EubrLOHG+AYtao0fUtPhU1DW530kmbs5IwrdF4r80MaEBePYWI/7y2X4FR+U9UgSFJ0nClCRTV4IYrl6WtoJaapFUuk6WtgjCl5zj4lCWPQ9Xj5qM8y9dAaC9lcbeypLAKpG4/bG257fV38X4nVgLnmRhOUdsCnUfplrv0OLDuuS7v/PSN8Woqm9z+hP8JMZmpECv4yzVWwHgyuRwydYQf4UDUFh6QdIxFxpacMEPUtlLFRRKJAnzFKm+JX4JpwNjZjLGeIEakSms9T9K9ssANAfFIuGJQ0gODOIjVlApynKnay1q6MhXSih4yLkQJILgsQmz7TGMj2qpLYNWUq0r9hq0ZMkS9OzZE8HBwcjKykJhYaFSXTklNlz7Bb7EYi08AN5PYvqq3dhYVNZu37EZKdg2ZySeH3eVr4anGgx8DZfYMIOkSeBCg3LJzcQyc0RvzBzRx+U+You4We9/XeNbuLvpOTzWNBN3Nz2H6xrf9Hl+j06RVn3oDLVHQHgAB4DplK12zgEIbKyC/gw/71kcP8UezzkXR84cYasRju3dHkbCc4fb5/fQ6flwWsvo7EcLn6daV0R8fPLJJ3j88ccxb9487N69GwMGDMCYMWNw7pzvypoDQHKksunV1UTQrvO/KobJQRpPvY5DfETHEV/uuN2qlooYdJy4Wz/EoMNrd/ZHbFig7G+4OX0S8Ocbr0BKVLDLtsUKitgw/oEq1FzJMw/DDrPR53k9gLYlI19mmPUp2TOBMS/BPOl9mKg4uMeoFTqtY80wDZ+Do8EZynbU6kNhcfyUEfsMXZHsIob+75/Yn/+R4wOME/hw2sgUuwNTfR5mCygkPl5//XU8+OCDmDZtGoxGI5YtW4bQ0FC89957SnTnFCG9ekeFASiruYzCUsfhmHLXtokO0e4q3WhjMpbeO9it4OTAL1lZL1O54lKzGV1jw/DK7RmW4+UgJarNb0cIM3aFGEHx/K398PGDQ/Hm3QPdWlQcIVyb5Mggr89TWDICfJvi3mccXAcU50GfcTv2D30dZtZBz7PDwkG/dxUuDXtS2W5afSjSs8agAnGy3iP2wk1Ylk0pmA9Ti5O8Q8YJwOwiYMoG4M53+X9nH1Clxovs4qOpqQm7du3C6NGj2zrR6TB69GgUFBTI3Z1LhAe7Ftdko0MM+Mc9g9y+9YrBWdVUQXx5Hfve+rPwzqux7N7BmhJ0woSZmRaLsRkp+HHuSPx5tONkPsJ1mDfeiKG94pwWWLPnXN1lS2RRst25p0QFY9Yo6cmD5o03WkKmhbZjwwIlt2NNciQvqm4b2AU5fdqnsxc7rhcn9APgvdASlowuIMLLljRI7Vk+MVNxHgaNnYZ9w95CA6ed7wXhDj601JgajQsIb2dFkIOmoDiLD4U+IMASwaKkSBWSi5Xs3ORiJz0fTtt/Ev9vR6lqe/78eZhMJiQl2XrNJiUloaSkpN3+jY2NaGxsW3+vq6uTdTzOwlFjwwy4c3BXrP35tMfZG71hyeTByOkTD52Oc5irQ0ruDmcWDkF8ic0FkhIVjAkDUpC3r8zmWiVbObcCsImuiQ8LAji+jHx8WBCeWLsPFbXKJD5zUhTaZiLX6zjMGn0FrkyOaPeZ25/HtJyeooq+CdfXPrJIiDoCgE9/PiUq4Vt0qAEL77ANmRbaHpmehKG537Xz7xFDilVBO6BNeIp1OrYflych3PZwAP5tzsRNPSNx58m/ij6uNeWRf9CamGnQmCkwpXUBVt+p9ogICegvnceJ7Ff4EFiZnVD/13Uc+lhN7IPGTMEe8OGznpYaEOugK9XHRA1Ut6Pn5uZi/vz5ivbhbNLQ6zjccEUiJr+7020b3tQasW8nOSoYQ3vFWcbm6EGfHBWM58cZseDrYqeTmn0VVUc4az8lKhjPj7sKMWFB7a7JU2OvcnitBOyja6x5cYLRUshPDoRzfH7cVVjw9SGXYsL+vJ195gIzR/bFiu0nUO0k8sXR9XV27u5EXligHg8N742ZI/s4rXMTGKDDK7f3l1wIkYOtABPGOW+8UXTytiW/H2xT/M/6+n1QcALfFpW7bSM2LNAmwkz4fLrVADjpfgwFXf6ALvGR6Hb8E6CuvSO1VMytKkY5jwzbxEz6PiOAkBjgkrToq86GmQEMOnAwq58wLjwJg8Zcjz0A0gqeRjTkCQ8HgMZeY9ttGzRmCkyjJuPgzk1oOP8/pO+ej3BccigohOuk59ryEV3gIhGLWrd9K+FjIjeyF5ZrampCaGgoPvvsM0ycONGyfcqUKaiursb69ett9re3fJw5cwZGo1HWwnKuMJkZrnv1Py7fWuPCArFtzkjsPVWN74rL8e6PJzzqS7i/lt47uN2EaTIzhxOls6q8rtpyhLP2lWBjURnmrT+IijrvIkrsz1GJc3CWfl/q9RXashd50SEGTMvpiZkj+4oeq6N2nBETakCuA0uKwDf7yzDz491OTb2CwNo2Z6TT8ZnMDENe2uxWpG15coRNzR7h8zG1tOD8S1cggVU6zVdiUwTLuurm+SPAllfhTIo5exMUzvfYVdPRt2Sp45OXi6GPAGNz+f8/sA74fJqy/fk5jAE7k+9BZvlqALYh5M7f7LlWYee83IBkOB0waQXQbyIAPpvombwFSD60AoHNNR4367SomwP2bFppSTzmKJR+T9YbCI5KsqRi7ztkFC4sNIr/LvkY1avaZmVlITMzE2+//TYAwGw2o3v37pg5cybmznVdGliJqrbukDrBS5kcrElx8abubnyOLBeetOUrTGaGWWv2YMN+z99gfXWOcl5fuQSSdTsnzjfg48L/2VRlliJqvtl/Fo+s3tNuuxSBtbGozKEVRWwb7h6yTst/A8DBL4G17f8mfFcdnX05WnMd3Hgv8FpvsEsXlF3K+e2HvNNewRJg0zNK9tQxiEzF8eSbEXb4S5slCMefaetvNzwN/PCKzAPh2kd6tIrfEz+uRY8jK8EgViBZid5+j6LvpPmi/CmElOvW18Fy/zr4Tnj1XVIY1cXHJ598gilTpuCf//wnMjMzsWjRInz66acoKSlp5wtijxriA5A+AbmbHFJal01iwgJleVP3peVCTnK/Kcby/5bavHnrOGDUVYkoOlNru4wSGYTfZ3ZHz/gwn5+j1q+vt+OTQ2B524bUh6wNDkuBd4HpppdRUhOIhspTaKn7FQHh8QiN725brKs4D/j0PlHn6DGRXfiogY1zgULpBcA6H/y9a5q0AiU1gW1F1qKaoP/3M+0+Z4xdyBc9W5ThIlGWoz4YXC+atybXmn3AoVBwdM9WIA7Huk2CrrEaV5z71vkyiIRS9VKLzXn1XVIQ1cUHACxevBivvfYaysvLMXDgQLz11lvIyspye5xa4gPw7gGv9clLTZpazPiw4AROVjWgR2wo7svuicAAHV0zHyPH9RbVhvWySXgS7/Hf+mCX+pBt127pf4GT2/i5JO16oOd1fNsu+gTAC5CvZslrtrdnygagfL+ilg+5nHG14dTLAREpwO3LgPpf2z43wPlnaSkLD7gVIJFdgMFTxFlLpmxwWlDN1T0rLNV0279IOCPb8wMUy6Hh1XdJITQhPjxFTfFBEISXOLRQiH8DlNx2RArQPRs4/h/gUrXrPvd/Cqx70LsxuOLOd4GrJgAvJwFMgaKFIbGAIdhNdVI/R8y94sQKhjGvAKFxtqLl4BfA5w+47/fOd/nQU6mYTa3WGGefiWvLSkdDivhQPdqFIIgOguWt1O59praM3+7NG6CztuvK+IRf9jjqM0Jh/6jwJCAgkM9+uv0t+dsf/ya/9HByO1C6Bdj6mudtpU8ASvLkG5tciLlXjBParoMzS5dA5TFx/XpaUE2Dper9BcoLTBCE95hN/NuoQ1N467aNc/n9ZG3bGQ76FIprKbHgEBLTtmTQ9VogMEy+tjk9MGklP+kKCaIS0r1rM/FKz44LifauX7eIvFfEJMoqzhOx5MLxVhNPC6ppsFS9v0DigyAI75HyBih72yL7dFlcy0uypvPtCxaaJvnyRWDSe0DGRNtt3pQ+j+wC9LjOs2Pv+qAtNfcYuSNPBBzcK4K/z4HP+H/diViLYBWBq4Jq7vrVYKl6f4GWXQiC8B4l3wC9fWu0Pl4oruXIZ6C5odVvRKIbXEgsMPwv0iw0hjC+P1f7cnpeeLTmobDBbYl0p43yk23a9RKPb/VdEBx8Af58CxYr54MifG6e+BGJFaw3PO28DTH9arBUvb9Alg+CILxHyTdAb98a7Y93Vlxr/FuQnseY430xdHppFprmeuA3bt7Mhz8JXDXe8d88seJEdmnzpZB0vJOS6zZtKEB4Upslyf66Cr4hxU78VsQK1rjejreL7VeDper9BRIfBEF4j1t/Ci/W1r3x1eB0QIODOhqOfAbMEiNUrCdzQHpK+Pi+fHKyyFTHf9+ykI+kECY6+yWA9HGOS6QHxwADJgMT/8kvjdyx3HH1Umcl1jm7acFVyXXjBKDfHeLO9+rfidsP4K9ttyzP/Yi8EcNS/Zc0VqreX6BlF4IgvEd4A/z0fjgtAejpG6DLtt3AzMDaqQDnZhIwm4BvHhfX5vV/AXrd0D7/xManxY8L4Ce+tOt5EbH1746dI4U37WGPAkWfOV4CmF3EW13qyvh8GWEJfGSPswgQaxxFjnTLAk7tdB9JAvDX7dQO9+cakQoMnAzs/8T9vgB/r5za6XkkiTfLIZ5EsEiJwCEAkPggCEIunPpTpPKTiTdvgM7aFktr9Vmnk8HJ7Y4tJI74+V9AygAHia8kiCJ7K9Du953s2Nqmo9Bd67BUAPhunmf5VQQrkDViw0LFLjV1H8r7i7jzMxH8XIwTeAuPGBwtsXgjhj31X3J0HQmnkPggCEI+lHwDtG+78hg/abud/ETkWpDi1Hqpmk/XPvQR4IqxwLdPwSOnT+GaeBPNA8555lY58qu4Q+x1O7gO6He7ewvWnVYOtt76EXkqhimCxSeQ+CAIQl6UfAO0b3v4X4AfcsUl3HI1UXoykez4B/8jhdB44NY3bCc+r6J5mIuU8a3ixJ3VxxukXLeNc3m/E2fRRvaCQI5IEk/EMEWw+AQSHwRBqIu7uiyu0OmBtN+IEx+uJsqGSt7RUom06NaMzW3/xq3oG7TCGTYtE7UIy40wDrGCQC4/IqliWEn/JcICRbsQBKEexXl8RMfKW/kaHCtvtY3wEEOPYXyuDVeExDp/Uy3O451SlRYegOMU70pmXhVQKsOm1HBbYRxiMpQC6kWSUASL4pDlgyAIdZC1FoyH9TE9St3uCS5M9d5E84glPMk7C5MrjBOAG54RVz3WEyuPWpEkFMGiKCQ+CILwPW5zKUjwVTi5Hbh0wfU+l6ocLz147OzpAa5M9c6cIyNSgcZaoOmi5/2GxAL1le2rr8pVbRjgfW9cOv966SehViQJRbAoBokPgiB8j5zVQL1J7e6Lgl+OnCkdkT4OCI7iE4hx4OuvcBzwgZfiwNQEfDal/XY5o2F0euCmXMf9kJ8E4QASHwRB+B45a8F4Exopm7Nn65v9bf8A6s9JT/blsI7IR4BxovdDc2o1EWFhErtUU5wH/NtJkjU58rwQHQ4SHwRB+B45cyl4ExrpcYE2u/YBfoLtfYP0w135vkgN5ZWMCwuT2IJu7pKsjXmFhAfRDop2IQjC98hZC8ab4l5SC7SFxLaPrPEmAkJMHRFOJ25s3mBvYRJbWM2twy4HbHrGcf0VolNDlg+CIHyP3LkUvEnt7vTYLvxbe2ic7bIDIF8EhBiHV0sIsKPrJFNkjLWFSYozsJy+O0SngsQHQRDqIHctGE9DI80mICQGGD1fvK+GXBOpWN+XoY8AxV+2v07G27xcmnGwJCVFUMjpu0N0Kkh8EAShHnLnUpAaGunKr0HpyAyzSfykfOUtwE0vtb9OJ7dLEB8iLUxSBAXVQSE8hMQHQRDqolYuBVmTnHnQt6gKvVaWCUfXSayz7U2v8NEoYixMUgQF1UEhPITEB0EQnQ85k5xJxV10iD2ufF/E+s4YJwDG8eIsTFIEBdVBITyEol0Iguh8SPFrkBMp6dw5PXDX++6tL2LrkIitpyI1eojqoBAeQJYPgiA6H2o5SkpJ585MfKSNGOT2nZHqDEx1UAiJkPggCKLzoZajpFQxI2V/uX1npAoKqoNCSIDEB0EQnQ+1HCWlihm1o0RIUBAKQT4fBEF0PrzJiuoNbjO7WiE2w6sjzCa+QN2Bz/h/KcMooTEUEx9LlixBz549ERwcjKysLBQWFirVFUEQhHTUcJS0ET2u4DwXP8V5wKIMYOWtwOcP8P8uymhLiU4QGoBjjMmUn7eNTz75BPfffz+WLVuGrKwsLFq0CGvXrsUvv/yCxMREl8eePn0a3bp1Q01NDSIjI+UeGkEQhC1iK7fKias8H5FdPK8C6zSMt9XSQtEnhILU1tYiKioKp06dQteuXV3uq4j4yMrKwrXXXovFixcDAMxmM7p164ZHH30Uc+fOdXksiQ+CIDoFguipKxOf1t1de4syXETTtPqxzD5AUSiEIkgRH7I7nDY1NWHXrl14+umnLdt0Oh1Gjx6NgoICubsjCILwT+R25qQib4QfIbv4OH/+PEwmE5KSbL20k5KSUFJS0m7/xsZGNDY2Wn6vq6uTe0gEQRAdHyryRvgRqke75ObmIioqyvJjNBrVHhJBEIT/QUXeCD9CdvERHx8PvV6PigpbdV1RUYHk5OR2+z/99NOoqamx/BQXF8s9JIIgiI6P2zBezrvwXYKQEdnFR2BgIIYMGYL8/HzLNrPZjPz8fGRnZ7fbPygoCJGRkZafiIgIuYdEEATR8VErdwlBeIAiyy6PP/44li9fjpUrV+LQoUOYPn066uvrMW3aNCW6IwiCIAAq8kb4DYqkV//d736HX3/9FS+88ALKy8sxcOBAbNy4sZ0TKkEQBCEzVOSN8AMUyfPhDZTngyAIgiD8Dyl5PlSPdiEIgiAIonNB4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ9C4oMgCIIgCJ+iSHp1bzCbzQD4TGkEQRAEQfgHwrwtzOOu0Jz4qKioAAB069ZN5ZEQBEEQBCGViooKdO/e3eU+mqvt0tLSgj179iApKQk6nftVobq6OhiNRhQXFyMiIkK2fTsySl2Hjnx9tXBuWhiDFGi8/jkGpaDnjrJoYS40m82oqKjAoEGDEBDg2rahOfEhFaGQjZhCdFL27cgodR068vXVwrlpYQxSoPH65xiUgp47yuJvcyE5nBIEQRAE4VNIfBAEQRAE4VP8XnwEBQVh3rx5CAoKknXfjoxS16EjX18tnJsWxiAFGq9/jkEp6LmjLP42F/q9zwdBEARBEP6F31s+CIIgCILwL0h8EARBEAThU0h8EARBEAThU0h8EARBEAThUzqM+Dhx4gQeeOABpKWlISQkBL1798a8efPQ1NQEAFiyZAl69uyJ4OBgZGVlobCwUOURq4fc1yI3NxfXXnstIiIikJiYiIkTJ+KXX36RabTqorVzW7hwITiOw+zZs1UbgxT8ZbxnzpzBvffei7i4OISEhKB///74+eefO03/SmIymfD888/bPJsXLFgAb2IdlGjTX9i6dSvGjx+P1NRUcByHL7/80ubvjDG88MILSElJQVBQEBISEpCYmOh235CQEIwePRpHjhzxzYmwDsK3337Lpk6dyjZt2sSOHTvG1q9fzxITE9kTTzzB1qxZwwIDA9l7773HDh48yB588EEWHR3NKioq1B62z1HiWowZM4atWLGCFRUVsb1797JbbrmFde/enV28eFHGkauDls6tsLCQ9ezZk1199dVs1qxZPu9fKv4y3qqqKtajRw82depUtnPnTnb8+HG2adMmdvTo0U7Rv9K8/PLLLC4ujm3YsIGVlpaytWvXsvDwcPbmm29qqk1/4ZtvvmHPPvssW7duHQPAvvjiC5u/L1y4kEVFRbEvv/ySLVmyhPXt25clJSW53Xffvn1swoQJLC0tjV26dEnx8+gw4sMRf/vb31haWhrLzMxkM2bMsGw3mUwsNTWV5ebmqjg6dfDFtTh37hwDwLZs2SJbm1pBrXOrq6tjffv2ZZs3b2a/+c1vND2ZM+Zf450zZw677rrrOm3/SjNu3Dj2hz/8wWbbHXfcwSZPnqypNv0Re0FhNptZcnIye+211yzbqqurWVBQkKR9P/74Y8XH3mGWXRxRU1OD6Oho7Nq1C6NHj7Zs1+l0GD16NAoKClQcne9pamryybWoqakBAMTGxsrWplZQ69xmzJiBcePG2Xx2WsafxpuXl4drrrkGd911FxITEzFo0CAsX7680/SvNMOGDUN+fj4OHz4MANi3bx+2bduGm2++WVNtdgRKS0tRXl5u872LiopCVlaWpH19MTe6Ljvnxxw9ehRvv/02nnvuOUuVXGuSkpJQUlKi0ujU4fz58zCZTIpeC7PZjNmzZyMnJwcZGRmytKkV1Dq3NWvWYPfu3fjpp5981qc3+Nt4jx8/jqVLl+Lxxx/HM888g59++gmPPfYYAgMDMWXKlA7fv9LMnTsXtbW1SE9Ph16vh8lkwssvv4zJkydrqs2OQHl5OQA4fMZL2Vf4m5JoXnzMnTsXr776qst9Dh06hPT0dMvvZ86cwdixY3HXXXdh8uTJeOqpp5QeJtHKjBkzUFRUhG3btqk9FNlR49xOnTqFWbNmYfPmzQgODvZZv57ib+MFeFF5zTXX4JVXXgEADBo0CEVFRVi2bJlPJn+1+1eaTz/9FB999BFWr16Nfv36Ye/evZg9ezZSU1M9Pj8l2iR8i+bFxxNPPIGpU6e63KdXr16W/z979ixGjBiBYcOG4Z133kFLSwv0ej0qKipsjqmoqEBycrISQ9Ys8fHxil6LmTNnYsOGDdi6dSu6du3qdXtaQq1z27VrF86dO4fBgwdbtplMJmzduhWLFy9GY2Mj9Hq9z8bjDn8bLwCkpKTAaDTabLvqqqvw+eefd4r+lebJJ5/E3LlzcffddwMA+vfvj5MnTyI3N9djoaBEmx0B4TleUVGBlJQUy3b7Z767fQcOHKjsQOEH4iMhIQEJCQmi9j1z5gxGjBiBIUOGYMWKFdDpdAgMDMSQIUOQn5+PiRMnAuDfNPLz8zFz5kwFR649lLoWjDE8+uij+OKLL/DDDz8gLS1NphGrj9rnNmrUKBw4cMBm27Rp05Ceno45c+ZobiL3t/ECQE5OTrvw6cOHD6NHjx6don+laWhogE5n616o1+thNps11WZHIC0tDcnJycjPz7cIiNraWuzcuVPSvtOnT1d+sIq7tPqI06dPsz59+rBRo0ax06dPs7KyMsvPmjVrWFBQEHv//fdZcXExe+ihh1h0dDQrLy9Xe9g+R4lrMX36dBYVFcV++OEHm+ve0NAg48jVQYvnpvXoEXu0Pt7CwkIWEBDAXn75ZXbkyBH20UcfsdDQULZq1apO0b/STJkyhXXp0sUSFrtu3ToWHx/PnnrqKU216S/U1dWxPXv2sD179jAA7PXXX2d79uxhJ0+eZIzx4bPR0dFs/fr1bMeOHeyGG25gXbp0cbvv/v372W233UahtlJZsWIFA+DwhzHG3n77bda9e3cWGBjIMjMz2Y4dO1QesXrIfS2cXfcVK1bIM2AV0eK5aX0yt8cfxvvVV1+xjIwMFhQUxNLT09k777zTqfpXktraWjZr1izWvXt3FhwczHr16sWeffZZ1tjYqKk2/YXvv//e4TNpypQpjDE+hPb5559nSUlJzGAwiN43KCiIjRo1iv3yyy8+OQ+OsU6QEo4gCIIgCM3QofN8EARBEAShPUh8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU0h8EARBEAThU/4feYKlPN51aCwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot to Check\n",
    "for r in range(1,3):\n",
    "    ax = plt.axes()    \n",
    "    ax.scatter(s[:, 0], s[:, 1])\n",
    "    ax.scatter(s_prime[r,:,0], s_prime[r,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "n_actions = 4\n",
    "hidden_size = 64\n",
    "latent_dim = 2\n",
    "\n",
    "def init_models():\n",
    "    encoder = nn.Sequential(\n",
    "                                nn.Linear(obs_dim, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size, latent_dim + 1)\n",
    "                        )\n",
    "\n",
    "    transition_model = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim + n_actions, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, latent_dim + 1)\n",
    "                                )\n",
    "\n",
    "    grounding_model = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, obs_dim + 1)\n",
    "                                )\n",
    "    \n",
    "    initiation_classifier = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, n_actions),\n",
    "                                    nn.Sigmoid()\n",
    "                                )\n",
    "    return encoder, transition_model, grounding_model, initiation_classifier\n",
    "\n",
    "encoder, transition_model, grounding_model, initiation_classifier = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training hyperparameters\n",
    "n_epochs = 15\n",
    "minibatch_size = 16\n",
    "n_samples = 10  # samples to approximate expectation\n",
    "learning_rate = 1e-4\n",
    "beta = 0.01 # hyperparameter to control information bottleneck\n",
    "trans_const = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data to float torch tensor and add action one-hot encoding\n",
    "in_data = []\n",
    "out_data = []\n",
    "initiations_s = []\n",
    "initiations_s_prime = []\n",
    "for i in range(n_actions):\n",
    "    data[i] = [torch.from_numpy(_tensor).float() for _tensor in data[i]]\n",
    "    actions = nn.functional.one_hot(torch.ones(data[i][0].size(0)).long() * i, n_actions)\n",
    "    data[i] = (torch.cat((data[i][0], actions), dim=1), data[i][1], data[i][2], data[i][3], data[i][4], data[i][5]) \n",
    "    in_data.append(data[i][0])\n",
    "    out_data.append(data[i][1])\n",
    "    initiations_s.append(data[i][2])\n",
    "    initiations_s_prime.append(data[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = torch.cat(in_data, dim=0)[masks == 1]\n",
    "out_data = torch.cat(out_data, dim=0)[masks == 1]\n",
    "initiations_s = torch.cat(initiations_s, dim=0)[masks==1]\n",
    "initiations_s_prime = torch.cat(initiations_s_prime, dim=0)[masks==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "rand_order = torch.randperm(in_data.size(0))\n",
    "in_data = torch.index_select(in_data, 0, rand_order)\n",
    "out_data = torch.index_select(out_data, 0, rand_order)\n",
    "initiations_s = torch.index_select(initiations_s, 0, rand_order)\n",
    "initiations_s_prime = torch.index_select(initiations_s_prime, 0, rand_order)\n",
    "\n",
    "\n",
    "def minibatches(data_list, batch_size):\n",
    "    N = in_data.size(0)\n",
    "    n_batches = N//batch_size + int(N%batch_size != 0)\n",
    "    for i in range(n_batches):\n",
    "        minibatch = [d[i*batch_size:(i+1)*batch_size] for d in data_list]\n",
    "        # yield (in_data[i*batch_size:(i+1)*batch_size], out_data[i*batch_size:(i+1)*batch_size], initiations[i*batch_size:(i+1)*batch_size])\n",
    "        yield minibatch\n",
    "\n",
    "models = (encoder, transition_model, grounding_model, initiation_classifier)\n",
    "models_params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        models_params.append(param)\n",
    "optimizer_forward = torch.optim.SGD(models_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_loss = nn.BCELoss()\n",
    "epsilon=1e-2\n",
    "def loss(target, predicted_s_prime, predicted_z):\n",
    "    \n",
    "    target = target.unsqueeze(1).unsqueeze(1)\n",
    "    encoding_loss = 0.5 * predicted_z[..., 0:latent_dim].pow(2).sum(dim=-1) + latent_dim * (torch.exp(predicted_z[..., -1]) - predicted_z[..., -1])\n",
    "    prediction_loss = -0.5 * (target - predicted_s_prime[..., 0:obs_dim]).pow(2).sum(dim=-1, keepdim=True)/(torch.exp(2*predicted_s_prime[..., obs_dim:]) + epsilon) - obs_dim * predicted_s_prime[..., obs_dim:]\n",
    "    _loss = -((1-beta)*prediction_loss - beta * encoding_loss)\n",
    "    return _loss\n",
    "\n",
    "def consistency_loss(target, encoded_z_prime, n_samples=10):\n",
    "\n",
    "    noise = torch.normal(0, 1, (target.size(0), n_samples, latent_dim))\n",
    "    z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * noise + encoded_z_prime.unsqueeze(1)[:,:, :-1]  \n",
    "    s_prime_params = grounding_model(z_prime_samples)\n",
    "    _means = s_prime_params[..., :obs_dim]\n",
    "    _vars = torch.exp(2*s_prime_params[..., obs_dim:])\n",
    "    _target_prediction_sim = torch.einsum('...bj,...bk->...b' ,target.unsqueeze(-2), _means)\n",
    "    _mean_norm = torch.einsum('...j,...k->...' , _means, _means)\n",
    "    _target_norm = torch.einsum('...bj,...bk->...b', target, target).unsqueeze(1)\n",
    "    _losses = obs_dim * torch.log(_vars.squeeze()) + (2 * _target_prediction_sim + _mean_norm + _target_norm)/_vars.squeeze() \n",
    "    return _losses\n",
    "\n",
    "def contrained_transition(transition_params, encoded_z_prime): # mean-seeking\n",
    "    epsilon=1\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_t  + ((mean_z.pow(2) + torch.exp(2*log_sigma_z))/(torch.exp(2*log_sigma_t) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon) + torch.einsum('...i, ...j->...', mean_t, mean_t).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "def transition_consistency(transition_params, encoded_z_prime): # mode-seeking\n",
    "    epsilon=1\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_z  + ((mean_t.pow(2) + torch.exp(2*log_sigma_t))/(torch.exp(2*log_sigma_z) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon) + torch.einsum('...i, ...j->...', mean_z, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "\n",
    "def forward_loss(inp, target, predicted_s_prime, predicted_z, predicted_z_prime, s_prime_params):\n",
    "    return loss(target, predicted_s_prime, predicted_z)\n",
    "\n",
    "def training_loop(in_data, out_data, initiations_s, initiations_s_prime, n_epochs, optimizer, loss_fn=forward_loss, print_loss=False):\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for _in, _out, _init_s, _init_s_prime in minibatches((in_data, out_data, initiations_s, initiations_s_prime), minibatch_size):\n",
    "            ## Forward pass\n",
    "            actions = _in[..., obs_dim:].unsqueeze(1).repeat_interleave(n_samples, dim=1)\n",
    "            z = encoder(_in[...,:obs_dim])  # encode\n",
    "            # Predict next abstract state\n",
    "            noise = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_ = torch.exp(z.unsqueeze(1)[:, :, -2:-1]) * noise + z.unsqueeze(1)[:,:, :-1]   \n",
    "            z_prime = transition_model(torch.cat((z_, actions), dim=-1))\n",
    "            \n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_prime_samples = torch.exp(z_prime[:, :, -2:-1]) * epsilon + z_prime[:,:, :-1]  \n",
    "            \n",
    "            encoded_z_prime = encoder(_out)\n",
    "            # z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * epsilon + encoded_z_prime.unsqueeze(1)[:,:, :-1] \n",
    "\n",
    "            # predict next ground state\n",
    "            s_prime = grounding_model(z_prime_samples).unsqueeze(2)\n",
    "            # s_prime = grounding_model(encoded_z_prime)\n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, n_samples, obs_dim))\n",
    "            s_prime_samples = torch.exp(s_prime[..., :obs_dim]) * epsilon + s_prime[..., :obs_dim]\n",
    "            # s_prime = s_prime.squeeze(1)\n",
    "            \n",
    "            # Predict initiation vector from (z, z')\n",
    "            predicted_I_s = initiation_classifier(z_[:, 0])\n",
    "            predicted_I_s_prime = initiation_classifier(z_prime_samples[:, 0])\n",
    "\n",
    "            # initiation_prediction = predicted_I_s.squeeze()#torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            # initiation_target = _init_s#torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            initiation_prediction = torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            initiation_target = torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            \n",
    "            \n",
    "            # binary classifier loss\n",
    "            _classifier_loss = classifier_loss(initiation_prediction, initiation_target)\n",
    "            \n",
    "            _target = _out\n",
    "            # _transition_constraint = contrained_transition(z_prime.detach(), encoded_z_prime).mean()\n",
    "            _transition_constraint = transition_consistency(z_prime, encoded_z_prime.detach()).mean()\n",
    "            _consistency_loss = consistency_loss(_out, encoded_z_prime).mean()\n",
    "            _encoding_loss = loss_fn(_in, _target, s_prime, z, z_prime_samples, s_prime).mean()\n",
    "\n",
    "            \n",
    "\n",
    "            _loss = (1-trans_const) * nn.functional.gelu(10*_encoding_loss) + nn.functional.relu(0 * _consistency_loss) + 100 * _classifier_loss + trans_const * nn.functional.leaky_relu(1 * _transition_constraint, negative_slope=0.001)\n",
    "            # _loss = _encoding_loss + _consistency_loss + _transition_constraint\n",
    "\n",
    "            \n",
    "            if print_loss:\n",
    "                print(f\"{i}: Encoding Loss {_encoding_loss}, Transition Loss {_transition_constraint}, Classifier Loss {_classifier_loss}, Total Loss {_loss}\")\n",
    "            \n",
    "            ### zero grads \n",
    "            optimizer.zero_grad()\n",
    "            ### backward pass\n",
    "            _loss.backward()\n",
    "            \n",
    "            ### update\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Encoding Loss 170.4368438720703, Transition Loss -0.672415018081665, Classifier Loss 0.6898467540740967, Total Loss 1091.60546875\n",
      "0: Encoding Loss 52.29717254638672, Transition Loss -0.20313894748687744, Classifier Loss 0.6762740612030029, Total Loss 381.4103698730469\n",
      "0: Encoding Loss 45.53602981567383, Transition Loss 0.08746522665023804, Classifier Loss 0.6902531385421753, Total Loss 342.2764892578125\n",
      "0: Encoding Loss 32.45750427246094, Transition Loss -0.29271161556243896, Classifier Loss 0.690716028213501, Total Loss 263.8165283203125\n",
      "0: Encoding Loss 23.39028549194336, Transition Loss -0.5139102935791016, Classifier Loss 0.6764278411865234, Total Loss 207.9842987060547\n",
      "0: Encoding Loss 30.49777603149414, Transition Loss 0.27832579612731934, Classifier Loss 0.6884254217147827, Total Loss 251.94053649902344\n",
      "0: Encoding Loss 23.416515350341797, Transition Loss 0.031660765409469604, Classifier Loss 0.6803246140480042, Total Loss 208.54421997070312\n",
      "0: Encoding Loss 21.898540496826172, Transition Loss -0.2760879397392273, Classifier Loss 0.6885796785354614, Total Loss 200.24911499023438\n",
      "0: Encoding Loss 24.732078552246094, Transition Loss 0.08433954417705536, Classifier Loss 0.6772291660308838, Total Loss 216.1491241455078\n",
      "0: Encoding Loss 23.867733001708984, Transition Loss -0.022140279412269592, Classifier Loss 0.6768845915794373, Total Loss 210.8948516845703\n",
      "0: Encoding Loss 20.463035583496094, Transition Loss -0.22503390908241272, Classifier Loss 0.6751980781555176, Total Loss 190.29794311523438\n",
      "0: Encoding Loss 22.546459197998047, Transition Loss -0.2797645628452301, Classifier Loss 0.6737440824508667, Total Loss 202.65306091308594\n",
      "0: Encoding Loss 25.056325912475586, Transition Loss -0.07834573090076447, Classifier Loss 0.6825324892997742, Total Loss 218.5911865234375\n",
      "0: Encoding Loss 23.692668914794922, Transition Loss 0.13720709085464478, Classifier Loss 0.6755378246307373, Total Loss 209.7646942138672\n",
      "0: Encoding Loss 20.319744110107422, Transition Loss 0.016304925084114075, Classifier Loss 0.6900139451026917, Total Loss 190.92637634277344\n",
      "0: Encoding Loss 22.976314544677734, Transition Loss 0.13162508606910706, Classifier Loss 0.6787199974060059, Total Loss 205.78253173828125\n",
      "0: Encoding Loss 21.681270599365234, Transition Loss -0.03111150860786438, Classifier Loss 0.6791219711303711, Total Loss 197.99981689453125\n",
      "0: Encoding Loss 24.424732208251953, Transition Loss -0.4011090099811554, Classifier Loss 0.6680281162261963, Total Loss 213.35104370117188\n",
      "0: Encoding Loss 21.626235961914062, Transition Loss 0.24071666598320007, Classifier Loss 0.6683939695358276, Total Loss 196.69308471679688\n",
      "0: Encoding Loss 20.0994930267334, Transition Loss -0.2670864462852478, Classifier Loss 0.6750994324684143, Total Loss 188.10679626464844\n",
      "0: Encoding Loss 21.66259002685547, Transition Loss 0.3163447380065918, Classifier Loss 0.6626638174057007, Total Loss 196.3684539794922\n",
      "0: Encoding Loss 21.02065658569336, Transition Loss -0.3472593426704407, Classifier Loss 0.6727390289306641, Total Loss 193.39772033691406\n",
      "0: Encoding Loss 21.356966018676758, Transition Loss -0.35743600130081177, Classifier Loss 0.6483610272407532, Total Loss 192.97776794433594\n",
      "0: Encoding Loss 23.493988037109375, Transition Loss -0.1797899603843689, Classifier Loss 0.6717516183853149, Total Loss 208.13902282714844\n",
      "0: Encoding Loss 20.622549057006836, Transition Loss -0.4745814800262451, Classifier Loss 0.6644626259803772, Total Loss 190.18138122558594\n",
      "0: Encoding Loss 19.138530731201172, Transition Loss -0.07621712982654572, Classifier Loss 0.6690373420715332, Total Loss 181.73489379882812\n",
      "0: Encoding Loss 20.567476272583008, Transition Loss -0.3616684079170227, Classifier Loss 0.6657133102416992, Total Loss 189.97605895996094\n",
      "0: Encoding Loss 21.983610153198242, Transition Loss 0.24095436930656433, Classifier Loss 0.6793349981307983, Total Loss 199.93154907226562\n",
      "0: Encoding Loss 22.12362289428711, Transition Loss 0.24147216975688934, Classifier Loss 0.6678422093391418, Total Loss 199.62255859375\n",
      "0: Encoding Loss 21.384138107299805, Transition Loss -0.10326587408781052, Classifier Loss 0.6686224937438965, Total Loss 195.1670379638672\n",
      "0: Encoding Loss 20.301448822021484, Transition Loss 0.037953365594148636, Classifier Loss 0.6722334623336792, Total Loss 189.04722595214844\n",
      "0: Encoding Loss 21.149555206298828, Transition Loss -0.4572608470916748, Classifier Loss 0.6602303385734558, Total Loss 192.92018127441406\n",
      "0: Encoding Loss 20.648794174194336, Transition Loss -0.08972334116697311, Classifier Loss 0.6641241312026978, Total Loss 190.30514526367188\n",
      "0: Encoding Loss 22.45394515991211, Transition Loss 0.25553280115127563, Classifier Loss 0.6707409620285034, Total Loss 201.89999389648438\n",
      "0: Encoding Loss 21.891246795654297, Transition Loss 0.17655205726623535, Classifier Loss 0.6611917018890381, Total Loss 197.5372772216797\n",
      "0: Encoding Loss 22.080852508544922, Transition Loss 0.01833893358707428, Classifier Loss 0.6628708839416504, Total Loss 198.77955627441406\n",
      "0: Encoding Loss 22.57069969177246, Transition Loss 0.18706801533699036, Classifier Loss 0.6652242541313171, Total Loss 202.02145385742188\n",
      "0: Encoding Loss 21.059978485107422, Transition Loss -0.4173533320426941, Classifier Loss 0.6557732224464417, Total Loss 191.93702697753906\n",
      "0: Encoding Loss 19.469921112060547, Transition Loss -0.28456559777259827, Classifier Loss 0.6558637022972107, Total Loss 182.4058074951172\n",
      "0: Encoding Loss 21.651309967041016, Transition Loss 0.007902726531028748, Classifier Loss 0.6494893431663513, Total Loss 194.85997009277344\n",
      "0: Encoding Loss 20.8536434173584, Transition Loss -0.16491544246673584, Classifier Loss 0.6504451632499695, Total Loss 190.16632080078125\n",
      "0: Encoding Loss 20.315460205078125, Transition Loss -0.32373571395874023, Classifier Loss 0.6526898145675659, Total Loss 187.16162109375\n",
      "0: Encoding Loss 22.257465362548828, Transition Loss 0.018888041377067566, Classifier Loss 0.6498405933380127, Total Loss 198.5364227294922\n",
      "0: Encoding Loss 21.88796615600586, Transition Loss 0.3145890533924103, Classifier Loss 0.6611201763153076, Total Loss 197.56565856933594\n",
      "0: Encoding Loss 21.283580780029297, Transition Loss 0.0971795842051506, Classifier Loss 0.6516076326370239, Total Loss 192.90113830566406\n",
      "0: Encoding Loss 22.15128517150879, Transition Loss -0.0003465712070465088, Classifier Loss 0.6465875506401062, Total Loss 197.56646728515625\n",
      "0: Encoding Loss 21.079914093017578, Transition Loss -0.06696218997240067, Classifier Loss 0.6595043540000916, Total Loss 192.4298858642578\n",
      "0: Encoding Loss 20.306468963623047, Transition Loss -0.3546316921710968, Classifier Loss 0.6456429958343506, Total Loss 186.40298461914062\n",
      "0: Encoding Loss 21.32293701171875, Transition Loss -0.09495723247528076, Classifier Loss 0.6452342867851257, Total Loss 192.46102905273438\n",
      "0: Encoding Loss 20.236373901367188, Transition Loss -0.26591867208480835, Classifier Loss 0.6515688300132751, Total Loss 186.5750274658203\n",
      "0: Encoding Loss 21.06612205505371, Transition Loss 0.39131397008895874, Classifier Loss 0.6471712589263916, Total Loss 191.2703857421875\n",
      "0: Encoding Loss 20.217761993408203, Transition Loss -0.0595443919301033, Classifier Loss 0.6492509245872498, Total Loss 186.23162841796875\n",
      "0: Encoding Loss 23.228267669677734, Transition Loss 0.3310316503047943, Classifier Loss 0.6444393992424011, Total Loss 203.9459686279297\n",
      "0: Encoding Loss 21.23217010498047, Transition Loss -0.31027063727378845, Classifier Loss 0.6416283249855042, Total Loss 191.5557403564453\n",
      "0: Encoding Loss 23.32233428955078, Transition Loss 0.5409911274909973, Classifier Loss 0.633441150188446, Total Loss 203.49452209472656\n",
      "0: Encoding Loss 21.490093231201172, Transition Loss -0.04456900805234909, Classifier Loss 0.6418494582176208, Total Loss 193.12550354003906\n",
      "0: Encoding Loss 21.34238624572754, Transition Loss 0.07012094557285309, Classifier Loss 0.6371925473213196, Total Loss 191.80162048339844\n",
      "0: Encoding Loss 22.92355728149414, Transition Loss 0.3432353436946869, Classifier Loss 0.6306135654449463, Total Loss 200.74000549316406\n",
      "0: Encoding Loss 21.923051834106445, Transition Loss 0.5433008670806885, Classifier Loss 0.6373462677001953, Total Loss 195.49026489257812\n",
      "0: Encoding Loss 20.221633911132812, Transition Loss 0.1100366935133934, Classifier Loss 0.6458339691162109, Total Loss 185.9572296142578\n",
      "0: Encoding Loss 20.005796432495117, Transition Loss -0.12548238039016724, Classifier Loss 0.6522759199142456, Total Loss 185.2623291015625\n",
      "0: Encoding Loss 22.461177825927734, Transition Loss 0.7116508483886719, Classifier Loss 0.6360893249511719, Total Loss 198.66067504882812\n",
      "0: Encoding Loss 22.374088287353516, Transition Loss 0.730970025062561, Classifier Loss 0.6283504962921143, Total Loss 197.37197875976562\n",
      "0: Encoding Loss 21.48665428161621, Transition Loss 1.0199531316757202, Classifier Loss 0.6493326425552368, Total Loss 194.2611846923828\n",
      "0: Encoding Loss 21.084341049194336, Transition Loss 0.09811661392450333, Classifier Loss 0.6306934952735901, Total Loss 189.61465454101562\n",
      "0: Encoding Loss 20.832279205322266, Transition Loss 0.101499542593956, Classifier Loss 0.6330819725990295, Total Loss 188.3424835205078\n",
      "0: Encoding Loss 21.54966163635254, Transition Loss 0.4867480993270874, Classifier Loss 0.6459354758262634, Total Loss 194.08621215820312\n",
      "0: Encoding Loss 20.362873077392578, Transition Loss 0.4278603196144104, Classifier Loss 0.6404101848602295, Total Loss 186.389404296875\n",
      "0: Encoding Loss 21.85323715209961, Transition Loss 0.15862351655960083, Classifier Loss 0.6234528422355652, Total Loss 193.52816772460938\n",
      "0: Encoding Loss 22.05253791809082, Transition Loss -0.0024514421820640564, Classifier Loss 0.6345051527023315, Total Loss 195.7657470703125\n",
      "0: Encoding Loss 21.265026092529297, Transition Loss 0.2929342985153198, Classifier Loss 0.631883978843689, Total Loss 190.89573669433594\n",
      "0: Encoding Loss 21.68160057067871, Transition Loss 0.49923890829086304, Classifier Loss 0.6280500888824463, Total Loss 193.0943145751953\n",
      "0: Encoding Loss 22.128841400146484, Transition Loss 1.0625133514404297, Classifier Loss 0.6383969783782959, Total Loss 197.0377655029297\n",
      "0: Encoding Loss 21.325942993164062, Transition Loss 0.14584486186504364, Classifier Loss 0.625904381275177, Total Loss 190.6044464111328\n",
      "0: Encoding Loss 19.943134307861328, Transition Loss 0.14056465029716492, Classifier Loss 0.6249933242797852, Total Loss 182.21437072753906\n",
      "0: Encoding Loss 20.597082138061523, Transition Loss 0.15864023566246033, Classifier Loss 0.622340202331543, Total Loss 185.87997436523438\n",
      "0: Encoding Loss 21.74503517150879, Transition Loss 0.9689873456954956, Classifier Loss 0.6372562646865845, Total Loss 194.58343505859375\n",
      "0: Encoding Loss 21.52954864501953, Transition Loss 0.43480250239372253, Classifier Loss 0.6359655261039734, Total Loss 192.94776916503906\n",
      "0: Encoding Loss 21.481426239013672, Transition Loss 0.6573562622070312, Classifier Loss 0.6316179037094116, Total Loss 192.31329345703125\n",
      "0: Encoding Loss 23.431209564208984, Transition Loss 1.0516616106033325, Classifier Loss 0.6224139332771301, Total Loss 203.24932861328125\n",
      "0: Encoding Loss 21.815698623657227, Transition Loss 0.5531193614006042, Classifier Loss 0.6165921688079834, Total Loss 192.774658203125\n",
      "0: Encoding Loss 20.763904571533203, Transition Loss 0.06066377833485603, Classifier Loss 0.6190755367279053, Total Loss 186.51524353027344\n",
      "0: Encoding Loss 20.42007064819336, Transition Loss -0.09164214134216309, Classifier Loss 0.6263930797576904, Total Loss 185.1597137451172\n",
      "0: Encoding Loss 21.622777938842773, Transition Loss 0.6443963646888733, Classifier Loss 0.6241400837898254, Total Loss 192.408447265625\n",
      "0: Encoding Loss 21.40766143798828, Transition Loss 0.7124530076980591, Classifier Loss 0.6058297753334045, Total Loss 189.31393432617188\n",
      "0: Encoding Loss 20.54880142211914, Transition Loss 0.5343207716941833, Classifier Loss 0.6219239234924316, Total Loss 185.6989288330078\n",
      "0: Encoding Loss 21.72751235961914, Transition Loss 0.03724835067987442, Classifier Loss 0.6173461675643921, Total Loss 192.11459350585938\n",
      "0: Encoding Loss 22.37698745727539, Transition Loss 1.120782494544983, Classifier Loss 0.6165428161621094, Total Loss 196.36453247070312\n",
      "0: Encoding Loss 21.40251350402832, Transition Loss 0.9560712575912476, Classifier Loss 0.611770749092102, Total Loss 189.97459411621094\n",
      "0: Encoding Loss 20.47769546508789, Transition Loss 0.2555760145187378, Classifier Loss 0.6196200847625732, Total Loss 184.930419921875\n",
      "0: Encoding Loss 21.212505340576172, Transition Loss 0.06617915630340576, Classifier Loss 0.6121087074279785, Total Loss 188.5123748779297\n",
      "0: Encoding Loss 21.739261627197266, Transition Loss 0.7117133140563965, Classifier Loss 0.6189987063407898, Total Loss 192.62013244628906\n",
      "0: Encoding Loss 23.059490203857422, Transition Loss 1.354874610900879, Classifier Loss 0.626078188419342, Total Loss 201.5067138671875\n",
      "0: Encoding Loss 20.927711486816406, Transition Loss -0.061336949467659, Classifier Loss 0.6142472624778748, Total Loss 186.990966796875\n",
      "0: Encoding Loss 22.090051651000977, Transition Loss 1.1571370363235474, Classifier Loss 0.6268258690834045, Total Loss 195.68576049804688\n",
      "0: Encoding Loss 20.50190544128418, Transition Loss 0.4359458088874817, Classifier Loss 0.6016061902046204, Total Loss 183.346435546875\n",
      "0: Encoding Loss 22.674640655517578, Transition Loss 1.0791047811508179, Classifier Loss 0.6207939386367798, Total Loss 198.5588836669922\n",
      "0: Encoding Loss 22.272871017456055, Transition Loss 0.7588440179824829, Classifier Loss 0.6121090650558472, Total Loss 195.1516876220703\n",
      "0: Encoding Loss 20.303058624267578, Transition Loss 0.3628028631210327, Classifier Loss 0.6089458465576172, Total Loss 182.85806274414062\n",
      "0: Encoding Loss 20.870624542236328, Transition Loss 0.3920137882232666, Classifier Loss 0.5983286499977112, Total Loss 185.21340942382812\n",
      "0: Encoding Loss 22.21851348876953, Transition Loss 0.9437187910079956, Classifier Loss 0.6109309792518616, Total Loss 194.7816619873047\n",
      "0: Encoding Loss 20.38355827331543, Transition Loss 0.7877317667007446, Classifier Loss 0.6068329811096191, Total Loss 183.29974365234375\n",
      "0: Encoding Loss 22.108858108520508, Transition Loss 1.0663257837295532, Classifier Loss 0.6186758279800415, Total Loss 194.947265625\n",
      "0: Encoding Loss 20.155851364135742, Transition Loss 0.467840313911438, Classifier Loss 0.6132833957672119, Total Loss 182.45059204101562\n",
      "0: Encoding Loss 21.998910903930664, Transition Loss 0.3955952525138855, Classifier Loss 0.5963502526283264, Total Loss 191.78672790527344\n",
      "0: Encoding Loss 22.065933227539062, Transition Loss 0.5022247433662415, Classifier Loss 0.610873818397522, Total Loss 193.6838836669922\n",
      "0: Encoding Loss 23.287487030029297, Transition Loss 1.4074866771697998, Classifier Loss 0.5934091806411743, Total Loss 199.62884521484375\n",
      "0: Encoding Loss 22.2142276763916, Transition Loss 1.822991967201233, Classifier Loss 0.6151393055915833, Total Loss 195.52850341796875\n",
      "0: Encoding Loss 21.204275131225586, Transition Loss 1.1318182945251465, Classifier Loss 0.59592205286026, Total Loss 187.2705841064453\n",
      "0: Encoding Loss 20.04339027404785, Transition Loss 0.20828548073768616, Classifier Loss 0.5862354636192322, Total Loss 178.9672088623047\n",
      "0: Encoding Loss 22.57280158996582, Transition Loss 1.6730986833572388, Classifier Loss 0.6196515560150146, Total Loss 198.0712127685547\n",
      "0: Encoding Loss 22.571697235107422, Transition Loss 1.2310618162155151, Classifier Loss 0.5936078429222107, Total Loss 195.2834014892578\n",
      "0: Encoding Loss 20.225650787353516, Transition Loss 0.8074793815612793, Classifier Loss 0.6058327555656433, Total Loss 182.2601776123047\n",
      "0: Encoding Loss 21.64932632446289, Transition Loss 0.9263615608215332, Classifier Loss 0.606593132019043, Total Loss 190.9258270263672\n",
      "0: Encoding Loss 21.441389083862305, Transition Loss 1.2837260961532593, Classifier Loss 0.6010456085205078, Total Loss 189.26638793945312\n",
      "0: Encoding Loss 21.46379852294922, Transition Loss 1.4176725149154663, Classifier Loss 0.5916971564292908, Total Loss 188.5195770263672\n",
      "0: Encoding Loss 20.393112182617188, Transition Loss 0.23676061630249023, Classifier Loss 0.5950279831886292, Total Loss 181.95619201660156\n",
      "0: Encoding Loss 20.680782318115234, Transition Loss 0.5457186698913574, Classifier Loss 0.5882250070571899, Total Loss 183.12548828125\n",
      "0: Encoding Loss 23.032987594604492, Transition Loss 1.8479502201080322, Classifier Loss 0.6049373149871826, Total Loss 199.43084716796875\n",
      "0: Encoding Loss 21.435100555419922, Transition Loss 0.7780972123146057, Classifier Loss 0.5882102251052856, Total Loss 187.7428741455078\n",
      "0: Encoding Loss 21.87394142150879, Transition Loss 1.1700090169906616, Classifier Loss 0.5894222259521484, Total Loss 190.6538848876953\n",
      "0: Encoding Loss 21.782379150390625, Transition Loss 1.1760045289993286, Classifier Loss 0.5911583304405212, Total Loss 190.28050231933594\n",
      "0: Encoding Loss 21.806461334228516, Transition Loss 1.1995211839675903, Classifier Loss 0.5836374759674072, Total Loss 189.6823272705078\n",
      "0: Encoding Loss 21.919815063476562, Transition Loss 1.3439443111419678, Classifier Loss 0.5996062755584717, Total Loss 192.01710510253906\n",
      "0: Encoding Loss 20.9128475189209, Transition Loss 0.6902619004249573, Classifier Loss 0.5785189270973206, Total Loss 183.60508728027344\n",
      "0: Encoding Loss 22.27020835876465, Transition Loss 1.6671006679534912, Classifier Loss 0.5862584710121155, Total Loss 192.9139404296875\n",
      "0: Encoding Loss 20.918869018554688, Transition Loss 1.0287752151489258, Classifier Loss 0.5869532823562622, Total Loss 184.6200714111328\n",
      "0: Encoding Loss 22.26036262512207, Transition Loss 2.114470958709717, Classifier Loss 0.587433934211731, Total Loss 193.1513671875\n",
      "0: Encoding Loss 20.306528091430664, Transition Loss 0.2433566451072693, Classifier Loss 0.5737723708152771, Total Loss 179.31373596191406\n",
      "0: Encoding Loss 20.71059226989746, Transition Loss 1.188103199005127, Classifier Loss 0.5862877368927002, Total Loss 183.36756896972656\n",
      "0: Encoding Loss 22.800552368164062, Transition Loss 1.8217103481292725, Classifier Loss 0.5849160552024841, Total Loss 196.0236053466797\n",
      "0: Encoding Loss 20.7872314453125, Transition Loss 0.6634119749069214, Classifier Loss 0.5880405902862549, Total Loss 183.79281616210938\n",
      "0: Encoding Loss 20.681007385253906, Transition Loss 0.5709495544433594, Classifier Loss 0.5961352586746216, Total Loss 183.92796325683594\n",
      "0: Encoding Loss 21.60777473449707, Transition Loss 1.2020699977874756, Classifier Loss 0.578022837638855, Total Loss 187.92977905273438\n",
      "0: Encoding Loss 22.132009506225586, Transition Loss 1.8435044288635254, Classifier Loss 0.5889610052108765, Total Loss 192.42556762695312\n",
      "0: Encoding Loss 21.7425479888916, Transition Loss 1.1439342498779297, Classifier Loss 0.5933658480644226, Total Loss 190.24945068359375\n",
      "0: Encoding Loss 20.180662155151367, Transition Loss -0.10976897925138474, Classifier Loss 0.5725234150886536, Total Loss 178.33627319335938\n",
      "0: Encoding Loss 20.420501708984375, Transition Loss 0.5632369518280029, Classifier Loss 0.5853201150894165, Total Loss 181.2803192138672\n",
      "0: Encoding Loss 20.74095916748047, Transition Loss 1.0597972869873047, Classifier Loss 0.5796531438827515, Total Loss 182.83499145507812\n",
      "0: Encoding Loss 23.02547836303711, Transition Loss 1.1025217771530151, Classifier Loss 0.5651199817657471, Total Loss 195.1058807373047\n",
      "0: Encoding Loss 21.425670623779297, Transition Loss 1.4757851362228394, Classifier Loss 0.5814015865325928, Total Loss 187.2845001220703\n",
      "0: Encoding Loss 21.05856704711914, Transition Loss 0.7915440797805786, Classifier Loss 0.6011418104171753, Total Loss 186.78219604492188\n",
      "0: Encoding Loss 21.130739212036133, Transition Loss 0.7931330800056458, Classifier Loss 0.5817126631736755, Total Loss 185.27296447753906\n",
      "0: Encoding Loss 21.426925659179688, Transition Loss 0.6654682755470276, Classifier Loss 0.5786204934120178, Total Loss 186.68978881835938\n",
      "0: Encoding Loss 19.303524017333984, Transition Loss 0.2967439889907837, Classifier Loss 0.5772462487220764, Total Loss 173.6644744873047\n",
      "0: Encoding Loss 21.192279815673828, Transition Loss 1.19070565700531, Classifier Loss 0.5759404897689819, Total Loss 185.22401428222656\n",
      "0: Encoding Loss 21.89376449584961, Transition Loss 1.9060534238815308, Classifier Loss 0.5945636034011841, Total Loss 191.5813751220703\n",
      "0: Encoding Loss 21.217817306518555, Transition Loss 0.9595840573310852, Classifier Loss 0.5834932923316956, Total Loss 186.04006958007812\n",
      "0: Encoding Loss 19.225343704223633, Transition Loss 0.3801836371421814, Classifier Loss 0.5917609930038452, Total Loss 174.68023681640625\n",
      "0: Encoding Loss 21.13754653930664, Transition Loss 1.3268029689788818, Classifier Loss 0.5832977890968323, Total Loss 185.68577575683594\n",
      "0: Encoding Loss 22.217914581298828, Transition Loss 2.252497911453247, Classifier Loss 0.594305157661438, Total Loss 193.63900756835938\n",
      "0: Encoding Loss 22.71017837524414, Transition Loss 1.9415427446365356, Classifier Loss 0.5898328423500061, Total Loss 196.02096557617188\n",
      "0: Encoding Loss 20.46885108947754, Transition Loss 1.2017488479614258, Classifier Loss 0.5679762363433838, Total Loss 180.0914306640625\n",
      "0: Encoding Loss 20.5487060546875, Transition Loss 1.5258711576461792, Classifier Loss 0.5841199159622192, Total Loss 182.31459045410156\n",
      "0: Encoding Loss 20.946218490600586, Transition Loss 1.9784337282180786, Classifier Loss 0.5859995484352112, Total Loss 185.06863403320312\n",
      "0: Encoding Loss 21.18008804321289, Transition Loss 1.8073948621749878, Classifier Loss 0.5698693990707397, Total Loss 184.79043579101562\n",
      "0: Encoding Loss 21.070995330810547, Transition Loss 1.1995636224746704, Classifier Loss 0.5868387222290039, Total Loss 185.5896759033203\n",
      "0: Encoding Loss 21.105182647705078, Transition Loss 1.631296157836914, Classifier Loss 0.584840714931488, Total Loss 185.76768493652344\n",
      "0: Encoding Loss 20.46276092529297, Transition Loss 0.6758880019187927, Classifier Loss 0.5660982728004456, Total Loss 179.65675354003906\n",
      "0: Encoding Loss 21.771282196044922, Transition Loss 1.657631278038025, Classifier Loss 0.564801037311554, Total Loss 187.77085876464844\n",
      "0: Encoding Loss 21.028560638427734, Transition Loss 0.6918825507164001, Classifier Loss 0.5720465779304504, Total Loss 183.6527862548828\n",
      "0: Encoding Loss 22.486671447753906, Transition Loss 1.530909776687622, Classifier Loss 0.5670191049575806, Total Loss 192.23431396484375\n",
      "0: Encoding Loss 20.26113510131836, Transition Loss 0.8097741007804871, Classifier Loss 0.5782778263092041, Total Loss 179.718505859375\n",
      "0: Encoding Loss 21.789562225341797, Transition Loss 1.6503709554672241, Classifier Loss 0.5740644335746765, Total Loss 188.80397033691406\n",
      "0: Encoding Loss 20.42365264892578, Transition Loss 0.4230612814426422, Classifier Loss 0.5820320844650269, Total Loss 180.9143524169922\n",
      "0: Encoding Loss 22.40773582458496, Transition Loss 1.8606966733932495, Classifier Loss 0.5882456302642822, Total Loss 194.01527404785156\n",
      "0: Encoding Loss 21.11802864074707, Transition Loss 2.367032051086426, Classifier Loss 0.5979194641113281, Total Loss 187.44692993164062\n",
      "0: Encoding Loss 22.073665618896484, Transition Loss 1.889082670211792, Classifier Loss 0.5710670948028564, Total Loss 190.30433654785156\n",
      "0: Encoding Loss 22.681880950927734, Transition Loss 1.9358830451965332, Classifier Loss 0.5643577575683594, Total Loss 193.30142211914062\n",
      "0: Encoding Loss 22.11574363708496, Transition Loss 1.8266115188598633, Classifier Loss 0.5881679058074951, Total Loss 192.24191284179688\n",
      "0: Encoding Loss 21.558961868286133, Transition Loss 1.6883625984191895, Classifier Loss 0.5724905729293823, Total Loss 187.27816772460938\n",
      "0: Encoding Loss 19.9810791015625, Transition Loss 1.3041481971740723, Classifier Loss 0.569588840007782, Total Loss 177.3670196533203\n",
      "0: Encoding Loss 22.837085723876953, Transition Loss 2.2475481033325195, Classifier Loss 0.5760875344276428, Total Loss 195.53028869628906\n",
      "0: Encoding Loss 19.86474609375, Transition Loss 0.566673219203949, Classifier Loss 0.5614683032035828, Total Loss 175.56198120117188\n",
      "0: Encoding Loss 21.229299545288086, Transition Loss 1.161604881286621, Classifier Loss 0.569571852684021, Total Loss 184.79762268066406\n",
      "0: Encoding Loss 20.947219848632812, Transition Loss 0.6370894908905029, Classifier Loss 0.5667303800582886, Total Loss 182.6112060546875\n",
      "0: Encoding Loss 21.87017822265625, Transition Loss 1.5630102157592773, Classifier Loss 0.5651825666427612, Total Loss 188.36451721191406\n",
      "0: Encoding Loss 19.95064926147461, Transition Loss 0.9212170243263245, Classifier Loss 0.5571069121360779, Total Loss 175.7830810546875\n",
      "0: Encoding Loss 21.760700225830078, Transition Loss 1.9076564311981201, Classifier Loss 0.577620267868042, Total Loss 189.08929443359375\n",
      "0: Encoding Loss 20.852554321289062, Transition Loss 0.2110825479030609, Classifier Loss 0.5686098337173462, Total Loss 182.0607452392578\n",
      "0: Encoding Loss 21.227718353271484, Transition Loss 1.2129672765731812, Classifier Loss 0.5709493160247803, Total Loss 184.94642639160156\n",
      "0: Encoding Loss 20.62460708618164, Transition Loss 0.3916136920452118, Classifier Loss 0.5546263456344604, Total Loss 179.36692810058594\n",
      "0: Encoding Loss 19.819549560546875, Transition Loss 0.5080790519714355, Classifier Loss 0.5637766718864441, Total Loss 175.49819946289062\n",
      "0: Encoding Loss 21.32429313659668, Transition Loss 1.2910913228988647, Classifier Loss 0.5713489055633545, Total Loss 185.5970916748047\n",
      "0: Encoding Loss 21.85247230529785, Transition Loss 1.7959604263305664, Classifier Loss 0.5754011273384094, Total Loss 189.3733367919922\n",
      "0: Encoding Loss 21.194133758544922, Transition Loss 1.5429223775863647, Classifier Loss 0.5686135292053223, Total Loss 184.64332580566406\n",
      "0: Encoding Loss 22.813016891479492, Transition Loss 1.492539882659912, Classifier Loss 0.5795811414718628, Total Loss 195.43324279785156\n",
      "0: Encoding Loss 20.902286529541016, Transition Loss 1.2172690629959106, Classifier Loss 0.5783631801605225, Total Loss 183.7369384765625\n",
      "0: Encoding Loss 21.25902557373047, Transition Loss 1.5093920230865479, Classifier Loss 0.5551191568374634, Total Loss 183.66983032226562\n",
      "0: Encoding Loss 19.716861724853516, Transition Loss 0.7913568019866943, Classifier Loss 0.5798685550689697, Total Loss 176.6045684814453\n",
      "0: Encoding Loss 22.137649536132812, Transition Loss 1.758837342262268, Classifier Loss 0.5621539950370789, Total Loss 189.7448272705078\n",
      "0: Encoding Loss 21.032636642456055, Transition Loss 0.7561247944831848, Classifier Loss 0.5612358450889587, Total Loss 182.62185668945312\n",
      "0: Encoding Loss 20.742725372314453, Transition Loss 0.6055169701576233, Classifier Loss 0.5620787739753723, Total Loss 180.90643310546875\n",
      "0: Encoding Loss 20.575782775878906, Transition Loss 1.0319825410842896, Classifier Loss 0.5710196495056152, Total Loss 180.96946716308594\n",
      "0: Encoding Loss 20.611215591430664, Transition Loss 0.7913962006568909, Classifier Loss 0.5655251145362854, Total Loss 180.536376953125\n",
      "0: Encoding Loss 21.672954559326172, Transition Loss 1.5773942470550537, Classifier Loss 0.5671178102493286, Total Loss 187.38046264648438\n",
      "0: Encoding Loss 20.67512321472168, Transition Loss 0.15873944759368896, Classifier Loss 0.557056725025177, Total Loss 179.8199005126953\n",
      "0: Encoding Loss 21.844533920288086, Transition Loss 1.5840613842010498, Classifier Loss 0.5746307969093323, Total Loss 189.16392517089844\n",
      "0: Encoding Loss 20.090824127197266, Transition Loss 0.7669714093208313, Classifier Loss 0.5439401865005493, Total Loss 175.24575805664062\n",
      "0: Encoding Loss 21.394065856933594, Transition Loss 1.4515410661697388, Classifier Loss 0.5804402828216553, Total Loss 186.98902893066406\n",
      "0: Encoding Loss 20.715906143188477, Transition Loss 1.3265434503555298, Classifier Loss 0.551438570022583, Total Loss 179.9699249267578\n",
      "0: Encoding Loss 20.65961265563965, Transition Loss 0.5878983736038208, Classifier Loss 0.5534358620643616, Total Loss 179.5364227294922\n",
      "0: Encoding Loss 22.83913230895996, Transition Loss 2.3113861083984375, Classifier Loss 0.5708250999450684, Total Loss 195.0418701171875\n",
      "0: Encoding Loss 21.32699966430664, Transition Loss 0.8035252094268799, Classifier Loss 0.5685311555862427, Total Loss 185.1365203857422\n",
      "0: Encoding Loss 19.702198028564453, Transition Loss 0.3845236897468567, Classifier Loss 0.5608359575271606, Total Loss 174.45059204101562\n",
      "0: Encoding Loss 21.129430770874023, Transition Loss 1.216965675354004, Classifier Loss 0.5758135914802551, Total Loss 184.8447265625\n",
      "0: Encoding Loss 22.461835861206055, Transition Loss 1.922033667564392, Classifier Loss 0.571441650390625, Total Loss 192.6840057373047\n",
      "0: Encoding Loss 21.86233901977539, Transition Loss 1.3240361213684082, Classifier Loss 0.5521846413612366, Total Loss 186.922119140625\n",
      "0: Encoding Loss 22.073820114135742, Transition Loss 1.77456533908844, Classifier Loss 0.5626016855239868, Total Loss 189.4129180908203\n",
      "0: Encoding Loss 23.0300350189209, Transition Loss 1.6840579509735107, Classifier Loss 0.5537716150283813, Total Loss 194.23101806640625\n",
      "0: Encoding Loss 22.047704696655273, Transition Loss 1.4679713249206543, Classifier Loss 0.5720362067222595, Total Loss 190.07705688476562\n",
      "0: Encoding Loss 21.173084259033203, Transition Loss 1.0157809257507324, Classifier Loss 0.5714833736419678, Total Loss 184.59315490722656\n",
      "0: Encoding Loss 21.39984703063965, Transition Loss 2.2004506587982178, Classifier Loss 0.5777335166931152, Total Loss 187.05262756347656\n",
      "0: Encoding Loss 22.235485076904297, Transition Loss 1.1269636154174805, Classifier Loss 0.5782483220100403, Total Loss 191.68853759765625\n",
      "0: Encoding Loss 20.74391746520996, Transition Loss 0.803055465221405, Classifier Loss 0.5505344271659851, Total Loss 179.8381805419922\n",
      "0: Encoding Loss 22.087444305419922, Transition Loss 2.1079628467559814, Classifier Loss 0.5590211153030396, Total Loss 189.2699737548828\n",
      "0: Encoding Loss 22.932668685913086, Transition Loss 2.339968681335449, Classifier Loss 0.5776592493057251, Total Loss 196.29794311523438\n",
      "0: Encoding Loss 21.950307846069336, Transition Loss 1.613234043121338, Classifier Loss 0.5664888620376587, Total Loss 188.99603271484375\n",
      "0: Encoding Loss 20.936824798583984, Transition Loss 0.9340882301330566, Classifier Loss 0.5720455646514893, Total Loss 183.19915771484375\n",
      "0: Encoding Loss 21.82468032836914, Transition Loss 1.5077857971191406, Classifier Loss 0.5763342380523682, Total Loss 189.18463134765625\n",
      "0: Encoding Loss 21.286296844482422, Transition Loss 1.1536800861358643, Classifier Loss 0.5522701144218445, Total Loss 183.40626525878906\n",
      "0: Encoding Loss 21.236474990844727, Transition Loss 1.0525944232940674, Classifier Loss 0.5653060078620911, Total Loss 184.37049865722656\n",
      "0: Encoding Loss 21.05364227294922, Transition Loss 1.554424524307251, Classifier Loss 0.5684769749641418, Total Loss 183.79132080078125\n",
      "0: Encoding Loss 21.75682830810547, Transition Loss 1.3281502723693848, Classifier Loss 0.5496844053268433, Total Loss 186.04067993164062\n",
      "0: Encoding Loss 20.24922752380371, Transition Loss 0.8540017604827881, Classifier Loss 0.5621247291564941, Total Loss 178.0494384765625\n",
      "0: Encoding Loss 21.45970344543457, Transition Loss 1.1279382705688477, Classifier Loss 0.5446144938468933, Total Loss 183.67083740234375\n",
      "0: Encoding Loss 21.871286392211914, Transition Loss 2.0312581062316895, Classifier Loss 0.5639234781265259, Total Loss 188.4325714111328\n",
      "0: Encoding Loss 21.406177520751953, Transition Loss 0.8467972278594971, Classifier Loss 0.5613232851028442, Total Loss 184.90811157226562\n",
      "0: Encoding Loss 19.60013198852539, Transition Loss 0.7737389206886292, Classifier Loss 0.573657214641571, Total Loss 175.27601623535156\n",
      "0: Encoding Loss 21.170631408691406, Transition Loss 1.6136138439178467, Classifier Loss 0.5553343296051025, Total Loss 183.20266723632812\n",
      "0: Encoding Loss 20.150043487548828, Transition Loss 0.6590019464492798, Classifier Loss 0.5525643825531006, Total Loss 176.42030334472656\n",
      "0: Encoding Loss 21.538930892944336, Transition Loss 1.3337738513946533, Classifier Loss 0.5777245759963989, Total Loss 187.53956604003906\n",
      "0: Encoding Loss 21.967844009399414, Transition Loss 1.6281377077102661, Classifier Loss 0.5686423778533936, Total Loss 189.32257080078125\n",
      "0: Encoding Loss 21.56732749938965, Transition Loss 1.3745006322860718, Classifier Loss 0.5756900906562805, Total Loss 187.52279663085938\n",
      "0: Encoding Loss 20.583438873291016, Transition Loss 0.27838724851608276, Classifier Loss 0.5560749769210815, Total Loss 179.219482421875\n",
      "0: Encoding Loss 21.126708984375, Transition Loss 0.6779709458351135, Classifier Loss 0.557917594909668, Total Loss 182.82321166992188\n",
      "0: Encoding Loss 21.652393341064453, Transition Loss 0.755785346031189, Classifier Loss 0.5684102773666382, Total Loss 187.05770874023438\n",
      "0: Encoding Loss 22.194843292236328, Transition Loss 1.9956185817718506, Classifier Loss 0.5716356039047241, Total Loss 191.13087463378906\n",
      "0: Encoding Loss 21.612991333007812, Transition Loss 0.7419797778129578, Classifier Loss 0.5554214119911194, Total Loss 185.5168914794922\n",
      "0: Encoding Loss 20.342247009277344, Transition Loss 0.5982086658477783, Classifier Loss 0.54209303855896, Total Loss 176.5020751953125\n",
      "0: Encoding Loss 20.23834228515625, Transition Loss 0.6812846660614014, Classifier Loss 0.5825173854827881, Total Loss 179.9542999267578\n",
      "0: Encoding Loss 20.884395599365234, Transition Loss 0.4860597848892212, Classifier Loss 0.5489420294761658, Total Loss 180.39501953125\n",
      "0: Encoding Loss 22.065338134765625, Transition Loss 1.086876392364502, Classifier Loss 0.5684112906455994, Total Loss 189.66790771484375\n",
      "0: Encoding Loss 20.884361267089844, Transition Loss 0.9084073305130005, Classifier Loss 0.535066545009613, Total Loss 179.1761932373047\n",
      "0: Encoding Loss 21.782302856445312, Transition Loss 0.8431313037872314, Classifier Loss 0.5536354184150696, Total Loss 186.3946075439453\n",
      "0: Encoding Loss 22.32245635986328, Transition Loss 1.5586994886398315, Classifier Loss 0.554762065410614, Total Loss 190.034423828125\n",
      "0: Encoding Loss 20.734210968017578, Transition Loss 0.20288169384002686, Classifier Loss 0.5370601415634155, Total Loss 178.1924285888672\n",
      "0: Encoding Loss 22.54453468322754, Transition Loss 1.331639289855957, Classifier Loss 0.5441442728042603, Total Loss 190.21429443359375\n",
      "0: Encoding Loss 21.789310455322266, Transition Loss 1.1355352401733398, Classifier Loss 0.5582025051116943, Total Loss 187.0103302001953\n",
      "0: Encoding Loss 21.732158660888672, Transition Loss 0.8812297582626343, Classifier Loss 0.5448439121246338, Total Loss 185.22984313964844\n",
      "0: Encoding Loss 20.71042251586914, Transition Loss 0.38822507858276367, Classifier Loss 0.5575007200241089, Total Loss 180.1678924560547\n",
      "0: Encoding Loss 21.92295265197754, Transition Loss 1.0797587633132935, Classifier Loss 0.556412935256958, Total Loss 187.6109161376953\n",
      "0: Encoding Loss 22.18516731262207, Transition Loss 1.7602214813232422, Classifier Loss 0.5626704692840576, Total Loss 190.08213806152344\n",
      "0: Encoding Loss 23.156261444091797, Transition Loss 2.3930773735046387, Classifier Loss 0.5864942669868469, Total Loss 198.5442352294922\n",
      "0: Encoding Loss 19.968801498413086, Transition Loss 0.41144049167633057, Classifier Loss 0.5540301203727722, Total Loss 175.38040161132812\n",
      "0: Encoding Loss 21.435863494873047, Transition Loss 1.589988350868225, Classifier Loss 0.5702528357505798, Total Loss 186.27647399902344\n",
      "0: Encoding Loss 22.061315536499023, Transition Loss 1.4129165410995483, Classifier Loss 0.5593822598457336, Total Loss 188.8712921142578\n",
      "0: Encoding Loss 21.4290771484375, Transition Loss 1.3849749565124512, Classifier Loss 0.574809193611145, Total Loss 186.609375\n",
      "0: Encoding Loss 20.704538345336914, Transition Loss 0.9926680326461792, Classifier Loss 0.5462619066238403, Total Loss 179.25048828125\n",
      "0: Encoding Loss 21.047571182250977, Transition Loss 0.5205049514770508, Classifier Loss 0.5378769636154175, Total Loss 180.2813262939453\n",
      "0: Encoding Loss 21.134586334228516, Transition Loss 0.803806722164154, Classifier Loss 0.5433452129364014, Total Loss 181.46356201171875\n",
      "0: Encoding Loss 20.826221466064453, Transition Loss 1.1229655742645264, Classifier Loss 0.5712144374847412, Total Loss 182.5279541015625\n",
      "0: Encoding Loss 20.34743881225586, Transition Loss 0.41681116819381714, Classifier Loss 0.5479336977005005, Total Loss 177.0447235107422\n",
      "0: Encoding Loss 22.00865364074707, Transition Loss 0.9229711890220642, Classifier Loss 0.576250433921814, Total Loss 190.04615783691406\n",
      "0: Encoding Loss 21.26919174194336, Transition Loss 0.6694985032081604, Classifier Loss 0.5581206679344177, Total Loss 183.6950225830078\n",
      "0: Encoding Loss 20.873579025268555, Transition Loss 0.4487514793872833, Classifier Loss 0.5509283542633057, Total Loss 180.51382446289062\n",
      "0: Encoding Loss 20.510679244995117, Transition Loss 1.1750221252441406, Classifier Loss 0.5676892995834351, Total Loss 180.30300903320312\n",
      "0: Encoding Loss 21.384845733642578, Transition Loss 1.3392324447631836, Classifier Loss 0.5635267496109009, Total Loss 185.19744873046875\n",
      "0: Encoding Loss 20.121929168701172, Transition Loss 0.6228668093681335, Classifier Loss 0.5449864268302917, Total Loss 175.4793701171875\n",
      "0: Encoding Loss 19.735280990600586, Transition Loss 0.735352635383606, Classifier Loss 0.5570379495620728, Total Loss 174.4096221923828\n",
      "0: Encoding Loss 20.330774307250977, Transition Loss 0.24629470705986023, Classifier Loss 0.545333981513977, Total Loss 176.61656188964844\n",
      "0: Encoding Loss 22.288047790527344, Transition Loss 1.3192452192306519, Classifier Loss 0.5381003022193909, Total Loss 188.06600952148438\n",
      "0: Encoding Loss 19.692394256591797, Transition Loss 0.13311515748500824, Classifier Loss 0.5556089878082275, Total Loss 173.76852416992188\n",
      "0: Encoding Loss 20.567829132080078, Transition Loss -0.0888676792383194, Classifier Loss 0.5398867130279541, Total Loss 177.39561462402344\n",
      "0: Encoding Loss 21.170053482055664, Transition Loss 0.6890881061553955, Classifier Loss 0.5461215376853943, Total Loss 181.90811157226562\n",
      "0: Encoding Loss 22.640071868896484, Transition Loss 1.1071500778198242, Classifier Loss 0.5533593893051147, Total Loss 191.61923217773438\n",
      "0: Encoding Loss 21.030990600585938, Transition Loss 1.0220162868499756, Classifier Loss 0.5612378120422363, Total Loss 182.71853637695312\n",
      "0: Encoding Loss 21.117395401000977, Transition Loss 0.4356273114681244, Classifier Loss 0.5352509617805481, Total Loss 180.4037322998047\n",
      "0: Encoding Loss 21.43654441833496, Transition Loss 1.3287787437438965, Classifier Loss 0.5347622632980347, Total Loss 182.62701416015625\n",
      "0: Encoding Loss 20.795610427856445, Transition Loss 0.7293252944946289, Classifier Loss 0.5443791747093201, Total Loss 179.50331115722656\n",
      "0: Encoding Loss 23.141870498657227, Transition Loss 1.6718026399612427, Classifier Loss 0.5574451684951782, Total Loss 195.26446533203125\n",
      "0: Encoding Loss 20.630325317382812, Transition Loss 1.134243369102478, Classifier Loss 0.5453092455863953, Total Loss 178.76658630371094\n",
      "0: Encoding Loss 22.498905181884766, Transition Loss 1.8655920028686523, Classifier Loss 0.5751508474349976, Total Loss 193.25474548339844\n",
      "0: Encoding Loss 21.322406768798828, Transition Loss 0.6073533296585083, Classifier Loss 0.5401562452316284, Total Loss 182.19300842285156\n",
      "0: Encoding Loss 23.197509765625, Transition Loss 2.6515064239501953, Classifier Loss 0.5558546781539917, Total Loss 195.8311309814453\n",
      "0: Encoding Loss 21.756254196166992, Transition Loss 0.9599654078483582, Classifier Loss 0.5512428879737854, Total Loss 186.0458221435547\n",
      "0: Encoding Loss 22.87087631225586, Transition Loss 1.5106828212738037, Classifier Loss 0.5599638223648071, Total Loss 193.825927734375\n",
      "0: Encoding Loss 20.653255462646484, Transition Loss 1.4419560432434082, Classifier Loss 0.5520766377449036, Total Loss 179.7039794921875\n",
      "0: Encoding Loss 22.668848037719727, Transition Loss 2.229708433151245, Classifier Loss 0.5449968576431274, Total Loss 191.4046630859375\n",
      "0: Encoding Loss 21.761672973632812, Transition Loss 1.3303172588348389, Classifier Loss 0.5574292540550232, Total Loss 186.84507751464844\n",
      "0: Encoding Loss 21.905364990234375, Transition Loss 1.561614751815796, Classifier Loss 0.5770153999328613, Total Loss 189.7583770751953\n",
      "0: Encoding Loss 22.74245834350586, Transition Loss 1.345726728439331, Classifier Loss 0.5552583932876587, Total Loss 192.5188751220703\n",
      "0: Encoding Loss 19.984302520751953, Transition Loss 0.2960470914840698, Classifier Loss 0.5329305529594421, Total Loss 173.31729125976562\n",
      "0: Encoding Loss 21.774822235107422, Transition Loss 1.205817699432373, Classifier Loss 0.5592888593673706, Total Loss 187.06015014648438\n",
      "0: Encoding Loss 20.626901626586914, Transition Loss 0.7504115104675293, Classifier Loss 0.5454779863357544, Total Loss 178.609375\n",
      "0: Encoding Loss 21.55535888671875, Transition Loss 1.249889850616455, Classifier Loss 0.5477047562599182, Total Loss 184.6025848388672\n",
      "0: Encoding Loss 20.38564109802246, Transition Loss -0.055719293653964996, Classifier Loss 0.5407099723815918, Total Loss 176.3848419189453\n",
      "0: Encoding Loss 21.323461532592773, Transition Loss 1.1178735494613647, Classifier Loss 0.5600041151046753, Total Loss 184.38833618164062\n",
      "0: Encoding Loss 20.74703025817871, Transition Loss 0.5116538405418396, Classifier Loss 0.5382086634635925, Total Loss 178.50772094726562\n",
      "0: Encoding Loss 23.839826583862305, Transition Loss 2.3501551151275635, Classifier Loss 0.554349958896637, Total Loss 199.41403198242188\n",
      "0: Encoding Loss 21.43288230895996, Transition Loss 1.0962014198303223, Classifier Loss 0.5567339658737183, Total Loss 184.7091827392578\n",
      "0: Encoding Loss 21.421981811523438, Transition Loss 1.3109186887741089, Classifier Loss 0.5517268776893616, Total Loss 184.22894287109375\n",
      "0: Encoding Loss 21.709125518798828, Transition Loss 0.7096775770187378, Classifier Loss 0.545413613319397, Total Loss 185.0800018310547\n",
      "0: Encoding Loss 19.777822494506836, Transition Loss 1.017295479774475, Classifier Loss 0.5536470413208008, Total Loss 174.43856811523438\n",
      "0: Encoding Loss 20.718196868896484, Transition Loss 0.7962958216667175, Classifier Loss 0.5537461042404175, Total Loss 180.0023193359375\n",
      "0: Encoding Loss 22.133045196533203, Transition Loss 1.6797068119049072, Classifier Loss 0.5580918192863464, Total Loss 189.2793426513672\n",
      "0: Encoding Loss 21.654308319091797, Transition Loss 0.6831172704696655, Classifier Loss 0.52778160572052, Total Loss 182.97726440429688\n",
      "0: Encoding Loss 21.134746551513672, Transition Loss 0.0405571311712265, Classifier Loss 0.5329675674438477, Total Loss 180.1214599609375\n",
      "0: Encoding Loss 20.45254135131836, Transition Loss 0.7780205607414246, Classifier Loss 0.5434989333152771, Total Loss 177.37635803222656\n",
      "0: Encoding Loss 21.255300521850586, Transition Loss 0.8942116498947144, Classifier Loss 0.5212880969047546, Total Loss 180.01829528808594\n",
      "0: Encoding Loss 21.138891220092773, Transition Loss 0.9974956512451172, Classifier Loss 0.5525393486022949, Total Loss 182.4862823486328\n",
      "0: Encoding Loss 22.102046966552734, Transition Loss 1.2917819023132324, Classifier Loss 0.5442496538162231, Total Loss 187.55397033691406\n",
      "0: Encoding Loss 20.805530548095703, Transition Loss 1.4193122386932373, Classifier Loss 0.5433173179626465, Total Loss 179.73263549804688\n",
      "0: Encoding Loss 22.613651275634766, Transition Loss 1.987272024154663, Classifier Loss 0.557988166809082, Total Loss 192.275634765625\n",
      "0: Encoding Loss 20.9285888671875, Transition Loss 0.4546565115451813, Classifier Loss 0.5423188805580139, Total Loss 179.9853057861328\n",
      "0: Encoding Loss 21.163820266723633, Transition Loss 1.1109877824783325, Classifier Loss 0.5318936109542847, Total Loss 180.61668395996094\n",
      "0: Encoding Loss 21.2799072265625, Transition Loss 1.4492219686508179, Classifier Loss 0.5491372346878052, Total Loss 183.1728515625\n",
      "0: Encoding Loss 22.0526123046875, Transition Loss 0.8824384808540344, Classifier Loss 0.529702365398407, Total Loss 185.63890075683594\n",
      "0: Encoding Loss 21.088756561279297, Transition Loss 1.1835036277770996, Classifier Loss 0.538364589214325, Total Loss 180.8424072265625\n",
      "0: Encoding Loss 21.17583465576172, Transition Loss 0.5589399337768555, Classifier Loss 0.534469485282898, Total Loss 180.7255401611328\n",
      "0: Encoding Loss 21.105831146240234, Transition Loss 0.38728392124176025, Classifier Loss 0.5435500144958496, Total Loss 181.1448974609375\n",
      "0: Encoding Loss 21.315181732177734, Transition Loss 1.2609328031539917, Classifier Loss 0.5466329455375671, Total Loss 183.05877685546875\n",
      "0: Encoding Loss 21.049962997436523, Transition Loss 1.162721872329712, Classifier Loss 0.5368235111236572, Total Loss 180.4472198486328\n",
      "0: Encoding Loss 21.52090835571289, Transition Loss 0.6141573190689087, Classifier Loss 0.5439403653144836, Total Loss 183.76516723632812\n",
      "0: Encoding Loss 21.681543350219727, Transition Loss 0.8697286248207092, Classifier Loss 0.5268760919570923, Total Loss 183.124755859375\n",
      "0: Encoding Loss 21.971105575561523, Transition Loss 1.002704381942749, Classifier Loss 0.5555717945098877, Total Loss 187.78489685058594\n",
      "0: Encoding Loss 21.441326141357422, Transition Loss 1.1683965921401978, Classifier Loss 0.5559073686599731, Total Loss 184.70606994628906\n",
      "0: Encoding Loss 21.69133949279785, Transition Loss 1.6350996494293213, Classifier Loss 0.5424408912658691, Total Loss 185.04615783691406\n",
      "0: Encoding Loss 20.894258499145508, Transition Loss 0.5663034915924072, Classifier Loss 0.525497555732727, Total Loss 178.14183044433594\n",
      "0: Encoding Loss 22.158645629882812, Transition Loss 1.6604063510894775, Classifier Loss 0.5568269491195679, Total Loss 189.29873657226562\n",
      "0: Encoding Loss 21.115266799926758, Transition Loss 0.5255204439163208, Classifier Loss 0.5300571918487549, Total Loss 179.90753173828125\n",
      "0: Encoding Loss 21.369922637939453, Transition Loss 0.08972455561161041, Classifier Loss 0.5366290807723999, Total Loss 181.9183349609375\n",
      "0: Encoding Loss 20.34699821472168, Transition Loss 0.7692089676856995, Classifier Loss 0.5452998876571655, Total Loss 176.91966247558594\n",
      "0: Encoding Loss 21.080995559692383, Transition Loss 0.5654973983764648, Classifier Loss 0.5320937037467957, Total Loss 179.92153930664062\n",
      "0: Encoding Loss 20.452280044555664, Transition Loss 0.6225227117538452, Classifier Loss 0.5295882821083069, Total Loss 175.92152404785156\n",
      "0: Encoding Loss 20.857702255249023, Transition Loss 0.9666878581047058, Classifier Loss 0.5163934230804443, Total Loss 177.17222595214844\n",
      "0: Encoding Loss 21.95620346069336, Transition Loss 1.0076149702072144, Classifier Loss 0.534544050693512, Total Loss 185.5946807861328\n",
      "0: Encoding Loss 21.537939071655273, Transition Loss 1.2419483661651611, Classifier Loss 0.5746251940727234, Total Loss 187.18695068359375\n",
      "0: Encoding Loss 22.60830307006836, Transition Loss 0.7830415368080139, Classifier Loss 0.5324217081069946, Total Loss 189.20521545410156\n",
      "0: Encoding Loss 21.149456024169922, Transition Loss 0.546444296836853, Classifier Loss 0.5287424325942993, Total Loss 179.9895782470703\n",
      "0: Encoding Loss 22.39154624938965, Transition Loss 0.9793402552604675, Classifier Loss 0.5564843416213989, Total Loss 190.38946533203125\n",
      "0: Encoding Loss 21.018665313720703, Transition Loss 0.6107193827629089, Classifier Loss 0.5247408151626587, Total Loss 178.8303680419922\n",
      "0: Encoding Loss 20.779977798461914, Transition Loss 0.6517885327339172, Classifier Loss 0.5356550216674805, Total Loss 178.50608825683594\n",
      "0: Encoding Loss 21.64974594116211, Transition Loss 1.1541410684585571, Classifier Loss 0.5428522825241089, Total Loss 184.64537048339844\n",
      "0: Encoding Loss 20.499103546142578, Transition Loss 0.5808282494544983, Classifier Loss 0.5529786944389343, Total Loss 178.5248260498047\n",
      "0: Encoding Loss 21.652339935302734, Transition Loss 0.7519055604934692, Classifier Loss 0.5162128210067749, Total Loss 181.83609008789062\n",
      "0: Encoding Loss 21.440258026123047, Transition Loss 1.141284465789795, Classifier Loss 0.5358379483222961, Total Loss 182.68185424804688\n",
      "0: Encoding Loss 20.91665267944336, Transition Loss 1.068474292755127, Classifier Loss 0.5437482595443726, Total Loss 180.30213928222656\n",
      "0: Encoding Loss 20.561771392822266, Transition Loss 0.7560648918151855, Classifier Loss 0.5598766803741455, Total Loss 179.6607208251953\n",
      "0: Encoding Loss 21.38542938232422, Transition Loss 0.32925713062286377, Classifier Loss 0.5241039395332336, Total Loss 180.8546600341797\n",
      "0: Encoding Loss 21.27866554260254, Transition Loss 1.3103382587432861, Classifier Loss 0.5309299230575562, Total Loss 181.28912353515625\n",
      "0: Encoding Loss 19.865875244140625, Transition Loss 0.6073523163795471, Classifier Loss 0.5273443460464478, Total Loss 172.17262268066406\n",
      "0: Encoding Loss 22.25417709350586, Transition Loss 0.8602938652038574, Classifier Loss 0.5498656034469604, Total Loss 188.85574340820312\n",
      "0: Encoding Loss 21.97080421447754, Transition Loss 0.975692629814148, Classifier Loss 0.538274347782135, Total Loss 186.04254150390625\n",
      "0: Encoding Loss 20.11563491821289, Transition Loss 0.41936051845550537, Classifier Loss 0.5211379528045654, Total Loss 172.975341796875\n",
      "0: Encoding Loss 20.703529357910156, Transition Loss 0.7901591062545776, Classifier Loss 0.5276784896850586, Total Loss 177.3050994873047\n",
      "0: Encoding Loss 20.374019622802734, Transition Loss -0.10600356757640839, Classifier Loss 0.5426265597343445, Total Loss 176.50672912597656\n",
      "0: Encoding Loss 21.12120819091797, Transition Loss 0.6844002604484558, Classifier Loss 0.5415851473808289, Total Loss 181.15953063964844\n",
      "0: Encoding Loss 20.713693618774414, Transition Loss 0.2383979856967926, Classifier Loss 0.5315755605697632, Total Loss 177.5350799560547\n",
      "0: Encoding Loss 20.17734718322754, Transition Loss 0.20878389477729797, Classifier Loss 0.519957423210144, Total Loss 173.14334106445312\n",
      "0: Encoding Loss 20.937814712524414, Transition Loss 0.5042621493339539, Classifier Loss 0.5408837795257568, Total Loss 179.91697692871094\n",
      "0: Encoding Loss 21.312576293945312, Transition Loss 1.3430750370025635, Classifier Loss 0.5376670360565186, Total Loss 182.1793975830078\n",
      "0: Encoding Loss 21.709224700927734, Transition Loss 0.8505861759185791, Classifier Loss 0.5261213779449463, Total Loss 183.20773315429688\n",
      "0: Encoding Loss 20.13191032409668, Transition Loss 0.4796264171600342, Classifier Loss 0.5131906270980835, Total Loss 172.30238342285156\n",
      "0: Encoding Loss 19.751941680908203, Transition Loss -0.07127684354782104, Classifier Loss 0.517662525177002, Total Loss 170.27786254882812\n",
      "0: Encoding Loss 20.457765579223633, Transition Loss 0.0338849201798439, Classifier Loss 0.5202227234840393, Total Loss 174.7824249267578\n",
      "0: Encoding Loss 20.653194427490234, Transition Loss 0.23940591514110565, Classifier Loss 0.5304054021835327, Total Loss 177.05548095703125\n",
      "0: Encoding Loss 20.717519760131836, Transition Loss 0.16440929472446442, Classifier Loss 0.5226929783821106, Total Loss 176.6401824951172\n",
      "0: Encoding Loss 21.40427017211914, Transition Loss 1.239035725593567, Classifier Loss 0.5317571759223938, Total Loss 182.0969696044922\n",
      "0: Encoding Loss 20.99641990661621, Transition Loss 1.1232540607452393, Classifier Loss 0.5417709946632385, Total Loss 180.60491943359375\n",
      "0: Encoding Loss 20.791812896728516, Transition Loss 0.8684969544410706, Classifier Loss 0.5256911516189575, Total Loss 177.66738891601562\n",
      "0: Encoding Loss 20.443925857543945, Transition Loss 0.585917592048645, Classifier Loss 0.5399009585380554, Total Loss 176.8880157470703\n",
      "0: Encoding Loss 20.591123580932617, Transition Loss 0.4255659282207489, Classifier Loss 0.5249752998352051, Total Loss 176.21450805664062\n",
      "0: Encoding Loss 21.661518096923828, Transition Loss 1.224225640296936, Classifier Loss 0.5164963006973267, Total Loss 182.10842895507812\n",
      "0: Encoding Loss 21.01461410522461, Transition Loss 0.7869836688041687, Classifier Loss 0.5263406038284302, Total Loss 179.0365447998047\n",
      "0: Encoding Loss 21.22433090209961, Transition Loss 1.1395264863967896, Classifier Loss 0.5304332971572876, Total Loss 180.8451385498047\n",
      "0: Encoding Loss 21.815500259399414, Transition Loss 0.9720988273620605, Classifier Loss 0.5231768488883972, Total Loss 183.5995330810547\n",
      "0: Encoding Loss 20.83444595336914, Transition Loss 0.8513673543930054, Classifier Loss 0.5065599083900452, Total Loss 176.00320434570312\n",
      "0: Encoding Loss 19.441469192504883, Transition Loss 0.2406952679157257, Classifier Loss 0.522560179233551, Total Loss 169.00112915039062\n",
      "0: Encoding Loss 20.68697738647461, Transition Loss 0.7598008513450623, Classifier Loss 0.5429352521896362, Total Loss 178.71932983398438\n",
      "0: Encoding Loss 20.95480728149414, Transition Loss 0.3940039575099945, Classifier Loss 0.5132148265838623, Total Loss 177.2079315185547\n",
      "0: Encoding Loss 20.890531539916992, Transition Loss 2.0032691955566406, Classifier Loss 0.5466026067733765, Total Loss 180.80474853515625\n",
      "0: Encoding Loss 19.419635772705078, Transition Loss 0.160411074757576, Classifier Loss 0.5150213837623596, Total Loss 168.08412170410156\n",
      "0: Encoding Loss 20.688810348510742, Transition Loss 0.5409078598022461, Classifier Loss 0.5542560815811157, Total Loss 179.77484130859375\n",
      "0: Encoding Loss 21.01246452331543, Transition Loss 1.1019997596740723, Classifier Loss 0.5205368995666504, Total Loss 178.56927490234375\n",
      "0: Encoding Loss 21.486618041992188, Transition Loss 0.6142133474349976, Classifier Loss 0.5228648781776428, Total Loss 181.45187377929688\n",
      "0: Encoding Loss 22.792499542236328, Transition Loss 1.479668140411377, Classifier Loss 0.5684967041015625, Total Loss 194.19654846191406\n",
      "0: Encoding Loss 21.274166107177734, Transition Loss 0.9247535467147827, Classifier Loss 0.5292384624481201, Total Loss 180.93875122070312\n",
      "0: Encoding Loss 21.725215911865234, Transition Loss 0.6704051494598389, Classifier Loss 0.5355876684188843, Total Loss 184.17822265625\n",
      "0: Encoding Loss 20.88237953186035, Transition Loss 0.6529092788696289, Classifier Loss 0.5174939036369324, Total Loss 177.30484008789062\n",
      "0: Encoding Loss 21.418542861938477, Transition Loss 1.1077697277069092, Classifier Loss 0.5020849704742432, Total Loss 179.16287231445312\n",
      "0: Encoding Loss 21.70787239074707, Transition Loss 1.5851588249206543, Classifier Loss 0.5467061996459961, Total Loss 185.5519256591797\n",
      "0: Encoding Loss 21.52275848388672, Transition Loss 1.5607833862304688, Classifier Loss 0.5311708450317383, Total Loss 182.87794494628906\n",
      "0: Encoding Loss 19.68143081665039, Transition Loss 0.5760268568992615, Classifier Loss 0.48325514793395996, Total Loss 166.64450073242188\n",
      "0: Encoding Loss 21.305078506469727, Transition Loss 1.1758825778961182, Classifier Loss 0.5304417014122009, Total Loss 181.34500122070312\n",
      "0: Encoding Loss 22.52695083618164, Transition Loss 0.9694755673408508, Classifier Loss 0.513506293296814, Total Loss 186.90013122558594\n",
      "0: Encoding Loss 20.54498291015625, Transition Loss 1.1033568382263184, Classifier Loss 0.5325733423233032, Total Loss 176.9685821533203\n",
      "0: Encoding Loss 19.941627502441406, Transition Loss 0.42830532789230347, Classifier Loss 0.5100741982460022, Total Loss 170.82852172851562\n",
      "0: Encoding Loss 22.285863876342773, Transition Loss 1.5625331401824951, Classifier Loss 0.5234998464584351, Total Loss 186.69020080566406\n",
      "0: Encoding Loss 20.17363166809082, Transition Loss 0.3715548515319824, Classifier Loss 0.5260728001594543, Total Loss 173.79769897460938\n",
      "0: Encoding Loss 21.900711059570312, Transition Loss 0.9821401834487915, Classifier Loss 0.5051957368850708, Total Loss 182.3166961669922\n",
      "0: Encoding Loss 21.808406829833984, Transition Loss 0.7076629400253296, Classifier Loss 0.5051465630531311, Total Loss 181.64817810058594\n",
      "0: Encoding Loss 20.619384765625, Transition Loss 1.1407153606414795, Classifier Loss 0.5035648941993713, Total Loss 174.5290985107422\n",
      "0: Encoding Loss 20.846479415893555, Transition Loss 0.5252729654312134, Classifier Loss 0.5263314247131348, Total Loss 177.92213439941406\n",
      "0: Encoding Loss 20.839353561401367, Transition Loss 1.130017876625061, Classifier Loss 0.5181242823600769, Total Loss 177.30056762695312\n",
      "0: Encoding Loss 21.474740982055664, Transition Loss 0.4450715184211731, Classifier Loss 0.5137102603912354, Total Loss 180.3975067138672\n",
      "0: Encoding Loss 20.407121658325195, Transition Loss 0.7177923917770386, Classifier Loss 0.5185300707817078, Total Loss 174.58285522460938\n",
      "0: Encoding Loss 20.959640502929688, Transition Loss 0.7237144708633423, Classifier Loss 0.5177517533302307, Total Loss 177.822509765625\n",
      "0: Encoding Loss 21.91506576538086, Transition Loss 0.6268026232719421, Classifier Loss 0.5145648121833801, Total Loss 183.19760131835938\n",
      "0: Encoding Loss 21.44207763671875, Transition Loss 1.1956794261932373, Classifier Loss 0.5000581741333008, Total Loss 179.1365509033203\n",
      "0: Encoding Loss 21.41829490661621, Transition Loss 0.5260137915611267, Classifier Loss 0.5280388593673706, Total Loss 181.52406311035156\n",
      "0: Encoding Loss 20.228252410888672, Transition Loss 1.238533854484558, Classifier Loss 0.5229067206382751, Total Loss 174.1555938720703\n",
      "0: Encoding Loss 21.033382415771484, Transition Loss 0.7233404517173767, Classifier Loss 0.5187572240829468, Total Loss 178.3653564453125\n",
      "0: Encoding Loss 22.05185890197754, Transition Loss 1.3966909646987915, Classifier Loss 0.5097116231918335, Total Loss 183.8409881591797\n",
      "0: Encoding Loss 20.173137664794922, Transition Loss 0.42387378215789795, Classifier Loss 0.5087573528289795, Total Loss 172.08412170410156\n",
      "0: Encoding Loss 20.520675659179688, Transition Loss 0.28219497203826904, Classifier Loss 0.5256342887878418, Total Loss 175.80038452148438\n",
      "0: Encoding Loss 20.018264770507812, Transition Loss 0.8079155087471008, Classifier Loss 0.5105007290840149, Total Loss 171.4828338623047\n",
      "0: Encoding Loss 20.896474838256836, Transition Loss 1.1021437644958496, Classifier Loss 0.5330895781517029, Total Loss 179.128662109375\n",
      "0: Encoding Loss 21.1662654876709, Transition Loss 0.6704785823822021, Classifier Loss 0.49930498003959656, Total Loss 177.1962890625\n",
      "0: Encoding Loss 20.751220703125, Transition Loss 1.2756199836730957, Classifier Loss 0.48337969183921814, Total Loss 173.35556030273438\n",
      "0: Encoding Loss 21.415456771850586, Transition Loss 1.0605108737945557, Classifier Loss 0.539423942565918, Total Loss 182.85935974121094\n",
      "0: Encoding Loss 20.249736785888672, Transition Loss 1.5073038339614868, Classifier Loss 0.4999036490917206, Total Loss 172.0917205810547\n",
      "0: Encoding Loss 22.252456665039062, Transition Loss 1.5902353525161743, Classifier Loss 0.5300794839859009, Total Loss 187.15878295898438\n",
      "0: Encoding Loss 20.746553421020508, Transition Loss 0.3767359256744385, Classifier Loss 0.5168170928955078, Total Loss 176.31173706054688\n",
      "0: Encoding Loss 22.09844207763672, Transition Loss 1.4568510055541992, Classifier Loss 0.5460671782493591, Total Loss 187.7801055908203\n",
      "0: Encoding Loss 20.722454071044922, Transition Loss 1.2760173082351685, Classifier Loss 0.5293850302696228, Total Loss 177.7836456298828\n",
      "0: Encoding Loss 20.458114624023438, Transition Loss 0.5165827870368958, Classifier Loss 0.51793372631073, Total Loss 174.7487030029297\n",
      "0: Encoding Loss 19.160730361938477, Transition Loss 0.5438588857650757, Classifier Loss 0.5038940906524658, Total Loss 165.5713348388672\n",
      "0: Encoding Loss 20.562389373779297, Transition Loss 1.2375984191894531, Classifier Loss 0.5331076979637146, Total Loss 177.18016052246094\n",
      "0: Encoding Loss 22.233884811401367, Transition Loss 1.2538604736328125, Classifier Loss 0.5257959365844727, Total Loss 186.4844512939453\n",
      "0: Encoding Loss 18.74764060974121, Transition Loss 0.33553656935691833, Classifier Loss 0.493635892868042, Total Loss 161.98365783691406\n",
      "0: Encoding Loss 23.26625633239746, Transition Loss 1.5826566219329834, Classifier Loss 0.5159592032432556, Total Loss 191.82652282714844\n",
      "0: Encoding Loss 20.707721710205078, Transition Loss 0.7770417928695679, Classifier Loss 0.5008412599563599, Total Loss 174.6412811279297\n",
      "0: Encoding Loss 20.528118133544922, Transition Loss 0.9064180254936218, Classifier Loss 0.5460351705551147, Total Loss 178.13479614257812\n",
      "0: Encoding Loss 20.98287010192871, Transition Loss 0.8853901028633118, Classifier Loss 0.519883930683136, Total Loss 178.23977661132812\n",
      "0: Encoding Loss 20.89324378967285, Transition Loss 0.7551654577255249, Classifier Loss 0.5039845108985901, Total Loss 176.0599822998047\n",
      "0: Encoding Loss 20.44825553894043, Transition Loss 0.34254130721092224, Classifier Loss 0.5259658098220825, Total Loss 175.4231414794922\n",
      "0: Encoding Loss 23.777448654174805, Transition Loss 1.9315195083618164, Classifier Loss 0.5184701085090637, Total Loss 195.28433227539062\n",
      "0: Encoding Loss 20.95701026916504, Transition Loss 0.6387694478034973, Classifier Loss 0.528166651725769, Total Loss 178.81423950195312\n",
      "0: Encoding Loss 20.055240631103516, Transition Loss 0.535234808921814, Classifier Loss 0.5059387683868408, Total Loss 171.13941955566406\n",
      "0: Encoding Loss 20.998641967773438, Transition Loss 0.7317399382591248, Classifier Loss 0.4858554005622864, Total Loss 174.87008666992188\n",
      "0: Encoding Loss 22.286344528198242, Transition Loss 1.1497057676315308, Classifier Loss 0.515977144241333, Total Loss 185.7756805419922\n",
      "0: Encoding Loss 21.446483612060547, Transition Loss 1.5925816297531128, Classifier Loss 0.5241233110427856, Total Loss 181.72828674316406\n",
      "0: Encoding Loss 20.83354377746582, Transition Loss 0.32819053530693054, Classifier Loss 0.525598406791687, Total Loss 177.6923828125\n",
      "0: Encoding Loss 20.112199783325195, Transition Loss 0.7667481899261475, Classifier Loss 0.5230019688606262, Total Loss 173.2801055908203\n",
      "0: Encoding Loss 21.22292137145996, Transition Loss 0.5221763849258423, Classifier Loss 0.5154024958610535, Total Loss 179.08665466308594\n",
      "0: Encoding Loss 21.452367782592773, Transition Loss 0.7984411716461182, Classifier Loss 0.5093234777450562, Total Loss 179.9659423828125\n",
      "0: Encoding Loss 22.450359344482422, Transition Loss 0.9416829943656921, Classifier Loss 0.523189127445221, Total Loss 187.3977508544922\n",
      "0: Encoding Loss 20.23876190185547, Transition Loss 0.7253259420394897, Classifier Loss 0.5067557096481323, Total Loss 172.39828491210938\n",
      "0: Encoding Loss 20.343557357788086, Transition Loss 0.8649434447288513, Classifier Loss 0.49563533067703247, Total Loss 171.97085571289062\n",
      "0: Encoding Loss 20.60982894897461, Transition Loss 0.8385009765625, Classifier Loss 0.5030961036682129, Total Loss 174.3040008544922\n",
      "0: Encoding Loss 20.691518783569336, Transition Loss 1.309085726737976, Classifier Loss 0.5181065201759338, Total Loss 176.48341369628906\n",
      "0: Encoding Loss 22.039968490600586, Transition Loss 1.043715000152588, Classifier Loss 0.5222254395484924, Total Loss 184.87985229492188\n",
      "0: Encoding Loss 21.006698608398438, Transition Loss 0.7239394783973694, Classifier Loss 0.48355263471603394, Total Loss 174.68504333496094\n",
      "0: Encoding Loss 22.174663543701172, Transition Loss 1.1123735904693604, Classifier Loss 0.5115658640861511, Total Loss 184.64952087402344\n",
      "0: Encoding Loss 21.70612335205078, Transition Loss 0.9141471982002258, Classifier Loss 0.5255653858184814, Total Loss 183.158935546875\n",
      "0: Encoding Loss 21.71924591064453, Transition Loss 1.136197805404663, Classifier Loss 0.5048971772193909, Total Loss 181.25967407226562\n",
      "0: Encoding Loss 21.498497009277344, Transition Loss 1.0747716426849365, Classifier Loss 0.5062896013259888, Total Loss 180.04983520507812\n",
      "0: Encoding Loss 20.781530380249023, Transition Loss 0.5802628993988037, Classifier Loss 0.514828085899353, Total Loss 176.4040985107422\n",
      "0: Encoding Loss 20.101829528808594, Transition Loss 0.05450703948736191, Classifier Loss 0.4921691417694092, Total Loss 169.84970092773438\n",
      "0: Encoding Loss 20.619964599609375, Transition Loss 0.5441411137580872, Classifier Loss 0.5203949213027954, Total Loss 175.97694396972656\n",
      "0: Encoding Loss 21.594478607177734, Transition Loss 1.241697907447815, Classifier Loss 0.49388551712036133, Total Loss 179.4521026611328\n",
      "0: Encoding Loss 21.585369110107422, Transition Loss 1.037671685218811, Classifier Loss 0.5359736680984497, Total Loss 183.524658203125\n",
      "0: Encoding Loss 20.57183837890625, Transition Loss 0.7652647495269775, Classifier Loss 0.49761202931404114, Total Loss 173.49835205078125\n",
      "0: Encoding Loss 21.288082122802734, Transition Loss 2.298248767852783, Classifier Loss 0.5306475162506104, Total Loss 181.71253967285156\n",
      "0: Encoding Loss 21.71430015563965, Transition Loss 1.794243574142456, Classifier Loss 0.5264392495155334, Total Loss 183.64743041992188\n",
      "0: Encoding Loss 21.607454299926758, Transition Loss 1.1734108924865723, Classifier Loss 0.5224146842956543, Total Loss 182.35556030273438\n",
      "0: Encoding Loss 21.780029296875, Transition Loss 0.2587105333805084, Classifier Loss 0.5150622129440308, Total Loss 182.28988647460938\n",
      "0: Encoding Loss 22.20354652404785, Transition Loss 0.8148452043533325, Classifier Loss 0.5079335570335388, Total Loss 184.340576171875\n",
      "0: Encoding Loss 19.846904754638672, Transition Loss 0.25592362880706787, Classifier Loss 0.5025975704193115, Total Loss 169.4435577392578\n",
      "0: Encoding Loss 20.564058303833008, Transition Loss 1.0526657104492188, Classifier Loss 0.5277739763259888, Total Loss 176.5828094482422\n",
      "0: Encoding Loss 21.88491439819336, Transition Loss 0.9450013637542725, Classifier Loss 0.5302481651306152, Total Loss 184.7123260498047\n",
      "0: Encoding Loss 21.69076156616211, Transition Loss 1.2824773788452148, Classifier Loss 0.48620378971099854, Total Loss 179.27793884277344\n",
      "0: Encoding Loss 20.648517608642578, Transition Loss 1.0502164363861084, Classifier Loss 0.49503135681152344, Total Loss 173.8143310546875\n",
      "0: Encoding Loss 21.058570861816406, Transition Loss 1.1464264392852783, Classifier Loss 0.4961826205253601, Total Loss 176.4282684326172\n",
      "0: Encoding Loss 20.829545974731445, Transition Loss 0.7805824875831604, Classifier Loss 0.4982422888278961, Total Loss 175.11375427246094\n",
      "0: Encoding Loss 19.44757843017578, Transition Loss 0.6313326358795166, Classifier Loss 0.5067396759986877, Total Loss 167.6119842529297\n",
      "0: Encoding Loss 19.425357818603516, Transition Loss 0.27341070771217346, Classifier Loss 0.4722575545310974, Total Loss 163.8872528076172\n",
      "0: Encoding Loss 20.401416778564453, Transition Loss 1.2827609777450562, Classifier Loss 0.5224655866622925, Total Loss 175.1681671142578\n",
      "0: Encoding Loss 21.13640594482422, Transition Loss 1.162474513053894, Classifier Loss 0.5185348391532898, Total Loss 179.13693237304688\n",
      "0: Encoding Loss 21.82183837890625, Transition Loss 1.345154047012329, Classifier Loss 0.5100082755088806, Total Loss 182.46990966796875\n",
      "0: Encoding Loss 20.244455337524414, Transition Loss -0.18673169612884521, Classifier Loss 0.5045769810676575, Total Loss 171.9243621826172\n",
      "0: Encoding Loss 20.565818786621094, Transition Loss 1.2826125621795654, Classifier Loss 0.5069341063499451, Total Loss 174.60137939453125\n",
      "0: Encoding Loss 19.595163345336914, Transition Loss 0.812263011932373, Classifier Loss 0.4895668923854828, Total Loss 166.8525848388672\n",
      "0: Encoding Loss 19.923683166503906, Transition Loss 0.1935107558965683, Classifier Loss 0.5034346580505371, Total Loss 169.96298217773438\n",
      "0: Encoding Loss 21.85897445678711, Transition Loss 1.0097832679748535, Classifier Loss 0.5157774090766907, Total Loss 183.13551330566406\n",
      "0: Encoding Loss 21.727256774902344, Transition Loss 1.4249004125595093, Classifier Loss 0.5090292096138, Total Loss 181.83642578125\n",
      "0: Encoding Loss 21.999128341674805, Transition Loss 0.8957523107528687, Classifier Loss 0.539489209651947, Total Loss 186.302001953125\n",
      "0: Encoding Loss 20.387113571166992, Transition Loss 1.3809192180633545, Classifier Loss 0.5225917100906372, Total Loss 175.13421630859375\n",
      "0: Encoding Loss 21.08250617980957, Transition Loss 0.4689798355102539, Classifier Loss 0.4950157403945923, Total Loss 176.1842041015625\n",
      "0: Encoding Loss 21.727724075317383, Transition Loss 0.6966903805732727, Classifier Loss 0.5031189322471619, Total Loss 180.9569091796875\n",
      "0: Encoding Loss 20.576751708984375, Transition Loss 0.9523577690124512, Classifier Loss 0.5062512159347534, Total Loss 174.46656799316406\n",
      "0: Encoding Loss 21.83067512512207, Transition Loss 1.0710945129394531, Classifier Loss 0.49538564682006836, Total Loss 180.9510498046875\n",
      "0: Encoding Loss 21.35837745666504, Transition Loss 0.6536262035369873, Classifier Loss 0.47840720415115356, Total Loss 176.25244140625\n",
      "0: Encoding Loss 20.483600616455078, Transition Loss 0.4781458079814911, Classifier Loss 0.5005617141723633, Total Loss 173.14903259277344\n",
      "0: Encoding Loss 22.300495147705078, Transition Loss 0.6769763827323914, Classifier Loss 0.5061190128326416, Total Loss 184.68568420410156\n",
      "0: Encoding Loss 22.046459197998047, Transition Loss 1.234830617904663, Classifier Loss 0.4905032813549042, Total Loss 181.82301330566406\n",
      "0: Encoding Loss 21.94761848449707, Transition Loss 0.8038265109062195, Classifier Loss 0.5018509030342102, Total Loss 182.1923370361328\n",
      "0: Encoding Loss 19.928926467895508, Transition Loss 0.3395141065120697, Classifier Loss 0.5023846626281738, Total Loss 169.9478302001953\n",
      "0: Encoding Loss 21.324626922607422, Transition Loss 1.277392029762268, Classifier Loss 0.5062547922134399, Total Loss 179.08419799804688\n",
      "0: Encoding Loss 22.26856231689453, Transition Loss 0.8464869260787964, Classifier Loss 0.5179731845855713, Total Loss 185.74728393554688\n",
      "0: Encoding Loss 19.69940185546875, Transition Loss 0.6781824231147766, Classifier Loss 0.501361608505249, Total Loss 168.60385131835938\n",
      "0: Encoding Loss 21.333049774169922, Transition Loss 0.6544528007507324, Classifier Loss 0.49806058406829834, Total Loss 178.06614685058594\n",
      "0: Encoding Loss 21.44853401184082, Transition Loss 0.7914535999298096, Classifier Loss 0.5484054684638977, Total Loss 183.84832763671875\n",
      "0: Encoding Loss 20.130983352661133, Transition Loss 0.5226713418960571, Classifier Loss 0.5095916390419006, Total Loss 171.9541473388672\n",
      "0: Encoding Loss 21.265731811523438, Transition Loss 0.9608948826789856, Classifier Loss 0.4885372519493103, Total Loss 176.8324737548828\n",
      "0: Encoding Loss 21.520267486572266, Transition Loss 1.1926099061965942, Classifier Loss 0.49395108222961426, Total Loss 178.9937744140625\n",
      "0: Encoding Loss 21.876855850219727, Transition Loss 1.0616087913513184, Classifier Loss 0.5289438366889954, Total Loss 184.5801544189453\n",
      "0: Encoding Loss 22.2281551361084, Transition Loss 1.412071704864502, Classifier Loss 0.46987757086753845, Total Loss 180.92153930664062\n",
      "0: Encoding Loss 21.01949691772461, Transition Loss 0.7686538100242615, Classifier Loss 0.4743320643901825, Total Loss 173.857666015625\n",
      "0: Encoding Loss 21.35171127319336, Transition Loss 1.560858964920044, Classifier Loss 0.529543399810791, Total Loss 181.68894958496094\n",
      "0: Encoding Loss 20.79344940185547, Transition Loss 0.9957227110862732, Classifier Loss 0.5259871482849121, Total Loss 177.75770568847656\n",
      "0: Encoding Loss 19.2390079498291, Transition Loss 0.15470662713050842, Classifier Loss 0.4970502257347107, Total Loss 165.20095825195312\n",
      "0: Encoding Loss 23.569011688232422, Transition Loss 1.6432650089263916, Classifier Loss 0.5095162391662598, Total Loss 193.0229949951172\n",
      "0: Encoding Loss 21.519214630126953, Transition Loss 0.635134220123291, Classifier Loss 0.4965881407260895, Total Loss 179.02816772460938\n",
      "0: Encoding Loss 21.035541534423828, Transition Loss 1.1171727180480957, Classifier Loss 0.49080541729927063, Total Loss 175.74066162109375\n",
      "0: Encoding Loss 21.784408569335938, Transition Loss 0.8103090524673462, Classifier Loss 0.5073949098587036, Total Loss 181.7700653076172\n",
      "0: Encoding Loss 20.374982833862305, Transition Loss 0.8692367672920227, Classifier Loss 0.5005462169647217, Total Loss 172.6522216796875\n",
      "0: Encoding Loss 21.039077758789062, Transition Loss 0.6834644675254822, Classifier Loss 0.4799948036670685, Total Loss 174.50733947753906\n",
      "0: Encoding Loss 20.767292022705078, Transition Loss 1.5918775796890259, Classifier Loss 0.4814361333847046, Total Loss 173.3841094970703\n",
      "0: Encoding Loss 20.737205505371094, Transition Loss 0.19640198349952698, Classifier Loss 0.4858717918395996, Total Loss 173.0889892578125\n",
      "0: Encoding Loss 20.502153396606445, Transition Loss 0.5556296110153198, Classifier Loss 0.476032555103302, Total Loss 170.8384246826172\n",
      "0: Encoding Loss 19.96337890625, Transition Loss 0.8169460892677307, Classifier Loss 0.4808809459209442, Total Loss 168.19515991210938\n",
      "0: Encoding Loss 20.66533660888672, Transition Loss 0.6292005777359009, Classifier Loss 0.47334253787994385, Total Loss 171.5779571533203\n",
      "0: Encoding Loss 21.173526763916016, Transition Loss 2.396432638168335, Classifier Loss 0.5362582206726074, Total Loss 181.6255645751953\n",
      "0: Encoding Loss 20.60836410522461, Transition Loss 1.4636160135269165, Classifier Loss 0.49752146005630493, Total Loss 173.98779296875\n",
      "0: Encoding Loss 19.23075294494629, Transition Loss 0.31519871950149536, Classifier Loss 0.5139902830123901, Total Loss 166.90963745117188\n",
      "0: Encoding Loss 20.816593170166016, Transition Loss 0.3119492828845978, Classifier Loss 0.46664994955062866, Total Loss 171.6893310546875\n",
      "0: Encoding Loss 21.37656021118164, Transition Loss 1.4076566696166992, Classifier Loss 0.5268169641494751, Total Loss 181.50413513183594\n",
      "0: Encoding Loss 21.137741088867188, Transition Loss 0.8398279547691345, Classifier Loss 0.4830450117588043, Total Loss 175.46688842773438\n",
      "0: Encoding Loss 20.809589385986328, Transition Loss 2.3271307945251465, Classifier Loss 0.48946183919906616, Total Loss 174.73455810546875\n",
      "0: Encoding Loss 20.1894588470459, Transition Loss 1.3116281032562256, Classifier Loss 0.4693698287010193, Total Loss 168.598388671875\n",
      "0: Encoding Loss 17.802284240722656, Transition Loss -0.15902723371982574, Classifier Loss 0.4688265323638916, Total Loss 153.69630432128906\n",
      "0: Encoding Loss 20.66151237487793, Transition Loss 0.5815747976303101, Classifier Loss 0.4765275716781616, Total Loss 171.85446166992188\n",
      "0: Encoding Loss 21.051834106445312, Transition Loss 0.7011626958847046, Classifier Loss 0.48536550998687744, Total Loss 175.12803649902344\n",
      "0: Encoding Loss 21.41482162475586, Transition Loss 0.7746152877807617, Classifier Loss 0.4661567807197571, Total Loss 175.41445922851562\n",
      "0: Encoding Loss 20.201629638671875, Transition Loss 0.5746312141418457, Classifier Loss 0.4926457405090332, Total Loss 170.7042236328125\n",
      "0: Encoding Loss 21.365306854248047, Transition Loss 1.4525038003921509, Classifier Loss 0.49987632036209106, Total Loss 178.76048278808594\n",
      "0: Encoding Loss 21.25713348388672, Transition Loss 1.947711706161499, Classifier Loss 0.5398094654083252, Total Loss 182.30284118652344\n",
      "0: Encoding Loss 22.436824798583984, Transition Loss 1.6384519338607788, Classifier Loss 0.44557344913482666, Total Loss 179.83367919921875\n",
      "0: Encoding Loss 20.9013614654541, Transition Loss 1.189983606338501, Classifier Loss 0.4841609001159668, Total Loss 174.30026245117188\n",
      "0: Encoding Loss 22.635406494140625, Transition Loss 1.049845576286316, Classifier Loss 0.4828130006790161, Total Loss 184.513671875\n",
      "0: Encoding Loss 21.20902442932129, Transition Loss 1.06709623336792, Classifier Loss 0.500828206539154, Total Loss 177.76380920410156\n",
      "0: Encoding Loss 21.337139129638672, Transition Loss 1.2029927968978882, Classifier Loss 0.48603424429893494, Total Loss 177.1074676513672\n",
      "0: Encoding Loss 21.671871185302734, Transition Loss 0.7256217002868652, Classifier Loss 0.505290687084198, Total Loss 180.85055541992188\n",
      "0: Encoding Loss 21.450302124023438, Transition Loss 0.5485649704933167, Classifier Loss 0.4767335057258606, Total Loss 176.59458923339844\n",
      "0: Encoding Loss 21.794605255126953, Transition Loss 0.8971297740936279, Classifier Loss 0.48995161056518555, Total Loss 180.1216583251953\n",
      "0: Encoding Loss 19.866044998168945, Transition Loss 1.3324007987976074, Classifier Loss 0.4976115822792053, Total Loss 169.49038696289062\n",
      "0: Encoding Loss 21.627613067626953, Transition Loss 1.094408631324768, Classifier Loss 0.5030004978179932, Total Loss 180.5034942626953\n",
      "0: Encoding Loss 22.284292221069336, Transition Loss 1.6305840015411377, Classifier Loss 0.5011866092681885, Total Loss 184.47666931152344\n",
      "0: Encoding Loss 21.350086212158203, Transition Loss 1.197532296180725, Classifier Loss 0.4857650697231293, Total Loss 177.1560516357422\n",
      "0: Encoding Loss 19.554767608642578, Transition Loss 0.4357970654964447, Classifier Loss 0.4542534649372101, Total Loss 162.9282684326172\n",
      "0: Encoding Loss 21.038715362548828, Transition Loss 0.9225426912307739, Classifier Loss 0.5270836353302002, Total Loss 179.30967712402344\n",
      "0: Encoding Loss 21.453472137451172, Transition Loss 1.8204116821289062, Classifier Loss 0.4787960350513458, Total Loss 177.32861328125\n",
      "0: Encoding Loss 20.86423110961914, Transition Loss 1.4802498817443848, Classifier Loss 0.4537031948566437, Total Loss 171.14781188964844\n",
      "0: Encoding Loss 21.456979751586914, Transition Loss 1.4771572351455688, Classifier Loss 0.46876364946365356, Total Loss 176.20912170410156\n",
      "0: Encoding Loss 20.770713806152344, Transition Loss 1.6269875764846802, Classifier Loss 0.5019341111183167, Total Loss 175.468505859375\n",
      "0: Encoding Loss 19.778480529785156, Transition Loss -0.16816850006580353, Classifier Loss 0.5065016150474548, Total Loss 169.32098388671875\n",
      "0: Encoding Loss 20.661828994750977, Transition Loss 1.2786589860916138, Classifier Loss 0.48943620920181274, Total Loss 173.42605590820312\n",
      "0: Encoding Loss 21.406864166259766, Transition Loss 0.844261646270752, Classifier Loss 0.4839421510696411, Total Loss 177.17311096191406\n",
      "0: Encoding Loss 21.517663955688477, Transition Loss 1.6833616495132446, Classifier Loss 0.4805956482887268, Total Loss 177.83889770507812\n",
      "0: Encoding Loss 19.768444061279297, Transition Loss 0.7509492635726929, Classifier Loss 0.4496774971485138, Total Loss 163.87881469726562\n",
      "0: Encoding Loss 20.04046058654785, Transition Loss 1.5505175590515137, Classifier Loss 0.4815296530723572, Total Loss 169.0159454345703\n",
      "0: Encoding Loss 20.992319107055664, Transition Loss 1.6068165302276611, Classifier Loss 0.46711477637290955, Total Loss 173.30813598632812\n",
      "0: Encoding Loss 21.786916732788086, Transition Loss 1.733059287071228, Classifier Loss 0.5072616338729858, Total Loss 182.14089965820312\n",
      "0: Encoding Loss 21.58139419555664, Transition Loss 1.4641064405441284, Classifier Loss 0.4575664699077606, Total Loss 175.83067321777344\n",
      "0: Encoding Loss 22.20609474182129, Transition Loss 1.9914534091949463, Classifier Loss 0.5183573365211487, Total Loss 185.868896484375\n",
      "0: Encoding Loss 20.536575317382812, Transition Loss 0.5580667853355408, Classifier Loss 0.45867395401000977, Total Loss 169.31007385253906\n",
      "0: Encoding Loss 22.210735321044922, Transition Loss 2.067176103591919, Classifier Loss 0.5227872729301453, Total Loss 186.37002563476562\n",
      "0: Encoding Loss 19.69137191772461, Transition Loss 0.2804029881954193, Classifier Loss 0.4472348093986511, Total Loss 162.98388671875\n",
      "0: Encoding Loss 21.2501220703125, Transition Loss 1.433767318725586, Classifier Loss 0.48510873317718506, Total Loss 176.58511352539062\n",
      "0: Encoding Loss 20.489437103271484, Transition Loss 1.6758143901824951, Classifier Loss 0.49341970682144165, Total Loss 172.94891357421875\n",
      "0: Encoding Loss 20.40325355529785, Transition Loss 0.8264023065567017, Classifier Loss 0.47115209698677063, Total Loss 169.86529541015625\n",
      "0: Encoding Loss 20.893024444580078, Transition Loss 1.6613825559616089, Classifier Loss 0.4500640034675598, Total Loss 171.0290985107422\n",
      "0: Encoding Loss 20.778451919555664, Transition Loss 1.8445227146148682, Classifier Loss 0.4891006648540497, Total Loss 174.31858825683594\n",
      "0: Encoding Loss 20.956148147583008, Transition Loss 1.377015471458435, Classifier Loss 0.46938398480415344, Total Loss 173.22610473632812\n",
      "0: Encoding Loss 21.139930725097656, Transition Loss 0.3465965688228607, Classifier Loss 0.4848920404911041, Total Loss 175.46743774414062\n",
      "0: Encoding Loss 20.117813110351562, Transition Loss 0.6981220841407776, Classifier Loss 0.45569902658462524, Total Loss 166.55604553222656\n",
      "0: Encoding Loss 20.7177734375, Transition Loss 0.7053748369216919, Classifier Loss 0.46945518255233765, Total Loss 171.53431701660156\n",
      "0: Encoding Loss 21.109329223632812, Transition Loss 0.8023180961608887, Classifier Loss 0.467032790184021, Total Loss 173.68017578125\n",
      "0: Encoding Loss 21.06297492980957, Transition Loss 0.9077173471450806, Classifier Loss 0.4521704614162445, Total Loss 171.95797729492188\n",
      "0: Encoding Loss 21.43278694152832, Transition Loss 2.1141374111175537, Classifier Loss 0.4659128189086914, Total Loss 176.03366088867188\n",
      "0: Encoding Loss 20.425525665283203, Transition Loss 0.4487137794494629, Classifier Loss 0.46898072957992554, Total Loss 169.63072204589844\n",
      "0: Encoding Loss 21.954784393310547, Transition Loss 1.804732322692871, Classifier Loss 0.47648078203201294, Total Loss 180.09869384765625\n",
      "0: Encoding Loss 20.004295349121094, Transition Loss 1.0631601810455322, Classifier Loss 0.48056599497795105, Total Loss 168.5076446533203\n",
      "0: Encoding Loss 20.334976196289062, Transition Loss 0.6952663064002991, Classifier Loss 0.5038981437683105, Total Loss 172.67779541015625\n",
      "0: Encoding Loss 20.56180191040039, Transition Loss 1.8180428743362427, Classifier Loss 0.49975067377090454, Total Loss 174.07310485839844\n",
      "0: Encoding Loss 20.363615036010742, Transition Loss 0.41380324959754944, Classifier Loss 0.46337199211120605, Total Loss 168.68441772460938\n",
      "0: Encoding Loss 21.240612030029297, Transition Loss 1.6860097646713257, Classifier Loss 0.46714794635772705, Total Loss 174.8328857421875\n",
      "0: Encoding Loss 20.944168090820312, Transition Loss 1.1086524724960327, Classifier Loss 0.46038803458213806, Total Loss 172.1472930908203\n",
      "0: Encoding Loss 20.24717140197754, Transition Loss 0.3434297442436218, Classifier Loss 0.4762130081653595, Total Loss 169.24171447753906\n",
      "0: Encoding Loss 20.402132034301758, Transition Loss 0.8962309956550598, Classifier Loss 0.44278696179389954, Total Loss 167.04998779296875\n",
      "0: Encoding Loss 22.659067153930664, Transition Loss 1.801709771156311, Classifier Loss 0.49429845809936523, Total Loss 186.1049346923828\n",
      "0: Encoding Loss 19.504711151123047, Transition Loss 0.5032296776771545, Classifier Loss 0.4700142741203308, Total Loss 164.23098754882812\n",
      "0: Encoding Loss 21.587299346923828, Transition Loss 1.8922804594039917, Classifier Loss 0.5493051409721375, Total Loss 185.2112274169922\n",
      "0: Encoding Loss 20.9300537109375, Transition Loss 1.372209906578064, Classifier Loss 0.4701358675956726, Total Loss 173.14280700683594\n",
      "0: Encoding Loss 18.992443084716797, Transition Loss 0.6305816173553467, Classifier Loss 0.48465409874916077, Total Loss 162.67230224609375\n",
      "0: Encoding Loss 20.676679611206055, Transition Loss 1.934390902519226, Classifier Loss 0.45893773436546326, Total Loss 170.7276153564453\n",
      "0: Encoding Loss 22.303478240966797, Transition Loss 2.372277021408081, Classifier Loss 0.5151935815811157, Total Loss 186.28915405273438\n",
      "0: Encoding Loss 20.775527954101562, Transition Loss 1.189321517944336, Classifier Loss 0.46952998638153076, Total Loss 172.08189392089844\n",
      "0: Encoding Loss 20.671119689941406, Transition Loss 0.4825781583786011, Classifier Loss 0.4449464678764343, Total Loss 168.7144012451172\n",
      "0: Encoding Loss 20.614116668701172, Transition Loss 0.8750911951065063, Classifier Loss 0.4815305471420288, Total Loss 172.18780517578125\n",
      "0: Encoding Loss 21.42112922668457, Transition Loss 1.210381031036377, Classifier Loss 0.5163439512252808, Total Loss 180.64532470703125\n",
      "0: Encoding Loss 20.861221313476562, Transition Loss 1.125687837600708, Classifier Loss 0.4169715642929077, Total Loss 167.31475830078125\n",
      "0: Encoding Loss 20.316577911376953, Transition Loss 1.0163497924804688, Classifier Loss 0.5125294923782349, Total Loss 173.5589599609375\n",
      "0: Encoding Loss 19.552684783935547, Transition Loss 1.2823281288146973, Classifier Loss 0.4752052426338196, Total Loss 165.3495635986328\n",
      "0: Encoding Loss 20.868907928466797, Transition Loss 1.7054765224456787, Classifier Loss 0.4650548994541168, Total Loss 172.40113830566406\n",
      "0: Encoding Loss 20.941152572631836, Transition Loss 2.652174711227417, Classifier Loss 0.5072500109672546, Total Loss 177.4327850341797\n",
      "0: Encoding Loss 21.065998077392578, Transition Loss 0.9994707703590393, Classifier Loss 0.4913403391838074, Total Loss 175.92982482910156\n",
      "0: Encoding Loss 21.31424331665039, Transition Loss 0.9873384237289429, Classifier Loss 0.4955880045890808, Total Loss 177.8391876220703\n",
      "0: Encoding Loss 21.124671936035156, Transition Loss 1.7786266803741455, Classifier Loss 0.491598904132843, Total Loss 176.619384765625\n",
      "0: Encoding Loss 20.667007446289062, Transition Loss 0.44070929288864136, Classifier Loss 0.4808693528175354, Total Loss 172.26527404785156\n",
      "0: Encoding Loss 20.876293182373047, Transition Loss 1.8299119472503662, Classifier Loss 0.5252000093460083, Total Loss 178.50973510742188\n",
      "0: Encoding Loss 20.287883758544922, Transition Loss 1.0245213508605957, Classifier Loss 0.44641947746276855, Total Loss 166.77906799316406\n",
      "0: Encoding Loss 20.3223819732666, Transition Loss 0.4160788953304291, Classifier Loss 0.4798774719238281, Total Loss 170.08847045898438\n",
      "0: Encoding Loss 21.028865814208984, Transition Loss 2.6851327419281006, Classifier Loss 0.5038328766822815, Total Loss 177.6305389404297\n",
      "0: Encoding Loss 18.95618438720703, Transition Loss 1.4701385498046875, Classifier Loss 0.49025189876556396, Total Loss 163.35035705566406\n",
      "0: Encoding Loss 19.97684097290039, Transition Loss 1.8713274002075195, Classifier Loss 0.48450058698654175, Total Loss 169.05963134765625\n",
      "0: Encoding Loss 20.908235549926758, Transition Loss 1.036354422569275, Classifier Loss 0.5045289397239685, Total Loss 176.3168487548828\n",
      "0: Encoding Loss 21.38739585876465, Transition Loss 1.6760327816009521, Classifier Loss 0.50448077917099, Total Loss 179.44287109375\n",
      "0: Encoding Loss 20.830007553100586, Transition Loss 2.818669557571411, Classifier Loss 0.4720485806465149, Total Loss 173.3123779296875\n",
      "0: Encoding Loss 19.980897903442383, Transition Loss 1.8060367107391357, Classifier Loss 0.488491028547287, Total Loss 169.4569091796875\n",
      "0: Encoding Loss 21.108421325683594, Transition Loss 1.5398080348968506, Classifier Loss 0.49918225407600403, Total Loss 177.18467712402344\n",
      "0: Encoding Loss 20.757593154907227, Transition Loss 1.1920686960220337, Classifier Loss 0.4760896563529968, Total Loss 172.63134765625\n",
      "0: Encoding Loss 21.615629196166992, Transition Loss 1.9270448684692383, Classifier Loss 0.5013960599899292, Total Loss 180.6042022705078\n",
      "0: Encoding Loss 20.12019157409668, Transition Loss 0.30150967836380005, Classifier Loss 0.4632747173309326, Total Loss 167.1692352294922\n",
      "0: Encoding Loss 21.557697296142578, Transition Loss 2.3429627418518066, Classifier Loss 0.5098506808280945, Total Loss 181.26844787597656\n",
      "0: Encoding Loss 20.567432403564453, Transition Loss 1.5924975872039795, Classifier Loss 0.45699000358581543, Total Loss 169.74058532714844\n",
      "0: Encoding Loss 20.615720748901367, Transition Loss 2.1532816886901855, Classifier Loss 0.4816155433654785, Total Loss 172.71719360351562\n",
      "0: Encoding Loss 19.87945556640625, Transition Loss 2.229616641998291, Classifier Loss 0.5123945474624634, Total Loss 171.40805053710938\n",
      "0: Encoding Loss 18.52245330810547, Transition Loss 0.3242805600166321, Classifier Loss 0.43899664282798767, Total Loss 155.1641082763672\n",
      "0: Encoding Loss 21.79604721069336, Transition Loss 4.382553577423096, Classifier Loss 0.42529022693634033, Total Loss 175.05833435058594\n",
      "0: Encoding Loss 21.30405044555664, Transition Loss 2.166766405105591, Classifier Loss 0.46522554755210876, Total Loss 175.21356201171875\n",
      "0: Encoding Loss 20.656986236572266, Transition Loss 1.3773468732833862, Classifier Loss 0.4336812496185303, Total Loss 167.86097717285156\n",
      "0: Encoding Loss 20.231191635131836, Transition Loss 1.5389617681503296, Classifier Loss 0.4377637505531311, Total Loss 165.77911376953125\n",
      "0: Encoding Loss 20.319913864135742, Transition Loss 2.162639617919922, Classifier Loss 0.45769789814949036, Total Loss 168.5543212890625\n",
      "0: Encoding Loss 21.577939987182617, Transition Loss 1.235975980758667, Classifier Loss 0.4661650061607361, Total Loss 176.5785369873047\n",
      "0: Encoding Loss 21.096052169799805, Transition Loss 1.2022733688354492, Classifier Loss 0.49770933389663696, Total Loss 176.8281707763672\n",
      "0: Encoding Loss 21.269956588745117, Transition Loss 1.605628490447998, Classifier Loss 0.49136584997177124, Total Loss 177.39859008789062\n",
      "0: Encoding Loss 21.47354507446289, Transition Loss 2.4271321296691895, Classifier Loss 0.4993748068809509, Total Loss 179.74961853027344\n",
      "0: Encoding Loss 20.315109252929688, Transition Loss 0.6856940388679504, Classifier Loss 0.44205334782600403, Total Loss 166.37026977539062\n",
      "0: Encoding Loss 20.838045120239258, Transition Loss 1.1473498344421387, Classifier Loss 0.4896523058414459, Total Loss 174.4524383544922\n",
      "0: Encoding Loss 20.765995025634766, Transition Loss 2.029057502746582, Classifier Loss 0.5115610957145691, Total Loss 176.56370544433594\n",
      "0: Encoding Loss 20.890140533447266, Transition Loss 1.3245903253555298, Classifier Loss 0.46292099356651306, Total Loss 172.1627655029297\n",
      "0: Encoding Loss 19.755277633666992, Transition Loss 1.1827419996261597, Classifier Loss 0.494324654340744, Total Loss 168.43724060058594\n",
      "0: Encoding Loss 20.245182037353516, Transition Loss 2.358644962310791, Classifier Loss 0.4711530804634094, Total Loss 169.52984619140625\n",
      "0: Encoding Loss 19.77025604248047, Transition Loss 1.441396951675415, Classifier Loss 0.441356897354126, Total Loss 163.3337860107422\n",
      "0: Encoding Loss 19.553180694580078, Transition Loss 1.4815949201583862, Classifier Loss 0.4539157450199127, Total Loss 163.3032989501953\n",
      "0: Encoding Loss 22.23337173461914, Transition Loss 1.1597118377685547, Classifier Loss 0.4571746289730072, Total Loss 179.5815887451172\n",
      "0: Encoding Loss 19.71323013305664, Transition Loss 1.4124016761779785, Classifier Loss 0.45813634991645813, Total Loss 164.65797424316406\n",
      "0: Encoding Loss 20.269800186157227, Transition Loss 1.790522575378418, Classifier Loss 0.4471142590045929, Total Loss 167.0464324951172\n",
      "0: Encoding Loss 21.367816925048828, Transition Loss 2.5425703525543213, Classifier Loss 0.4810454249382019, Total Loss 177.32847595214844\n",
      "0: Encoding Loss 20.842533111572266, Transition Loss 0.8686814904212952, Classifier Loss 0.4716128408908844, Total Loss 172.56396484375\n",
      "0: Encoding Loss 21.357837677001953, Transition Loss 1.7761849164962769, Classifier Loss 0.4500255882740021, Total Loss 173.86007690429688\n",
      "0: Encoding Loss 20.915897369384766, Transition Loss 2.3382363319396973, Classifier Loss 0.46690404415130615, Total Loss 173.12107849121094\n",
      "0: Encoding Loss 20.545921325683594, Transition Loss 1.7254736423492432, Classifier Loss 0.45615196228027344, Total Loss 169.58091735839844\n",
      "0: Encoding Loss 21.519750595092773, Transition Loss 2.224914073944092, Classifier Loss 0.49178558588027954, Total Loss 179.18704223632812\n",
      "0: Encoding Loss 20.108829498291016, Transition Loss 1.0300289392471313, Classifier Loss 0.43568897247314453, Total Loss 164.63389587402344\n",
      "0: Encoding Loss 19.844959259033203, Transition Loss 0.8831392526626587, Classifier Loss 0.44491541385650635, Total Loss 163.91455078125\n",
      "0: Encoding Loss 21.525798797607422, Transition Loss 2.6911308765411377, Classifier Loss 0.4574788212776184, Total Loss 175.9791259765625\n",
      "0: Encoding Loss 21.957368850708008, Transition Loss 1.5237091779708862, Classifier Loss 0.46984684467315674, Total Loss 179.33837890625\n",
      "0: Encoding Loss 21.037403106689453, Transition Loss 2.151949644088745, Classifier Loss 0.5425995588302612, Total Loss 181.34515380859375\n",
      "0: Encoding Loss 19.617708206176758, Transition Loss 1.4359936714172363, Classifier Loss 0.4706352651119232, Total Loss 165.34417724609375\n",
      "0: Encoding Loss 22.515810012817383, Transition Loss 1.5091745853424072, Classifier Loss 0.4680069386959076, Total Loss 182.4992218017578\n",
      "0: Encoding Loss 20.613439559936523, Transition Loss 1.5209449529647827, Classifier Loss 0.4409877061843872, Total Loss 168.38780212402344\n",
      "0: Encoding Loss 21.526912689208984, Transition Loss 1.462311029434204, Classifier Loss 0.5212886929512024, Total Loss 181.87527465820312\n",
      "0: Encoding Loss 21.370155334472656, Transition Loss 2.659641742706299, Classifier Loss 0.48440665006637573, Total Loss 177.72544860839844\n",
      "0: Encoding Loss 21.651153564453125, Transition Loss 2.6422598361968994, Classifier Loss 0.4848138093948364, Total Loss 179.44520568847656\n",
      "0: Encoding Loss 19.1179256439209, Transition Loss 3.217420816421509, Classifier Loss 0.4781789183616638, Total Loss 163.8124237060547\n",
      "0: Encoding Loss 20.4416446685791, Transition Loss 1.7905129194259644, Classifier Loss 0.4584503769874573, Total Loss 169.2111053466797\n",
      "0: Encoding Loss 20.6171875, Transition Loss 2.0019760131835938, Classifier Loss 0.4433607757091522, Total Loss 168.8400115966797\n",
      "0: Encoding Loss 20.4196834564209, Transition Loss 1.6161004304885864, Classifier Loss 0.4790072441101074, Total Loss 171.0652618408203\n",
      "0: Encoding Loss 20.52179718017578, Transition Loss 1.5839779376983643, Classifier Loss 0.4502599835395813, Total Loss 168.79037475585938\n",
      "0: Encoding Loss 21.01106071472168, Transition Loss 1.8762869834899902, Classifier Loss 0.42573732137680054, Total Loss 169.390625\n",
      "0: Encoding Loss 19.013898849487305, Transition Loss 1.0173588991165161, Classifier Loss 0.4544425904750824, Total Loss 159.93458557128906\n",
      "0: Encoding Loss 21.41558837890625, Transition Loss 2.4523041248321533, Classifier Loss 0.4836875796318054, Total Loss 177.8432159423828\n",
      "0: Encoding Loss 20.561182022094727, Transition Loss 3.2169644832611084, Classifier Loss 0.5348415374755859, Total Loss 178.13804626464844\n",
      "0: Encoding Loss 19.577482223510742, Transition Loss 1.697594165802002, Classifier Loss 0.4225921034812927, Total Loss 160.40313720703125\n",
      "0: Encoding Loss 20.97531509399414, Transition Loss 2.7885515689849854, Classifier Loss 0.5390201807022095, Total Loss 180.86932373046875\n",
      "0: Encoding Loss 21.0227108001709, Transition Loss 2.5275681018829346, Classifier Loss 0.4687361717224121, Total Loss 174.0209197998047\n",
      "0: Encoding Loss 20.776325225830078, Transition Loss 2.1763195991516113, Classifier Loss 0.4845936894416809, Total Loss 173.98785400390625\n",
      "0: Encoding Loss 21.44869041442871, Transition Loss 3.0930278301239014, Classifier Loss 0.494858980178833, Total Loss 179.41526794433594\n",
      "0: Encoding Loss 20.025218963623047, Transition Loss 1.416351556777954, Classifier Loss 0.49302470684051514, Total Loss 170.0203399658203\n",
      "0: Encoding Loss 20.188913345336914, Transition Loss 1.3113486766815186, Classifier Loss 0.44613319635391235, Total Loss 166.27133178710938\n",
      "0: Encoding Loss 20.741703033447266, Transition Loss 1.5675885677337646, Classifier Loss 0.45698267221450806, Total Loss 170.7755126953125\n",
      "0: Encoding Loss 21.400043487548828, Transition Loss 1.3511534929275513, Classifier Loss 0.46972420811653137, Total Loss 175.91314697265625\n",
      "0: Encoding Loss 21.761245727539062, Transition Loss 0.965697169303894, Classifier Loss 0.44751793146133423, Total Loss 175.70555114746094\n",
      "0: Encoding Loss 19.47903060913086, Transition Loss 0.6639600992202759, Classifier Loss 0.43648314476013184, Total Loss 160.7880859375\n",
      "0: Encoding Loss 21.609838485717773, Transition Loss 2.330066204071045, Classifier Loss 0.44095081090927124, Total Loss 174.68614196777344\n",
      "0: Encoding Loss 20.082035064697266, Transition Loss 0.7470735311508179, Classifier Loss 0.43792903423309326, Total Loss 164.5839385986328\n",
      "0: Encoding Loss 20.97599983215332, Transition Loss 1.4313551187515259, Classifier Loss 0.4602503776550293, Total Loss 172.45358276367188\n",
      "0: Encoding Loss 18.64396095275879, Transition Loss 1.526336908340454, Classifier Loss 0.416150838136673, Total Loss 154.08938598632812\n",
      "0: Encoding Loss 21.064586639404297, Transition Loss 2.9666945934295654, Classifier Loss 0.4824461042881012, Total Loss 175.81881713867188\n",
      "0: Encoding Loss 21.0108585357666, Transition Loss 3.1802752017974854, Classifier Loss 0.4635911285877228, Total Loss 173.69638061523438\n",
      "0: Encoding Loss 19.277568817138672, Transition Loss 1.8263808488845825, Classifier Loss 0.46986299753189087, Total Loss 163.38226318359375\n",
      "0: Encoding Loss 19.407602310180664, Transition Loss 1.717389464378357, Classifier Loss 0.4501042068004608, Total Loss 162.1429901123047\n",
      "0: Encoding Loss 21.667312622070312, Transition Loss 1.9628502130508423, Classifier Loss 0.45376259088516235, Total Loss 176.16526794433594\n",
      "0: Encoding Loss 20.075756072998047, Transition Loss 2.8625946044921875, Classifier Loss 0.46080467104911804, Total Loss 167.68003845214844\n",
      "0: Encoding Loss 20.916942596435547, Transition Loss 2.553720235824585, Classifier Loss 0.49892860651016235, Total Loss 176.416015625\n",
      "0: Encoding Loss 19.316577911376953, Transition Loss 2.4032206535339355, Classifier Loss 0.4279806613922119, Total Loss 159.65882873535156\n",
      "0: Encoding Loss 18.70332145690918, Transition Loss 1.416680097579956, Classifier Loss 0.47872546315193176, Total Loss 160.65914916992188\n",
      "0: Encoding Loss 19.809864044189453, Transition Loss 1.8645920753479004, Classifier Loss 0.49912136793136597, Total Loss 169.51715087890625\n",
      "0: Encoding Loss 21.385351181030273, Transition Loss 1.6323108673095703, Classifier Loss 0.4732496738433838, Total Loss 176.29000854492188\n",
      "0: Encoding Loss 18.92646598815918, Transition Loss 1.5581467151641846, Classifier Loss 0.43976128101348877, Total Loss 158.15818786621094\n",
      "0: Encoding Loss 17.96920394897461, Transition Loss 1.7239253520965576, Classifier Loss 0.43886929750442505, Total Loss 152.39173889160156\n",
      "0: Encoding Loss 20.286657333374023, Transition Loss 3.5255589485168457, Classifier Loss 0.4768275022506714, Total Loss 170.8129119873047\n",
      "0: Encoding Loss 19.944683074951172, Transition Loss 2.0309407711029053, Classifier Loss 0.4785400331020355, Total Loss 168.33448791503906\n",
      "0: Encoding Loss 19.591318130493164, Transition Loss 2.344360113143921, Classifier Loss 0.449401319026947, Total Loss 163.42578125\n",
      "0: Encoding Loss 20.159618377685547, Transition Loss 2.5567452907562256, Classifier Loss 0.44226133823394775, Total Loss 166.20655822753906\n",
      "0: Encoding Loss 19.336515426635742, Transition Loss 1.93411386013031, Classifier Loss 0.44295942783355713, Total Loss 161.08868408203125\n",
      "0: Encoding Loss 19.970029830932617, Transition Loss 3.883934497833252, Classifier Loss 0.5048461556434631, Total Loss 171.85836791992188\n",
      "0: Encoding Loss 21.621898651123047, Transition Loss 1.9172250032424927, Classifier Loss 0.4803362488746643, Total Loss 178.53192138671875\n",
      "0: Encoding Loss 20.207576751708984, Transition Loss 2.255734920501709, Classifier Loss 0.46835002303123474, Total Loss 168.98277282714844\n",
      "0: Encoding Loss 18.356351852416992, Transition Loss 1.662562370300293, Classifier Loss 0.4561130404472351, Total Loss 156.41444396972656\n",
      "0: Encoding Loss 18.786771774291992, Transition Loss 1.4264404773712158, Classifier Loss 0.47014319896698, Total Loss 160.30552673339844\n",
      "0: Encoding Loss 21.045835494995117, Transition Loss 4.189065933227539, Classifier Loss 0.5190974473953247, Total Loss 179.86038208007812\n",
      "0: Encoding Loss 22.241729736328125, Transition Loss 1.8508729934692383, Classifier Loss 0.4389685392379761, Total Loss 178.08758544921875\n",
      "0: Encoding Loss 20.55381202697754, Transition Loss 1.7189167737960815, Classifier Loss 0.4455874562263489, Total Loss 168.56918334960938\n",
      "0: Encoding Loss 20.746673583984375, Transition Loss 2.438382387161255, Classifier Loss 0.4424275755882263, Total Loss 169.6981658935547\n",
      "0: Encoding Loss 21.542272567749023, Transition Loss 3.617779493331909, Classifier Loss 0.4768441915512085, Total Loss 178.3851776123047\n",
      "0: Encoding Loss 19.880456924438477, Transition Loss 3.1496005058288574, Classifier Loss 0.4572065770626068, Total Loss 166.26324462890625\n",
      "0: Encoding Loss 19.577287673950195, Transition Loss 3.2635109424591064, Classifier Loss 0.465928852558136, Total Loss 165.3620147705078\n",
      "0: Encoding Loss 20.614635467529297, Transition Loss 1.6090898513793945, Classifier Loss 0.4429182708263397, Total Loss 168.62327575683594\n",
      "0: Encoding Loss 22.00737190246582, Transition Loss 2.8965117931365967, Classifier Loss 0.48482295870780945, Total Loss 181.6851348876953\n",
      "0: Encoding Loss 20.080352783203125, Transition Loss 2.1776015758514404, Classifier Loss 0.47259950637817383, Total Loss 168.6131134033203\n",
      "0: Encoding Loss 20.278600692749023, Transition Loss 0.9212242364883423, Classifier Loss 0.46409910917282104, Total Loss 168.4499969482422\n",
      "0: Encoding Loss 17.927404403686523, Transition Loss 1.3545210361480713, Classifier Loss 0.4244684875011444, Total Loss 150.55308532714844\n",
      "0: Encoding Loss 20.380271911621094, Transition Loss 2.8255505561828613, Classifier Loss 0.5183742046356201, Total Loss 175.24928283691406\n",
      "0: Encoding Loss 21.301349639892578, Transition Loss 1.9727588891983032, Classifier Loss 0.45287489891052246, Total Loss 173.8846893310547\n",
      "0: Encoding Loss 20.370616912841797, Transition Loss 2.509190082550049, Classifier Loss 0.4953485429286957, Total Loss 172.76223754882812\n",
      "0: Encoding Loss 20.312137603759766, Transition Loss 2.2565362453460693, Classifier Loss 0.4944492280483246, Total Loss 172.22036743164062\n",
      "0: Encoding Loss 20.380897521972656, Transition Loss 2.428905487060547, Classifier Loss 0.4517112970352173, Total Loss 168.42808532714844\n",
      "0: Encoding Loss 20.78832244873047, Transition Loss 3.3411865234375, Classifier Loss 0.47011804580688477, Total Loss 173.07821655273438\n",
      "0: Encoding Loss 19.716012954711914, Transition Loss 2.5128674507141113, Classifier Loss 0.46633297204971313, Total Loss 165.9345245361328\n",
      "0: Encoding Loss 19.6186466217041, Transition Loss 3.0666840076446533, Classifier Loss 0.442135214805603, Total Loss 163.15206909179688\n",
      "0: Encoding Loss 20.482563018798828, Transition Loss 1.621531367301941, Classifier Loss 0.4646414518356323, Total Loss 170.0081329345703\n",
      "0: Encoding Loss 20.99941635131836, Transition Loss 2.0765271186828613, Classifier Loss 0.48520588874816895, Total Loss 175.3477020263672\n",
      "0: Encoding Loss 21.025554656982422, Transition Loss 2.175687789916992, Classifier Loss 0.4067964553833008, Total Loss 167.7032470703125\n",
      "0: Encoding Loss 19.292377471923828, Transition Loss 0.8842141032218933, Classifier Loss 0.46079692244529724, Total Loss 162.18763732910156\n",
      "0: Encoding Loss 19.556053161621094, Transition Loss 1.8447532653808594, Classifier Loss 0.5077931880950928, Total Loss 168.85354614257812\n",
      "0: Encoding Loss 19.78619384765625, Transition Loss 3.380732536315918, Classifier Loss 0.522289514541626, Total Loss 172.2984161376953\n",
      "0: Encoding Loss 18.500045776367188, Transition Loss 1.5076100826263428, Classifier Loss 0.4360530376434326, Total Loss 155.20863342285156\n",
      "0: Encoding Loss 20.800373077392578, Transition Loss 2.624660015106201, Classifier Loss 0.46685945987701416, Total Loss 172.53805541992188\n",
      "0: Encoding Loss 20.054027557373047, Transition Loss 2.01629638671875, Classifier Loss 0.431203693151474, Total Loss 164.25106811523438\n",
      "0: Encoding Loss 20.462875366210938, Transition Loss 1.9567655324935913, Classifier Loss 0.4295479357242584, Total Loss 166.51475524902344\n",
      "0: Encoding Loss 20.58114242553711, Transition Loss 1.8738194704055786, Classifier Loss 0.472438782453537, Total Loss 171.4802703857422\n",
      "0: Encoding Loss 19.782623291015625, Transition Loss 3.463658571243286, Classifier Loss 0.45382773876190186, Total Loss 165.4639892578125\n",
      "0: Encoding Loss 20.5118408203125, Transition Loss 0.9447586536407471, Classifier Loss 0.424503892660141, Total Loss 165.8993377685547\n",
      "0: Encoding Loss 20.29554557800293, Transition Loss 2.5277371406555176, Classifier Loss 0.4610089361667633, Total Loss 168.88526916503906\n",
      "0: Encoding Loss 18.442733764648438, Transition Loss 3.7082629203796387, Classifier Loss 0.45690470933914185, Total Loss 157.83018493652344\n",
      "0: Encoding Loss 19.179872512817383, Transition Loss 1.7999873161315918, Classifier Loss 0.40806543827056885, Total Loss 156.6057891845703\n",
      "0: Encoding Loss 20.45053482055664, Transition Loss 1.869578242301941, Classifier Loss 0.4563831090927124, Total Loss 169.08935546875\n",
      "0: Encoding Loss 19.888931274414062, Transition Loss 0.6173816919326782, Classifier Loss 0.496309757232666, Total Loss 169.21151733398438\n",
      "0: Encoding Loss 20.011079788208008, Transition Loss 1.0792596340179443, Classifier Loss 0.4521554112434387, Total Loss 165.71372985839844\n",
      "0: Encoding Loss 21.015300750732422, Transition Loss 2.9618237018585205, Classifier Loss 0.47998589277267456, Total Loss 175.27511596679688\n",
      "0: Encoding Loss 20.141693115234375, Transition Loss 1.0214762687683105, Classifier Loss 0.4239395260810852, Total Loss 163.65269470214844\n",
      "0: Encoding Loss 20.53358268737793, Transition Loss 2.5559463500976562, Classifier Loss 0.4496833086013794, Total Loss 169.1922149658203\n",
      "0: Encoding Loss 19.4659481048584, Transition Loss 1.9463058710098267, Classifier Loss 0.48638203740119934, Total Loss 166.21241760253906\n",
      "0: Encoding Loss 18.012521743774414, Transition Loss 1.1696956157684326, Classifier Loss 0.4479350745677948, Total Loss 153.33651733398438\n",
      "0: Encoding Loss 19.64624786376953, Transition Loss 5.065145015716553, Classifier Loss 0.4570801258087158, Total Loss 165.611572265625\n",
      "0: Encoding Loss 19.496658325195312, Transition Loss 1.5343282222747803, Classifier Loss 0.4547399878501892, Total Loss 163.0676727294922\n",
      "0: Encoding Loss 19.659603118896484, Transition Loss 2.8137478828430176, Classifier Loss 0.3911368250846863, Total Loss 158.19680786132812\n",
      "0: Encoding Loss 20.121784210205078, Transition Loss 2.249161720275879, Classifier Loss 0.42704373598098755, Total Loss 164.33473205566406\n",
      "0: Encoding Loss 20.435531616210938, Transition Loss 2.599735975265503, Classifier Loss 0.477209210395813, Total Loss 171.3740234375\n",
      "0: Encoding Loss 18.608169555664062, Transition Loss 1.4245725870132446, Classifier Loss 0.4375251233577728, Total Loss 155.9713592529297\n",
      "0: Encoding Loss 20.662006378173828, Transition Loss 1.9738463163375854, Classifier Loss 0.476989209651947, Total Loss 172.4604949951172\n",
      "0: Encoding Loss 20.167312622070312, Transition Loss 2.2343764305114746, Classifier Loss 0.3979474902153015, Total Loss 161.6923828125\n",
      "0: Encoding Loss 20.603923797607422, Transition Loss 4.17391300201416, Classifier Loss 0.46962445974349976, Total Loss 172.2555694580078\n",
      "0: Encoding Loss 20.242416381835938, Transition Loss 3.8221328258514404, Classifier Loss 0.4565262198448181, Total Loss 168.635986328125\n",
      "0: Encoding Loss 20.001789093017578, Transition Loss 2.8238635063171387, Classifier Loss 0.473999559879303, Total Loss 168.5402374267578\n",
      "0: Encoding Loss 19.57184410095215, Transition Loss 1.8536521196365356, Classifier Loss 0.4522573947906494, Total Loss 163.39825439453125\n",
      "0: Encoding Loss 19.555356979370117, Transition Loss 1.853750228881836, Classifier Loss 0.4793773889541626, Total Loss 166.01138305664062\n",
      "0: Encoding Loss 19.261642456054688, Transition Loss 2.903677225112915, Classifier Loss 0.47559189796447754, Total Loss 164.29052734375\n",
      "0: Encoding Loss 18.740215301513672, Transition Loss 3.74135684967041, Classifier Loss 0.44946831464767456, Total Loss 158.88465881347656\n",
      "0: Encoding Loss 18.70253562927246, Transition Loss 2.490396738052368, Classifier Loss 0.4303419291973114, Total Loss 156.2455596923828\n",
      "0: Encoding Loss 19.159107208251953, Transition Loss 0.9865195751190186, Classifier Loss 0.4254106283187866, Total Loss 157.8903045654297\n",
      "0: Encoding Loss 17.811634063720703, Transition Loss 1.3039487600326538, Classifier Loss 0.43935611844062805, Total Loss 151.32699584960938\n",
      "0: Encoding Loss 19.296337127685547, Transition Loss 1.4282827377319336, Classifier Loss 0.44144919514656067, Total Loss 160.4942626953125\n",
      "0: Encoding Loss 19.25265121459961, Transition Loss 1.7548831701278687, Classifier Loss 0.4456201493740082, Total Loss 160.77987670898438\n",
      "0: Encoding Loss 20.73798370361328, Transition Loss 2.5479319095611572, Classifier Loss 0.44769567251205444, Total Loss 170.21665954589844\n",
      "0: Encoding Loss 19.893512725830078, Transition Loss 2.8184027671813965, Classifier Loss 0.4570096731185913, Total Loss 166.1894073486328\n",
      "0: Encoding Loss 19.761367797851562, Transition Loss 1.9308916330337524, Classifier Loss 0.5006755590438843, Total Loss 169.4081268310547\n",
      "0: Encoding Loss 16.91803741455078, Transition Loss 0.9242417216300964, Classifier Loss 0.43932318687438965, Total Loss 145.81024169921875\n",
      "0: Encoding Loss 19.462642669677734, Transition Loss 2.4015004634857178, Classifier Loss 0.44132518768310547, Total Loss 161.86898803710938\n",
      "0: Encoding Loss 19.427631378173828, Transition Loss 1.193223476409912, Classifier Loss 0.43303510546684265, Total Loss 160.34658813476562\n",
      "0: Encoding Loss 19.547527313232422, Transition Loss 1.717502474784851, Classifier Loss 0.443418949842453, Total Loss 162.31407165527344\n",
      "0: Encoding Loss 19.549095153808594, Transition Loss 1.1280943155288696, Classifier Loss 0.44855421781539917, Total Loss 162.60122680664062\n",
      "0: Encoding Loss 19.073657989501953, Transition Loss 0.46311649680137634, Classifier Loss 0.4704154133796692, Total Loss 161.66873168945312\n",
      "0: Encoding Loss 19.770780563354492, Transition Loss 0.7710816860198975, Classifier Loss 0.4384995698928833, Total Loss 162.78306579589844\n",
      "0: Encoding Loss 19.261934280395508, Transition Loss 1.8108896017074585, Classifier Loss 0.44515830278396606, Total Loss 160.81178283691406\n",
      "0: Encoding Loss 19.31373405456543, Transition Loss 1.275078535079956, Classifier Loss 0.4315965175628662, Total Loss 159.5520782470703\n",
      "0: Encoding Loss 19.103090286254883, Transition Loss 1.1158323287963867, Classifier Loss 0.45095109939575195, Total Loss 160.1599884033203\n",
      "0: Encoding Loss 18.149385452270508, Transition Loss 0.5402590036392212, Classifier Loss 0.43906521797180176, Total Loss 153.01895141601562\n",
      "0: Encoding Loss 18.593921661376953, Transition Loss 0.26495110988616943, Classifier Loss 0.4162929952144623, Total Loss 153.29881286621094\n",
      "0: Encoding Loss 18.979537963867188, Transition Loss 1.7903773784637451, Classifier Loss 0.4639712870121002, Total Loss 160.9905242919922\n",
      "0: Encoding Loss 19.423320770263672, Transition Loss 2.7785723209381104, Classifier Loss 0.4781390428543091, Total Loss 165.46527099609375\n",
      "0: Encoding Loss 18.214916229248047, Transition Loss 0.2755458354949951, Classifier Loss 0.44343632459640503, Total Loss 153.74334716796875\n",
      "0: Encoding Loss 20.872974395751953, Transition Loss 1.8513222932815552, Classifier Loss 0.45147621631622314, Total Loss 171.12599182128906\n",
      "0: Encoding Loss 18.438880920410156, Transition Loss 2.2477235794067383, Classifier Loss 0.46825486421585083, Total Loss 158.3578643798828\n",
      "0: Encoding Loss 19.914066314697266, Transition Loss 3.331043243408203, Classifier Loss 0.44447121024131775, Total Loss 165.26393127441406\n",
      "0: Encoding Loss 17.946224212646484, Transition Loss 1.8593261241912842, Classifier Loss 0.4448698163032532, Total Loss 152.90806579589844\n",
      "0: Encoding Loss 19.53154182434082, Transition Loss 0.23746097087860107, Classifier Loss 0.4466986060142517, Total Loss 161.9541015625\n",
      "0: Encoding Loss 18.95669937133789, Transition Loss 0.8940799236297607, Classifier Loss 0.37373608350753784, Total Loss 151.471435546875\n",
      "0: Encoding Loss 18.851543426513672, Transition Loss 2.592556953430176, Classifier Loss 0.47633251547813416, Total Loss 161.779541015625\n",
      "0: Encoding Loss 18.10475730895996, Transition Loss 2.838064670562744, Classifier Loss 0.4887681305408478, Total Loss 158.6405792236328\n",
      "0: Encoding Loss 19.021663665771484, Transition Loss -0.547680139541626, Classifier Loss 0.4124981462955475, Total Loss 155.3795928955078\n",
      "0: Encoding Loss 18.65382957458496, Transition Loss 2.0453238487243652, Classifier Loss 0.47268348932266235, Total Loss 160.00946044921875\n",
      "0: Encoding Loss 18.856199264526367, Transition Loss 3.665435791015625, Classifier Loss 0.40512344241142273, Total Loss 155.11570739746094\n",
      "0: Encoding Loss 18.884489059448242, Transition Loss 4.107498645782471, Classifier Loss 0.5299959182739258, Total Loss 167.9495391845703\n",
      "0: Encoding Loss 19.494077682495117, Transition Loss 4.397017478942871, Classifier Loss 0.4283139109611511, Total Loss 161.55467224121094\n",
      "0: Encoding Loss 19.65519142150879, Transition Loss 2.0740537643432617, Classifier Loss 0.4485805630683899, Total Loss 163.61883544921875\n",
      "0: Encoding Loss 19.605804443359375, Transition Loss 4.7576518058776855, Classifier Loss 0.5101388692855835, Total Loss 170.55177307128906\n",
      "0: Encoding Loss 19.892745971679688, Transition Loss 2.7392640113830566, Classifier Loss 0.4642905294895172, Total Loss 166.88124084472656\n",
      "0: Encoding Loss 19.363862991333008, Transition Loss 2.156003713607788, Classifier Loss 0.4554659426212311, Total Loss 162.5921630859375\n",
      "0: Encoding Loss 18.242202758789062, Transition Loss 2.0981616973876953, Classifier Loss 0.3998720049858093, Total Loss 150.27969360351562\n",
      "0: Encoding Loss 16.230113983154297, Transition Loss 2.3501594066619873, Classifier Loss 0.45888227224349976, Total Loss 144.208984375\n",
      "0: Encoding Loss 17.671016693115234, Transition Loss -0.3581361472606659, Classifier Loss 0.36702704429626465, Total Loss 142.7286834716797\n",
      "0: Encoding Loss 17.035490036010742, Transition Loss 2.576988935470581, Classifier Loss 0.4656297564506531, Total Loss 149.8067169189453\n",
      "0: Encoding Loss 20.321739196777344, Transition Loss 2.2678561210632324, Classifier Loss 0.45959195494651794, Total Loss 168.79676818847656\n",
      "0: Encoding Loss 19.37856101989746, Transition Loss 4.967804908752441, Classifier Loss 0.47995954751968384, Total Loss 166.25445556640625\n",
      "0: Encoding Loss 17.48600196838379, Transition Loss 1.7183626890182495, Classifier Loss 0.39685171842575073, Total Loss 145.28854370117188\n",
      "0: Encoding Loss 17.228191375732422, Transition Loss 1.0440157651901245, Classifier Loss 0.44509005546569824, Total Loss 148.29576110839844\n",
      "0: Encoding Loss 17.947961807250977, Transition Loss 1.13009512424469, Classifier Loss 0.45089635252952576, Total Loss 153.2294464111328\n",
      "0: Encoding Loss 18.813989639282227, Transition Loss 4.387697219848633, Classifier Loss 0.49660104513168335, Total Loss 164.2991180419922\n",
      "0: Encoding Loss 19.4552001953125, Transition Loss 2.0143356323242188, Classifier Loss 0.4682632088661194, Total Loss 164.36326599121094\n",
      "0: Encoding Loss 18.258527755737305, Transition Loss 2.9581689834594727, Classifier Loss 0.5104204416275024, Total Loss 161.7764892578125\n",
      "0: Encoding Loss 16.98345947265625, Transition Loss 2.133241653442383, Classifier Loss 0.3940558433532715, Total Loss 142.15965270996094\n",
      "0: Encoding Loss 18.971500396728516, Transition Loss 1.8908333778381348, Classifier Loss 0.47057417035102844, Total Loss 161.6427459716797\n",
      "0: Encoding Loss 17.94997787475586, Transition Loss 0.45475393533706665, Classifier Loss 0.471876323223114, Total Loss 155.0694122314453\n",
      "0: Encoding Loss 18.26540184020996, Transition Loss 5.567773342132568, Classifier Loss 0.4633896052837372, Total Loss 158.15847778320312\n",
      "0: Encoding Loss 17.84914779663086, Transition Loss 0.3931083381175995, Classifier Loss 0.4800523817539215, Total Loss 155.2573699951172\n",
      "0: Encoding Loss 17.6897029876709, Transition Loss 2.7683634757995605, Classifier Loss 0.4306815564632416, Total Loss 150.313720703125\n",
      "0: Encoding Loss 17.347993850708008, Transition Loss 4.88446569442749, Classifier Loss 0.4585421085357666, Total Loss 151.8959503173828\n",
      "0: Encoding Loss 17.809886932373047, Transition Loss 0.8755567669868469, Classifier Loss 0.4507891535758972, Total Loss 152.28846740722656\n",
      "0: Encoding Loss 19.16231918334961, Transition Loss 4.634865760803223, Classifier Loss 0.41239091753959656, Total Loss 158.06695556640625\n",
      "0: Encoding Loss 18.032611846923828, Transition Loss 2.639467477798462, Classifier Loss 0.45598745346069336, Total Loss 154.85020446777344\n",
      "0: Encoding Loss 17.963319778442383, Transition Loss 3.1666312217712402, Classifier Loss 0.4509180784225464, Total Loss 154.1383819580078\n",
      "0: Encoding Loss 19.827409744262695, Transition Loss 6.02382755279541, Classifier Loss 0.4381762146949768, Total Loss 165.19161987304688\n",
      "0: Encoding Loss 18.389633178710938, Transition Loss 3.5595221519470215, Classifier Loss 0.48366373777389526, Total Loss 160.12799072265625\n",
      "0: Encoding Loss 20.252275466918945, Transition Loss 4.806204795837402, Classifier Loss 0.43638116121292114, Total Loss 167.07424926757812\n",
      "0: Encoding Loss 18.537078857421875, Transition Loss 1.007967233657837, Classifier Loss 0.47881752252578735, Total Loss 159.50741577148438\n",
      "0: Encoding Loss 19.271900177001953, Transition Loss 2.3299756050109863, Classifier Loss 0.4541245996952057, Total Loss 161.97584533691406\n",
      "0: Encoding Loss 18.886825561523438, Transition Loss 4.744359016418457, Classifier Loss 0.4739476442337036, Total Loss 162.6134796142578\n",
      "0: Encoding Loss 18.41953468322754, Transition Loss 1.6078587770462036, Classifier Loss 0.39608168601989746, Total Loss 150.76852416992188\n",
      "0: Encoding Loss 18.006542205810547, Transition Loss 5.130566596984863, Classifier Loss 0.47766101360321045, Total Loss 157.8575897216797\n",
      "0: Encoding Loss 17.627962112426758, Transition Loss 0.5563612580299377, Classifier Loss 0.44549989700317383, Total Loss 150.54031372070312\n",
      "0: Encoding Loss 17.400115966796875, Transition Loss 2.3671998977661133, Classifier Loss 0.4159548878669739, Total Loss 146.9430694580078\n",
      "0: Encoding Loss 17.27589225769043, Transition Loss 1.831598162651062, Classifier Loss 0.464359849691391, Total Loss 150.823974609375\n",
      "0: Encoding Loss 18.308788299560547, Transition Loss 2.0058960914611816, Classifier Loss 0.4595984220504761, Total Loss 156.61492919921875\n",
      "0: Encoding Loss 18.570619583129883, Transition Loss 4.19715690612793, Classifier Loss 0.4704296588897705, Total Loss 160.1455535888672\n",
      "0: Encoding Loss 18.664329528808594, Transition Loss 3.864713430404663, Classifier Loss 0.4716346263885498, Total Loss 160.69532775878906\n",
      "0: Encoding Loss 19.26358413696289, Transition Loss 2.021665573120117, Classifier Loss 0.41650083661079407, Total Loss 158.04026794433594\n",
      "0: Encoding Loss 16.45227813720703, Transition Loss 3.5744576454162598, Classifier Loss 0.44716402888298035, Total Loss 144.85986328125\n",
      "0: Encoding Loss 21.153024673461914, Transition Loss 4.838321208953857, Classifier Loss 0.4811420142650604, Total Loss 176.96768188476562\n",
      "0: Encoding Loss 21.699766159057617, Transition Loss 3.799605131149292, Classifier Loss 0.4565369188785553, Total Loss 177.37213134765625\n",
      "0: Encoding Loss 19.39849281311035, Transition Loss 5.360630512237549, Classifier Loss 0.47766563296318054, Total Loss 166.30178833007812\n",
      "0: Encoding Loss 17.91127586364746, Transition Loss 1.9297375679016113, Classifier Loss 0.43141165375709534, Total Loss 151.38072204589844\n",
      "0: Encoding Loss 17.294845581054688, Transition Loss 0.8697659373283386, Classifier Loss 0.4538712203502655, Total Loss 149.5041046142578\n",
      "0: Encoding Loss 18.431102752685547, Transition Loss 5.345735549926758, Classifier Loss 0.5011610984802246, Total Loss 162.8410186767578\n",
      "0: Encoding Loss 18.616092681884766, Transition Loss 4.797165870666504, Classifier Loss 0.45161107182502747, Total Loss 158.7765350341797\n",
      "0: Encoding Loss 16.687259674072266, Transition Loss 3.3237287998199463, Classifier Loss 0.4269188642501831, Total Loss 144.1449432373047\n",
      "0: Encoding Loss 18.857948303222656, Transition Loss 5.039529323577881, Classifier Loss 0.47297918796539307, Total Loss 162.46142578125\n",
      "0: Encoding Loss 18.339303970336914, Transition Loss 2.010512590408325, Classifier Loss 0.4653341770172119, Total Loss 157.37344360351562\n",
      "0: Encoding Loss 19.672855377197266, Transition Loss 4.187173843383789, Classifier Loss 0.4191523790359497, Total Loss 161.62722778320312\n",
      "0: Encoding Loss 18.122652053833008, Transition Loss 0.06466728448867798, Classifier Loss 0.4256630539894104, Total Loss 151.3280792236328\n",
      "0: Encoding Loss 17.20566749572754, Transition Loss 2.575639009475708, Classifier Loss 0.4295949935913086, Total Loss 147.22377014160156\n",
      "0: Encoding Loss 19.33823013305664, Transition Loss 4.461589813232422, Classifier Loss 0.43712854385375977, Total Loss 161.52687072753906\n",
      "0: Encoding Loss 16.688718795776367, Transition Loss 2.214156150817871, Classifier Loss 0.4636995792388916, Total Loss 147.387939453125\n",
      "0: Encoding Loss 18.733116149902344, Transition Loss 2.414058208465576, Classifier Loss 0.4363742470741272, Total Loss 157.0017547607422\n",
      "0: Encoding Loss 19.975616455078125, Transition Loss 2.933412551879883, Classifier Loss 0.4873318374156952, Total Loss 169.76025390625\n",
      "0: Encoding Loss 16.64356231689453, Transition Loss 1.3718217611312866, Classifier Loss 0.5011953115463257, Total Loss 150.52963256835938\n",
      "0: Encoding Loss 16.94172477722168, Transition Loss 2.007294178009033, Classifier Loss 0.464592307806015, Total Loss 148.91250610351562\n",
      "0: Encoding Loss 16.739444732666016, Transition Loss 4.851394176483154, Classifier Loss 0.4518061578273773, Total Loss 147.55783081054688\n",
      "0: Encoding Loss 18.004493713378906, Transition Loss 2.6106460094451904, Classifier Loss 0.45812416076660156, Total Loss 154.88365173339844\n",
      "0: Encoding Loss 17.96319007873535, Transition Loss 1.007275104522705, Classifier Loss 0.42588549852371216, Total Loss 150.77059936523438\n",
      "0: Encoding Loss 18.84671401977539, Transition Loss 2.9837019443511963, Classifier Loss 0.4110555052757263, Total Loss 155.3793182373047\n",
      "0: Encoding Loss 18.766002655029297, Transition Loss 4.569244861602783, Classifier Loss 0.4979082942008972, Total Loss 164.2145538330078\n",
      "0: Encoding Loss 17.957483291625977, Transition Loss 2.9412593841552734, Classifier Loss 0.4939114451408386, Total Loss 158.3125457763672\n",
      "0: Encoding Loss 16.914356231689453, Transition Loss 2.068716526031494, Classifier Loss 0.4471715986728668, Total Loss 147.03077697753906\n",
      "0: Encoding Loss 17.794452667236328, Transition Loss 2.6258182525634766, Classifier Loss 0.4997107684612274, Total Loss 157.78811645507812\n",
      "0: Encoding Loss 17.909502029418945, Transition Loss 1.265946388244629, Classifier Loss 0.4609781503677368, Total Loss 154.06121826171875\n",
      "0: Encoding Loss 16.529926300048828, Transition Loss 3.6235504150390625, Classifier Loss 0.4514267146587372, Total Loss 145.7716522216797\n",
      "0: Encoding Loss 16.385799407958984, Transition Loss 1.4212549924850464, Classifier Loss 0.4353259801864624, Total Loss 142.4158935546875\n",
      "0: Encoding Loss 16.83087730407715, Transition Loss 0.8064374327659607, Classifier Loss 0.4588700830936432, Total Loss 147.19485473632812\n",
      "0: Encoding Loss 17.442302703857422, Transition Loss 2.1041979789733887, Classifier Loss 0.44380247592926025, Total Loss 149.87574768066406\n",
      "0: Encoding Loss 16.648759841918945, Transition Loss 3.5700252056121826, Classifier Loss 0.41951829195022583, Total Loss 143.27239990234375\n",
      "0: Encoding Loss 17.895883560180664, Transition Loss -0.5137885808944702, Classifier Loss 0.48539331555366516, Total Loss 155.91444396972656\n",
      "0: Encoding Loss 16.738567352294922, Transition Loss 2.677354335784912, Classifier Loss 0.4139736294746399, Total Loss 142.8997039794922\n",
      "0: Encoding Loss 17.476856231689453, Transition Loss 0.6211687326431274, Classifier Loss 0.43710216879844666, Total Loss 148.81982421875\n",
      "0: Encoding Loss 19.547277450561523, Transition Loss 0.5897724628448486, Classifier Loss 0.41248077154159546, Total Loss 158.7676544189453\n",
      "0: Encoding Loss 15.558067321777344, Transition Loss -0.5229527354240417, Classifier Loss 0.44304564595222473, Total Loss 137.6527557373047\n",
      "0: Encoding Loss 17.429357528686523, Transition Loss 2.8056893348693848, Classifier Loss 0.3984210193157196, Total Loss 145.54051208496094\n",
      "0: Encoding Loss 16.982290267944336, Transition Loss 4.004684925079346, Classifier Loss 0.4500361979007721, Total Loss 148.49923706054688\n",
      "0: Encoding Loss 18.499055862426758, Transition Loss -1.2145090103149414, Classifier Loss 0.42168354988098145, Total Loss 153.16220092773438\n",
      "0: Encoding Loss 19.219757080078125, Transition Loss 4.294577598571777, Classifier Loss 0.4409617781639099, Total Loss 161.132568359375\n",
      "0: Encoding Loss 16.367881774902344, Transition Loss 1.70393967628479, Classifier Loss 0.42895278334617615, Total Loss 141.78414916992188\n",
      "0: Encoding Loss 16.523664474487305, Transition Loss 4.647663116455078, Classifier Loss 0.4320145547389984, Total Loss 144.2025146484375\n",
      "0: Encoding Loss 16.932504653930664, Transition Loss 1.9709409475326538, Classifier Loss 0.4400126338005066, Total Loss 146.38467407226562\n",
      "0: Encoding Loss 17.27413558959961, Transition Loss 2.720672845840454, Classifier Loss 0.4639902412891388, Total Loss 151.1321258544922\n",
      "0: Encoding Loss 16.310527801513672, Transition Loss 2.995361804962158, Classifier Loss 0.42050978541374207, Total Loss 141.1123046875\n",
      "0: Encoding Loss 16.257658004760742, Transition Loss 4.899533748626709, Classifier Loss 0.4486025273799896, Total Loss 144.3660125732422\n",
      "0: Encoding Loss 16.544387817382812, Transition Loss 1.7468547821044922, Classifier Loss 0.4385264813899994, Total Loss 143.81773376464844\n",
      "0: Encoding Loss 16.86911392211914, Transition Loss 3.9149410724639893, Classifier Loss 0.47208625078201294, Total Loss 149.98928833007812\n",
      "0: Encoding Loss 16.115032196044922, Transition Loss 2.364086627960205, Classifier Loss 0.48901310563087463, Total Loss 146.53713989257812\n",
      "0: Encoding Loss 15.948060989379883, Transition Loss 2.343078374862671, Classifier Loss 0.4720446467399597, Total Loss 143.83006286621094\n",
      "0: Encoding Loss 17.345813751220703, Transition Loss 3.058683395385742, Classifier Loss 0.438041090965271, Total Loss 149.10247802734375\n",
      "0: Encoding Loss 15.879867553710938, Transition Loss 2.5501134395599365, Classifier Loss 0.4566245675086975, Total Loss 141.9617156982422\n",
      "0: Encoding Loss 17.64872169494629, Transition Loss 3.593742847442627, Classifier Loss 0.42311781644821167, Total Loss 149.64161682128906\n",
      "0: Encoding Loss 17.081628799438477, Transition Loss 1.5823357105255127, Classifier Loss 0.4462873339653015, Total Loss 147.75144958496094\n",
      "0: Encoding Loss 16.706470489501953, Transition Loss 3.1216177940368652, Classifier Loss 0.46006473898887634, Total Loss 147.4939422607422\n",
      "0: Encoding Loss 17.059825897216797, Transition Loss 4.654675483703613, Classifier Loss 0.4717491865158081, Total Loss 151.395751953125\n",
      "0: Encoding Loss 17.796483993530273, Transition Loss 1.964945912361145, Classifier Loss 0.4364815950393677, Total Loss 151.21304321289062\n",
      "0: Encoding Loss 16.766237258911133, Transition Loss 0.7482993006706238, Classifier Loss 0.4814833104610443, Total Loss 149.04507446289062\n",
      "0: Encoding Loss 18.933448791503906, Transition Loss 5.2149553298950195, Classifier Loss 0.46758389472961426, Total Loss 162.445068359375\n",
      "0: Encoding Loss 19.723939895629883, Transition Loss 3.2051515579223633, Classifier Loss 0.4559639096260071, Total Loss 165.2220916748047\n",
      "0: Encoding Loss 16.118515014648438, Transition Loss 0.44606664776802063, Classifier Loss 0.45636308193206787, Total Loss 142.5258331298828\n",
      "0: Encoding Loss 17.388153076171875, Transition Loss 3.061363935470581, Classifier Loss 0.4721282124519348, Total Loss 152.76629638671875\n",
      "0: Encoding Loss 16.98336410522461, Transition Loss 3.2459917068481445, Classifier Loss 0.48721396923065186, Total Loss 151.91998291015625\n",
      "0: Encoding Loss 16.553417205810547, Transition Loss 4.771418571472168, Classifier Loss 0.450222909450531, Total Loss 146.25137329101562\n",
      "0: Encoding Loss 16.47248077392578, Transition Loss 3.3041634559631348, Classifier Loss 0.4569317102432251, Total Loss 145.8497314453125\n",
      "0: Encoding Loss 18.248151779174805, Transition Loss 2.88993501663208, Classifier Loss 0.46886762976646423, Total Loss 157.53164672851562\n",
      "0: Encoding Loss 17.994503021240234, Transition Loss 3.150723934173584, Classifier Loss 0.4137439429759979, Total Loss 150.60169982910156\n",
      "0: Encoding Loss 15.624273300170898, Transition Loss 0.5918737053871155, Classifier Loss 0.4921559691429138, Total Loss 143.197998046875\n",
      "0: Encoding Loss 18.88017463684082, Transition Loss 1.2719053030014038, Classifier Loss 0.4306693375110626, Total Loss 156.85675048828125\n",
      "0: Encoding Loss 21.00307846069336, Transition Loss 3.3648037910461426, Classifier Loss 0.4483771324157715, Total Loss 172.20211791992188\n",
      "0: Encoding Loss 18.065019607543945, Transition Loss 1.2098336219787598, Classifier Loss 0.4466361403465271, Total Loss 153.5376739501953\n",
      "0: Encoding Loss 18.135082244873047, Transition Loss 8.409506797790527, Classifier Loss 0.471388041973114, Total Loss 159.3131103515625\n",
      "0: Encoding Loss 16.6911678314209, Transition Loss -0.744668185710907, Classifier Loss 0.457554429769516, Total Loss 145.9021453857422\n",
      "0: Encoding Loss 15.567639350891113, Transition Loss 4.060564041137695, Classifier Loss 0.43725743889808655, Total Loss 138.75579833984375\n",
      "0: Encoding Loss 18.374380111694336, Transition Loss 2.5083999633789062, Classifier Loss 0.5233425498008728, Total Loss 163.58389282226562\n",
      "0: Encoding Loss 19.883167266845703, Transition Loss 3.516516923904419, Classifier Loss 0.41794824600219727, Total Loss 162.50042724609375\n",
      "0: Encoding Loss 16.382822036743164, Transition Loss 3.2569944858551025, Classifier Loss 0.417255699634552, Total Loss 141.32530212402344\n",
      "0: Encoding Loss 16.05950927734375, Transition Loss 1.2742537260055542, Classifier Loss 0.4673362970352173, Total Loss 143.60040283203125\n",
      "0: Encoding Loss 16.461196899414062, Transition Loss 3.126901626586914, Classifier Loss 0.44799453020095825, Total Loss 144.81741333007812\n",
      "0: Encoding Loss 16.781980514526367, Transition Loss 3.330923080444336, Classifier Loss 0.43134593963623047, Total Loss 145.15884399414062\n",
      "0: Encoding Loss 17.033634185791016, Transition Loss 4.04718017578125, Classifier Loss 0.43566352128982544, Total Loss 147.38702392578125\n",
      "0: Encoding Loss 15.985852241516113, Transition Loss 4.879290580749512, Classifier Loss 0.4044021964073181, Total Loss 138.3070526123047\n",
      "0: Encoding Loss 13.526211738586426, Transition Loss 0.7616603374481201, Classifier Loss 0.36730343103408813, Total Loss 118.1922836303711\n",
      "1: Encoding Loss 18.041217803955078, Transition Loss 3.824873685836792, Classifier Loss 0.4822327494621277, Total Loss 158.0005340576172\n",
      "1: Encoding Loss 17.417757034301758, Transition Loss 3.1436872482299805, Classifier Loss 0.43478429317474365, Total Loss 149.24244689941406\n",
      "1: Encoding Loss 19.293903350830078, Transition Loss 8.476395606994629, Classifier Loss 0.5173485279083252, Total Loss 170.8888397216797\n",
      "1: Encoding Loss 17.5659236907959, Transition Loss 0.3368695080280304, Classifier Loss 0.47714775800704956, Total Loss 153.2450714111328\n",
      "1: Encoding Loss 15.433627128601074, Transition Loss 0.05432772636413574, Classifier Loss 0.4344373643398285, Total Loss 136.06723022460938\n",
      "1: Encoding Loss 18.914913177490234, Transition Loss 7.9877166748046875, Classifier Loss 0.44367796182632446, Total Loss 161.0523681640625\n",
      "1: Encoding Loss 17.207897186279297, Transition Loss 3.573537826538086, Classifier Loss 0.4686363935470581, Total Loss 151.54043579101562\n",
      "1: Encoding Loss 14.931961059570312, Transition Loss -0.7032389640808105, Classifier Loss 0.4741186499595642, Total Loss 137.00335693359375\n",
      "1: Encoding Loss 17.113086700439453, Transition Loss 4.476787567138672, Classifier Loss 0.43664202094078064, Total Loss 148.13343811035156\n",
      "1: Encoding Loss 16.94083023071289, Transition Loss 3.7391815185546875, Classifier Loss 0.48363256454467773, Total Loss 151.50390625\n",
      "1: Encoding Loss 15.038167953491211, Transition Loss 0.5644680261611938, Classifier Loss 0.44740521907806396, Total Loss 135.1953125\n",
      "1: Encoding Loss 15.874545097351074, Transition Loss 0.23418688774108887, Classifier Loss 0.4270439147949219, Total Loss 138.0453338623047\n",
      "1: Encoding Loss 17.031723022460938, Transition Loss 4.023879528045654, Classifier Loss 0.47078752517700195, Total Loss 150.878662109375\n",
      "1: Encoding Loss 18.142822265625, Transition Loss 3.9983437061309814, Classifier Loss 0.4054122269153595, Total Loss 150.99749755859375\n",
      "1: Encoding Loss 17.493595123291016, Transition Loss 3.822923421859741, Classifier Loss 0.5010785460472107, Total Loss 156.59860229492188\n",
      "1: Encoding Loss 19.62234878540039, Transition Loss 4.412535190582275, Classifier Loss 0.470528781414032, Total Loss 166.55198669433594\n",
      "1: Encoding Loss 20.377593994140625, Transition Loss 5.010082721710205, Classifier Loss 0.47557100653648376, Total Loss 171.8267059326172\n",
      "1: Encoding Loss 17.572494506835938, Transition Loss 0.18201369047164917, Classifier Loss 0.46422260999679565, Total Loss 151.93003845214844\n",
      "1: Encoding Loss 16.549428939819336, Transition Loss 3.59368634223938, Classifier Loss 0.43469303846359253, Total Loss 144.20335388183594\n",
      "1: Encoding Loss 14.934956550598145, Transition Loss 1.953526496887207, Classifier Loss 0.4470120072364807, Total Loss 135.0923614501953\n",
      "1: Encoding Loss 15.152000427246094, Transition Loss 2.2255942821502686, Classifier Loss 0.3851732611656189, Total Loss 130.31956481933594\n",
      "1: Encoding Loss 15.013731002807617, Transition Loss 0.685224175453186, Classifier Loss 0.4465053975582123, Total Loss 135.00701904296875\n",
      "1: Encoding Loss 16.133756637573242, Transition Loss 0.0683588981628418, Classifier Loss 0.3696044087409973, Total Loss 133.7903289794922\n",
      "1: Encoding Loss 18.62018394470215, Transition Loss 1.2722537517547607, Classifier Loss 0.43332189321517944, Total Loss 155.56219482421875\n",
      "1: Encoding Loss 15.031417846679688, Transition Loss 2.2147223949432373, Classifier Loss 0.44454842805862427, Total Loss 135.52923583984375\n",
      "1: Encoding Loss 14.788938522338867, Transition Loss 3.6403441429138184, Classifier Loss 0.41172438859939575, Total Loss 131.36219787597656\n",
      "1: Encoding Loss 14.53051471710205, Transition Loss 1.2095043659210205, Classifier Loss 0.4413660764694214, Total Loss 131.80349731445312\n",
      "1: Encoding Loss 21.101425170898438, Transition Loss 6.633890151977539, Classifier Loss 0.4925791621208191, Total Loss 178.52001953125\n",
      "1: Encoding Loss 21.752010345458984, Transition Loss 1.674004316329956, Classifier Loss 0.43381524085998535, Total Loss 174.5631866455078\n",
      "1: Encoding Loss 17.312353134155273, Transition Loss 1.6823545694351196, Classifier Loss 0.46716925501823425, Total Loss 151.2639923095703\n",
      "1: Encoding Loss 15.277791023254395, Transition Loss 1.8418530225753784, Classifier Loss 0.41735219955444336, Total Loss 134.1387176513672\n",
      "1: Encoding Loss 16.420886993408203, Transition Loss 0.3737212121486664, Classifier Loss 0.4499599039554596, Total Loss 143.67080688476562\n",
      "1: Encoding Loss 16.49853515625, Transition Loss 5.131883144378662, Classifier Loss 0.4202907085418701, Total Loss 143.0730438232422\n",
      "1: Encoding Loss 17.507984161376953, Transition Loss 2.796201467514038, Classifier Loss 0.44895291328430176, Total Loss 151.06167602539062\n",
      "1: Encoding Loss 17.1932315826416, Transition Loss 5.329111576080322, Classifier Loss 0.4473625421524048, Total Loss 150.02728271484375\n",
      "1: Encoding Loss 16.69071388244629, Transition Loss 2.5887537002563477, Classifier Loss 0.5000473260879517, Total Loss 151.1845245361328\n",
      "1: Encoding Loss 18.61225128173828, Transition Loss 4.144402980804443, Classifier Loss 0.4592618942260742, Total Loss 159.25746154785156\n",
      "1: Encoding Loss 15.600019454956055, Transition Loss 0.36195847392082214, Classifier Loss 0.4597781300544739, Total Loss 139.7227325439453\n",
      "1: Encoding Loss 15.389373779296875, Transition Loss 4.673439979553223, Classifier Loss 0.4361228942871094, Total Loss 137.81790161132812\n",
      "1: Encoding Loss 16.93260383605957, Transition Loss 0.8361713886260986, Classifier Loss 0.39000052213668823, Total Loss 140.93014526367188\n",
      "1: Encoding Loss 15.97784423828125, Transition Loss 2.5412254333496094, Classifier Loss 0.40503519773483276, Total Loss 137.3870849609375\n",
      "1: Encoding Loss 15.188552856445312, Transition Loss 1.1355398893356323, Classifier Loss 0.428161084651947, Total Loss 134.40162658691406\n",
      "1: Encoding Loss 17.818157196044922, Transition Loss 1.8530569076538086, Classifier Loss 0.4444628357887268, Total Loss 152.09646606445312\n",
      "1: Encoding Loss 17.450712203979492, Transition Loss 1.6282764673233032, Classifier Loss 0.46566689014434814, Total Loss 151.92227172851562\n",
      "1: Encoding Loss 16.216915130615234, Transition Loss 0.48948824405670166, Classifier Loss 0.44091862440109253, Total Loss 141.5891571044922\n",
      "1: Encoding Loss 17.197864532470703, Transition Loss 3.8735225200653076, Classifier Loss 0.44292670488357544, Total Loss 149.02926635742188\n",
      "1: Encoding Loss 16.244915008544922, Transition Loss 5.34807825088501, Classifier Loss 0.4510771334171295, Total Loss 144.71644592285156\n",
      "1: Encoding Loss 15.367271423339844, Transition Loss 0.6230878829956055, Classifier Loss 0.45925435423851013, Total Loss 138.3782958984375\n",
      "1: Encoding Loss 15.312085151672363, Transition Loss 1.255976915359497, Classifier Loss 0.453149676322937, Total Loss 137.68988037109375\n",
      "1: Encoding Loss 15.599189758300781, Transition Loss 3.205501079559326, Classifier Loss 0.4306257367134094, Total Loss 137.93991088867188\n",
      "1: Encoding Loss 16.248319625854492, Transition Loss 3.69169282913208, Classifier Loss 0.44663766026496887, Total Loss 143.63037109375\n",
      "1: Encoding Loss 15.96472454071045, Transition Loss 0.5692362785339355, Classifier Loss 0.440515398979187, Total Loss 140.0675811767578\n",
      "1: Encoding Loss 17.120468139648438, Transition Loss 1.2057034969329834, Classifier Loss 0.43525469303131104, Total Loss 146.73057556152344\n",
      "1: Encoding Loss 16.563331604003906, Transition Loss 1.1581573486328125, Classifier Loss 0.42152461409568787, Total Loss 141.99571228027344\n",
      "1: Encoding Loss 17.079547882080078, Transition Loss -1.0241621732711792, Classifier Loss 0.3837616741657257, Total Loss 140.85304260253906\n",
      "1: Encoding Loss 16.720966339111328, Transition Loss 1.3800218105316162, Classifier Loss 0.4515748620033264, Total Loss 146.0352783203125\n",
      "1: Encoding Loss 16.56870460510254, Transition Loss 2.4529571533203125, Classifier Loss 0.43301457166671753, Total Loss 143.6948699951172\n",
      "1: Encoding Loss 17.366113662719727, Transition Loss 4.173931121826172, Classifier Loss 0.4124852120876312, Total Loss 147.11477661132812\n",
      "1: Encoding Loss 16.923297882080078, Transition Loss 4.858973026275635, Classifier Loss 0.43166905641555786, Total Loss 146.65028381347656\n",
      "1: Encoding Loss 15.618087768554688, Transition Loss 3.0567286014556885, Classifier Loss 0.42458468675613403, Total Loss 137.38967895507812\n",
      "1: Encoding Loss 18.19211196899414, Transition Loss 1.5274858474731445, Classifier Loss 0.48011213541030884, Total Loss 157.77487182617188\n",
      "1: Encoding Loss 22.83903694152832, Transition Loss 3.4670066833496094, Classifier Loss 0.4443597197532654, Total Loss 182.85699462890625\n",
      "1: Encoding Loss 18.367000579833984, Transition Loss 3.7415964603424072, Classifier Loss 0.42627793550491333, Total Loss 154.32644653320312\n",
      "1: Encoding Loss 16.42965316772461, Transition Loss 4.908725738525391, Classifier Loss 0.47339582443237305, Total Loss 147.88099670410156\n",
      "1: Encoding Loss 15.90968132019043, Transition Loss 1.1159172058105469, Classifier Loss 0.4123843014240265, Total Loss 137.14288330078125\n",
      "1: Encoding Loss 15.577486991882324, Transition Loss 1.6094615459442139, Classifier Loss 0.4528762698173523, Total Loss 139.39634704589844\n",
      "1: Encoding Loss 17.01861572265625, Transition Loss 4.480163097381592, Classifier Loss 0.5009443163871765, Total Loss 153.99819946289062\n",
      "1: Encoding Loss 15.840060234069824, Transition Loss 3.486401081085205, Classifier Loss 0.4414694905281067, Total Loss 140.58187866210938\n",
      "1: Encoding Loss 17.04779052734375, Transition Loss 1.9940011501312256, Classifier Loss 0.4285278022289276, Total Loss 145.9371337890625\n",
      "1: Encoding Loss 16.742116928100586, Transition Loss 2.7767162322998047, Classifier Loss 0.5380990505218506, Total Loss 155.373291015625\n",
      "1: Encoding Loss 16.30147933959961, Transition Loss 2.127607822418213, Classifier Loss 0.4344444274902344, Total Loss 142.1043701171875\n",
      "1: Encoding Loss 16.669307708740234, Transition Loss 0.7590435743331909, Classifier Loss 0.4031989276409149, Total Loss 140.63937377929688\n",
      "1: Encoding Loss 17.07485008239746, Transition Loss 3.6033453941345215, Classifier Loss 0.47577792406082153, Total Loss 151.46824645996094\n",
      "1: Encoding Loss 16.04817771911621, Transition Loss 0.5541290044784546, Classifier Loss 0.4197571873664856, Total Loss 138.48643493652344\n",
      "1: Encoding Loss 13.702313423156738, Transition Loss 0.8943365812301636, Classifier Loss 0.42554977536201477, Total Loss 125.12659454345703\n",
      "1: Encoding Loss 17.251623153686523, Transition Loss 3.083920955657959, Classifier Loss 0.4325782358646393, Total Loss 148.00112915039062\n",
      "1: Encoding Loss 18.2943058013916, Transition Loss 3.0075340270996094, Classifier Loss 0.4386640787124634, Total Loss 154.83526611328125\n",
      "1: Encoding Loss 16.901721954345703, Transition Loss -0.5609880685806274, Classifier Loss 0.45560717582702637, Total Loss 146.9708251953125\n",
      "1: Encoding Loss 16.559715270996094, Transition Loss 3.4990334510803223, Classifier Loss 0.47036057710647583, Total Loss 147.79396057128906\n",
      "1: Encoding Loss 16.993701934814453, Transition Loss 1.825371503829956, Classifier Loss 0.43987953662872314, Total Loss 146.68031311035156\n",
      "1: Encoding Loss 17.12856674194336, Transition Loss 5.226726055145264, Classifier Loss 0.42928585410118103, Total Loss 147.79067993164062\n",
      "1: Encoding Loss 14.326708793640137, Transition Loss 1.1836304664611816, Classifier Loss 0.4719841480255127, Total Loss 133.6321258544922\n",
      "1: Encoding Loss 14.354703903198242, Transition Loss 1.302673578262329, Classifier Loss 0.4982278048992157, Total Loss 136.47207641601562\n",
      "1: Encoding Loss 16.57533073425293, Transition Loss 3.7071778774261475, Classifier Loss 0.47977006435394287, Total Loss 148.91184997558594\n",
      "1: Encoding Loss 16.360210418701172, Transition Loss 1.9510878324508667, Classifier Loss 0.4044847786426544, Total Loss 139.3901824951172\n",
      "1: Encoding Loss 15.284031867980957, Transition Loss 0.3504623770713806, Classifier Loss 0.4264875650405884, Total Loss 134.49313354492188\n",
      "1: Encoding Loss 16.159439086914062, Transition Loss 1.8083678483963013, Classifier Loss 0.4641034007072449, Total Loss 144.09031677246094\n",
      "1: Encoding Loss 16.912918090820312, Transition Loss 3.058607578277588, Classifier Loss 0.44522538781166077, Total Loss 147.2235107421875\n",
      "1: Encoding Loss 15.208948135375977, Transition Loss 2.1061856746673584, Classifier Loss 0.39964690804481506, Total Loss 132.06085205078125\n",
      "1: Encoding Loss 16.627601623535156, Transition Loss 2.42091703414917, Classifier Loss 0.44981062412261963, Total Loss 145.7150421142578\n",
      "1: Encoding Loss 16.376035690307617, Transition Loss 3.9167897701263428, Classifier Loss 0.4211716949939728, Total Loss 141.94009399414062\n",
      "1: Encoding Loss 15.96198844909668, Transition Loss 3.3682007789611816, Classifier Loss 0.44085943698883057, Total Loss 141.2051544189453\n",
      "1: Encoding Loss 18.004791259765625, Transition Loss 2.7638838291168213, Classifier Loss 0.493510365486145, Total Loss 158.4853515625\n",
      "1: Encoding Loss 17.159358978271484, Transition Loss 2.592154026031494, Classifier Loss 0.47086647152900696, Total Loss 151.07968139648438\n",
      "1: Encoding Loss 16.865337371826172, Transition Loss 3.1442453861236572, Classifier Loss 0.4622640609741211, Total Loss 148.6761474609375\n",
      "1: Encoding Loss 14.72752571105957, Transition Loss 0.4093397855758667, Classifier Loss 0.4211899936199188, Total Loss 130.6479034423828\n",
      "1: Encoding Loss 17.14902114868164, Transition Loss 4.690025806427002, Classifier Loss 0.5118389129638672, Total Loss 155.9540252685547\n",
      "1: Encoding Loss 17.778989791870117, Transition Loss 3.2732913494110107, Classifier Loss 0.4831472635269165, Total Loss 156.29798889160156\n",
      "1: Encoding Loss 14.617329597473145, Transition Loss 1.7510908842086792, Classifier Loss 0.41158249974250793, Total Loss 129.5626678466797\n",
      "1: Encoding Loss 16.08514976501465, Transition Loss 0.48717474937438965, Classifier Loss 0.3878322243690491, Total Loss 135.48899841308594\n",
      "1: Encoding Loss 16.46087646484375, Transition Loss 4.169002056121826, Classifier Loss 0.42371776700019836, Total Loss 142.8046417236328\n",
      "1: Encoding Loss 14.629959106445312, Transition Loss 1.094000220298767, Classifier Loss 0.4091009795665741, Total Loss 129.12745666503906\n",
      "1: Encoding Loss 16.808917999267578, Transition Loss 1.9866583347320557, Classifier Loss 0.48855602741241455, Total Loss 150.50376892089844\n",
      "1: Encoding Loss 16.162349700927734, Transition Loss 1.4509330987930298, Classifier Loss 0.45797601342201233, Total Loss 143.35206604003906\n",
      "1: Encoding Loss 16.506275177001953, Transition Loss 2.8994288444519043, Classifier Loss 0.47612637281417847, Total Loss 147.81005859375\n",
      "1: Encoding Loss 17.481544494628906, Transition Loss 5.408473491668701, Classifier Loss 0.4876001477241516, Total Loss 155.81268310546875\n",
      "1: Encoding Loss 17.349376678466797, Transition Loss 1.1642041206359863, Classifier Loss 0.3876560926437378, Total Loss 143.3275604248047\n",
      "1: Encoding Loss 17.28529167175293, Transition Loss 3.7043609619140625, Classifier Loss 0.4336300194263458, Total Loss 148.55650329589844\n",
      "1: Encoding Loss 16.979877471923828, Transition Loss 3.209714412689209, Classifier Loss 0.38407275080680847, Total Loss 141.5704345703125\n",
      "1: Encoding Loss 15.953468322753906, Transition Loss 2.073887825012207, Classifier Loss 0.3641950786113739, Total Loss 132.96987915039062\n",
      "1: Encoding Loss 17.965787887573242, Transition Loss 2.297545909881592, Classifier Loss 0.472256064414978, Total Loss 155.93936157226562\n",
      "1: Encoding Loss 15.854875564575195, Transition Loss 2.937678337097168, Classifier Loss 0.41589540243148804, Total Loss 137.89385986328125\n",
      "1: Encoding Loss 15.61426067352295, Transition Loss 3.132284164428711, Classifier Loss 0.42030736804008484, Total Loss 136.96922302246094\n",
      "1: Encoding Loss 15.077860832214355, Transition Loss 0.5735805034637451, Classifier Loss 0.44497615098953247, Total Loss 135.1942138671875\n",
      "1: Encoding Loss 16.295501708984375, Transition Loss 1.8995425701141357, Classifier Loss 0.4190962314605713, Total Loss 140.4424591064453\n",
      "1: Encoding Loss 15.7051362991333, Transition Loss 1.0939676761627197, Classifier Loss 0.400791734457016, Total Loss 134.74758911132812\n",
      "1: Encoding Loss 15.666784286499023, Transition Loss 0.789961040019989, Classifier Loss 0.4536169469356537, Total Loss 139.67837524414062\n",
      "1: Encoding Loss 13.398813247680664, Transition Loss 0.13686001300811768, Classifier Loss 0.41923585534095764, Total Loss 122.37120819091797\n",
      "1: Encoding Loss 18.6959171295166, Transition Loss 2.0656187534332275, Classifier Loss 0.47906774282455444, Total Loss 160.9085235595703\n",
      "1: Encoding Loss 18.562213897705078, Transition Loss 1.1683831214904785, Classifier Loss 0.46913275122642517, Total Loss 158.75390625\n",
      "1: Encoding Loss 16.247604370117188, Transition Loss 2.2079644203186035, Classifier Loss 0.4388043284416199, Total Loss 142.24923706054688\n",
      "1: Encoding Loss 16.635690689086914, Transition Loss -0.13218271732330322, Classifier Loss 0.4414358139038086, Total Loss 143.9576873779297\n",
      "1: Encoding Loss 17.681434631347656, Transition Loss -0.20739421248435974, Classifier Loss 0.38063278794288635, Total Loss 144.1518096923828\n",
      "1: Encoding Loss 16.754573822021484, Transition Loss 1.212842345237732, Classifier Loss 0.426751971244812, Total Loss 143.6877899169922\n",
      "1: Encoding Loss 15.102259635925293, Transition Loss 1.276382565498352, Classifier Loss 0.4379444122314453, Total Loss 134.91856384277344\n",
      "1: Encoding Loss 16.766925811767578, Transition Loss 2.1209518909454346, Classifier Loss 0.4229767918586731, Total Loss 143.7476043701172\n",
      "1: Encoding Loss 15.720138549804688, Transition Loss 1.1259571313858032, Classifier Loss 0.4360921382904053, Total Loss 138.38043212890625\n",
      "1: Encoding Loss 16.45780372619629, Transition Loss 1.9594521522521973, Classifier Loss 0.39594513177871704, Total Loss 139.1251220703125\n",
      "1: Encoding Loss 15.598257064819336, Transition Loss 0.6837652921676636, Classifier Loss 0.4009942412376404, Total Loss 133.96246337890625\n",
      "1: Encoding Loss 16.367420196533203, Transition Loss 3.091323137283325, Classifier Loss 0.43651384115219116, Total Loss 143.09242248535156\n",
      "1: Encoding Loss 18.033584594726562, Transition Loss 3.0875792503356934, Classifier Loss 0.4648798704147339, Total Loss 155.92453002929688\n",
      "1: Encoding Loss 15.831903457641602, Transition Loss 2.111928701400757, Classifier Loss 0.47275128960609436, Total Loss 143.111328125\n",
      "1: Encoding Loss 16.057178497314453, Transition Loss 2.8338563442230225, Classifier Loss 0.45720475912094116, Total Loss 143.19708251953125\n",
      "1: Encoding Loss 16.079608917236328, Transition Loss 0.04167221486568451, Classifier Loss 0.41555601358413696, Total Loss 138.0499267578125\n",
      "1: Encoding Loss 16.600034713745117, Transition Loss 1.8684077262878418, Classifier Loss 0.42222824692726135, Total Loss 142.5703887939453\n",
      "1: Encoding Loss 16.109539031982422, Transition Loss 3.9921231269836426, Classifier Loss 0.4805641174316406, Total Loss 146.3105010986328\n",
      "1: Encoding Loss 14.493612289428711, Transition Loss 3.4335174560546875, Classifier Loss 0.38067758083343506, Total Loss 126.40283966064453\n",
      "1: Encoding Loss 15.729896545410156, Transition Loss 3.594440221786499, Classifier Loss 0.4296872317790985, Total Loss 138.78587341308594\n",
      "1: Encoding Loss 15.782119750976562, Transition Loss 2.4156579971313477, Classifier Loss 0.4132700562477112, Total Loss 136.98599243164062\n",
      "1: Encoding Loss 17.85599136352539, Transition Loss 3.0946969985961914, Classifier Loss 0.41117388010025024, Total Loss 149.49122619628906\n",
      "1: Encoding Loss 17.498332977294922, Transition Loss 1.1394023895263672, Classifier Loss 0.3945159912109375, Total Loss 144.89736938476562\n",
      "1: Encoding Loss 15.880182266235352, Transition Loss 2.8780360221862793, Classifier Loss 0.47804373502731323, Total Loss 144.2366943359375\n",
      "1: Encoding Loss 15.266914367675781, Transition Loss 2.681877613067627, Classifier Loss 0.4134809374809265, Total Loss 134.0223388671875\n",
      "1: Encoding Loss 15.99677562713623, Transition Loss 4.875869274139404, Classifier Loss 0.49671700596809387, Total Loss 147.6027069091797\n",
      "1: Encoding Loss 14.950965881347656, Transition Loss 2.148955821990967, Classifier Loss 0.3783847689628601, Total Loss 128.4038543701172\n",
      "1: Encoding Loss 15.060953140258789, Transition Loss -0.5474728345870972, Classifier Loss 0.37836119532585144, Total Loss 128.20162963867188\n",
      "1: Encoding Loss 16.411174774169922, Transition Loss 2.0462565422058105, Classifier Loss 0.4340296685695648, Total Loss 142.6885223388672\n",
      "1: Encoding Loss 14.75364875793457, Transition Loss 2.3853137493133545, Classifier Loss 0.48537227511405945, Total Loss 138.0132598876953\n",
      "1: Encoding Loss 13.722249984741211, Transition Loss 1.7060421705245972, Classifier Loss 0.4296511113643646, Total Loss 125.98103332519531\n",
      "1: Encoding Loss 16.41510772705078, Transition Loss 0.1565047651529312, Classifier Loss 0.458371102809906, Total Loss 144.39036560058594\n",
      "1: Encoding Loss 17.03795051574707, Transition Loss 3.2803499698638916, Classifier Loss 0.46108633279800415, Total Loss 149.64846801757812\n",
      "1: Encoding Loss 16.5167236328125, Transition Loss 4.069668769836426, Classifier Loss 0.4674720764160156, Total Loss 147.47543334960938\n",
      "1: Encoding Loss 14.476781845092773, Transition Loss 1.4224196672439575, Classifier Loss 0.38097822666168213, Total Loss 125.5274887084961\n",
      "1: Encoding Loss 16.26861572265625, Transition Loss 6.98141622543335, Classifier Loss 0.4690781235694885, Total Loss 147.3120880126953\n",
      "1: Encoding Loss 16.61497688293457, Transition Loss 4.513238430023193, Classifier Loss 0.4410579800605774, Total Loss 145.6009521484375\n",
      "1: Encoding Loss 17.197683334350586, Transition Loss 1.1587302684783936, Classifier Loss 0.4154764413833618, Total Loss 145.19723510742188\n",
      "1: Encoding Loss 15.415542602539062, Transition Loss 3.3216731548309326, Classifier Loss 0.4863479733467102, Total Loss 142.45672607421875\n",
      "1: Encoding Loss 14.986709594726562, Transition Loss 1.8092710971832275, Classifier Loss 0.4701218008995056, Total Loss 137.65614318847656\n",
      "1: Encoding Loss 14.229390144348145, Transition Loss 0.5928580164909363, Classifier Loss 0.42448848485946655, Total Loss 128.0623321533203\n",
      "1: Encoding Loss 15.771108627319336, Transition Loss 0.35952967405319214, Classifier Loss 0.3914637863636017, Total Loss 133.91685485839844\n",
      "1: Encoding Loss 16.673864364624023, Transition Loss 1.7459120750427246, Classifier Loss 0.47245171666145325, Total Loss 147.98672485351562\n",
      "1: Encoding Loss 16.8122615814209, Transition Loss 2.402806282043457, Classifier Loss 0.46357136964797974, Total Loss 148.19183349609375\n",
      "1: Encoding Loss 15.243186950683594, Transition Loss 3.3324427604675293, Classifier Loss 0.45098239183425903, Total Loss 137.8903350830078\n",
      "1: Encoding Loss 16.282230377197266, Transition Loss 2.1370491981506348, Classifier Loss 0.43342092633247375, Total Loss 141.89028930664062\n",
      "1: Encoding Loss 16.551937103271484, Transition Loss 1.9507675170898438, Classifier Loss 0.40627819299697876, Total Loss 140.71975708007812\n",
      "1: Encoding Loss 16.73604965209961, Transition Loss 4.217101097106934, Classifier Loss 0.4797343909740448, Total Loss 150.0765838623047\n",
      "1: Encoding Loss 16.51951789855957, Transition Loss 7.270827770233154, Classifier Loss 0.4977394640445709, Total Loss 151.79937744140625\n",
      "1: Encoding Loss 15.651697158813477, Transition Loss 2.968754529953003, Classifier Loss 0.4447364807128906, Total Loss 139.5713348388672\n",
      "1: Encoding Loss 16.394969940185547, Transition Loss 2.4122743606567383, Classifier Loss 0.43593263626098633, Total Loss 142.92799377441406\n",
      "1: Encoding Loss 16.359455108642578, Transition Loss 4.480027198791504, Classifier Loss 0.4759235382080078, Total Loss 147.5410919189453\n",
      "1: Encoding Loss 14.920964241027832, Transition Loss 1.6790636777877808, Classifier Loss 0.41807347536087036, Total Loss 132.0047607421875\n",
      "1: Encoding Loss 16.973752975463867, Transition Loss 4.084273338317871, Classifier Loss 0.4142700433731079, Total Loss 144.9032440185547\n",
      "1: Encoding Loss 17.66343116760254, Transition Loss 2.705026388168335, Classifier Loss 0.4973070025444031, Total Loss 156.79330444335938\n",
      "1: Encoding Loss 15.906521797180176, Transition Loss 3.467820644378662, Classifier Loss 0.4012359380722046, Total Loss 136.94985961914062\n",
      "1: Encoding Loss 16.01263999938965, Transition Loss 1.463209629058838, Classifier Loss 0.46259206533432007, Total Loss 142.9203338623047\n",
      "1: Encoding Loss 15.690913200378418, Transition Loss 4.159148693084717, Classifier Loss 0.43292009830474854, Total Loss 139.10116577148438\n",
      "1: Encoding Loss 15.521456718444824, Transition Loss -0.4580927789211273, Classifier Loss 0.44068777561187744, Total Loss 137.1973419189453\n",
      "1: Encoding Loss 15.317357063293457, Transition Loss -1.3106887340545654, Classifier Loss 0.4173896610736847, Total Loss 133.64259338378906\n",
      "1: Encoding Loss 17.04886817932129, Transition Loss 2.4536967277526855, Classifier Loss 0.43685999512672424, Total Loss 146.960693359375\n",
      "1: Encoding Loss 15.674943923950195, Transition Loss 1.6593420505523682, Classifier Loss 0.4197283387184143, Total Loss 136.68624877929688\n",
      "1: Encoding Loss 15.500411987304688, Transition Loss 1.7287955284118652, Classifier Loss 0.44933485984802246, Total Loss 138.62747192382812\n",
      "1: Encoding Loss 14.235452651977539, Transition Loss 1.7298903465270996, Classifier Loss 0.40499380230903625, Total Loss 126.60405731201172\n",
      "1: Encoding Loss 14.031042098999023, Transition Loss 0.21981820464134216, Classifier Loss 0.3718119263648987, Total Loss 121.45537567138672\n",
      "1: Encoding Loss 16.132915496826172, Transition Loss 0.8247601985931396, Classifier Loss 0.4373297095298767, Total Loss 140.86038208007812\n",
      "1: Encoding Loss 16.798166275024414, Transition Loss 2.2281837463378906, Classifier Loss 0.4183286130428314, Total Loss 143.51312255859375\n",
      "1: Encoding Loss 15.313119888305664, Transition Loss 1.2745908498764038, Classifier Loss 0.4063461422920227, Total Loss 133.02317810058594\n",
      "1: Encoding Loss 18.148265838623047, Transition Loss 2.0175161361694336, Classifier Loss 0.5044487714767456, Total Loss 160.1414794921875\n",
      "1: Encoding Loss 16.6774959564209, Transition Loss 4.042016983032227, Classifier Loss 0.4791333079338074, Total Loss 149.59510803222656\n",
      "1: Encoding Loss 15.375425338745117, Transition Loss 1.6166718006134033, Classifier Loss 0.402368426322937, Total Loss 133.1360626220703\n",
      "1: Encoding Loss 14.993054389953613, Transition Loss 3.077206611633301, Classifier Loss 0.43899422883987427, Total Loss 135.08863830566406\n",
      "1: Encoding Loss 16.654346466064453, Transition Loss 0.7805273532867432, Classifier Loss 0.4514545798301697, Total Loss 145.3837432861328\n",
      "1: Encoding Loss 16.79703140258789, Transition Loss 0.3336103558540344, Classifier Loss 0.4433515667915344, Total Loss 145.2507781982422\n",
      "1: Encoding Loss 14.4506196975708, Transition Loss 1.1957353353500366, Classifier Loss 0.4498808979988098, Total Loss 132.1700897216797\n",
      "1: Encoding Loss 15.181750297546387, Transition Loss 0.378591924905777, Classifier Loss 0.4047808051109314, Total Loss 131.72003173828125\n",
      "1: Encoding Loss 15.061037063598633, Transition Loss 2.420217514038086, Classifier Loss 0.39202919602394104, Total Loss 130.53724670410156\n",
      "1: Encoding Loss 16.406707763671875, Transition Loss 1.45835280418396, Classifier Loss 0.45479774475097656, Total Loss 144.5033721923828\n",
      "1: Encoding Loss 15.135699272155762, Transition Loss 2.5691475868225098, Classifier Loss 0.46786579489707947, Total Loss 138.62844848632812\n",
      "1: Encoding Loss 16.374393463134766, Transition Loss 3.575824737548828, Classifier Loss 0.49033352732658386, Total Loss 148.7100372314453\n",
      "1: Encoding Loss 15.811297416687012, Transition Loss 1.5293563604354858, Classifier Loss 0.3925042748451233, Total Loss 134.72996520996094\n",
      "1: Encoding Loss 15.68028736114502, Transition Loss 2.6756744384765625, Classifier Loss 0.44084101915359497, Total Loss 139.23609924316406\n",
      "1: Encoding Loss 15.567706108093262, Transition Loss 2.4038357734680176, Classifier Loss 0.4288959801197052, Total Loss 137.2573699951172\n",
      "1: Encoding Loss 15.081067085266113, Transition Loss 3.0762810707092285, Classifier Loss 0.44735708832740784, Total Loss 136.45262145996094\n",
      "1: Encoding Loss 18.099084854125977, Transition Loss 5.615867614746094, Classifier Loss 0.4544743299484253, Total Loss 156.28829956054688\n",
      "1: Encoding Loss 16.367883682250977, Transition Loss 0.6398754119873047, Classifier Loss 0.4250883162021637, Total Loss 140.9720916748047\n",
      "1: Encoding Loss 13.709230422973633, Transition Loss 2.658259868621826, Classifier Loss 0.42174699902534485, Total Loss 125.4933853149414\n",
      "1: Encoding Loss 16.610929489135742, Transition Loss 0.8975445032119751, Classifier Loss 0.40981173515319824, Total Loss 141.0057830810547\n",
      "1: Encoding Loss 17.69466781616211, Transition Loss 2.6272220611572266, Classifier Loss 0.45892977714538574, Total Loss 153.11187744140625\n",
      "1: Encoding Loss 15.82914924621582, Transition Loss 0.7106828689575195, Classifier Loss 0.4503309428691864, Total Loss 140.29226684570312\n",
      "1: Encoding Loss 16.295989990234375, Transition Loss 2.7115254402160645, Classifier Loss 0.4166768491268158, Total Loss 140.5282440185547\n",
      "1: Encoding Loss 16.720836639404297, Transition Loss 1.5126831531524658, Classifier Loss 0.4265584647655487, Total Loss 143.5859375\n",
      "1: Encoding Loss 16.465545654296875, Transition Loss 1.7882001399993896, Classifier Loss 0.44804874062538147, Total Loss 144.31344604492188\n",
      "1: Encoding Loss 16.198789596557617, Transition Loss 2.501807451248169, Classifier Loss 0.4549276530742645, Total Loss 143.68621826171875\n",
      "1: Encoding Loss 15.26948070526123, Transition Loss 2.8142833709716797, Classifier Loss 0.4310208261013031, Total Loss 135.8446807861328\n",
      "1: Encoding Loss 16.35758399963379, Transition Loss 1.5780634880065918, Classifier Loss 0.4834023714065552, Total Loss 147.11697387695312\n",
      "1: Encoding Loss 15.012092590332031, Transition Loss 1.4725075960159302, Classifier Loss 0.43941444158554077, Total Loss 134.60301208496094\n",
      "1: Encoding Loss 14.378667831420898, Transition Loss 0.7751317024230957, Classifier Loss 0.35373127460479736, Total Loss 121.95519256591797\n",
      "1: Encoding Loss 16.55402183532715, Transition Loss 1.648742437362671, Classifier Loss 0.4412708878517151, Total Loss 144.11073303222656\n",
      "1: Encoding Loss 15.616594314575195, Transition Loss 2.2945945262908936, Classifier Loss 0.41441473364830017, Total Loss 136.0588836669922\n",
      "1: Encoding Loss 14.703326225280762, Transition Loss 3.5857224464416504, Classifier Loss 0.47240394353866577, Total Loss 136.8946533203125\n",
      "1: Encoding Loss 16.33803367614746, Transition Loss 1.6203160285949707, Classifier Loss 0.4058045744895935, Total Loss 139.2567901611328\n",
      "1: Encoding Loss 14.617624282836914, Transition Loss 2.797187328338623, Classifier Loss 0.4051821231842041, Total Loss 129.3428497314453\n",
      "1: Encoding Loss 16.951335906982422, Transition Loss 2.8969645500183105, Classifier Loss 0.43630513548851013, Total Loss 146.497314453125\n",
      "1: Encoding Loss 17.108449935913086, Transition Loss 3.9005608558654785, Classifier Loss 0.43069854378700256, Total Loss 147.28077697753906\n",
      "1: Encoding Loss 15.854684829711914, Transition Loss 2.644012212753296, Classifier Loss 0.41989246010780334, Total Loss 138.17495727539062\n",
      "1: Encoding Loss 15.518940925598145, Transition Loss 0.2180304229259491, Classifier Loss 0.4052855968475342, Total Loss 133.72943115234375\n",
      "1: Encoding Loss 15.846200942993164, Transition Loss 2.919700860977173, Classifier Loss 0.39678019285202026, Total Loss 135.92311096191406\n",
      "1: Encoding Loss 16.9736385345459, Transition Loss 3.39493989944458, Classifier Loss 0.41431981325149536, Total Loss 144.6317901611328\n",
      "1: Encoding Loss 15.830648422241211, Transition Loss 1.1139851808547974, Classifier Loss 0.43513044714927673, Total Loss 138.94253540039062\n",
      "1: Encoding Loss 14.453373908996582, Transition Loss 3.0264363288879395, Classifier Loss 0.45367565751075745, Total Loss 133.2983856201172\n",
      "1: Encoding Loss 16.525564193725586, Transition Loss 2.431142807006836, Classifier Loss 0.44016334414482117, Total Loss 144.14218139648438\n",
      "1: Encoding Loss 16.383535385131836, Transition Loss 2.878870964050293, Classifier Loss 0.5138708353042603, Total Loss 150.83984375\n",
      "1: Encoding Loss 15.774699211120605, Transition Loss 3.0717906951904297, Classifier Loss 0.4749588072299957, Total Loss 143.37278747558594\n",
      "1: Encoding Loss 15.05982494354248, Transition Loss 1.5638866424560547, Classifier Loss 0.4527314603328705, Total Loss 136.25765991210938\n",
      "1: Encoding Loss 16.957223892211914, Transition Loss 3.559251308441162, Classifier Loss 0.447984904050827, Total Loss 147.96554565429688\n",
      "1: Encoding Loss 15.562113761901855, Transition Loss 2.064382791519165, Classifier Loss 0.43890008330345154, Total Loss 138.0884552001953\n",
      "1: Encoding Loss 14.709335327148438, Transition Loss -0.14715757966041565, Classifier Loss 0.42688295245170593, Total Loss 130.94424438476562\n",
      "1: Encoding Loss 17.750503540039062, Transition Loss 3.4325127601623535, Classifier Loss 0.4597855806350708, Total Loss 153.85458374023438\n",
      "1: Encoding Loss 17.159021377563477, Transition Loss 2.010891914367676, Classifier Loss 0.4318103790283203, Total Loss 146.93951416015625\n",
      "1: Encoding Loss 17.017662048339844, Transition Loss 0.6753461360931396, Classifier Loss 0.4772200584411621, Total Loss 150.09812927246094\n",
      "1: Encoding Loss 16.465415954589844, Transition Loss 1.239901065826416, Classifier Loss 0.40851712226867676, Total Loss 140.14016723632812\n",
      "1: Encoding Loss 15.656736373901367, Transition Loss 2.79599666595459, Classifier Loss 0.46057841181755066, Total Loss 141.1166534423828\n",
      "1: Encoding Loss 14.335537910461426, Transition Loss 0.4762248694896698, Classifier Loss 0.40323007106781006, Total Loss 126.52672576904297\n",
      "1: Encoding Loss 16.89080238342285, Transition Loss 2.7071924209594727, Classifier Loss 0.4809877872467041, Total Loss 150.52647399902344\n",
      "1: Encoding Loss 15.4316987991333, Transition Loss 1.5656883716583252, Classifier Loss 0.38776886463165283, Total Loss 131.9933624267578\n",
      "1: Encoding Loss 16.388404846191406, Transition Loss 3.479487180709839, Classifier Loss 0.4789166748523712, Total Loss 147.61390686035156\n",
      "1: Encoding Loss 16.553220748901367, Transition Loss 4.62426233291626, Classifier Loss 0.4364016056060791, Total Loss 144.80918884277344\n",
      "1: Encoding Loss 14.887136459350586, Transition Loss 1.9483020305633545, Classifier Loss 0.4400402903556824, Total Loss 134.10617065429688\n",
      "1: Encoding Loss 15.686978340148926, Transition Loss -0.2276933193206787, Classifier Loss 0.38951608538627625, Total Loss 133.07339477539062\n",
      "1: Encoding Loss 17.101051330566406, Transition Loss 3.6142475605010986, Classifier Loss 0.4695505201816559, Total Loss 151.00706481933594\n",
      "1: Encoding Loss 15.266416549682617, Transition Loss 3.121551990509033, Classifier Loss 0.37673234939575195, Total Loss 130.52035522460938\n",
      "1: Encoding Loss 15.532081604003906, Transition Loss 3.589247941970825, Classifier Loss 0.46902433037757874, Total Loss 141.53062438964844\n",
      "1: Encoding Loss 15.596710205078125, Transition Loss 2.654223680496216, Classifier Loss 0.44581425189971924, Total Loss 139.22337341308594\n",
      "1: Encoding Loss 16.09946060180664, Transition Loss 1.6178197860717773, Classifier Loss 0.42423856258392334, Total Loss 139.66773986816406\n",
      "1: Encoding Loss 16.959089279174805, Transition Loss 1.175431489944458, Classifier Loss 0.46779435873031616, Total Loss 149.00413513183594\n",
      "1: Encoding Loss 15.429081916809082, Transition Loss 2.1823770999908447, Classifier Loss 0.41145214438438416, Total Loss 134.59266662597656\n",
      "1: Encoding Loss 15.108036994934082, Transition Loss 2.5831916332244873, Classifier Loss 0.4285256266593933, Total Loss 134.53407287597656\n",
      "1: Encoding Loss 17.697050094604492, Transition Loss 3.18217134475708, Classifier Loss 0.4870067238807678, Total Loss 156.15585327148438\n",
      "1: Encoding Loss 16.467281341552734, Transition Loss 2.9665088653564453, Classifier Loss 0.45922836661338806, Total Loss 145.9131317138672\n",
      "1: Encoding Loss 15.43415355682373, Transition Loss 2.2726049423217773, Classifier Loss 0.41501501202583313, Total Loss 135.01547241210938\n",
      "1: Encoding Loss 15.72496509552002, Transition Loss 3.069507122039795, Classifier Loss 0.44147682189941406, Total Loss 139.7252655029297\n",
      "1: Encoding Loss 14.958944320678711, Transition Loss 0.9209083318710327, Classifier Loss 0.43822258710861206, Total Loss 133.9442901611328\n",
      "1: Encoding Loss 14.363716125488281, Transition Loss 1.739767074584961, Classifier Loss 0.4474896192550659, Total Loss 131.62716674804688\n",
      "1: Encoding Loss 16.100627899169922, Transition Loss 2.3089630603790283, Classifier Loss 0.43776726722717285, Total Loss 141.3040771484375\n",
      "1: Encoding Loss 17.014074325561523, Transition Loss 2.2974507808685303, Classifier Loss 0.48546290397644043, Total Loss 151.54971313476562\n",
      "1: Encoding Loss 15.566986083984375, Transition Loss 2.2634875774383545, Classifier Loss 0.472781777381897, Total Loss 141.5854949951172\n",
      "1: Encoding Loss 14.30378246307373, Transition Loss 1.030697226524353, Classifier Loss 0.4414060413837433, Total Loss 130.37557983398438\n",
      "1: Encoding Loss 16.8456974029541, Transition Loss 2.3083043098449707, Classifier Loss 0.4115039110183716, Total Loss 143.1479034423828\n",
      "1: Encoding Loss 15.525250434875488, Transition Loss 3.0270001888275146, Classifier Loss 0.43604031205177307, Total Loss 137.96633911132812\n",
      "1: Encoding Loss 15.417211532592773, Transition Loss 3.301267147064209, Classifier Loss 0.4453340768814087, Total Loss 138.35719299316406\n",
      "1: Encoding Loss 15.881800651550293, Transition Loss 5.279214859008789, Classifier Loss 0.4113365411758423, Total Loss 138.53614807128906\n",
      "1: Encoding Loss 14.978096008300781, Transition Loss 2.910895824432373, Classifier Loss 0.464057058095932, Total Loss 137.43862915039062\n",
      "1: Encoding Loss 14.603195190429688, Transition Loss 1.7411013841629028, Classifier Loss 0.41920408606529236, Total Loss 130.23602294921875\n",
      "1: Encoding Loss 16.662797927856445, Transition Loss 1.1639810800552368, Classifier Loss 0.41204628348350525, Total Loss 141.6470184326172\n",
      "1: Encoding Loss 15.536354064941406, Transition Loss 2.9720168113708496, Classifier Loss 0.47364968061447144, Total Loss 141.77191162109375\n",
      "1: Encoding Loss 15.814709663391113, Transition Loss 5.487863540649414, Classifier Loss 0.498991459608078, Total Loss 146.98255920410156\n",
      "1: Encoding Loss 16.910791397094727, Transition Loss 2.652662515640259, Classifier Loss 0.4070051312446594, Total Loss 143.22633361816406\n",
      "1: Encoding Loss 15.697237014770508, Transition Loss 1.152950406074524, Classifier Loss 0.4492628872394562, Total Loss 139.57089233398438\n",
      "1: Encoding Loss 15.784311294555664, Transition Loss -0.24198377132415771, Classifier Loss 0.4331444799900055, Total Loss 138.02023315429688\n",
      "1: Encoding Loss 16.245929718017578, Transition Loss 2.7805581092834473, Classifier Loss 0.3525373935699463, Total Loss 133.84153747558594\n",
      "1: Encoding Loss 15.92448616027832, Transition Loss 3.270637035369873, Classifier Loss 0.4521525502204895, Total Loss 142.0704345703125\n",
      "1: Encoding Loss 17.19143295288086, Transition Loss 3.0741307735443115, Classifier Loss 0.47353965044021606, Total Loss 151.73220825195312\n",
      "1: Encoding Loss 15.4085111618042, Transition Loss 1.9130713939666748, Classifier Loss 0.42506375908851624, Total Loss 135.72267150878906\n",
      "1: Encoding Loss 17.31978416442871, Transition Loss 3.848611354827881, Classifier Loss 0.47898298501968384, Total Loss 153.35646057128906\n",
      "1: Encoding Loss 15.390054702758789, Transition Loss 5.258247375488281, Classifier Loss 0.4723048806190491, Total Loss 141.6741180419922\n",
      "1: Encoding Loss 16.773174285888672, Transition Loss 3.827021360397339, Classifier Loss 0.42407330870628357, Total Loss 144.5771942138672\n",
      "1: Encoding Loss 16.423311233520508, Transition Loss 3.0816054344177246, Classifier Loss 0.45478352904319763, Total Loss 145.2508544921875\n",
      "1: Encoding Loss 16.29654312133789, Transition Loss 0.5342178344726562, Classifier Loss 0.4370139539241791, Total Loss 141.6943359375\n",
      "1: Encoding Loss 15.41340446472168, Transition Loss 4.9319987297058105, Classifier Loss 0.4364655017852783, Total Loss 138.0997772216797\n",
      "1: Encoding Loss 16.489608764648438, Transition Loss 1.8869506120681763, Classifier Loss 0.42957672476768494, Total Loss 142.6501007080078\n",
      "1: Encoding Loss 16.74207878112793, Transition Loss 1.3572239875793457, Classifier Loss 0.43985337018966675, Total Loss 144.98069763183594\n",
      "1: Encoding Loss 15.530965805053711, Transition Loss 2.4416470527648926, Classifier Loss 0.45646676421165466, Total Loss 139.8091278076172\n",
      "1: Encoding Loss 16.173736572265625, Transition Loss 2.589207410812378, Classifier Loss 0.42314624786376953, Total Loss 140.3927459716797\n",
      "1: Encoding Loss 15.717645645141602, Transition Loss 1.2378290891647339, Classifier Loss 0.41084110736846924, Total Loss 135.88511657714844\n",
      "1: Encoding Loss 16.945222854614258, Transition Loss 2.262296676635742, Classifier Loss 0.46627914905548096, Total Loss 149.2041778564453\n",
      "1: Encoding Loss 15.475103378295898, Transition Loss 3.3713951110839844, Classifier Loss 0.3863387107849121, Total Loss 132.8330535888672\n",
      "1: Encoding Loss 15.342082023620605, Transition Loss -0.09150877594947815, Classifier Loss 0.39199212193489075, Total Loss 131.25167846679688\n",
      "1: Encoding Loss 14.652748107910156, Transition Loss 1.1853362321853638, Classifier Loss 0.42093998193740845, Total Loss 130.484619140625\n",
      "1: Encoding Loss 15.436746597290039, Transition Loss 1.630812406539917, Classifier Loss 0.46792125701904297, Total Loss 140.06494140625\n",
      "1: Encoding Loss 14.817179679870605, Transition Loss 1.08748459815979, Classifier Loss 0.4264519512653351, Total Loss 131.9832763671875\n",
      "1: Encoding Loss 18.176250457763672, Transition Loss 2.68558669090271, Classifier Loss 0.4654885530471802, Total Loss 156.68060302734375\n",
      "1: Encoding Loss 16.369503021240234, Transition Loss 3.6357734203338623, Classifier Loss 0.4640263319015503, Total Loss 146.073974609375\n",
      "1: Encoding Loss 14.909358024597168, Transition Loss 2.5897154808044434, Classifier Loss 0.4388645589351654, Total Loss 134.3784942626953\n",
      "1: Encoding Loss 16.082393646240234, Transition Loss 2.032513380050659, Classifier Loss 0.4201319217681885, Total Loss 139.32057189941406\n",
      "1: Encoding Loss 16.6689453125, Transition Loss 4.173490047454834, Classifier Loss 0.43381035327911377, Total Loss 145.06411743164062\n",
      "1: Encoding Loss 13.830245971679688, Transition Loss 1.7089500427246094, Classifier Loss 0.4325452148914337, Total Loss 126.91957092285156\n",
      "1: Encoding Loss 17.970199584960938, Transition Loss 4.281209945678711, Classifier Loss 0.446328341960907, Total Loss 154.16651916503906\n",
      "1: Encoding Loss 15.610239028930664, Transition Loss 0.3961533010005951, Classifier Loss 0.39095333218574524, Total Loss 132.9152374267578\n",
      "1: Encoding Loss 17.61111831665039, Transition Loss 2.895298957824707, Classifier Loss 0.44401130080223083, Total Loss 151.22596740722656\n",
      "1: Encoding Loss 17.57784652709961, Transition Loss 0.24785065650939941, Classifier Loss 0.43268758058547974, Total Loss 148.83497619628906\n",
      "1: Encoding Loss 14.013914108276367, Transition Loss 1.7917948961257935, Classifier Loss 0.40479546785354614, Total Loss 125.27975463867188\n",
      "1: Encoding Loss 15.527758598327637, Transition Loss 5.6772589683532715, Classifier Loss 0.504243016242981, Total Loss 145.86175537109375\n",
      "1: Encoding Loss 15.48686408996582, Transition Loss 3.6827938556671143, Classifier Loss 0.469570517539978, Total Loss 141.35134887695312\n",
      "1: Encoding Loss 16.729389190673828, Transition Loss 2.906122922897339, Classifier Loss 0.3910459876060486, Total Loss 140.64337158203125\n",
      "1: Encoding Loss 16.952957153320312, Transition Loss 4.738628387451172, Classifier Loss 0.4654674530029297, Total Loss 150.15994262695312\n",
      "1: Encoding Loss 13.679707527160645, Transition Loss -0.10394954681396484, Classifier Loss 0.36152833700180054, Total Loss 118.23104095458984\n",
      "1: Encoding Loss 17.999574661254883, Transition Loss -0.7230645418167114, Classifier Loss 0.3909614384174347, Total Loss 147.0933074951172\n",
      "1: Encoding Loss 18.211496353149414, Transition Loss 3.473220109939575, Classifier Loss 0.426330029964447, Total Loss 153.291259765625\n",
      "1: Encoding Loss 15.78448486328125, Transition Loss 0.389781653881073, Classifier Loss 0.3987174928188324, Total Loss 134.7345733642578\n",
      "1: Encoding Loss 15.447232246398926, Transition Loss 3.5952274799346924, Classifier Loss 0.4261428713798523, Total Loss 136.73577880859375\n",
      "1: Encoding Loss 16.091476440429688, Transition Loss 1.980383276939392, Classifier Loss 0.4234801232814789, Total Loss 139.6890411376953\n",
      "1: Encoding Loss 15.081871032714844, Transition Loss 0.9574173092842102, Classifier Loss 0.4555908739566803, Total Loss 136.4332733154297\n",
      "1: Encoding Loss 17.43364715576172, Transition Loss 1.9469417333602905, Classifier Loss 0.42428961396217346, Total Loss 147.80963134765625\n",
      "1: Encoding Loss 16.90631103515625, Transition Loss 2.9710826873779297, Classifier Loss 0.39682310819625854, Total Loss 142.30860900878906\n",
      "1: Encoding Loss 17.22677993774414, Transition Loss 2.3194239139556885, Classifier Loss 0.4014602303504944, Total Loss 144.43446350097656\n",
      "1: Encoding Loss 16.19157600402832, Transition Loss 1.725929617881775, Classifier Loss 0.4287227392196655, Total Loss 140.71209716796875\n",
      "1: Encoding Loss 15.534576416015625, Transition Loss 2.5135576725006104, Classifier Loss 0.4495410621166229, Total Loss 139.16697692871094\n",
      "1: Encoding Loss 15.641143798828125, Transition Loss 4.141520977020264, Classifier Loss 0.4084940552711487, Total Loss 136.35287475585938\n",
      "1: Encoding Loss 16.46047019958496, Transition Loss 3.161963939666748, Classifier Loss 0.43980690836906433, Total Loss 144.00830078125\n",
      "1: Encoding Loss 15.466557502746582, Transition Loss 1.217154860496521, Classifier Loss 0.43749910593032837, Total Loss 137.03611755371094\n",
      "1: Encoding Loss 17.065200805664062, Transition Loss 5.221881866455078, Classifier Loss 0.42684340476989746, Total Loss 147.164306640625\n",
      "1: Encoding Loss 15.880212783813477, Transition Loss 2.572896957397461, Classifier Loss 0.40772101283073425, Total Loss 137.08253479003906\n",
      "1: Encoding Loss 15.895419120788574, Transition Loss 1.9086414575576782, Classifier Loss 0.473774254322052, Total Loss 143.51339721679688\n",
      "1: Encoding Loss 15.742401123046875, Transition Loss 1.7926326990127563, Classifier Loss 0.4415608048439026, Total Loss 139.32754516601562\n",
      "1: Encoding Loss 16.511226654052734, Transition Loss 1.232870101928711, Classifier Loss 0.42978358268737793, Total Loss 142.53887939453125\n",
      "1: Encoding Loss 15.161270141601562, Transition Loss 2.8564305305480957, Classifier Loss 0.3781780004501343, Total Loss 129.92799377441406\n",
      "1: Encoding Loss 15.485072135925293, Transition Loss 0.5896010398864746, Classifier Loss 0.3906116783618927, Total Loss 132.2074432373047\n",
      "1: Encoding Loss 15.746091842651367, Transition Loss 1.2298928499221802, Classifier Loss 0.3954685628414154, Total Loss 134.51536560058594\n",
      "1: Encoding Loss 15.82213306427002, Transition Loss 4.416263103485107, Classifier Loss 0.4863303303718567, Total Loss 145.33233642578125\n",
      "1: Encoding Loss 16.04776954650879, Transition Loss 1.7647082805633545, Classifier Loss 0.38331419229507446, Total Loss 135.3239288330078\n",
      "1: Encoding Loss 14.966692924499512, Transition Loss 1.1984796524047852, Classifier Loss 0.432791531085968, Total Loss 133.55870056152344\n",
      "1: Encoding Loss 16.94631004333496, Transition Loss 1.4499197006225586, Classifier Loss 0.42608118057250977, Total Loss 144.86595153808594\n",
      "1: Encoding Loss 15.810015678405762, Transition Loss 1.4701530933380127, Classifier Loss 0.3937537670135498, Total Loss 134.8235321044922\n",
      "1: Encoding Loss 16.29078483581543, Transition Loss 3.307326078414917, Classifier Loss 0.4037022590637207, Total Loss 139.4378662109375\n",
      "1: Encoding Loss 15.6846342086792, Transition Loss 1.9204007387161255, Classifier Loss 0.43916618824005127, Total Loss 138.79258728027344\n",
      "1: Encoding Loss 16.744518280029297, Transition Loss 2.8471245765686035, Classifier Loss 0.43502068519592285, Total Loss 145.1080322265625\n",
      "1: Encoding Loss 16.024845123291016, Transition Loss 2.163487434387207, Classifier Loss 0.3940702974796295, Total Loss 136.42149353027344\n",
      "1: Encoding Loss 16.429973602294922, Transition Loss 2.9288864135742188, Classifier Loss 0.43230873346328735, Total Loss 142.98228454589844\n",
      "1: Encoding Loss 16.269515991210938, Transition Loss 4.256622314453125, Classifier Loss 0.41578152775764465, Total Loss 140.8979034423828\n",
      "1: Encoding Loss 14.876962661743164, Transition Loss 0.8564125299453735, Classifier Loss 0.4270794689655304, Total Loss 132.31228637695312\n",
      "1: Encoding Loss 15.892362594604492, Transition Loss 0.9288485050201416, Classifier Loss 0.44775933027267456, Total Loss 140.50164794921875\n",
      "1: Encoding Loss 15.85229778289795, Transition Loss 0.03362637758255005, Classifier Loss 0.4271906018257141, Total Loss 137.84629821777344\n",
      "1: Encoding Loss 14.710668563842773, Transition Loss 2.715862989425659, Classifier Loss 0.4665849804878235, Total Loss 136.0088653564453\n",
      "1: Encoding Loss 15.992033004760742, Transition Loss 1.843160629272461, Classifier Loss 0.424826979637146, Total Loss 139.1721649169922\n",
      "1: Encoding Loss 16.037439346313477, Transition Loss 2.853841781616211, Classifier Loss 0.4463692009449005, Total Loss 142.0030975341797\n",
      "1: Encoding Loss 14.349863052368164, Transition Loss 0.831612229347229, Classifier Loss 0.4189900755882263, Total Loss 128.33082580566406\n",
      "1: Encoding Loss 15.6470365524292, Transition Loss 4.7385406494140625, Classifier Loss 0.42209526896476746, Total Loss 137.98716735839844\n",
      "1: Encoding Loss 15.997570991516113, Transition Loss 0.7885042428970337, Classifier Loss 0.40538835525512695, Total Loss 136.83966064453125\n",
      "1: Encoding Loss 14.770780563354492, Transition Loss 3.749422550201416, Classifier Loss 0.44372808933258057, Total Loss 134.4972686767578\n",
      "1: Encoding Loss 15.85963249206543, Transition Loss 2.777773857116699, Classifier Loss 0.4251534342765808, Total Loss 138.7842559814453\n",
      "1: Encoding Loss 14.297998428344727, Transition Loss 0.7427147030830383, Classifier Loss 0.4196612238883972, Total Loss 128.05120849609375\n",
      "1: Encoding Loss 14.363037109375, Transition Loss 0.941857099533081, Classifier Loss 0.39427128434181213, Total Loss 125.98208618164062\n",
      "1: Encoding Loss 16.604907989501953, Transition Loss 3.4716625213623047, Classifier Loss 0.4755687117576599, Total Loss 148.57498168945312\n",
      "1: Encoding Loss 16.033519744873047, Transition Loss 1.3475220203399658, Classifier Loss 0.3740254044532776, Total Loss 134.14266967773438\n",
      "1: Encoding Loss 14.166104316711426, Transition Loss 1.842247486114502, Classifier Loss 0.35827550292015076, Total Loss 121.56107330322266\n",
      "1: Encoding Loss 15.980430603027344, Transition Loss 1.3962591886520386, Classifier Loss 0.4133625328540802, Total Loss 137.77734375\n",
      "1: Encoding Loss 16.165876388549805, Transition Loss 1.8698968887329102, Classifier Loss 0.44079846143722534, Total Loss 141.82305908203125\n",
      "1: Encoding Loss 14.898298263549805, Transition Loss 0.6774373054504395, Classifier Loss 0.39228323101997375, Total Loss 128.88909912109375\n",
      "1: Encoding Loss 15.106971740722656, Transition Loss 1.1578956842422485, Classifier Loss 0.3863179087638855, Total Loss 129.73678588867188\n",
      "1: Encoding Loss 15.28015422821045, Transition Loss 4.586610794067383, Classifier Loss 0.43766355514526367, Total Loss 137.2819366455078\n",
      "1: Encoding Loss 15.081174850463867, Transition Loss 2.9089841842651367, Classifier Loss 0.4250833988189697, Total Loss 134.1589813232422\n",
      "1: Encoding Loss 15.166674613952637, Transition Loss 2.9160401821136475, Classifier Loss 0.4275839924812317, Total Loss 134.92486572265625\n",
      "1: Encoding Loss 14.740605354309082, Transition Loss 1.3453394174575806, Classifier Loss 0.4637947082519531, Total Loss 135.36123657226562\n",
      "1: Encoding Loss 14.443354606628418, Transition Loss -0.26836806535720825, Classifier Loss 0.4000377655029297, Total Loss 126.66380310058594\n",
      "1: Encoding Loss 16.502851486206055, Transition Loss 3.2383310794830322, Classifier Loss 0.405222088098526, Total Loss 140.83465576171875\n",
      "1: Encoding Loss 15.468111991882324, Transition Loss 1.4167076349258423, Classifier Loss 0.4221706986427307, Total Loss 135.59243774414062\n",
      "1: Encoding Loss 15.85803508758545, Transition Loss 2.3194820880889893, Classifier Loss 0.4063881039619446, Total Loss 136.71481323242188\n",
      "1: Encoding Loss 15.691906929016113, Transition Loss 1.76302170753479, Classifier Loss 0.3854854702949524, Total Loss 133.4051971435547\n",
      "1: Encoding Loss 14.22797966003418, Transition Loss 1.5434961318969727, Classifier Loss 0.34708964824676514, Total Loss 120.69424438476562\n",
      "1: Encoding Loss 14.572565078735352, Transition Loss 3.8911800384521484, Classifier Loss 0.4233953058719635, Total Loss 131.33140563964844\n",
      "1: Encoding Loss 15.303421020507812, Transition Loss 3.065687894821167, Classifier Loss 0.4269227385520935, Total Loss 135.73907470703125\n",
      "1: Encoding Loss 14.731958389282227, Transition Loss 0.44194501638412476, Classifier Loss 0.41456925868988037, Total Loss 130.02545166015625\n",
      "1: Encoding Loss 15.824457168579102, Transition Loss 5.93978214263916, Classifier Loss 0.4545442759990692, Total Loss 142.77708435058594\n",
      "1: Encoding Loss 14.42480182647705, Transition Loss 1.5969173908233643, Classifier Loss 0.40141692757606506, Total Loss 127.32926940917969\n",
      "1: Encoding Loss 15.122441291809082, Transition Loss 1.9277585744857788, Classifier Loss 0.4770285487174988, Total Loss 139.20860290527344\n",
      "1: Encoding Loss 17.365741729736328, Transition Loss 2.113008737564087, Classifier Loss 0.4063955545425415, Total Loss 145.67921447753906\n",
      "1: Encoding Loss 16.438316345214844, Transition Loss 2.3551173210144043, Classifier Loss 0.3937076926231384, Total Loss 138.94271850585938\n",
      "1: Encoding Loss 17.89216423034668, Transition Loss 3.157438039779663, Classifier Loss 0.48856544494628906, Total Loss 157.47250366210938\n",
      "1: Encoding Loss 15.7055082321167, Transition Loss 2.529611349105835, Classifier Loss 0.4659320116043091, Total Loss 141.83810424804688\n",
      "1: Encoding Loss 16.214618682861328, Transition Loss 1.6705290079116821, Classifier Loss 0.45080137252807617, Total Loss 143.03607177734375\n",
      "1: Encoding Loss 15.15165901184082, Transition Loss 1.6092627048492432, Classifier Loss 0.40508514642715454, Total Loss 132.0621795654297\n",
      "1: Encoding Loss 16.435853958129883, Transition Loss 4.805055618286133, Classifier Loss 0.3465324342250824, Total Loss 135.19039916992188\n",
      "1: Encoding Loss 15.111498832702637, Transition Loss 3.2877249717712402, Classifier Loss 0.42435595393180847, Total Loss 134.41969299316406\n",
      "1: Encoding Loss 16.881362915039062, Transition Loss 6.139928340911865, Classifier Loss 0.46540939807891846, Total Loss 150.2851104736328\n",
      "1: Encoding Loss 16.367900848388672, Transition Loss 2.526334285736084, Classifier Loss 0.3689042329788208, Total Loss 136.10836791992188\n",
      "1: Encoding Loss 15.042196273803711, Transition Loss 1.8220839500427246, Classifier Loss 0.4469084143638611, Total Loss 135.67286682128906\n",
      "1: Encoding Loss 15.530130386352539, Transition Loss 2.305097818374634, Classifier Loss 0.3925217390060425, Total Loss 133.35499572753906\n",
      "1: Encoding Loss 15.52049732208252, Transition Loss 3.5610461235046387, Classifier Loss 0.42509445548057556, Total Loss 137.05685424804688\n",
      "1: Encoding Loss 15.324885368347168, Transition Loss 1.4262768030166626, Classifier Loss 0.39683496952056885, Total Loss 132.2033233642578\n",
      "1: Encoding Loss 16.903778076171875, Transition Loss 3.1922292709350586, Classifier Loss 0.4078059792518616, Total Loss 143.48016357421875\n",
      "1: Encoding Loss 14.806796073913574, Transition Loss 0.35812076926231384, Classifier Loss 0.4181162416934967, Total Loss 130.795654296875\n",
      "1: Encoding Loss 17.37594985961914, Transition Loss 1.5678465366363525, Classifier Loss 0.3751325011253357, Total Loss 142.39608764648438\n",
      "1: Encoding Loss 16.600887298583984, Transition Loss 0.6842978000640869, Classifier Loss 0.3830169439315796, Total Loss 138.1807403564453\n",
      "1: Encoding Loss 15.293269157409668, Transition Loss 2.6342597007751465, Classifier Loss 0.3893822133541107, Total Loss 131.75155639648438\n",
      "1: Encoding Loss 14.88421630859375, Transition Loss 1.4015257358551025, Classifier Loss 0.41340914368629456, Total Loss 131.20681762695312\n",
      "1: Encoding Loss 15.682171821594238, Transition Loss 0.4139378070831299, Classifier Loss 0.3813067078590393, Total Loss 132.3892822265625\n",
      "1: Encoding Loss 16.296838760375977, Transition Loss 0.9823541641235352, Classifier Loss 0.3910045921802521, Total Loss 137.27444458007812\n",
      "1: Encoding Loss 15.066522598266602, Transition Loss 3.0253891944885254, Classifier Loss 0.43195560574531555, Total Loss 134.8048553466797\n",
      "1: Encoding Loss 15.880546569824219, Transition Loss 2.8745508193969727, Classifier Loss 0.44058579206466675, Total Loss 140.49168395996094\n",
      "1: Encoding Loss 16.073776245117188, Transition Loss 1.7469300031661987, Classifier Loss 0.3853621482849121, Total Loss 135.6776580810547\n",
      "1: Encoding Loss 17.277185440063477, Transition Loss 3.09846568107605, Classifier Loss 0.39030343294143677, Total Loss 143.93283081054688\n",
      "1: Encoding Loss 15.765295028686523, Transition Loss 1.2622082233428955, Classifier Loss 0.41450631618499756, Total Loss 136.5472869873047\n",
      "1: Encoding Loss 14.243121147155762, Transition Loss 2.901723861694336, Classifier Loss 0.39215755462646484, Total Loss 125.83517456054688\n",
      "1: Encoding Loss 17.88988494873047, Transition Loss 3.1510393619537354, Classifier Loss 0.40225672721862793, Total Loss 148.82540893554688\n",
      "1: Encoding Loss 17.49642562866211, Transition Loss 2.2584269046783447, Classifier Loss 0.3892812728881836, Total Loss 144.81005859375\n",
      "1: Encoding Loss 15.185121536254883, Transition Loss 1.6940150260925293, Classifier Loss 0.4213944375514984, Total Loss 133.92779541015625\n",
      "1: Encoding Loss 14.982717514038086, Transition Loss 0.9162431359291077, Classifier Loss 0.43700411915779114, Total Loss 133.96322631835938\n",
      "1: Encoding Loss 14.575296401977539, Transition Loss 3.1810824871063232, Classifier Loss 0.4116305708885193, Total Loss 129.88726806640625\n",
      "1: Encoding Loss 15.415351867675781, Transition Loss 3.744880437850952, Classifier Loss 0.43563175201416016, Total Loss 137.55323791503906\n",
      "1: Encoding Loss 14.208430290222168, Transition Loss 3.23746657371521, Classifier Loss 0.40505295991897583, Total Loss 127.05086517333984\n",
      "1: Encoding Loss 15.426580429077148, Transition Loss 5.2660441398620605, Classifier Loss 0.363186776638031, Total Loss 130.9845733642578\n",
      "1: Encoding Loss 16.08353042602539, Transition Loss 2.2677910327911377, Classifier Loss 0.4868764579296112, Total Loss 146.095947265625\n",
      "1: Encoding Loss 15.292488098144531, Transition Loss 3.424917697906494, Classifier Loss 0.3477271795272827, Total Loss 127.89761352539062\n",
      "1: Encoding Loss 17.08326530456543, Transition Loss 3.218372344970703, Classifier Loss 0.43849438428878784, Total Loss 147.63638305664062\n",
      "1: Encoding Loss 16.1611385345459, Transition Loss 2.875744581222534, Classifier Loss 0.452861487865448, Total Loss 143.40328979492188\n",
      "1: Encoding Loss 16.054264068603516, Transition Loss 2.9569060802459717, Classifier Loss 0.47274866700172424, Total Loss 144.78321838378906\n",
      "1: Encoding Loss 15.441741943359375, Transition Loss 2.7206778526306152, Classifier Loss 0.4334663152694702, Total Loss 137.08535766601562\n",
      "1: Encoding Loss 15.799077033996582, Transition Loss 2.89109468460083, Classifier Loss 0.406979501247406, Total Loss 136.6488494873047\n",
      "1: Encoding Loss 14.111310005187988, Transition Loss 1.0987582206726074, Classifier Loss 0.39347535371780396, Total Loss 124.45490264892578\n",
      "1: Encoding Loss 15.061619758605957, Transition Loss 3.0891835689544678, Classifier Loss 0.4140692353248596, Total Loss 133.01231384277344\n",
      "1: Encoding Loss 16.028413772583008, Transition Loss 1.305607557296753, Classifier Loss 0.4528747498989105, Total Loss 141.98020935058594\n",
      "1: Encoding Loss 14.544987678527832, Transition Loss 2.3405988216400146, Classifier Loss 0.3787845969200134, Total Loss 126.08463287353516\n",
      "1: Encoding Loss 16.542631149291992, Transition Loss 4.308804512023926, Classifier Loss 0.41108009219169617, Total Loss 142.0873260498047\n",
      "1: Encoding Loss 15.695253372192383, Transition Loss 2.538050413131714, Classifier Loss 0.38301530480384827, Total Loss 133.48826599121094\n",
      "1: Encoding Loss 15.611239433288574, Transition Loss 3.2111713886260986, Classifier Loss 0.4719413220882416, Total Loss 142.1460418701172\n",
      "1: Encoding Loss 15.37554931640625, Transition Loss 2.1263198852539062, Classifier Loss 0.42343997955322266, Total Loss 135.44781494140625\n",
      "1: Encoding Loss 14.889200210571289, Transition Loss 0.741073489189148, Classifier Loss 0.41814279556274414, Total Loss 131.44590759277344\n",
      "1: Encoding Loss 14.50269603729248, Transition Loss 0.9756238460540771, Classifier Loss 0.4613499641418457, Total Loss 133.5414276123047\n",
      "1: Encoding Loss 17.088394165039062, Transition Loss 1.8192951679229736, Classifier Loss 0.4120174050331116, Total Loss 144.4598388671875\n",
      "1: Encoding Loss 15.517814636230469, Transition Loss 1.1453373432159424, Classifier Loss 0.4391210079193115, Total Loss 137.47711181640625\n",
      "1: Encoding Loss 14.280194282531738, Transition Loss 1.0656498670578003, Classifier Loss 0.39958009123802185, Total Loss 126.06543731689453\n",
      "1: Encoding Loss 14.559402465820312, Transition Loss 0.5072313547134399, Classifier Loss 0.36052918434143066, Total Loss 123.61222839355469\n",
      "1: Encoding Loss 16.508987426757812, Transition Loss 3.501631259918213, Classifier Loss 0.4323995113372803, Total Loss 143.6945343017578\n",
      "1: Encoding Loss 15.674825668334961, Transition Loss 3.282054901123047, Classifier Loss 0.4080751836299896, Total Loss 136.16929626464844\n",
      "1: Encoding Loss 15.785360336303711, Transition Loss 0.43248695135116577, Classifier Loss 0.44477272033691406, Total Loss 139.3624267578125\n",
      "1: Encoding Loss 14.886609077453613, Transition Loss 1.7041301727294922, Classifier Loss 0.4226169288158417, Total Loss 132.26300048828125\n",
      "1: Encoding Loss 14.273567199707031, Transition Loss 0.41829195618629456, Classifier Loss 0.4085314869880676, Total Loss 126.66187286376953\n",
      "1: Encoding Loss 15.524470329284668, Transition Loss 2.6741433143615723, Classifier Loss 0.3776598572731018, Total Loss 131.9824676513672\n",
      "1: Encoding Loss 16.56740951538086, Transition Loss 1.3721649646759033, Classifier Loss 0.42494529485702515, Total Loss 142.44784545898438\n",
      "1: Encoding Loss 14.178479194641113, Transition Loss 0.1809864640235901, Classifier Loss 0.3851153254508972, Total Loss 123.6548080444336\n",
      "1: Encoding Loss 13.182154655456543, Transition Loss 1.2272202968597412, Classifier Loss 0.3674313724040985, Total Loss 116.32696533203125\n",
      "1: Encoding Loss 14.533086776733398, Transition Loss 2.302028179168701, Classifier Loss 0.3948696255683899, Total Loss 127.6063003540039\n",
      "1: Encoding Loss 15.113935470581055, Transition Loss 2.7242820262908936, Classifier Loss 0.4253518283367157, Total Loss 134.30850219726562\n",
      "1: Encoding Loss 15.9714937210083, Transition Loss 2.4965453147888184, Classifier Loss 0.44850870966911316, Total Loss 141.67845153808594\n",
      "1: Encoding Loss 16.459054946899414, Transition Loss 2.9933204650878906, Classifier Loss 0.3764526844024658, Total Loss 137.596923828125\n",
      "1: Encoding Loss 16.460525512695312, Transition Loss 2.4399309158325195, Classifier Loss 0.38849854469299316, Total Loss 138.58897399902344\n",
      "1: Encoding Loss 15.669988632202148, Transition Loss 3.258389472961426, Classifier Loss 0.43581074476242065, Total Loss 138.9043731689453\n",
      "1: Encoding Loss 15.550992012023926, Transition Loss 2.2163171768188477, Classifier Loss 0.4041571617126465, Total Loss 134.6082000732422\n",
      "1: Encoding Loss 15.500221252441406, Transition Loss 3.8884241580963135, Classifier Loss 0.4135769307613373, Total Loss 135.91439819335938\n",
      "1: Encoding Loss 15.734342575073242, Transition Loss 2.5066239833831787, Classifier Loss 0.4256245493888855, Total Loss 137.97116088867188\n",
      "1: Encoding Loss 14.87730598449707, Transition Loss -0.4609018862247467, Classifier Loss 0.4000256657600403, Total Loss 129.26622009277344\n",
      "1: Encoding Loss 14.537313461303711, Transition Loss 1.969883918762207, Classifier Loss 0.414760559797287, Total Loss 129.48788452148438\n",
      "1: Encoding Loss 14.068941116333008, Transition Loss 1.00227952003479, Classifier Loss 0.36012041568756104, Total Loss 120.82659912109375\n",
      "1: Encoding Loss 15.964874267578125, Transition Loss 2.3874666690826416, Classifier Loss 0.484878808259964, Total Loss 145.23211669921875\n",
      "1: Encoding Loss 15.503094673156738, Transition Loss 1.506274700164795, Classifier Loss 0.3987802267074585, Total Loss 133.4990997314453\n",
      "1: Encoding Loss 14.342844009399414, Transition Loss 3.753485679626465, Classifier Loss 0.47476792335510254, Total Loss 135.03524780273438\n",
      "1: Encoding Loss 15.612831115722656, Transition Loss 3.7966489791870117, Classifier Loss 0.44901448488235474, Total Loss 140.0970916748047\n",
      "1: Encoding Loss 15.851800918579102, Transition Loss 3.0100045204162598, Classifier Loss 0.46833741664886475, Total Loss 143.14854431152344\n",
      "1: Encoding Loss 15.519680976867676, Transition Loss 1.3029910326004028, Classifier Loss 0.400890588760376, Total Loss 133.7283477783203\n",
      "1: Encoding Loss 15.253888130187988, Transition Loss 0.5376178026199341, Classifier Loss 0.4090595543384552, Total Loss 132.6443328857422\n",
      "1: Encoding Loss 13.877212524414062, Transition Loss 1.1865280866622925, Classifier Loss 0.41848304867744446, Total Loss 125.58618927001953\n",
      "1: Encoding Loss 16.19868278503418, Transition Loss 3.233410120010376, Classifier Loss 0.4708942770957947, Total Loss 145.57489013671875\n",
      "1: Encoding Loss 15.625730514526367, Transition Loss 2.4210188388824463, Classifier Loss 0.4214733839035034, Total Loss 136.87014770507812\n",
      "1: Encoding Loss 15.268524169921875, Transition Loss 1.6409242153167725, Classifier Loss 0.4019690752029419, Total Loss 132.46441650390625\n",
      "1: Encoding Loss 14.903158187866211, Transition Loss 3.472374200820923, Classifier Loss 0.39316755533218384, Total Loss 130.12466430664062\n",
      "1: Encoding Loss 13.928509712219238, Transition Loss 1.4464435577392578, Classifier Loss 0.3947827219963074, Total Loss 123.6279067993164\n",
      "1: Encoding Loss 15.865506172180176, Transition Loss 0.7663596272468567, Classifier Loss 0.40684375166893005, Total Loss 136.1839599609375\n",
      "1: Encoding Loss 15.08625602722168, Transition Loss 2.3409481048583984, Classifier Loss 0.4105243384838104, Total Loss 132.50636291503906\n",
      "1: Encoding Loss 14.345819473266602, Transition Loss 0.568023681640625, Classifier Loss 0.366413950920105, Total Loss 122.94352722167969\n",
      "1: Encoding Loss 14.835241317749023, Transition Loss 3.4456794261932373, Classifier Loss 0.41813212633132935, Total Loss 132.2029266357422\n",
      "1: Encoding Loss 15.731053352355957, Transition Loss 2.8834149837493896, Classifier Loss 0.4398459792137146, Total Loss 139.52427673339844\n",
      "1: Encoding Loss 15.218727111816406, Transition Loss 3.2117700576782227, Classifier Loss 0.4107392430305481, Total Loss 133.67100524902344\n",
      "1: Encoding Loss 14.376523971557617, Transition Loss 0.2941462993621826, Classifier Loss 0.4445498287677765, Total Loss 130.831787109375\n",
      "1: Encoding Loss 14.552574157714844, Transition Loss 3.1368918418884277, Classifier Loss 0.40999090671539307, Total Loss 129.56930541992188\n",
      "1: Encoding Loss 14.074121475219727, Transition Loss 2.4679672718048096, Classifier Loss 0.37679973244667053, Total Loss 123.11190032958984\n",
      "1: Encoding Loss 15.684562683105469, Transition Loss 1.3556922674179077, Classifier Loss 0.4201244115829468, Total Loss 136.66209411621094\n",
      "1: Encoding Loss 15.967241287231445, Transition Loss 1.7631893157958984, Classifier Loss 0.42278680205345154, Total Loss 138.7873992919922\n",
      "1: Encoding Loss 15.570806503295898, Transition Loss 2.0935800075531006, Classifier Loss 0.4102000594139099, Total Loss 135.28228759765625\n",
      "1: Encoding Loss 15.958723068237305, Transition Loss 1.370493769645691, Classifier Loss 0.47775954008102417, Total Loss 144.0764923095703\n",
      "1: Encoding Loss 14.858663558959961, Transition Loss 3.9997336864471436, Classifier Loss 0.4605005979537964, Total Loss 136.80194091796875\n",
      "1: Encoding Loss 15.31298542022705, Transition Loss 1.9475181102752686, Classifier Loss 0.4121398627758026, Total Loss 133.87091064453125\n",
      "1: Encoding Loss 15.419626235961914, Transition Loss 0.6891206502914429, Classifier Loss 0.42287030816078186, Total Loss 135.0804443359375\n",
      "1: Encoding Loss 15.507623672485352, Transition Loss 0.4702206254005432, Classifier Loss 0.41099807620048523, Total Loss 134.33364868164062\n",
      "1: Encoding Loss 15.983694076538086, Transition Loss 2.293020248413086, Classifier Loss 0.416929692029953, Total Loss 138.51234436035156\n",
      "1: Encoding Loss 14.892109870910645, Transition Loss -0.3416898846626282, Classifier Loss 0.3555581569671631, Total Loss 124.90834045410156\n",
      "1: Encoding Loss 14.138368606567383, Transition Loss 1.1062884330749512, Classifier Loss 0.3979147970676422, Total Loss 125.064208984375\n",
      "1: Encoding Loss 15.01319408416748, Transition Loss 0.789856493473053, Classifier Loss 0.4009248912334442, Total Loss 130.48760986328125\n",
      "1: Encoding Loss 13.687947273254395, Transition Loss 1.6940176486968994, Classifier Loss 0.3863668143749237, Total Loss 121.44197082519531\n",
      "1: Encoding Loss 15.580537796020508, Transition Loss 2.6241137981414795, Classifier Loss 0.4157031774520874, Total Loss 136.1031951904297\n",
      "1: Encoding Loss 15.141677856445312, Transition Loss 2.3946151733398438, Classifier Loss 0.4185856878757477, Total Loss 133.66647338867188\n",
      "1: Encoding Loss 14.468324661254883, Transition Loss 2.245865821838379, Classifier Loss 0.4044492244720459, Total Loss 128.15322875976562\n",
      "1: Encoding Loss 17.631784439086914, Transition Loss 2.1585803031921387, Classifier Loss 0.4523262679576874, Total Loss 151.88677978515625\n",
      "1: Encoding Loss 15.99970817565918, Transition Loss 1.021457314491272, Classifier Loss 0.3966413140296936, Total Loss 136.0709686279297\n",
      "1: Encoding Loss 15.57020092010498, Transition Loss 0.6394329071044922, Classifier Loss 0.403411865234375, Total Loss 134.01815795898438\n",
      "1: Encoding Loss 15.07056713104248, Transition Loss 2.4876694679260254, Classifier Loss 0.4911486804485321, Total Loss 140.53334045410156\n",
      "1: Encoding Loss 15.236076354980469, Transition Loss 0.9258898496627808, Classifier Loss 0.41270753741264343, Total Loss 133.0575714111328\n",
      "1: Encoding Loss 15.840757369995117, Transition Loss 0.3865061402320862, Classifier Loss 0.3875064253807068, Total Loss 133.94979858398438\n",
      "1: Encoding Loss 15.832244873046875, Transition Loss 2.6087234020233154, Classifier Loss 0.3904649317264557, Total Loss 135.0834503173828\n",
      "1: Encoding Loss 15.711576461791992, Transition Loss 1.7958157062530518, Classifier Loss 0.46757185459136963, Total Loss 141.74496459960938\n",
      "1: Encoding Loss 15.914886474609375, Transition Loss 1.3186891078948975, Classifier Loss 0.3657565712928772, Total Loss 132.5924530029297\n",
      "1: Encoding Loss 15.136138916015625, Transition Loss -0.2637435793876648, Classifier Loss 0.34900903701782227, Total Loss 125.7176284790039\n",
      "1: Encoding Loss 15.656667709350586, Transition Loss 4.359121799468994, Classifier Loss 0.4854910373687744, Total Loss 144.23275756835938\n",
      "1: Encoding Loss 15.879366874694824, Transition Loss 2.855405807495117, Classifier Loss 0.454021155834198, Total Loss 141.8204803466797\n",
      "1: Encoding Loss 14.024665832519531, Transition Loss 0.0011083483695983887, Classifier Loss 0.4138735234737396, Total Loss 125.5357894897461\n",
      "1: Encoding Loss 15.852571487426758, Transition Loss 2.1283113956451416, Classifier Loss 0.40244826674461365, Total Loss 136.21157836914062\n",
      "1: Encoding Loss 15.008450508117676, Transition Loss 0.7842371463775635, Classifier Loss 0.3903723955154419, Total Loss 129.40164184570312\n",
      "1: Encoding Loss 14.378416061401367, Transition Loss 1.2665843963623047, Classifier Loss 0.41142868995666504, Total Loss 127.91999816894531\n",
      "1: Encoding Loss 14.5000581741333, Transition Loss 1.4909493923187256, Classifier Loss 0.43972134590148926, Total Loss 131.56886291503906\n",
      "1: Encoding Loss 14.251623153686523, Transition Loss 2.677377939224243, Classifier Loss 0.4118785560131073, Total Loss 127.7685546875\n",
      "1: Encoding Loss 16.015714645385742, Transition Loss 0.5931283831596375, Classifier Loss 0.4173758625984192, Total Loss 138.0691375732422\n",
      "1: Encoding Loss 15.357694625854492, Transition Loss 3.5624032020568848, Classifier Loss 0.4275367558002472, Total Loss 136.32479858398438\n",
      "1: Encoding Loss 16.28069496154785, Transition Loss 0.06386613845825195, Classifier Loss 0.3995969891548157, Total Loss 137.66941833496094\n",
      "1: Encoding Loss 14.83618450164795, Transition Loss 1.257364273071289, Classifier Loss 0.3780646324157715, Total Loss 127.32652282714844\n",
      "1: Encoding Loss 13.234018325805664, Transition Loss 1.7637825012207031, Classifier Loss 0.41864317655563354, Total Loss 121.97394561767578\n",
      "1: Encoding Loss 15.239110946655273, Transition Loss 0.7841516733169556, Classifier Loss 0.3947813808917999, Total Loss 131.22647094726562\n",
      "1: Encoding Loss 15.972002029418945, Transition Loss 6.447483062744141, Classifier Loss 0.47683048248291016, Total Loss 146.09405517578125\n",
      "1: Encoding Loss 14.609130859375, Transition Loss 2.357825756072998, Classifier Loss 0.39691194891929626, Total Loss 128.2891082763672\n",
      "1: Encoding Loss 15.018986701965332, Transition Loss 1.9699604511260986, Classifier Loss 0.44037091732025146, Total Loss 134.93899536132812\n",
      "1: Encoding Loss 14.210439682006836, Transition Loss 0.40518873929977417, Classifier Loss 0.4054449498653412, Total Loss 125.9692153930664\n",
      "1: Encoding Loss 15.376042366027832, Transition Loss 2.750048875808716, Classifier Loss 0.4614173173904419, Total Loss 139.4980010986328\n",
      "1: Encoding Loss 14.998478889465332, Transition Loss 2.4950592517852783, Classifier Loss 0.4112616181373596, Total Loss 132.11505126953125\n",
      "1: Encoding Loss 15.828222274780273, Transition Loss 4.909848690032959, Classifier Loss 0.39901116490364075, Total Loss 136.8343963623047\n",
      "1: Encoding Loss 13.944007873535156, Transition Loss 2.384977102279663, Classifier Loss 0.3701825737953186, Total Loss 121.63629150390625\n",
      "1: Encoding Loss 13.479846000671387, Transition Loss -0.035875290632247925, Classifier Loss 0.38967928290367126, Total Loss 119.84700012207031\n",
      "1: Encoding Loss 14.229823112487793, Transition Loss 1.3843034505844116, Classifier Loss 0.3840640187263489, Total Loss 124.33906555175781\n",
      "1: Encoding Loss 15.653643608093262, Transition Loss 1.317480444908142, Classifier Loss 0.40219324827194214, Total Loss 134.66818237304688\n",
      "1: Encoding Loss 15.315829277038574, Transition Loss 0.22469326853752136, Classifier Loss 0.38947969675064087, Total Loss 130.93283081054688\n",
      "1: Encoding Loss 14.435075759887695, Transition Loss 2.2639808654785156, Classifier Loss 0.3990221321582794, Total Loss 127.41826629638672\n",
      "1: Encoding Loss 14.453579902648926, Transition Loss 2.707697629928589, Classifier Loss 0.4277912676334381, Total Loss 130.5836944580078\n",
      "1: Encoding Loss 13.880815505981445, Transition Loss 3.810081720352173, Classifier Loss 0.47115904092788696, Total Loss 131.92483520507812\n",
      "1: Encoding Loss 14.158088684082031, Transition Loss 1.987802505493164, Classifier Loss 0.3158405125141144, Total Loss 117.32770538330078\n",
      "1: Encoding Loss 14.736700057983398, Transition Loss 2.056940793991089, Classifier Loss 0.39682498574256897, Total Loss 128.92547607421875\n",
      "1: Encoding Loss 16.432125091552734, Transition Loss 2.3368663787841797, Classifier Loss 0.3969670534133911, Total Loss 139.22421264648438\n",
      "1: Encoding Loss 15.364900588989258, Transition Loss 0.7594519853591919, Classifier Loss 0.4130033552646637, Total Loss 133.7935333251953\n",
      "1: Encoding Loss 15.810922622680664, Transition Loss 1.696793794631958, Classifier Loss 0.37625852227211, Total Loss 133.17010498046875\n",
      "1: Encoding Loss 15.317413330078125, Transition Loss 1.4821101427078247, Classifier Loss 0.4181685447692871, Total Loss 134.31419372558594\n",
      "1: Encoding Loss 13.552075386047363, Transition Loss 0.4796835780143738, Classifier Loss 0.3792654871940613, Total Loss 119.43087005615234\n",
      "1: Encoding Loss 14.917022705078125, Transition Loss 1.2526254653930664, Classifier Loss 0.39044493436813354, Total Loss 129.0476837158203\n",
      "1: Encoding Loss 14.1925630569458, Transition Loss 2.7822093963623047, Classifier Loss 0.43058469891548157, Total Loss 129.3267364501953\n",
      "1: Encoding Loss 15.47624397277832, Transition Loss 1.798262357711792, Classifier Loss 0.4287310540676117, Total Loss 136.4498748779297\n",
      "1: Encoding Loss 14.912813186645508, Transition Loss 1.1247093677520752, Classifier Loss 0.37557050585746765, Total Loss 127.48381805419922\n",
      "1: Encoding Loss 14.826825141906738, Transition Loss 1.8942934274673462, Classifier Loss 0.3959544003009796, Total Loss 129.31411743164062\n",
      "1: Encoding Loss 14.37037181854248, Transition Loss 1.972792387008667, Classifier Loss 0.36638081073760986, Total Loss 123.6494369506836\n",
      "1: Encoding Loss 15.858555793762207, Transition Loss 2.67691707611084, Classifier Loss 0.5156733393669128, Total Loss 147.78944396972656\n",
      "1: Encoding Loss 14.56193733215332, Transition Loss 2.921391248703003, Classifier Loss 0.3882232904434204, Total Loss 127.36251068115234\n",
      "1: Encoding Loss 15.176363945007324, Transition Loss 1.129948616027832, Classifier Loss 0.3704901933670044, Total Loss 128.55918884277344\n",
      "1: Encoding Loss 14.960766792297363, Transition Loss 1.802638292312622, Classifier Loss 0.3821128010749817, Total Loss 128.6969451904297\n",
      "1: Encoding Loss 15.480669975280762, Transition Loss 3.3071229457855225, Classifier Loss 0.4467617869377136, Total Loss 138.883056640625\n",
      "1: Encoding Loss 15.465917587280273, Transition Loss -0.0810190886259079, Classifier Loss 0.42736706137657166, Total Loss 135.5321807861328\n",
      "1: Encoding Loss 14.534497261047363, Transition Loss 2.050786018371582, Classifier Loss 0.37539127469062805, Total Loss 125.56642150878906\n",
      "1: Encoding Loss 14.780339241027832, Transition Loss 0.7585616707801819, Classifier Loss 0.40307915210723877, Total Loss 129.2933807373047\n",
      "1: Encoding Loss 14.869341850280762, Transition Loss 2.845672130584717, Classifier Loss 0.3879275321960449, Total Loss 129.1470947265625\n",
      "1: Encoding Loss 13.000337600708008, Transition Loss 1.6481281518936157, Classifier Loss 0.34123495221138, Total Loss 112.78477478027344\n",
      "1: Encoding Loss 14.175697326660156, Transition Loss 2.037184953689575, Classifier Loss 0.4024340808391571, Total Loss 126.11246490478516\n",
      "1: Encoding Loss 15.959692001342773, Transition Loss 2.7376575469970703, Classifier Loss 0.38311368227005005, Total Loss 135.16458129882812\n",
      "1: Encoding Loss 15.625974655151367, Transition Loss 2.3859591484069824, Classifier Loss 0.4591400623321533, Total Loss 140.62423706054688\n",
      "1: Encoding Loss 15.515024185180664, Transition Loss 1.1445592641830444, Classifier Loss 0.388083279132843, Total Loss 132.35629272460938\n",
      "1: Encoding Loss 15.4149751663208, Transition Loss 3.1773178577423096, Classifier Loss 0.4271168112754822, Total Loss 136.4724578857422\n",
      "1: Encoding Loss 13.676051139831543, Transition Loss 1.3172252178192139, Classifier Loss 0.336880624294281, Total Loss 116.2712631225586\n",
      "1: Encoding Loss 13.62901496887207, Transition Loss 2.538466691970825, Classifier Loss 0.42253395915031433, Total Loss 125.04287719726562\n",
      "1: Encoding Loss 15.697784423828125, Transition Loss 0.3591771423816681, Classifier Loss 0.33466026186943054, Total Loss 127.79640197753906\n",
      "1: Encoding Loss 15.037858963012695, Transition Loss 2.362623691558838, Classifier Loss 0.400246262550354, Total Loss 131.19683837890625\n",
      "1: Encoding Loss 15.317054748535156, Transition Loss 3.1205854415893555, Classifier Loss 0.417183518409729, Total Loss 134.86891174316406\n",
      "1: Encoding Loss 13.495768547058105, Transition Loss 1.1381580829620361, Classifier Loss 0.3756605386734009, Total Loss 118.99593353271484\n",
      "1: Encoding Loss 14.591954231262207, Transition Loss 2.910423994064331, Classifier Loss 0.36351853609085083, Total Loss 125.0677490234375\n",
      "1: Encoding Loss 15.625228881835938, Transition Loss 4.030784606933594, Classifier Loss 0.4277495741844177, Total Loss 138.13865661621094\n",
      "1: Encoding Loss 14.861332893371582, Transition Loss 2.9734060764312744, Classifier Loss 0.39120328426361084, Total Loss 129.47769165039062\n",
      "1: Encoding Loss 15.023295402526855, Transition Loss 0.3255932331085205, Classifier Loss 0.40468794107437134, Total Loss 130.7388153076172\n",
      "1: Encoding Loss 14.666433334350586, Transition Loss 1.2515629529953003, Classifier Loss 0.3563334345817566, Total Loss 124.132568359375\n",
      "1: Encoding Loss 15.19514274597168, Transition Loss 1.101980447769165, Classifier Loss 0.39547187089920044, Total Loss 131.15884399414062\n",
      "1: Encoding Loss 15.30517864227295, Transition Loss 2.217602014541626, Classifier Loss 0.39467653632164, Total Loss 132.18577575683594\n",
      "1: Encoding Loss 15.15351390838623, Transition Loss 1.3656045198440552, Classifier Loss 0.3652638792991638, Total Loss 127.99372100830078\n",
      "1: Encoding Loss 14.58708667755127, Transition Loss 2.945430040359497, Classifier Loss 0.3784172534942627, Total Loss 126.54241180419922\n",
      "1: Encoding Loss 14.538164138793945, Transition Loss 1.378725290298462, Classifier Loss 0.3760985732078552, Total Loss 125.39033508300781\n",
      "1: Encoding Loss 15.267732620239258, Transition Loss 3.0523433685302734, Classifier Loss 0.40226301550865173, Total Loss 133.0536346435547\n",
      "1: Encoding Loss 13.613592147827148, Transition Loss 1.8102939128875732, Classifier Loss 0.39762645959854126, Total Loss 122.16831970214844\n",
      "1: Encoding Loss 15.891757011413574, Transition Loss 1.2848801612854004, Classifier Loss 0.43909111618995667, Total Loss 139.7736053466797\n",
      "1: Encoding Loss 15.049333572387695, Transition Loss 3.254621744155884, Classifier Loss 0.4210209250450134, Total Loss 133.699951171875\n",
      "1: Encoding Loss 14.323654174804688, Transition Loss 0.8773082494735718, Classifier Loss 0.39810341596603394, Total Loss 126.10318756103516\n",
      "1: Encoding Loss 15.130064010620117, Transition Loss 1.7875096797943115, Classifier Loss 0.364790141582489, Total Loss 127.97440338134766\n",
      "1: Encoding Loss 14.901250839233398, Transition Loss 1.5199153423309326, Classifier Loss 0.3714986741542816, Total Loss 127.16533660888672\n",
      "1: Encoding Loss 14.979001998901367, Transition Loss 0.08749514818191528, Classifier Loss 0.3860262334346771, Total Loss 128.5116424560547\n",
      "1: Encoding Loss 14.449949264526367, Transition Loss 0.724557101726532, Classifier Loss 0.3867989778518677, Total Loss 125.66941833496094\n",
      "1: Encoding Loss 16.568241119384766, Transition Loss 2.026702880859375, Classifier Loss 0.40927448868751526, Total Loss 141.1475830078125\n",
      "1: Encoding Loss 14.877105712890625, Transition Loss 0.3852454125881195, Classifier Loss 0.3966076374053955, Total Loss 129.07749938964844\n",
      "1: Encoding Loss 15.676007270812988, Transition Loss 3.3634238243103027, Classifier Loss 0.48376378417015076, Total Loss 143.7777862548828\n",
      "1: Encoding Loss 15.564619064331055, Transition Loss 1.6972954273223877, Classifier Loss 0.3862805664539337, Total Loss 132.6947021484375\n",
      "1: Encoding Loss 14.986865043640137, Transition Loss 1.030033826828003, Classifier Loss 0.4337628483772278, Total Loss 133.70950317382812\n",
      "1: Encoding Loss 14.920161247253418, Transition Loss 3.2251782417297363, Classifier Loss 0.4233030676841736, Total Loss 133.141357421875\n",
      "1: Encoding Loss 17.024883270263672, Transition Loss 4.299144744873047, Classifier Loss 0.46358954906463623, Total Loss 150.22793579101562\n",
      "1: Encoding Loss 14.450531005859375, Transition Loss 2.3300609588623047, Classifier Loss 0.3964870870113373, Total Loss 127.28392028808594\n",
      "1: Encoding Loss 15.507206916809082, Transition Loss 0.06144355237483978, Classifier Loss 0.3531684875488281, Total Loss 128.38467407226562\n",
      "1: Encoding Loss 15.897741317749023, Transition Loss 1.8902194499969482, Classifier Loss 0.4098178446292877, Total Loss 137.12432861328125\n",
      "1: Encoding Loss 14.648119926452637, Transition Loss 1.4202120304107666, Classifier Loss 0.46381455659866333, Total Loss 134.8382568359375\n",
      "1: Encoding Loss 14.786602973937988, Transition Loss -0.08013403415679932, Classifier Loss 0.32293763756752014, Total Loss 121.01335144042969\n",
      "1: Encoding Loss 14.632061958312988, Transition Loss 1.4180666208267212, Classifier Loss 0.45722684264183044, Total Loss 134.08229064941406\n",
      "1: Encoding Loss 14.072359085083008, Transition Loss 1.6261329650878906, Classifier Loss 0.40422746539115906, Total Loss 125.50735473632812\n",
      "1: Encoding Loss 14.261360168457031, Transition Loss 1.9896082878112793, Classifier Loss 0.38855087757110596, Total Loss 125.2190933227539\n",
      "1: Encoding Loss 15.335512161254883, Transition Loss 4.826128005981445, Classifier Loss 0.44515496492385864, Total Loss 138.45901489257812\n",
      "1: Encoding Loss 14.507781982421875, Transition Loss 1.8364089727401733, Classifier Loss 0.4319346845149994, Total Loss 130.97471618652344\n",
      "1: Encoding Loss 15.019250869750977, Transition Loss 1.4518649578094482, Classifier Loss 0.42001667618751526, Total Loss 132.6979217529297\n",
      "1: Encoding Loss 16.28621482849121, Transition Loss 1.9576992988586426, Classifier Loss 0.41666072607040405, Total Loss 140.16644287109375\n",
      "1: Encoding Loss 13.4219970703125, Transition Loss 0.3124818801879883, Classifier Loss 0.40754884481430054, Total Loss 121.41185760498047\n",
      "1: Encoding Loss 15.503484725952148, Transition Loss 2.158884048461914, Classifier Loss 0.4588671326637268, Total Loss 139.77117919921875\n",
      "1: Encoding Loss 14.922996520996094, Transition Loss 0.9554647207260132, Classifier Loss 0.3656023442745209, Total Loss 126.48040008544922\n",
      "1: Encoding Loss 13.5988187789917, Transition Loss 0.004092991352081299, Classifier Loss 0.39404770731925964, Total Loss 120.99932861328125\n",
      "1: Encoding Loss 15.888277053833008, Transition Loss 3.7030463218688965, Classifier Loss 0.4216569662094116, Total Loss 138.97657775878906\n",
      "1: Encoding Loss 14.798316955566406, Transition Loss 2.6629695892333984, Classifier Loss 0.41255804896354675, Total Loss 131.11090087890625\n",
      "1: Encoding Loss 12.749666213989258, Transition Loss 2.0481207370758057, Classifier Loss 0.4179134964942932, Total Loss 119.10860443115234\n",
      "1: Encoding Loss 16.938583374023438, Transition Loss 1.672025442123413, Classifier Loss 0.4270654618740082, Total Loss 145.00686645507812\n",
      "1: Encoding Loss 16.077377319335938, Transition Loss 2.728469133377075, Classifier Loss 0.4323446750640869, Total Loss 140.7901153564453\n",
      "1: Encoding Loss 16.36033821105957, Transition Loss 5.066555023193359, Classifier Loss 0.39301955699920654, Total Loss 139.49061584472656\n",
      "1: Encoding Loss 15.500536918640137, Transition Loss 2.668394088745117, Classifier Loss 0.40862464904785156, Total Loss 134.93304443359375\n",
      "1: Encoding Loss 15.24230670928955, Transition Loss 2.7651495933532715, Classifier Loss 0.42814165353775024, Total Loss 135.3740692138672\n",
      "1: Encoding Loss 14.916067123413086, Transition Loss 0.965836763381958, Classifier Loss 0.41043198108673096, Total Loss 130.9259490966797\n",
      "1: Encoding Loss 15.485018730163574, Transition Loss 3.1862854957580566, Classifier Loss 0.4370637834072113, Total Loss 137.89102172851562\n",
      "1: Encoding Loss 13.785687446594238, Transition Loss 0.46288299560546875, Classifier Loss 0.37452107667922974, Total Loss 120.35137939453125\n",
      "1: Encoding Loss 13.939260482788086, Transition Loss 2.791264533996582, Classifier Loss 0.422818124294281, Total Loss 127.03388214111328\n",
      "1: Encoding Loss 14.617570877075195, Transition Loss 3.6244843006134033, Classifier Loss 0.3938039541244507, Total Loss 128.53561401367188\n",
      "1: Encoding Loss 14.97461223602295, Transition Loss 3.7850730419158936, Classifier Loss 0.410520076751709, Total Loss 132.41371154785156\n",
      "1: Encoding Loss 14.373087882995605, Transition Loss 3.4180052280426025, Classifier Loss 0.4613434374332428, Total Loss 133.74008178710938\n",
      "1: Encoding Loss 13.299896240234375, Transition Loss 0.044001758098602295, Classifier Loss 0.35711610317230225, Total Loss 115.5285873413086\n",
      "1: Encoding Loss 15.426831245422363, Transition Loss 5.7410736083984375, Classifier Loss 0.3369709849357605, Total Loss 128.5545196533203\n",
      "1: Encoding Loss 14.631414413452148, Transition Loss 2.4662978649139404, Classifier Loss 0.38826990127563477, Total Loss 127.60199737548828\n",
      "1: Encoding Loss 14.05499267578125, Transition Loss 2.199726104736328, Classifier Loss 0.3425389528274536, Total Loss 119.46373748779297\n",
      "1: Encoding Loss 13.309285163879395, Transition Loss 2.780670404434204, Classifier Loss 0.34420934319496155, Total Loss 115.38890838623047\n",
      "1: Encoding Loss 14.125188827514648, Transition Loss 3.155080795288086, Classifier Loss 0.39773446321487427, Total Loss 125.78661346435547\n",
      "1: Encoding Loss 13.700658798217773, Transition Loss -0.06398718059062958, Classifier Loss 0.38958871364593506, Total Loss 121.16280364990234\n",
      "1: Encoding Loss 13.936515808105469, Transition Loss 2.1558053493499756, Classifier Loss 0.4064410328865051, Total Loss 125.12551879882812\n",
      "1: Encoding Loss 15.281872749328613, Transition Loss 1.848508596420288, Classifier Loss 0.4145089387893677, Total Loss 133.8815460205078\n",
      "1: Encoding Loss 14.748390197753906, Transition Loss 2.647217273712158, Classifier Loss 0.4199029207229614, Total Loss 131.53952026367188\n",
      "1: Encoding Loss 16.17715835571289, Transition Loss 0.9369689226150513, Classifier Loss 0.38950151205062866, Total Loss 136.38787841796875\n",
      "1: Encoding Loss 14.992904663085938, Transition Loss 1.2955944538116455, Classifier Loss 0.40126171708106995, Total Loss 130.60183715820312\n",
      "1: Encoding Loss 13.697932243347168, Transition Loss 1.8462883234024048, Classifier Loss 0.45674067735671997, Total Loss 128.6001739501953\n",
      "1: Encoding Loss 13.436177253723145, Transition Loss 0.4669806957244873, Classifier Loss 0.38812676072120667, Total Loss 119.61653137207031\n",
      "1: Encoding Loss 14.978012084960938, Transition Loss 0.7084624171257019, Classifier Loss 0.4199560880661011, Total Loss 132.14706420898438\n",
      "1: Encoding Loss 14.787388801574707, Transition Loss 2.4172756671905518, Classifier Loss 0.3974708318710327, Total Loss 129.43832397460938\n",
      "1: Encoding Loss 14.92057991027832, Transition Loss 0.8470730781555176, Classifier Loss 0.33871588110923767, Total Loss 123.73390197753906\n",
      "1: Encoding Loss 13.837138175964355, Transition Loss 2.509667158126831, Classifier Loss 0.3768274188041687, Total Loss 121.7094497680664\n",
      "1: Encoding Loss 15.286526679992676, Transition Loss 0.5643326640129089, Classifier Loss 0.3778466582298279, Total Loss 129.7295684814453\n",
      "1: Encoding Loss 14.247587203979492, Transition Loss 2.634232759475708, Classifier Loss 0.40430817008018494, Total Loss 126.97003936767578\n",
      "1: Encoding Loss 14.4714937210083, Transition Loss 1.8685648441314697, Classifier Loss 0.35719314217567444, Total Loss 123.29570770263672\n",
      "1: Encoding Loss 15.614087104797363, Transition Loss 3.334336519241333, Classifier Loss 0.40900757908821106, Total Loss 135.9190216064453\n",
      "1: Encoding Loss 14.822054862976074, Transition Loss 0.5311480760574341, Classifier Loss 0.4005935788154602, Total Loss 129.20416259765625\n",
      "1: Encoding Loss 14.898345947265625, Transition Loss 2.174914598464966, Classifier Loss 0.3993434011936188, Total Loss 130.1943817138672\n",
      "1: Encoding Loss 14.606480598449707, Transition Loss 3.105496883392334, Classifier Loss 0.406144917011261, Total Loss 129.49557495117188\n",
      "1: Encoding Loss 14.152626037597656, Transition Loss 2.8274426460266113, Classifier Loss 0.3875283896923065, Total Loss 124.79956817626953\n",
      "1: Encoding Loss 14.382452011108398, Transition Loss 2.3026223182678223, Classifier Loss 0.42139026522636414, Total Loss 129.35479736328125\n",
      "1: Encoding Loss 14.255151748657227, Transition Loss 0.6041103601455688, Classifier Loss 0.3453672230243683, Total Loss 120.30928802490234\n",
      "1: Encoding Loss 13.625747680664062, Transition Loss -0.09296852350234985, Classifier Loss 0.3820897936820984, Total Loss 119.96343231201172\n",
      "1: Encoding Loss 18.669410705566406, Transition Loss 3.5563879013061523, Classifier Loss 0.3922927677631378, Total Loss 152.66831970214844\n",
      "1: Encoding Loss 16.140697479248047, Transition Loss 1.3119356632232666, Classifier Loss 0.40701910853385925, Total Loss 138.07089233398438\n",
      "1: Encoding Loss 15.506532669067383, Transition Loss 2.771284341812134, Classifier Loss 0.5010965466499329, Total Loss 144.2573699951172\n",
      "1: Encoding Loss 15.515953063964844, Transition Loss 1.8366193771362305, Classifier Loss 0.40390750765800476, Total Loss 134.2211151123047\n",
      "1: Encoding Loss 15.844747543334961, Transition Loss 1.5640279054641724, Classifier Loss 0.39426714181900024, Total Loss 135.12081909179688\n",
      "1: Encoding Loss 14.639505386352539, Transition Loss 0.8285732865333557, Classifier Loss 0.35231348872184753, Total Loss 123.39981842041016\n",
      "1: Encoding Loss 13.474268913269043, Transition Loss 1.0337923765182495, Classifier Loss 0.44975584745407104, Total Loss 126.2347183227539\n",
      "1: Encoding Loss 13.65180778503418, Transition Loss 3.502948760986328, Classifier Loss 0.4083542823791504, Total Loss 124.14745330810547\n",
      "1: Encoding Loss 16.394596099853516, Transition Loss 2.368678569793701, Classifier Loss 0.4089813232421875, Total Loss 140.2131805419922\n",
      "1: Encoding Loss 15.39570426940918, Transition Loss 5.164340019226074, Classifier Loss 0.4434272348880768, Total Loss 138.78268432617188\n",
      "1: Encoding Loss 14.2170991897583, Transition Loss 1.9765933752059937, Classifier Loss 0.3773585855960846, Total Loss 123.82908630371094\n",
      "1: Encoding Loss 16.243301391601562, Transition Loss 2.4071285724639893, Classifier Loss 0.3847358524799347, Total Loss 136.89625549316406\n",
      "1: Encoding Loss 15.459501266479492, Transition Loss 1.0739420652389526, Classifier Loss 0.4145112633705139, Total Loss 134.63771057128906\n",
      "1: Encoding Loss 15.180826187133789, Transition Loss 0.05529710650444031, Classifier Loss 0.362845778465271, Total Loss 127.39165496826172\n",
      "1: Encoding Loss 14.474658012390137, Transition Loss 0.6492708325386047, Classifier Loss 0.34930992126464844, Total Loss 122.03865051269531\n",
      "1: Encoding Loss 14.096435546875, Transition Loss 1.3387584686279297, Classifier Loss 0.38575756549835205, Total Loss 123.68988037109375\n",
      "1: Encoding Loss 14.882641792297363, Transition Loss 2.2137978076934814, Classifier Loss 0.4089601933956146, Total Loss 131.07737731933594\n",
      "1: Encoding Loss 14.269556045532227, Transition Loss 4.045583248138428, Classifier Loss 0.48180174827575684, Total Loss 135.41575622558594\n",
      "1: Encoding Loss 14.49535846710205, Transition Loss 1.7370755672454834, Classifier Loss 0.3547860085964203, Total Loss 123.14558410644531\n",
      "1: Encoding Loss 15.735021591186523, Transition Loss 2.65107798576355, Classifier Loss 0.477758526802063, Total Loss 143.2464141845703\n",
      "1: Encoding Loss 14.805195808410645, Transition Loss 3.2728559970855713, Classifier Loss 0.4138607084751129, Total Loss 131.52639770507812\n",
      "1: Encoding Loss 14.623689651489258, Transition Loss 2.2641000747680664, Classifier Loss 0.42369794845581055, Total Loss 131.017578125\n",
      "1: Encoding Loss 14.836810111999512, Transition Loss 2.849395275115967, Classifier Loss 0.4393536150455475, Total Loss 134.09597778320312\n",
      "1: Encoding Loss 15.598213195800781, Transition Loss 2.1453473567962646, Classifier Loss 0.4177491366863251, Total Loss 136.2223358154297\n",
      "1: Encoding Loss 14.28480052947998, Transition Loss 0.21182340383529663, Classifier Loss 0.366560161113739, Total Loss 122.44955444335938\n",
      "1: Encoding Loss 13.095776557922363, Transition Loss 1.420992136001587, Classifier Loss 0.37692710757255554, Total Loss 116.83576965332031\n",
      "1: Encoding Loss 15.94379711151123, Transition Loss 1.0636866092681885, Classifier Loss 0.40338483452796936, Total Loss 136.42674255371094\n",
      "1: Encoding Loss 14.490565299987793, Transition Loss -0.035747677087783813, Classifier Loss 0.3580785095691681, Total Loss 122.75123596191406\n",
      "1: Encoding Loss 15.16919994354248, Transition Loss -0.31242573261260986, Classifier Loss 0.3524337410926819, Total Loss 126.25845336914062\n",
      "1: Encoding Loss 15.543928146362305, Transition Loss 1.7987350225448608, Classifier Loss 0.3675309717655182, Total Loss 130.7361602783203\n",
      "1: Encoding Loss 13.857522964477539, Transition Loss -0.5896432399749756, Classifier Loss 0.36403024196624756, Total Loss 119.54792785644531\n",
      "1: Encoding Loss 13.156031608581543, Transition Loss 0.6148425340652466, Classifier Loss 0.3631308078765869, Total Loss 115.49520874023438\n",
      "1: Encoding Loss 15.257795333862305, Transition Loss 1.7550688982009888, Classifier Loss 0.3394874632358551, Total Loss 126.19754791259766\n",
      "1: Encoding Loss 14.433801651000977, Transition Loss 1.9580318927764893, Classifier Loss 0.41961097717285156, Total Loss 129.3471221923828\n",
      "1: Encoding Loss 15.260213851928711, Transition Loss 4.126176834106445, Classifier Loss 0.39900773763656616, Total Loss 133.11253356933594\n",
      "1: Encoding Loss 14.940559387207031, Transition Loss 2.5201759338378906, Classifier Loss 0.3873986601829529, Total Loss 129.39129638671875\n",
      "1: Encoding Loss 13.56346321105957, Transition Loss 2.6054298877716064, Classifier Loss 0.35898202657699585, Total Loss 118.32115936279297\n",
      "1: Encoding Loss 15.170344352722168, Transition Loss 1.688676357269287, Classifier Loss 0.39783480763435364, Total Loss 131.4810333251953\n",
      "1: Encoding Loss 14.358304023742676, Transition Loss 3.3097941875457764, Classifier Loss 0.3857201635837555, Total Loss 126.04576873779297\n",
      "1: Encoding Loss 13.649812698364258, Transition Loss 1.9868379831314087, Classifier Loss 0.42351585626602173, Total Loss 125.04520416259766\n",
      "1: Encoding Loss 14.515849113464355, Transition Loss 3.38472843170166, Classifier Loss 0.37751349806785583, Total Loss 126.2003402709961\n",
      "1: Encoding Loss 13.440808296203613, Transition Loss 1.3764203786849976, Classifier Loss 0.40192654728889465, Total Loss 121.38807678222656\n",
      "1: Encoding Loss 14.001571655273438, Transition Loss 1.590651273727417, Classifier Loss 0.42873576283454895, Total Loss 127.51927185058594\n",
      "1: Encoding Loss 14.143977165222168, Transition Loss 1.139526128768921, Classifier Loss 0.41309022903442383, Total Loss 126.62870025634766\n",
      "1: Encoding Loss 14.63563346862793, Transition Loss 1.645542860031128, Classifier Loss 0.3608858287334442, Total Loss 124.56060791015625\n",
      "1: Encoding Loss 13.696627616882324, Transition Loss 2.2970430850982666, Classifier Loss 0.37438833713531494, Total Loss 120.53742218017578\n",
      "1: Encoding Loss 14.752828598022461, Transition Loss 3.0383613109588623, Classifier Loss 0.3998934328556061, Total Loss 129.72166442871094\n",
      "1: Encoding Loss 14.619100570678711, Transition Loss 1.7173868417739868, Classifier Loss 0.4105386435985565, Total Loss 129.45541381835938\n",
      "1: Encoding Loss 13.8915433883667, Transition Loss 2.6846201419830322, Classifier Loss 0.39659804105758667, Total Loss 124.08291625976562\n",
      "1: Encoding Loss 15.528635025024414, Transition Loss 1.9140281677246094, Classifier Loss 0.37156280875205994, Total Loss 131.0937042236328\n",
      "1: Encoding Loss 14.325335502624512, Transition Loss 1.8076386451721191, Classifier Loss 0.3719126284122467, Total Loss 123.8663330078125\n",
      "1: Encoding Loss 13.94340991973877, Transition Loss 4.139273643493652, Classifier Loss 0.43220576643943787, Total Loss 128.53675842285156\n",
      "1: Encoding Loss 15.244630813598633, Transition Loss 1.9196548461914062, Classifier Loss 0.4081650972366333, Total Loss 133.0521697998047\n",
      "1: Encoding Loss 14.531193733215332, Transition Loss 2.3749477863311768, Classifier Loss 0.3970358967781067, Total Loss 127.84073638916016\n",
      "1: Encoding Loss 14.055093765258789, Transition Loss 2.1248602867126465, Classifier Loss 0.3976643681526184, Total Loss 124.94694519042969\n",
      "1: Encoding Loss 13.897369384765625, Transition Loss 0.9940932393074036, Classifier Loss 0.42088356614112854, Total Loss 125.87020874023438\n",
      "1: Encoding Loss 15.71577262878418, Transition Loss 5.416641712188721, Classifier Loss 0.49042367935180664, Total Loss 145.503662109375\n",
      "1: Encoding Loss 15.622026443481445, Transition Loss 2.473090410232544, Classifier Loss 0.36474379897117615, Total Loss 131.19578552246094\n",
      "1: Encoding Loss 13.711160659790039, Transition Loss 0.9391800761222839, Classifier Loss 0.36737173795700073, Total Loss 119.37981414794922\n",
      "1: Encoding Loss 13.9597806930542, Transition Loss 2.1712164878845215, Classifier Loss 0.38207241892814636, Total Loss 122.83441162109375\n",
      "1: Encoding Loss 13.988561630249023, Transition Loss 3.0277647972106934, Classifier Loss 0.41488465666770935, Total Loss 126.63094329833984\n",
      "1: Encoding Loss 15.224452018737793, Transition Loss 3.1635422706604004, Classifier Loss 0.37588727474212646, Total Loss 130.2008514404297\n",
      "1: Encoding Loss 14.180932998657227, Transition Loss 3.4993350505828857, Classifier Loss 0.4222548007965088, Total Loss 128.7108154296875\n",
      "1: Encoding Loss 16.2352237701416, Transition Loss 1.4948399066925049, Classifier Loss 0.3872257471084595, Total Loss 136.7318572998047\n",
      "1: Encoding Loss 15.361628532409668, Transition Loss 3.2724051475524902, Classifier Loss 0.4014853835105896, Total Loss 133.6272735595703\n",
      "1: Encoding Loss 14.154815673828125, Transition Loss 1.2675471305847168, Classifier Loss 0.39392420649528503, Total Loss 124.82833862304688\n",
      "1: Encoding Loss 13.421597480773926, Transition Loss 0.4775611162185669, Classifier Loss 0.3978673815727234, Total Loss 120.50735473632812\n",
      "1: Encoding Loss 15.599798202514648, Transition Loss 1.8771064281463623, Classifier Loss 0.34457799792289734, Total Loss 128.80743408203125\n",
      "1: Encoding Loss 15.288996696472168, Transition Loss 3.439992904663086, Classifier Loss 0.4558103680610657, Total Loss 138.69100952148438\n",
      "1: Encoding Loss 13.86866283416748, Transition Loss 1.0459052324295044, Classifier Loss 0.3733890652656555, Total Loss 120.96925354003906\n",
      "1: Encoding Loss 14.228349685668945, Transition Loss 1.9608441591262817, Classifier Loss 0.4310131072998047, Total Loss 129.2557373046875\n",
      "1: Encoding Loss 13.780006408691406, Transition Loss 2.590994358062744, Classifier Loss 0.42856934666633606, Total Loss 126.57337188720703\n",
      "1: Encoding Loss 13.033792495727539, Transition Loss 1.081484079360962, Classifier Loss 0.35076814889907837, Total Loss 113.71216583251953\n",
      "1: Encoding Loss 14.081860542297363, Transition Loss 3.8185219764709473, Classifier Loss 0.40490099787712097, Total Loss 126.50867462158203\n",
      "1: Encoding Loss 13.11505126953125, Transition Loss 1.5440139770507812, Classifier Loss 0.4130881428718567, Total Loss 120.61672973632812\n",
      "1: Encoding Loss 14.336874008178711, Transition Loss 2.9903042316436768, Classifier Loss 0.3825976252555847, Total Loss 125.47712707519531\n",
      "1: Encoding Loss 14.308270454406738, Transition Loss 0.9181458950042725, Classifier Loss 0.41042596101760864, Total Loss 127.25947570800781\n",
      "1: Encoding Loss 14.085530281066895, Transition Loss 1.6464747190475464, Classifier Loss 0.4417017698287964, Total Loss 129.34194946289062\n",
      "1: Encoding Loss 13.020657539367676, Transition Loss 1.4323029518127441, Classifier Loss 0.3221687376499176, Total Loss 110.91374206542969\n",
      "1: Encoding Loss 14.128190994262695, Transition Loss 0.12332108616828918, Classifier Loss 0.3839477598667145, Total Loss 123.2132568359375\n",
      "1: Encoding Loss 14.318819046020508, Transition Loss 1.3139894008636475, Classifier Loss 0.4455055296421051, Total Loss 130.9890594482422\n",
      "1: Encoding Loss 14.2698335647583, Transition Loss 2.9637458324432373, Classifier Loss 0.46709340810775757, Total Loss 133.5138397216797\n",
      "1: Encoding Loss 14.142687797546387, Transition Loss 1.6770179271697998, Classifier Loss 0.3619266450405121, Total Loss 121.7196044921875\n",
      "1: Encoding Loss 15.002110481262207, Transition Loss 2.19023060798645, Classifier Loss 0.40212494134902954, Total Loss 131.10125732421875\n",
      "1: Encoding Loss 13.762761116027832, Transition Loss 1.2563035488128662, Classifier Loss 0.3597436249256134, Total Loss 119.05345153808594\n",
      "1: Encoding Loss 14.784721374511719, Transition Loss 1.500744104385376, Classifier Loss 0.36524662375450134, Total Loss 125.83328247070312\n",
      "1: Encoding Loss 14.139483451843262, Transition Loss 1.646716833114624, Classifier Loss 0.38787758350372314, Total Loss 124.2833480834961\n",
      "1: Encoding Loss 13.770751953125, Transition Loss 2.9044671058654785, Classifier Loss 0.3880699872970581, Total Loss 122.59329986572266\n",
      "1: Encoding Loss 14.084256172180176, Transition Loss 0.7802450656890869, Classifier Loss 0.3933696150779724, Total Loss 124.15459442138672\n",
      "1: Encoding Loss 14.49255084991455, Transition Loss 2.391749858856201, Classifier Loss 0.41242480278015137, Total Loss 129.15447998046875\n",
      "1: Encoding Loss 13.424939155578613, Transition Loss 2.710775375366211, Classifier Loss 0.38668859004974365, Total Loss 120.30280303955078\n",
      "1: Encoding Loss 13.042126655578613, Transition Loss 0.8151856660842896, Classifier Loss 0.3322107195854187, Total Loss 111.7999038696289\n",
      "1: Encoding Loss 14.306035995483398, Transition Loss 1.588759183883667, Classifier Loss 0.38635075092315674, Total Loss 125.10680389404297\n",
      "1: Encoding Loss 13.011405944824219, Transition Loss -0.007819674909114838, Classifier Loss 0.4109152555465698, Total Loss 119.15995788574219\n",
      "1: Encoding Loss 13.340655326843262, Transition Loss 0.47466331720352173, Classifier Loss 0.38290679454803467, Total Loss 118.52448272705078\n",
      "1: Encoding Loss 15.830293655395508, Transition Loss 2.660750150680542, Classifier Loss 0.41831061244010925, Total Loss 137.87713623046875\n",
      "1: Encoding Loss 14.84745979309082, Transition Loss 0.7050958871841431, Classifier Loss 0.3575073778629303, Total Loss 125.1175308227539\n",
      "1: Encoding Loss 14.968942642211914, Transition Loss 2.6689321994781494, Classifier Loss 0.38945862650871277, Total Loss 129.82708740234375\n",
      "1: Encoding Loss 14.288103103637695, Transition Loss 1.9624909162521362, Classifier Loss 0.40962353348731995, Total Loss 127.4759750366211\n",
      "1: Encoding Loss 12.791315078735352, Transition Loss 0.9069774150848389, Classifier Loss 0.3772408664226532, Total Loss 114.83477783203125\n",
      "1: Encoding Loss 15.2410306930542, Transition Loss 4.91131591796875, Classifier Loss 0.41148412227630615, Total Loss 134.5591278076172\n",
      "1: Encoding Loss 14.162145614624023, Transition Loss 1.5243709087371826, Classifier Loss 0.37225645780563354, Total Loss 122.80827331542969\n",
      "1: Encoding Loss 13.753193855285645, Transition Loss 1.6691603660583496, Classifier Loss 0.32421186566352844, Total Loss 115.60801696777344\n",
      "1: Encoding Loss 13.141645431518555, Transition Loss 0.8677394390106201, Classifier Loss 0.34519776701927185, Total Loss 113.71675109863281\n",
      "1: Encoding Loss 14.045083999633789, Transition Loss 1.8476389646530151, Classifier Loss 0.4130527079105377, Total Loss 126.31482696533203\n",
      "1: Encoding Loss 13.614230155944824, Transition Loss 0.448604941368103, Classifier Loss 0.33485186100006104, Total Loss 115.35002136230469\n",
      "1: Encoding Loss 13.466897964477539, Transition Loss 1.5254591703414917, Classifier Loss 0.4050348401069641, Total Loss 121.91505432128906\n",
      "1: Encoding Loss 11.769368171691895, Transition Loss 1.2256864309310913, Classifier Loss 0.3052186071872711, Total Loss 101.62834167480469\n",
      "1: Encoding Loss 16.380191802978516, Transition Loss 3.726832628250122, Classifier Loss 0.4007384777069092, Total Loss 139.84573364257812\n",
      "1: Encoding Loss 14.445121765136719, Transition Loss 2.9682135581970215, Classifier Loss 0.412069708108902, Total Loss 129.0649871826172\n",
      "1: Encoding Loss 14.936808586120605, Transition Loss 1.5159475803375244, Classifier Loss 0.41846126317977905, Total Loss 132.0733642578125\n",
      "1: Encoding Loss 14.113357543945312, Transition Loss 0.4045390784740448, Classifier Loss 0.3707627058029175, Total Loss 121.91822814941406\n",
      "1: Encoding Loss 13.580902099609375, Transition Loss 1.0533263683319092, Classifier Loss 0.4040570855140686, Total Loss 122.31245422363281\n",
      "1: Encoding Loss 14.566515922546387, Transition Loss 2.570347547531128, Classifier Loss 0.4178919494152069, Total Loss 130.2164306640625\n",
      "1: Encoding Loss 14.239213943481445, Transition Loss 4.052210807800293, Classifier Loss 0.3758662939071655, Total Loss 124.6427993774414\n",
      "1: Encoding Loss 13.383111953735352, Transition Loss 2.7755353450775146, Classifier Loss 0.36486515402793884, Total Loss 117.89540100097656\n",
      "1: Encoding Loss 13.949804306030273, Transition Loss -0.48147833347320557, Classifier Loss 0.35062047839164734, Total Loss 118.76068878173828\n",
      "1: Encoding Loss 12.344155311584473, Transition Loss -0.004561901092529297, Classifier Loss 0.3617808222770691, Total Loss 110.24301147460938\n",
      "1: Encoding Loss 12.515035629272461, Transition Loss 2.15373158454895, Classifier Loss 0.38171812891960144, Total Loss 114.12352752685547\n",
      "1: Encoding Loss 13.62657356262207, Transition Loss 1.7120230197906494, Classifier Loss 0.37420281767845154, Total Loss 119.86453247070312\n",
      "1: Encoding Loss 15.18150520324707, Transition Loss 1.6760555505752563, Classifier Loss 0.38850998878479004, Total Loss 130.61045837402344\n",
      "1: Encoding Loss 14.626443862915039, Transition Loss 2.16758394241333, Classifier Loss 0.3994552195072174, Total Loss 128.57122802734375\n",
      "1: Encoding Loss 14.141365051269531, Transition Loss 0.5245867967605591, Classifier Loss 0.4225255250930786, Total Loss 127.3105697631836\n",
      "1: Encoding Loss 12.51751708984375, Transition Loss 1.315286636352539, Classifier Loss 0.3764951825141907, Total Loss 113.2807388305664\n",
      "1: Encoding Loss 15.78516674041748, Transition Loss 2.4497456550598145, Classifier Loss 0.3849881887435913, Total Loss 134.18972778320312\n",
      "1: Encoding Loss 14.4741849899292, Transition Loss 0.40757831931114197, Classifier Loss 0.3499220907688141, Total Loss 122.00035858154297\n",
      "1: Encoding Loss 13.839218139648438, Transition Loss 1.3574248552322388, Classifier Loss 0.3610526919364929, Total Loss 119.68354797363281\n",
      "1: Encoding Loss 14.084230422973633, Transition Loss 1.3935883045196533, Classifier Loss 0.3886500597000122, Total Loss 123.92782592773438\n",
      "1: Encoding Loss 13.462063789367676, Transition Loss -0.2849579453468323, Classifier Loss 0.4113089144229889, Total Loss 121.90316009521484\n",
      "1: Encoding Loss 14.291529655456543, Transition Loss 1.1147308349609375, Classifier Loss 0.3713532090187073, Total Loss 123.33039855957031\n",
      "1: Encoding Loss 13.015619277954102, Transition Loss 1.2499988079071045, Classifier Loss 0.36542853713035583, Total Loss 115.1365737915039\n",
      "1: Encoding Loss 15.216163635253906, Transition Loss 1.1541149616241455, Classifier Loss 0.35715869069099426, Total Loss 127.47449493408203\n",
      "1: Encoding Loss 14.063606262207031, Transition Loss 0.5671297311782837, Classifier Loss 0.38369858264923096, Total Loss 122.97834777832031\n",
      "1: Encoding Loss 13.676554679870605, Transition Loss 0.4148396849632263, Classifier Loss 0.3721456527709961, Total Loss 119.43983459472656\n",
      "1: Encoding Loss 12.030064582824707, Transition Loss -0.47563549876213074, Classifier Loss 0.3278264105319977, Total Loss 104.96283721923828\n",
      "1: Encoding Loss 15.686792373657227, Transition Loss 3.320880174636841, Classifier Loss 0.3983363211154938, Total Loss 135.28274536132812\n",
      "1: Encoding Loss 13.442855834960938, Transition Loss 2.443329095840454, Classifier Loss 0.4152187705039978, Total Loss 123.1563491821289\n",
      "1: Encoding Loss 13.121673583984375, Transition Loss 0.24519851803779602, Classifier Loss 0.3621036410331726, Total Loss 115.03848266601562\n",
      "1: Encoding Loss 13.57972240447998, Transition Loss 1.7504050731658936, Classifier Loss 0.37656158208847046, Total Loss 119.83466339111328\n",
      "1: Encoding Loss 12.752405166625977, Transition Loss 1.3737788200378418, Classifier Loss 0.3906351625919342, Total Loss 116.1274642944336\n",
      "1: Encoding Loss 13.451863288879395, Transition Loss 2.5322015285491943, Classifier Loss 0.3752521276473999, Total Loss 119.249267578125\n",
      "1: Encoding Loss 13.064714431762695, Transition Loss 1.0074983835220337, Classifier Loss 0.3712220788002014, Total Loss 115.91349792480469\n",
      "1: Encoding Loss 12.235029220581055, Transition Loss -0.08086445927619934, Classifier Loss 0.3681642413139343, Total Loss 110.22657775878906\n",
      "1: Encoding Loss 14.118705749511719, Transition Loss 0.18978700041770935, Classifier Loss 0.2969647943973541, Total Loss 114.48462677001953\n",
      "1: Encoding Loss 13.525346755981445, Transition Loss 2.657179355621338, Classifier Loss 0.4036846160888672, Total Loss 122.58341979980469\n",
      "1: Encoding Loss 12.755594253540039, Transition Loss 1.5612488985061646, Classifier Loss 0.39799919724464417, Total Loss 116.9579849243164\n",
      "1: Encoding Loss 13.498693466186523, Transition Loss -0.1909893900156021, Classifier Loss 0.34515684843063354, Total Loss 115.50777435302734\n",
      "1: Encoding Loss 13.26612377166748, Transition Loss 2.434140920639038, Classifier Loss 0.40796008706092834, Total Loss 121.36641693115234\n",
      "1: Encoding Loss 12.52685832977295, Transition Loss 2.9001355171203613, Classifier Loss 0.3481157720088959, Total Loss 111.1327896118164\n",
      "1: Encoding Loss 15.153078079223633, Transition Loss 2.1544833183288574, Classifier Loss 0.4357358515262604, Total Loss 135.35385131835938\n",
      "1: Encoding Loss 13.729459762573242, Transition Loss 4.620380878448486, Classifier Loss 0.361506849527359, Total Loss 120.37560272216797\n",
      "1: Encoding Loss 13.299992561340332, Transition Loss 1.6552143096923828, Classifier Loss 0.38442039489746094, Total Loss 118.90408325195312\n",
      "1: Encoding Loss 13.561098098754883, Transition Loss 3.250659227371216, Classifier Loss 0.4427901804447174, Total Loss 126.94587707519531\n",
      "1: Encoding Loss 14.88139820098877, Transition Loss 3.306851387023926, Classifier Loss 0.3862285912036896, Total Loss 129.23399353027344\n",
      "1: Encoding Loss 13.720502853393555, Transition Loss 2.498878240585327, Classifier Loss 0.4030851423740387, Total Loss 123.6310806274414\n",
      "1: Encoding Loss 12.729387283325195, Transition Loss 2.1793947219848633, Classifier Loss 0.3334513306617737, Total Loss 110.59322357177734\n",
      "1: Encoding Loss 13.293988227844238, Transition Loss 1.2686196565628052, Classifier Loss 0.3904402554035187, Total Loss 119.31539916992188\n",
      "1: Encoding Loss 12.627704620361328, Transition Loss -0.4093407094478607, Classifier Loss 0.29348841309547424, Total Loss 105.11490631103516\n",
      "1: Encoding Loss 11.496103286743164, Transition Loss 2.0994112491607666, Classifier Loss 0.4094153642654419, Total Loss 110.75792694091797\n",
      "1: Encoding Loss 16.787425994873047, Transition Loss 1.8361892700195312, Classifier Loss 0.3907867670059204, Total Loss 140.5377197265625\n",
      "1: Encoding Loss 14.133615493774414, Transition Loss 3.144010543823242, Classifier Loss 0.4158007502555847, Total Loss 127.6393814086914\n",
      "1: Encoding Loss 13.184969902038574, Transition Loss 1.0091614723205566, Classifier Loss 0.303924024105072, Total Loss 109.90589141845703\n",
      "1: Encoding Loss 12.850732803344727, Transition Loss 1.0841556787490845, Classifier Loss 0.3687512278556824, Total Loss 114.4131851196289\n",
      "1: Encoding Loss 13.44006633758545, Transition Loss 1.9125144481658936, Classifier Loss 0.38040682673454285, Total Loss 119.44609832763672\n",
      "1: Encoding Loss 15.1597900390625, Transition Loss 4.299920558929443, Classifier Loss 0.4307616949081421, Total Loss 135.7548828125\n",
      "1: Encoding Loss 14.018365859985352, Transition Loss 0.9396083354949951, Classifier Loss 0.38945746421813965, Total Loss 123.43179321289062\n",
      "1: Encoding Loss 14.721769332885742, Transition Loss 1.8937193155288696, Classifier Loss 0.42372262477874756, Total Loss 131.4603729248047\n",
      "1: Encoding Loss 14.639470100402832, Transition Loss 4.38099479675293, Classifier Loss 0.3343735635280609, Total Loss 123.02657318115234\n",
      "1: Encoding Loss 14.514318466186523, Transition Loss 2.836007595062256, Classifier Loss 0.3976365923881531, Total Loss 127.98397827148438\n",
      "1: Encoding Loss 13.27971076965332, Transition Loss -0.1629353016614914, Classifier Loss 0.3698149621486664, Total Loss 116.6596908569336\n",
      "1: Encoding Loss 15.840458869934082, Transition Loss 3.7269692420959473, Classifier Loss 0.3850846588611603, Total Loss 135.04200744628906\n",
      "1: Encoding Loss 14.245160102844238, Transition Loss 0.3594716787338257, Classifier Loss 0.3968603014945984, Total Loss 125.30078887939453\n",
      "1: Encoding Loss 14.785494804382324, Transition Loss 3.514681339263916, Classifier Loss 0.38068073987960815, Total Loss 128.18692016601562\n",
      "1: Encoding Loss 14.466543197631836, Transition Loss 3.5221974849700928, Classifier Loss 0.37179768085479736, Total Loss 125.3879165649414\n",
      "1: Encoding Loss 13.561433792114258, Transition Loss 2.1756725311279297, Classifier Loss 0.37856921553611755, Total Loss 120.09579467773438\n",
      "1: Encoding Loss 14.626678466796875, Transition Loss 3.21494197845459, Classifier Loss 0.3271837532520294, Total Loss 121.7644271850586\n",
      "1: Encoding Loss 13.961637496948242, Transition Loss 2.1088294982910156, Classifier Loss 0.3796653151512146, Total Loss 122.57988739013672\n",
      "1: Encoding Loss 13.806530952453613, Transition Loss 2.1539769172668457, Classifier Loss 0.37826281785964966, Total Loss 121.52705383300781\n",
      "1: Encoding Loss 14.721491813659668, Transition Loss 2.9340617656707764, Classifier Loss 0.379352331161499, Total Loss 127.43781280517578\n",
      "1: Encoding Loss 13.689727783203125, Transition Loss 1.1766501665115356, Classifier Loss 0.39899909496307373, Total Loss 122.5089340209961\n",
      "1: Encoding Loss 14.930038452148438, Transition Loss 2.790743827819824, Classifier Loss 0.37793242931365967, Total Loss 128.48977661132812\n",
      "1: Encoding Loss 13.959281921386719, Transition Loss 0.9371330142021179, Classifier Loss 0.39297056198120117, Total Loss 123.42760467529297\n",
      "1: Encoding Loss 13.13987922668457, Transition Loss 1.2574256658554077, Classifier Loss 0.38195502758026123, Total Loss 117.53775024414062\n",
      "1: Encoding Loss 13.2105131149292, Transition Loss 2.044337034225464, Classifier Loss 0.4008578360080719, Total Loss 120.1666030883789\n",
      "1: Encoding Loss 13.174982070922852, Transition Loss 1.091140866279602, Classifier Loss 0.33801138401031494, Total Loss 113.28749084472656\n",
      "1: Encoding Loss 13.8654203414917, Transition Loss 3.59447979927063, Classifier Loss 0.4154319167137146, Total Loss 126.17350769042969\n",
      "1: Encoding Loss 13.203071594238281, Transition Loss 0.9248500466346741, Classifier Loss 0.37336891889572144, Total Loss 116.92526245117188\n",
      "1: Encoding Loss 13.334131240844727, Transition Loss 0.7746304273605347, Classifier Loss 0.3305583894252777, Total Loss 113.3704833984375\n",
      "1: Encoding Loss 12.90254020690918, Transition Loss 0.9431521892547607, Classifier Loss 0.38255903124809265, Total Loss 116.04840087890625\n",
      "1: Encoding Loss 12.616147994995117, Transition Loss 1.3265482187271118, Classifier Loss 0.39102092385292053, Total Loss 115.32959747314453\n",
      "1: Encoding Loss 15.416279792785645, Transition Loss 2.142873525619507, Classifier Loss 0.38349008560180664, Total Loss 131.70382690429688\n",
      "1: Encoding Loss 15.153905868530273, Transition Loss 3.755462884902954, Classifier Loss 0.4264220893383026, Total Loss 135.0678253173828\n",
      "1: Encoding Loss 14.362691879272461, Transition Loss 1.8938199281692505, Classifier Loss 0.35288700461387634, Total Loss 122.2223892211914\n",
      "1: Encoding Loss 13.292340278625488, Transition Loss 0.9990831017494202, Classifier Loss 0.35416585206985474, Total Loss 115.57025909423828\n",
      "1: Encoding Loss 13.583765983581543, Transition Loss 1.281242847442627, Classifier Loss 0.38061410188674927, Total Loss 120.07650756835938\n",
      "1: Encoding Loss 12.71462345123291, Transition Loss 1.4760733842849731, Classifier Loss 0.36257797479629517, Total Loss 113.13597106933594\n",
      "1: Encoding Loss 15.951484680175781, Transition Loss 2.8553264141082764, Classifier Loss 0.40027230978012085, Total Loss 136.87828063964844\n",
      "1: Encoding Loss 14.039271354675293, Transition Loss 1.6182589530944824, Classifier Loss 0.38013896346092224, Total Loss 122.8968276977539\n",
      "1: Encoding Loss 12.985761642456055, Transition Loss 0.9008246064186096, Classifier Loss 0.3732549846172333, Total Loss 115.60039520263672\n",
      "1: Encoding Loss 13.857412338256836, Transition Loss 2.4636387825012207, Classifier Loss 0.426627516746521, Total Loss 126.79268646240234\n",
      "1: Encoding Loss 13.077285766601562, Transition Loss 2.0970356464385986, Classifier Loss 0.368804395198822, Total Loss 116.18296813964844\n",
      "1: Encoding Loss 14.147781372070312, Transition Loss 1.045276403427124, Classifier Loss 0.3380186855792999, Total Loss 119.1066665649414\n",
      "1: Encoding Loss 12.946340560913086, Transition Loss 2.9380130767822266, Classifier Loss 0.4044923782348633, Total Loss 119.302490234375\n",
      "1: Encoding Loss 12.113279342651367, Transition Loss 0.43219250440597534, Classifier Loss 0.3935326933860779, Total Loss 112.20581817626953\n",
      "1: Encoding Loss 14.157293319702148, Transition Loss 1.4365593194961548, Classifier Loss 0.3596350848674774, Total Loss 121.48189544677734\n",
      "1: Encoding Loss 13.156062126159668, Transition Loss 0.009766928851604462, Classifier Loss 0.3313705325126648, Total Loss 112.07733917236328\n",
      "1: Encoding Loss 14.01862907409668, Transition Loss 1.700765609741211, Classifier Loss 0.32987868785858154, Total Loss 117.77995300292969\n",
      "1: Encoding Loss 13.047845840454102, Transition Loss 2.253286838531494, Classifier Loss 0.3431673049926758, Total Loss 113.50511932373047\n",
      "1: Encoding Loss 14.049962043762207, Transition Loss 1.3371005058288574, Classifier Loss 0.3843497335910797, Total Loss 123.26959228515625\n",
      "1: Encoding Loss 13.9423828125, Transition Loss 2.473574161529541, Classifier Loss 0.3829616904258728, Total Loss 122.93990325927734\n",
      "1: Encoding Loss 12.454992294311523, Transition Loss 1.4126460552215576, Classifier Loss 0.40044885873794556, Total Loss 115.33989715576172\n",
      "1: Encoding Loss 12.506924629211426, Transition Loss 0.8287056088447571, Classifier Loss 0.39807406067848206, Total Loss 115.18043518066406\n",
      "1: Encoding Loss 14.702458381652832, Transition Loss 1.93853759765625, Classifier Loss 0.38698628544807434, Total Loss 127.68878936767578\n",
      "1: Encoding Loss 13.849027633666992, Transition Loss 5.025071620941162, Classifier Loss 0.4129253029823303, Total Loss 126.396728515625\n",
      "1: Encoding Loss 13.386594772338867, Transition Loss 2.0841333866119385, Classifier Loss 0.363796204328537, Total Loss 117.53284454345703\n",
      "1: Encoding Loss 12.477791786193848, Transition Loss 1.9860986471176147, Classifier Loss 0.34160515666007996, Total Loss 109.82170867919922\n",
      "1: Encoding Loss 13.256701469421387, Transition Loss 2.315171718597412, Classifier Loss 0.3415776491165161, Total Loss 114.62405395507812\n",
      "1: Encoding Loss 12.344522476196289, Transition Loss 3.019066572189331, Classifier Loss 0.460461288690567, Total Loss 121.32089233398438\n",
      "1: Encoding Loss 13.98760986328125, Transition Loss 1.1815462112426758, Classifier Loss 0.38519030809402466, Total Loss 122.91730499267578\n",
      "1: Encoding Loss 13.521675109863281, Transition Loss 1.6548652648925781, Classifier Loss 0.3714088797569275, Total Loss 118.9328842163086\n",
      "1: Encoding Loss 13.276229858398438, Transition Loss 3.007753849029541, Classifier Loss 0.44017812609672546, Total Loss 124.87828826904297\n",
      "1: Encoding Loss 12.93615436553955, Transition Loss 1.8375895023345947, Classifier Loss 0.3833942711353302, Total Loss 116.69139862060547\n",
      "1: Encoding Loss 14.496030807495117, Transition Loss 3.104628086090088, Classifier Loss 0.36436882615089417, Total Loss 124.65492248535156\n",
      "1: Encoding Loss 13.660593032836914, Transition Loss 0.5472111701965332, Classifier Loss 0.3366146385669708, Total Loss 115.84391784667969\n",
      "1: Encoding Loss 12.034031867980957, Transition Loss 0.9928368926048279, Classifier Loss 0.3726523220539093, Total Loss 109.8665542602539\n",
      "1: Encoding Loss 15.456469535827637, Transition Loss 1.425161600112915, Classifier Loss 0.3493478298187256, Total Loss 128.24366760253906\n",
      "1: Encoding Loss 13.960416793823242, Transition Loss 1.479567289352417, Classifier Loss 0.3313489854335785, Total Loss 117.48922729492188\n",
      "1: Encoding Loss 13.123619079589844, Transition Loss 0.19957779347896576, Classifier Loss 0.3907504975795746, Total Loss 117.89659881591797\n",
      "1: Encoding Loss 13.49995231628418, Transition Loss 1.1994110345840454, Classifier Loss 0.3445553183555603, Total Loss 115.93501281738281\n",
      "1: Encoding Loss 13.75994873046875, Transition Loss 0.21837686002254486, Classifier Loss 0.3487536311149597, Total Loss 117.52240753173828\n",
      "1: Encoding Loss 13.63556957244873, Transition Loss 2.2572503089904785, Classifier Loss 0.333334743976593, Total Loss 116.04979705810547\n",
      "1: Encoding Loss 14.339241027832031, Transition Loss 2.614934206008911, Classifier Loss 0.3776065707206726, Total Loss 124.84207916259766\n",
      "1: Encoding Loss 11.84024429321289, Transition Loss 1.4365415573120117, Classifier Loss 0.3539575934410095, Total Loss 107.0118408203125\n",
      "1: Encoding Loss 12.842643737792969, Transition Loss 1.2750945091247559, Classifier Loss 0.3617348074913025, Total Loss 113.7393798828125\n",
      "1: Encoding Loss 13.727712631225586, Transition Loss 0.46899867057800293, Classifier Loss 0.3387657105922699, Total Loss 116.43045043945312\n",
      "1: Encoding Loss 13.26361083984375, Transition Loss 2.3880906105041504, Classifier Loss 0.34712550044059753, Total Loss 115.24945831298828\n",
      "1: Encoding Loss 11.700638771057129, Transition Loss 0.18282753229141235, Classifier Loss 0.3328358829021454, Total Loss 103.56055450439453\n",
      "1: Encoding Loss 12.689385414123535, Transition Loss 2.1846208572387695, Classifier Loss 0.35155153274536133, Total Loss 112.16531372070312\n",
      "1: Encoding Loss 11.869027137756348, Transition Loss 0.9015249609947205, Classifier Loss 0.360775351524353, Total Loss 107.65231323242188\n",
      "1: Encoding Loss 14.521517753601074, Transition Loss 1.8721246719360352, Classifier Loss 0.349956750869751, Total Loss 122.8736343383789\n",
      "1: Encoding Loss 12.511877059936523, Transition Loss 2.137422561645508, Classifier Loss 0.3492431640625, Total Loss 110.85055541992188\n",
      "1: Encoding Loss 13.267755508422852, Transition Loss 3.1294870376586914, Classifier Loss 0.37785521149635315, Total Loss 118.64385223388672\n",
      "1: Encoding Loss 12.691949844360352, Transition Loss 0.4062842130661011, Classifier Loss 0.34944620728492737, Total Loss 111.25884246826172\n",
      "1: Encoding Loss 13.892873764038086, Transition Loss 3.1736268997192383, Classifier Loss 0.4012642502784729, Total Loss 124.75312042236328\n",
      "1: Encoding Loss 12.562543869018555, Transition Loss 1.360886573791504, Classifier Loss 0.41118061542510986, Total Loss 117.03768920898438\n",
      "1: Encoding Loss 13.30237102508545, Transition Loss 3.6261730194091797, Classifier Loss 0.38494622707366943, Total Loss 119.75932312011719\n",
      "1: Encoding Loss 13.031816482543945, Transition Loss 0.75789475440979, Classifier Loss 0.36321964859962463, Total Loss 114.81602478027344\n",
      "1: Encoding Loss 13.212486267089844, Transition Loss 1.7392865419387817, Classifier Loss 0.371469646692276, Total Loss 117.11759948730469\n",
      "1: Encoding Loss 12.978094100952148, Transition Loss 1.3880743980407715, Classifier Loss 0.34456706047058105, Total Loss 112.88050079345703\n",
      "1: Encoding Loss 13.759332656860352, Transition Loss 1.1946769952774048, Classifier Loss 0.35396504402160645, Total Loss 118.43036651611328\n",
      "1: Encoding Loss 12.589155197143555, Transition Loss 2.4417688846588135, Classifier Loss 0.3977634012699127, Total Loss 116.2879867553711\n",
      "1: Encoding Loss 11.247889518737793, Transition Loss 2.6253814697265625, Classifier Loss 0.4075462818145752, Total Loss 109.29212951660156\n",
      "1: Encoding Loss 14.196884155273438, Transition Loss 1.694878101348877, Classifier Loss 0.3256100118160248, Total Loss 118.42025756835938\n",
      "1: Encoding Loss 12.251850128173828, Transition Loss 0.835618257522583, Classifier Loss 0.39115768671035767, Total Loss 112.96111297607422\n",
      "1: Encoding Loss 13.522424697875977, Transition Loss 2.240845203399658, Classifier Loss 0.38415300846099854, Total Loss 120.4461898803711\n",
      "1: Encoding Loss 13.261480331420898, Transition Loss 1.755765676498413, Classifier Loss 0.3635621666908264, Total Loss 116.62741088867188\n",
      "1: Encoding Loss 12.659186363220215, Transition Loss 0.8232375979423523, Classifier Loss 0.362243115901947, Total Loss 112.50872802734375\n",
      "1: Encoding Loss 12.91784381866455, Transition Loss 2.8396124839782715, Classifier Loss 0.3859523832798004, Total Loss 117.23814392089844\n",
      "1: Encoding Loss 12.811098098754883, Transition Loss 1.998718023300171, Classifier Loss 0.3900366425514221, Total Loss 116.66973876953125\n",
      "1: Encoding Loss 12.534640312194824, Transition Loss 1.2687408924102783, Classifier Loss 0.37229490280151367, Total Loss 112.94483947753906\n",
      "1: Encoding Loss 11.624417304992676, Transition Loss 1.3333191871643066, Classifier Loss 0.3647206127643585, Total Loss 106.75189208984375\n",
      "1: Encoding Loss 12.999707221984863, Transition Loss 1.4663777351379395, Classifier Loss 0.3750939667224884, Total Loss 116.09419250488281\n",
      "1: Encoding Loss 14.078974723815918, Transition Loss 2.4353418350219727, Classifier Loss 0.34866344928741455, Total Loss 120.3143310546875\n",
      "1: Encoding Loss 13.273226737976074, Transition Loss 1.8400862216949463, Classifier Loss 0.3985852003097534, Total Loss 120.23392486572266\n",
      "1: Encoding Loss 13.533968925476074, Transition Loss 1.767876148223877, Classifier Loss 0.3580426871776581, Total Loss 117.71524047851562\n",
      "1: Encoding Loss 12.19763469696045, Transition Loss 0.884713888168335, Classifier Loss 0.36368924379348755, Total Loss 109.90861511230469\n",
      "1: Encoding Loss 11.883182525634766, Transition Loss 1.3715760707855225, Classifier Loss 0.3596700131893158, Total Loss 107.81472778320312\n",
      "1: Encoding Loss 14.430496215820312, Transition Loss 3.6471986770629883, Classifier Loss 0.3897104859352112, Total Loss 127.01290893554688\n",
      "1: Encoding Loss 13.255292892456055, Transition Loss 0.2985967695713043, Classifier Loss 0.3576865792274475, Total Loss 115.41985321044922\n",
      "1: Encoding Loss 12.8175630569458, Transition Loss 2.353553056716919, Classifier Loss 0.35768231749534607, Total Loss 113.61503601074219\n",
      "1: Encoding Loss 12.77989387512207, Transition Loss 1.1889055967330933, Classifier Loss 0.43010610342025757, Total Loss 120.16553497314453\n",
      "1: Encoding Loss 13.73045539855957, Transition Loss 2.5839531421661377, Classifier Loss 0.32782503962516785, Total Loss 116.19882202148438\n",
      "1: Encoding Loss 13.928518295288086, Transition Loss 1.7538286447525024, Classifier Loss 0.34875068068504333, Total Loss 119.14771270751953\n",
      "1: Encoding Loss 13.234479904174805, Transition Loss 0.364531010389328, Classifier Loss 0.36390405893325806, Total Loss 115.94309997558594\n",
      "1: Encoding Loss 13.185226440429688, Transition Loss 1.866459608078003, Classifier Loss 0.37103718519210815, Total Loss 116.96165466308594\n",
      "1: Encoding Loss 14.298480033874512, Transition Loss 1.5852978229522705, Classifier Loss 0.36406779289245605, Total Loss 122.83177947998047\n",
      "1: Encoding Loss 13.131784439086914, Transition Loss 1.910637617111206, Classifier Loss 0.33987557888031006, Total Loss 113.54251861572266\n",
      "1: Encoding Loss 13.383817672729492, Transition Loss 2.408484697341919, Classifier Loss 0.3221154808998108, Total Loss 113.47785186767578\n",
      "1: Encoding Loss 11.39519214630127, Transition Loss 0.023053616285324097, Classifier Loss 0.27522405982017517, Total Loss 95.90278625488281\n",
      "2: Encoding Loss 14.350003242492676, Transition Loss 2.5014007091522217, Classifier Loss 0.38478273153305054, Total Loss 125.57884979248047\n",
      "2: Encoding Loss 12.210925102233887, Transition Loss 1.3139305114746094, Classifier Loss 0.35440027713775635, Total Loss 109.23115539550781\n",
      "2: Encoding Loss 13.52003288269043, Transition Loss 3.2187986373901367, Classifier Loss 0.4240621328353882, Total Loss 124.81393432617188\n",
      "2: Encoding Loss 12.74206256866455, Transition Loss 1.087065577507019, Classifier Loss 0.3686513900756836, Total Loss 113.75234985351562\n",
      "2: Encoding Loss 12.230682373046875, Transition Loss 0.3430793881416321, Classifier Loss 0.36953455209732056, Total Loss 110.47478485107422\n",
      "2: Encoding Loss 13.392000198364258, Transition Loss 4.434649467468262, Classifier Loss 0.37650662660598755, Total Loss 119.77652740478516\n",
      "2: Encoding Loss 11.84516716003418, Transition Loss 2.172595500946045, Classifier Loss 0.3997613787651062, Total Loss 111.91619110107422\n",
      "2: Encoding Loss 12.179444313049316, Transition Loss 1.3909066915512085, Classifier Loss 0.37969323992729187, Total Loss 111.60235595703125\n",
      "2: Encoding Loss 12.053550720214844, Transition Loss 1.4412087202072144, Classifier Loss 0.3667357861995697, Total Loss 109.57137298583984\n",
      "2: Encoding Loss 11.515411376953125, Transition Loss 1.8338024616241455, Classifier Loss 0.3923894762992859, Total Loss 109.06494140625\n",
      "2: Encoding Loss 13.69030475616455, Transition Loss 1.0242362022399902, Classifier Loss 0.3629683256149292, Total Loss 118.84835052490234\n",
      "2: Encoding Loss 12.412782669067383, Transition Loss -0.03083088994026184, Classifier Loss 0.3233698010444641, Total Loss 106.81365966796875\n",
      "2: Encoding Loss 10.928762435913086, Transition Loss 0.7961958050727844, Classifier Loss 0.37234213948249817, Total Loss 103.12527465820312\n",
      "2: Encoding Loss 13.309816360473633, Transition Loss 2.3916637897491455, Classifier Loss 0.33016735315322876, Total Loss 113.83230590820312\n",
      "2: Encoding Loss 12.666364669799805, Transition Loss 2.000187873840332, Classifier Loss 0.4023042321205139, Total Loss 117.0286865234375\n",
      "2: Encoding Loss 13.137014389038086, Transition Loss 1.7177895307540894, Classifier Loss 0.3931552469730377, Total Loss 118.82473754882812\n",
      "2: Encoding Loss 13.934442520141602, Transition Loss 2.8636279106140137, Classifier Loss 0.40278416872024536, Total Loss 125.03052520751953\n",
      "2: Encoding Loss 12.804079055786133, Transition Loss 0.14920318126678467, Classifier Loss 0.35772550106048584, Total Loss 112.6567153930664\n",
      "2: Encoding Loss 14.319971084594727, Transition Loss 3.879425525665283, Classifier Loss 0.34293070435523987, Total Loss 121.76467895507812\n",
      "2: Encoding Loss 12.312978744506836, Transition Loss 0.06385017186403275, Classifier Loss 0.33917883038520813, Total Loss 107.82130432128906\n",
      "2: Encoding Loss 10.919288635253906, Transition Loss 1.9821377992630005, Classifier Loss 0.32218503952026367, Total Loss 98.52709197998047\n",
      "2: Encoding Loss 12.892663955688477, Transition Loss 1.050174355506897, Classifier Loss 0.34376680850982666, Total Loss 112.1527328491211\n",
      "2: Encoding Loss 12.53046989440918, Transition Loss 0.41886940598487854, Classifier Loss 0.30335959792137146, Total Loss 105.68633270263672\n",
      "2: Encoding Loss 11.986417770385742, Transition Loss 0.07911945134401321, Classifier Loss 0.3500138521194458, Total Loss 106.9515380859375\n",
      "2: Encoding Loss 12.81421947479248, Transition Loss 0.9247564077377319, Classifier Loss 0.3537485599517822, Total Loss 112.63008117675781\n",
      "2: Encoding Loss 12.268035888671875, Transition Loss 1.5029385089874268, Classifier Loss 0.3345451354980469, Total Loss 107.66390228271484\n",
      "2: Encoding Loss 11.96329116821289, Transition Loss 0.5742102861404419, Classifier Loss 0.3580344021320343, Total Loss 107.81287384033203\n",
      "2: Encoding Loss 13.576299667358398, Transition Loss 1.792718529701233, Classifier Loss 0.4092380404472351, Total Loss 123.09869384765625\n",
      "2: Encoding Loss 12.09284496307373, Transition Loss 1.298290729522705, Classifier Loss 0.3498614430427551, Total Loss 108.06253814697266\n",
      "2: Encoding Loss 11.545969009399414, Transition Loss 0.9254617691040039, Classifier Loss 0.3497849702835083, Total Loss 104.62450408935547\n",
      "2: Encoding Loss 10.876654624938965, Transition Loss 1.226158857345581, Classifier Loss 0.3136938214302063, Total Loss 97.11978149414062\n",
      "2: Encoding Loss 12.082133293151855, Transition Loss 0.5606558918952942, Classifier Loss 0.3443280756473541, Total Loss 107.1498794555664\n",
      "2: Encoding Loss 13.53490161895752, Transition Loss 2.513395071029663, Classifier Loss 0.34871408343315125, Total Loss 117.08617401123047\n",
      "2: Encoding Loss 11.589404106140137, Transition Loss 1.272125482559204, Classifier Loss 0.35580092668533325, Total Loss 105.62537384033203\n",
      "2: Encoding Loss 12.30048942565918, Transition Loss 2.5007433891296387, Classifier Loss 0.3796367943286896, Total Loss 112.76691436767578\n",
      "2: Encoding Loss 12.213334083557129, Transition Loss 1.5858232975006104, Classifier Loss 0.42035332322120667, Total Loss 115.94966888427734\n",
      "2: Encoding Loss 13.29271125793457, Transition Loss 1.7565170526504517, Classifier Loss 0.3609585762023926, Total Loss 116.55473327636719\n",
      "2: Encoding Loss 12.175105094909668, Transition Loss 0.19676297903060913, Classifier Loss 0.3490749001502991, Total Loss 108.03683471679688\n",
      "2: Encoding Loss 12.985666275024414, Transition Loss 1.4422898292541504, Classifier Loss 0.3771500885486603, Total Loss 116.2059326171875\n",
      "2: Encoding Loss 13.089677810668945, Transition Loss 0.7436310052871704, Classifier Loss 0.3149993419647217, Total Loss 110.33546447753906\n",
      "2: Encoding Loss 13.425043106079102, Transition Loss 1.0037564039230347, Classifier Loss 0.32435062527656555, Total Loss 113.38682556152344\n",
      "2: Encoding Loss 13.21702766418457, Transition Loss 1.1993240118026733, Classifier Loss 0.34105753898620605, Total Loss 113.88765716552734\n",
      "2: Encoding Loss 13.137487411499023, Transition Loss 1.3198933601379395, Classifier Loss 0.3604795038700104, Total Loss 115.40083312988281\n",
      "2: Encoding Loss 12.10073471069336, Transition Loss 0.5733522772789001, Classifier Loss 0.35582995414733887, Total Loss 108.416748046875\n",
      "2: Encoding Loss 11.039387702941895, Transition Loss -0.024384260177612305, Classifier Loss 0.34636107087135315, Total Loss 100.87242889404297\n",
      "2: Encoding Loss 13.233302116394043, Transition Loss 1.9464585781097412, Classifier Loss 0.36473214626312256, Total Loss 116.651611328125\n",
      "2: Encoding Loss 12.979799270629883, Transition Loss 2.104215383529663, Classifier Loss 0.37215113639831543, Total Loss 115.93560028076172\n",
      "2: Encoding Loss 12.467580795288086, Transition Loss 0.6190105080604553, Classifier Loss 0.37788769602775574, Total Loss 112.84185791015625\n",
      "2: Encoding Loss 10.299514770507812, Transition Loss 0.05487966164946556, Classifier Loss 0.37411418557167053, Total Loss 99.23046112060547\n",
      "2: Encoding Loss 12.435312271118164, Transition Loss 0.827767014503479, Classifier Loss 0.327750027179718, Total Loss 107.71798706054688\n",
      "2: Encoding Loss 11.601034164428711, Transition Loss 1.2281908988952637, Classifier Loss 0.37987565994262695, Total Loss 108.08505249023438\n",
      "2: Encoding Loss 13.12500286102295, Transition Loss 1.2088446617126465, Classifier Loss 0.3381691873073578, Total Loss 113.05047607421875\n",
      "2: Encoding Loss 10.722646713256836, Transition Loss -0.1160939410328865, Classifier Loss 0.35922691226005554, Total Loss 100.25852966308594\n",
      "2: Encoding Loss 12.339070320129395, Transition Loss 0.40095630288124084, Classifier Loss 0.33677390217781067, Total Loss 107.87220001220703\n",
      "2: Encoding Loss 10.090961456298828, Transition Loss -0.6409620642662048, Classifier Loss 0.31009575724601746, Total Loss 91.55508422851562\n",
      "2: Encoding Loss 12.718973159790039, Transition Loss 1.34303879737854, Classifier Loss 0.34052351117134094, Total Loss 110.90341186523438\n",
      "2: Encoding Loss 12.62602710723877, Transition Loss 1.2686024904251099, Classifier Loss 0.3775790333747864, Total Loss 114.0215072631836\n",
      "2: Encoding Loss 13.260908126831055, Transition Loss 1.5645864009857178, Classifier Loss 0.319499671459198, Total Loss 112.14125061035156\n",
      "2: Encoding Loss 13.737106323242188, Transition Loss 2.277172565460205, Classifier Loss 0.38840073347091675, Total Loss 122.17357635498047\n",
      "2: Encoding Loss 11.871675491333008, Transition Loss 1.0841199159622192, Classifier Loss 0.33821985125541687, Total Loss 105.48568725585938\n",
      "2: Encoding Loss 11.5274019241333, Transition Loss 0.8846850991249084, Classifier Loss 0.3454517126083374, Total Loss 104.0634536743164\n",
      "2: Encoding Loss 11.121176719665527, Transition Loss 1.6381573677062988, Classifier Loss 0.3619987964630127, Total Loss 103.58220672607422\n",
      "2: Encoding Loss 12.106335639953613, Transition Loss 2.171886444091797, Classifier Loss 0.35581719875335693, Total Loss 109.08848571777344\n",
      "2: Encoding Loss 12.61695671081543, Transition Loss 3.872098922729492, Classifier Loss 0.38531753420829773, Total Loss 115.78233337402344\n",
      "2: Encoding Loss 12.393878936767578, Transition Loss 0.9108321666717529, Classifier Loss 0.329870343208313, Total Loss 107.71464538574219\n",
      "2: Encoding Loss 12.736709594726562, Transition Loss 0.6604061126708984, Classifier Loss 0.3762141168117523, Total Loss 114.30583190917969\n",
      "2: Encoding Loss 11.334628105163574, Transition Loss 1.4289664030075073, Classifier Loss 0.41892826557159424, Total Loss 110.4721908569336\n",
      "2: Encoding Loss 12.092767715454102, Transition Loss 2.0148940086364746, Classifier Loss 0.33325469493865967, Total Loss 106.68803405761719\n",
      "2: Encoding Loss 12.468844413757324, Transition Loss 0.316726416349411, Classifier Loss 0.33529940247535706, Total Loss 108.46971130371094\n",
      "2: Encoding Loss 12.945317268371582, Transition Loss 0.8921735286712646, Classifier Loss 0.4428546130657196, Total Loss 122.31423950195312\n",
      "2: Encoding Loss 12.781618118286133, Transition Loss 1.5387322902679443, Classifier Loss 0.3481787443161011, Total Loss 112.12307739257812\n",
      "2: Encoding Loss 10.783063888549805, Transition Loss -0.16499309241771698, Classifier Loss 0.3066418170928955, Total Loss 95.36249542236328\n",
      "2: Encoding Loss 10.896051406860352, Transition Loss 1.2806904315948486, Classifier Loss 0.38344642519950867, Total Loss 104.23323059082031\n",
      "2: Encoding Loss 11.728811264038086, Transition Loss 0.4063607156276703, Classifier Loss 0.32782816886901855, Total Loss 103.31822967529297\n",
      "2: Encoding Loss 11.096049308776855, Transition Loss 0.34088942408561707, Classifier Loss 0.32862502336502075, Total Loss 99.57515716552734\n",
      "2: Encoding Loss 14.786264419555664, Transition Loss 1.2141088247299194, Classifier Loss 0.3574027717113495, Total Loss 124.94351196289062\n",
      "2: Encoding Loss 12.13776969909668, Transition Loss 1.5819743871688843, Classifier Loss 0.339952290058136, Total Loss 107.45464324951172\n",
      "2: Encoding Loss 12.709769248962402, Transition Loss 1.416656494140625, Classifier Loss 0.33764952421188354, Total Loss 110.59024047851562\n",
      "2: Encoding Loss 11.157674789428711, Transition Loss 0.9481136798858643, Classifier Loss 0.3666501045227051, Total Loss 103.99030303955078\n",
      "2: Encoding Loss 9.963560104370117, Transition Loss 0.0611034631729126, Classifier Loss 0.3414599597454071, Total Loss 93.9518051147461\n",
      "2: Encoding Loss 17.184846878051758, Transition Loss 2.0957155227661133, Classifier Loss 0.32727938890457153, Total Loss 136.67530822753906\n",
      "2: Encoding Loss 15.004277229309082, Transition Loss 0.2601357102394104, Classifier Loss 0.39066755771636963, Total Loss 129.19647216796875\n",
      "2: Encoding Loss 12.354948997497559, Transition Loss 0.3967021703720093, Classifier Loss 0.41207563877105713, Total Loss 115.4959487915039\n",
      "2: Encoding Loss 12.362203598022461, Transition Loss 1.3607245683670044, Classifier Loss 0.38320857286453247, Total Loss 113.03836822509766\n",
      "2: Encoding Loss 11.065773010253906, Transition Loss 0.4914777874946594, Classifier Loss 0.31939154863357544, Total Loss 98.53038787841797\n",
      "2: Encoding Loss 11.04292106628418, Transition Loss 0.8797969818115234, Classifier Loss 0.31116071343421936, Total Loss 97.72552490234375\n",
      "2: Encoding Loss 12.0263090133667, Transition Loss -0.0005264170467853546, Classifier Loss 0.3672986626625061, Total Loss 108.88772583007812\n",
      "2: Encoding Loss 11.6123685836792, Transition Loss 2.2844367027282715, Classifier Loss 0.36447471380233765, Total Loss 107.03546142578125\n",
      "2: Encoding Loss 11.589399337768555, Transition Loss 0.9658001661300659, Classifier Loss 0.3232216238975525, Total Loss 102.24488830566406\n",
      "2: Encoding Loss 12.666519165039062, Transition Loss 0.9372714757919312, Classifier Loss 0.3502615988254547, Total Loss 111.40018463134766\n",
      "2: Encoding Loss 12.349859237670898, Transition Loss 1.176470160484314, Classifier Loss 0.3410739600658417, Total Loss 108.6771469116211\n",
      "2: Encoding Loss 11.867609977722168, Transition Loss 1.3736947774887085, Classifier Loss 0.3617105185985565, Total Loss 107.92619323730469\n",
      "2: Encoding Loss 12.42310619354248, Transition Loss 0.6817760467529297, Classifier Loss 0.36524730920791626, Total Loss 111.3360824584961\n",
      "2: Encoding Loss 12.334951400756836, Transition Loss -0.2543071210384369, Classifier Loss 0.3605964183807373, Total Loss 110.06925201416016\n",
      "2: Encoding Loss 11.129911422729492, Transition Loss 1.537767767906189, Classifier Loss 0.3585851788520813, Total Loss 103.25308990478516\n",
      "2: Encoding Loss 10.20491886138916, Transition Loss 0.2679087817668915, Classifier Loss 0.3330767750740051, Total Loss 94.64435577392578\n",
      "2: Encoding Loss 11.264986038208008, Transition Loss 0.8028659820556641, Classifier Loss 0.40116238594055176, Total Loss 108.02730560302734\n",
      "2: Encoding Loss 12.621960639953613, Transition Loss 0.7616764307022095, Classifier Loss 0.38075414299964905, Total Loss 114.11184692382812\n",
      "2: Encoding Loss 12.005515098571777, Transition Loss 0.2925134003162384, Classifier Loss 0.31945765018463135, Total Loss 104.09586334228516\n",
      "2: Encoding Loss 12.252483367919922, Transition Loss 0.6750444173812866, Classifier Loss 0.3154202997684479, Total Loss 105.32695007324219\n",
      "2: Encoding Loss 12.668611526489258, Transition Loss 1.9512722492218018, Classifier Loss 0.335432767868042, Total Loss 110.33545684814453\n",
      "2: Encoding Loss 10.845370292663574, Transition Loss 0.41818827390670776, Classifier Loss 0.30174845457077026, Total Loss 95.41434478759766\n",
      "2: Encoding Loss 10.551692962646484, Transition Loss 0.4632079005241394, Classifier Loss 0.3851027488708496, Total Loss 102.0057144165039\n",
      "2: Encoding Loss 12.942956924438477, Transition Loss 1.1352362632751465, Classifier Loss 0.3632257282733917, Total Loss 114.43441009521484\n",
      "2: Encoding Loss 12.904589653015137, Transition Loss 0.511925995349884, Classifier Loss 0.34448373317718506, Total Loss 112.0806884765625\n",
      "2: Encoding Loss 13.449673652648926, Transition Loss 0.6966708898544312, Classifier Loss 0.3576393723487854, Total Loss 116.74065399169922\n",
      "2: Encoding Loss 11.59135627746582, Transition Loss -0.11054778099060059, Classifier Loss 0.2845819592475891, Total Loss 98.00629425048828\n",
      "2: Encoding Loss 10.002232551574707, Transition Loss 1.0438085794448853, Classifier Loss 0.3328481912612915, Total Loss 93.71574401855469\n",
      "2: Encoding Loss 13.195684432983398, Transition Loss 1.2863411903381348, Classifier Loss 0.29714393615722656, Total Loss 109.40303802490234\n",
      "2: Encoding Loss 12.728596687316895, Transition Loss 0.5775353908538818, Classifier Loss 0.28707221150398254, Total Loss 105.309814453125\n",
      "2: Encoding Loss 11.299456596374512, Transition Loss 0.37487518787384033, Classifier Loss 0.36054113507270813, Total Loss 104.00080871582031\n",
      "2: Encoding Loss 11.782301902770996, Transition Loss 1.0410069227218628, Classifier Loss 0.3290369212627411, Total Loss 104.01390838623047\n",
      "2: Encoding Loss 11.38197135925293, Transition Loss 0.20354148745536804, Classifier Loss 0.30630818009376526, Total Loss 99.00406646728516\n",
      "2: Encoding Loss 10.363262176513672, Transition Loss -0.2683788537979126, Classifier Loss 0.3248518109321594, Total Loss 94.6646499633789\n",
      "2: Encoding Loss 12.366572380065918, Transition Loss 0.3032568693161011, Classifier Loss 0.33992162346839905, Total Loss 108.31290435791016\n",
      "2: Encoding Loss 11.26448917388916, Transition Loss 0.37871479988098145, Classifier Loss 0.30942609906196594, Total Loss 98.68103790283203\n",
      "2: Encoding Loss 11.410558700561523, Transition Loss -0.06829887628555298, Classifier Loss 0.34376025199890137, Total Loss 102.83935546875\n",
      "2: Encoding Loss 9.146171569824219, Transition Loss -0.5788448452949524, Classifier Loss 0.3246614336967468, Total Loss 87.34294891357422\n",
      "2: Encoding Loss 11.3214693069458, Transition Loss 0.5193727016448975, Classifier Loss 0.36617282032966614, Total Loss 104.75384521484375\n",
      "2: Encoding Loss 11.372880935668945, Transition Loss -0.10833434760570526, Classifier Loss 0.3301462233066559, Total Loss 101.25186157226562\n",
      "2: Encoding Loss 14.00914192199707, Transition Loss 0.826767086982727, Classifier Loss 0.35511693358421326, Total Loss 119.89724731445312\n",
      "2: Encoding Loss 12.724710464477539, Transition Loss 0.33149927854537964, Classifier Loss 0.34563642740249634, Total Loss 111.04450988769531\n",
      "2: Encoding Loss 13.99351692199707, Transition Loss 0.41519486904144287, Classifier Loss 0.2856117784976959, Total Loss 112.68836212158203\n",
      "2: Encoding Loss 12.679586410522461, Transition Loss 1.5042645931243896, Classifier Loss 0.36684420704841614, Total Loss 113.36365509033203\n",
      "2: Encoding Loss 10.921890258789062, Transition Loss 0.26382750272750854, Classifier Loss 0.3316318392753601, Total Loss 98.80005645751953\n",
      "2: Encoding Loss 12.804571151733398, Transition Loss 1.4328135251998901, Classifier Loss 0.316324383020401, Total Loss 109.03299713134766\n",
      "2: Encoding Loss 12.213980674743652, Transition Loss 1.182918906211853, Classifier Loss 0.3620922267436981, Total Loss 109.96627807617188\n",
      "2: Encoding Loss 9.648468971252441, Transition Loss 0.13701315224170685, Classifier Loss 0.2832852900028229, Total Loss 86.2741470336914\n",
      "2: Encoding Loss 11.268718719482422, Transition Loss -0.23359541594982147, Classifier Loss 0.2978564500808716, Total Loss 97.39786529541016\n",
      "2: Encoding Loss 12.719900131225586, Transition Loss 1.8820139169692993, Classifier Loss 0.3452734351158142, Total Loss 111.59955596923828\n",
      "2: Encoding Loss 10.065286636352539, Transition Loss 0.5270907878875732, Classifier Loss 0.34360769391059875, Total Loss 94.96332550048828\n",
      "2: Encoding Loss 13.210882186889648, Transition Loss 0.7523362636566162, Classifier Loss 0.37302166223526, Total Loss 116.86839294433594\n",
      "2: Encoding Loss 13.291372299194336, Transition Loss 1.0368794202804565, Classifier Loss 0.3654353618621826, Total Loss 116.70652770996094\n",
      "2: Encoding Loss 11.3562593460083, Transition Loss 0.21306897699832916, Classifier Loss 0.2934081256389618, Total Loss 97.5635986328125\n",
      "2: Encoding Loss 10.183486938476562, Transition Loss -0.1960061937570572, Classifier Loss 0.31975093483924866, Total Loss 93.07594299316406\n",
      "2: Encoding Loss 13.365506172180176, Transition Loss 1.1122732162475586, Classifier Loss 0.39354339241981506, Total Loss 119.9922866821289\n",
      "2: Encoding Loss 12.740636825561523, Transition Loss 0.3591974675655365, Classifier Loss 0.2833084166049957, Total Loss 104.91834259033203\n",
      "2: Encoding Loss 12.193643569946289, Transition Loss 0.6931837797164917, Classifier Loss 0.320868581533432, Total Loss 105.52599334716797\n",
      "2: Encoding Loss 11.840991020202637, Transition Loss 1.034814715385437, Classifier Loss 0.30540525913238525, Total Loss 102.00040435791016\n",
      "2: Encoding Loss 15.086196899414062, Transition Loss 0.16575157642364502, Classifier Loss 0.2935517728328705, Total Loss 119.93865966796875\n",
      "2: Encoding Loss 12.96748161315918, Transition Loss 0.295136421918869, Classifier Loss 0.30530786514282227, Total Loss 108.4537353515625\n",
      "2: Encoding Loss 12.79975700378418, Transition Loss 0.9554249048233032, Classifier Loss 0.3798828423023224, Total Loss 115.16900634765625\n",
      "2: Encoding Loss 12.03966999053955, Transition Loss 0.8402500152587891, Classifier Loss 0.31952357292175293, Total Loss 104.52648162841797\n",
      "2: Encoding Loss 13.713841438293457, Transition Loss 1.3079113960266113, Classifier Loss 0.3870932161808014, Total Loss 121.51553344726562\n",
      "2: Encoding Loss 11.900184631347656, Transition Loss 0.5174448490142822, Classifier Loss 0.27986273169517517, Total Loss 99.5943603515625\n",
      "2: Encoding Loss 9.943835258483887, Transition Loss -0.2529427111148834, Classifier Loss 0.30338940024375916, Total Loss 90.0018539428711\n",
      "2: Encoding Loss 9.932705879211426, Transition Loss 0.9138439297676086, Classifier Loss 0.33188769221305847, Total Loss 93.15054321289062\n",
      "2: Encoding Loss 12.668108940124512, Transition Loss 1.0894720554351807, Classifier Loss 0.40906161069869995, Total Loss 117.35061645507812\n",
      "2: Encoding Loss 11.244616508483887, Transition Loss 0.09388033300638199, Classifier Loss 0.34013625979423523, Total Loss 101.51888275146484\n",
      "2: Encoding Loss 10.14178466796875, Transition Loss 0.07776308059692383, Classifier Loss 0.3322165012359619, Total Loss 94.10346221923828\n",
      "2: Encoding Loss 11.059674263000488, Transition Loss 0.9559927582740784, Classifier Loss 0.368797242641449, Total Loss 103.62017059326172\n",
      "2: Encoding Loss 11.221075057983398, Transition Loss 0.857552170753479, Classifier Loss 0.367300808429718, Total Loss 104.39955139160156\n",
      "2: Encoding Loss 10.543665885925293, Transition Loss 0.2426477074623108, Classifier Loss 0.30015459656715393, Total Loss 93.37451934814453\n",
      "2: Encoding Loss 12.292671203613281, Transition Loss 1.7772544622421265, Classifier Loss 0.3594028353691101, Total Loss 110.40721130371094\n",
      "2: Encoding Loss 12.281750679016113, Transition Loss 2.31195330619812, Classifier Loss 0.33300843834877014, Total Loss 107.91613006591797\n",
      "2: Encoding Loss 11.3997802734375, Transition Loss -0.3692278265953064, Classifier Loss 0.2994650900363922, Total Loss 98.34504699707031\n",
      "2: Encoding Loss 10.372360229492188, Transition Loss 0.1678428053855896, Classifier Loss 0.35771000385284424, Total Loss 98.07230377197266\n",
      "2: Encoding Loss 10.010607719421387, Transition Loss 0.5522883534431458, Classifier Loss 0.35261183977127075, Total Loss 95.54574584960938\n",
      "2: Encoding Loss 11.261911392211914, Transition Loss 0.23409655690193176, Classifier Loss 0.2942090332508087, Total Loss 97.08601379394531\n",
      "2: Encoding Loss 10.8859224319458, Transition Loss 0.13678431510925293, Classifier Loss 0.27075687050819397, Total Loss 92.44593048095703\n",
      "2: Encoding Loss 12.736010551452637, Transition Loss -0.000279746949672699, Classifier Loss 0.34084072709083557, Total Loss 110.50013732910156\n",
      "2: Encoding Loss 12.601781845092773, Transition Loss 0.7604706287384033, Classifier Loss 0.32950112223625183, Total Loss 108.86499786376953\n",
      "2: Encoding Loss 11.580029487609863, Transition Loss 0.5651808381080627, Classifier Loss 0.3429000675678253, Total Loss 103.99626159667969\n",
      "2: Encoding Loss 10.453143119812012, Transition Loss 0.16737306118011475, Classifier Loss 0.31369954347610474, Total Loss 94.15576171875\n",
      "2: Encoding Loss 13.874341011047363, Transition Loss 0.21364125609397888, Classifier Loss 0.3232932984828949, Total Loss 115.66083526611328\n",
      "2: Encoding Loss 12.605181694030762, Transition Loss 1.9359831809997559, Classifier Loss 0.3898889124393463, Total Loss 115.39437866210938\n",
      "2: Encoding Loss 12.216830253601074, Transition Loss 2.6499340534210205, Classifier Loss 0.428561806678772, Total Loss 117.2171401977539\n",
      "2: Encoding Loss 11.45214557647705, Transition Loss 1.258120059967041, Classifier Loss 0.3219790458679199, Total Loss 101.41403198242188\n",
      "2: Encoding Loss 12.112105369567871, Transition Loss 0.22182047367095947, Classifier Loss 0.31815630197525024, Total Loss 104.57699584960938\n",
      "2: Encoding Loss 12.018086433410645, Transition Loss 0.6699279546737671, Classifier Loss 0.37892434000968933, Total Loss 110.26892852783203\n",
      "2: Encoding Loss 10.215815544128418, Transition Loss -0.01851850003004074, Classifier Loss 0.303657591342926, Total Loss 91.66064453125\n",
      "2: Encoding Loss 11.908658027648926, Transition Loss 0.8225376009941101, Classifier Loss 0.2797073721885681, Total Loss 99.75170135498047\n",
      "2: Encoding Loss 11.52937126159668, Transition Loss 0.40088820457458496, Classifier Loss 0.38646984100341797, Total Loss 107.98356628417969\n",
      "2: Encoding Loss 15.620577812194824, Transition Loss 2.1121256351470947, Classifier Loss 0.30257824063301086, Total Loss 124.82614135742188\n",
      "2: Encoding Loss 12.495696067810059, Transition Loss 0.1718944013118744, Classifier Loss 0.33244526386260986, Total Loss 108.28746032714844\n",
      "2: Encoding Loss 11.913501739501953, Transition Loss 0.3539680540561676, Classifier Loss 0.3235834836959839, Total Loss 103.98094177246094\n",
      "2: Encoding Loss 11.359251022338867, Transition Loss -0.05321145057678223, Classifier Loss 0.32794490456581116, Total Loss 100.9499740600586\n",
      "2: Encoding Loss 11.131011962890625, Transition Loss -0.49473249912261963, Classifier Loss 0.28399309515953064, Total Loss 95.1851806640625\n",
      "2: Encoding Loss 11.829742431640625, Transition Loss 0.7753039598464966, Classifier Loss 0.3221234679222107, Total Loss 103.50091552734375\n",
      "2: Encoding Loss 11.861478805541992, Transition Loss -0.6481468677520752, Classifier Loss 0.3064720034599304, Total Loss 101.8158187866211\n",
      "2: Encoding Loss 10.94870376586914, Transition Loss -0.3269990086555481, Classifier Loss 0.34982866048812866, Total Loss 100.67495727539062\n",
      "2: Encoding Loss 11.652196884155273, Transition Loss 0.25250130891799927, Classifier Loss 0.2967127859592438, Total Loss 99.68546295166016\n",
      "2: Encoding Loss 10.994418144226074, Transition Loss 0.17325158417224884, Classifier Loss 0.26776427030563354, Total Loss 92.81224060058594\n",
      "2: Encoding Loss 11.846619606018066, Transition Loss -0.28305214643478394, Classifier Loss 0.3199748992919922, Total Loss 103.07709503173828\n",
      "2: Encoding Loss 10.944791793823242, Transition Loss -0.05538316071033478, Classifier Loss 0.3429386615753174, Total Loss 99.96259307861328\n",
      "2: Encoding Loss 11.487655639648438, Transition Loss 0.750367283821106, Classifier Loss 0.3249605894088745, Total Loss 101.7221450805664\n",
      "2: Encoding Loss 11.46734619140625, Transition Loss -0.37490543723106384, Classifier Loss 0.37714171409606934, Total Loss 106.51809692382812\n",
      "2: Encoding Loss 11.088895797729492, Transition Loss 0.7414461970329285, Classifier Loss 0.3468759059906006, Total Loss 101.51754760742188\n",
      "2: Encoding Loss 9.14371395111084, Transition Loss -0.04419704154133797, Classifier Loss 0.28119826316833496, Total Loss 82.98210144042969\n",
      "2: Encoding Loss 13.93971061706543, Transition Loss 0.8363981246948242, Classifier Loss 0.3738671839237213, Total Loss 121.35954284667969\n",
      "2: Encoding Loss 11.001398086547852, Transition Loss -0.13187268376350403, Classifier Loss 0.32972264289855957, Total Loss 98.98059844970703\n",
      "2: Encoding Loss 12.599817276000977, Transition Loss -0.19528865814208984, Classifier Loss 0.3182053864002228, Total Loss 107.41937255859375\n",
      "2: Encoding Loss 12.437335014343262, Transition Loss -0.07937926054000854, Classifier Loss 0.3434489965438843, Total Loss 108.96888732910156\n",
      "2: Encoding Loss 10.676843643188477, Transition Loss 0.0885527953505516, Classifier Loss 0.3139398992061615, Total Loss 95.490478515625\n",
      "2: Encoding Loss 11.275171279907227, Transition Loss 0.40152138471603394, Classifier Loss 0.3080253601074219, Total Loss 98.61417388916016\n",
      "2: Encoding Loss 10.754441261291504, Transition Loss 0.3866778016090393, Classifier Loss 0.3128162622451782, Total Loss 95.96294403076172\n",
      "2: Encoding Loss 13.907737731933594, Transition Loss 0.7394042611122131, Classifier Loss 0.3549547493457794, Total Loss 119.23766326904297\n",
      "2: Encoding Loss 12.381494522094727, Transition Loss 0.9110434055328369, Classifier Loss 0.4057677686214447, Total Loss 115.23017120361328\n",
      "2: Encoding Loss 12.474985122680664, Transition Loss 0.03247576951980591, Classifier Loss 0.27842122316360474, Total Loss 102.70503234863281\n",
      "2: Encoding Loss 10.361810684204102, Transition Loss 0.1424391269683838, Classifier Loss 0.3381534516811371, Total Loss 96.04318237304688\n",
      "2: Encoding Loss 11.07923698425293, Transition Loss 0.4346311092376709, Classifier Loss 0.30200648307800293, Total Loss 96.84992218017578\n",
      "2: Encoding Loss 11.85265064239502, Transition Loss 0.7815009951591492, Classifier Loss 0.31918561458587646, Total Loss 103.3470687866211\n",
      "2: Encoding Loss 11.48237419128418, Transition Loss 1.4275723695755005, Classifier Loss 0.32512158155441284, Total Loss 101.97743225097656\n",
      "2: Encoding Loss 10.500943183898926, Transition Loss -0.3070504665374756, Classifier Loss 0.3109842538833618, Total Loss 94.10396575927734\n",
      "2: Encoding Loss 11.755044937133789, Transition Loss 0.2663324177265167, Classifier Loss 0.3280716836452484, Total Loss 103.4439697265625\n",
      "2: Encoding Loss 9.785983085632324, Transition Loss -0.4183012545108795, Classifier Loss 0.3082541227340698, Total Loss 89.54114532470703\n",
      "2: Encoding Loss 10.474469184875488, Transition Loss 0.1472853720188141, Classifier Loss 0.34790363907814026, Total Loss 97.69609069824219\n",
      "2: Encoding Loss 9.148383140563965, Transition Loss -0.441922664642334, Classifier Loss 0.3084251284599304, Total Loss 85.73263549804688\n",
      "2: Encoding Loss 11.55020809173584, Transition Loss 0.7627012133598328, Classifier Loss 0.29188674688339233, Total Loss 98.79501342773438\n",
      "2: Encoding Loss 11.225543022155762, Transition Loss 0.033848315477371216, Classifier Loss 0.29925400018692017, Total Loss 97.29220581054688\n",
      "2: Encoding Loss 10.932103157043457, Transition Loss 0.1852622628211975, Classifier Loss 0.31268933415412903, Total Loss 96.93566131591797\n",
      "2: Encoding Loss 12.17548942565918, Transition Loss 0.7254135608673096, Classifier Loss 0.33195966482162476, Total Loss 106.53907012939453\n",
      "2: Encoding Loss 10.254353523254395, Transition Loss 1.039292573928833, Classifier Loss 0.339505672454834, Total Loss 95.89241027832031\n",
      "2: Encoding Loss 10.243051528930664, Transition Loss -0.10483631491661072, Classifier Loss 0.35929200053215027, Total Loss 97.3874740600586\n",
      "2: Encoding Loss 11.639577865600586, Transition Loss 0.9589537978172302, Classifier Loss 0.3109566569328308, Total Loss 101.31671905517578\n",
      "2: Encoding Loss 9.167912483215332, Transition Loss 0.05156928300857544, Classifier Loss 0.2496263086795807, Total Loss 79.99073791503906\n",
      "2: Encoding Loss 8.7077054977417, Transition Loss -0.018264755606651306, Classifier Loss 0.32831794023513794, Total Loss 85.0780258178711\n",
      "2: Encoding Loss 9.346566200256348, Transition Loss 0.006664454936981201, Classifier Loss 0.28878670930862427, Total Loss 84.96073150634766\n",
      "2: Encoding Loss 12.72999382019043, Transition Loss 0.7757004499435425, Classifier Loss 0.3695918023586273, Total Loss 113.64942169189453\n",
      "2: Encoding Loss 11.221227645874023, Transition Loss 0.16620899736881256, Classifier Loss 0.3000519871711731, Total Loss 97.3990478515625\n",
      "2: Encoding Loss 10.348297119140625, Transition Loss 0.4258676767349243, Classifier Loss 0.2825899124145508, Total Loss 90.51912689208984\n",
      "2: Encoding Loss 14.353155136108398, Transition Loss 1.0103929042816162, Classifier Loss 0.3363129198551178, Total Loss 120.15438842773438\n",
      "2: Encoding Loss 11.791757583618164, Transition Loss 0.34059467911720276, Classifier Loss 0.3041757047176361, Total Loss 101.30435943603516\n",
      "2: Encoding Loss 11.284631729125977, Transition Loss 0.20570817589759827, Classifier Loss 0.3001464307308197, Total Loss 97.80471801757812\n",
      "2: Encoding Loss 9.665144920349121, Transition Loss -0.5234904289245605, Classifier Loss 0.2808490991592407, Total Loss 86.07557678222656\n",
      "2: Encoding Loss 11.846405982971191, Transition Loss -0.10996261984109879, Classifier Loss 0.2880690395832062, Total Loss 99.88529968261719\n",
      "2: Encoding Loss 9.913476943969727, Transition Loss -0.12657877802848816, Classifier Loss 0.3008449971675873, Total Loss 89.5653076171875\n",
      "2: Encoding Loss 11.302221298217773, Transition Loss -0.17351962625980377, Classifier Loss 0.2992021143436432, Total Loss 97.73347473144531\n",
      "2: Encoding Loss 10.991203308105469, Transition Loss 0.767776370048523, Classifier Loss 0.3538004159927368, Total Loss 101.6343765258789\n",
      "2: Encoding Loss 12.878506660461426, Transition Loss 1.0157767534255981, Classifier Loss 0.3248496651649475, Total Loss 110.16232299804688\n",
      "2: Encoding Loss 11.566584587097168, Transition Loss 0.02415892481803894, Classifier Loss 0.35999277234077454, Total Loss 105.40845489501953\n",
      "2: Encoding Loss 11.540796279907227, Transition Loss 0.6472741365432739, Classifier Loss 0.3808460533618927, Total Loss 107.58830261230469\n",
      "2: Encoding Loss 8.894417762756348, Transition Loss -0.5303823351860046, Classifier Loss 0.3328057825565338, Total Loss 86.6468734741211\n",
      "2: Encoding Loss 12.654911994934082, Transition Loss 0.421536922454834, Classifier Loss 0.3305397629737854, Total Loss 109.15206909179688\n",
      "2: Encoding Loss 12.735574722290039, Transition Loss 0.2077477127313614, Classifier Loss 0.3269067704677582, Total Loss 109.18722534179688\n",
      "2: Encoding Loss 11.254420280456543, Transition Loss -0.37187227606773376, Classifier Loss 0.3070320785045624, Total Loss 98.2295913696289\n",
      "2: Encoding Loss 14.262927055358887, Transition Loss 0.8308433294296265, Classifier Loss 0.35890740156173706, Total Loss 121.80064392089844\n",
      "2: Encoding Loss 12.05058765411377, Transition Loss 0.6704362034797668, Classifier Loss 0.335785448551178, Total Loss 106.1502456665039\n",
      "2: Encoding Loss 12.18113899230957, Transition Loss 0.3094751536846161, Classifier Loss 0.3451872169971466, Total Loss 107.7293472290039\n",
      "2: Encoding Loss 12.664900779724121, Transition Loss 0.28703129291534424, Classifier Loss 0.29367345571517944, Total Loss 105.47157287597656\n",
      "2: Encoding Loss 11.250171661376953, Transition Loss 0.23680759966373444, Classifier Loss 0.3610096871852875, Total Loss 103.69672393798828\n",
      "2: Encoding Loss 9.827230453491211, Transition Loss -0.6007373929023743, Classifier Loss 0.283598393201828, Total Loss 87.32299041748047\n",
      "2: Encoding Loss 11.803030014038086, Transition Loss 0.3148450255393982, Classifier Loss 0.3602217733860016, Total Loss 106.96630096435547\n",
      "2: Encoding Loss 11.704172134399414, Transition Loss 0.34916210174560547, Classifier Loss 0.2543792724609375, Total Loss 95.80262756347656\n",
      "2: Encoding Loss 12.995274543762207, Transition Loss 0.5210022330284119, Classifier Loss 0.3469212055206299, Total Loss 112.87217712402344\n",
      "2: Encoding Loss 12.610410690307617, Transition Loss 0.7887296080589294, Classifier Loss 0.3253433108329773, Total Loss 108.51229095458984\n",
      "2: Encoding Loss 11.308845520019531, Transition Loss -0.1251029372215271, Classifier Loss 0.29838305711746216, Total Loss 97.69132232666016\n",
      "2: Encoding Loss 8.901324272155762, Transition Loss -0.5295246839523315, Classifier Loss 0.2527206242084503, Total Loss 78.67979431152344\n",
      "2: Encoding Loss 17.277233123779297, Transition Loss 0.7270094752311707, Classifier Loss 0.3441307544708252, Total Loss 138.36727905273438\n",
      "2: Encoding Loss 14.848896980285645, Transition Loss 0.22742579877376556, Classifier Loss 0.26177892088890076, Total Loss 115.36225128173828\n",
      "2: Encoding Loss 12.199460983276367, Transition Loss 0.054051339626312256, Classifier Loss 0.32965338230133057, Total Loss 106.18373107910156\n",
      "2: Encoding Loss 12.962185859680176, Transition Loss 0.6050056219100952, Classifier Loss 0.3745473325252533, Total Loss 115.46985626220703\n",
      "2: Encoding Loss 11.449089050292969, Transition Loss 0.49613428115844727, Classifier Loss 0.314493328332901, Total Loss 100.34232330322266\n",
      "2: Encoding Loss 8.846275329589844, Transition Loss -0.10729235410690308, Classifier Loss 0.3413763642311096, Total Loss 87.21524810791016\n",
      "2: Encoding Loss 13.013822555541992, Transition Loss 0.5162801146507263, Classifier Loss 0.31932052969932556, Total Loss 110.22150421142578\n",
      "2: Encoding Loss 10.37321662902832, Transition Loss 0.3409448266029358, Classifier Loss 0.3565894663333893, Total Loss 98.03462219238281\n",
      "2: Encoding Loss 10.840238571166992, Transition Loss 0.10422878712415695, Classifier Loss 0.33800965547561646, Total Loss 98.88409423828125\n",
      "2: Encoding Loss 11.099510192871094, Transition Loss 0.39962145686149597, Classifier Loss 0.3576827645301819, Total Loss 102.52519226074219\n",
      "2: Encoding Loss 11.57712173461914, Transition Loss 0.11634919047355652, Classifier Loss 0.297617644071579, Total Loss 99.27103424072266\n",
      "2: Encoding Loss 13.312461853027344, Transition Loss 0.6227051019668579, Classifier Loss 0.30944526195526123, Total Loss 111.0683822631836\n",
      "2: Encoding Loss 11.034533500671387, Transition Loss -0.21830730140209198, Classifier Loss 0.30902883410453796, Total Loss 97.1100082397461\n",
      "2: Encoding Loss 10.564752578735352, Transition Loss 0.8023785948753357, Classifier Loss 0.34664323925971985, Total Loss 98.37379455566406\n",
      "2: Encoding Loss 10.652524948120117, Transition Loss -0.22720275819301605, Classifier Loss 0.2915302515029907, Total Loss 93.06808471679688\n",
      "2: Encoding Loss 11.364112854003906, Transition Loss 0.19804933667182922, Classifier Loss 0.3579583764076233, Total Loss 104.05973052978516\n",
      "2: Encoding Loss 9.79421329498291, Transition Loss -0.02738085202872753, Classifier Loss 0.33210453391075134, Total Loss 91.9757308959961\n",
      "2: Encoding Loss 10.62818431854248, Transition Loss -0.5529888272285461, Classifier Loss 0.32619428634643555, Total Loss 96.38831329345703\n",
      "2: Encoding Loss 12.114645004272461, Transition Loss 0.6862425804138184, Classifier Loss 0.3058677911758423, Total Loss 103.54914855957031\n",
      "2: Encoding Loss 11.242948532104492, Transition Loss 1.056686520576477, Classifier Loss 0.32933589816093445, Total Loss 100.81395721435547\n",
      "2: Encoding Loss 11.399824142456055, Transition Loss 0.4765561819076538, Classifier Loss 0.3080778121948242, Total Loss 99.39734649658203\n",
      "2: Encoding Loss 12.905881881713867, Transition Loss 1.216965913772583, Classifier Loss 0.3210821747779846, Total Loss 110.03030395507812\n",
      "2: Encoding Loss 11.388854026794434, Transition Loss 0.22823838889598846, Classifier Loss 0.3317248523235321, Total Loss 101.59690856933594\n",
      "2: Encoding Loss 10.866833686828613, Transition Loss -0.28373920917510986, Classifier Loss 0.2714933156967163, Total Loss 92.3502197265625\n",
      "2: Encoding Loss 11.189126968383789, Transition Loss -0.2444026619195938, Classifier Loss 0.30704453587532043, Total Loss 97.83911895751953\n",
      "2: Encoding Loss 12.330622673034668, Transition Loss 0.13568688929080963, Classifier Loss 0.3277705907821655, Total Loss 106.81507873535156\n",
      "2: Encoding Loss 13.42762565612793, Transition Loss 1.7822304964065552, Classifier Loss 0.37268659472465515, Total Loss 118.54730224609375\n",
      "2: Encoding Loss 12.658625602722168, Transition Loss 0.5489543676376343, Classifier Loss 0.29802387952804565, Total Loss 105.9737319946289\n",
      "2: Encoding Loss 9.477388381958008, Transition Loss -0.2311963140964508, Classifier Loss 0.30586230754852295, Total Loss 87.45046997070312\n",
      "2: Encoding Loss 11.52766227722168, Transition Loss -0.36514410376548767, Classifier Loss 0.2862647771835327, Total Loss 97.79231262207031\n",
      "2: Encoding Loss 10.746236801147461, Transition Loss 0.3853020668029785, Classifier Loss 0.267362505197525, Total Loss 91.3677978515625\n",
      "2: Encoding Loss 12.314216613769531, Transition Loss 0.7407970428466797, Classifier Loss 0.297115296125412, Total Loss 103.89315032958984\n",
      "2: Encoding Loss 10.253501892089844, Transition Loss 0.11522701382637024, Classifier Loss 0.32430145144462585, Total Loss 93.99724578857422\n",
      "2: Encoding Loss 12.431943893432617, Transition Loss 0.6933060884475708, Classifier Loss 0.3235228657722473, Total Loss 107.22127532958984\n",
      "2: Encoding Loss 12.007707595825195, Transition Loss 0.8682050108909607, Classifier Loss 0.36293578147888184, Total Loss 108.6871109008789\n",
      "2: Encoding Loss 13.339357376098633, Transition Loss 1.3593710660934448, Classifier Loss 0.34324243664741516, Total Loss 114.90414428710938\n",
      "2: Encoding Loss 11.585631370544434, Transition Loss 1.243334412574768, Classifier Loss 0.32197898626327515, Total Loss 102.20903015136719\n",
      "2: Encoding Loss 10.957805633544922, Transition Loss 0.1786496639251709, Classifier Loss 0.31503280997276306, Total Loss 97.32157135009766\n",
      "2: Encoding Loss 10.096105575561523, Transition Loss -0.16029131412506104, Classifier Loss 0.31528764963150024, Total Loss 92.10533905029297\n",
      "2: Encoding Loss 11.031987190246582, Transition Loss 0.822576642036438, Classifier Loss 0.3142453730106354, Total Loss 97.94549560546875\n",
      "2: Encoding Loss 11.319836616516113, Transition Loss 0.2614681124687195, Classifier Loss 0.3103867173194885, Total Loss 99.0622787475586\n",
      "2: Encoding Loss 10.195058822631836, Transition Loss -0.17054709792137146, Classifier Loss 0.29112952947616577, Total Loss 90.28324127197266\n",
      "2: Encoding Loss 9.645622253417969, Transition Loss 0.24339087307453156, Classifier Loss 0.3396351933479309, Total Loss 91.93461608886719\n",
      "2: Encoding Loss 9.761981010437012, Transition Loss -0.14337825775146484, Classifier Loss 0.29981034994125366, Total Loss 88.55286407470703\n",
      "2: Encoding Loss 11.030203819274902, Transition Loss -0.15618497133255005, Classifier Loss 0.33552664518356323, Total Loss 99.73383331298828\n",
      "2: Encoding Loss 11.130019187927246, Transition Loss -0.1544615626335144, Classifier Loss 0.3558693826198578, Total Loss 102.36699676513672\n",
      "2: Encoding Loss 12.730718612670898, Transition Loss 0.7910656929016113, Classifier Loss 0.29493141174316406, Total Loss 106.19388580322266\n",
      "2: Encoding Loss 9.413824081420898, Transition Loss -0.6635459065437317, Classifier Loss 0.27038049697875977, Total Loss 83.5207290649414\n",
      "2: Encoding Loss 11.71389102935791, Transition Loss 0.029193855822086334, Classifier Loss 0.30094701051712036, Total Loss 100.38973236083984\n",
      "2: Encoding Loss 10.39003849029541, Transition Loss 0.33871886134147644, Classifier Loss 0.35904330015182495, Total Loss 98.38005828857422\n",
      "2: Encoding Loss 10.183716773986816, Transition Loss -0.22659721970558167, Classifier Loss 0.28529947996139526, Total Loss 89.63215637207031\n",
      "2: Encoding Loss 10.729999542236328, Transition Loss -0.4397963583469391, Classifier Loss 0.3054264485836029, Total Loss 94.9224624633789\n",
      "2: Encoding Loss 11.743359565734863, Transition Loss 0.6657490730285645, Classifier Loss 0.35222381353378296, Total Loss 105.94883728027344\n",
      "2: Encoding Loss 10.883026123046875, Transition Loss 0.710343599319458, Classifier Loss 0.31284651160240173, Total Loss 96.866943359375\n",
      "2: Encoding Loss 10.956514358520508, Transition Loss -0.014166168868541718, Classifier Loss 0.3110155165195465, Total Loss 96.84062957763672\n",
      "2: Encoding Loss 11.486997604370117, Transition Loss 0.8954721689224243, Classifier Loss 0.324045866727829, Total Loss 101.68476867675781\n",
      "2: Encoding Loss 9.897753715515137, Transition Loss -0.11284256726503372, Classifier Loss 0.3320329189300537, Total Loss 92.58976745605469\n",
      "2: Encoding Loss 15.0914888381958, Transition Loss 1.2773178815841675, Classifier Loss 0.33319833874702454, Total Loss 124.37969970703125\n",
      "2: Encoding Loss 11.502781867980957, Transition Loss -0.9558600783348083, Classifier Loss 0.2545323967933655, Total Loss 94.46955108642578\n",
      "2: Encoding Loss 12.21220588684082, Transition Loss 0.047259896993637085, Classifier Loss 0.29413047432899475, Total Loss 102.70519256591797\n",
      "2: Encoding Loss 9.182037353515625, Transition Loss -0.6533688306808472, Classifier Loss 0.29173240065574646, Total Loss 84.26521301269531\n",
      "2: Encoding Loss 12.345288276672363, Transition Loss -0.14756153523921967, Classifier Loss 0.2880943715572357, Total Loss 102.881103515625\n",
      "2: Encoding Loss 11.692273139953613, Transition Loss 0.9093822240829468, Classifier Loss 0.36771368980407715, Total Loss 107.28876495361328\n",
      "2: Encoding Loss 11.26650619506836, Transition Loss 0.45764797925949097, Classifier Loss 0.3273213505744934, Total Loss 100.51423645019531\n",
      "2: Encoding Loss 11.04389762878418, Transition Loss -0.0740474984049797, Classifier Loss 0.2767375111579895, Total Loss 93.9371109008789\n",
      "2: Encoding Loss 10.257131576538086, Transition Loss -0.07670895755290985, Classifier Loss 0.30294278264045715, Total Loss 91.83704376220703\n",
      "2: Encoding Loss 10.191959381103516, Transition Loss -0.6956855654716492, Classifier Loss 0.2636050581932068, Total Loss 87.51199340820312\n",
      "2: Encoding Loss 9.150391578674316, Transition Loss -1.1014087200164795, Classifier Loss 0.26105934381484985, Total Loss 81.00784301757812\n",
      "2: Encoding Loss 13.165388107299805, Transition Loss 0.6437889337539673, Classifier Loss 0.31350550055503845, Total Loss 110.60039520263672\n",
      "2: Encoding Loss 11.413202285766602, Transition Loss -0.6384372115135193, Classifier Loss 0.25729483366012573, Total Loss 94.20845031738281\n",
      "2: Encoding Loss 11.5506591796875, Transition Loss 0.4771808981895447, Classifier Loss 0.3025916814804077, Total Loss 99.75399780273438\n",
      "2: Encoding Loss 12.506946563720703, Transition Loss 0.2868815064430237, Classifier Loss 0.30648162961006165, Total Loss 105.80459594726562\n",
      "2: Encoding Loss 10.463587760925293, Transition Loss -0.4344937205314636, Classifier Loss 0.3077031970024109, Total Loss 93.55167388916016\n",
      "2: Encoding Loss 9.541298866271973, Transition Loss -0.43656545877456665, Classifier Loss 0.2904895544052124, Total Loss 86.29657745361328\n",
      "2: Encoding Loss 14.038592338562012, Transition Loss 0.5231846570968628, Classifier Loss 0.28349965810775757, Total Loss 112.79080200195312\n",
      "2: Encoding Loss 11.393601417541504, Transition Loss -0.41609853506088257, Classifier Loss 0.2513411343097687, Total Loss 93.49555969238281\n",
      "2: Encoding Loss 11.051668167114258, Transition Loss 0.04761017858982086, Classifier Loss 0.2981703579425812, Total Loss 96.14608764648438\n",
      "2: Encoding Loss 9.475395202636719, Transition Loss -0.42814940214157104, Classifier Loss 0.3032475709915161, Total Loss 87.17696380615234\n",
      "2: Encoding Loss 10.637794494628906, Transition Loss 0.30886131525039673, Classifier Loss 0.2870834767818451, Total Loss 92.65866088867188\n",
      "2: Encoding Loss 10.992105484008789, Transition Loss 0.8031327128410339, Classifier Loss 0.324401319026947, Total Loss 98.71401977539062\n",
      "2: Encoding Loss 11.483560562133789, Transition Loss 0.28803402185440063, Classifier Loss 0.29367032647132874, Total Loss 98.38361358642578\n",
      "2: Encoding Loss 11.437057495117188, Transition Loss 0.6007577180862427, Classifier Loss 0.31736645102500916, Total Loss 100.59929656982422\n",
      "2: Encoding Loss 11.29525089263916, Transition Loss -0.40339070558547974, Classifier Loss 0.3006625175476074, Total Loss 97.83760070800781\n",
      "2: Encoding Loss 11.985496520996094, Transition Loss 0.2710556983947754, Classifier Loss 0.33203399181365967, Total Loss 105.22480010986328\n",
      "2: Encoding Loss 11.627685546875, Transition Loss 0.16443480551242828, Classifier Loss 0.31326478719711304, Total Loss 101.15836334228516\n",
      "2: Encoding Loss 11.146662712097168, Transition Loss 0.03589019179344177, Classifier Loss 0.29035237431526184, Total Loss 95.92958068847656\n",
      "2: Encoding Loss 11.276707649230957, Transition Loss -0.2223370373249054, Classifier Loss 0.26421892642974854, Total Loss 94.08204650878906\n",
      "2: Encoding Loss 11.338695526123047, Transition Loss -0.4195117950439453, Classifier Loss 0.26314300298690796, Total Loss 94.34630584716797\n",
      "2: Encoding Loss 9.386777877807617, Transition Loss -0.6904541850090027, Classifier Loss 0.25085362792015076, Total Loss 81.40576171875\n",
      "2: Encoding Loss 12.608346939086914, Transition Loss 0.5648307800292969, Classifier Loss 0.3669830560684204, Total Loss 112.5743179321289\n",
      "2: Encoding Loss 11.349483489990234, Transition Loss -0.6374940872192383, Classifier Loss 0.2550680935382843, Total Loss 93.60346221923828\n",
      "2: Encoding Loss 10.369440078735352, Transition Loss -0.17550057172775269, Classifier Loss 0.2956335246562958, Total Loss 91.77992248535156\n",
      "2: Encoding Loss 9.870864868164062, Transition Loss -0.409534752368927, Classifier Loss 0.2933051884174347, Total Loss 88.55554962158203\n",
      "2: Encoding Loss 13.421049118041992, Transition Loss 0.05470168963074684, Classifier Loss 0.271714985370636, Total Loss 107.71968078613281\n",
      "2: Encoding Loss 12.421819686889648, Transition Loss 0.32874828577041626, Classifier Loss 0.281577467918396, Total Loss 102.8201675415039\n",
      "2: Encoding Loss 10.996197700500488, Transition Loss -0.09325556457042694, Classifier Loss 0.3277176320552826, Total Loss 98.74890899658203\n",
      "2: Encoding Loss 10.596120834350586, Transition Loss -0.2890697717666626, Classifier Loss 0.2913314402103424, Total Loss 92.70975494384766\n",
      "2: Encoding Loss 12.041730880737305, Transition Loss -0.09031569957733154, Classifier Loss 0.2823461890220642, Total Loss 100.48497009277344\n",
      "2: Encoding Loss 11.966756820678711, Transition Loss 0.5313929319381714, Classifier Loss 0.2999500334262848, Total Loss 102.00810241699219\n",
      "2: Encoding Loss 12.628884315490723, Transition Loss 0.8156289458274841, Classifier Loss 0.3079366981983185, Total Loss 106.89322662353516\n",
      "2: Encoding Loss 10.442543029785156, Transition Loss -0.15304706990718842, Classifier Loss 0.3311265707015991, Total Loss 95.76785278320312\n",
      "2: Encoding Loss 11.79538631439209, Transition Loss -0.46531981229782104, Classifier Loss 0.3072830140590668, Total Loss 101.50044250488281\n",
      "2: Encoding Loss 8.54365348815918, Transition Loss -0.6744743585586548, Classifier Loss 0.28746795654296875, Total Loss 80.00845336914062\n",
      "2: Encoding Loss 12.544215202331543, Transition Loss 0.3566196858882904, Classifier Loss 0.3282594680786133, Total Loss 108.23389434814453\n",
      "2: Encoding Loss 10.842084884643555, Transition Loss -0.07989159226417542, Classifier Loss 0.29364511370658875, Total Loss 94.4169921875\n",
      "2: Encoding Loss 10.78443717956543, Transition Loss -0.26046913862228394, Classifier Loss 0.29848840832710266, Total Loss 94.55535888671875\n",
      "2: Encoding Loss 10.083346366882324, Transition Loss -0.411257803440094, Classifier Loss 0.27618125081062317, Total Loss 88.1180419921875\n",
      "2: Encoding Loss 13.584419250488281, Transition Loss 0.5086597800254822, Classifier Loss 0.3027697205543518, Total Loss 111.98694610595703\n",
      "2: Encoding Loss 12.052302360534668, Transition Loss -0.5161334872245789, Classifier Loss 0.2846096158027649, Total Loss 100.77457427978516\n",
      "2: Encoding Loss 11.205511093139648, Transition Loss 0.3235856592655182, Classifier Loss 0.3037354648113251, Total Loss 97.73604583740234\n",
      "2: Encoding Loss 12.044906616210938, Transition Loss -0.27422505617141724, Classifier Loss 0.28876760601997375, Total Loss 101.1460952758789\n",
      "2: Encoding Loss 10.57419490814209, Transition Loss -0.33905404806137085, Classifier Loss 0.32428085803985596, Total Loss 95.87312316894531\n",
      "2: Encoding Loss 9.922995567321777, Transition Loss 0.08369826525449753, Classifier Loss 0.2738586664199829, Total Loss 86.95732116699219\n",
      "2: Encoding Loss 10.236700057983398, Transition Loss -0.054750412702560425, Classifier Loss 0.31799134612083435, Total Loss 93.21931457519531\n",
      "2: Encoding Loss 11.289913177490234, Transition Loss -0.06961828470230103, Classifier Loss 0.25064706802368164, Total Loss 92.80415344238281\n",
      "2: Encoding Loss 10.420199394226074, Transition Loss -0.3961288332939148, Classifier Loss 0.22942325472831726, Total Loss 85.46336364746094\n",
      "2: Encoding Loss 12.366336822509766, Transition Loss 0.07713266462087631, Classifier Loss 0.2721903920173645, Total Loss 101.44791412353516\n",
      "2: Encoding Loss 12.507181167602539, Transition Loss -0.08583849668502808, Classifier Loss 0.3029789328575134, Total Loss 105.34095001220703\n",
      "2: Encoding Loss 9.583826065063477, Transition Loss -0.6943694949150085, Classifier Loss 0.24452897906303406, Total Loss 81.95558166503906\n",
      "2: Encoding Loss 11.066622734069824, Transition Loss -0.1941424161195755, Classifier Loss 0.25718367099761963, Total Loss 92.11803436279297\n",
      "2: Encoding Loss 11.729101181030273, Transition Loss 1.0016213655471802, Classifier Loss 0.2989599406719208, Total Loss 100.67125701904297\n",
      "2: Encoding Loss 9.593350410461426, Transition Loss 0.022772785276174545, Classifier Loss 0.29885387420654297, Total Loss 87.45460510253906\n",
      "2: Encoding Loss 12.380258560180664, Transition Loss 0.20430585741996765, Classifier Loss 0.27872124314308167, Total Loss 102.23540496826172\n",
      "2: Encoding Loss 10.435457229614258, Transition Loss -0.5432761311531067, Classifier Loss 0.34446313977241516, Total Loss 97.05884552001953\n",
      "2: Encoding Loss 7.939774990081787, Transition Loss -0.7069956660270691, Classifier Loss 0.27349698543548584, Total Loss 74.98806762695312\n",
      "2: Encoding Loss 11.812837600708008, Transition Loss -0.16923251748085022, Classifier Loss 0.2563896179199219, Total Loss 96.51592254638672\n",
      "2: Encoding Loss 10.51471996307373, Transition Loss -0.31527745723724365, Classifier Loss 0.2978658974170685, Total Loss 92.8747787475586\n",
      "2: Encoding Loss 9.83643913269043, Transition Loss -0.13378165662288666, Classifier Loss 0.2760522961616516, Total Loss 86.62381744384766\n",
      "2: Encoding Loss 9.400812149047852, Transition Loss -0.3077980875968933, Classifier Loss 0.24666398763656616, Total Loss 81.07115173339844\n",
      "2: Encoding Loss 10.134636878967285, Transition Loss -0.3632461130619049, Classifier Loss 0.227395161986351, Total Loss 83.54719543457031\n",
      "2: Encoding Loss 12.482961654663086, Transition Loss 0.5473081469535828, Classifier Loss 0.28960809111595154, Total Loss 104.07750701904297\n",
      "2: Encoding Loss 12.350048065185547, Transition Loss 0.24732372164726257, Classifier Loss 0.3147556781768799, Total Loss 105.6747817993164\n",
      "2: Encoding Loss 9.966700553894043, Transition Loss -1.1035877466201782, Classifier Loss 0.253701388835907, Total Loss 85.1698989868164\n",
      "2: Encoding Loss 11.885295867919922, Transition Loss 0.9560868144035339, Classifier Loss 0.3534988760948181, Total Loss 107.04409790039062\n",
      "2: Encoding Loss 11.034774780273438, Transition Loss -0.4677678942680359, Classifier Loss 0.2796434164047241, Total Loss 94.17279815673828\n",
      "2: Encoding Loss 10.009105682373047, Transition Loss 0.04891306161880493, Classifier Loss 0.34807878732681274, Total Loss 94.882080078125\n",
      "2: Encoding Loss 8.907001495361328, Transition Loss -0.18127873539924622, Classifier Loss 0.2588191032409668, Total Loss 79.32384490966797\n",
      "2: Encoding Loss 11.033483505249023, Transition Loss -0.3550606369972229, Classifier Loss 0.2510387897491455, Total Loss 91.30463409423828\n",
      "2: Encoding Loss 10.663958549499512, Transition Loss -0.227760910987854, Classifier Loss 0.35946154594421387, Total Loss 99.92981719970703\n",
      "2: Encoding Loss 10.826108932495117, Transition Loss -0.10546614974737167, Classifier Loss 0.3297181725502014, Total Loss 97.92842864990234\n",
      "2: Encoding Loss 10.71049690246582, Transition Loss -0.37728849053382874, Classifier Loss 0.31167057156562805, Total Loss 95.42988586425781\n",
      "2: Encoding Loss 11.776565551757812, Transition Loss -0.1802697479724884, Classifier Loss 0.2791703939437866, Total Loss 98.57636260986328\n",
      "2: Encoding Loss 13.542078971862793, Transition Loss 0.39120712876319885, Classifier Loss 0.2424110323190689, Total Loss 105.65007019042969\n",
      "2: Encoding Loss 10.980589866638184, Transition Loss 0.004205532371997833, Classifier Loss 0.29972952604293823, Total Loss 95.8581771850586\n",
      "2: Encoding Loss 11.726045608520508, Transition Loss 0.6919816136360168, Classifier Loss 0.314401239156723, Total Loss 102.07319641113281\n",
      "2: Encoding Loss 13.089343070983887, Transition Loss 0.2652937173843384, Classifier Loss 0.2548680603504181, Total Loss 104.12899017333984\n",
      "2: Encoding Loss 9.576736450195312, Transition Loss -0.46732038259506226, Classifier Loss 0.2964189350605011, Total Loss 87.10212707519531\n",
      "2: Encoding Loss 8.775157928466797, Transition Loss -0.7673029899597168, Classifier Loss 0.2449740767478943, Total Loss 77.14805603027344\n",
      "2: Encoding Loss 12.055181503295898, Transition Loss 0.2714932858943939, Classifier Loss 0.29568737745285034, Total Loss 102.0084228515625\n",
      "2: Encoding Loss 10.90053939819336, Transition Loss -0.3934869170188904, Classifier Loss 0.28361308574676514, Total Loss 93.7643814086914\n",
      "2: Encoding Loss 11.497390747070312, Transition Loss 0.12662723660469055, Classifier Loss 0.2717270255088806, Total Loss 96.20769500732422\n",
      "2: Encoding Loss 10.400128364562988, Transition Loss -0.3491984009742737, Classifier Loss 0.2957950234413147, Total Loss 91.98013305664062\n",
      "2: Encoding Loss 11.691946983337402, Transition Loss -0.8799207210540771, Classifier Loss 0.22237710654735565, Total Loss 92.38904571533203\n",
      "2: Encoding Loss 10.552213668823242, Transition Loss -0.2724187672138214, Classifier Loss 0.24673160910606384, Total Loss 87.98634338378906\n",
      "2: Encoding Loss 11.73084831237793, Transition Loss 0.0028167366981506348, Classifier Loss 0.2408115416765213, Total Loss 94.46737670898438\n",
      "2: Encoding Loss 10.539237976074219, Transition Loss -0.3493680953979492, Classifier Loss 0.26895931363105774, Total Loss 90.1312255859375\n",
      "2: Encoding Loss 10.688136100769043, Transition Loss -0.4255755543708801, Classifier Loss 0.26317304372787476, Total Loss 90.44596099853516\n",
      "2: Encoding Loss 11.740213394165039, Transition Loss -0.6499381065368652, Classifier Loss 0.25125619769096375, Total Loss 95.56664276123047\n",
      "2: Encoding Loss 11.439640998840332, Transition Loss 0.10964010655879974, Classifier Loss 0.30003660917282104, Total Loss 98.68536376953125\n",
      "2: Encoding Loss 11.256120681762695, Transition Loss 0.11216463148593903, Classifier Loss 0.28124886751174927, Total Loss 95.70648193359375\n",
      "2: Encoding Loss 11.301179885864258, Transition Loss -0.44494757056236267, Classifier Loss 0.24324679374694824, Total Loss 92.13158416748047\n",
      "2: Encoding Loss 13.74200439453125, Transition Loss 0.42478078603744507, Classifier Loss 0.2612273395061493, Total Loss 108.74467468261719\n",
      "2: Encoding Loss 11.47520637512207, Transition Loss -0.6941721439361572, Classifier Loss 0.2978082597255707, Total Loss 98.63179016113281\n",
      "2: Encoding Loss 10.40410041809082, Transition Loss 0.21440744400024414, Classifier Loss 0.25096651911735535, Total Loss 87.60701751708984\n",
      "2: Encoding Loss 10.770761489868164, Transition Loss 0.14695176482200623, Classifier Loss 0.28164952993392944, Total Loss 92.84831237792969\n",
      "2: Encoding Loss 9.622472763061523, Transition Loss -0.4584595263004303, Classifier Loss 0.22883747518062592, Total Loss 80.618408203125\n",
      "2: Encoding Loss 10.33574390411377, Transition Loss -0.16652826964855194, Classifier Loss 0.26552489399909973, Total Loss 88.56688690185547\n",
      "2: Encoding Loss 9.68885612487793, Transition Loss -0.7449514865875244, Classifier Loss 0.2902868688106537, Total Loss 87.16152954101562\n",
      "2: Encoding Loss 11.856412887573242, Transition Loss 0.15924367308616638, Classifier Loss 0.3007250130176544, Total Loss 101.2746810913086\n",
      "2: Encoding Loss 10.969911575317383, Transition Loss 0.22288000583648682, Classifier Loss 0.2995075285434723, Total Loss 95.859375\n",
      "2: Encoding Loss 10.981295585632324, Transition Loss 0.2873072624206543, Classifier Loss 0.2451910674571991, Total Loss 90.52180480957031\n",
      "2: Encoding Loss 12.88567066192627, Transition Loss 0.743740439414978, Classifier Loss 0.26614969968795776, Total Loss 104.22648620605469\n",
      "2: Encoding Loss 10.211967468261719, Transition Loss -0.41608744859695435, Classifier Loss 0.3047713339328766, Total Loss 91.74877166748047\n",
      "2: Encoding Loss 10.497493743896484, Transition Loss 0.3341664671897888, Classifier Loss 0.23630620539188385, Total Loss 86.74925231933594\n",
      "2: Encoding Loss 10.167464256286621, Transition Loss -0.3814866542816162, Classifier Loss 0.29668617248535156, Total Loss 90.67324829101562\n",
      "2: Encoding Loss 12.191530227661133, Transition Loss -0.008021354675292969, Classifier Loss 0.3048664927482605, Total Loss 103.63583374023438\n",
      "2: Encoding Loss 10.54942798614502, Transition Loss 0.021823927760124207, Classifier Loss 0.33997541666030884, Total Loss 97.30284118652344\n",
      "2: Encoding Loss 11.134655952453613, Transition Loss 0.09659267216920853, Classifier Loss 0.3367089331150055, Total Loss 100.51747131347656\n",
      "2: Encoding Loss 11.545489311218262, Transition Loss 0.07042869180440903, Classifier Loss 0.26116839051246643, Total Loss 95.4179458618164\n",
      "2: Encoding Loss 10.887927055358887, Transition Loss -0.4134007692337036, Classifier Loss 0.28647321462631226, Total Loss 93.97472381591797\n",
      "2: Encoding Loss 10.668193817138672, Transition Loss 0.13377425074577332, Classifier Loss 0.305415540933609, Total Loss 94.60423278808594\n",
      "2: Encoding Loss 9.781675338745117, Transition Loss -0.8031493425369263, Classifier Loss 0.2985720634460449, Total Loss 88.54694366455078\n",
      "2: Encoding Loss 10.989771842956543, Transition Loss -0.29076480865478516, Classifier Loss 0.25358596444129944, Total Loss 91.297119140625\n",
      "2: Encoding Loss 11.43153190612793, Transition Loss -0.28073906898498535, Classifier Loss 0.27158185839653015, Total Loss 95.74726867675781\n",
      "2: Encoding Loss 10.905984878540039, Transition Loss -0.20952573418617249, Classifier Loss 0.24475404620170593, Total Loss 89.9112319946289\n",
      "2: Encoding Loss 9.606420516967773, Transition Loss 0.1748078167438507, Classifier Loss 0.30563366413116455, Total Loss 88.27181243896484\n",
      "2: Encoding Loss 10.424417495727539, Transition Loss -0.4160342216491699, Classifier Loss 0.27411702275276184, Total Loss 89.95803833007812\n",
      "2: Encoding Loss 8.554348945617676, Transition Loss -0.8926985263824463, Classifier Loss 0.2496253252029419, Total Loss 76.28826904296875\n",
      "2: Encoding Loss 12.132914543151855, Transition Loss -0.7974587082862854, Classifier Loss 0.31297969818115234, Total Loss 104.09513854980469\n",
      "2: Encoding Loss 10.446059226989746, Transition Loss -0.49656054377555847, Classifier Loss 0.24711346626281738, Total Loss 87.38750457763672\n",
      "2: Encoding Loss 9.06707763671875, Transition Loss -0.8640956878662109, Classifier Loss 0.31534290313720703, Total Loss 85.93641662597656\n",
      "2: Encoding Loss 9.12629508972168, Transition Loss -0.3568069338798523, Classifier Loss 0.27862074971199036, Total Loss 82.61970520019531\n",
      "2: Encoding Loss 10.906170845031738, Transition Loss -0.7966535687446594, Classifier Loss 0.24150323867797852, Total Loss 89.58702850341797\n",
      "2: Encoding Loss 11.480097770690918, Transition Loss -0.12049620598554611, Classifier Loss 0.2778034210205078, Total Loss 96.660888671875\n",
      "2: Encoding Loss 9.307703018188477, Transition Loss -0.3586322069168091, Classifier Loss 0.2549954652786255, Total Loss 81.34561920166016\n",
      "2: Encoding Loss 11.004108428955078, Transition Loss -0.5810970067977905, Classifier Loss 0.2832663655281067, Total Loss 94.35105895996094\n",
      "2: Encoding Loss 10.569125175476074, Transition Loss -0.223704531788826, Classifier Loss 0.26712679862976074, Total Loss 90.1273422241211\n",
      "2: Encoding Loss 8.907435417175293, Transition Loss -0.7850985527038574, Classifier Loss 0.2712532579898834, Total Loss 80.56962585449219\n",
      "2: Encoding Loss 12.262083053588867, Transition Loss -0.2617073059082031, Classifier Loss 0.24702179431915283, Total Loss 98.27457427978516\n",
      "2: Encoding Loss 10.640241622924805, Transition Loss -0.4311017096042633, Classifier Loss 0.2615877687931061, Total Loss 90.00005340576172\n",
      "2: Encoding Loss 7.517672061920166, Transition Loss -0.7429210543632507, Classifier Loss 0.2469896376132965, Total Loss 69.80469512939453\n",
      "2: Encoding Loss 7.9926652908325195, Transition Loss -0.8394672870635986, Classifier Loss 0.25432443618774414, Total Loss 73.38809967041016\n",
      "2: Encoding Loss 9.933725357055664, Transition Loss -0.65682053565979, Classifier Loss 0.27778011560440063, Total Loss 87.3801040649414\n",
      "2: Encoding Loss 9.974875450134277, Transition Loss -0.1497543603181839, Classifier Loss 0.29112547636032104, Total Loss 88.96173858642578\n",
      "2: Encoding Loss 10.60351276397705, Transition Loss -0.5419677495956421, Classifier Loss 0.3071313798427582, Total Loss 94.33399963378906\n",
      "2: Encoding Loss 11.441495895385742, Transition Loss -0.2803077697753906, Classifier Loss 0.23465462028980255, Total Loss 92.11432647705078\n",
      "2: Encoding Loss 10.727209091186523, Transition Loss -0.49550819396972656, Classifier Loss 0.23406556248664856, Total Loss 87.76961517333984\n",
      "2: Encoding Loss 10.878371238708496, Transition Loss 0.04608154296875, Classifier Loss 0.260648638010025, Total Loss 91.35353088378906\n",
      "2: Encoding Loss 9.649806022644043, Transition Loss -0.6259132623672485, Classifier Loss 0.24693894386291504, Total Loss 82.59247589111328\n",
      "2: Encoding Loss 11.328215599060059, Transition Loss 0.0840451717376709, Classifier Loss 0.2542223334312439, Total Loss 93.4251480102539\n",
      "2: Encoding Loss 11.798791885375977, Transition Loss -0.3994850218296051, Classifier Loss 0.3103027045726776, Total Loss 101.82286834716797\n",
      "2: Encoding Loss 10.592931747436523, Transition Loss -1.0580006837844849, Classifier Loss 0.2482932209968567, Total Loss 88.3864974975586\n",
      "2: Encoding Loss 11.18004035949707, Transition Loss -0.03283382207155228, Classifier Loss 0.2748441994190216, Total Loss 94.56465148925781\n",
      "2: Encoding Loss 9.84216022491455, Transition Loss -0.31801867485046387, Classifier Loss 0.23851868510246277, Total Loss 82.90470123291016\n",
      "2: Encoding Loss 11.376761436462402, Transition Loss 0.20014271140098572, Classifier Loss 0.331792414188385, Total Loss 101.5198745727539\n",
      "2: Encoding Loss 10.776540756225586, Transition Loss -0.7440952658653259, Classifier Loss 0.25741106271743774, Total Loss 90.40005493164062\n",
      "2: Encoding Loss 9.02670669555664, Transition Loss -0.12896418571472168, Classifier Loss 0.34029409289360046, Total Loss 88.1895980834961\n",
      "2: Encoding Loss 11.082000732421875, Transition Loss 0.023732341825962067, Classifier Loss 0.33768075704574585, Total Loss 100.26956939697266\n",
      "2: Encoding Loss 9.71960163116455, Transition Loss -0.735296905040741, Classifier Loss 0.2929548919200897, Total Loss 87.61280059814453\n",
      "2: Encoding Loss 11.738241195678711, Transition Loss -0.39646750688552856, Classifier Loss 0.27438846230506897, Total Loss 97.8681411743164\n",
      "2: Encoding Loss 10.605592727661133, Transition Loss -0.8173888325691223, Classifier Loss 0.30890852212905884, Total Loss 94.52407836914062\n",
      "2: Encoding Loss 9.98978328704834, Transition Loss -0.6562765836715698, Classifier Loss 0.28872594237327576, Total Loss 88.81103515625\n",
      "2: Encoding Loss 11.632238388061523, Transition Loss -0.3108177185058594, Classifier Loss 0.3072429597377777, Total Loss 100.51760864257812\n",
      "2: Encoding Loss 11.38566780090332, Transition Loss 0.12916721403598785, Classifier Loss 0.29629001021385193, Total Loss 97.99467468261719\n",
      "2: Encoding Loss 10.332330703735352, Transition Loss -0.42765432596206665, Classifier Loss 0.24355123937129974, Total Loss 86.34893798828125\n",
      "2: Encoding Loss 10.568907737731934, Transition Loss -0.1831451654434204, Classifier Loss 0.2605017423629761, Total Loss 89.46354675292969\n",
      "2: Encoding Loss 10.040226936340332, Transition Loss -1.0334562063217163, Classifier Loss 0.26786884665489197, Total Loss 87.02783203125\n",
      "2: Encoding Loss 9.535172462463379, Transition Loss -1.1088590621948242, Classifier Loss 0.26510295271873474, Total Loss 83.72088623046875\n",
      "2: Encoding Loss 10.056798934936523, Transition Loss -0.020619213581085205, Classifier Loss 0.26395660638809204, Total Loss 86.7364501953125\n",
      "2: Encoding Loss 9.950826644897461, Transition Loss -0.7238781452178955, Classifier Loss 0.2574208974838257, Total Loss 85.44676208496094\n",
      "2: Encoding Loss 10.633774757385254, Transition Loss -0.12054570019245148, Classifier Loss 0.29394373297691345, Total Loss 93.19697570800781\n",
      "2: Encoding Loss 9.905901908874512, Transition Loss -0.5931700468063354, Classifier Loss 0.2789858877658844, Total Loss 87.3337631225586\n",
      "2: Encoding Loss 12.459161758422852, Transition Loss -0.2265489250421524, Classifier Loss 0.2980971038341522, Total Loss 104.56459045410156\n",
      "2: Encoding Loss 10.773822784423828, Transition Loss -0.9034308195114136, Classifier Loss 0.2859981656074524, Total Loss 93.24239349365234\n",
      "2: Encoding Loss 9.407228469848633, Transition Loss -0.5394696593284607, Classifier Loss 0.26890432834625244, Total Loss 83.33358764648438\n",
      "2: Encoding Loss 11.129514694213867, Transition Loss -0.06538162380456924, Classifier Loss 0.24695974588394165, Total Loss 91.4730453491211\n",
      "2: Encoding Loss 11.35884952545166, Transition Loss -0.6506876349449158, Classifier Loss 0.25957441329956055, Total Loss 94.11028289794922\n",
      "2: Encoding Loss 10.389898300170898, Transition Loss -0.6682497262954712, Classifier Loss 0.2453000694513321, Total Loss 86.86913299560547\n",
      "2: Encoding Loss 9.433131217956543, Transition Loss -0.800463855266571, Classifier Loss 0.24635717272758484, Total Loss 81.23418426513672\n",
      "2: Encoding Loss 9.587066650390625, Transition Loss -0.5486752986907959, Classifier Loss 0.3291592001914978, Total Loss 90.43810272216797\n",
      "2: Encoding Loss 10.78923225402832, Transition Loss 0.09095825254917145, Classifier Loss 0.3101392090320587, Total Loss 95.78570556640625\n",
      "2: Encoding Loss 11.261758804321289, Transition Loss -0.35890838503837585, Classifier Loss 0.2648698687553406, Total Loss 94.0573959350586\n",
      "2: Encoding Loss 10.078229904174805, Transition Loss -0.8237983584403992, Classifier Loss 0.25882360339164734, Total Loss 86.3514175415039\n",
      "2: Encoding Loss 7.937928676605225, Transition Loss -1.009727954864502, Classifier Loss 0.2635645866394043, Total Loss 73.98362731933594\n",
      "2: Encoding Loss 12.071001052856445, Transition Loss -0.6854256391525269, Classifier Loss 0.2769908905029297, Total Loss 100.12482452392578\n",
      "2: Encoding Loss 9.625187873840332, Transition Loss -1.3527570962905884, Classifier Loss 0.20940405130386353, Total Loss 78.69099426269531\n",
      "2: Encoding Loss 8.806315422058105, Transition Loss -0.6099107265472412, Classifier Loss 0.2630133032798767, Total Loss 79.13897705078125\n",
      "2: Encoding Loss 8.653135299682617, Transition Loss -0.8320794105529785, Classifier Loss 0.25692126154899597, Total Loss 77.61060333251953\n",
      "2: Encoding Loss 9.181940078735352, Transition Loss -0.9556418657302856, Classifier Loss 0.24476727843284607, Total Loss 79.56798553466797\n",
      "2: Encoding Loss 11.00882625579834, Transition Loss -0.4803171753883362, Classifier Loss 0.26634150743484497, Total Loss 92.68692779541016\n",
      "2: Encoding Loss 10.835509300231934, Transition Loss -0.01109849289059639, Classifier Loss 0.26460474729537964, Total Loss 91.47352600097656\n",
      "2: Encoding Loss 8.547621726989746, Transition Loss -0.4870245158672333, Classifier Loss 0.2683528959751129, Total Loss 78.12081909179688\n",
      "2: Encoding Loss 11.512025833129883, Transition Loss -0.6605333089828491, Classifier Loss 0.2828163504600525, Total Loss 97.35352325439453\n",
      "2: Encoding Loss 10.480158805847168, Transition Loss -0.7615249752998352, Classifier Loss 0.26055270433425903, Total Loss 88.93592071533203\n",
      "2: Encoding Loss 9.057421684265137, Transition Loss -0.8640259504318237, Classifier Loss 0.2513587772846222, Total Loss 79.48006439208984\n",
      "2: Encoding Loss 10.038459777832031, Transition Loss -0.21677206456661224, Classifier Loss 0.3101528584957123, Total Loss 91.24596405029297\n",
      "2: Encoding Loss 8.805459976196289, Transition Loss -0.6766976118087769, Classifier Loss 0.26182812452316284, Total Loss 79.01530456542969\n",
      "2: Encoding Loss 8.163116455078125, Transition Loss -1.2527713775634766, Classifier Loss 0.21375331282615662, Total Loss 70.35353088378906\n",
      "2: Encoding Loss 10.987505912780762, Transition Loss -0.4143797755241394, Classifier Loss 0.25514236092567444, Total Loss 91.4391098022461\n",
      "2: Encoding Loss 9.66222858428955, Transition Loss -0.3311637043952942, Classifier Loss 0.3091219365596771, Total Loss 88.88543701171875\n",
      "2: Encoding Loss 9.267852783203125, Transition Loss -1.132825493812561, Classifier Loss 0.18920303881168365, Total Loss 74.52696990966797\n",
      "2: Encoding Loss 8.940858840942383, Transition Loss -1.090631127357483, Classifier Loss 0.2266889065504074, Total Loss 76.31360626220703\n",
      "2: Encoding Loss 10.269266128540039, Transition Loss -0.3277202248573303, Classifier Loss 0.30509284138679504, Total Loss 92.12474822998047\n",
      "2: Encoding Loss 10.735666275024414, Transition Loss -0.31670430302619934, Classifier Loss 0.2874210774898529, Total Loss 93.1559829711914\n",
      "2: Encoding Loss 8.868062019348145, Transition Loss -0.7880134582519531, Classifier Loss 0.2792629599571228, Total Loss 81.13436126708984\n",
      "2: Encoding Loss 6.823922157287598, Transition Loss -0.8937979936599731, Classifier Loss 0.22189393639564514, Total Loss 63.132568359375\n",
      "2: Encoding Loss 7.787672519683838, Transition Loss -1.0334960222244263, Classifier Loss 0.21092374622821808, Total Loss 67.81800079345703\n",
      "2: Encoding Loss 12.130940437316895, Transition Loss -0.6286736130714417, Classifier Loss 0.3015367388725281, Total Loss 102.9390640258789\n",
      "2: Encoding Loss 11.781562805175781, Transition Loss -0.5670604705810547, Classifier Loss 0.29954928159713745, Total Loss 100.64407348632812\n",
      "2: Encoding Loss 10.568479537963867, Transition Loss -0.17639487981796265, Classifier Loss 0.293220192193985, Total Loss 92.73283386230469\n",
      "2: Encoding Loss 10.380956649780273, Transition Loss -0.88204026222229, Classifier Loss 0.25818219780921936, Total Loss 88.1036148071289\n",
      "2: Encoding Loss 11.063591003417969, Transition Loss -0.11626043915748596, Classifier Loss 0.27734076976776123, Total Loss 94.1155776977539\n",
      "2: Encoding Loss 9.743200302124023, Transition Loss -0.8138095140457153, Classifier Loss 0.24202263355255127, Total Loss 82.66114044189453\n",
      "2: Encoding Loss 10.03342342376709, Transition Loss -0.6174619793891907, Classifier Loss 0.2222309112548828, Total Loss 82.42338562011719\n",
      "2: Encoding Loss 8.863545417785645, Transition Loss -0.403352290391922, Classifier Loss 0.24743027985095978, Total Loss 77.92414093017578\n",
      "2: Encoding Loss 11.918375015258789, Transition Loss -0.43499210476875305, Classifier Loss 0.2431766390800476, Total Loss 95.82774353027344\n",
      "2: Encoding Loss 12.091312408447266, Transition Loss 0.7298475503921509, Classifier Loss 0.3138990104198456, Total Loss 104.2297134399414\n",
      "2: Encoding Loss 9.779550552368164, Transition Loss -0.5141891837120056, Classifier Loss 0.2615983784198761, Total Loss 84.8369369506836\n",
      "2: Encoding Loss 11.792695999145508, Transition Loss -0.1454840898513794, Classifier Loss 0.3008001744747162, Total Loss 100.83613586425781\n",
      "2: Encoding Loss 10.037721633911133, Transition Loss -1.1054084300994873, Classifier Loss 0.22729726135730743, Total Loss 82.95561218261719\n",
      "2: Encoding Loss 9.287324905395508, Transition Loss -0.682350754737854, Classifier Loss 0.30040913820266724, Total Loss 85.76458740234375\n",
      "2: Encoding Loss 9.743131637573242, Transition Loss -0.4846496880054474, Classifier Loss 0.23503032326698303, Total Loss 81.96163177490234\n",
      "2: Encoding Loss 12.1318941116333, Transition Loss 0.1894122064113617, Classifier Loss 0.27038171887397766, Total Loss 99.90530395507812\n",
      "2: Encoding Loss 9.910898208618164, Transition Loss -0.44233864545822144, Classifier Loss 0.24331128597259521, Total Loss 83.79634094238281\n",
      "2: Encoding Loss 8.191387176513672, Transition Loss -0.7930997014045715, Classifier Loss 0.24972155690193176, Total Loss 74.12016296386719\n",
      "2: Encoding Loss 13.15381145477295, Transition Loss -0.44252079725265503, Classifier Loss 0.27304187417030334, Total Loss 106.22689056396484\n",
      "2: Encoding Loss 11.76564884185791, Transition Loss -0.6645990610122681, Classifier Loss 0.24848103523254395, Total Loss 95.44173431396484\n",
      "2: Encoding Loss 9.953115463256836, Transition Loss -1.3484532833099365, Classifier Loss 0.23273779451847076, Total Loss 82.99193572998047\n",
      "2: Encoding Loss 10.705554962158203, Transition Loss -0.17958664894104004, Classifier Loss 0.24730849266052246, Total Loss 88.964111328125\n",
      "2: Encoding Loss 10.674601554870605, Transition Loss -0.6805893182754517, Classifier Loss 0.30195096135139465, Total Loss 94.24243927001953\n",
      "2: Encoding Loss 10.415193557739258, Transition Loss -0.18512660264968872, Classifier Loss 0.3037072718143463, Total Loss 92.86181640625\n",
      "2: Encoding Loss 10.148982048034668, Transition Loss -0.588222324848175, Classifier Loss 0.17871199548244476, Total Loss 78.76485443115234\n",
      "2: Encoding Loss 8.003702163696289, Transition Loss -0.881901741027832, Classifier Loss 0.2511965334415436, Total Loss 73.14151763916016\n",
      "2: Encoding Loss 11.315900802612305, Transition Loss -0.9556513428688049, Classifier Loss 0.21347810328006744, Total Loss 89.24283599853516\n",
      "2: Encoding Loss 9.381564140319824, Transition Loss -1.1278314590454102, Classifier Loss 0.24570231139659882, Total Loss 80.85916900634766\n",
      "2: Encoding Loss 11.078993797302246, Transition Loss -0.7417227029800415, Classifier Loss 0.2322782427072525, Total Loss 89.70149993896484\n",
      "2: Encoding Loss 10.547894477844238, Transition Loss -0.5075178742408752, Classifier Loss 0.2713576555252075, Total Loss 90.42292785644531\n",
      "2: Encoding Loss 9.342960357666016, Transition Loss -0.6408237218856812, Classifier Loss 0.2358330339193344, Total Loss 79.64080810546875\n",
      "2: Encoding Loss 11.886756896972656, Transition Loss -0.7652817964553833, Classifier Loss 0.22883832454681396, Total Loss 94.20407104492188\n",
      "2: Encoding Loss 10.524674415588379, Transition Loss -0.43699929118156433, Classifier Loss 0.2928219437599182, Total Loss 92.43006896972656\n",
      "2: Encoding Loss 10.284622192382812, Transition Loss -0.609131932258606, Classifier Loss 0.2628753185272217, Total Loss 87.99502563476562\n",
      "2: Encoding Loss 7.314414978027344, Transition Loss -0.7882044315338135, Classifier Loss 0.21692132949829102, Total Loss 65.57830810546875\n",
      "2: Encoding Loss 9.562384605407715, Transition Loss -0.7684846520423889, Classifier Loss 0.22826658189296722, Total Loss 80.2006607055664\n",
      "2: Encoding Loss 11.271631240844727, Transition Loss -0.34585341811180115, Classifier Loss 0.22783058881759644, Total Loss 90.41271209716797\n",
      "2: Encoding Loss 10.00731372833252, Transition Loss -0.2797107994556427, Classifier Loss 0.2999332845211029, Total Loss 90.03710174560547\n",
      "2: Encoding Loss 8.609530448913574, Transition Loss -0.7006486654281616, Classifier Loss 0.2214420735836029, Total Loss 73.80110931396484\n",
      "2: Encoding Loss 8.096715927124023, Transition Loss -1.150523066520691, Classifier Loss 0.2088720053434372, Total Loss 69.467041015625\n",
      "2: Encoding Loss 11.690555572509766, Transition Loss -1.1536692380905151, Classifier Loss 0.22856152057647705, Total Loss 92.9990234375\n",
      "2: Encoding Loss 11.631607055664062, Transition Loss -0.5406661629676819, Classifier Loss 0.3066971004009247, Total Loss 100.45913696289062\n",
      "2: Encoding Loss 9.189148902893066, Transition Loss -0.785834789276123, Classifier Loss 0.2777103781700134, Total Loss 82.9056167602539\n",
      "2: Encoding Loss 10.43896198272705, Transition Loss -0.3830171823501587, Classifier Loss 0.24320007860660553, Total Loss 86.95362854003906\n",
      "2: Encoding Loss 10.036073684692383, Transition Loss -0.8148484826087952, Classifier Loss 0.2673298418521881, Total Loss 86.9490966796875\n",
      "2: Encoding Loss 10.272051811218262, Transition Loss -0.5080328583717346, Classifier Loss 0.24251064658164978, Total Loss 85.88317108154297\n",
      "2: Encoding Loss 9.727810859680176, Transition Loss -0.7619738578796387, Classifier Loss 0.20751844346523285, Total Loss 79.118408203125\n",
      "2: Encoding Loss 8.67108154296875, Transition Loss -0.7932124137878418, Classifier Loss 0.2568504214286804, Total Loss 77.71121215820312\n",
      "2: Encoding Loss 8.48375415802002, Transition Loss -1.0178630352020264, Classifier Loss 0.2076893001794815, Total Loss 71.67105102539062\n",
      "2: Encoding Loss 9.218082427978516, Transition Loss -1.007002592086792, Classifier Loss 0.28005534410476685, Total Loss 83.31362915039062\n",
      "2: Encoding Loss 8.959298133850098, Transition Loss -1.626736044883728, Classifier Loss 0.20448026061058044, Total Loss 74.20317077636719\n",
      "2: Encoding Loss 9.294475555419922, Transition Loss -0.40226393938064575, Classifier Loss 0.2464606612920761, Total Loss 80.41276550292969\n",
      "2: Encoding Loss 9.396738052368164, Transition Loss -0.5140759348869324, Classifier Loss 0.20444102585315704, Total Loss 76.82432556152344\n",
      "2: Encoding Loss 8.73375415802002, Transition Loss -0.8267203569412231, Classifier Loss 0.2529296576976776, Total Loss 77.6951675415039\n",
      "2: Encoding Loss 9.295034408569336, Transition Loss -0.9654331803321838, Classifier Loss 0.2056167870759964, Total Loss 76.33149719238281\n",
      "2: Encoding Loss 8.550905227661133, Transition Loss -0.6778668761253357, Classifier Loss 0.21994423866271973, Total Loss 73.29957580566406\n",
      "2: Encoding Loss 10.391634941101074, Transition Loss -0.8760713934898376, Classifier Loss 0.25483983755111694, Total Loss 87.83344268798828\n",
      "2: Encoding Loss 10.221287727355957, Transition Loss -0.43850404024124146, Classifier Loss 0.23752406239509583, Total Loss 85.07996368408203\n",
      "2: Encoding Loss 10.116518020629883, Transition Loss -0.1381203830242157, Classifier Loss 0.24267448484897614, Total Loss 84.96649932861328\n",
      "2: Encoding Loss 10.032342910766602, Transition Loss -0.08819186687469482, Classifier Loss 0.25023072957992554, Total Loss 85.21709442138672\n",
      "2: Encoding Loss 10.900585174560547, Transition Loss -0.5797740817070007, Classifier Loss 0.24206502735614777, Total Loss 89.60978698730469\n",
      "2: Encoding Loss 9.305883407592773, Transition Loss -1.0393872261047363, Classifier Loss 0.25838494300842285, Total Loss 81.67338562011719\n",
      "2: Encoding Loss 12.002177238464355, Transition Loss -0.9047719240188599, Classifier Loss 0.228044793009758, Total Loss 94.81719207763672\n",
      "2: Encoding Loss 9.5038423538208, Transition Loss -0.9495686292648315, Classifier Loss 0.2404598444700241, Total Loss 81.06865692138672\n",
      "2: Encoding Loss 11.294161796569824, Transition Loss -0.3731236457824707, Classifier Loss 0.2385411411523819, Total Loss 91.61894226074219\n",
      "2: Encoding Loss 10.19924545288086, Transition Loss -0.8079413175582886, Classifier Loss 0.20713989436626434, Total Loss 81.90914916992188\n",
      "2: Encoding Loss 8.627558708190918, Transition Loss -0.5804637670516968, Classifier Loss 0.23572441935539246, Total Loss 75.33757019042969\n",
      "2: Encoding Loss 11.792825698852539, Transition Loss -0.46693187952041626, Classifier Loss 0.22581583261489868, Total Loss 93.3383560180664\n",
      "2: Encoding Loss 10.499570846557617, Transition Loss -0.6849979162216187, Classifier Loss 0.2246384620666504, Total Loss 85.46099853515625\n",
      "2: Encoding Loss 9.312970161437988, Transition Loss -0.9157078266143799, Classifier Loss 0.2552213966846466, Total Loss 81.39959716796875\n",
      "2: Encoding Loss 9.719921112060547, Transition Loss -0.9261434674263, Classifier Loss 0.27237197756767273, Total Loss 85.55635833740234\n",
      "2: Encoding Loss 9.374978065490723, Transition Loss -0.4561653137207031, Classifier Loss 0.26676106452941895, Total Loss 82.92579650878906\n",
      "2: Encoding Loss 9.370739936828613, Transition Loss -0.8167341947555542, Classifier Loss 0.2533561587333679, Total Loss 81.55973052978516\n",
      "2: Encoding Loss 9.162508010864258, Transition Loss -0.962002158164978, Classifier Loss 0.22733329236507416, Total Loss 77.70799255371094\n",
      "2: Encoding Loss 9.603543281555176, Transition Loss -0.975620448589325, Classifier Loss 0.22087930142879486, Total Loss 79.70880126953125\n",
      "2: Encoding Loss 7.0550856590271, Transition Loss -0.8680845499038696, Classifier Loss 0.2136315107345581, Total Loss 63.693321228027344\n",
      "2: Encoding Loss 9.434045791625977, Transition Loss -1.2378243207931519, Classifier Loss 0.2409004271030426, Total Loss 80.6938247680664\n",
      "2: Encoding Loss 8.925044059753418, Transition Loss -1.1612215042114258, Classifier Loss 0.23727479577064514, Total Loss 77.27728271484375\n",
      "2: Encoding Loss 7.549442291259766, Transition Loss -1.1084884405136108, Classifier Loss 0.22606313228607178, Total Loss 67.90252685546875\n",
      "2: Encoding Loss 10.381295204162598, Transition Loss -0.2644364535808563, Classifier Loss 0.29265379905700684, Total Loss 91.55304718017578\n",
      "2: Encoding Loss 8.946327209472656, Transition Loss -1.0102730989456177, Classifier Loss 0.2264823615550995, Total Loss 76.32579803466797\n",
      "2: Encoding Loss 9.854473114013672, Transition Loss -0.4941138029098511, Classifier Loss 0.27077189087867737, Total Loss 86.2038345336914\n",
      "2: Encoding Loss 11.014337539672852, Transition Loss -0.7173848152160645, Classifier Loss 0.2916410267353058, Total Loss 95.24983978271484\n",
      "2: Encoding Loss 11.777276992797852, Transition Loss -0.13190028071403503, Classifier Loss 0.27479109168052673, Total Loss 98.1427230834961\n",
      "2: Encoding Loss 10.405158996582031, Transition Loss -0.5053089261054993, Classifier Loss 0.22218284010887146, Total Loss 84.6490478515625\n",
      "2: Encoding Loss 10.461116790771484, Transition Loss -1.1006851196289062, Classifier Loss 0.2202187478542328, Total Loss 84.78813934326172\n",
      "2: Encoding Loss 11.562664031982422, Transition Loss -0.6233113408088684, Classifier Loss 0.2708481550216675, Total Loss 96.4605484008789\n",
      "2: Encoding Loss 9.32819938659668, Transition Loss -0.774419903755188, Classifier Loss 0.2850976586341858, Total Loss 84.47865295410156\n",
      "2: Encoding Loss 9.04554557800293, Transition Loss -1.8340243101119995, Classifier Loss 0.18699488043785095, Total Loss 72.97203063964844\n",
      "2: Encoding Loss 8.827250480651855, Transition Loss -0.3948706388473511, Classifier Loss 0.27216750383377075, Total Loss 80.18009185791016\n",
      "2: Encoding Loss 9.005297660827637, Transition Loss -0.39874646067619324, Classifier Loss 0.2429274022579193, Total Loss 78.3243637084961\n",
      "2: Encoding Loss 10.00712776184082, Transition Loss -0.6979343295097351, Classifier Loss 0.257854163646698, Total Loss 85.8279037475586\n",
      "2: Encoding Loss 10.834015846252441, Transition Loss -0.04820465296506882, Classifier Loss 0.3079622983932495, Total Loss 95.80030059814453\n",
      "2: Encoding Loss 10.484197616577148, Transition Loss -0.2113264799118042, Classifier Loss 0.2586571276187897, Total Loss 88.77082061767578\n",
      "2: Encoding Loss 9.604273796081543, Transition Loss -0.6627499461174011, Classifier Loss 0.24469543993473053, Total Loss 82.09491729736328\n",
      "2: Encoding Loss 10.99711799621582, Transition Loss -0.7230314612388611, Classifier Loss 0.2330814152956009, Total Loss 89.29056549072266\n",
      "2: Encoding Loss 9.850784301757812, Transition Loss -0.6921500563621521, Classifier Loss 0.28282806277275085, Total Loss 87.38723754882812\n",
      "2: Encoding Loss 7.7548828125, Transition Loss -0.6502540111541748, Classifier Loss 0.26790302991867065, Total Loss 73.3193359375\n",
      "2: Encoding Loss 8.680892944335938, Transition Loss -0.775149941444397, Classifier Loss 0.21521270275115967, Total Loss 73.60631561279297\n",
      "2: Encoding Loss 8.700511932373047, Transition Loss -0.8324389457702637, Classifier Loss 0.2167893946170807, Total Loss 73.88168334960938\n",
      "2: Encoding Loss 10.018678665161133, Transition Loss -0.5484398007392883, Classifier Loss 0.22551760077476501, Total Loss 82.66361236572266\n",
      "2: Encoding Loss 9.631442070007324, Transition Loss -0.11551107466220856, Classifier Loss 0.25678351521492004, Total Loss 83.46696472167969\n",
      "2: Encoding Loss 8.510665893554688, Transition Loss -0.6526968479156494, Classifier Loss 0.3025064468383789, Total Loss 81.31438446044922\n",
      "2: Encoding Loss 11.694241523742676, Transition Loss -0.31677037477493286, Classifier Loss 0.28739336133003235, Total Loss 98.90465545654297\n",
      "2: Encoding Loss 10.414966583251953, Transition Loss -0.507904052734375, Classifier Loss 0.24605116248130798, Total Loss 87.09471130371094\n",
      "2: Encoding Loss 11.416683197021484, Transition Loss -0.2919842600822449, Classifier Loss 0.26882198452949524, Total Loss 95.3821792602539\n",
      "2: Encoding Loss 10.286934852600098, Transition Loss -0.562224268913269, Classifier Loss 0.2536432445049286, Total Loss 87.0857162475586\n",
      "2: Encoding Loss 10.443510055541992, Transition Loss -0.5562273859977722, Classifier Loss 0.2507462203502655, Total Loss 87.73546600341797\n",
      "2: Encoding Loss 8.749828338623047, Transition Loss -1.06934654712677, Classifier Loss 0.23168015480041504, Total Loss 75.66656494140625\n",
      "2: Encoding Loss 9.696905136108398, Transition Loss -0.7353635430335999, Classifier Loss 0.26125437021255493, Total Loss 84.30657196044922\n",
      "2: Encoding Loss 9.848745346069336, Transition Loss -0.5163613557815552, Classifier Loss 0.2006063312292099, Total Loss 79.15290069580078\n",
      "2: Encoding Loss 8.703123092651367, Transition Loss -0.3672530949115753, Classifier Loss 0.25044533610343933, Total Loss 77.26313018798828\n",
      "2: Encoding Loss 9.897616386413574, Transition Loss 0.061003878712654114, Classifier Loss 0.2352314591407776, Total Loss 82.93324279785156\n",
      "2: Encoding Loss 10.54902458190918, Transition Loss -0.3076518177986145, Classifier Loss 0.22520238161087036, Total Loss 85.81427001953125\n",
      "2: Encoding Loss 9.778092384338379, Transition Loss -0.31208518147468567, Classifier Loss 0.3151554465293884, Total Loss 90.18397521972656\n",
      "2: Encoding Loss 8.599783897399902, Transition Loss -1.2344988584518433, Classifier Loss 0.21853365004062653, Total Loss 73.45157623291016\n",
      "2: Encoding Loss 10.559412002563477, Transition Loss -0.8118610382080078, Classifier Loss 0.1929922103881836, Total Loss 82.65536499023438\n",
      "2: Encoding Loss 10.069695472717285, Transition Loss -0.9870067834854126, Classifier Loss 0.2268560826778412, Total Loss 83.10338592529297\n",
      "2: Encoding Loss 9.345846176147461, Transition Loss -0.6560913920402527, Classifier Loss 0.1918443888425827, Total Loss 75.25926208496094\n",
      "2: Encoding Loss 10.028818130493164, Transition Loss -0.2913718819618225, Classifier Loss 0.18759329617023468, Total Loss 78.93212127685547\n",
      "2: Encoding Loss 9.792220115661621, Transition Loss -1.3156540393829346, Classifier Loss 0.2810455858707428, Total Loss 86.85735321044922\n",
      "2: Encoding Loss 8.53912353515625, Transition Loss -1.6566851139068604, Classifier Loss 0.24706587195396423, Total Loss 75.94066619873047\n",
      "2: Encoding Loss 9.522098541259766, Transition Loss -0.029273727908730507, Classifier Loss 0.22921673953533173, Total Loss 80.05425262451172\n",
      "2: Encoding Loss 9.316529273986816, Transition Loss -0.9886903762817383, Classifier Loss 0.2760753333568573, Total Loss 83.50631713867188\n",
      "2: Encoding Loss 7.6332597732543945, Transition Loss -1.1135975122451782, Classifier Loss 0.24300916492938995, Total Loss 70.10003662109375\n",
      "2: Encoding Loss 10.07078742980957, Transition Loss -0.9846552014350891, Classifier Loss 0.23505088686943054, Total Loss 83.92941284179688\n",
      "2: Encoding Loss 8.532620429992676, Transition Loss -0.45807603001594543, Classifier Loss 0.2250490039587021, Total Loss 73.700439453125\n",
      "2: Encoding Loss 7.480402946472168, Transition Loss -0.7865138053894043, Classifier Loss 0.29776179790496826, Total Loss 74.65828704833984\n",
      "2: Encoding Loss 7.449798583984375, Transition Loss -1.5299670696258545, Classifier Loss 0.2224491387605667, Total Loss 66.9430923461914\n",
      "2: Encoding Loss 6.987401008605957, Transition Loss -1.092516303062439, Classifier Loss 0.2576051950454712, Total Loss 67.68449401855469\n",
      "2: Encoding Loss 7.9781084060668945, Transition Loss -1.1165223121643066, Classifier Loss 0.24661482870578766, Total Loss 72.5296859741211\n",
      "2: Encoding Loss 9.15776252746582, Transition Loss -1.0429277420043945, Classifier Loss 0.20275545120239258, Total Loss 75.2217025756836\n",
      "2: Encoding Loss 9.03306770324707, Transition Loss -0.9818999171257019, Classifier Loss 0.24241985380649567, Total Loss 78.44000244140625\n",
      "2: Encoding Loss 9.218979835510254, Transition Loss -1.34749436378479, Classifier Loss 0.23844537138938904, Total Loss 79.15787506103516\n",
      "2: Encoding Loss 9.920018196105957, Transition Loss -0.9361030459403992, Classifier Loss 0.2646510899066925, Total Loss 85.98484802246094\n",
      "2: Encoding Loss 9.188238143920898, Transition Loss -1.082292914390564, Classifier Loss 0.1896495223045349, Total Loss 74.09394836425781\n",
      "2: Encoding Loss 9.09070873260498, Transition Loss -1.044765830039978, Classifier Loss 0.22285397350788116, Total Loss 76.82923126220703\n",
      "2: Encoding Loss 9.453672409057617, Transition Loss -1.2043452262878418, Classifier Loss 0.23437605798244476, Total Loss 80.15916442871094\n",
      "2: Encoding Loss 8.905807495117188, Transition Loss -1.2205842733383179, Classifier Loss 0.21941983699798584, Total Loss 75.3763427734375\n",
      "2: Encoding Loss 8.568426132202148, Transition Loss -1.083579421043396, Classifier Loss 0.22251757979393005, Total Loss 73.6618881225586\n",
      "2: Encoding Loss 8.703718185424805, Transition Loss -0.6573014855384827, Classifier Loss 0.2057967334985733, Total Loss 72.80172729492188\n",
      "2: Encoding Loss 9.095402717590332, Transition Loss -1.1836596727371216, Classifier Loss 0.26144304871559143, Total Loss 80.71624755859375\n",
      "2: Encoding Loss 9.39199161529541, Transition Loss -0.5212544798851013, Classifier Loss 0.18176758289337158, Total Loss 74.52850341796875\n",
      "2: Encoding Loss 8.803823471069336, Transition Loss -1.1210812330245972, Classifier Loss 0.23011404275894165, Total Loss 75.83390045166016\n",
      "2: Encoding Loss 11.634004592895508, Transition Loss -0.7303836345672607, Classifier Loss 0.23704242706298828, Total Loss 93.50798034667969\n",
      "2: Encoding Loss 10.745062828063965, Transition Loss -1.0665006637573242, Classifier Loss 0.250093936920166, Total Loss 89.4793472290039\n",
      "2: Encoding Loss 10.801262855529785, Transition Loss -0.14797928929328918, Classifier Loss 0.33287402987480164, Total Loss 98.09492492675781\n",
      "2: Encoding Loss 10.68304443359375, Transition Loss -0.4370025396347046, Classifier Loss 0.28243520855903625, Total Loss 92.34161376953125\n",
      "2: Encoding Loss 8.63023853302002, Transition Loss -0.8863142728805542, Classifier Loss 0.18993350863456726, Total Loss 70.77442932128906\n",
      "2: Encoding Loss 7.589425086975098, Transition Loss -1.0915180444717407, Classifier Loss 0.2012721747159958, Total Loss 65.66333770751953\n",
      "2: Encoding Loss 7.005949974060059, Transition Loss -0.6884514093399048, Classifier Loss 0.2569858431816101, Total Loss 67.7340087890625\n",
      "2: Encoding Loss 10.291511535644531, Transition Loss -0.379733145236969, Classifier Loss 0.2462020218372345, Total Loss 86.36912536621094\n",
      "2: Encoding Loss 10.179142951965332, Transition Loss -0.8520510792732239, Classifier Loss 0.23868432641029358, Total Loss 84.94294738769531\n",
      "2: Encoding Loss 10.582527160644531, Transition Loss 0.2620756924152374, Classifier Loss 0.30471667647361755, Total Loss 94.07166290283203\n",
      "2: Encoding Loss 9.470312118530273, Transition Loss -0.9849019646644592, Classifier Loss 0.20994527637958527, Total Loss 77.81600952148438\n",
      "2: Encoding Loss 10.384590148925781, Transition Loss -0.9692867398262024, Classifier Loss 0.23647184669971466, Total Loss 85.95433807373047\n",
      "2: Encoding Loss 8.612120628356934, Transition Loss -1.0325148105621338, Classifier Loss 0.23131760954856873, Total Loss 74.8040771484375\n",
      "2: Encoding Loss 6.523015022277832, Transition Loss -1.674174427986145, Classifier Loss 0.20586822926998138, Total Loss 59.7242431640625\n",
      "2: Encoding Loss 7.0707526206970215, Transition Loss -1.9119958877563477, Classifier Loss 0.18979042768478394, Total Loss 61.40279769897461\n",
      "2: Encoding Loss 9.513395309448242, Transition Loss -1.0794651508331299, Classifier Loss 0.2206435352563858, Total Loss 79.14429473876953\n",
      "2: Encoding Loss 7.587454319000244, Transition Loss -1.0357178449630737, Classifier Loss 0.22273321449756622, Total Loss 67.79763793945312\n",
      "2: Encoding Loss 8.990239143371582, Transition Loss -0.49786660075187683, Classifier Loss 0.32973721623420715, Total Loss 86.91496276855469\n",
      "2: Encoding Loss 9.085264205932617, Transition Loss -1.0645703077316284, Classifier Loss 0.2042194902896881, Total Loss 74.93311309814453\n",
      "2: Encoding Loss 7.644036293029785, Transition Loss -1.1581610441207886, Classifier Loss 0.25966379046440125, Total Loss 71.83013153076172\n",
      "2: Encoding Loss 9.18058967590332, Transition Loss -0.7253068685531616, Classifier Loss 0.24107104539871216, Total Loss 79.19035339355469\n",
      "2: Encoding Loss 9.207077026367188, Transition Loss -0.7009536027908325, Classifier Loss 0.2684839963912964, Total Loss 82.09058380126953\n",
      "2: Encoding Loss 9.886791229248047, Transition Loss -0.83211350440979, Classifier Loss 0.27769821882247925, Total Loss 87.09024047851562\n",
      "2: Encoding Loss 10.013296127319336, Transition Loss -0.4797027111053467, Classifier Loss 0.2581181824207306, Total Loss 85.89141082763672\n",
      "2: Encoding Loss 7.94854736328125, Transition Loss -1.217029333114624, Classifier Loss 0.20795170962810516, Total Loss 68.48596954345703\n",
      "2: Encoding Loss 9.407909393310547, Transition Loss -1.1529581546783447, Classifier Loss 0.21363846957683563, Total Loss 77.81085205078125\n",
      "2: Encoding Loss 9.195598602294922, Transition Loss -1.1691687107086182, Classifier Loss 0.21532617509365082, Total Loss 76.70574951171875\n",
      "2: Encoding Loss 7.182599067687988, Transition Loss -1.3782756328582764, Classifier Loss 0.1666417419910431, Total Loss 59.759220123291016\n",
      "2: Encoding Loss 9.229947090148926, Transition Loss -0.8623682856559753, Classifier Loss 0.20835399627685547, Total Loss 76.2147445678711\n",
      "2: Encoding Loss 9.393346786499023, Transition Loss -0.7760982513427734, Classifier Loss 0.20438659191131592, Total Loss 76.79843139648438\n",
      "2: Encoding Loss 7.31417989730835, Transition Loss -1.326519250869751, Classifier Loss 0.1779850721359253, Total Loss 61.68305969238281\n",
      "2: Encoding Loss 7.091533660888672, Transition Loss -0.5956003665924072, Classifier Loss 0.17492172122001648, Total Loss 60.0411376953125\n",
      "2: Encoding Loss 9.158921241760254, Transition Loss -0.7369837760925293, Classifier Loss 0.19003885984420776, Total Loss 73.95711517333984\n",
      "2: Encoding Loss 7.8259124755859375, Transition Loss -1.011960506439209, Classifier Loss 0.2560666501522064, Total Loss 72.56173706054688\n",
      "2: Encoding Loss 10.046796798706055, Transition Loss -0.19624975323677063, Classifier Loss 0.1929091215133667, Total Loss 79.57162475585938\n",
      "2: Encoding Loss 9.499533653259277, Transition Loss -0.4167119264602661, Classifier Loss 0.21688933670520782, Total Loss 78.68597412109375\n",
      "2: Encoding Loss 9.28419017791748, Transition Loss -0.354535311460495, Classifier Loss 0.19524620473384857, Total Loss 75.22962188720703\n",
      "2: Encoding Loss 8.742947578430176, Transition Loss -0.987992525100708, Classifier Loss 0.19966155290603638, Total Loss 72.42344665527344\n",
      "2: Encoding Loss 9.029827117919922, Transition Loss -0.5364055633544922, Classifier Loss 0.22280292212963104, Total Loss 76.45904541015625\n",
      "2: Encoding Loss 8.03462028503418, Transition Loss -1.2631826400756836, Classifier Loss 0.24941566586494446, Total Loss 73.14878845214844\n",
      "2: Encoding Loss 9.161066055297852, Transition Loss -0.9234014749526978, Classifier Loss 0.20680435001850128, Total Loss 75.64646911621094\n",
      "2: Encoding Loss 7.633775234222412, Transition Loss -0.7185099124908447, Classifier Loss 0.2502788007259369, Total Loss 70.83024597167969\n",
      "2: Encoding Loss 9.977221488952637, Transition Loss -0.8196758031845093, Classifier Loss 0.2484169900417328, Total Loss 84.70470428466797\n",
      "2: Encoding Loss 7.7260894775390625, Transition Loss -1.0929453372955322, Classifier Loss 0.20803481340408325, Total Loss 67.15958404541016\n",
      "2: Encoding Loss 10.5870361328125, Transition Loss -0.6826479434967041, Classifier Loss 0.21317362785339355, Total Loss 84.83930969238281\n",
      "2: Encoding Loss 9.509031295776367, Transition Loss -0.8861886858940125, Classifier Loss 0.2341970056295395, Total Loss 80.47354125976562\n",
      "2: Encoding Loss 8.528722763061523, Transition Loss -0.6025576591491699, Classifier Loss 0.20071937143802643, Total Loss 71.24403381347656\n",
      "2: Encoding Loss 7.667559623718262, Transition Loss -0.7239447832107544, Classifier Loss 0.2294778972864151, Total Loss 68.9528579711914\n",
      "2: Encoding Loss 8.71440315246582, Transition Loss -0.9723703861236572, Classifier Loss 0.2610832750797272, Total Loss 78.39435577392578\n",
      "2: Encoding Loss 7.1726884841918945, Transition Loss -1.5758079290390015, Classifier Loss 0.20621906220912933, Total Loss 63.65740966796875\n",
      "2: Encoding Loss 7.971869468688965, Transition Loss -0.770606279373169, Classifier Loss 0.19942310452461243, Total Loss 67.77322387695312\n",
      "2: Encoding Loss 8.013666152954102, Transition Loss -0.7880311012268066, Classifier Loss 0.2521868348121643, Total Loss 73.30036926269531\n",
      "2: Encoding Loss 9.113088607788086, Transition Loss -1.05876886844635, Classifier Loss 0.24797087907791138, Total Loss 79.4751968383789\n",
      "2: Encoding Loss 8.79558277130127, Transition Loss -0.8202441334724426, Classifier Loss 0.23427796363830566, Total Loss 76.20096588134766\n",
      "2: Encoding Loss 8.227457046508789, Transition Loss -0.9829707741737366, Classifier Loss 0.22207875549793243, Total Loss 71.57221984863281\n",
      "2: Encoding Loss 7.875850677490234, Transition Loss -0.9991886615753174, Classifier Loss 0.27620428800582886, Total Loss 74.87513732910156\n",
      "2: Encoding Loss 12.044454574584961, Transition Loss -0.5487884879112244, Classifier Loss 0.33998170495033264, Total Loss 106.26467895507812\n",
      "2: Encoding Loss 11.147317886352539, Transition Loss -1.2221052646636963, Classifier Loss 0.2166067361831665, Total Loss 88.54409790039062\n",
      "2: Encoding Loss 8.009712219238281, Transition Loss -1.7569689750671387, Classifier Loss 0.2082328498363495, Total Loss 68.880859375\n",
      "2: Encoding Loss 6.744414329528809, Transition Loss -1.586074709892273, Classifier Loss 0.2083917111158371, Total Loss 61.305023193359375\n",
      "2: Encoding Loss 11.423608779907227, Transition Loss -1.4326742887496948, Classifier Loss 0.2275620400905609, Total Loss 91.29728698730469\n",
      "2: Encoding Loss 13.218057632446289, Transition Loss -0.3903012275695801, Classifier Loss 0.22645078599452972, Total Loss 101.95327758789062\n",
      "2: Encoding Loss 11.091899871826172, Transition Loss -1.128440260887146, Classifier Loss 0.30361756682395935, Total Loss 96.91270446777344\n",
      "2: Encoding Loss 11.193156242370605, Transition Loss -1.2317019701004028, Classifier Loss 0.2349892258644104, Total Loss 90.6573715209961\n",
      "2: Encoding Loss 10.843271255493164, Transition Loss -0.22495156526565552, Classifier Loss 0.21934205293655396, Total Loss 86.99374389648438\n",
      "2: Encoding Loss 7.768214225769043, Transition Loss -1.3111302852630615, Classifier Loss 0.20703767240047455, Total Loss 67.31253051757812\n",
      "2: Encoding Loss 6.7165117263793945, Transition Loss -1.3354501724243164, Classifier Loss 0.21101027727127075, Total Loss 61.399566650390625\n",
      "2: Encoding Loss 10.014719009399414, Transition Loss -0.33054524660110474, Classifier Loss 0.20994949340820312, Total Loss 81.08313751220703\n",
      "2: Encoding Loss 10.40583610534668, Transition Loss 0.33916348218917847, Classifier Loss 0.24235963821411133, Total Loss 86.80664825439453\n",
      "2: Encoding Loss 6.234689712524414, Transition Loss -0.997957706451416, Classifier Loss 0.18441015481948853, Total Loss 55.848751068115234\n",
      "2: Encoding Loss 7.909018516540527, Transition Loss -0.7972428798675537, Classifier Loss 0.2676246166229248, Total Loss 74.21625518798828\n",
      "2: Encoding Loss 8.833422660827637, Transition Loss -0.6880788207054138, Classifier Loss 0.26555365324020386, Total Loss 79.55562591552734\n",
      "2: Encoding Loss 9.102592468261719, Transition Loss -0.7213282585144043, Classifier Loss 0.19186624884605408, Total Loss 73.80189514160156\n",
      "2: Encoding Loss 8.273530960083008, Transition Loss -0.9936150908470154, Classifier Loss 0.2246309518814087, Total Loss 72.1038818359375\n",
      "2: Encoding Loss 9.319682121276855, Transition Loss -1.2898496389389038, Classifier Loss 0.3278476595878601, Total Loss 88.70234680175781\n",
      "2: Encoding Loss 8.921971321105957, Transition Loss -1.0435980558395386, Classifier Loss 0.24602095782756805, Total Loss 78.13350677490234\n",
      "2: Encoding Loss 7.5194501876831055, Transition Loss -1.4141112565994263, Classifier Loss 0.20300310850143433, Total Loss 65.41645050048828\n",
      "2: Encoding Loss 9.30876636505127, Transition Loss -1.2271604537963867, Classifier Loss 0.2511405050754547, Total Loss 80.9661636352539\n",
      "2: Encoding Loss 9.09760570526123, Transition Loss -1.2586666345596313, Classifier Loss 0.2023470550775528, Total Loss 74.81983947753906\n",
      "2: Encoding Loss 9.410569190979004, Transition Loss -0.7901653051376343, Classifier Loss 0.22747069597244263, Total Loss 79.21017456054688\n",
      "2: Encoding Loss 8.822599411010742, Transition Loss -0.10247987508773804, Classifier Loss 0.23961248993873596, Total Loss 76.89681243896484\n",
      "2: Encoding Loss 8.329996109008789, Transition Loss -0.9257792234420776, Classifier Loss 0.2719216048717499, Total Loss 77.17176055908203\n",
      "2: Encoding Loss 9.369134902954102, Transition Loss -0.98582923412323, Classifier Loss 0.20613323152065277, Total Loss 76.8277359008789\n",
      "2: Encoding Loss 8.635251998901367, Transition Loss -0.541828453540802, Classifier Loss 0.23369967937469482, Total Loss 75.1812744140625\n",
      "2: Encoding Loss 7.077662467956543, Transition Loss -0.7215065360069275, Classifier Loss 0.17332370579242706, Total Loss 59.798057556152344\n",
      "2: Encoding Loss 8.457732200622559, Transition Loss -1.468512773513794, Classifier Loss 0.20718225836753845, Total Loss 71.46403503417969\n",
      "2: Encoding Loss 8.579227447509766, Transition Loss -0.006601110100746155, Classifier Loss 0.18930484354496002, Total Loss 70.40585327148438\n",
      "2: Encoding Loss 9.18784236907959, Transition Loss -0.9478954076766968, Classifier Loss 0.22549039125442505, Total Loss 77.67571258544922\n",
      "2: Encoding Loss 7.068804740905762, Transition Loss -1.6382981538772583, Classifier Loss 0.20113196969032288, Total Loss 62.52537155151367\n",
      "2: Encoding Loss 9.805631637573242, Transition Loss -0.8644832372665405, Classifier Loss 0.22342976927757263, Total Loss 81.17642974853516\n",
      "2: Encoding Loss 8.469121932983398, Transition Loss -0.903181791305542, Classifier Loss 0.22599880397319794, Total Loss 73.41425323486328\n",
      "2: Encoding Loss 8.164156913757324, Transition Loss -1.3189432621002197, Classifier Loss 0.19460077583789825, Total Loss 68.44449615478516\n",
      "2: Encoding Loss 7.898123264312744, Transition Loss -1.3736412525177002, Classifier Loss 0.22217188775539398, Total Loss 69.60537719726562\n",
      "2: Encoding Loss 7.270258903503418, Transition Loss -0.5807697176933289, Classifier Loss 0.21421976387500763, Total Loss 65.04330444335938\n",
      "2: Encoding Loss 6.461721897125244, Transition Loss -0.9753130674362183, Classifier Loss 0.19739606976509094, Total Loss 58.509552001953125\n",
      "2: Encoding Loss 11.703856468200684, Transition Loss -1.0825529098510742, Classifier Loss 0.2464897781610489, Total Loss 94.87168884277344\n",
      "2: Encoding Loss 10.958041191101074, Transition Loss -1.3784925937652588, Classifier Loss 0.19835501909255981, Total Loss 85.58320617675781\n",
      "2: Encoding Loss 9.217330932617188, Transition Loss -0.5298265218734741, Classifier Loss 0.21153588593006134, Total Loss 76.45736694335938\n",
      "2: Encoding Loss 8.310164451599121, Transition Loss -0.9469575881958008, Classifier Loss 0.20675963163375854, Total Loss 70.53656768798828\n",
      "2: Encoding Loss 6.767960548400879, Transition Loss -0.9307610392570496, Classifier Loss 0.2354774922132492, Total Loss 64.15514373779297\n",
      "2: Encoding Loss 11.334774017333984, Transition Loss -0.4906127452850342, Classifier Loss 0.2592000365257263, Total Loss 93.92845153808594\n",
      "2: Encoding Loss 9.466812133789062, Transition Loss -0.4938148856163025, Classifier Loss 0.19245435297489166, Total Loss 76.04611206054688\n",
      "2: Encoding Loss 7.406393527984619, Transition Loss -1.6109930276870728, Classifier Loss 0.16124823689460754, Total Loss 60.56254196166992\n",
      "2: Encoding Loss 5.797698974609375, Transition Loss -1.1586657762527466, Classifier Loss 0.15576712787151337, Total Loss 50.36244583129883\n",
      "2: Encoding Loss 8.93329906463623, Transition Loss -1.5957047939300537, Classifier Loss 0.21234776079654694, Total Loss 74.83393096923828\n",
      "2: Encoding Loss 9.628134727478027, Transition Loss -1.1223160028457642, Classifier Loss 0.1819266825914383, Total Loss 75.96102905273438\n",
      "2: Encoding Loss 7.6099395751953125, Transition Loss -0.6339299082756042, Classifier Loss 0.23654943704605103, Total Loss 69.3143310546875\n",
      "2: Encoding Loss 6.669297695159912, Transition Loss -1.2375088930130005, Classifier Loss 0.1500239074230194, Total Loss 55.01768493652344\n",
      "2: Encoding Loss 7.506313323974609, Transition Loss -1.3253906965255737, Classifier Loss 0.21500173211097717, Total Loss 66.53752899169922\n",
      "2: Encoding Loss 8.142664909362793, Transition Loss -1.2860099077224731, Classifier Loss 0.26236456632614136, Total Loss 75.09193420410156\n",
      "2: Encoding Loss 8.017671585083008, Transition Loss -1.388816237449646, Classifier Loss 0.2158345878124237, Total Loss 69.68892669677734\n",
      "2: Encoding Loss 8.150239944458008, Transition Loss -1.2910277843475342, Classifier Loss 0.20357221364974976, Total Loss 69.25814056396484\n",
      "2: Encoding Loss 7.200949668884277, Transition Loss -0.6906183362007141, Classifier Loss 0.21410784125328064, Total Loss 64.6162109375\n",
      "2: Encoding Loss 8.687101364135742, Transition Loss -0.31793320178985596, Classifier Loss 0.22701233625411987, Total Loss 74.82371520996094\n",
      "2: Encoding Loss 9.478470802307129, Transition Loss -0.44855988025665283, Classifier Loss 0.22195468842983246, Total Loss 79.06611633300781\n",
      "2: Encoding Loss 8.832731246948242, Transition Loss -1.1440019607543945, Classifier Loss 0.2447793334722519, Total Loss 77.47386932373047\n",
      "2: Encoding Loss 8.212878227233887, Transition Loss -2.1003835201263428, Classifier Loss 0.20584489405155182, Total Loss 69.86092376708984\n",
      "2: Encoding Loss 5.915925025939941, Transition Loss -1.5950031280517578, Classifier Loss 0.1875593662261963, Total Loss 54.250850677490234\n",
      "2: Encoding Loss 6.30996036529541, Transition Loss -1.4445977210998535, Classifier Loss 0.21448084712028503, Total Loss 59.30727005004883\n",
      "2: Encoding Loss 7.686620235443115, Transition Loss -1.032360315322876, Classifier Loss 0.1963563859462738, Total Loss 65.75495147705078\n",
      "2: Encoding Loss 12.696284294128418, Transition Loss -1.4039850234985352, Classifier Loss 0.22936326265335083, Total Loss 99.11347198486328\n",
      "2: Encoding Loss 13.490724563598633, Transition Loss -0.8475704193115234, Classifier Loss 0.20809414982795715, Total Loss 101.75343322753906\n",
      "2: Encoding Loss 8.942872047424316, Transition Loss -0.663003146648407, Classifier Loss 0.2090657353401184, Total Loss 74.56354522705078\n",
      "2: Encoding Loss 7.0926971435546875, Transition Loss -0.5871874690055847, Classifier Loss 0.2200503647327423, Total Loss 64.56098175048828\n",
      "2: Encoding Loss 9.810663223266602, Transition Loss -0.9021533131599426, Classifier Loss 0.2019820213317871, Total Loss 79.06182098388672\n",
      "2: Encoding Loss 7.5374956130981445, Transition Loss -1.4893263578414917, Classifier Loss 0.16134552657604218, Total Loss 61.35893249511719\n",
      "2: Encoding Loss 8.455474853515625, Transition Loss -1.6623239517211914, Classifier Loss 0.1668631136417389, Total Loss 67.41850280761719\n",
      "2: Encoding Loss 7.482726097106934, Transition Loss -1.4172656536102295, Classifier Loss 0.19902898371219635, Total Loss 64.79869079589844\n",
      "2: Encoding Loss 8.33860969543457, Transition Loss -0.8906235694885254, Classifier Loss 0.29431450366973877, Total Loss 79.4627456665039\n",
      "2: Encoding Loss 7.3439154624938965, Transition Loss -1.285035490989685, Classifier Loss 0.20637103915214539, Total Loss 64.70008850097656\n",
      "2: Encoding Loss 7.523336887359619, Transition Loss -1.4557725191116333, Classifier Loss 0.20366136729717255, Total Loss 65.50557708740234\n",
      "2: Encoding Loss 9.1555757522583, Transition Loss -0.49741843342781067, Classifier Loss 0.18407244980335236, Total Loss 73.34049987792969\n",
      "2: Encoding Loss 8.168497085571289, Transition Loss -1.0682982206344604, Classifier Loss 0.2519773840904236, Total Loss 74.20829772949219\n",
      "2: Encoding Loss 7.906431674957275, Transition Loss -1.239702582359314, Classifier Loss 0.2298079878091812, Total Loss 70.41889190673828\n",
      "2: Encoding Loss 6.189311981201172, Transition Loss -1.1239327192306519, Classifier Loss 0.17701835930347443, Total Loss 54.837257385253906\n",
      "2: Encoding Loss 8.933314323425293, Transition Loss -0.852485716342926, Classifier Loss 0.2420123815536499, Total Loss 77.80078125\n",
      "2: Encoding Loss 7.4474639892578125, Transition Loss -0.7315509915351868, Classifier Loss 0.23920178413391113, Total Loss 68.60467529296875\n",
      "2: Encoding Loss 7.546145439147949, Transition Loss -0.7972356081008911, Classifier Loss 0.19766058027744293, Total Loss 65.04261016845703\n",
      "2: Encoding Loss 7.010819911956787, Transition Loss -1.0644400119781494, Classifier Loss 0.17459341883659363, Total Loss 59.523834228515625\n",
      "2: Encoding Loss 7.421945095062256, Transition Loss -1.0063531398773193, Classifier Loss 0.21634890139102936, Total Loss 66.1661605834961\n",
      "2: Encoding Loss 6.511290550231934, Transition Loss -1.0501703023910522, Classifier Loss 0.19005872309207916, Total Loss 58.07319641113281\n",
      "2: Encoding Loss 6.453449249267578, Transition Loss -0.8910016417503357, Classifier Loss 0.19081635773181915, Total Loss 57.80197525024414\n",
      "2: Encoding Loss 6.130272388458252, Transition Loss -1.4098289012908936, Classifier Loss 0.20662909746170044, Total Loss 57.4439811706543\n",
      "2: Encoding Loss 5.3729753494262695, Transition Loss -1.9298315048217773, Classifier Loss 0.15232989192008972, Total Loss 47.47007369995117\n",
      "2: Encoding Loss 7.051039218902588, Transition Loss -0.8900251984596252, Classifier Loss 0.23713529109954834, Total Loss 66.0194091796875\n",
      "2: Encoding Loss 6.03124475479126, Transition Loss -0.6223245859146118, Classifier Loss 0.19192031025886536, Total Loss 55.37925338745117\n",
      "2: Encoding Loss 6.332880020141602, Transition Loss -1.1310173273086548, Classifier Loss 0.16859006881713867, Total Loss 54.855831146240234\n",
      "2: Encoding Loss 9.879390716552734, Transition Loss -0.5428781509399414, Classifier Loss 0.2732677757740021, Total Loss 86.60291290283203\n",
      "2: Encoding Loss 9.428840637207031, Transition Loss -0.8904318809509277, Classifier Loss 0.24053508043289185, Total Loss 80.6261978149414\n",
      "2: Encoding Loss 7.927279472351074, Transition Loss -0.7227389812469482, Classifier Loss 0.24674232304096222, Total Loss 72.23762512207031\n",
      "2: Encoding Loss 8.713353157043457, Transition Loss -0.7558873891830444, Classifier Loss 0.1992643028497696, Total Loss 72.20624542236328\n",
      "2: Encoding Loss 6.772422790527344, Transition Loss -1.5106351375579834, Classifier Loss 0.20784258842468262, Total Loss 61.41819381713867\n",
      "2: Encoding Loss 7.133719444274902, Transition Loss -1.1475263833999634, Classifier Loss 0.25959324836730957, Total Loss 68.76118469238281\n",
      "2: Encoding Loss 7.92272424697876, Transition Loss -0.7334247827529907, Classifier Loss 0.18938294053077698, Total Loss 66.47434997558594\n",
      "2: Encoding Loss 6.184870719909668, Transition Loss -0.9507808089256287, Classifier Loss 0.20856109261512756, Total Loss 57.9649543762207\n",
      "2: Encoding Loss 6.5400190353393555, Transition Loss -1.5912617444992065, Classifier Loss 0.1663137525320053, Total Loss 55.870853424072266\n",
      "2: Encoding Loss 6.971685886383057, Transition Loss -0.7707370519638062, Classifier Loss 0.24057120084762573, Total Loss 65.88693237304688\n",
      "2: Encoding Loss 6.648480415344238, Transition Loss -1.1620999574661255, Classifier Loss 0.15940529108047485, Total Loss 55.83094787597656\n",
      "2: Encoding Loss 9.570332527160645, Transition Loss -1.049670696258545, Classifier Loss 0.29657721519470215, Total Loss 87.07929992675781\n",
      "2: Encoding Loss 10.396051406860352, Transition Loss -0.6777687072753906, Classifier Loss 0.22725851833820343, Total Loss 85.10188293457031\n",
      "2: Encoding Loss 8.090349197387695, Transition Loss -1.406676173210144, Classifier Loss 0.2532833516597748, Total Loss 73.86986541748047\n",
      "2: Encoding Loss 7.477625846862793, Transition Loss -1.2276972532272339, Classifier Loss 0.157469242811203, Total Loss 60.61219024658203\n",
      "2: Encoding Loss 5.800759792327881, Transition Loss -1.6683907508850098, Classifier Loss 0.19374455511569977, Total Loss 54.178348541259766\n",
      "2: Encoding Loss 6.016988277435303, Transition Loss -1.080417275428772, Classifier Loss 0.18758045136928558, Total Loss 54.85954666137695\n",
      "2: Encoding Loss 7.914227485656738, Transition Loss -1.425601840019226, Classifier Loss 0.2246866077184677, Total Loss 69.95345306396484\n",
      "2: Encoding Loss 7.065640926361084, Transition Loss -0.7576592564582825, Classifier Loss 0.1912359595298767, Total Loss 61.51714324951172\n",
      "2: Encoding Loss 8.84936809539795, Transition Loss -0.6880854964256287, Classifier Loss 0.22706088423728943, Total Loss 75.8020248413086\n",
      "2: Encoding Loss 9.065800666809082, Transition Loss -1.5517752170562744, Classifier Loss 0.19221553206443787, Total Loss 73.6157455444336\n",
      "2: Encoding Loss 7.638102054595947, Transition Loss -1.4342565536499023, Classifier Loss 0.21075503528118134, Total Loss 66.9035415649414\n",
      "2: Encoding Loss 6.02018928527832, Transition Loss -0.8971782922744751, Classifier Loss 0.18565639853477478, Total Loss 54.68641662597656\n",
      "2: Encoding Loss 9.376251220703125, Transition Loss -1.3146806955337524, Classifier Loss 0.1841544806957245, Total Loss 74.67243194580078\n",
      "2: Encoding Loss 9.14080810546875, Transition Loss -1.303403615951538, Classifier Loss 0.22737258672714233, Total Loss 77.58158874511719\n",
      "2: Encoding Loss 8.797640800476074, Transition Loss -1.3773071765899658, Classifier Loss 0.21859793365001678, Total Loss 74.64508819580078\n",
      "2: Encoding Loss 9.228904724121094, Transition Loss -0.9419926404953003, Classifier Loss 0.20066583156585693, Total Loss 75.43964385986328\n",
      "2: Encoding Loss 8.530360221862793, Transition Loss -0.8331058025360107, Classifier Loss 0.24576245248317719, Total Loss 75.75807189941406\n",
      "2: Encoding Loss 8.354426383972168, Transition Loss -1.3547115325927734, Classifier Loss 0.14338192343711853, Total Loss 64.4642105102539\n",
      "2: Encoding Loss 7.754171848297119, Transition Loss -1.1724473237991333, Classifier Loss 0.23080354928970337, Total Loss 69.60491943359375\n",
      "2: Encoding Loss 6.340761184692383, Transition Loss -0.5035102367401123, Classifier Loss 0.19276349246501923, Total Loss 57.32071304321289\n",
      "2: Encoding Loss 9.19074535369873, Transition Loss -1.372499704360962, Classifier Loss 0.18753720819950104, Total Loss 73.89764404296875\n",
      "2: Encoding Loss 8.425623893737793, Transition Loss -0.887299120426178, Classifier Loss 0.21596311032772064, Total Loss 72.14969635009766\n",
      "2: Encoding Loss 7.408303260803223, Transition Loss -0.6250500679016113, Classifier Loss 0.17989730834960938, Total Loss 62.439300537109375\n",
      "2: Encoding Loss 6.081736087799072, Transition Loss -1.2489601373672485, Classifier Loss 0.20943325757980347, Total Loss 57.43324279785156\n",
      "2: Encoding Loss 7.051957130432129, Transition Loss -1.2405123710632324, Classifier Loss 0.19811305403709412, Total Loss 62.122554779052734\n",
      "2: Encoding Loss 7.652013301849365, Transition Loss -0.7054629325866699, Classifier Loss 0.22677184641361237, Total Loss 68.58898162841797\n",
      "2: Encoding Loss 6.853315830230713, Transition Loss -2.1352429389953613, Classifier Loss 0.16068682074546814, Total Loss 57.187721252441406\n",
      "2: Encoding Loss 6.5627899169921875, Transition Loss -0.5966280102729797, Classifier Loss 0.20968486368656158, Total Loss 60.34498596191406\n",
      "2: Encoding Loss 8.128202438354492, Transition Loss -1.4414886236190796, Classifier Loss 0.2354981005191803, Total Loss 72.31845092773438\n",
      "2: Encoding Loss 8.27663516998291, Transition Loss -0.8524966239929199, Classifier Loss 0.1349756121635437, Total Loss 63.15703582763672\n",
      "2: Encoding Loss 7.7646965980529785, Transition Loss -1.009151816368103, Classifier Loss 0.2323082685470581, Total Loss 69.818603515625\n",
      "2: Encoding Loss 6.725914001464844, Transition Loss -1.1708472967147827, Classifier Loss 0.22766372561454773, Total Loss 63.12138748168945\n",
      "2: Encoding Loss 8.710832595825195, Transition Loss -0.6854263544082642, Classifier Loss 0.18392492830753326, Total Loss 70.65721130371094\n",
      "2: Encoding Loss 8.804595947265625, Transition Loss -0.9228271245956421, Classifier Loss 0.28747811913490295, Total Loss 81.57502746582031\n",
      "2: Encoding Loss 7.047992706298828, Transition Loss -0.9557093977928162, Classifier Loss 0.17102162539958954, Total Loss 59.389739990234375\n",
      "2: Encoding Loss 7.401188850402832, Transition Loss -0.16223333775997162, Classifier Loss 0.1925288289785385, Total Loss 63.65995407104492\n",
      "2: Encoding Loss 6.266104698181152, Transition Loss -0.9799793362617493, Classifier Loss 0.1849786639213562, Total Loss 56.0941047668457\n",
      "2: Encoding Loss 4.909580707550049, Transition Loss -0.42273980379104614, Classifier Loss 0.1450132131576538, Total Loss 43.958641052246094\n",
      "2: Encoding Loss 8.100591659545898, Transition Loss -0.8198592662811279, Classifier Loss 0.18634748458862305, Total Loss 67.23797607421875\n",
      "2: Encoding Loss 7.218602180480957, Transition Loss -1.3641650676727295, Classifier Loss 0.23566877841949463, Total Loss 66.87794494628906\n",
      "2: Encoding Loss 5.60134220123291, Transition Loss -1.1607125997543335, Classifier Loss 0.1895342469215393, Total Loss 52.561012268066406\n",
      "2: Encoding Loss 6.155637741088867, Transition Loss -1.2581961154937744, Classifier Loss 0.20629824697971344, Total Loss 57.563148498535156\n",
      "2: Encoding Loss 6.264847755432129, Transition Loss -1.2014185190200806, Classifier Loss 0.17153003811836243, Total Loss 54.74161148071289\n",
      "2: Encoding Loss 7.857565402984619, Transition Loss -0.6056812405586243, Classifier Loss 0.15688937902450562, Total Loss 62.83408737182617\n",
      "2: Encoding Loss 7.8208723068237305, Transition Loss -0.8879272937774658, Classifier Loss 0.20935867726802826, Total Loss 67.8607406616211\n",
      "2: Encoding Loss 5.622647285461426, Transition Loss -0.6648887991905212, Classifier Loss 0.234269380569458, Total Loss 57.16255569458008\n",
      "2: Encoding Loss 5.377837181091309, Transition Loss -1.4972577095031738, Classifier Loss 0.17119650542736053, Total Loss 49.38607406616211\n",
      "2: Encoding Loss 5.817370891571045, Transition Loss -1.2559565305709839, Classifier Loss 0.13691724836826324, Total Loss 48.59545135498047\n",
      "2: Encoding Loss 6.3007683753967285, Transition Loss -1.773899793624878, Classifier Loss 0.15266937017440796, Total Loss 53.070838928222656\n",
      "2: Encoding Loss 3.805773973464966, Transition Loss -1.3102456331253052, Classifier Loss 0.14399397373199463, Total Loss 37.233516693115234\n",
      "2: Encoding Loss 8.100110054016113, Transition Loss -1.1952331066131592, Classifier Loss 0.22970706224441528, Total Loss 71.57088470458984\n",
      "2: Encoding Loss 6.889086723327637, Transition Loss -1.5244358777999878, Classifier Loss 0.17301374673843384, Total Loss 58.635284423828125\n",
      "2: Encoding Loss 5.341565132141113, Transition Loss -0.9574328660964966, Classifier Loss 0.18447819352149963, Total Loss 50.496829986572266\n",
      "2: Encoding Loss 5.84090518951416, Transition Loss -0.3314322531223297, Classifier Loss 0.20664675533771515, Total Loss 55.7099723815918\n",
      "2: Encoding Loss 7.179238796234131, Transition Loss -1.1285539865493774, Classifier Loss 0.24541805684566498, Total Loss 67.61679077148438\n",
      "2: Encoding Loss 8.418691635131836, Transition Loss -1.3453271389007568, Classifier Loss 0.25265637040138245, Total Loss 75.7772445678711\n",
      "2: Encoding Loss 6.585667610168457, Transition Loss -1.3341574668884277, Classifier Loss 0.18798542022705078, Total Loss 58.312015533447266\n",
      "2: Encoding Loss 5.20318078994751, Transition Loss -1.5071665048599243, Classifier Loss 0.15934224426746368, Total Loss 47.152706146240234\n",
      "2: Encoding Loss 4.125458717346191, Transition Loss -1.9005974531173706, Classifier Loss 0.15562961995601654, Total Loss 40.3149528503418\n",
      "2: Encoding Loss 5.420950889587402, Transition Loss -1.9224956035614014, Classifier Loss 0.2936367392539978, Total Loss 61.88861083984375\n",
      "2: Encoding Loss 6.423832893371582, Transition Loss -0.9227774739265442, Classifier Loss 0.23242077231407166, Total Loss 61.784706115722656\n",
      "2: Encoding Loss 7.10892391204834, Transition Loss -0.5498706698417664, Classifier Loss 0.19432583451271057, Total Loss 62.085906982421875\n",
      "2: Encoding Loss 5.555117607116699, Transition Loss -0.8641676902770996, Classifier Loss 0.20389658212661743, Total Loss 53.72002029418945\n",
      "2: Encoding Loss 4.789302349090576, Transition Loss -1.4549212455749512, Classifier Loss 0.19663673639297485, Total Loss 48.39890670776367\n",
      "2: Encoding Loss 8.54531478881836, Transition Loss -1.2338764667510986, Classifier Loss 0.16344617307186127, Total Loss 67.61601257324219\n",
      "2: Encoding Loss 7.296238899230957, Transition Loss -1.0803277492523193, Classifier Loss 0.15612125396728516, Total Loss 59.389129638671875\n",
      "2: Encoding Loss 5.869485378265381, Transition Loss -1.1055608987808228, Classifier Loss 0.19370999932289124, Total Loss 54.58747100830078\n",
      "2: Encoding Loss 6.826634407043457, Transition Loss -1.2875428199768066, Classifier Loss 0.170523539185524, Total Loss 58.01164627075195\n",
      "2: Encoding Loss 7.17135763168335, Transition Loss -1.1302543878555298, Classifier Loss 0.15462443232536316, Total Loss 58.49013900756836\n",
      "2: Encoding Loss 6.149798393249512, Transition Loss -1.3581796884536743, Classifier Loss 0.20886194705963135, Total Loss 57.784446716308594\n",
      "2: Encoding Loss 5.894680023193359, Transition Loss -0.8655545115470886, Classifier Loss 0.1844319999217987, Total Loss 53.81093215942383\n",
      "2: Encoding Loss 7.022058963775635, Transition Loss -0.36303073167800903, Classifier Loss 0.19224512577056885, Total Loss 61.356719970703125\n",
      "2: Encoding Loss 5.881636619567871, Transition Loss -1.4668368101119995, Classifier Loss 0.14966970682144165, Total Loss 50.256202697753906\n",
      "2: Encoding Loss 6.790319919586182, Transition Loss -1.2184010744094849, Classifier Loss 0.18467845022678375, Total Loss 59.20927810668945\n",
      "2: Encoding Loss 4.547271728515625, Transition Loss -1.1231931447982788, Classifier Loss 0.19665144383907318, Total Loss 46.948326110839844\n",
      "2: Encoding Loss 5.419189929962158, Transition Loss -0.4899914860725403, Classifier Loss 0.16984045505523682, Total Loss 49.49898910522461\n",
      "2: Encoding Loss 7.365086555480957, Transition Loss -1.8194491863250732, Classifier Loss 0.15790022909641266, Total Loss 59.97981643676758\n",
      "2: Encoding Loss 7.436373233795166, Transition Loss -0.756159245967865, Classifier Loss 0.16443023085594177, Total Loss 61.06096267700195\n",
      "2: Encoding Loss 7.264710426330566, Transition Loss -0.8832829594612122, Classifier Loss 0.22097094357013702, Total Loss 65.68500518798828\n",
      "2: Encoding Loss 6.550900459289551, Transition Loss -1.2491188049316406, Classifier Loss 0.17615166306495667, Total Loss 56.92007064819336\n",
      "2: Encoding Loss 7.666146755218506, Transition Loss -0.9007464647293091, Classifier Loss 0.21633371710777283, Total Loss 67.62989807128906\n",
      "2: Encoding Loss 6.590179443359375, Transition Loss -0.9062995910644531, Classifier Loss 0.14786240458488464, Total Loss 54.32695388793945\n",
      "2: Encoding Loss 4.991567611694336, Transition Loss -0.8413047790527344, Classifier Loss 0.2014896422624588, Total Loss 50.09803771972656\n",
      "2: Encoding Loss 5.4826340675354, Transition Loss -0.8157792091369629, Classifier Loss 0.20850145816802979, Total Loss 53.74562072753906\n",
      "2: Encoding Loss 5.803892135620117, Transition Loss -0.8510419726371765, Classifier Loss 0.1836872696876526, Total Loss 53.19173812866211\n",
      "2: Encoding Loss 7.873690605163574, Transition Loss -0.9161168336868286, Classifier Loss 0.215012788772583, Total Loss 68.74305725097656\n",
      "2: Encoding Loss 5.594693660736084, Transition Loss -0.7730002999305725, Classifier Loss 0.2044784128665924, Total Loss 54.01569747924805\n",
      "2: Encoding Loss 6.032339096069336, Transition Loss -0.6992025375366211, Classifier Loss 0.19896334409713745, Total Loss 56.090091705322266\n",
      "2: Encoding Loss 7.556665897369385, Transition Loss -1.087265968322754, Classifier Loss 0.18368369340896606, Total Loss 63.70793151855469\n",
      "2: Encoding Loss 6.614161491394043, Transition Loss -0.2102770209312439, Classifier Loss 0.17810054123401642, Total Loss 57.49494171142578\n",
      "2: Encoding Loss 7.585762023925781, Transition Loss -0.3858864903450012, Classifier Loss 0.20023150742053986, Total Loss 65.53756713867188\n",
      "2: Encoding Loss 6.569342613220215, Transition Loss -1.1717195510864258, Classifier Loss 0.20115184783935547, Total Loss 59.5307731628418\n",
      "2: Encoding Loss 5.123780250549316, Transition Loss -1.054370641708374, Classifier Loss 0.1979334056377411, Total Loss 50.53559875488281\n",
      "2: Encoding Loss 6.314434051513672, Transition Loss -1.225667953491211, Classifier Loss 0.23205846548080444, Total Loss 61.09196090698242\n",
      "2: Encoding Loss 7.330728054046631, Transition Loss -0.5582116842269897, Classifier Loss 0.1681959331035614, Total Loss 60.80373764038086\n",
      "2: Encoding Loss 5.630319118499756, Transition Loss -1.0254106521606445, Classifier Loss 0.23554080724716187, Total Loss 57.33558654785156\n",
      "2: Encoding Loss 7.444927215576172, Transition Loss -1.1017581224441528, Classifier Loss 0.2274734377861023, Total Loss 67.41646575927734\n",
      "2: Encoding Loss 7.537415981292725, Transition Loss -1.1634089946746826, Classifier Loss 0.17537054419517517, Total Loss 62.761085510253906\n",
      "2: Encoding Loss 6.391613960266113, Transition Loss -1.5019118785858154, Classifier Loss 0.18002474308013916, Total Loss 56.351558685302734\n",
      "2: Encoding Loss 6.183751106262207, Transition Loss -0.9157162308692932, Classifier Loss 0.1859319806098938, Total Loss 55.695343017578125\n",
      "2: Encoding Loss 6.165940761566162, Transition Loss -0.9327447414398193, Classifier Loss 0.19253942370414734, Total Loss 56.24921417236328\n",
      "2: Encoding Loss 6.075627326965332, Transition Loss -0.3854232132434845, Classifier Loss 0.17866209149360657, Total Loss 54.31982421875\n",
      "2: Encoding Loss 4.680656433105469, Transition Loss -0.6687032580375671, Classifier Loss 0.16006456315517426, Total Loss 44.09012985229492\n",
      "2: Encoding Loss 11.118755340576172, Transition Loss -1.5830568075180054, Classifier Loss 0.1999339759349823, Total Loss 86.7052993774414\n",
      "2: Encoding Loss 14.398107528686523, Transition Loss -1.2601982355117798, Classifier Loss 0.19366109371185303, Total Loss 105.75425720214844\n",
      "2: Encoding Loss 9.04947280883789, Transition Loss -0.5636411309242249, Classifier Loss 0.20704005658626556, Total Loss 75.00061798095703\n",
      "2: Encoding Loss 7.970314979553223, Transition Loss -1.357428789138794, Classifier Loss 0.157078817486763, Total Loss 63.529232025146484\n",
      "2: Encoding Loss 5.628261566162109, Transition Loss -1.0382531881332397, Classifier Loss 0.18271872401237488, Total Loss 52.0410270690918\n",
      "2: Encoding Loss 5.047642707824707, Transition Loss -1.0891445875167847, Classifier Loss 0.15339164435863495, Total Loss 45.62458801269531\n",
      "2: Encoding Loss 8.837555885314941, Transition Loss -1.1860092878341675, Classifier Loss 0.1899302452802658, Total Loss 72.01789093017578\n",
      "2: Encoding Loss 7.416869163513184, Transition Loss -1.179212212562561, Classifier Loss 0.22333455085754395, Total Loss 66.83419799804688\n",
      "2: Encoding Loss 6.99017333984375, Transition Loss -0.8657659888267517, Classifier Loss 0.19044870138168335, Total Loss 60.98556137084961\n",
      "2: Encoding Loss 5.1726250648498535, Transition Loss -0.574782133102417, Classifier Loss 0.2047143578529358, Total Loss 51.5069580078125\n",
      "2: Encoding Loss 6.696587562561035, Transition Loss -1.5357410907745361, Classifier Loss 0.17766757309436798, Total Loss 57.94567108154297\n",
      "2: Encoding Loss 6.440171718597412, Transition Loss -1.535045862197876, Classifier Loss 0.15961194038391113, Total Loss 54.60161209106445\n",
      "2: Encoding Loss 6.302367210388184, Transition Loss -0.246376633644104, Classifier Loss 0.19194826483726501, Total Loss 57.008934020996094\n",
      "2: Encoding Loss 7.395279884338379, Transition Loss -1.3249003887176514, Classifier Loss 0.20182093977928162, Total Loss 64.5532455444336\n",
      "2: Encoding Loss 6.279874324798584, Transition Loss -1.6154066324234009, Classifier Loss 0.15723946690559387, Total Loss 53.402549743652344\n",
      "2: Encoding Loss 6.141439437866211, Transition Loss -1.0793957710266113, Classifier Loss 0.13973262906074524, Total Loss 50.821468353271484\n",
      "2: Encoding Loss 7.5366106033325195, Transition Loss -0.5553714036941528, Classifier Loss 0.22214609384536743, Total Loss 67.43405151367188\n",
      "2: Encoding Loss 6.575072288513184, Transition Loss -1.0887621641159058, Classifier Loss 0.15697790682315826, Total Loss 55.14779281616211\n",
      "3: Encoding Loss 5.466722011566162, Transition Loss -0.8912689089775085, Classifier Loss 0.18672797083854675, Total Loss 51.472774505615234\n",
      "3: Encoding Loss 6.062675952911377, Transition Loss -0.7479605078697205, Classifier Loss 0.1613057553768158, Total Loss 52.50633239746094\n",
      "3: Encoding Loss 6.211887359619141, Transition Loss -1.2081661224365234, Classifier Loss 0.19955959916114807, Total Loss 57.22679901123047\n",
      "3: Encoding Loss 6.241032600402832, Transition Loss -1.0133867263793945, Classifier Loss 0.17100223898887634, Total Loss 54.546016693115234\n",
      "3: Encoding Loss 8.557207107543945, Transition Loss -0.4161800742149353, Classifier Loss 0.22733893990516663, Total Loss 74.07696533203125\n",
      "3: Encoding Loss 8.159616470336914, Transition Loss -1.7416810989379883, Classifier Loss 0.16251881420612335, Total Loss 65.2088851928711\n",
      "3: Encoding Loss 5.155328273773193, Transition Loss -1.5051989555358887, Classifier Loss 0.2137933224439621, Total Loss 52.310699462890625\n",
      "3: Encoding Loss 4.623081684112549, Transition Loss -1.2028394937515259, Classifier Loss 0.20869335532188416, Total Loss 48.60734558105469\n",
      "3: Encoding Loss 6.153868198394775, Transition Loss -1.3518691062927246, Classifier Loss 0.197694793343544, Total Loss 56.69214630126953\n",
      "3: Encoding Loss 4.746730804443359, Transition Loss -0.815333366394043, Classifier Loss 0.18501918017864227, Total Loss 46.98198318481445\n",
      "3: Encoding Loss 7.004939079284668, Transition Loss -1.1705634593963623, Classifier Loss 0.22596751153469086, Total Loss 64.62592315673828\n",
      "3: Encoding Loss 4.999985694885254, Transition Loss -1.2851800918579102, Classifier Loss 0.1592922806739807, Total Loss 45.928627014160156\n",
      "3: Encoding Loss 3.5451889038085938, Transition Loss -1.072177529335022, Classifier Loss 0.1507803201675415, Total Loss 36.34873962402344\n",
      "3: Encoding Loss 5.853610038757324, Transition Loss -1.7904384136199951, Classifier Loss 0.1538105607032776, Total Loss 50.50199890136719\n",
      "3: Encoding Loss 4.779326915740967, Transition Loss -1.4764353036880493, Classifier Loss 0.20560699701309204, Total Loss 49.2360725402832\n",
      "3: Encoding Loss 5.78888463973999, Transition Loss -1.2548097372055054, Classifier Loss 0.17676064372062683, Total Loss 52.408870697021484\n",
      "3: Encoding Loss 5.66066312789917, Transition Loss -1.2052335739135742, Classifier Loss 0.196524515748024, Total Loss 53.61595153808594\n",
      "3: Encoding Loss 4.299849510192871, Transition Loss -0.6517464518547058, Classifier Loss 0.1428435742855072, Total Loss 40.08319854736328\n",
      "3: Encoding Loss 8.48977279663086, Transition Loss -1.5742288827896118, Classifier Loss 0.1843605786561966, Total Loss 69.37406158447266\n",
      "3: Encoding Loss 7.106989860534668, Transition Loss -1.0832531452178955, Classifier Loss 0.23160462081432343, Total Loss 65.80196380615234\n",
      "3: Encoding Loss 7.583789825439453, Transition Loss -2.037045478820801, Classifier Loss 0.22959353029727936, Total Loss 68.46127319335938\n",
      "3: Encoding Loss 6.485819339752197, Transition Loss -1.1962800025939941, Classifier Loss 0.18447871506214142, Total Loss 57.36231231689453\n",
      "3: Encoding Loss 6.693650722503662, Transition Loss -0.9481031894683838, Classifier Loss 0.14776375889778137, Total Loss 54.937904357910156\n",
      "3: Encoding Loss 7.217633247375488, Transition Loss -0.8635610342025757, Classifier Loss 0.1691310852766037, Total Loss 60.218563079833984\n",
      "3: Encoding Loss 6.629853248596191, Transition Loss -0.008340589702129364, Classifier Loss 0.17158260941505432, Total Loss 56.9373779296875\n",
      "3: Encoding Loss 5.823905944824219, Transition Loss -1.2547986507415771, Classifier Loss 0.18406036496162415, Total Loss 53.348968505859375\n",
      "3: Encoding Loss 3.9857356548309326, Transition Loss -0.6833308935165405, Classifier Loss 0.17395427823066711, Total Loss 41.3095703125\n",
      "3: Encoding Loss 9.67389965057373, Transition Loss -1.2276654243469238, Classifier Loss 0.2248021811246872, Total Loss 80.52313232421875\n",
      "3: Encoding Loss 7.819748878479004, Transition Loss -1.7899986505508423, Classifier Loss 0.16480328142642975, Total Loss 63.39810562133789\n",
      "3: Encoding Loss 5.058989524841309, Transition Loss -1.367444396018982, Classifier Loss 0.2001773566007614, Total Loss 50.37112808227539\n",
      "3: Encoding Loss 4.958757400512695, Transition Loss -1.3812615871429443, Classifier Loss 0.17478756606578827, Total Loss 47.23074722290039\n",
      "3: Encoding Loss 4.815934181213379, Transition Loss -0.01121828705072403, Classifier Loss 0.1544044464826584, Total Loss 44.3360481262207\n",
      "3: Encoding Loss 6.541104793548584, Transition Loss -0.7348918318748474, Classifier Loss 0.22500479221343994, Total Loss 61.7468147277832\n",
      "3: Encoding Loss 5.023845672607422, Transition Loss -1.565265417098999, Classifier Loss 0.17017871141433716, Total Loss 47.16032409667969\n",
      "3: Encoding Loss 5.68357515335083, Transition Loss -1.080757737159729, Classifier Loss 0.2056550830602646, Total Loss 54.66653060913086\n",
      "3: Encoding Loss 5.665982723236084, Transition Loss -1.2397018671035767, Classifier Loss 0.27897799015045166, Total Loss 61.89320373535156\n",
      "3: Encoding Loss 5.937451362609863, Transition Loss -1.2346901893615723, Classifier Loss 0.1793653964996338, Total Loss 53.56075668334961\n",
      "3: Encoding Loss 5.379709720611572, Transition Loss -0.7295680046081543, Classifier Loss 0.1711401790380478, Total Loss 49.39198303222656\n",
      "3: Encoding Loss 8.586565017700195, Transition Loss -0.07481914758682251, Classifier Loss 0.24714131653308868, Total Loss 76.23348999023438\n",
      "3: Encoding Loss 7.489630699157715, Transition Loss -1.6208871603012085, Classifier Loss 0.1682274043560028, Total Loss 61.75988006591797\n",
      "3: Encoding Loss 5.881800651550293, Transition Loss -1.058706521987915, Classifier Loss 0.18922388553619385, Total Loss 54.212772369384766\n",
      "3: Encoding Loss 6.894198417663574, Transition Loss -0.9724103212356567, Classifier Loss 0.19592832028865814, Total Loss 60.95763397216797\n",
      "3: Encoding Loss 6.201282501220703, Transition Loss -1.1755577325820923, Classifier Loss 0.1890690177679062, Total Loss 56.11412811279297\n",
      "3: Encoding Loss 8.456924438476562, Transition Loss -1.9476457834243774, Classifier Loss 0.17781579494476318, Total Loss 68.52235412597656\n",
      "3: Encoding Loss 5.98170280456543, Transition Loss -1.5195177793502808, Classifier Loss 0.18335294723510742, Total Loss 54.22490310668945\n",
      "3: Encoding Loss 3.9107391834259033, Transition Loss -0.231959730386734, Classifier Loss 0.15678466856479645, Total Loss 39.1428108215332\n",
      "3: Encoding Loss 4.953542232513428, Transition Loss -0.792500913143158, Classifier Loss 0.20938673615455627, Total Loss 50.659610748291016\n",
      "3: Encoding Loss 7.514914035797119, Transition Loss -0.2900121212005615, Classifier Loss 0.19745686650276184, Total Loss 64.8350601196289\n",
      "3: Encoding Loss 6.948785305023193, Transition Loss -0.678831934928894, Classifier Loss 0.23124676942825317, Total Loss 64.8171157836914\n",
      "3: Encoding Loss 5.559884071350098, Transition Loss -0.4851262867450714, Classifier Loss 0.16736048460006714, Total Loss 50.095157623291016\n",
      "3: Encoding Loss 4.74468994140625, Transition Loss -1.3313857316970825, Classifier Loss 0.20741653442382812, Total Loss 49.209259033203125\n",
      "3: Encoding Loss 4.155198574066162, Transition Loss -1.4440910816192627, Classifier Loss 0.14302349090576172, Total Loss 39.232967376708984\n",
      "3: Encoding Loss 8.344152450561523, Transition Loss -1.401676893234253, Classifier Loss 0.162579745054245, Total Loss 66.32233428955078\n",
      "3: Encoding Loss 7.554965019226074, Transition Loss -0.3570224344730377, Classifier Loss 0.20948901772499084, Total Loss 66.27854919433594\n",
      "3: Encoding Loss 3.8519446849823, Transition Loss -1.8300706148147583, Classifier Loss 0.16119924187660217, Total Loss 39.23086166381836\n",
      "3: Encoding Loss 7.636927604675293, Transition Loss -0.4508393108844757, Classifier Loss 0.1553679257631302, Total Loss 61.35818099975586\n",
      "3: Encoding Loss 7.744121551513672, Transition Loss -0.7306029796600342, Classifier Loss 0.21162714064121246, Total Loss 67.62715148925781\n",
      "3: Encoding Loss 5.301636219024658, Transition Loss -1.1092119216918945, Classifier Loss 0.1384042203426361, Total Loss 45.64979553222656\n",
      "3: Encoding Loss 11.651094436645508, Transition Loss -1.5063166618347168, Classifier Loss 0.23087991774082184, Total Loss 92.99395751953125\n",
      "3: Encoding Loss 10.9649019241333, Transition Loss -1.3086875677108765, Classifier Loss 0.15034784376621246, Total Loss 80.82366943359375\n",
      "3: Encoding Loss 8.128031730651855, Transition Loss -0.7662982940673828, Classifier Loss 0.18466469645500183, Total Loss 67.23435974121094\n",
      "3: Encoding Loss 5.5670366287231445, Transition Loss -1.8567415475845337, Classifier Loss 0.16343353688716888, Total Loss 49.74483108520508\n",
      "3: Encoding Loss 4.980797290802002, Transition Loss -1.6983463764190674, Classifier Loss 0.1934477984905243, Total Loss 49.228885650634766\n",
      "3: Encoding Loss 5.666172981262207, Transition Loss -2.612942695617676, Classifier Loss 0.2009306401014328, Total Loss 54.08905792236328\n",
      "3: Encoding Loss 4.941547393798828, Transition Loss -0.9422225952148438, Classifier Loss 0.17729593813419342, Total Loss 47.378501892089844\n",
      "3: Encoding Loss 6.748950004577637, Transition Loss -1.580991268157959, Classifier Loss 0.23355263471603394, Total Loss 63.848331451416016\n",
      "3: Encoding Loss 4.494179725646973, Transition Loss -1.0234616994857788, Classifier Loss 0.2499883472919464, Total Loss 51.963504791259766\n",
      "3: Encoding Loss 7.12127685546875, Transition Loss -1.0692702531814575, Classifier Loss 0.19092538952827454, Total Loss 61.81977081298828\n",
      "3: Encoding Loss 5.465174674987793, Transition Loss -0.5914297699928284, Classifier Loss 0.15674760937690735, Total Loss 48.465576171875\n",
      "3: Encoding Loss 3.35394024848938, Transition Loss -0.5291904211044312, Classifier Loss 0.20263119041919708, Total Loss 40.38655090332031\n",
      "3: Encoding Loss 5.155875205993652, Transition Loss -1.586167812347412, Classifier Loss 0.21080614626407623, Total Loss 52.015235900878906\n",
      "3: Encoding Loss 3.4112563133239746, Transition Loss -1.497201919555664, Classifier Loss 0.17512322962284088, Total Loss 37.97926330566406\n",
      "3: Encoding Loss 3.622541904449463, Transition Loss -1.6938872337341309, Classifier Loss 0.18993687629699707, Total Loss 40.7282600402832\n",
      "3: Encoding Loss 5.78568172454834, Transition Loss -0.6398178935050964, Classifier Loss 0.1559165120124817, Total Loss 50.30548858642578\n",
      "3: Encoding Loss 5.388678550720215, Transition Loss -1.3408782482147217, Classifier Loss 0.1533251404762268, Total Loss 47.6640510559082\n",
      "3: Encoding Loss 5.930252552032471, Transition Loss -1.387150526046753, Classifier Loss 0.2066364288330078, Total Loss 56.244606018066406\n",
      "3: Encoding Loss 4.985947608947754, Transition Loss -1.4499878883361816, Classifier Loss 0.1447344720363617, Total Loss 44.388553619384766\n",
      "3: Encoding Loss 6.593400478363037, Transition Loss -1.295760154724121, Classifier Loss 0.1710706353187561, Total Loss 56.66695022583008\n",
      "3: Encoding Loss 5.578267574310303, Transition Loss -1.3533008098602295, Classifier Loss 0.21026986837387085, Total Loss 54.496055603027344\n",
      "3: Encoding Loss 3.8238978385925293, Transition Loss -1.6310021877288818, Classifier Loss 0.13024592399597168, Total Loss 35.96732711791992\n",
      "3: Encoding Loss 10.46867847442627, Transition Loss -0.6668076515197754, Classifier Loss 0.14879456162452698, Total Loss 77.6912612915039\n",
      "3: Encoding Loss 9.044689178466797, Transition Loss -1.0676149129867554, Classifier Loss 0.20603543519973755, Total Loss 74.87125396728516\n",
      "3: Encoding Loss 5.38546085357666, Transition Loss -0.44657713174819946, Classifier Loss 0.23535415530204773, Total Loss 55.84800338745117\n",
      "3: Encoding Loss 5.292818069458008, Transition Loss -1.0986069440841675, Classifier Loss 0.19316059350967407, Total Loss 51.07252883911133\n",
      "3: Encoding Loss 5.754652976989746, Transition Loss -1.2417434453964233, Classifier Loss 0.17731668055057526, Total Loss 52.25909423828125\n",
      "3: Encoding Loss 6.300976753234863, Transition Loss -1.8244640827178955, Classifier Loss 0.18703565001487732, Total Loss 56.50870132446289\n",
      "3: Encoding Loss 5.4466657638549805, Transition Loss 0.09322990477085114, Classifier Loss 0.15511901676654816, Total Loss 48.229190826416016\n",
      "3: Encoding Loss 5.348954677581787, Transition Loss -1.2290258407592773, Classifier Loss 0.20157210528850555, Total Loss 52.250450134277344\n",
      "3: Encoding Loss 5.417572498321533, Transition Loss -1.9841424226760864, Classifier Loss 0.1339506357908249, Total Loss 45.89970397949219\n",
      "3: Encoding Loss 5.022839069366455, Transition Loss -1.3099079132080078, Classifier Loss 0.18950167298316956, Total Loss 49.0866813659668\n",
      "3: Encoding Loss 4.660000324249268, Transition Loss 0.2387363165616989, Classifier Loss 0.18105173110961914, Total Loss 46.16067123413086\n",
      "3: Encoding Loss 3.465071439743042, Transition Loss -1.2026385068893433, Classifier Loss 0.14900699257850647, Total Loss 35.690650939941406\n",
      "3: Encoding Loss 7.105757236480713, Transition Loss -1.5784685611724854, Classifier Loss 0.16294465959072113, Total Loss 58.928375244140625\n",
      "3: Encoding Loss 6.123782157897949, Transition Loss 0.09831030666828156, Classifier Loss 0.1746838539838791, Total Loss 54.25040817260742\n",
      "3: Encoding Loss 4.297481536865234, Transition Loss -1.407153844833374, Classifier Loss 0.15484657883644104, Total Loss 41.268985748291016\n",
      "3: Encoding Loss 3.2898101806640625, Transition Loss -1.1700397729873657, Classifier Loss 0.1762411743402481, Total Loss 37.36250686645508\n",
      "3: Encoding Loss 2.642211437225342, Transition Loss -0.38049912452697754, Classifier Loss 0.16976365447044373, Total Loss 32.82948303222656\n",
      "3: Encoding Loss 13.217074394226074, Transition Loss -0.3122878670692444, Classifier Loss 0.2631208598613739, Total Loss 105.61441802978516\n",
      "3: Encoding Loss 13.193571090698242, Transition Loss -1.0714775323867798, Classifier Loss 0.18676233291625977, Total Loss 97.83723449707031\n",
      "3: Encoding Loss 11.860570907592773, Transition Loss -0.8429939150810242, Classifier Loss 0.20788392424583435, Total Loss 91.95148468017578\n",
      "3: Encoding Loss 9.163597106933594, Transition Loss -1.20093834400177, Classifier Loss 0.1587107926607132, Total Loss 70.85218811035156\n",
      "3: Encoding Loss 6.9294939041137695, Transition Loss -1.4195945262908936, Classifier Loss 0.15368744730949402, Total Loss 56.94514083862305\n",
      "3: Encoding Loss 5.836987495422363, Transition Loss -0.6300147771835327, Classifier Loss 0.2200758308172226, Total Loss 57.029258728027344\n",
      "3: Encoding Loss 4.959752082824707, Transition Loss -1.3941502571105957, Classifier Loss 0.18556825816631317, Total Loss 48.314781188964844\n",
      "3: Encoding Loss 4.276005744934082, Transition Loss -0.629528284072876, Classifier Loss 0.1834983080625534, Total Loss 44.005615234375\n",
      "3: Encoding Loss 4.262803077697754, Transition Loss 0.06390218436717987, Classifier Loss 0.14393936097621918, Total Loss 39.99631881713867\n",
      "3: Encoding Loss 6.006437301635742, Transition Loss -0.6143616437911987, Classifier Loss 0.13136553764343262, Total Loss 49.17493438720703\n",
      "3: Encoding Loss 5.255007266998291, Transition Loss -1.8354463577270508, Classifier Loss 0.1429690718650818, Total Loss 45.82621765136719\n",
      "3: Encoding Loss 8.511894226074219, Transition Loss -1.5848844051361084, Classifier Loss 0.16550324857234955, Total Loss 67.62106323242188\n",
      "3: Encoding Loss 7.166672706604004, Transition Loss -0.9362898468971252, Classifier Loss 0.18946681916713715, Total Loss 61.946346282958984\n",
      "3: Encoding Loss 4.995086193084717, Transition Loss -1.4581595659255981, Classifier Loss 0.16754890978336334, Total Loss 46.72482681274414\n",
      "3: Encoding Loss 6.968863487243652, Transition Loss -0.7067723274230957, Classifier Loss 0.15626922249794006, Total Loss 57.439823150634766\n",
      "3: Encoding Loss 7.921664237976074, Transition Loss -0.8298054933547974, Classifier Loss 0.16110341250896454, Total Loss 63.63999557495117\n",
      "3: Encoding Loss 6.846639156341553, Transition Loss -1.321288824081421, Classifier Loss 0.13205459713935852, Total Loss 54.284767150878906\n",
      "3: Encoding Loss 6.959246635437012, Transition Loss -1.3486295938491821, Classifier Loss 0.15640771389007568, Total Loss 57.39571762084961\n",
      "3: Encoding Loss 4.633737087249756, Transition Loss -1.991926670074463, Classifier Loss 0.15628580749034882, Total Loss 43.430206298828125\n",
      "3: Encoding Loss 5.683349609375, Transition Loss -0.44288498163223267, Classifier Loss 0.1743643879890442, Total Loss 51.53636169433594\n",
      "3: Encoding Loss 4.088452339172363, Transition Loss -1.0755671262741089, Classifier Loss 0.14853385090827942, Total Loss 39.3836669921875\n",
      "3: Encoding Loss 6.23223876953125, Transition Loss -1.0942635536193848, Classifier Loss 0.18288898468017578, Total Loss 55.68189239501953\n",
      "3: Encoding Loss 6.572569847106934, Transition Loss -0.6006386280059814, Classifier Loss 0.1668032556772232, Total Loss 56.11550521850586\n",
      "3: Encoding Loss 7.4544782638549805, Transition Loss -0.8496572971343994, Classifier Loss 0.2457064688205719, Total Loss 69.29717254638672\n",
      "3: Encoding Loss 6.171870231628418, Transition Loss -1.416912317276001, Classifier Loss 0.18946623802185059, Total Loss 55.97727966308594\n",
      "3: Encoding Loss 7.1451096534729, Transition Loss -1.2494056224822998, Classifier Loss 0.14792239665985107, Total Loss 57.66239929199219\n",
      "3: Encoding Loss 5.545493125915527, Transition Loss -1.2896382808685303, Classifier Loss 0.17250847816467285, Total Loss 50.523292541503906\n",
      "3: Encoding Loss 3.2045435905456543, Transition Loss -1.1946306228637695, Classifier Loss 0.1547049880027771, Total Loss 34.69728469848633\n",
      "3: Encoding Loss 7.781573295593262, Transition Loss -1.7339190244674683, Classifier Loss 0.1468610316514969, Total Loss 61.37485122680664\n",
      "3: Encoding Loss 8.063871383666992, Transition Loss -1.535988211631775, Classifier Loss 0.24203802645206451, Total Loss 72.58641815185547\n",
      "3: Encoding Loss 5.8461079597473145, Transition Loss -1.873105525970459, Classifier Loss 0.10953227430582047, Total Loss 46.02912902832031\n",
      "3: Encoding Loss 3.1920931339263916, Transition Loss -0.9972618818283081, Classifier Loss 0.13544981181621552, Total Loss 32.697139739990234\n",
      "3: Encoding Loss 2.6627180576324463, Transition Loss -2.2708375453948975, Classifier Loss 0.14705538749694824, Total Loss 30.680938720703125\n",
      "3: Encoding Loss 0.7706359624862671, Transition Loss -1.2576403617858887, Classifier Loss 0.16561132669448853, Total Loss 21.184444427490234\n",
      "3: Encoding Loss 6.960171699523926, Transition Loss -1.0110738277435303, Classifier Loss 0.18489864468574524, Total Loss 60.250492095947266\n",
      "3: Encoding Loss 5.775905132293701, Transition Loss -1.1257672309875488, Classifier Loss 0.1819208562374115, Total Loss 52.847068786621094\n",
      "3: Encoding Loss 4.703300476074219, Transition Loss -1.2158539295196533, Classifier Loss 0.12876449525356293, Total Loss 41.095767974853516\n",
      "3: Encoding Loss 6.136591911315918, Transition Loss -1.2533625364303589, Classifier Loss 0.1763131320476532, Total Loss 54.45036697387695\n",
      "3: Encoding Loss 6.664673805236816, Transition Loss -1.0213947296142578, Classifier Loss 0.24148502945899963, Total Loss 64.13613891601562\n",
      "3: Encoding Loss 5.6120805740356445, Transition Loss -0.24785715341567993, Classifier Loss 0.14841961860656738, Total Loss 48.514347076416016\n",
      "3: Encoding Loss 6.5550055503845215, Transition Loss -0.1864040493965149, Classifier Loss 0.184247225522995, Total Loss 57.75468444824219\n",
      "3: Encoding Loss 4.651495933532715, Transition Loss -1.3780019283294678, Classifier Loss 0.15241771936416626, Total Loss 43.15019989013672\n",
      "3: Encoding Loss 9.538840293884277, Transition Loss -0.5785330533981323, Classifier Loss 0.12311744689941406, Total Loss 69.5445556640625\n",
      "3: Encoding Loss 8.967455863952637, Transition Loss -1.2090678215026855, Classifier Loss 0.14087367057800293, Total Loss 67.8916244506836\n",
      "3: Encoding Loss 6.592158317565918, Transition Loss -0.5201900601387024, Classifier Loss 0.21402060985565186, Total Loss 60.954803466796875\n",
      "3: Encoding Loss 5.607123374938965, Transition Loss -0.8504778146743774, Classifier Loss 0.1352112591266632, Total Loss 47.16352844238281\n",
      "3: Encoding Loss 5.22122049331665, Transition Loss -0.37971678376197815, Classifier Loss 0.16460688412189484, Total Loss 47.78785705566406\n",
      "3: Encoding Loss 4.296084403991699, Transition Loss -1.035383939743042, Classifier Loss 0.1633942723274231, Total Loss 42.11552047729492\n",
      "3: Encoding Loss 5.450985908508301, Transition Loss -1.0905325412750244, Classifier Loss 0.18088196218013763, Total Loss 50.793678283691406\n",
      "3: Encoding Loss 3.7484259605407715, Transition Loss -1.3047749996185303, Classifier Loss 0.13804082572460175, Total Loss 36.29411697387695\n",
      "3: Encoding Loss 6.7453155517578125, Transition Loss -1.0486088991165161, Classifier Loss 0.2948499917984009, Total Loss 69.95647430419922\n",
      "3: Encoding Loss 6.672921180725098, Transition Loss -0.6994593143463135, Classifier Loss 0.25126779079437256, Total Loss 65.16402435302734\n",
      "3: Encoding Loss 4.772444248199463, Transition Loss -1.3641245365142822, Classifier Loss 0.18908797204494476, Total Loss 47.54291915893555\n",
      "3: Encoding Loss 5.840018272399902, Transition Loss -1.6525248289108276, Classifier Loss 0.17063239216804504, Total Loss 52.102691650390625\n",
      "3: Encoding Loss 6.498189449310303, Transition Loss -0.6882340312004089, Classifier Loss 0.16761159896850586, Total Loss 55.750022888183594\n",
      "3: Encoding Loss 7.601315498352051, Transition Loss -0.9166885614395142, Classifier Loss 0.19056467711925507, Total Loss 64.66399383544922\n",
      "3: Encoding Loss 6.4026031494140625, Transition Loss -1.184685468673706, Classifier Loss 0.1641135811805725, Total Loss 54.82650375366211\n",
      "3: Encoding Loss 4.993875980377197, Transition Loss -2.1823582649230957, Classifier Loss 0.1779196411371231, Total Loss 47.75434494018555\n",
      "3: Encoding Loss 7.630751609802246, Transition Loss -1.9823360443115234, Classifier Loss 0.17995727062225342, Total Loss 63.77944564819336\n",
      "3: Encoding Loss 6.292514324188232, Transition Loss -0.8716358542442322, Classifier Loss 0.1656533181667328, Total Loss 54.320072174072266\n",
      "3: Encoding Loss 4.103640556335449, Transition Loss -1.7580828666687012, Classifier Loss 0.17521744966506958, Total Loss 42.14289093017578\n",
      "3: Encoding Loss 2.9821505546569824, Transition Loss -1.0204375982284546, Classifier Loss 0.14371077716350555, Total Loss 32.263572692871094\n",
      "3: Encoding Loss 7.67659854888916, Transition Loss -1.7822399139404297, Classifier Loss 0.16068804264068604, Total Loss 62.127681732177734\n",
      "3: Encoding Loss 6.319920063018799, Transition Loss -0.28194695711135864, Classifier Loss 0.1713494062423706, Total Loss 55.05434799194336\n",
      "3: Encoding Loss 6.282817840576172, Transition Loss -0.6135950088500977, Classifier Loss 0.1345781534910202, Total Loss 51.15447998046875\n",
      "3: Encoding Loss 4.782193183898926, Transition Loss -0.826438307762146, Classifier Loss 0.14414510130882263, Total Loss 43.107337951660156\n",
      "3: Encoding Loss 4.5579514503479, Transition Loss -1.4074921607971191, Classifier Loss 0.15700320899486542, Total Loss 43.04746627807617\n",
      "3: Encoding Loss 9.310053825378418, Transition Loss -0.26111623644828796, Classifier Loss 0.21804986894130707, Total Loss 77.66520690917969\n",
      "3: Encoding Loss 7.065296173095703, Transition Loss -0.9810127019882202, Classifier Loss 0.2391994148492813, Total Loss 66.31133270263672\n",
      "3: Encoding Loss 6.289395332336426, Transition Loss -1.9460475444793701, Classifier Loss 0.2433992326259613, Total Loss 62.07551956176758\n",
      "3: Encoding Loss 4.582228183746338, Transition Loss -1.5953961610794067, Classifier Loss 0.13881298899650574, Total Loss 41.37403106689453\n",
      "3: Encoding Loss 8.216085433959961, Transition Loss -1.6074254512786865, Classifier Loss 0.18748702108860016, Total Loss 68.0445785522461\n",
      "3: Encoding Loss 7.813956260681152, Transition Loss -0.7011154890060425, Classifier Loss 0.15721705555915833, Total Loss 62.60516357421875\n",
      "3: Encoding Loss 5.398495197296143, Transition Loss -1.2096590995788574, Classifier Loss 0.1516920030117035, Total Loss 47.559688568115234\n",
      "3: Encoding Loss 4.284150123596191, Transition Loss -1.5470201969146729, Classifier Loss 0.12913747131824493, Total Loss 38.6180305480957\n",
      "3: Encoding Loss 5.830660820007324, Transition Loss -1.2501147985458374, Classifier Loss 0.2194165587425232, Total Loss 56.92512130737305\n",
      "3: Encoding Loss 7.077185153961182, Transition Loss -1.9645661115646362, Classifier Loss 0.17814357578754425, Total Loss 60.27668380737305\n",
      "3: Encoding Loss 4.677467346191406, Transition Loss -0.9548822641372681, Classifier Loss 0.1665249764919281, Total Loss 44.716922760009766\n",
      "3: Encoding Loss 7.056829929351807, Transition Loss -0.48342281579971313, Classifier Loss 0.1948094218969345, Total Loss 61.82172775268555\n",
      "3: Encoding Loss 6.748806953430176, Transition Loss -1.473583459854126, Classifier Loss 0.18088677525520325, Total Loss 58.580928802490234\n",
      "3: Encoding Loss 6.302388668060303, Transition Loss -1.895040512084961, Classifier Loss 0.16485798358917236, Total Loss 54.299373626708984\n",
      "3: Encoding Loss 6.562158107757568, Transition Loss -1.0167540311813354, Classifier Loss 0.14186955988407135, Total Loss 53.55949783325195\n",
      "3: Encoding Loss 5.893093585968018, Transition Loss -0.25042861700057983, Classifier Loss 0.1535869687795639, Total Loss 50.717159271240234\n",
      "3: Encoding Loss 5.7232866287231445, Transition Loss -0.9332219362258911, Classifier Loss 0.192102313041687, Total Loss 53.54957962036133\n",
      "3: Encoding Loss 6.878973484039307, Transition Loss -0.8268895149230957, Classifier Loss 0.12649205327033997, Total Loss 53.92271423339844\n",
      "3: Encoding Loss 6.019639492034912, Transition Loss -1.6598684787750244, Classifier Loss 0.1302483081817627, Total Loss 49.142005920410156\n",
      "3: Encoding Loss 6.950944423675537, Transition Loss -1.3917131423950195, Classifier Loss 0.17947158217430115, Total Loss 59.65227127075195\n",
      "3: Encoding Loss 5.880334854125977, Transition Loss -1.0520967245101929, Classifier Loss 0.21977071464061737, Total Loss 57.25865936279297\n",
      "3: Encoding Loss 5.279541969299316, Transition Loss -1.9621092081069946, Classifier Loss 0.2027403712272644, Total Loss 51.95050048828125\n",
      "3: Encoding Loss 3.6728737354278564, Transition Loss -0.30924493074417114, Classifier Loss 0.18117627501487732, Total Loss 40.154747009277344\n",
      "3: Encoding Loss 3.9553139209747314, Transition Loss -0.6069963574409485, Classifier Loss 0.16290611028671265, Total Loss 40.02225112915039\n",
      "3: Encoding Loss 3.1474905014038086, Transition Loss -1.2146716117858887, Classifier Loss 0.133372500538826, Total Loss 32.22170639038086\n",
      "3: Encoding Loss 8.060446739196777, Transition Loss -1.4564298391342163, Classifier Loss 0.22992463409900665, Total Loss 71.35456848144531\n",
      "3: Encoding Loss 6.167158603668213, Transition Loss -1.4520177841186523, Classifier Loss 0.1655302792787552, Total Loss 53.55540084838867\n",
      "3: Encoding Loss 6.960479736328125, Transition Loss -1.4387710094451904, Classifier Loss 0.17120811343193054, Total Loss 58.883113861083984\n",
      "3: Encoding Loss 4.9072675704956055, Transition Loss -1.178499698638916, Classifier Loss 0.1823977380990982, Total Loss 47.68290710449219\n",
      "3: Encoding Loss 4.809454441070557, Transition Loss -1.4791598320007324, Classifier Loss 0.19146762788295746, Total Loss 48.002899169921875\n",
      "3: Encoding Loss 5.540555953979492, Transition Loss -1.3931931257247925, Classifier Loss 0.19317184388637543, Total Loss 52.55996322631836\n",
      "3: Encoding Loss 5.227880001068115, Transition Loss -1.2136625051498413, Classifier Loss 0.16967499256134033, Total Loss 48.334293365478516\n",
      "3: Encoding Loss 6.006214618682861, Transition Loss -0.9377028346061707, Classifier Loss 0.19333291053771973, Total Loss 55.37020492553711\n",
      "3: Encoding Loss 5.104175090789795, Transition Loss -0.7657361030578613, Classifier Loss 0.2724548876285553, Total Loss 57.870235443115234\n",
      "3: Encoding Loss 7.834552764892578, Transition Loss -1.5306379795074463, Classifier Loss 0.14930184185504913, Total Loss 61.9368896484375\n",
      "3: Encoding Loss 5.529698371887207, Transition Loss -1.6640956401824951, Classifier Loss 0.16200163960456848, Total Loss 49.37769317626953\n",
      "3: Encoding Loss 6.636366844177246, Transition Loss -1.4456934928894043, Classifier Loss 0.14599475264549255, Total Loss 54.41709899902344\n",
      "3: Encoding Loss 4.877292633056641, Transition Loss -0.8953146934509277, Classifier Loss 0.1461603194475174, Total Loss 43.879432678222656\n",
      "3: Encoding Loss 5.162181854248047, Transition Loss -0.9521199464797974, Classifier Loss 0.12856461107730865, Total Loss 43.82917404174805\n",
      "3: Encoding Loss 3.8823390007019043, Transition Loss -0.8096945285797119, Classifier Loss 0.15527746081352234, Total Loss 38.82145690917969\n",
      "3: Encoding Loss 6.434133052825928, Transition Loss -0.8992666602134705, Classifier Loss 0.1558476686477661, Total Loss 54.189208984375\n",
      "3: Encoding Loss 6.644689083099365, Transition Loss -1.1926984786987305, Classifier Loss 0.21593211591243744, Total Loss 61.460872650146484\n",
      "3: Encoding Loss 4.053759574890137, Transition Loss -1.4670696258544922, Classifier Loss 0.2127159833908081, Total Loss 45.593570709228516\n",
      "3: Encoding Loss 4.019186973571777, Transition Loss -0.6832659244537354, Classifier Loss 0.1505376249551773, Total Loss 39.168609619140625\n",
      "3: Encoding Loss 7.504380702972412, Transition Loss -1.394391417503357, Classifier Loss 0.21728622913360596, Total Loss 66.75435638427734\n",
      "3: Encoding Loss 6.713735103607178, Transition Loss -0.7850039005279541, Classifier Loss 0.14479924738407135, Total Loss 54.76202392578125\n",
      "3: Encoding Loss 5.857506275177002, Transition Loss -0.9610459208488464, Classifier Loss 0.13843707740306854, Total Loss 48.98836135864258\n",
      "3: Encoding Loss 6.535854339599609, Transition Loss -0.6802514791488647, Classifier Loss 0.183829203248024, Total Loss 57.597774505615234\n",
      "3: Encoding Loss 5.180819511413574, Transition Loss -2.3056838512420654, Classifier Loss 0.1812446266412735, Total Loss 49.208457946777344\n",
      "3: Encoding Loss 3.21992564201355, Transition Loss 0.051466211676597595, Classifier Loss 0.16501079499721527, Total Loss 35.84122085571289\n",
      "3: Encoding Loss 3.3890063762664795, Transition Loss -1.699256420135498, Classifier Loss 0.1499837189912796, Total Loss 35.33173370361328\n",
      "3: Encoding Loss 8.854721069335938, Transition Loss -2.0278968811035156, Classifier Loss 0.10569688677787781, Total Loss 63.697208404541016\n",
      "3: Encoding Loss 11.458949089050293, Transition Loss -1.6399108171463013, Classifier Loss 0.159332737326622, Total Loss 84.68631744384766\n",
      "3: Encoding Loss 9.477278709411621, Transition Loss -1.355277419090271, Classifier Loss 0.13930417597293854, Total Loss 70.79354858398438\n",
      "3: Encoding Loss 6.535233974456787, Transition Loss -0.9340832233428955, Classifier Loss 0.20380771160125732, Total Loss 59.59180450439453\n",
      "3: Encoding Loss 6.560244083404541, Transition Loss -0.9084871411323547, Classifier Loss 0.13812373578548431, Total Loss 53.17347717285156\n",
      "3: Encoding Loss 4.544079780578613, Transition Loss -0.8945270776748657, Classifier Loss 0.1403304785490036, Total Loss 41.29716873168945\n",
      "3: Encoding Loss 8.242341995239258, Transition Loss -1.6948657035827637, Classifier Loss 0.1579693853855133, Total Loss 65.25031280517578\n",
      "3: Encoding Loss 6.397022247314453, Transition Loss -1.1932913064956665, Classifier Loss 0.13356538116931915, Total Loss 51.73819351196289\n",
      "3: Encoding Loss 6.475072383880615, Transition Loss -1.4959652423858643, Classifier Loss 0.18126317858695984, Total Loss 56.97615432739258\n",
      "3: Encoding Loss 4.746415138244629, Transition Loss -1.359292984008789, Classifier Loss 0.11738668382167816, Total Loss 40.21661376953125\n",
      "3: Encoding Loss 5.551512718200684, Transition Loss -0.6658735275268555, Classifier Loss 0.12055002152919769, Total Loss 45.36381530761719\n",
      "3: Encoding Loss 5.5229973793029785, Transition Loss -1.3629785776138306, Classifier Loss 0.16385824978351593, Total Loss 49.52326583862305\n",
      "3: Encoding Loss 4.72553014755249, Transition Loss -1.266378402709961, Classifier Loss 0.13252539932727814, Total Loss 41.6052131652832\n",
      "3: Encoding Loss 3.4607880115509033, Transition Loss -1.6471079587936401, Classifier Loss 0.16186806559562683, Total Loss 36.95087432861328\n",
      "3: Encoding Loss 6.805996894836426, Transition Loss -2.362823963165283, Classifier Loss 0.14057710766792297, Total Loss 54.89274597167969\n",
      "3: Encoding Loss 5.340392112731934, Transition Loss -0.7406474351882935, Classifier Loss 0.20558860898017883, Total Loss 52.60091781616211\n",
      "3: Encoding Loss 5.137840270996094, Transition Loss -0.8593764305114746, Classifier Loss 0.21274392306804657, Total Loss 52.10109329223633\n",
      "3: Encoding Loss 5.590060234069824, Transition Loss -0.9594240784645081, Classifier Loss 0.16599178314208984, Total Loss 50.139156341552734\n",
      "3: Encoding Loss 4.419848442077637, Transition Loss -0.5604138970375061, Classifier Loss 0.14159739017486572, Total Loss 40.67860794067383\n",
      "3: Encoding Loss 8.316363334655762, Transition Loss -0.37602144479751587, Classifier Loss 0.18413905799388885, Total Loss 68.31193542480469\n",
      "3: Encoding Loss 5.598361015319824, Transition Loss -1.0979580879211426, Classifier Loss 0.148736834526062, Total Loss 48.46341323852539\n",
      "3: Encoding Loss 7.8034820556640625, Transition Loss -0.5045762658119202, Classifier Loss 0.23363041877746582, Total Loss 70.18373107910156\n",
      "3: Encoding Loss 6.264432907104492, Transition Loss -1.9965851306915283, Classifier Loss 0.20512829720973969, Total Loss 58.098628997802734\n",
      "3: Encoding Loss 6.189859867095947, Transition Loss -1.0301213264465332, Classifier Loss 0.17078009247779846, Total Loss 54.216758728027344\n",
      "3: Encoding Loss 5.892214298248291, Transition Loss -1.454511046409607, Classifier Loss 0.16710536181926727, Total Loss 52.06324005126953\n",
      "3: Encoding Loss 3.535698413848877, Transition Loss -1.0276141166687012, Classifier Loss 0.1760759949684143, Total Loss 38.821380615234375\n",
      "3: Encoding Loss 4.056240081787109, Transition Loss -0.7240805625915527, Classifier Loss 0.13667236268520355, Total Loss 38.00438690185547\n",
      "3: Encoding Loss 4.500368595123291, Transition Loss -0.7668836116790771, Classifier Loss 0.16756151616573334, Total Loss 43.758056640625\n",
      "3: Encoding Loss 4.46322774887085, Transition Loss -2.004692316055298, Classifier Loss 0.12444674223661423, Total Loss 39.22323989868164\n",
      "3: Encoding Loss 5.43787145614624, Transition Loss -1.2727792263031006, Classifier Loss 0.1920154094696045, Total Loss 51.82826614379883\n",
      "3: Encoding Loss 5.3732147216796875, Transition Loss -1.3375905752182007, Classifier Loss 0.1417751908302307, Total Loss 46.41627502441406\n",
      "3: Encoding Loss 3.9198806285858154, Transition Loss -0.7095354199409485, Classifier Loss 0.14085663855075836, Total Loss 37.60466766357422\n",
      "3: Encoding Loss 4.170917510986328, Transition Loss -1.6223151683807373, Classifier Loss 0.10908837616443634, Total Loss 35.93369674682617\n",
      "3: Encoding Loss 6.342921257019043, Transition Loss -1.0460302829742432, Classifier Loss 0.1946636140346527, Total Loss 57.52347183227539\n",
      "3: Encoding Loss 5.583531379699707, Transition Loss -0.41221094131469727, Classifier Loss 0.10013008117675781, Total Loss 43.514034271240234\n",
      "3: Encoding Loss 7.133357048034668, Transition Loss -0.2508206367492676, Classifier Loss 0.18004444241523743, Total Loss 60.80448913574219\n",
      "3: Encoding Loss 6.2614545822143555, Transition Loss -1.4340063333511353, Classifier Loss 0.3075208067893982, Total Loss 68.32023620605469\n",
      "3: Encoding Loss 6.559786319732666, Transition Loss -1.4370487928390503, Classifier Loss 0.1262418031692505, Total Loss 51.982322692871094\n",
      "3: Encoding Loss 5.4009199142456055, Transition Loss -1.403688669204712, Classifier Loss 0.16778118908405304, Total Loss 49.18307876586914\n",
      "3: Encoding Loss 4.8387451171875, Transition Loss -1.0017930269241333, Classifier Loss 0.1462559551000595, Total Loss 43.65766525268555\n",
      "3: Encoding Loss 5.738638401031494, Transition Loss -1.6793580055236816, Classifier Loss 0.24311617016792297, Total Loss 58.742774963378906\n",
      "3: Encoding Loss 4.90226411819458, Transition Loss -0.9800598621368408, Classifier Loss 0.14997738599777222, Total Loss 44.41093063354492\n",
      "3: Encoding Loss 7.195076942443848, Transition Loss -1.646000862121582, Classifier Loss 0.17059358954429626, Total Loss 60.22916030883789\n",
      "3: Encoding Loss 6.44330358505249, Transition Loss -1.2887868881225586, Classifier Loss 0.17076361179351807, Total Loss 55.73566818237305\n",
      "3: Encoding Loss 6.010495662689209, Transition Loss -1.458687424659729, Classifier Loss 0.12717004120349884, Total Loss 48.779396057128906\n",
      "3: Encoding Loss 6.8633246421813965, Transition Loss -0.8420282006263733, Classifier Loss 0.1965257227420807, Total Loss 60.83218765258789\n",
      "3: Encoding Loss 2.837433338165283, Transition Loss -1.4443027973175049, Classifier Loss 0.1629578024148941, Total Loss 33.31980514526367\n",
      "3: Encoding Loss 3.4742705821990967, Transition Loss -0.7821053266525269, Classifier Loss 0.13298867642879486, Total Loss 34.14418029785156\n",
      "3: Encoding Loss 6.211434841156006, Transition Loss -0.6169518232345581, Classifier Loss 0.1668601632118225, Total Loss 53.95438003540039\n",
      "3: Encoding Loss 5.725214958190918, Transition Loss -0.05360221862792969, Classifier Loss 0.1582583636045456, Total Loss 50.17710494995117\n",
      "3: Encoding Loss 5.408590793609619, Transition Loss -0.944122314453125, Classifier Loss 0.20445355772972107, Total Loss 52.89652633666992\n",
      "3: Encoding Loss 6.277599334716797, Transition Loss -1.4765516519546509, Classifier Loss 0.1484166830778122, Total Loss 52.50667190551758\n",
      "3: Encoding Loss 3.9501452445983887, Transition Loss -1.864018201828003, Classifier Loss 0.16792066395282745, Total Loss 40.49219512939453\n",
      "3: Encoding Loss 3.3957431316375732, Transition Loss -1.1844761371612549, Classifier Loss 0.14907553791999817, Total Loss 35.28153991699219\n",
      "3: Encoding Loss 9.602216720581055, Transition Loss -1.0099356174468994, Classifier Loss 0.19762691855430603, Total Loss 77.3755874633789\n",
      "3: Encoding Loss 5.735299110412598, Transition Loss -0.3799998164176941, Classifier Loss 0.203063502907753, Total Loss 54.717994689941406\n",
      "3: Encoding Loss 4.949977874755859, Transition Loss -1.5681638717651367, Classifier Loss 0.10073632001876831, Total Loss 39.77287673950195\n",
      "3: Encoding Loss 6.451570987701416, Transition Loss -0.28370559215545654, Classifier Loss 0.17999941110610962, Total Loss 56.709251403808594\n",
      "3: Encoding Loss 3.228367567062378, Transition Loss 0.047430843114852905, Classifier Loss 0.1385176181793213, Total Loss 33.24094009399414\n",
      "3: Encoding Loss 3.851795196533203, Transition Loss -1.045737385749817, Classifier Loss 0.16742753982543945, Total Loss 39.85310363769531\n",
      "3: Encoding Loss 6.281723976135254, Transition Loss -1.056087851524353, Classifier Loss 0.15824466943740845, Total Loss 53.51438903808594\n",
      "3: Encoding Loss 4.35247278213501, Transition Loss -1.1010873317718506, Classifier Loss 0.17065303027629852, Total Loss 43.17970275878906\n",
      "3: Encoding Loss 7.167635440826416, Transition Loss -1.0126310586929321, Classifier Loss 0.16283103823661804, Total Loss 59.28851318359375\n",
      "3: Encoding Loss 7.4230499267578125, Transition Loss -1.1830222606658936, Classifier Loss 0.18092522025108337, Total Loss 62.630348205566406\n",
      "3: Encoding Loss 4.98586368560791, Transition Loss -0.835844099521637, Classifier Loss 0.11611241102218628, Total Loss 41.52608871459961\n",
      "3: Encoding Loss 4.723223686218262, Transition Loss -0.6906867027282715, Classifier Loss 0.20034904778003693, Total Loss 48.37397384643555\n",
      "3: Encoding Loss 6.462310791015625, Transition Loss -2.3220584392547607, Classifier Loss 0.22249628603458405, Total Loss 61.02256393432617\n",
      "3: Encoding Loss 6.8896942138671875, Transition Loss -1.2736188173294067, Classifier Loss 0.15321387350559235, Total Loss 56.65904235839844\n",
      "3: Encoding Loss 5.317337989807129, Transition Loss -1.0289396047592163, Classifier Loss 0.14777731895446777, Total Loss 46.68134689331055\n",
      "3: Encoding Loss 7.085258483886719, Transition Loss -1.7066401243209839, Classifier Loss 0.17925004661083221, Total Loss 60.43587112426758\n",
      "3: Encoding Loss 5.324279308319092, Transition Loss -0.5990233421325684, Classifier Loss 0.13933929800987244, Total Loss 45.87936782836914\n",
      "3: Encoding Loss 2.8159260749816895, Transition Loss -1.4886081218719482, Classifier Loss 0.1475265473127365, Total Loss 31.64761734008789\n",
      "3: Encoding Loss 7.231837272644043, Transition Loss -1.3979990482330322, Classifier Loss 0.19218824803829193, Total Loss 62.60928726196289\n",
      "3: Encoding Loss 9.06181526184082, Transition Loss -2.514601230621338, Classifier Loss 0.22496074438095093, Total Loss 76.86595916748047\n",
      "3: Encoding Loss 5.953744411468506, Transition Loss -1.3307217359542847, Classifier Loss 0.14159107208251953, Total Loss 49.88104248046875\n",
      "3: Encoding Loss 5.306964874267578, Transition Loss -1.0861307382583618, Classifier Loss 0.1495363563299179, Total Loss 46.79499053955078\n",
      "3: Encoding Loss 5.560810089111328, Transition Loss -0.7772566080093384, Classifier Loss 0.18543227016925812, Total Loss 51.90777587890625\n",
      "3: Encoding Loss 6.739236354827881, Transition Loss -0.9497106075286865, Classifier Loss 0.26090991497039795, Total Loss 66.52603149414062\n",
      "3: Encoding Loss 6.443286895751953, Transition Loss -1.3734296560287476, Classifier Loss 0.22325092554092407, Total Loss 60.9842643737793\n",
      "3: Encoding Loss 6.683488368988037, Transition Loss -1.5330491065979004, Classifier Loss 0.18262340128421783, Total Loss 58.3626594543457\n",
      "3: Encoding Loss 5.578587532043457, Transition Loss -1.3262890577316284, Classifier Loss 0.16082963347434998, Total Loss 49.553958892822266\n",
      "3: Encoding Loss 5.989860534667969, Transition Loss -0.5425058603286743, Classifier Loss 0.1864859163761139, Total Loss 54.5875358581543\n",
      "3: Encoding Loss 7.112743377685547, Transition Loss -0.5891327857971191, Classifier Loss 0.2123226821422577, Total Loss 63.90849304199219\n",
      "3: Encoding Loss 5.322904586791992, Transition Loss -0.6212809085845947, Classifier Loss 0.13070650398731232, Total Loss 45.00783157348633\n",
      "3: Encoding Loss 8.233101844787598, Transition Loss -1.7636768817901611, Classifier Loss 0.1336919367313385, Total Loss 62.7671012878418\n",
      "3: Encoding Loss 7.019445896148682, Transition Loss -0.875084400177002, Classifier Loss 0.17170685529708862, Total Loss 59.287010192871094\n",
      "3: Encoding Loss 4.667692184448242, Transition Loss -1.197609543800354, Classifier Loss 0.15646913647651672, Total Loss 43.652587890625\n",
      "3: Encoding Loss 4.989320755004883, Transition Loss -0.6566581130027771, Classifier Loss 0.2097640484571457, Total Loss 50.91206741333008\n",
      "3: Encoding Loss 5.630715370178223, Transition Loss -1.0151528120040894, Classifier Loss 0.1491105705499649, Total Loss 48.6949462890625\n",
      "3: Encoding Loss 5.148078441619873, Transition Loss -1.3691297769546509, Classifier Loss 0.21437446773052216, Total Loss 52.32537078857422\n",
      "3: Encoding Loss 11.414915084838867, Transition Loss -1.48219895362854, Classifier Loss 0.167474627494812, Total Loss 85.23635864257812\n",
      "3: Encoding Loss 8.787546157836914, Transition Loss -0.9841648936271667, Classifier Loss 0.11517683416604996, Total Loss 64.24256134033203\n",
      "3: Encoding Loss 6.3114752769470215, Transition Loss 0.1876702904701233, Classifier Loss 0.13151951134204865, Total Loss 51.09587478637695\n",
      "3: Encoding Loss 4.947129249572754, Transition Loss -1.8525173664093018, Classifier Loss 0.16486823558807373, Total Loss 46.168861389160156\n",
      "3: Encoding Loss 6.6810784339904785, Transition Loss -1.5170691013336182, Classifier Loss 0.15580502152442932, Total Loss 55.66636657714844\n",
      "3: Encoding Loss 4.793032169342041, Transition Loss -0.40590688586235046, Classifier Loss 0.15932966768741608, Total Loss 44.69099807739258\n",
      "3: Encoding Loss 4.700196743011475, Transition Loss -1.1569197177886963, Classifier Loss 0.20349419116973877, Total Loss 48.55013656616211\n",
      "3: Encoding Loss 9.698705673217773, Transition Loss -1.2219433784484863, Classifier Loss 0.13330954313278198, Total Loss 71.522705078125\n",
      "3: Encoding Loss 6.441126346588135, Transition Loss -1.5902893543243408, Classifier Loss 0.16495540738105774, Total Loss 55.14166259765625\n",
      "3: Encoding Loss 6.30071496963501, Transition Loss -1.2411298751831055, Classifier Loss 0.13057376444339752, Total Loss 50.86117172241211\n",
      "3: Encoding Loss 6.391193866729736, Transition Loss -1.8222216367721558, Classifier Loss 0.1207316666841507, Total Loss 50.41960144042969\n",
      "3: Encoding Loss 5.218830108642578, Transition Loss -1.6032414436340332, Classifier Loss 0.13126954436302185, Total Loss 44.43929672241211\n",
      "3: Encoding Loss 5.38906192779541, Transition Loss -1.100215196609497, Classifier Loss 0.10166645050048828, Total Loss 42.500579833984375\n",
      "3: Encoding Loss 4.360915660858154, Transition Loss -1.725205659866333, Classifier Loss 0.17206528782844543, Total Loss 43.371334075927734\n",
      "3: Encoding Loss 6.574287414550781, Transition Loss -0.5365666747093201, Classifier Loss 0.153377965092659, Total Loss 54.78330612182617\n",
      "3: Encoding Loss 5.9638495445251465, Transition Loss -0.8411741852760315, Classifier Loss 0.15013694763183594, Total Loss 50.79645919799805\n",
      "3: Encoding Loss 6.082055568695068, Transition Loss -2.3302323818206787, Classifier Loss 0.16075558960437775, Total Loss 52.56696319580078\n",
      "3: Encoding Loss 7.791088104248047, Transition Loss -2.163987874984741, Classifier Loss 0.15915314853191376, Total Loss 62.66097640991211\n",
      "3: Encoding Loss 6.5685882568359375, Transition Loss -0.3709982633590698, Classifier Loss 0.11645292490720749, Total Loss 51.05667495727539\n",
      "3: Encoding Loss 4.957884788513184, Transition Loss -1.5324290990829468, Classifier Loss 0.18613696098327637, Total Loss 48.36039352416992\n",
      "3: Encoding Loss 4.983053684234619, Transition Loss -1.2943223714828491, Classifier Loss 0.14156563580036163, Total Loss 44.05436706542969\n",
      "3: Encoding Loss 3.98791241645813, Transition Loss -1.4843840599060059, Classifier Loss 0.1335432529449463, Total Loss 37.28120422363281\n",
      "3: Encoding Loss 5.3120574951171875, Transition Loss -2.085930585861206, Classifier Loss 0.18101271986961365, Total Loss 49.972782135009766\n",
      "3: Encoding Loss 6.89526891708374, Transition Loss -1.7459415197372437, Classifier Loss 0.15934549272060394, Total Loss 57.30546951293945\n",
      "3: Encoding Loss 6.667407989501953, Transition Loss -1.939407229423523, Classifier Loss 0.17117413878440857, Total Loss 57.121089935302734\n",
      "3: Encoding Loss 7.6991753578186035, Transition Loss -0.9540406465530396, Classifier Loss 0.18099193274974823, Total Loss 64.29386901855469\n",
      "3: Encoding Loss 5.208446025848389, Transition Loss 0.06020798534154892, Classifier Loss 0.17007917165756226, Total Loss 48.282676696777344\n",
      "3: Encoding Loss 5.346665382385254, Transition Loss -1.853545069694519, Classifier Loss 0.1572646200656891, Total Loss 47.80571746826172\n",
      "3: Encoding Loss 5.3792877197265625, Transition Loss -1.5767450332641602, Classifier Loss 0.18365046381950378, Total Loss 50.640140533447266\n",
      "3: Encoding Loss 4.949265480041504, Transition Loss -1.4361858367919922, Classifier Loss 0.1295243799686432, Total Loss 42.647457122802734\n",
      "3: Encoding Loss 4.235555648803711, Transition Loss -1.7082078456878662, Classifier Loss 0.12061889469623566, Total Loss 37.474544525146484\n",
      "3: Encoding Loss 3.99640154838562, Transition Loss -1.9201501607894897, Classifier Loss 0.11304327845573425, Total Loss 35.2819709777832\n",
      "3: Encoding Loss 4.274845123291016, Transition Loss -0.5481129288673401, Classifier Loss 0.18674303591251373, Total Loss 44.323158264160156\n",
      "3: Encoding Loss 4.3961052894592285, Transition Loss -0.912075400352478, Classifier Loss 0.14886577427387238, Total Loss 41.26284408569336\n",
      "3: Encoding Loss 6.762602806091309, Transition Loss -0.49703294038772583, Classifier Loss 0.16180656850337982, Total Loss 56.75607681274414\n",
      "3: Encoding Loss 5.183642387390137, Transition Loss -0.885799765586853, Classifier Loss 0.150044247508049, Total Loss 46.105926513671875\n",
      "3: Encoding Loss 8.058374404907227, Transition Loss -1.1659717559814453, Classifier Loss 0.1499805748462677, Total Loss 63.34783935546875\n",
      "3: Encoding Loss 7.435354709625244, Transition Loss -0.8086565732955933, Classifier Loss 0.15916450321674347, Total Loss 60.528255462646484\n",
      "3: Encoding Loss 5.309502124786377, Transition Loss -1.3726227283477783, Classifier Loss 0.21091876924037933, Total Loss 52.948341369628906\n",
      "3: Encoding Loss 6.038399696350098, Transition Loss -0.7135412693023682, Classifier Loss 0.1714155077934265, Total Loss 53.37166213989258\n",
      "3: Encoding Loss 7.546202659606934, Transition Loss -1.837935209274292, Classifier Loss 0.1938350647687912, Total Loss 64.65998840332031\n",
      "3: Encoding Loss 5.431506156921387, Transition Loss -1.311658263206482, Classifier Loss 0.12004682421684265, Total Loss 44.59319305419922\n",
      "3: Encoding Loss 8.398848533630371, Transition Loss -0.7543529868125916, Classifier Loss 0.2268560826778412, Total Loss 73.07839965820312\n",
      "3: Encoding Loss 6.8750810623168945, Transition Loss -0.6803799271583557, Classifier Loss 0.21340665221214294, Total Loss 62.590885162353516\n",
      "3: Encoding Loss 5.164087295532227, Transition Loss -1.4942171573638916, Classifier Loss 0.1738092303276062, Total Loss 48.36484909057617\n",
      "3: Encoding Loss 4.449685573577881, Transition Loss -2.14080810546875, Classifier Loss 0.2008238434791565, Total Loss 46.77964401245117\n",
      "3: Encoding Loss 7.372325897216797, Transition Loss -1.4833791255950928, Classifier Loss 0.19459165632724762, Total Loss 63.692527770996094\n",
      "3: Encoding Loss 5.08141565322876, Transition Loss -0.8194468021392822, Classifier Loss 0.14941179752349854, Total Loss 45.429344177246094\n",
      "3: Encoding Loss 2.855660915374756, Transition Loss -1.0597546100616455, Classifier Loss 0.1318819224834442, Total Loss 30.321735382080078\n",
      "3: Encoding Loss 3.5161356925964355, Transition Loss -1.3955897092819214, Classifier Loss 0.14441178739070892, Total Loss 35.537437438964844\n",
      "3: Encoding Loss 4.227272987365723, Transition Loss -1.2431480884552002, Classifier Loss 0.14610294997692108, Total Loss 39.97343826293945\n",
      "3: Encoding Loss 9.128107070922852, Transition Loss 0.06691409647464752, Classifier Loss 0.17513912916183472, Total Loss 72.30931854248047\n",
      "3: Encoding Loss 6.634016990661621, Transition Loss -0.15571196377277374, Classifier Loss 0.17984101176261902, Total Loss 57.788143157958984\n",
      "3: Encoding Loss 4.834861755371094, Transition Loss -0.005951136350631714, Classifier Loss 0.12497182190418243, Total Loss 41.506351470947266\n",
      "3: Encoding Loss 8.30851936340332, Transition Loss -0.6159728765487671, Classifier Loss 0.26575103402137756, Total Loss 76.42597961425781\n",
      "3: Encoding Loss 5.736214637756348, Transition Loss -1.264917254447937, Classifier Loss 0.12679296731948853, Total Loss 47.0960807800293\n",
      "3: Encoding Loss 5.343531608581543, Transition Loss -1.7752588987350464, Classifier Loss 0.1701274812221527, Total Loss 49.0732307434082\n",
      "3: Encoding Loss 5.961759090423584, Transition Loss -1.135232925415039, Classifier Loss 0.17917682230472565, Total Loss 53.68778610229492\n",
      "3: Encoding Loss 5.31927490234375, Transition Loss -1.3319467306137085, Classifier Loss 0.09731769561767578, Total Loss 41.646888732910156\n",
      "3: Encoding Loss 4.7556657791137695, Transition Loss -0.6520445346832275, Classifier Loss 0.12472530454397202, Total Loss 41.006263732910156\n",
      "3: Encoding Loss 6.93366003036499, Transition Loss -0.7759950160980225, Classifier Loss 0.18085159361362457, Total Loss 59.68681335449219\n",
      "3: Encoding Loss 5.007126808166504, Transition Loss -0.4061118960380554, Classifier Loss 0.11792293936014175, Total Loss 41.83488845825195\n",
      "3: Encoding Loss 7.165287971496582, Transition Loss 0.030823171138763428, Classifier Loss 0.12229141592979431, Total Loss 55.23320007324219\n",
      "3: Encoding Loss 6.073414325714111, Transition Loss -1.4855157136917114, Classifier Loss 0.15714724361896515, Total Loss 52.15461730957031\n",
      "3: Encoding Loss 4.779036998748779, Transition Loss -1.3539111614227295, Classifier Loss 0.18248814344406128, Total Loss 46.92249298095703\n",
      "3: Encoding Loss 6.3203020095825195, Transition Loss -1.5512621402740479, Classifier Loss 0.13140302896499634, Total Loss 51.06149673461914\n",
      "3: Encoding Loss 4.410340785980225, Transition Loss -1.1728426218032837, Classifier Loss 0.1847996562719345, Total Loss 44.9415397644043\n",
      "3: Encoding Loss 2.5505716800689697, Transition Loss -1.429966926574707, Classifier Loss 0.1534465253353119, Total Loss 30.647510528564453\n",
      "3: Encoding Loss 3.5134682655334473, Transition Loss -1.1671252250671387, Classifier Loss 0.0990007072687149, Total Loss 30.98041343688965\n",
      "3: Encoding Loss 7.690062046051025, Transition Loss -1.8035471439361572, Classifier Loss 0.16898803412914276, Total Loss 63.038455963134766\n",
      "3: Encoding Loss 5.200379848480225, Transition Loss -1.5384360551834106, Classifier Loss 0.11378452926874161, Total Loss 42.58012008666992\n",
      "3: Encoding Loss 2.4315712451934814, Transition Loss -1.7060010433197021, Classifier Loss 0.11921946704387665, Total Loss 26.510692596435547\n",
      "3: Encoding Loss 6.6800031661987305, Transition Loss -1.2904560565948486, Classifier Loss 0.14725005626678467, Total Loss 54.80451202392578\n",
      "3: Encoding Loss 4.556181907653809, Transition Loss -0.7221381664276123, Classifier Loss 0.1343030035495758, Total Loss 40.76710510253906\n",
      "3: Encoding Loss 6.2015275955200195, Transition Loss -0.8381438255310059, Classifier Loss 0.1705225706100464, Total Loss 54.26108932495117\n",
      "3: Encoding Loss 5.714252471923828, Transition Loss -0.8115394115447998, Classifier Loss 0.1289774477481842, Total Loss 47.18293380737305\n",
      "3: Encoding Loss 5.508543491363525, Transition Loss -1.9329891204833984, Classifier Loss 0.1895299255847931, Total Loss 52.003482818603516\n",
      "3: Encoding Loss 5.93705940246582, Transition Loss -0.6944587826728821, Classifier Loss 0.15953342616558075, Total Loss 51.57542037963867\n",
      "3: Encoding Loss 5.131885051727295, Transition Loss -1.1805243492126465, Classifier Loss 0.23358076810836792, Total Loss 54.1489143371582\n",
      "3: Encoding Loss 4.686192989349365, Transition Loss -2.0135114192962646, Classifier Loss 0.13927456736564636, Total Loss 42.0438117980957\n",
      "3: Encoding Loss 4.01768159866333, Transition Loss -1.1766583919525146, Classifier Loss 0.10066700726747513, Total Loss 34.17232131958008\n",
      "3: Encoding Loss 8.173820495605469, Transition Loss -0.9623250961303711, Classifier Loss 0.2214755117893219, Total Loss 71.19009399414062\n",
      "3: Encoding Loss 7.811863899230957, Transition Loss -0.9418168067932129, Classifier Loss 0.1940871924161911, Total Loss 66.27953338623047\n",
      "3: Encoding Loss 5.512542724609375, Transition Loss -1.095765233039856, Classifier Loss 0.2056037187576294, Total Loss 53.635189056396484\n",
      "3: Encoding Loss 3.670790433883667, Transition Loss -1.4057493209838867, Classifier Loss 0.11975272744894028, Total Loss 33.999454498291016\n",
      "3: Encoding Loss 12.4419527053833, Transition Loss -0.9837732315063477, Classifier Loss 0.1846788078546524, Total Loss 93.11920166015625\n",
      "3: Encoding Loss 11.370945930480957, Transition Loss -2.0546271800994873, Classifier Loss 0.14328572154045105, Total Loss 82.55342864990234\n",
      "3: Encoding Loss 6.723901271820068, Transition Loss -1.4898757934570312, Classifier Loss 0.18629023432731628, Total Loss 58.971839904785156\n",
      "3: Encoding Loss 6.7734575271606445, Transition Loss -1.55291748046875, Classifier Loss 0.17967098951339722, Total Loss 58.60722351074219\n",
      "3: Encoding Loss 7.576513767242432, Transition Loss -1.30753755569458, Classifier Loss 0.17008499801158905, Total Loss 62.46706008911133\n",
      "3: Encoding Loss 5.636054515838623, Transition Loss -0.9302157163619995, Classifier Loss 0.12246087938547134, Total Loss 46.062042236328125\n",
      "3: Encoding Loss 7.756204128265381, Transition Loss -1.880384922027588, Classifier Loss 0.1530875563621521, Total Loss 61.84523391723633\n",
      "3: Encoding Loss 6.9875807762146, Transition Loss -1.259905219078064, Classifier Loss 0.21756458282470703, Total Loss 63.68144226074219\n",
      "3: Encoding Loss 7.343936920166016, Transition Loss -1.6689373254776, Classifier Loss 0.12648098170757294, Total Loss 56.71105194091797\n",
      "3: Encoding Loss 6.709735870361328, Transition Loss -0.5144338011741638, Classifier Loss 0.2236236035823822, Total Loss 62.620567321777344\n",
      "3: Encoding Loss 6.096022605895996, Transition Loss -1.53969407081604, Classifier Loss 0.11172149330377579, Total Loss 47.74767303466797\n",
      "3: Encoding Loss 5.221241474151611, Transition Loss -1.1883906126022339, Classifier Loss 0.11879829317331314, Total Loss 43.20680236816406\n",
      "3: Encoding Loss 4.365774631500244, Transition Loss -1.8514072895050049, Classifier Loss 0.10874351859092712, Total Loss 37.068260192871094\n",
      "3: Encoding Loss 4.490996837615967, Transition Loss -0.9340322613716125, Classifier Loss 0.15871790051460266, Total Loss 42.81739807128906\n",
      "3: Encoding Loss 4.197847366333008, Transition Loss -2.2374067306518555, Classifier Loss 0.1425686776638031, Total Loss 39.443058013916016\n",
      "3: Encoding Loss 8.668010711669922, Transition Loss -0.9187776446342468, Classifier Loss 0.15350474417209625, Total Loss 67.3581771850586\n",
      "3: Encoding Loss 6.051058292388916, Transition Loss -1.3819364309310913, Classifier Loss 0.15564167499542236, Total Loss 51.869964599609375\n",
      "3: Encoding Loss 3.4409496784210205, Transition Loss -0.9180705547332764, Classifier Loss 0.11796444654464722, Total Loss 32.441776275634766\n",
      "3: Encoding Loss 7.818169593811035, Transition Loss -1.7755335569381714, Classifier Loss 0.13700851798057556, Total Loss 60.609161376953125\n",
      "3: Encoding Loss 8.566400527954102, Transition Loss -1.208234190940857, Classifier Loss 0.14135770499706268, Total Loss 65.53369140625\n",
      "3: Encoding Loss 6.875966548919678, Transition Loss -0.6408063173294067, Classifier Loss 0.17356088757514954, Total Loss 58.611637115478516\n",
      "3: Encoding Loss 6.649416446685791, Transition Loss -1.3401927947998047, Classifier Loss 0.10455606132745743, Total Loss 50.351566314697266\n",
      "3: Encoding Loss 5.574184417724609, Transition Loss -0.9189342856407166, Classifier Loss 0.16084283590316772, Total Loss 49.529022216796875\n",
      "3: Encoding Loss 6.582087516784668, Transition Loss -1.4394233226776123, Classifier Loss 0.12470323592424393, Total Loss 51.962276458740234\n",
      "3: Encoding Loss 6.036747932434082, Transition Loss -0.9902353882789612, Classifier Loss 0.13217276334762573, Total Loss 49.43737030029297\n",
      "3: Encoding Loss 4.5325517654418945, Transition Loss -0.9027511477470398, Classifier Loss 0.14621102809906006, Total Loss 41.8160514831543\n",
      "3: Encoding Loss 5.958150863647461, Transition Loss -1.5232045650482178, Classifier Loss 0.20426753163337708, Total Loss 56.175048828125\n",
      "3: Encoding Loss 4.277652740478516, Transition Loss -0.9932617545127869, Classifier Loss 0.15101519227027893, Total Loss 40.76704025268555\n",
      "3: Encoding Loss 5.162296295166016, Transition Loss -0.5420091152191162, Classifier Loss 0.09102752804756165, Total Loss 40.07631301879883\n",
      "3: Encoding Loss 7.195318698883057, Transition Loss -1.1728122234344482, Classifier Loss 0.19226473569869995, Total Loss 62.39791488647461\n",
      "3: Encoding Loss 5.514522552490234, Transition Loss -0.8667359948158264, Classifier Loss 0.1386065036058426, Total Loss 46.94743728637695\n",
      "3: Encoding Loss 5.758188247680664, Transition Loss -2.6429548263549805, Classifier Loss 0.12352944165468216, Total Loss 46.90101623535156\n",
      "3: Encoding Loss 4.336421489715576, Transition Loss -1.513296365737915, Classifier Loss 0.11558452248573303, Total Loss 37.57637405395508\n",
      "3: Encoding Loss 4.973221302032471, Transition Loss -0.3964545726776123, Classifier Loss 0.16940653324127197, Total Loss 46.779823303222656\n",
      "3: Encoding Loss 3.5232017040252686, Transition Loss -1.2119759321212769, Classifier Loss 0.1563366800546646, Total Loss 36.772396087646484\n",
      "3: Encoding Loss 9.244355201721191, Transition Loss -1.5435445308685303, Classifier Loss 0.20266373455524445, Total Loss 75.73188781738281\n",
      "3: Encoding Loss 7.858010292053223, Transition Loss -0.7923608422279358, Classifier Loss 0.13778609037399292, Total Loss 60.92635726928711\n",
      "3: Encoding Loss 5.8872528076171875, Transition Loss -2.054330587387085, Classifier Loss 0.15772856771945953, Total Loss 51.09555435180664\n",
      "3: Encoding Loss 5.249061584472656, Transition Loss -2.0379364490509033, Classifier Loss 0.178685262799263, Total Loss 49.362083435058594\n",
      "3: Encoding Loss 4.91227388381958, Transition Loss -1.5495702028274536, Classifier Loss 0.16917738318443298, Total Loss 46.39076232910156\n",
      "3: Encoding Loss 6.157652854919434, Transition Loss -1.1093051433563232, Classifier Loss 0.14442457258701324, Total Loss 51.38793182373047\n",
      "3: Encoding Loss 5.5245161056518555, Transition Loss -1.6578258275985718, Classifier Loss 0.19451004266738892, Total Loss 52.59743881225586\n",
      "3: Encoding Loss 6.1233601570129395, Transition Loss -1.1585248708724976, Classifier Loss 0.11619827896356583, Total Loss 48.359527587890625\n",
      "3: Encoding Loss 5.138195514678955, Transition Loss -0.06119966506958008, Classifier Loss 0.1288280189037323, Total Loss 43.711952209472656\n",
      "3: Encoding Loss 5.635315895080566, Transition Loss -1.6337144374847412, Classifier Loss 0.13765230774879456, Total Loss 47.57647705078125\n",
      "3: Encoding Loss 4.260889053344727, Transition Loss -1.3779133558273315, Classifier Loss 0.1240350753068924, Total Loss 37.968292236328125\n",
      "3: Encoding Loss 7.494966983795166, Transition Loss -0.6175371408462524, Classifier Loss 0.18237760663032532, Total Loss 63.20731735229492\n",
      "3: Encoding Loss 6.554645538330078, Transition Loss -2.0017995834350586, Classifier Loss 0.10468322783708572, Total Loss 49.79539489746094\n",
      "3: Encoding Loss 7.7389936447143555, Transition Loss -1.4712363481521606, Classifier Loss 0.2398882508277893, Total Loss 70.42220306396484\n",
      "3: Encoding Loss 6.410011291503906, Transition Loss -0.9486609697341919, Classifier Loss 0.16299840807914734, Total Loss 54.7595329284668\n",
      "3: Encoding Loss 5.684632778167725, Transition Loss -1.5639045238494873, Classifier Loss 0.11717884987592697, Total Loss 45.82505798339844\n",
      "3: Encoding Loss 5.798138618469238, Transition Loss -0.32046669721603394, Classifier Loss 0.12872757017612457, Total Loss 47.661460876464844\n",
      "3: Encoding Loss 5.555872440338135, Transition Loss -1.1085586547851562, Classifier Loss 0.10682747513055801, Total Loss 44.017539978027344\n",
      "3: Encoding Loss 6.176136493682861, Transition Loss -0.5625190734863281, Classifier Loss 0.16455331444740295, Total Loss 53.51192855834961\n",
      "3: Encoding Loss 7.664102077484131, Transition Loss -0.4778375029563904, Classifier Loss 0.1204981729388237, Total Loss 58.03424072265625\n",
      "3: Encoding Loss 4.923336029052734, Transition Loss -1.3936847448349, Classifier Loss 0.19596606492996216, Total Loss 49.136070251464844\n",
      "3: Encoding Loss 5.781156063079834, Transition Loss -0.6935951709747314, Classifier Loss 0.10863479226827621, Total Loss 45.550140380859375\n",
      "3: Encoding Loss 5.099220275878906, Transition Loss -0.6101607084274292, Classifier Loss 0.13688570261001587, Total Loss 44.28364944458008\n",
      "3: Encoding Loss 4.858514308929443, Transition Loss -1.4833858013153076, Classifier Loss 0.10909248143434525, Total Loss 40.05973815917969\n",
      "3: Encoding Loss 5.5391106605529785, Transition Loss -1.678270936012268, Classifier Loss 0.205612450838089, Total Loss 53.79523849487305\n",
      "3: Encoding Loss 5.023281097412109, Transition Loss -1.6148594617843628, Classifier Loss 0.1661299765110016, Total Loss 46.75204086303711\n",
      "3: Encoding Loss 3.763500213623047, Transition Loss -2.1509461402893066, Classifier Loss 0.14771869778633118, Total Loss 37.35200881958008\n",
      "3: Encoding Loss 4.144304275512695, Transition Loss -0.8623428344726562, Classifier Loss 0.16853103041648865, Total Loss 41.718589782714844\n",
      "3: Encoding Loss 6.439060211181641, Transition Loss -0.6842223405838013, Classifier Loss 0.14523153007030487, Total Loss 53.15724182128906\n",
      "3: Encoding Loss 4.789130687713623, Transition Loss -1.8822457790374756, Classifier Loss 0.11494605988264084, Total Loss 40.228641510009766\n",
      "3: Encoding Loss 5.019906044006348, Transition Loss -0.2543620765209198, Classifier Loss 0.0991133451461792, Total Loss 40.03066635131836\n",
      "3: Encoding Loss 5.89316987991333, Transition Loss -1.8790711164474487, Classifier Loss 0.1375894844532013, Total Loss 49.117218017578125\n",
      "3: Encoding Loss 4.63386344909668, Transition Loss -0.8577074408531189, Classifier Loss 0.12970975041389465, Total Loss 40.7738151550293\n",
      "3: Encoding Loss 9.136570930480957, Transition Loss -0.1266164481639862, Classifier Loss 0.2497263252735138, Total Loss 79.79200744628906\n",
      "3: Encoding Loss 7.733922004699707, Transition Loss -1.3828755617141724, Classifier Loss 0.12116546928882599, Total Loss 58.519527435302734\n",
      "3: Encoding Loss 7.567050933837891, Transition Loss -0.012653052806854248, Classifier Loss 0.13884472846984863, Total Loss 59.286773681640625\n",
      "3: Encoding Loss 4.878884792327881, Transition Loss -1.4958040714263916, Classifier Loss 0.15998303890228271, Total Loss 45.27101516723633\n",
      "3: Encoding Loss 3.189466953277588, Transition Loss -0.9897942543029785, Classifier Loss 0.18607254326343536, Total Loss 37.74365997314453\n",
      "3: Encoding Loss 7.017763614654541, Transition Loss -1.458118200302124, Classifier Loss 0.1572512537240982, Total Loss 57.83112335205078\n",
      "3: Encoding Loss 8.713496208190918, Transition Loss -2.2254433631896973, Classifier Loss 0.25090229511260986, Total Loss 77.37031555175781\n",
      "3: Encoding Loss 7.549290657043457, Transition Loss -1.2275328636169434, Classifier Loss 0.28103965520858765, Total Loss 73.39922332763672\n",
      "3: Encoding Loss 4.165036678314209, Transition Loss -1.184725284576416, Classifier Loss 0.1595621109008789, Total Loss 40.945960998535156\n",
      "3: Encoding Loss 8.60058879852295, Transition Loss 0.012411803007125854, Classifier Loss 0.16192121803760529, Total Loss 67.80062103271484\n",
      "3: Encoding Loss 8.629770278930664, Transition Loss -1.301473617553711, Classifier Loss 0.28983208537101746, Total Loss 80.76130676269531\n",
      "3: Encoding Loss 6.923051357269287, Transition Loss -0.8101693391799927, Classifier Loss 0.17354895174503326, Total Loss 58.892879486083984\n",
      "3: Encoding Loss 7.781075477600098, Transition Loss -0.43102556467056274, Classifier Loss 0.16701780259609222, Total Loss 63.388065338134766\n",
      "3: Encoding Loss 7.372594356536865, Transition Loss -0.5671436190605164, Classifier Loss 0.16713383793830872, Total Loss 60.948726654052734\n",
      "3: Encoding Loss 5.472052574157715, Transition Loss -1.6088460683822632, Classifier Loss 0.13389363884925842, Total Loss 46.22103500366211\n",
      "3: Encoding Loss 4.765355587005615, Transition Loss -1.5271060466766357, Classifier Loss 0.14854802191257477, Total Loss 43.446327209472656\n",
      "3: Encoding Loss 7.229366302490234, Transition Loss -2.645231008529663, Classifier Loss 0.16824236512184143, Total Loss 60.19937515258789\n",
      "3: Encoding Loss 5.925636291503906, Transition Loss -1.8513357639312744, Classifier Loss 0.14328934252262115, Total Loss 49.88201141357422\n",
      "3: Encoding Loss 4.517429351806641, Transition Loss -0.9684510827064514, Classifier Loss 0.13319131731987, Total Loss 40.42332077026367\n",
      "3: Encoding Loss 5.430198669433594, Transition Loss -1.4316049814224243, Classifier Loss 0.18578821420669556, Total Loss 51.15943908691406\n",
      "3: Encoding Loss 3.811983823776245, Transition Loss -1.383569359779358, Classifier Loss 0.147685706615448, Total Loss 37.639923095703125\n",
      "3: Encoding Loss 5.523090839385986, Transition Loss -1.3580466508865356, Classifier Loss 0.1628451943397522, Total Loss 49.422523498535156\n",
      "3: Encoding Loss 5.955657005310059, Transition Loss -1.312904715538025, Classifier Loss 0.23399853706359863, Total Loss 59.133270263671875\n",
      "3: Encoding Loss 5.470155239105225, Transition Loss -0.5146385431289673, Classifier Loss 0.18311068415641785, Total Loss 51.131797790527344\n",
      "3: Encoding Loss 4.289555072784424, Transition Loss -1.4753539562225342, Classifier Loss 0.17734281718730927, Total Loss 43.47101974487305\n",
      "3: Encoding Loss 5.871645927429199, Transition Loss -1.6058884859085083, Classifier Loss 0.14827686548233032, Total Loss 50.056922912597656\n",
      "3: Encoding Loss 4.295538902282715, Transition Loss -0.6369698643684387, Classifier Loss 0.1213475838303566, Total Loss 37.907737731933594\n",
      "3: Encoding Loss 6.186157703399658, Transition Loss -1.3043107986450195, Classifier Loss 0.11041168868541718, Total Loss 48.1575927734375\n",
      "3: Encoding Loss 5.136874198913574, Transition Loss -1.3352634906768799, Classifier Loss 0.11629489064216614, Total Loss 42.45020294189453\n",
      "3: Encoding Loss 4.070469379425049, Transition Loss -0.7842774987220764, Classifier Loss 0.1897391378879547, Total Loss 43.39641571044922\n",
      "3: Encoding Loss 6.977034091949463, Transition Loss -0.7034579515457153, Classifier Loss 0.17728807032108307, Total Loss 59.590728759765625\n",
      "3: Encoding Loss 7.160254001617432, Transition Loss -0.5092931985855103, Classifier Loss 0.15888603031635284, Total Loss 58.84992599487305\n",
      "3: Encoding Loss 6.760880470275879, Transition Loss -0.9255359768867493, Classifier Loss 0.14077052474021912, Total Loss 54.6419677734375\n",
      "3: Encoding Loss 3.383697509765625, Transition Loss -1.8460743427276611, Classifier Loss 0.14542549848556519, Total Loss 34.843994140625\n",
      "3: Encoding Loss 8.087268829345703, Transition Loss -1.4909849166870117, Classifier Loss 0.19421328604221344, Total Loss 67.94435119628906\n",
      "3: Encoding Loss 6.238872051239014, Transition Loss -2.0686466693878174, Classifier Loss 0.14303532242774963, Total Loss 51.735939025878906\n",
      "3: Encoding Loss 5.970574378967285, Transition Loss -1.0499680042266846, Classifier Loss 0.1429557353258133, Total Loss 50.11860275268555\n",
      "3: Encoding Loss 7.0508527755737305, Transition Loss -1.825978398323059, Classifier Loss 0.19592176377773285, Total Loss 61.89656448364258\n",
      "3: Encoding Loss 5.8767499923706055, Transition Loss -1.7578777074813843, Classifier Loss 0.17849045991897583, Total Loss 53.108848571777344\n",
      "3: Encoding Loss 7.831118583679199, Transition Loss -0.49445676803588867, Classifier Loss 0.15184643864631653, Total Loss 62.17115783691406\n",
      "3: Encoding Loss 5.582992076873779, Transition Loss -0.08479832857847214, Classifier Loss 0.15794114768505096, Total Loss 49.29203414916992\n",
      "3: Encoding Loss 5.218437194824219, Transition Loss -1.7530707120895386, Classifier Loss 0.14178696274757385, Total Loss 45.48862075805664\n",
      "3: Encoding Loss 7.114480495452881, Transition Loss -0.8750271201133728, Classifier Loss 0.18133392930030823, Total Loss 60.81992721557617\n",
      "3: Encoding Loss 7.472038269042969, Transition Loss -1.1906167268753052, Classifier Loss 0.16127990186214447, Total Loss 60.95974349975586\n",
      "3: Encoding Loss 5.846149921417236, Transition Loss -0.9122068881988525, Classifier Loss 0.14534111320972443, Total Loss 49.61064529418945\n",
      "3: Encoding Loss 3.8667216300964355, Transition Loss -0.09065547585487366, Classifier Loss 0.16518375277519226, Total Loss 39.718666076660156\n",
      "3: Encoding Loss 3.9060511589050293, Transition Loss -0.8872133493423462, Classifier Loss 0.1323028802871704, Total Loss 36.66624069213867\n",
      "3: Encoding Loss 5.572754859924316, Transition Loss -1.6893284320831299, Classifier Loss 0.12019848078489304, Total Loss 45.45570373535156\n",
      "3: Encoding Loss 4.898632049560547, Transition Loss -1.2325541973114014, Classifier Loss 0.13210508227348328, Total Loss 42.601810455322266\n",
      "3: Encoding Loss 4.83541202545166, Transition Loss -0.8507257699966431, Classifier Loss 0.22380897402763367, Total Loss 51.39303207397461\n",
      "3: Encoding Loss 4.829298973083496, Transition Loss -1.9991973638534546, Classifier Loss 0.08293202519416809, Total Loss 37.26819610595703\n",
      "3: Encoding Loss 4.5737481117248535, Transition Loss -2.158400058746338, Classifier Loss 0.14485250413417816, Total Loss 41.9268798828125\n",
      "3: Encoding Loss 2.548165798187256, Transition Loss -0.753831148147583, Classifier Loss 0.1362752467393875, Total Loss 28.916217803955078\n",
      "3: Encoding Loss 5.059490203857422, Transition Loss -0.8265076875686646, Classifier Loss 0.12764716148376465, Total Loss 43.1213264465332\n",
      "3: Encoding Loss 4.649104595184326, Transition Loss -1.2128021717071533, Classifier Loss 0.18795442581176758, Total Loss 46.6895866394043\n",
      "3: Encoding Loss 3.534619092941284, Transition Loss -0.9781579971313477, Classifier Loss 0.11240638047456741, Total Loss 32.447959899902344\n",
      "3: Encoding Loss 5.504972457885742, Transition Loss -0.569593071937561, Classifier Loss 0.0987875908613205, Total Loss 42.908363342285156\n",
      "3: Encoding Loss 7.332207679748535, Transition Loss -1.7260265350341797, Classifier Loss 0.23223698139190674, Total Loss 67.21626281738281\n",
      "3: Encoding Loss 4.770088195800781, Transition Loss -0.6835476160049438, Classifier Loss 0.20593681931495667, Total Loss 49.21393585205078\n",
      "3: Encoding Loss 6.083334922790527, Transition Loss -1.001649260520935, Classifier Loss 0.22891777753829956, Total Loss 59.391387939453125\n",
      "3: Encoding Loss 5.492532253265381, Transition Loss -1.491005301475525, Classifier Loss 0.14765769243240356, Total Loss 47.72037124633789\n",
      "3: Encoding Loss 6.654656410217285, Transition Loss -1.3891029357910156, Classifier Loss 0.17972129583358765, Total Loss 57.899513244628906\n",
      "3: Encoding Loss 6.122830390930176, Transition Loss -0.9858635067939758, Classifier Loss 0.14143916964530945, Total Loss 50.88050842285156\n",
      "3: Encoding Loss 5.988055229187012, Transition Loss -0.6735519766807556, Classifier Loss 0.12031328678131104, Total Loss 47.95939254760742\n",
      "3: Encoding Loss 4.169349193572998, Transition Loss -1.5160505771636963, Classifier Loss 0.14415046572685242, Total Loss 39.430538177490234\n",
      "3: Encoding Loss 6.628673553466797, Transition Loss -0.3691738247871399, Classifier Loss 0.13360054790973663, Total Loss 53.1319465637207\n",
      "3: Encoding Loss 5.661338806152344, Transition Loss -1.0649757385253906, Classifier Loss 0.1732410490512848, Total Loss 51.291709899902344\n",
      "3: Encoding Loss 5.862871170043945, Transition Loss -1.302739143371582, Classifier Loss 0.15258906781673431, Total Loss 50.435611724853516\n",
      "3: Encoding Loss 6.576452732086182, Transition Loss -0.5770503282546997, Classifier Loss 0.15378542244434357, Total Loss 54.8370246887207\n",
      "3: Encoding Loss 4.875278472900391, Transition Loss -1.565886378288269, Classifier Loss 0.09899917989969254, Total Loss 39.150962829589844\n",
      "3: Encoding Loss 4.267245292663574, Transition Loss -1.2150417566299438, Classifier Loss 0.16005101799964905, Total Loss 41.608089447021484\n",
      "3: Encoding Loss 4.009796619415283, Transition Loss -1.2442495822906494, Classifier Loss 0.1336853802204132, Total Loss 37.426822662353516\n",
      "3: Encoding Loss 7.749742031097412, Transition Loss -1.824905514717102, Classifier Loss 0.13705269992351532, Total Loss 60.20299530029297\n",
      "3: Encoding Loss 4.605481147766113, Transition Loss -2.516270875930786, Classifier Loss 0.13104765117168427, Total Loss 40.73664474487305\n",
      "3: Encoding Loss 2.953526020050049, Transition Loss -1.0249736309051514, Classifier Loss 0.14298781752586365, Total Loss 32.01953125\n",
      "3: Encoding Loss 10.841135025024414, Transition Loss -0.049893394112586975, Classifier Loss 0.20013399422168732, Total Loss 85.06018829345703\n",
      "3: Encoding Loss 8.136439323425293, Transition Loss -0.3978806436061859, Classifier Loss 0.1553848683834076, Total Loss 64.35696411132812\n",
      "3: Encoding Loss 5.272036552429199, Transition Loss -2.1477720737457275, Classifier Loss 0.13347168266773224, Total Loss 44.97853088378906\n",
      "3: Encoding Loss 4.534285068511963, Transition Loss -0.6229344606399536, Classifier Loss 0.12459047138690948, Total Loss 39.66450881958008\n",
      "3: Encoding Loss 7.040642738342285, Transition Loss -2.041221857070923, Classifier Loss 0.2265593707561493, Total Loss 64.89897918701172\n",
      "3: Encoding Loss 5.23564338684082, Transition Loss -1.0316752195358276, Classifier Loss 0.1335660219192505, Total Loss 44.770050048828125\n",
      "3: Encoding Loss 4.0890679359436035, Transition Loss -1.125193476676941, Classifier Loss 0.11218543350696564, Total Loss 35.75250244140625\n",
      "3: Encoding Loss 4.949460983276367, Transition Loss -1.386152982711792, Classifier Loss 0.165659561753273, Total Loss 46.26217269897461\n",
      "3: Encoding Loss 6.153263092041016, Transition Loss -0.8348299264907837, Classifier Loss 0.08970466256141663, Total Loss 45.88970947265625\n",
      "3: Encoding Loss 4.953814506530762, Transition Loss -1.4746921062469482, Classifier Loss 0.13900092244148254, Total Loss 43.62239074707031\n",
      "3: Encoding Loss 7.493459224700928, Transition Loss -1.7102621793746948, Classifier Loss 0.1305297315120697, Total Loss 58.0130500793457\n",
      "3: Encoding Loss 6.763841152191162, Transition Loss -0.658202052116394, Classifier Loss 0.1359912008047104, Total Loss 54.181907653808594\n",
      "3: Encoding Loss 5.814337253570557, Transition Loss -0.5729374885559082, Classifier Loss 0.15936076641082764, Total Loss 50.82187271118164\n",
      "3: Encoding Loss 7.636514186859131, Transition Loss -1.1216576099395752, Classifier Loss 0.11426745355129242, Total Loss 57.245384216308594\n",
      "3: Encoding Loss 6.746182441711426, Transition Loss -1.0636513233184814, Classifier Loss 0.21619687974452972, Total Loss 62.09635925292969\n",
      "3: Encoding Loss 5.338553428649902, Transition Loss -0.7006087303161621, Classifier Loss 0.1527014672756195, Total Loss 47.30118942260742\n",
      "3: Encoding Loss 3.8287391662597656, Transition Loss -1.178513526916504, Classifier Loss 0.11625297367572784, Total Loss 34.597259521484375\n",
      "3: Encoding Loss 5.104165077209473, Transition Loss -1.4246739149093628, Classifier Loss 0.12634576857089996, Total Loss 43.25899887084961\n",
      "3: Encoding Loss 5.383671283721924, Transition Loss -0.4000827372074127, Classifier Loss 0.13511475920677185, Total Loss 45.8133430480957\n",
      "3: Encoding Loss 4.6213555335998535, Transition Loss -0.03761988878250122, Classifier Loss 0.16487030684947968, Total Loss 44.21514892578125\n",
      "3: Encoding Loss 4.576579570770264, Transition Loss -1.4631879329681396, Classifier Loss 0.11409863829612732, Total Loss 38.86875915527344\n",
      "3: Encoding Loss 3.512962818145752, Transition Loss -2.1746952533721924, Classifier Loss 0.11452978849411011, Total Loss 32.52988815307617\n",
      "3: Encoding Loss 6.777497291564941, Transition Loss -1.7516777515411377, Classifier Loss 0.14145758748054504, Total Loss 54.81004333496094\n",
      "3: Encoding Loss 6.484735012054443, Transition Loss -1.6314926147460938, Classifier Loss 0.20412708818912506, Total Loss 59.32046890258789\n",
      "3: Encoding Loss 5.763959884643555, Transition Loss -0.27790209650993347, Classifier Loss 0.16318687796592712, Total Loss 50.90233612060547\n",
      "3: Encoding Loss 5.338639259338379, Transition Loss -0.9143127202987671, Classifier Loss 0.15758338570594788, Total Loss 47.78981018066406\n",
      "3: Encoding Loss 5.994441032409668, Transition Loss -0.7393081188201904, Classifier Loss 0.19070468842983246, Total Loss 55.03681945800781\n",
      "3: Encoding Loss 5.633791923522949, Transition Loss -0.771644115447998, Classifier Loss 0.15837739408016205, Total Loss 49.64018249511719\n",
      "3: Encoding Loss 4.903265953063965, Transition Loss -1.0471749305725098, Classifier Loss 0.12893790006637573, Total Loss 42.31296920776367\n",
      "3: Encoding Loss 5.395186424255371, Transition Loss -2.1697916984558105, Classifier Loss 0.14570128917694092, Total Loss 46.94038009643555\n",
      "3: Encoding Loss 4.739019870758057, Transition Loss -1.0631014108657837, Classifier Loss 0.10809535533189774, Total Loss 39.24323272705078\n",
      "3: Encoding Loss 4.939209938049316, Transition Loss -1.3838661909103394, Classifier Loss 0.1544385850429535, Total Loss 45.07856369018555\n",
      "3: Encoding Loss 6.201146125793457, Transition Loss -2.4010095596313477, Classifier Loss 0.09770529717206955, Total Loss 46.97644805908203\n",
      "3: Encoding Loss 7.250118255615234, Transition Loss -0.9071606397628784, Classifier Loss 0.1129782497882843, Total Loss 54.79817199707031\n",
      "3: Encoding Loss 3.9362952709198, Transition Loss -0.7993733882904053, Classifier Loss 0.10902795940637589, Total Loss 34.52024841308594\n",
      "3: Encoding Loss 6.501245975494385, Transition Loss -1.3776540756225586, Classifier Loss 0.12753242254257202, Total Loss 51.760169982910156\n",
      "3: Encoding Loss 6.537832736968994, Transition Loss -1.155005931854248, Classifier Loss 0.10720273107290268, Total Loss 49.946807861328125\n",
      "3: Encoding Loss 4.478451728820801, Transition Loss -0.7420273423194885, Classifier Loss 0.08683070540428162, Total Loss 35.55348205566406\n",
      "3: Encoding Loss 5.357272148132324, Transition Loss -2.124957799911499, Classifier Loss 0.1686212718486786, Total Loss 49.00490951538086\n",
      "3: Encoding Loss 6.85480260848999, Transition Loss -0.48025768995285034, Classifier Loss 0.1333107203245163, Total Loss 54.45970153808594\n",
      "3: Encoding Loss 6.809403896331787, Transition Loss -0.9479614496231079, Classifier Loss 0.15119491517543793, Total Loss 55.97554016113281\n",
      "3: Encoding Loss 5.309573650360107, Transition Loss -1.1313884258270264, Classifier Loss 0.12170491367578506, Total Loss 44.02748107910156\n",
      "3: Encoding Loss 5.550739288330078, Transition Loss -1.358115315437317, Classifier Loss 0.12422648072242737, Total Loss 45.72654342651367\n",
      "3: Encoding Loss 5.246537685394287, Transition Loss -0.8259256482124329, Classifier Loss 0.19938087463378906, Total Loss 51.4169807434082\n",
      "3: Encoding Loss 6.2490668296813965, Transition Loss -1.4031505584716797, Classifier Loss 0.11958649754524231, Total Loss 49.452491760253906\n",
      "3: Encoding Loss 3.4058690071105957, Transition Loss -1.1160433292388916, Classifier Loss 0.1583741307258606, Total Loss 36.27218246459961\n",
      "3: Encoding Loss 9.287646293640137, Transition Loss 0.025456100702285767, Classifier Loss 0.15534254908561707, Total Loss 71.27031707763672\n",
      "3: Encoding Loss 6.3557305335998535, Transition Loss -1.1018177270889282, Classifier Loss 0.09518555551767349, Total Loss 47.652496337890625\n",
      "3: Encoding Loss 3.353893995285034, Transition Loss -1.8916701078414917, Classifier Loss 0.13538138568401337, Total Loss 33.66074752807617\n",
      "3: Encoding Loss 6.474895477294922, Transition Loss -0.3884499967098236, Classifier Loss 0.11074910312891006, Total Loss 49.92412567138672\n",
      "3: Encoding Loss 3.6539905071258545, Transition Loss -0.8828562498092651, Classifier Loss 0.08168081194162369, Total Loss 30.091672897338867\n",
      "3: Encoding Loss 8.169228553771973, Transition Loss -1.48405122756958, Classifier Loss 0.17711761593818665, Total Loss 66.7265396118164\n",
      "3: Encoding Loss 8.747241020202637, Transition Loss -1.0004229545593262, Classifier Loss 0.17056159675121307, Total Loss 69.53921508789062\n",
      "3: Encoding Loss 6.421064853668213, Transition Loss -1.6038353443145752, Classifier Loss 0.13307885825634003, Total Loss 51.83363342285156\n",
      "3: Encoding Loss 4.610682010650635, Transition Loss -1.0139845609664917, Classifier Loss 0.1577390432357788, Total Loss 43.437591552734375\n",
      "3: Encoding Loss 4.638017177581787, Transition Loss -1.4605703353881836, Classifier Loss 0.1409018635749817, Total Loss 41.91770553588867\n",
      "3: Encoding Loss 5.875129222869873, Transition Loss -1.4184774160385132, Classifier Loss 0.13122959434986115, Total Loss 48.3731689453125\n",
      "3: Encoding Loss 4.117489814758301, Transition Loss -0.871082603931427, Classifier Loss 0.13498084247112274, Total Loss 38.202674865722656\n",
      "3: Encoding Loss 7.438793182373047, Transition Loss -1.7987310886383057, Classifier Loss 0.18778954446315765, Total Loss 63.41099166870117\n",
      "3: Encoding Loss 5.4955925941467285, Transition Loss -1.6563172340393066, Classifier Loss 0.12286223471164703, Total Loss 45.259117126464844\n",
      "3: Encoding Loss 3.7303409576416016, Transition Loss -0.79603111743927, Classifier Loss 0.1272047907114029, Total Loss 35.10220718383789\n",
      "3: Encoding Loss 6.122847557067871, Transition Loss -0.16977950930595398, Classifier Loss 0.1881508082151413, Total Loss 55.552101135253906\n",
      "3: Encoding Loss 5.717601299285889, Transition Loss -1.5880751609802246, Classifier Loss 0.1178693026304245, Total Loss 46.09190368652344\n",
      "3: Encoding Loss 6.9213080406188965, Transition Loss -0.739626407623291, Classifier Loss 0.15857426822185516, Total Loss 57.384979248046875\n",
      "3: Encoding Loss 3.746691942214966, Transition Loss -1.9710309505462646, Classifier Loss 0.19779329001903534, Total Loss 42.25869369506836\n",
      "3: Encoding Loss 8.255693435668945, Transition Loss -1.0659643411636353, Classifier Loss 0.14075182378292084, Total Loss 63.608917236328125\n",
      "3: Encoding Loss 6.8021039962768555, Transition Loss -1.051619052886963, Classifier Loss 0.09554754197597504, Total Loss 50.36695861816406\n",
      "3: Encoding Loss 8.314118385314941, Transition Loss -0.5828495025634766, Classifier Loss 0.16057546436786652, Total Loss 65.94202423095703\n",
      "3: Encoding Loss 5.451294422149658, Transition Loss -1.1632812023162842, Classifier Loss 0.16682767868041992, Total Loss 49.39006805419922\n",
      "3: Encoding Loss 4.3974809646606445, Transition Loss -1.065375804901123, Classifier Loss 0.152430921792984, Total Loss 41.6275520324707\n",
      "3: Encoding Loss 5.583837032318115, Transition Loss -2.37972092628479, Classifier Loss 0.11134585738182068, Total Loss 44.63665771484375\n",
      "3: Encoding Loss 4.058289527893066, Transition Loss 0.19145303964614868, Classifier Loss 0.1696946620941162, Total Loss 41.39578628540039\n",
      "3: Encoding Loss 4.163905143737793, Transition Loss -0.4225412607192993, Classifier Loss 0.10385928303003311, Total Loss 35.36919403076172\n",
      "3: Encoding Loss 4.666813850402832, Transition Loss -1.999819278717041, Classifier Loss 0.18096603453159332, Total Loss 46.09668731689453\n",
      "3: Encoding Loss 6.136906623840332, Transition Loss -1.370333194732666, Classifier Loss 0.20044167339801788, Total Loss 56.86505889892578\n",
      "3: Encoding Loss 4.691692352294922, Transition Loss -0.6765130162239075, Classifier Loss 0.14237283170223236, Total Loss 42.387168884277344\n",
      "3: Encoding Loss 4.890029430389404, Transition Loss -0.6783657073974609, Classifier Loss 0.12079542875289917, Total Loss 41.41944885253906\n",
      "3: Encoding Loss 4.762048244476318, Transition Loss -1.1106138229370117, Classifier Loss 0.09954110532999039, Total Loss 38.52595901489258\n",
      "3: Encoding Loss 7.490696907043457, Transition Loss -0.32106339931488037, Classifier Loss 0.2368352711200714, Total Loss 68.62757873535156\n",
      "3: Encoding Loss 6.26759147644043, Transition Loss -1.0988233089447021, Classifier Loss 0.13931632041931152, Total Loss 51.5367431640625\n",
      "3: Encoding Loss 7.150109767913818, Transition Loss -1.3530681133270264, Classifier Loss 0.12057612091302872, Total Loss 54.957733154296875\n",
      "3: Encoding Loss 4.481568813323975, Transition Loss -0.36012059450149536, Classifier Loss 0.11613016575574875, Total Loss 38.50228500366211\n",
      "3: Encoding Loss 5.974421501159668, Transition Loss -1.435455560684204, Classifier Loss 0.10223259776830673, Total Loss 46.0692138671875\n",
      "3: Encoding Loss 5.540449142456055, Transition Loss -0.7582612633705139, Classifier Loss 0.1523580104112625, Total Loss 48.47819137573242\n",
      "3: Encoding Loss 5.0235114097595215, Transition Loss -1.242286205291748, Classifier Loss 0.2234048694372177, Total Loss 52.48106002807617\n",
      "3: Encoding Loss 8.935629844665527, Transition Loss -0.48314347863197327, Classifier Loss 0.23312446475028992, Total Loss 76.92603302001953\n",
      "3: Encoding Loss 8.139700889587402, Transition Loss -1.2267287969589233, Classifier Loss 0.12778054177761078, Total Loss 61.61576843261719\n",
      "3: Encoding Loss 7.0962629318237305, Transition Loss -1.1677546501159668, Classifier Loss 0.2044394463300705, Total Loss 63.02105712890625\n",
      "3: Encoding Loss 5.573886871337891, Transition Loss -1.438561201095581, Classifier Loss 0.11186148226261139, Total Loss 44.6288948059082\n",
      "3: Encoding Loss 5.463008403778076, Transition Loss -1.0024296045303345, Classifier Loss 0.13469555974006653, Total Loss 46.24720764160156\n",
      "3: Encoding Loss 7.138761043548584, Transition Loss -1.1005353927612305, Classifier Loss 0.13240912556648254, Total Loss 56.07304382324219\n",
      "3: Encoding Loss 5.756958961486816, Transition Loss -1.2102162837982178, Classifier Loss 0.1596696972846985, Total Loss 50.50823974609375\n",
      "3: Encoding Loss 3.6860435009002686, Transition Loss -0.020781666040420532, Classifier Loss 0.09211216866970062, Total Loss 31.327472686767578\n",
      "3: Encoding Loss 4.064526557922363, Transition Loss -0.7682706713676453, Classifier Loss 0.12434952706098557, Total Loss 36.82180404663086\n",
      "3: Encoding Loss 4.847318172454834, Transition Loss -0.04804041236639023, Classifier Loss 0.12792398035526276, Total Loss 41.87628936767578\n",
      "3: Encoding Loss 4.792835712432861, Transition Loss -1.2457878589630127, Classifier Loss 0.116906076669693, Total Loss 40.44712448120117\n",
      "3: Encoding Loss 7.399745941162109, Transition Loss -0.48735350370407104, Classifier Loss 0.23628266155719757, Total Loss 68.02654266357422\n",
      "3: Encoding Loss 6.296224117279053, Transition Loss -1.5109471082687378, Classifier Loss 0.12669123709201813, Total Loss 50.44586944580078\n",
      "3: Encoding Loss 8.42261028289795, Transition Loss -2.321608304977417, Classifier Loss 0.14163808524608612, Total Loss 64.69853973388672\n",
      "3: Encoding Loss 6.445243835449219, Transition Loss -1.4536997079849243, Classifier Loss 0.11453329026699066, Total Loss 50.12421417236328\n",
      "3: Encoding Loss 3.8972315788269043, Transition Loss -1.0034979581832886, Classifier Loss 0.12290339171886444, Total Loss 35.6733283996582\n",
      "3: Encoding Loss 4.336869239807129, Transition Loss -0.49242040514945984, Classifier Loss 0.07122655212879181, Total Loss 33.143672943115234\n",
      "3: Encoding Loss 6.206676483154297, Transition Loss -2.478882312774658, Classifier Loss 0.2523285448551178, Total Loss 62.471923828125\n",
      "3: Encoding Loss 5.508423805236816, Transition Loss -1.3901294469833374, Classifier Loss 0.17979346215724945, Total Loss 51.029335021972656\n",
      "3: Encoding Loss 3.5427639484405518, Transition Loss 0.41315019130706787, Classifier Loss 0.1000756174325943, Total Loss 31.429405212402344\n",
      "3: Encoding Loss 5.823096752166748, Transition Loss -0.9768896698951721, Classifier Loss 0.24209816753864288, Total Loss 59.14801025390625\n",
      "3: Encoding Loss 5.069833278656006, Transition Loss -1.6115238666534424, Classifier Loss 0.17388156056404114, Total Loss 47.806514739990234\n",
      "3: Encoding Loss 6.789517402648926, Transition Loss -0.9029321074485779, Classifier Loss 0.16524885594844818, Total Loss 57.26163101196289\n",
      "3: Encoding Loss 3.7697653770446777, Transition Loss -0.5688864588737488, Classifier Loss 0.11583064496517181, Total Loss 34.2014274597168\n",
      "3: Encoding Loss 3.5056354999542236, Transition Loss -0.7529540061950684, Classifier Loss 0.17584668099880219, Total Loss 38.61817932128906\n",
      "3: Encoding Loss 5.7661943435668945, Transition Loss -2.0403456687927246, Classifier Loss 0.14468897879123688, Total Loss 49.065250396728516\n",
      "3: Encoding Loss 5.239177703857422, Transition Loss -1.3420451879501343, Classifier Loss 0.18926450610160828, Total Loss 50.36098098754883\n",
      "3: Encoding Loss 6.101174831390381, Transition Loss -1.9078105688095093, Classifier Loss 0.16413401067256927, Total Loss 53.019691467285156\n",
      "3: Encoding Loss 5.797740459442139, Transition Loss -1.5764358043670654, Classifier Loss 0.09771831333637238, Total Loss 44.557647705078125\n",
      "3: Encoding Loss 5.8490800857543945, Transition Loss -2.0987002849578857, Classifier Loss 0.1582947075366974, Total Loss 50.92311477661133\n",
      "3: Encoding Loss 7.2130231857299805, Transition Loss -1.114109754562378, Classifier Loss 0.18269462883472443, Total Loss 61.547157287597656\n",
      "3: Encoding Loss 6.753224849700928, Transition Loss -1.3732136487960815, Classifier Loss 0.19581939280033112, Total Loss 60.10074234008789\n",
      "3: Encoding Loss 6.187840938568115, Transition Loss -0.9121531248092651, Classifier Loss 0.09861432760953903, Total Loss 46.98811340332031\n",
      "3: Encoding Loss 4.560949325561523, Transition Loss -1.8798906803131104, Classifier Loss 0.12598401308059692, Total Loss 39.963348388671875\n",
      "3: Encoding Loss 6.381130695343018, Transition Loss -1.4178524017333984, Classifier Loss 0.14047692716121674, Total Loss 52.33390808105469\n",
      "3: Encoding Loss 4.706910610198975, Transition Loss -1.8572702407836914, Classifier Loss 0.11566314846277237, Total Loss 39.80703353881836\n",
      "3: Encoding Loss 6.018609046936035, Transition Loss -1.5219312906265259, Classifier Loss 0.10540155321359634, Total Loss 46.65119934082031\n",
      "3: Encoding Loss 4.95319938659668, Transition Loss -0.9106684923171997, Classifier Loss 0.11163952946662903, Total Loss 40.882789611816406\n",
      "3: Encoding Loss 5.528773307800293, Transition Loss -1.460644006729126, Classifier Loss 0.17908170819282532, Total Loss 51.08022689819336\n",
      "3: Encoding Loss 4.249173164367676, Transition Loss -0.1302037239074707, Classifier Loss 0.0936918631196022, Total Loss 34.864173889160156\n",
      "3: Encoding Loss 9.184440612792969, Transition Loss -1.1947187185287476, Classifier Loss 0.20195196568965912, Total Loss 75.30136108398438\n",
      "3: Encoding Loss 8.153985977172852, Transition Loss -1.6635701656341553, Classifier Loss 0.18454928696155548, Total Loss 67.37818145751953\n",
      "3: Encoding Loss 7.594670295715332, Transition Loss -1.0341930389404297, Classifier Loss 0.2011953592300415, Total Loss 65.68714904785156\n",
      "3: Encoding Loss 6.228486061096191, Transition Loss -0.34999412298202515, Classifier Loss 0.19386856257915497, Total Loss 56.757633209228516\n",
      "3: Encoding Loss 7.00192928314209, Transition Loss -0.30143192410469055, Classifier Loss 0.23322688043117523, Total Loss 65.33414459228516\n",
      "3: Encoding Loss 4.480088233947754, Transition Loss -0.11396792531013489, Classifier Loss 0.09502509981393814, Total Loss 36.38299560546875\n",
      "3: Encoding Loss 4.198851585388184, Transition Loss -0.9705648422241211, Classifier Loss 0.14350014925003052, Total Loss 39.5427360534668\n",
      "3: Encoding Loss 2.880347728729248, Transition Loss -0.024642348289489746, Classifier Loss 0.15185977518558502, Total Loss 32.46805191040039\n",
      "3: Encoding Loss 5.746380805969238, Transition Loss -0.6344254016876221, Classifier Loss 0.1560916304588318, Total Loss 50.08719253540039\n",
      "3: Encoding Loss 6.433106899261475, Transition Loss -1.5335625410079956, Classifier Loss 0.14258407056331635, Total Loss 52.85643768310547\n",
      "3: Encoding Loss 7.631401062011719, Transition Loss -1.147415041923523, Classifier Loss 0.22299568355083466, Total Loss 68.08751678466797\n",
      "3: Encoding Loss 4.458452224731445, Transition Loss -1.373723030090332, Classifier Loss 0.1225041002035141, Total Loss 39.00057601928711\n",
      "3: Encoding Loss 7.03627872467041, Transition Loss -1.900259256362915, Classifier Loss 0.17617562413215637, Total Loss 59.834476470947266\n",
      "3: Encoding Loss 5.45350456237793, Transition Loss -1.0945892333984375, Classifier Loss 0.11286613345146179, Total Loss 44.0072021484375\n",
      "3: Encoding Loss 5.274783134460449, Transition Loss -1.638358473777771, Classifier Loss 0.12751725316047668, Total Loss 44.3997688293457\n",
      "3: Encoding Loss 5.106928825378418, Transition Loss -2.6045162677764893, Classifier Loss 0.10232346504926682, Total Loss 40.87287902832031\n",
      "3: Encoding Loss 2.8388612270355225, Transition Loss -1.4281467199325562, Classifier Loss 0.1231210008263588, Total Loss 29.344696044921875\n",
      "3: Encoding Loss 6.3885393142700195, Transition Loss -1.5127191543579102, Classifier Loss 0.12437596172094345, Total Loss 50.768226623535156\n",
      "3: Encoding Loss 5.781162261962891, Transition Loss -1.4927759170532227, Classifier Loss 0.22357980906963348, Total Loss 57.04435348510742\n",
      "3: Encoding Loss 3.5243048667907715, Transition Loss -1.4899061918258667, Classifier Loss 0.11776527762413025, Total Loss 32.92176055908203\n",
      "3: Encoding Loss 7.306377410888672, Transition Loss -1.8645093441009521, Classifier Loss 0.15270523726940155, Total Loss 59.10803985595703\n",
      "3: Encoding Loss 5.260794162750244, Transition Loss -0.916803240776062, Classifier Loss 0.131282776594162, Total Loss 44.69267654418945\n",
      "3: Encoding Loss 6.974470615386963, Transition Loss -0.9667094349861145, Classifier Loss 0.14514261484146118, Total Loss 56.36069869995117\n",
      "3: Encoding Loss 7.067753791809082, Transition Loss -1.2501733303070068, Classifier Loss 0.19487974047660828, Total Loss 61.89400100708008\n",
      "3: Encoding Loss 6.656846523284912, Transition Loss -0.7285629510879517, Classifier Loss 0.16246044635772705, Total Loss 56.18683624267578\n",
      "3: Encoding Loss 5.0756306648254395, Transition Loss -1.3820991516113281, Classifier Loss 0.13248731195926666, Total Loss 43.701961517333984\n",
      "3: Encoding Loss 6.400595188140869, Transition Loss -1.477699637413025, Classifier Loss 0.14394842088222504, Total Loss 52.79782485961914\n",
      "3: Encoding Loss 5.465149879455566, Transition Loss -1.3421998023986816, Classifier Loss 0.121944859623909, Total Loss 44.98484802246094\n",
      "3: Encoding Loss 3.0334744453430176, Transition Loss -0.44975656270980835, Classifier Loss 0.07569770514965057, Total Loss 25.77043914794922\n",
      "3: Encoding Loss 7.914766788482666, Transition Loss -0.4700948894023895, Classifier Loss 0.13035596907138824, Total Loss 60.52401351928711\n",
      "3: Encoding Loss 6.155932903289795, Transition Loss -1.1515426635742188, Classifier Loss 0.11761841177940369, Total Loss 48.69697952270508\n",
      "3: Encoding Loss 5.793076992034912, Transition Loss -1.1350425481796265, Classifier Loss 0.07593591511249542, Total Loss 42.35160446166992\n",
      "3: Encoding Loss 4.139045238494873, Transition Loss -0.33361485600471497, Classifier Loss 0.09188932180404663, Total Loss 34.0230712890625\n",
      "3: Encoding Loss 4.933115005493164, Transition Loss -1.649457335472107, Classifier Loss 0.08810758590698242, Total Loss 38.408790588378906\n",
      "3: Encoding Loss 4.451642036437988, Transition Loss -1.5420050621032715, Classifier Loss 0.1618199199438095, Total Loss 42.89122772216797\n",
      "3: Encoding Loss 5.8463311195373535, Transition Loss -0.41403070092201233, Classifier Loss 0.10010644048452377, Total Loss 45.08846664428711\n",
      "3: Encoding Loss 5.000805854797363, Transition Loss -0.9124933481216431, Classifier Loss 0.10207709670066833, Total Loss 40.21217727661133\n",
      "3: Encoding Loss 4.798550128936768, Transition Loss -0.5352598428726196, Classifier Loss 0.08798396587371826, Total Loss 37.58948516845703\n",
      "3: Encoding Loss 3.754723310470581, Transition Loss -0.95783531665802, Classifier Loss 0.11513641476631165, Total Loss 34.041603088378906\n",
      "3: Encoding Loss 6.194433689117432, Transition Loss -1.2643476724624634, Classifier Loss 0.1237441822886467, Total Loss 49.5405158996582\n",
      "3: Encoding Loss 4.813511848449707, Transition Loss -1.6383394002914429, Classifier Loss 0.1441013216972351, Total Loss 43.29054641723633\n",
      "3: Encoding Loss 4.035151958465576, Transition Loss -1.9211944341659546, Classifier Loss 0.10741588473320007, Total Loss 34.95173263549805\n",
      "3: Encoding Loss 3.836282253265381, Transition Loss -1.1695160865783691, Classifier Loss 0.1614837497472763, Total Loss 39.16559982299805\n",
      "3: Encoding Loss 6.671361446380615, Transition Loss -0.8223211765289307, Classifier Loss 0.14251573383808136, Total Loss 54.2794189453125\n",
      "3: Encoding Loss 4.196741580963135, Transition Loss -0.9383538961410522, Classifier Loss 0.10712029784917831, Total Loss 35.89210510253906\n",
      "3: Encoding Loss 5.033185005187988, Transition Loss -1.0118082761764526, Classifier Loss 0.12712106108665466, Total Loss 42.91081237792969\n",
      "3: Encoding Loss 5.211427211761475, Transition Loss -2.119631290435791, Classifier Loss 0.13460186123847961, Total Loss 44.7279052734375\n",
      "3: Encoding Loss 5.13920783996582, Transition Loss -1.0591444969177246, Classifier Loss 0.09325893968343735, Total Loss 40.16072082519531\n",
      "3: Encoding Loss 3.9331252574920654, Transition Loss -0.8409137725830078, Classifier Loss 0.11619718372821808, Total Loss 35.218135833740234\n",
      "3: Encoding Loss 4.5990142822265625, Transition Loss -1.4251220226287842, Classifier Loss 0.18937714397907257, Total Loss 46.53123092651367\n",
      "3: Encoding Loss 3.7811310291290283, Transition Loss -2.069669723510742, Classifier Loss 0.12457165122032166, Total Loss 35.143123626708984\n",
      "3: Encoding Loss 4.162196636199951, Transition Loss -0.9091795682907104, Classifier Loss 0.11113400012254715, Total Loss 36.086219787597656\n",
      "3: Encoding Loss 4.976641654968262, Transition Loss -1.957966685295105, Classifier Loss 0.13129428029060364, Total Loss 42.98849868774414\n",
      "3: Encoding Loss 5.019083023071289, Transition Loss -1.097299575805664, Classifier Loss 0.18258200585842133, Total Loss 48.37226104736328\n",
      "3: Encoding Loss 6.0228776931762695, Transition Loss -1.3309872150421143, Classifier Loss 0.14738646149635315, Total Loss 50.87538146972656\n",
      "3: Encoding Loss 3.9585137367248535, Transition Loss -1.675219178199768, Classifier Loss 0.12062658369541168, Total Loss 35.813072204589844\n",
      "3: Encoding Loss 6.397439002990723, Transition Loss -1.1021374464035034, Classifier Loss 0.19845783710479736, Total Loss 58.229976654052734\n",
      "3: Encoding Loss 7.402674198150635, Transition Loss -1.4727463722229004, Classifier Loss 0.2872919738292694, Total Loss 73.1446533203125\n",
      "3: Encoding Loss 7.357152462005615, Transition Loss -1.4307023286819458, Classifier Loss 0.1657215803861618, Total Loss 60.71450424194336\n",
      "3: Encoding Loss 7.1597137451171875, Transition Loss -2.0101916790008545, Classifier Loss 0.14619819819927216, Total Loss 57.57729721069336\n",
      "3: Encoding Loss 4.43310546875, Transition Loss -2.308011054992676, Classifier Loss 0.12874768674373627, Total Loss 39.47248077392578\n",
      "3: Encoding Loss 9.062671661376953, Transition Loss -1.8404428958892822, Classifier Loss 0.149709552526474, Total Loss 69.34625244140625\n",
      "3: Encoding Loss 9.103819847106934, Transition Loss -0.24404260516166687, Classifier Loss 0.1528836488723755, Total Loss 69.91118621826172\n",
      "3: Encoding Loss 7.37908935546875, Transition Loss -1.980085015296936, Classifier Loss 0.2638683021068573, Total Loss 70.66056823730469\n",
      "3: Encoding Loss 7.770438194274902, Transition Loss -1.5516690015792847, Classifier Loss 0.17550943791866302, Total Loss 64.17295837402344\n",
      "3: Encoding Loss 6.89764928817749, Transition Loss 0.16144505143165588, Classifier Loss 0.11613131314516068, Total Loss 53.0636100769043\n",
      "3: Encoding Loss 5.315816402435303, Transition Loss -1.4669640064239502, Classifier Loss 0.09190884232521057, Total Loss 41.08519744873047\n",
      "3: Encoding Loss 2.7695212364196777, Transition Loss -1.2521389722824097, Classifier Loss 0.14001794159412384, Total Loss 30.61842155456543\n",
      "3: Encoding Loss 5.535155296325684, Transition Loss -0.6296485066413879, Classifier Loss 0.12688226997852325, Total Loss 45.89891052246094\n",
      "3: Encoding Loss 4.2112531661987305, Transition Loss 0.2906510829925537, Classifier Loss 0.12479976564645767, Total Loss 37.8637580871582\n",
      "3: Encoding Loss 2.1754837036132812, Transition Loss -1.1775429248809814, Classifier Loss 0.09447315335273743, Total Loss 22.499746322631836\n",
      "3: Encoding Loss 4.519169807434082, Transition Loss -0.9538508057594299, Classifier Loss 0.1845439374446869, Total Loss 45.56903076171875\n",
      "3: Encoding Loss 4.175872325897217, Transition Loss -0.8491846919059753, Classifier Loss 0.1770627200603485, Total Loss 42.761165618896484\n",
      "3: Encoding Loss 7.0018415451049805, Transition Loss -0.478485107421875, Classifier Loss 0.10830700397491455, Total Loss 52.84156036376953\n",
      "3: Encoding Loss 5.835127353668213, Transition Loss -1.8136744499206543, Classifier Loss 0.12126760184764862, Total Loss 47.136802673339844\n",
      "3: Encoding Loss 7.723368167877197, Transition Loss -1.1845626831054688, Classifier Loss 0.29855698347091675, Total Loss 76.1954345703125\n",
      "3: Encoding Loss 5.845631122589111, Transition Loss -1.9981709718704224, Classifier Loss 0.1939241588115692, Total Loss 54.46540069580078\n",
      "3: Encoding Loss 4.398904323577881, Transition Loss -1.3536489009857178, Classifier Loss 0.1034678965806961, Total Loss 36.73967361450195\n",
      "3: Encoding Loss 7.110304832458496, Transition Loss -1.5412812232971191, Classifier Loss 0.16597053408622742, Total Loss 59.25826644897461\n",
      "3: Encoding Loss 6.637360572814941, Transition Loss -1.5156471729278564, Classifier Loss 0.14702105522155762, Total Loss 54.525665283203125\n",
      "3: Encoding Loss 5.3385515213012695, Transition Loss -0.15820512175559998, Classifier Loss 0.1612953543663025, Total Loss 48.16078186035156\n",
      "3: Encoding Loss 4.226193428039551, Transition Loss 0.33876779675483704, Classifier Loss 0.13250374794006348, Total Loss 38.7430419921875\n",
      "3: Encoding Loss 5.7075276374816895, Transition Loss -1.6873594522476196, Classifier Loss 0.15229013562202454, Total Loss 49.473506927490234\n",
      "3: Encoding Loss 6.734030723571777, Transition Loss -1.331113576889038, Classifier Loss 0.12440712749958038, Total Loss 52.844364166259766\n",
      "3: Encoding Loss 5.501639366149902, Transition Loss -0.46230176091194153, Classifier Loss 0.16565382480621338, Total Loss 49.575035095214844\n",
      "3: Encoding Loss 3.3662853240966797, Transition Loss -0.5692187547683716, Classifier Loss 0.08365406095981598, Total Loss 28.562891006469727\n",
      "3: Encoding Loss 5.215699195861816, Transition Loss -1.6419928073883057, Classifier Loss 0.14378321170806885, Total Loss 45.67185974121094\n",
      "3: Encoding Loss 4.01354455947876, Transition Loss 0.4366799294948578, Classifier Loss 0.1361985057592392, Total Loss 37.875789642333984\n",
      "3: Encoding Loss 6.447505950927734, Transition Loss -1.527174472808838, Classifier Loss 0.1264859437942505, Total Loss 51.3330192565918\n",
      "3: Encoding Loss 4.361542701721191, Transition Loss -1.8387079238891602, Classifier Loss 0.11629893630743027, Total Loss 37.79841232299805\n",
      "3: Encoding Loss 6.253911018371582, Transition Loss -1.0986069440841675, Classifier Loss 0.1473878026008606, Total Loss 52.261810302734375\n",
      "3: Encoding Loss 5.24282693862915, Transition Loss -1.6706610918045044, Classifier Loss 0.11581787467002869, Total Loss 43.038082122802734\n",
      "3: Encoding Loss 5.702986717224121, Transition Loss -1.3076449632644653, Classifier Loss 0.14162157475948334, Total Loss 48.37955856323242\n",
      "3: Encoding Loss 6.101624965667725, Transition Loss -1.8074103593826294, Classifier Loss 0.14808645844459534, Total Loss 51.41767501831055\n",
      "3: Encoding Loss 4.291484832763672, Transition Loss 0.3182430565357208, Classifier Loss 0.13823309540748596, Total Loss 39.69951629638672\n",
      "3: Encoding Loss 1.866158127784729, Transition Loss -0.8105753064155579, Classifier Loss 0.119736447930336, Total Loss 23.170269012451172\n",
      "3: Encoding Loss 13.038098335266113, Transition Loss -1.5951610803604126, Classifier Loss 0.14218683540821075, Total Loss 92.4466323852539\n",
      "3: Encoding Loss 13.719703674316406, Transition Loss -1.3581790924072266, Classifier Loss 0.13050544261932373, Total Loss 95.36822509765625\n",
      "3: Encoding Loss 9.091188430786133, Transition Loss -0.8200870156288147, Classifier Loss 0.1477053314447403, Total Loss 69.31733703613281\n",
      "3: Encoding Loss 6.184208869934082, Transition Loss -1.042933702468872, Classifier Loss 0.11249510943889618, Total Loss 48.35435104370117\n",
      "3: Encoding Loss 4.4877800941467285, Transition Loss -1.0439321994781494, Classifier Loss 0.1592290699481964, Total Loss 42.84917068481445\n",
      "3: Encoding Loss 5.48935604095459, Transition Loss -1.3341983556747437, Classifier Loss 0.18546763062477112, Total Loss 51.48236846923828\n",
      "3: Encoding Loss 3.584311008453369, Transition Loss -0.026862546801567078, Classifier Loss 0.09577463567256927, Total Loss 31.08331871032715\n",
      "3: Encoding Loss 7.127710819244385, Transition Loss -1.9933979511260986, Classifier Loss 0.09366942942142487, Total Loss 52.13241195678711\n",
      "3: Encoding Loss 5.559204578399658, Transition Loss -1.1746556758880615, Classifier Loss 0.07781369984149933, Total Loss 41.13612747192383\n",
      "3: Encoding Loss 5.069527626037598, Transition Loss -1.7893561124801636, Classifier Loss 0.1320730298757553, Total Loss 43.62375259399414\n",
      "3: Encoding Loss 4.350180149078369, Transition Loss -0.9864409565925598, Classifier Loss 0.0873192772269249, Total Loss 34.83261489868164\n",
      "3: Encoding Loss 4.364144802093506, Transition Loss -0.1305917650461197, Classifier Loss 0.17753131687641144, Total Loss 43.937950134277344\n",
      "3: Encoding Loss 4.963503360748291, Transition Loss -1.1772596836090088, Classifier Loss 0.077151820063591, Total Loss 37.49573516845703\n",
      "3: Encoding Loss 3.995936870574951, Transition Loss -2.0407934188842773, Classifier Loss 0.14335860311985016, Total Loss 38.310665130615234\n",
      "3: Encoding Loss 5.449749946594238, Transition Loss -1.4435808658599854, Classifier Loss 0.22081780433654785, Total Loss 54.77970504760742\n",
      "3: Encoding Loss 5.599179744720459, Transition Loss -1.2240744829177856, Classifier Loss 0.14119039475917816, Total Loss 47.71363067626953\n",
      "3: Encoding Loss 7.441638946533203, Transition Loss -1.2373132705688477, Classifier Loss 0.13098597526550293, Total Loss 57.7479362487793\n",
      "3: Encoding Loss 5.175655841827393, Transition Loss -0.5206927061080933, Classifier Loss 0.13414466381072998, Total Loss 44.46819305419922\n",
      "3: Encoding Loss 4.172819137573242, Transition Loss -0.051298707723617554, Classifier Loss 0.154178187251091, Total Loss 40.454715728759766\n",
      "3: Encoding Loss 7.8986358642578125, Transition Loss -1.1614983081817627, Classifier Loss 0.14183597266674042, Total Loss 61.574947357177734\n",
      "3: Encoding Loss 5.961247444152832, Transition Loss -1.7693657875061035, Classifier Loss 0.1824711114168167, Total Loss 54.013885498046875\n",
      "3: Encoding Loss 7.314356327056885, Transition Loss -1.9118826389312744, Classifier Loss 0.14769677817821503, Total Loss 58.655052185058594\n",
      "3: Encoding Loss 5.611422538757324, Transition Loss -1.8394341468811035, Classifier Loss 0.09508892148733139, Total Loss 43.176692962646484\n",
      "3: Encoding Loss 3.891188383102417, Transition Loss -1.773379921913147, Classifier Loss 0.16001693904399872, Total Loss 39.348114013671875\n",
      "3: Encoding Loss 6.66272497177124, Transition Loss -1.0051002502441406, Classifier Loss 0.14132125675678253, Total Loss 54.10807800292969\n",
      "3: Encoding Loss 8.448896408081055, Transition Loss -1.3500678539276123, Classifier Loss 0.17078594863414764, Total Loss 67.77143096923828\n",
      "3: Encoding Loss 6.550360679626465, Transition Loss -1.2179219722747803, Classifier Loss 0.0970764234662056, Total Loss 49.00931930541992\n",
      "3: Encoding Loss 4.142948150634766, Transition Loss -0.4193839430809021, Classifier Loss 0.10889998078346252, Total Loss 35.747520446777344\n",
      "3: Encoding Loss 3.9193387031555176, Transition Loss -0.8659195899963379, Classifier Loss 0.1379566490650177, Total Loss 37.31135177612305\n",
      "3: Encoding Loss 6.164029121398926, Transition Loss -1.250798225402832, Classifier Loss 0.11411964893341064, Total Loss 48.3956413269043\n",
      "3: Encoding Loss 4.69368314743042, Transition Loss -1.160169005393982, Classifier Loss 0.07338157296180725, Total Loss 35.49979019165039\n",
      "3: Encoding Loss 6.5785932540893555, Transition Loss -1.7503787279129028, Classifier Loss 0.08782146126031876, Total Loss 48.25300598144531\n",
      "3: Encoding Loss 4.061580657958984, Transition Loss -1.5832529067993164, Classifier Loss 0.13205790519714355, Total Loss 37.574642181396484\n",
      "3: Encoding Loss 8.51254653930664, Transition Loss -0.2364049255847931, Classifier Loss 0.2633002996444702, Total Loss 77.40522003173828\n",
      "3: Encoding Loss 6.296938419342041, Transition Loss -1.0962610244750977, Classifier Loss 0.15213121473789215, Total Loss 52.99431228637695\n",
      "3: Encoding Loss 6.2816996574401855, Transition Loss -1.2553372383117676, Classifier Loss 0.13406406342983246, Total Loss 51.09610366821289\n",
      "3: Encoding Loss 5.95581579208374, Transition Loss -0.08560554683208466, Classifier Loss 0.12846225500106812, Total Loss 48.58108901977539\n",
      "3: Encoding Loss 6.049391746520996, Transition Loss -0.6619834303855896, Classifier Loss 0.20659495890140533, Total Loss 56.95558547973633\n",
      "3: Encoding Loss 5.990667819976807, Transition Loss -1.2675191164016724, Classifier Loss 0.1774791181087494, Total Loss 53.691410064697266\n",
      "3: Encoding Loss 4.756326198577881, Transition Loss -0.7474440932273865, Classifier Loss 0.11561407148838043, Total Loss 40.09906768798828\n",
      "3: Encoding Loss 5.066570281982422, Transition Loss -1.6405166387557983, Classifier Loss 0.15528587996959686, Total Loss 45.9273567199707\n",
      "3: Encoding Loss 5.134491920471191, Transition Loss -0.7116743326187134, Classifier Loss 0.1770523339509964, Total Loss 48.511898040771484\n",
      "3: Encoding Loss 6.018139839172363, Transition Loss -0.36695048213005066, Classifier Loss 0.14778268337249756, Total Loss 50.886962890625\n",
      "3: Encoding Loss 3.6527645587921143, Transition Loss -1.039839506149292, Classifier Loss 0.0971648320555687, Total Loss 31.63265609741211\n",
      "3: Encoding Loss 4.0673980712890625, Transition Loss -0.9472651481628418, Classifier Loss 0.14036864042282104, Total Loss 38.44087600708008\n",
      "3: Encoding Loss 3.657263994216919, Transition Loss -1.5031754970550537, Classifier Loss 0.10918523371219635, Total Loss 32.86150360107422\n",
      "3: Encoding Loss 4.881925582885742, Transition Loss -0.8074327707290649, Classifier Loss 0.1060340404510498, Total Loss 39.89463424682617\n",
      "3: Encoding Loss 5.1819562911987305, Transition Loss -1.0693814754486084, Classifier Loss 0.1495092660188675, Total Loss 46.042240142822266\n",
      "3: Encoding Loss 4.1487932205200195, Transition Loss -2.000986099243164, Classifier Loss 0.10902944207191467, Total Loss 35.79490280151367\n",
      "3: Encoding Loss 5.54372501373291, Transition Loss -1.178447961807251, Classifier Loss 0.17461389303207397, Total Loss 50.7232666015625\n",
      "3: Encoding Loss 4.10099458694458, Transition Loss -0.3159434199333191, Classifier Loss 0.09636293351650238, Total Loss 34.24213790893555\n",
      "3: Encoding Loss 3.52504301071167, Transition Loss -0.5491452217102051, Classifier Loss 0.09487821161746979, Total Loss 30.637861251831055\n",
      "3: Encoding Loss 5.103461742401123, Transition Loss -0.6317614316940308, Classifier Loss 0.23032329976558685, Total Loss 53.65284729003906\n",
      "3: Encoding Loss 6.012236595153809, Transition Loss -1.4596047401428223, Classifier Loss 0.21429070830345154, Total Loss 57.50191116333008\n",
      "3: Encoding Loss 4.636775493621826, Transition Loss -0.7672485709190369, Classifier Loss 0.16415748000144958, Total Loss 44.23609924316406\n",
      "3: Encoding Loss 6.356390476226807, Transition Loss -1.4949973821640015, Classifier Loss 0.16346265375614166, Total Loss 54.484012603759766\n",
      "3: Encoding Loss 4.538005828857422, Transition Loss -1.8796107769012451, Classifier Loss 0.12698839604854584, Total Loss 39.926124572753906\n",
      "3: Encoding Loss 6.310101509094238, Transition Loss -1.5841565132141113, Classifier Loss 0.1743975728750229, Total Loss 55.29973602294922\n",
      "3: Encoding Loss 5.340684413909912, Transition Loss -0.988071620464325, Classifier Loss 0.099037304520607, Total Loss 41.94744110107422\n",
      "3: Encoding Loss 3.0688414573669434, Transition Loss -1.0950348377227783, Classifier Loss 0.14583522081375122, Total Loss 32.996131896972656\n",
      "3: Encoding Loss 4.171013355255127, Transition Loss -1.9455695152282715, Classifier Loss 0.10422693192958832, Total Loss 35.447998046875\n",
      "3: Encoding Loss 3.9499294757843018, Transition Loss -0.44075366854667664, Classifier Loss 0.19028741121292114, Total Loss 42.72814178466797\n",
      "3: Encoding Loss 6.960333347320557, Transition Loss -0.8879144191741943, Classifier Loss 0.09666796028614044, Total Loss 51.42844009399414\n",
      "3: Encoding Loss 5.824496269226074, Transition Loss -1.2240370512008667, Classifier Loss 0.26683080196380615, Total Loss 61.62957000732422\n",
      "3: Encoding Loss 6.7804975509643555, Transition Loss -0.36563125252723694, Classifier Loss 0.1630832850933075, Total Loss 56.991172790527344\n",
      "3: Encoding Loss 5.443317413330078, Transition Loss -1.952215552330017, Classifier Loss 0.18679198622703552, Total Loss 51.338321685791016\n",
      "3: Encoding Loss 4.815648078918457, Transition Loss -0.9808146357536316, Classifier Loss 0.09662507474422455, Total Loss 38.55600357055664\n",
      "3: Encoding Loss 3.090549945831299, Transition Loss -1.9841147661209106, Classifier Loss 0.11743535846471786, Total Loss 30.286043167114258\n",
      "3: Encoding Loss 2.7578768730163574, Transition Loss -1.1274592876434326, Classifier Loss 0.11702881008386612, Total Loss 28.249692916870117\n",
      "3: Encoding Loss 4.328319549560547, Transition Loss -2.053070545196533, Classifier Loss 0.12791912257671356, Total Loss 38.76101303100586\n",
      "3: Encoding Loss 6.670448303222656, Transition Loss -0.18732547760009766, Classifier Loss 0.10514310002326965, Total Loss 50.536922454833984\n",
      "3: Encoding Loss 5.845977306365967, Transition Loss -0.456318736076355, Classifier Loss 0.14228083193302155, Total Loss 49.303768157958984\n",
      "3: Encoding Loss 6.519739627838135, Transition Loss -2.5764236450195312, Classifier Loss 0.10954337567090988, Total Loss 50.071746826171875\n",
      "3: Encoding Loss 5.565690517425537, Transition Loss -1.6407802104949951, Classifier Loss 0.12314855307340622, Total Loss 45.708343505859375\n",
      "3: Encoding Loss 4.7148847579956055, Transition Loss -0.29705023765563965, Classifier Loss 0.132078155875206, Total Loss 41.497005462646484\n",
      "3: Encoding Loss 5.251342296600342, Transition Loss -1.9046025276184082, Classifier Loss 0.10096416622400284, Total Loss 41.60370635986328\n",
      "3: Encoding Loss 7.5150299072265625, Transition Loss -1.0727009773254395, Classifier Loss 0.1581927090883255, Total Loss 60.90902328491211\n",
      "3: Encoding Loss 6.381978511810303, Transition Loss -2.1148738861083984, Classifier Loss 0.19033126533031464, Total Loss 57.324153900146484\n",
      "3: Encoding Loss 6.456783294677734, Transition Loss -1.4675143957138062, Classifier Loss 0.13397163152694702, Total Loss 52.13727569580078\n",
      "3: Encoding Loss 6.00847053527832, Transition Loss -0.671703577041626, Classifier Loss 0.19362105429172516, Total Loss 55.412662506103516\n",
      "3: Encoding Loss 6.19799280166626, Transition Loss -1.610319972038269, Classifier Loss 0.06405109912157059, Total Loss 43.59242248535156\n",
      "3: Encoding Loss 5.3533501625061035, Transition Loss -1.4144415855407715, Classifier Loss 0.1974228471517563, Total Loss 51.86182403564453\n",
      "3: Encoding Loss 4.200463771820068, Transition Loss -0.6610491275787354, Classifier Loss 0.11446493864059448, Total Loss 36.64901351928711\n",
      "3: Encoding Loss 7.253641605377197, Transition Loss -1.8463841676712036, Classifier Loss 0.14363008737564087, Total Loss 57.884117126464844\n",
      "3: Encoding Loss 6.2989630699157715, Transition Loss -0.7749305367469788, Classifier Loss 0.1519554853439331, Total Loss 52.98902130126953\n",
      "3: Encoding Loss 5.300045967102051, Transition Loss -0.7396522164344788, Classifier Loss 0.11258681863546371, Total Loss 43.05866241455078\n",
      "3: Encoding Loss 4.579390525817871, Transition Loss -1.1343011856079102, Classifier Loss 0.15441575646400452, Total Loss 42.9174690246582\n",
      "3: Encoding Loss 5.182971954345703, Transition Loss -0.9586765766143799, Classifier Loss 0.1541527956724167, Total Loss 46.51272964477539\n",
      "3: Encoding Loss 5.681046962738037, Transition Loss -0.632596492767334, Classifier Loss 0.16093146800994873, Total Loss 50.17918014526367\n",
      "3: Encoding Loss 5.073572635650635, Transition Loss -2.4703831672668457, Classifier Loss 0.08704061061143875, Total Loss 39.14451217651367\n",
      "3: Encoding Loss 2.680919647216797, Transition Loss -1.3537296056747437, Classifier Loss 0.12848864495754242, Total Loss 28.933841705322266\n",
      "3: Encoding Loss 10.719555854797363, Transition Loss -1.8099199533462524, Classifier Loss 0.18847373127937317, Total Loss 83.16398620605469\n",
      "3: Encoding Loss 9.245612144470215, Transition Loss -0.26884305477142334, Classifier Loss 0.07936062663793564, Total Loss 63.409629821777344\n",
      "3: Encoding Loss 6.306901931762695, Transition Loss -0.790185809135437, Classifier Loss 0.19895735383033752, Total Loss 57.7368278503418\n",
      "3: Encoding Loss 5.214423179626465, Transition Loss -1.0114893913269043, Classifier Loss 0.18388588726520538, Total Loss 49.67472457885742\n",
      "3: Encoding Loss 6.402455806732178, Transition Loss -0.7460566759109497, Classifier Loss 0.1084403470158577, Total Loss 49.25847625732422\n",
      "3: Encoding Loss 6.8415350914001465, Transition Loss -1.2912158966064453, Classifier Loss 0.265167772769928, Total Loss 67.56547546386719\n",
      "3: Encoding Loss 4.954663276672363, Transition Loss -0.5005812048912048, Classifier Loss 0.11931794881820679, Total Loss 41.659576416015625\n",
      "3: Encoding Loss 5.1210174560546875, Transition Loss 0.1835474818944931, Classifier Loss 0.14095096290111542, Total Loss 44.894622802734375\n",
      "3: Encoding Loss 5.310437202453613, Transition Loss -0.7470799088478088, Classifier Loss 0.12437058240175247, Total Loss 44.29938507080078\n",
      "3: Encoding Loss 3.9115536212921143, Transition Loss -0.018337488174438477, Classifier Loss 0.0660611018538475, Total Loss 30.075424194335938\n",
      "3: Encoding Loss 6.753969669342041, Transition Loss -1.036447286605835, Classifier Loss 0.12214404344558716, Total Loss 52.73780822753906\n",
      "3: Encoding Loss 7.173111915588379, Transition Loss -1.4427900314331055, Classifier Loss 0.20638705790042877, Total Loss 63.67680358886719\n",
      "3: Encoding Loss 4.804721832275391, Transition Loss -1.0041686296463013, Classifier Loss 0.11999297142028809, Total Loss 40.82722854614258\n",
      "3: Encoding Loss 4.055129051208496, Transition Loss -1.5364903211593628, Classifier Loss 0.12686306238174438, Total Loss 37.0164680480957\n",
      "3: Encoding Loss 4.058775424957275, Transition Loss -1.1019978523254395, Classifier Loss 0.11820518225431442, Total Loss 36.1727294921875\n",
      "3: Encoding Loss 4.924332141876221, Transition Loss -0.42434167861938477, Classifier Loss 0.0849665030837059, Total Loss 38.042476654052734\n",
      "3: Encoding Loss 5.8183393478393555, Transition Loss -0.9597789645195007, Classifier Loss 0.1793823540210724, Total Loss 52.8478889465332\n",
      "3: Encoding Loss 3.7775065898895264, Transition Loss -0.5187987685203552, Classifier Loss 0.18205371499061584, Total Loss 40.870208740234375\n",
      "3: Encoding Loss 5.752966403961182, Transition Loss -1.8275649547576904, Classifier Loss 0.11438979208469391, Total Loss 45.95604705810547\n",
      "3: Encoding Loss 4.0011491775512695, Transition Loss -0.9376263618469238, Classifier Loss 0.08317010849714279, Total Loss 32.32353210449219\n",
      "3: Encoding Loss 4.971338272094727, Transition Loss -2.228222608566284, Classifier Loss 0.07833977788686752, Total Loss 37.66111755371094\n",
      "3: Encoding Loss 2.481991767883301, Transition Loss -1.3498278856277466, Classifier Loss 0.07626508176326752, Total Loss 22.517919540405273\n",
      "3: Encoding Loss 6.936145782470703, Transition Loss -1.3624626398086548, Classifier Loss 0.16971537470817566, Total Loss 58.58786392211914\n",
      "3: Encoding Loss 4.83060359954834, Transition Loss -1.5080606937408447, Classifier Loss 0.11997167766094208, Total Loss 40.980186462402344\n",
      "3: Encoding Loss 3.611619472503662, Transition Loss -0.675147294998169, Classifier Loss 0.11205975711345673, Total Loss 32.875423431396484\n",
      "3: Encoding Loss 6.053794860839844, Transition Loss 0.012889787554740906, Classifier Loss 0.14733591675758362, Total Loss 51.061519622802734\n",
      "3: Encoding Loss 6.479024410247803, Transition Loss -1.4784893989562988, Classifier Loss 0.1810954213142395, Total Loss 56.98310089111328\n",
      "3: Encoding Loss 6.5987138748168945, Transition Loss -2.6811742782592773, Classifier Loss 0.17213644087314606, Total Loss 56.80485916137695\n",
      "3: Encoding Loss 4.894237995147705, Transition Loss -1.4659947156906128, Classifier Loss 0.12266774475574493, Total Loss 41.631614685058594\n",
      "3: Encoding Loss 5.402073860168457, Transition Loss -1.9265809059143066, Classifier Loss 0.08368002623319626, Total Loss 40.77967834472656\n",
      "3: Encoding Loss 2.7526559829711914, Transition Loss -2.2622697353363037, Classifier Loss 0.08915968239307404, Total Loss 25.430999755859375\n",
      "3: Encoding Loss 5.612554550170898, Transition Loss -2.3781864643096924, Classifier Loss 0.22595065832138062, Total Loss 56.26944351196289\n",
      "3: Encoding Loss 5.505585670471191, Transition Loss -0.7358239889144897, Classifier Loss 0.21443887054920197, Total Loss 54.477108001708984\n",
      "3: Encoding Loss 7.325422763824463, Transition Loss -0.5222628116607666, Classifier Loss 0.12362419068813324, Total Loss 56.31474685668945\n",
      "3: Encoding Loss 4.283962726593018, Transition Loss -1.013580322265625, Classifier Loss 0.10905413329601288, Total Loss 36.608787536621094\n",
      "3: Encoding Loss 3.4773426055908203, Transition Loss -1.5278267860412598, Classifier Loss 0.11387799680233002, Total Loss 32.251243591308594\n",
      "3: Encoding Loss 6.140907287597656, Transition Loss -1.331526756286621, Classifier Loss 0.08608986437320709, Total Loss 45.453895568847656\n",
      "3: Encoding Loss 5.135695457458496, Transition Loss -0.8749087452888489, Classifier Loss 0.0923415869474411, Total Loss 40.04798126220703\n",
      "3: Encoding Loss 5.03458833694458, Transition Loss -1.3010591268539429, Classifier Loss 0.11593151092529297, Total Loss 41.80016326904297\n",
      "3: Encoding Loss 5.211118221282959, Transition Loss -1.3440780639648438, Classifier Loss 0.10348375141620636, Total Loss 41.61454772949219\n",
      "3: Encoding Loss 5.68564510345459, Transition Loss -1.2690975666046143, Classifier Loss 0.09402956068515778, Total Loss 43.51632308959961\n",
      "3: Encoding Loss 4.91473388671875, Transition Loss -1.1772732734680176, Classifier Loss 0.15744683146476746, Total Loss 45.23262023925781\n",
      "3: Encoding Loss 4.255017280578613, Transition Loss -0.8346636891365051, Classifier Loss 0.11876759678125381, Total Loss 37.40652847290039\n",
      "3: Encoding Loss 5.929500579833984, Transition Loss 0.044017769396305084, Classifier Loss 0.13800083100795746, Total Loss 49.39469528198242\n",
      "3: Encoding Loss 4.702159404754639, Transition Loss -1.5944674015045166, Classifier Loss 0.10126695036888123, Total Loss 38.33901596069336\n",
      "3: Encoding Loss 5.13109016418457, Transition Loss -1.5052318572998047, Classifier Loss 0.124603271484375, Total Loss 43.24626922607422\n",
      "3: Encoding Loss 4.519430160522461, Transition Loss -0.822026252746582, Classifier Loss 0.16919073462486267, Total Loss 44.03532791137695\n",
      "3: Encoding Loss 4.271862983703613, Transition Loss -0.3179757595062256, Classifier Loss 0.09341318905353546, Total Loss 34.97237014770508\n",
      "3: Encoding Loss 5.092804908752441, Transition Loss -1.7937170267105103, Classifier Loss 0.10615859925746918, Total Loss 41.171974182128906\n",
      "3: Encoding Loss 4.724041938781738, Transition Loss -0.8842884302139282, Classifier Loss 0.0930103063583374, Total Loss 37.644927978515625\n",
      "3: Encoding Loss 6.293449401855469, Transition Loss -0.28189617395401, Classifier Loss 0.2109355926513672, Total Loss 58.85414123535156\n",
      "3: Encoding Loss 6.136960506439209, Transition Loss -1.4884333610534668, Classifier Loss 0.11971768736839294, Total Loss 48.792938232421875\n",
      "3: Encoding Loss 6.482243537902832, Transition Loss -0.6556540727615356, Classifier Loss 0.18478238582611084, Total Loss 57.37144088745117\n",
      "3: Encoding Loss 4.735670566558838, Transition Loss -0.957083523273468, Classifier Loss 0.07497766613960266, Total Loss 35.911407470703125\n",
      "3: Encoding Loss 4.937060356140137, Transition Loss -0.9100955128669739, Classifier Loss 0.15989936888217926, Total Loss 45.6119384765625\n",
      "3: Encoding Loss 3.0109198093414307, Transition Loss -1.185186743736267, Classifier Loss 0.1443749964237213, Total Loss 32.50254821777344\n",
      "3: Encoding Loss 6.567261695861816, Transition Loss -0.5101833343505859, Classifier Loss 0.15930303931236267, Total Loss 55.333675384521484\n",
      "3: Encoding Loss 5.927338600158691, Transition Loss -1.1935312747955322, Classifier Loss 0.14412513375282288, Total Loss 49.976070404052734\n",
      "3: Encoding Loss 4.151012420654297, Transition Loss -0.558708131313324, Classifier Loss 0.14548832178115845, Total Loss 39.45468521118164\n",
      "3: Encoding Loss 5.254471778869629, Transition Loss -0.9922682642936707, Classifier Loss 0.13563747704029083, Total Loss 45.090179443359375\n",
      "3: Encoding Loss 6.146359443664551, Transition Loss -0.6428303718566895, Classifier Loss 0.1090085431933403, Total Loss 47.77875900268555\n",
      "3: Encoding Loss 3.994424343109131, Transition Loss -0.0013343840837478638, Classifier Loss 0.1117127388715744, Total Loss 35.137821197509766\n",
      "3: Encoding Loss 6.8795270919799805, Transition Loss -0.30200502276420593, Classifier Loss 0.17139336466789246, Total Loss 58.416378021240234\n",
      "3: Encoding Loss 6.016077995300293, Transition Loss -1.1145309209823608, Classifier Loss 0.1709183305501938, Total Loss 53.1878547668457\n",
      "3: Encoding Loss 4.624561309814453, Transition Loss -1.2141329050064087, Classifier Loss 0.12197620421648026, Total Loss 39.94450759887695\n",
      "3: Encoding Loss 4.306328773498535, Transition Loss -1.347063660621643, Classifier Loss 0.18552586436271667, Total Loss 44.390018463134766\n",
      "3: Encoding Loss 6.061796188354492, Transition Loss -0.31606876850128174, Classifier Loss 0.14948099851608276, Total Loss 51.31875228881836\n",
      "3: Encoding Loss 5.460092544555664, Transition Loss -1.2403926849365234, Classifier Loss 0.1645922064781189, Total Loss 49.21928024291992\n",
      "3: Encoding Loss 5.987180233001709, Transition Loss -1.0962835550308228, Classifier Loss 0.18910115957260132, Total Loss 54.832759857177734\n",
      "3: Encoding Loss 5.353610992431641, Transition Loss -1.070946455001831, Classifier Loss 0.11136111617088318, Total Loss 43.25735092163086\n",
      "3: Encoding Loss 6.1259942054748535, Transition Loss -1.4138643741607666, Classifier Loss 0.12482894212007523, Total Loss 49.23829650878906\n",
      "3: Encoding Loss 5.780645370483398, Transition Loss -0.8434016704559326, Classifier Loss 0.1365138292312622, Total Loss 48.33491897583008\n",
      "3: Encoding Loss 5.064370632171631, Transition Loss -0.9857640862464905, Classifier Loss 0.13408058881759644, Total Loss 43.79389190673828\n",
      "3: Encoding Loss 5.086208820343018, Transition Loss -0.23799291253089905, Classifier Loss 0.11017059534788132, Total Loss 41.534217834472656\n",
      "3: Encoding Loss 3.920245409011841, Transition Loss -0.4191078543663025, Classifier Loss 0.0883890837430954, Total Loss 32.36021423339844\n",
      "3: Encoding Loss 7.754713535308838, Transition Loss -1.407105565071106, Classifier Loss 0.18572574853897095, Total Loss 65.10029602050781\n",
      "3: Encoding Loss 8.471680641174316, Transition Loss -1.5308786630630493, Classifier Loss 0.14627379179000854, Total Loss 65.45685577392578\n",
      "3: Encoding Loss 4.649228096008301, Transition Loss -0.7496945261955261, Classifier Loss 0.13250145316123962, Total Loss 41.14521408081055\n",
      "3: Encoding Loss 6.541207313537598, Transition Loss -1.3988797664642334, Classifier Loss 0.12289769947528839, Total Loss 51.53645706176758\n",
      "3: Encoding Loss 5.220990180969238, Transition Loss -0.9553006887435913, Classifier Loss 0.11380012333393097, Total Loss 42.70557403564453\n",
      "3: Encoding Loss 4.733550071716309, Transition Loss -0.9588766098022461, Classifier Loss 0.08773732930421829, Total Loss 37.17464828491211\n",
      "3: Encoding Loss 6.630040645599365, Transition Loss -1.6048907041549683, Classifier Loss 0.12877511978149414, Total Loss 52.65711975097656\n",
      "3: Encoding Loss 6.1827392578125, Transition Loss -0.9809363484382629, Classifier Loss 0.20263414084911346, Total Loss 57.35945510864258\n",
      "3: Encoding Loss 5.429587364196777, Transition Loss -1.1758521795272827, Classifier Loss 0.12153038382530212, Total Loss 44.73009490966797\n",
      "3: Encoding Loss 4.765863418579102, Transition Loss -0.24200737476348877, Classifier Loss 0.13824492692947388, Total Loss 42.419578552246094\n",
      "3: Encoding Loss 5.610993385314941, Transition Loss -1.9317240715026855, Classifier Loss 0.12375703454017639, Total Loss 46.040889739990234\n",
      "3: Encoding Loss 5.191813945770264, Transition Loss -1.5744590759277344, Classifier Loss 0.11472395807504654, Total Loss 42.622650146484375\n",
      "3: Encoding Loss 6.2186598777771, Transition Loss 0.06071804463863373, Classifier Loss 0.15644893050193787, Total Loss 52.981143951416016\n",
      "3: Encoding Loss 7.855931282043457, Transition Loss -1.2771801948547363, Classifier Loss 0.16711972653865814, Total Loss 63.84705352783203\n",
      "3: Encoding Loss 5.500101566314697, Transition Loss -1.81315016746521, Classifier Loss 0.09157811850309372, Total Loss 42.15769958496094\n",
      "3: Encoding Loss 5.952303886413574, Transition Loss -0.993480384349823, Classifier Loss 0.08540210127830505, Total Loss 44.253639221191406\n",
      "3: Encoding Loss 5.607771873474121, Transition Loss -0.662246584892273, Classifier Loss 0.19420185685157776, Total Loss 53.06655502319336\n",
      "3: Encoding Loss 5.273446083068848, Transition Loss -1.1019316911697388, Classifier Loss 0.07991917431354523, Total Loss 39.63215255737305\n",
      "4: Encoding Loss 4.083630561828613, Transition Loss -1.034286379814148, Classifier Loss 0.11373841017484665, Total Loss 35.875213623046875\n",
      "4: Encoding Loss 6.691716194152832, Transition Loss -0.4084858298301697, Classifier Loss 0.13155300915241241, Total Loss 53.30543518066406\n",
      "4: Encoding Loss 5.815108776092529, Transition Loss -1.4557383060455322, Classifier Loss 0.138755202293396, Total Loss 48.76559066772461\n",
      "4: Encoding Loss 6.843557834625244, Transition Loss -1.0473926067352295, Classifier Loss 0.10978307574987411, Total Loss 52.03923797607422\n",
      "4: Encoding Loss 7.1627936363220215, Transition Loss 0.08151040971279144, Classifier Loss 0.21084602177143097, Total Loss 64.0939712524414\n",
      "4: Encoding Loss 6.796841621398926, Transition Loss -2.2649340629577637, Classifier Loss 0.09379034489393234, Total Loss 50.159183502197266\n",
      "4: Encoding Loss 4.4195756912231445, Transition Loss -1.7027740478515625, Classifier Loss 0.1580360382795334, Total Loss 42.320377349853516\n",
      "4: Encoding Loss 3.368244171142578, Transition Loss -1.3790757656097412, Classifier Loss 0.15791788697242737, Total Loss 36.000701904296875\n",
      "4: Encoding Loss 5.589260101318359, Transition Loss -1.1855833530426025, Classifier Loss 0.1655023843050003, Total Loss 50.0853271484375\n",
      "4: Encoding Loss 3.962683916091919, Transition Loss -0.7442611455917358, Classifier Loss 0.12460355460643768, Total Loss 36.23616027832031\n",
      "4: Encoding Loss 6.248035907745361, Transition Loss -1.231245994567871, Classifier Loss 0.1787218302488327, Total Loss 55.35990524291992\n",
      "4: Encoding Loss 4.256699085235596, Transition Loss -1.214577555656433, Classifier Loss 0.11873696744441986, Total Loss 37.41340637207031\n",
      "4: Encoding Loss 2.8711721897125244, Transition Loss -0.7266002297401428, Classifier Loss 0.08558963239192963, Total Loss 25.78570556640625\n",
      "4: Encoding Loss 5.075343132019043, Transition Loss -1.9812079668045044, Classifier Loss 0.09850355982780457, Total Loss 40.3016242980957\n",
      "4: Encoding Loss 3.938088893890381, Transition Loss -1.771720290184021, Classifier Loss 0.12722264230251312, Total Loss 36.35009002685547\n",
      "4: Encoding Loss 4.886336326599121, Transition Loss -1.191868782043457, Classifier Loss 0.11170884966850281, Total Loss 40.48843002319336\n",
      "4: Encoding Loss 4.481896877288818, Transition Loss -1.4729492664337158, Classifier Loss 0.12349673360586166, Total Loss 39.2404670715332\n",
      "4: Encoding Loss 3.607933759689331, Transition Loss 0.0037284791469573975, Classifier Loss 0.0985550582408905, Total Loss 31.504600524902344\n",
      "4: Encoding Loss 7.123109340667725, Transition Loss -2.116492748260498, Classifier Loss 0.1395498663187027, Total Loss 56.69279861450195\n",
      "4: Encoding Loss 5.157597541809082, Transition Loss -0.7991591095924377, Classifier Loss 0.2035953402519226, Total Loss 51.3047981262207\n",
      "4: Encoding Loss 6.7689008712768555, Transition Loss -2.1621651649475098, Classifier Loss 0.24042440950870514, Total Loss 64.65498352050781\n",
      "4: Encoding Loss 5.599992275238037, Transition Loss -1.1296923160552979, Classifier Loss 0.14335547387599945, Total Loss 47.935054779052734\n",
      "4: Encoding Loss 6.489798545837402, Transition Loss -0.6333812475204468, Classifier Loss 0.12167107313871384, Total Loss 51.105648040771484\n",
      "4: Encoding Loss 6.502419471740723, Transition Loss -0.4008159935474396, Classifier Loss 0.1256602257490158, Total Loss 51.58038330078125\n",
      "4: Encoding Loss 5.714910984039307, Transition Loss 0.18513639271259308, Classifier Loss 0.10076625645160675, Total Loss 44.440147399902344\n",
      "4: Encoding Loss 5.332248687744141, Transition Loss -1.6238216161727905, Classifier Loss 0.11760801076889038, Total Loss 43.75364685058594\n",
      "4: Encoding Loss 3.502964735031128, Transition Loss -0.4820009469985962, Classifier Loss 0.11632181704044342, Total Loss 32.649776458740234\n",
      "4: Encoding Loss 8.873071670532227, Transition Loss -1.4038300514221191, Classifier Loss 0.1882323920726776, Total Loss 72.06110382080078\n",
      "4: Encoding Loss 5.7721991539001465, Transition Loss -1.9873963594436646, Classifier Loss 0.10210689157247543, Total Loss 44.84309387207031\n",
      "4: Encoding Loss 4.060644626617432, Transition Loss -1.3076403141021729, Classifier Loss 0.17195965349674225, Total Loss 41.55931091308594\n",
      "4: Encoding Loss 4.827564716339111, Transition Loss -1.579293966293335, Classifier Loss 0.11768373847007751, Total Loss 40.733131408691406\n",
      "4: Encoding Loss 4.7901716232299805, Transition Loss 0.3971598744392395, Classifier Loss 0.1030111312866211, Total Loss 39.20100784301758\n",
      "4: Encoding Loss 6.227786064147949, Transition Loss -0.9634835124015808, Classifier Loss 0.19015465676784515, Total Loss 56.381797790527344\n",
      "4: Encoding Loss 4.4784369468688965, Transition Loss -1.5037113428115845, Classifier Loss 0.1142161414027214, Total Loss 38.29163360595703\n",
      "4: Encoding Loss 4.369915962219238, Transition Loss -1.1482007503509521, Classifier Loss 0.1612914800643921, Total Loss 42.34818649291992\n",
      "4: Encoding Loss 5.167768955230713, Transition Loss -1.2597140073776245, Classifier Loss 0.26436173915863037, Total Loss 57.442283630371094\n",
      "4: Encoding Loss 4.951600551605225, Transition Loss -1.217603087425232, Classifier Loss 0.13173963129520416, Total Loss 42.883079528808594\n",
      "4: Encoding Loss 4.904598712921143, Transition Loss -0.3895452320575714, Classifier Loss 0.11878818273544312, Total Loss 41.30625534057617\n",
      "4: Encoding Loss 8.49788761138916, Transition Loss 0.048431396484375, Classifier Loss 0.21057964861392975, Total Loss 72.06465911865234\n",
      "4: Encoding Loss 7.683157920837402, Transition Loss -1.623555064201355, Classifier Loss 0.13538973033428192, Total Loss 59.63727569580078\n",
      "4: Encoding Loss 5.4289398193359375, Transition Loss -1.0858668088912964, Classifier Loss 0.14838699996471405, Total Loss 47.411903381347656\n",
      "4: Encoding Loss 6.444886684417725, Transition Loss -1.1079097986221313, Classifier Loss 0.14373554289340973, Total Loss 53.04243469238281\n",
      "4: Encoding Loss 5.7735700607299805, Transition Loss -1.1253308057785034, Classifier Loss 0.15382958948612213, Total Loss 50.02393341064453\n",
      "4: Encoding Loss 8.893863677978516, Transition Loss -2.1029744148254395, Classifier Loss 0.11601912975311279, Total Loss 64.96426391601562\n",
      "4: Encoding Loss 6.156264305114746, Transition Loss -1.5436699390411377, Classifier Loss 0.13871783018112183, Total Loss 50.808753967285156\n",
      "4: Encoding Loss 3.5387120246887207, Transition Loss -0.02080819010734558, Classifier Loss 0.09935808926820755, Total Loss 31.168075561523438\n",
      "4: Encoding Loss 4.381933212280273, Transition Loss -0.8959425687789917, Classifier Loss 0.1464066207408905, Total Loss 40.93190383911133\n",
      "4: Encoding Loss 6.735867977142334, Transition Loss -0.10092709958553314, Classifier Loss 0.13535743951797485, Total Loss 53.95091247558594\n",
      "4: Encoding Loss 6.8540544509887695, Transition Loss -0.4469236135482788, Classifier Loss 0.21946275234222412, Total Loss 63.0704231262207\n",
      "4: Encoding Loss 4.498566150665283, Transition Loss -0.380298376083374, Classifier Loss 0.1091611459851265, Total Loss 37.9073600769043\n",
      "4: Encoding Loss 4.624524116516113, Transition Loss -1.4032914638519287, Classifier Loss 0.15883636474609375, Total Loss 43.63022232055664\n",
      "4: Encoding Loss 3.8754096031188965, Transition Loss -1.550581693649292, Classifier Loss 0.08258585631847382, Total Loss 31.51042366027832\n",
      "4: Encoding Loss 8.00599479675293, Transition Loss -1.153976321220398, Classifier Loss 0.11259940266609192, Total Loss 59.29545211791992\n",
      "4: Encoding Loss 6.10425329208374, Transition Loss -0.02094791829586029, Classifier Loss 0.1829186975955963, Total Loss 54.91738510131836\n",
      "4: Encoding Loss 3.227418899536133, Transition Loss -1.6606435775756836, Classifier Loss 0.1417972445487976, Total Loss 33.543575286865234\n",
      "4: Encoding Loss 7.928567409515381, Transition Loss -0.33177125453948975, Classifier Loss 0.0985259860754013, Total Loss 57.42387008666992\n",
      "4: Encoding Loss 6.937228202819824, Transition Loss -0.6350226402282715, Classifier Loss 0.17967063188552856, Total Loss 59.590179443359375\n",
      "4: Encoding Loss 4.34610652923584, Transition Loss -1.044536828994751, Classifier Loss 0.1009540781378746, Total Loss 36.171630859375\n",
      "4: Encoding Loss 11.44711685180664, Transition Loss -1.759621262550354, Classifier Loss 0.21779049932956696, Total Loss 90.46105194091797\n",
      "4: Encoding Loss 9.771377563476562, Transition Loss -1.4189612865447998, Classifier Loss 0.08364742994308472, Total Loss 66.99244689941406\n",
      "4: Encoding Loss 7.187234401702881, Transition Loss -0.7923299074172974, Classifier Loss 0.12350717186927795, Total Loss 55.473812103271484\n",
      "4: Encoding Loss 5.28855037689209, Transition Loss -2.0788204669952393, Classifier Loss 0.11088376492261887, Total Loss 42.81884765625\n",
      "4: Encoding Loss 3.938688278198242, Transition Loss -1.9814704656600952, Classifier Loss 0.153250053524971, Total Loss 38.95634078979492\n",
      "4: Encoding Loss 5.1758904457092285, Transition Loss -3.3248560428619385, Classifier Loss 0.13792166113853455, Total Loss 44.8461799621582\n",
      "4: Encoding Loss 4.924036979675293, Transition Loss -0.8192227482795715, Classifier Loss 0.1409069150686264, Total Loss 43.634586334228516\n",
      "4: Encoding Loss 5.849991321563721, Transition Loss -1.7186684608459473, Classifier Loss 0.22219914197921753, Total Loss 57.319175720214844\n",
      "4: Encoding Loss 3.6642708778381348, Transition Loss -1.131568431854248, Classifier Loss 0.2222352921962738, Total Loss 44.208702087402344\n",
      "4: Encoding Loss 6.154685974121094, Transition Loss -1.3697236776351929, Classifier Loss 0.13057014346122742, Total Loss 49.984580993652344\n",
      "4: Encoding Loss 4.7026848793029785, Transition Loss -0.2566034197807312, Classifier Loss 0.12330955266952515, Total Loss 40.54696273803711\n",
      "4: Encoding Loss 2.9001383781433105, Transition Loss -0.2150557041168213, Classifier Loss 0.1395532637834549, Total Loss 31.35607147216797\n",
      "4: Encoding Loss 5.147076606750488, Transition Loss -1.8076680898666382, Classifier Loss 0.16476093232631683, Total Loss 47.357826232910156\n",
      "4: Encoding Loss 3.341027021408081, Transition Loss -1.4130650758743286, Classifier Loss 0.13261114060878754, Total Loss 33.30671310424805\n",
      "4: Encoding Loss 2.7924678325653076, Transition Loss -1.885199785232544, Classifier Loss 0.1360805481672287, Total Loss 30.362110137939453\n",
      "4: Encoding Loss 7.0786614418029785, Transition Loss -0.412110298871994, Classifier Loss 0.08434248715639114, Total Loss 50.90605545043945\n",
      "4: Encoding Loss 5.2467546463012695, Transition Loss -1.3207411766052246, Classifier Loss 0.09575003385543823, Total Loss 41.05500411987305\n",
      "4: Encoding Loss 5.704419136047363, Transition Loss -1.4941534996032715, Classifier Loss 0.1686958223581314, Total Loss 51.09550094604492\n",
      "4: Encoding Loss 4.357857704162598, Transition Loss -1.6949832439422607, Classifier Loss 0.07654810696840286, Total Loss 33.80127716064453\n",
      "4: Encoding Loss 7.2328667640686035, Transition Loss -1.4277654886245728, Classifier Loss 0.13065382838249207, Total Loss 56.462013244628906\n",
      "4: Encoding Loss 5.169302940368652, Transition Loss -1.4197890758514404, Classifier Loss 0.17837603390216827, Total Loss 48.85285568237305\n",
      "4: Encoding Loss 3.3476409912109375, Transition Loss -1.4289345741271973, Classifier Loss 0.08723963797092438, Total Loss 28.80923843383789\n",
      "4: Encoding Loss 8.477781295776367, Transition Loss -0.8251404762268066, Classifier Loss 0.10213644802570343, Total Loss 61.08000564575195\n",
      "4: Encoding Loss 6.895317077636719, Transition Loss -0.9982073307037354, Classifier Loss 0.18250219523906708, Total Loss 59.62172317504883\n",
      "4: Encoding Loss 4.183291435241699, Transition Loss -0.29025357961654663, Classifier Loss 0.18654637038707733, Total Loss 43.7542724609375\n",
      "4: Encoding Loss 4.9404706954956055, Transition Loss -1.0534831285476685, Classifier Loss 0.14664943516254425, Total Loss 44.307350158691406\n",
      "4: Encoding Loss 5.803187847137451, Transition Loss -1.198943853378296, Classifier Loss 0.14264638721942902, Total Loss 49.083290100097656\n",
      "4: Encoding Loss 4.980020523071289, Transition Loss -2.0811591148376465, Classifier Loss 0.14841674268245697, Total Loss 44.72096633911133\n",
      "4: Encoding Loss 4.6140031814575195, Transition Loss 0.5793074369430542, Classifier Loss 0.09944853186607361, Total Loss 37.860595703125\n",
      "4: Encoding Loss 5.464369773864746, Transition Loss -1.421821117401123, Classifier Loss 0.168169304728508, Total Loss 49.60258102416992\n",
      "4: Encoding Loss 5.2535905838012695, Transition Loss -2.0782313346862793, Classifier Loss 0.07234983146190643, Total Loss 38.75569534301758\n",
      "4: Encoding Loss 4.825549602508545, Transition Loss -1.4982800483703613, Classifier Loss 0.1585341989994049, Total Loss 44.80611801147461\n",
      "4: Encoding Loss 4.0904221534729, Transition Loss 0.6149426698684692, Classifier Loss 0.1332327425479889, Total Loss 38.11178207397461\n",
      "4: Encoding Loss 3.027367115020752, Transition Loss -1.1503000259399414, Classifier Loss 0.09586601704359055, Total Loss 27.75034523010254\n",
      "4: Encoding Loss 6.11298131942749, Transition Loss -1.6436843872070312, Classifier Loss 0.11618019640445709, Total Loss 48.29525375366211\n",
      "4: Encoding Loss 5.375035285949707, Transition Loss 0.5708813667297363, Classifier Loss 0.13362354040145874, Total Loss 45.840919494628906\n",
      "4: Encoding Loss 3.797649383544922, Transition Loss -1.5355219841003418, Classifier Loss 0.09734338521957397, Total Loss 32.51961898803711\n",
      "4: Encoding Loss 2.982863664627075, Transition Loss -1.1841075420379639, Classifier Loss 0.13549557328224182, Total Loss 31.446266174316406\n",
      "4: Encoding Loss 3.517483949661255, Transition Loss 0.029802024364471436, Classifier Loss 0.10727221518754959, Total Loss 31.84404754638672\n",
      "4: Encoding Loss 10.442952156066895, Transition Loss 0.053862541913986206, Classifier Loss 0.2619822919368744, Total Loss 88.87748718261719\n",
      "4: Encoding Loss 8.562026023864746, Transition Loss -1.1435354948043823, Classifier Loss 0.13519012928009033, Total Loss 64.89071655273438\n",
      "4: Encoding Loss 8.649770736694336, Transition Loss -0.900250256061554, Classifier Loss 0.17792987823486328, Total Loss 69.6912612915039\n",
      "4: Encoding Loss 6.507126808166504, Transition Loss -1.3837376832962036, Classifier Loss 0.09177686274051666, Total Loss 48.21989822387695\n",
      "4: Encoding Loss 5.44916296005249, Transition Loss -1.628882646560669, Classifier Loss 0.08748113363981247, Total Loss 41.442440032958984\n",
      "4: Encoding Loss 4.860091209411621, Transition Loss -0.43835118412971497, Classifier Loss 0.16632798314094543, Total Loss 45.793174743652344\n",
      "4: Encoding Loss 4.475499153137207, Transition Loss -1.685299038887024, Classifier Loss 0.13396215438842773, Total Loss 40.24853515625\n",
      "4: Encoding Loss 3.104663133621216, Transition Loss -0.2557010352611542, Classifier Loss 0.15758362412452698, Total Loss 34.38623809814453\n",
      "4: Encoding Loss 2.910737991333008, Transition Loss 0.45366179943084717, Classifier Loss 0.08171327412128448, Total Loss 25.81722068786621\n",
      "4: Encoding Loss 6.501645565032959, Transition Loss -0.18085023760795593, Classifier Loss 0.10794717073440552, Total Loss 49.80451965332031\n",
      "4: Encoding Loss 5.169647693634033, Transition Loss -2.0839602947235107, Classifier Loss 0.08191915601491928, Total Loss 39.20896530151367\n",
      "4: Encoding Loss 8.902093887329102, Transition Loss -1.8870618343353271, Classifier Loss 0.1415812075138092, Total Loss 67.56993103027344\n",
      "4: Encoding Loss 7.426157474517822, Transition Loss -0.9133685827255249, Classifier Loss 0.1676725298166275, Total Loss 61.32383346557617\n",
      "4: Encoding Loss 4.788945198059082, Transition Loss -1.4972822666168213, Classifier Loss 0.11260156333446503, Total Loss 39.993228912353516\n",
      "4: Encoding Loss 6.793837070465088, Transition Loss -0.4292007088661194, Classifier Loss 0.12799802422523499, Total Loss 53.56265640258789\n",
      "4: Encoding Loss 6.977232456207275, Transition Loss -0.950360119342804, Classifier Loss 0.09901949763298035, Total Loss 51.76496505737305\n",
      "4: Encoding Loss 6.403847694396973, Transition Loss -1.1751773357391357, Classifier Loss 0.0783686414361, Total Loss 46.259483337402344\n",
      "4: Encoding Loss 6.996861457824707, Transition Loss -1.3002170324325562, Classifier Loss 0.10571080446243286, Total Loss 52.55173110961914\n",
      "4: Encoding Loss 4.266480922698975, Transition Loss -2.2403080463409424, Classifier Loss 0.1201048269867897, Total Loss 37.60847091674805\n",
      "4: Encoding Loss 4.555674076080322, Transition Loss -0.24231821298599243, Classifier Loss 0.12324479967355728, Total Loss 39.65842819213867\n",
      "4: Encoding Loss 3.4894866943359375, Transition Loss -0.9667876958847046, Classifier Loss 0.09213308990001678, Total Loss 30.149843215942383\n",
      "4: Encoding Loss 4.970742225646973, Transition Loss -1.1253814697265625, Classifier Loss 0.11783067137002945, Total Loss 41.60707092285156\n",
      "4: Encoding Loss 5.894597053527832, Transition Loss -0.2727944850921631, Classifier Loss 0.1299363076686859, Total Loss 48.36110305786133\n",
      "4: Encoding Loss 6.863152503967285, Transition Loss -0.7640876770019531, Classifier Loss 0.2269221842288971, Total Loss 63.87083053588867\n",
      "4: Encoding Loss 5.39752721786499, Transition Loss -1.5717135667800903, Classifier Loss 0.16115863621234894, Total Loss 48.50040054321289\n",
      "4: Encoding Loss 6.8848137855529785, Transition Loss -1.3110164403915405, Classifier Loss 0.11589682847261429, Total Loss 52.89804458618164\n",
      "4: Encoding Loss 4.830120086669922, Transition Loss -1.4896600246429443, Classifier Loss 0.10697205364704132, Total Loss 39.67733383178711\n",
      "4: Encoding Loss 2.6747653484344482, Transition Loss -1.2231628894805908, Classifier Loss 0.10095110535621643, Total Loss 26.143213272094727\n",
      "4: Encoding Loss 6.524810791015625, Transition Loss -1.9609971046447754, Classifier Loss 0.10235343128442764, Total Loss 49.3834228515625\n",
      "4: Encoding Loss 6.906825065612793, Transition Loss -1.8785700798034668, Classifier Loss 0.22181770205497742, Total Loss 63.621971130371094\n",
      "4: Encoding Loss 5.269952774047852, Transition Loss -1.9330272674560547, Classifier Loss 0.07925046980381012, Total Loss 39.54399108886719\n",
      "4: Encoding Loss 2.8692381381988525, Transition Loss -0.971643328666687, Classifier Loss 0.08769875764846802, Total Loss 25.98491668701172\n",
      "4: Encoding Loss 2.5308799743652344, Transition Loss -2.7588367462158203, Classifier Loss 0.09202723205089569, Total Loss 24.386899948120117\n",
      "4: Encoding Loss 1.1933252811431885, Transition Loss -1.0735325813293457, Classifier Loss 0.15085488557815552, Total Loss 22.245010375976562\n",
      "4: Encoding Loss 6.086512565612793, Transition Loss -1.0769035816192627, Classifier Loss 0.12700408697128296, Total Loss 49.21905517578125\n",
      "4: Encoding Loss 5.396698951721191, Transition Loss -1.3027759790420532, Classifier Loss 0.1292387992143631, Total Loss 45.30355453491211\n",
      "4: Encoding Loss 4.234958648681641, Transition Loss -1.314582109451294, Classifier Loss 0.08714564889669418, Total Loss 34.12379455566406\n",
      "4: Encoding Loss 6.076235771179199, Transition Loss -1.219968318939209, Classifier Loss 0.13412347435951233, Total Loss 49.86927795410156\n",
      "4: Encoding Loss 6.200454235076904, Transition Loss -1.0157356262207031, Classifier Loss 0.21371397376060486, Total Loss 58.5737190246582\n",
      "4: Encoding Loss 4.88375186920166, Transition Loss 0.0024178028106689453, Classifier Loss 0.09948143362998962, Total Loss 39.251625061035156\n",
      "4: Encoding Loss 5.738651752471924, Transition Loss 0.02577359974384308, Classifier Loss 0.13511665165424347, Total Loss 47.953887939453125\n",
      "4: Encoding Loss 4.118237018585205, Transition Loss -1.5601286888122559, Classifier Loss 0.10108472406864166, Total Loss 34.81726837158203\n",
      "4: Encoding Loss 8.506486892700195, Transition Loss -0.13467372953891754, Classifier Loss 0.08483259379863739, Total Loss 59.522125244140625\n",
      "4: Encoding Loss 7.242130279541016, Transition Loss -1.3469154834747314, Classifier Loss 0.09336645901203156, Total Loss 52.78889083862305\n",
      "4: Encoding Loss 6.325675964355469, Transition Loss -0.4863099157810211, Classifier Loss 0.1421811580657959, Total Loss 52.17197799682617\n",
      "4: Encoding Loss 5.325854301452637, Transition Loss -0.8280855417251587, Classifier Loss 0.08857487142086029, Total Loss 40.81228256225586\n",
      "4: Encoding Loss 5.145319938659668, Transition Loss -0.15822391211986542, Classifier Loss 0.11416260898113251, Total Loss 42.288116455078125\n",
      "4: Encoding Loss 4.07648229598999, Transition Loss -1.1034107208251953, Classifier Loss 0.10688189417123795, Total Loss 35.14664077758789\n",
      "4: Encoding Loss 5.5047197341918945, Transition Loss -0.9752623438835144, Classifier Loss 0.17874711751937866, Total Loss 50.90264129638672\n",
      "4: Encoding Loss 3.4070041179656982, Transition Loss -1.4646365642547607, Classifier Loss 0.08608628064393997, Total Loss 29.05006980895996\n",
      "4: Encoding Loss 6.921900749206543, Transition Loss -1.021278738975525, Classifier Loss 0.307478129863739, Total Loss 72.27880859375\n",
      "4: Encoding Loss 6.589209079742432, Transition Loss -0.6526122689247131, Classifier Loss 0.21298864483833313, Total Loss 60.8338623046875\n",
      "4: Encoding Loss 4.584158897399902, Transition Loss -1.4102195501327515, Classifier Loss 0.16523060202598572, Total Loss 44.02745056152344\n",
      "4: Encoding Loss 5.754617214202881, Transition Loss -1.802534818649292, Classifier Loss 0.10829274356365204, Total Loss 45.356258392333984\n",
      "4: Encoding Loss 6.349471092224121, Transition Loss -0.47241294384002686, Classifier Loss 0.1252537965774536, Total Loss 50.62201690673828\n",
      "4: Encoding Loss 7.086589813232422, Transition Loss -0.7925674915313721, Classifier Loss 0.17314942181110382, Total Loss 59.834163665771484\n",
      "4: Encoding Loss 5.899440765380859, Transition Loss -1.472745418548584, Classifier Loss 0.10506407171487808, Total Loss 45.9024658203125\n",
      "4: Encoding Loss 4.76713228225708, Transition Loss -2.6969470977783203, Classifier Loss 0.1181323379278183, Total Loss 40.414947509765625\n",
      "4: Encoding Loss 7.278512001037598, Transition Loss -2.0950653553009033, Classifier Loss 0.15217754244804382, Total Loss 58.88798904418945\n",
      "4: Encoding Loss 6.1924872398376465, Transition Loss -0.8007744550704956, Classifier Loss 0.10462147742509842, Total Loss 47.61675262451172\n",
      "4: Encoding Loss 3.9393622875213623, Transition Loss -1.973183035850525, Classifier Loss 0.13179676234722137, Total Loss 36.815059661865234\n",
      "4: Encoding Loss 2.6055235862731934, Transition Loss -0.9121577143669128, Classifier Loss 0.10322128981351852, Total Loss 25.95490837097168\n",
      "4: Encoding Loss 8.131219863891602, Transition Loss -1.8798518180847168, Classifier Loss 0.129898801445961, Total Loss 61.77644729614258\n",
      "4: Encoding Loss 5.951156139373779, Transition Loss -0.09272465109825134, Classifier Loss 0.11769159883260727, Total Loss 47.4760627746582\n",
      "4: Encoding Loss 6.200342655181885, Transition Loss -0.6170581579208374, Classifier Loss 0.10269049555063248, Total Loss 47.47085952758789\n",
      "4: Encoding Loss 4.193784713745117, Transition Loss -0.88333660364151, Classifier Loss 0.08866696804761887, Total Loss 34.029052734375\n",
      "4: Encoding Loss 4.274390697479248, Transition Loss -1.4011554718017578, Classifier Loss 0.10888879746198654, Total Loss 36.534664154052734\n",
      "4: Encoding Loss 8.641919136047363, Transition Loss -0.03024280071258545, Classifier Loss 0.18016982078552246, Total Loss 69.86848449707031\n",
      "4: Encoding Loss 6.377928733825684, Transition Loss -1.1653108596801758, Classifier Loss 0.20831601321697235, Total Loss 59.09870910644531\n",
      "4: Encoding Loss 5.823731899261475, Transition Loss -2.519524097442627, Classifier Loss 0.20074257254600525, Total Loss 55.01564407348633\n",
      "4: Encoding Loss 3.9970288276672363, Transition Loss -1.7337262630462646, Classifier Loss 0.0956403836607933, Total Loss 33.54551696777344\n",
      "4: Encoding Loss 8.262025833129883, Transition Loss -1.680251955986023, Classifier Loss 0.1620580106973648, Total Loss 65.77728271484375\n",
      "4: Encoding Loss 7.025588035583496, Transition Loss -0.6684483885765076, Classifier Loss 0.10014670342206955, Total Loss 52.16793441772461\n",
      "4: Encoding Loss 4.989713191986084, Transition Loss -1.2991690635681152, Classifier Loss 0.10079990327358246, Total Loss 40.01775360107422\n",
      "4: Encoding Loss 3.8801448345184326, Transition Loss -1.7583011388778687, Classifier Loss 0.07869616150856018, Total Loss 31.149782180786133\n",
      "4: Encoding Loss 5.54384708404541, Transition Loss -1.2270021438598633, Classifier Loss 0.200658917427063, Total Loss 53.32848358154297\n",
      "4: Encoding Loss 6.652439117431641, Transition Loss -2.5009818077087402, Classifier Loss 0.1487620770931244, Total Loss 54.78984069824219\n",
      "4: Encoding Loss 4.658923149108887, Transition Loss -0.9076074361801147, Classifier Loss 0.12398525327444077, Total Loss 40.35170364379883\n",
      "4: Encoding Loss 6.529502868652344, Transition Loss -0.28365153074264526, Classifier Loss 0.17843343317508698, Total Loss 57.02024841308594\n",
      "4: Encoding Loss 6.8056488037109375, Transition Loss -1.4237992763519287, Classifier Loss 0.1557055562734604, Total Loss 56.40388107299805\n",
      "4: Encoding Loss 5.563235282897949, Transition Loss -2.2270264625549316, Classifier Loss 0.11378733813762665, Total Loss 44.75725555419922\n",
      "4: Encoding Loss 6.555295467376709, Transition Loss -1.088363766670227, Classifier Loss 0.08605778962373734, Total Loss 47.93711853027344\n",
      "4: Encoding Loss 5.583186626434326, Transition Loss 0.06632953882217407, Classifier Loss 0.1226404681801796, Total Loss 45.78969955444336\n",
      "4: Encoding Loss 5.34517765045166, Transition Loss -0.8247259855270386, Classifier Loss 0.1563708782196045, Total Loss 47.707828521728516\n",
      "4: Encoding Loss 6.799836158752441, Transition Loss -0.8049716353416443, Classifier Loss 0.08037560433149338, Total Loss 48.83625793457031\n",
      "4: Encoding Loss 5.363563537597656, Transition Loss -1.8575291633605957, Classifier Loss 0.0810604989528656, Total Loss 40.286685943603516\n",
      "4: Encoding Loss 6.964456081390381, Transition Loss -1.4117310047149658, Classifier Loss 0.15172284841537476, Total Loss 56.958457946777344\n",
      "4: Encoding Loss 5.422830581665039, Transition Loss -0.9980008006095886, Classifier Loss 0.215445414185524, Total Loss 54.08112716674805\n",
      "4: Encoding Loss 5.344006061553955, Transition Loss -2.2862954139709473, Classifier Loss 0.1564861536026001, Total Loss 47.71173858642578\n",
      "4: Encoding Loss 3.605630397796631, Transition Loss 0.09804490208625793, Classifier Loss 0.13209831714630127, Total Loss 34.88283157348633\n",
      "4: Encoding Loss 3.0437934398651123, Transition Loss -0.5857467651367188, Classifier Loss 0.10445373505353928, Total Loss 28.707901000976562\n",
      "4: Encoding Loss 2.8801960945129395, Transition Loss -1.1431541442871094, Classifier Loss 0.09331977367401123, Total Loss 26.61269760131836\n",
      "4: Encoding Loss 7.559075355529785, Transition Loss -1.7232874631881714, Classifier Loss 0.19099494814872742, Total Loss 64.45326232910156\n",
      "4: Encoding Loss 5.722171306610107, Transition Loss -1.3752379417419434, Classifier Loss 0.1389348953962326, Total Loss 48.22597122192383\n",
      "4: Encoding Loss 6.650813102722168, Transition Loss -1.4238810539245605, Classifier Loss 0.13564002513885498, Total Loss 53.46831512451172\n",
      "4: Encoding Loss 4.724194526672363, Transition Loss -1.0190393924713135, Classifier Loss 0.15026968717575073, Total Loss 43.371726989746094\n",
      "4: Encoding Loss 4.54052734375, Transition Loss -1.5541844367980957, Classifier Loss 0.14813655614852905, Total Loss 42.05620193481445\n",
      "4: Encoding Loss 4.599350929260254, Transition Loss -1.5147323608398438, Classifier Loss 0.16133415699005127, Total Loss 43.72891616821289\n",
      "4: Encoding Loss 4.610807418823242, Transition Loss -1.252915859222412, Classifier Loss 0.12897276878356934, Total Loss 40.561622619628906\n",
      "4: Encoding Loss 6.431546211242676, Transition Loss -0.9054434299468994, Classifier Loss 0.15809045732021332, Total Loss 54.3979606628418\n",
      "4: Encoding Loss 4.994110107421875, Transition Loss -0.713737964630127, Classifier Loss 0.24994754791259766, Total Loss 54.95913314819336\n",
      "4: Encoding Loss 7.337101936340332, Transition Loss -1.6141058206558228, Classifier Loss 0.1169360876083374, Total Loss 55.715579986572266\n",
      "4: Encoding Loss 5.095780372619629, Transition Loss -1.776713490486145, Classifier Loss 0.10769795626401901, Total Loss 41.34376907348633\n",
      "4: Encoding Loss 6.2467851638793945, Transition Loss -1.497536301612854, Classifier Loss 0.10446341335773468, Total Loss 47.926456451416016\n",
      "4: Encoding Loss 4.610126972198486, Transition Loss -0.8586528301239014, Classifier Loss 0.11292177438735962, Total Loss 38.952598571777344\n",
      "4: Encoding Loss 4.4664201736450195, Transition Loss -0.9198360443115234, Classifier Loss 0.08485803008079529, Total Loss 35.283958435058594\n",
      "4: Encoding Loss 3.3089728355407715, Transition Loss -0.5858455300331116, Classifier Loss 0.11352831870317459, Total Loss 31.20643424987793\n",
      "4: Encoding Loss 6.194016933441162, Transition Loss -0.8814665079116821, Classifier Loss 0.10272272676229477, Total Loss 47.43602752685547\n",
      "4: Encoding Loss 6.047779560089111, Transition Loss -1.0100276470184326, Classifier Loss 0.18580105900764465, Total Loss 54.86637878417969\n",
      "4: Encoding Loss 3.8598380088806152, Transition Loss -1.4940459728240967, Classifier Loss 0.18703988194465637, Total Loss 41.86241912841797\n",
      "4: Encoding Loss 3.9177277088165283, Transition Loss -0.4131668210029602, Classifier Loss 0.12889422476291656, Total Loss 36.395626068115234\n",
      "4: Encoding Loss 7.7328362464904785, Transition Loss -1.4114524126052856, Classifier Loss 0.20840762555599213, Total Loss 67.23721313476562\n",
      "4: Encoding Loss 6.5663957595825195, Transition Loss -0.554267406463623, Classifier Loss 0.10826124250888824, Total Loss 50.224281311035156\n",
      "4: Encoding Loss 5.547781944274902, Transition Loss -0.9337064623832703, Classifier Loss 0.08902492374181747, Total Loss 42.188812255859375\n",
      "4: Encoding Loss 6.457236289978027, Transition Loss -0.6027333736419678, Classifier Loss 0.14397113025188446, Total Loss 53.14029312133789\n",
      "4: Encoding Loss 5.057268142700195, Transition Loss -2.656069278717041, Classifier Loss 0.1496528834104538, Total Loss 45.30783462524414\n",
      "4: Encoding Loss 3.407527446746826, Transition Loss 0.5149909853935242, Classifier Loss 0.12482863664627075, Total Loss 33.13402557373047\n",
      "4: Encoding Loss 3.4496638774871826, Transition Loss -1.9848365783691406, Classifier Loss 0.10870996117591858, Total Loss 31.568187713623047\n",
      "4: Encoding Loss 8.071380615234375, Transition Loss -2.0426065921783447, Classifier Loss 0.06940805912017822, Total Loss 55.36827850341797\n",
      "4: Encoding Loss 9.220812797546387, Transition Loss -1.726075530052185, Classifier Loss 0.10578157007694244, Total Loss 65.90235137939453\n",
      "4: Encoding Loss 8.335375785827637, Transition Loss -1.3431051969528198, Classifier Loss 0.09125753492116928, Total Loss 59.13747024536133\n",
      "4: Encoding Loss 5.322700023651123, Transition Loss -0.8879107236862183, Classifier Loss 0.1534232795238495, Total Loss 47.278175354003906\n",
      "4: Encoding Loss 5.703120231628418, Transition Loss -0.8597621321678162, Classifier Loss 0.08166977763175964, Total Loss 42.38535690307617\n",
      "4: Encoding Loss 3.440749406814575, Transition Loss -0.7966089248657227, Classifier Loss 0.10773201286792755, Total Loss 31.41737937927246\n",
      "4: Encoding Loss 7.632058143615723, Transition Loss -1.98618483543396, Classifier Loss 0.10961080342531204, Total Loss 56.75263977050781\n",
      "4: Encoding Loss 5.874882698059082, Transition Loss -1.272021770477295, Classifier Loss 0.0941377729177475, Total Loss 44.662567138671875\n",
      "4: Encoding Loss 6.40131139755249, Transition Loss -1.5322757959365845, Classifier Loss 0.14536963403224945, Total Loss 52.94422149658203\n",
      "4: Encoding Loss 4.400975227355957, Transition Loss -1.3691637516021729, Classifier Loss 0.05891638994216919, Total Loss 32.296939849853516\n",
      "4: Encoding Loss 6.003286361694336, Transition Loss -0.46543052792549133, Classifier Loss 0.08630584925413132, Total Loss 44.650115966796875\n",
      "4: Encoding Loss 5.109318733215332, Transition Loss -1.3717846870422363, Classifier Loss 0.1426890790462494, Total Loss 44.92427062988281\n",
      "4: Encoding Loss 4.4670586585998535, Transition Loss -1.2543315887451172, Classifier Loss 0.10242078453302383, Total Loss 37.04393005371094\n",
      "4: Encoding Loss 3.2948031425476074, Transition Loss -1.9448336362838745, Classifier Loss 0.1000983864068985, Total Loss 29.777881622314453\n",
      "4: Encoding Loss 6.080700874328613, Transition Loss -2.803166151046753, Classifier Loss 0.08297690749168396, Total Loss 44.78077697753906\n",
      "4: Encoding Loss 4.6618733406066895, Transition Loss -0.6899082064628601, Classifier Loss 0.17870908975601196, Total Loss 45.84187316894531\n",
      "4: Encoding Loss 4.787509918212891, Transition Loss -0.869745135307312, Classifier Loss 0.19329862296581268, Total Loss 48.0545768737793\n",
      "4: Encoding Loss 5.675826072692871, Transition Loss -0.7249820232391357, Classifier Loss 0.12670713663101196, Total Loss 46.725379943847656\n",
      "4: Encoding Loss 4.052663803100586, Transition Loss -0.451083242893219, Classifier Loss 0.08117232471704483, Total Loss 32.43303680419922\n",
      "4: Encoding Loss 8.431334495544434, Transition Loss -0.23276716470718384, Classifier Loss 0.14669673144817352, Total Loss 65.2575912475586\n",
      "4: Encoding Loss 5.1196699142456055, Transition Loss -0.983546257019043, Classifier Loss 0.11542074382305145, Total Loss 42.259700775146484\n",
      "4: Encoding Loss 7.821587562561035, Transition Loss -0.4361189901828766, Classifier Loss 0.20772738754749298, Total Loss 67.70209503173828\n",
      "4: Encoding Loss 6.182011604309082, Transition Loss -2.2827305793762207, Classifier Loss 0.18311505019664764, Total Loss 55.40266418457031\n",
      "4: Encoding Loss 5.819831371307373, Transition Loss -1.0464318990707397, Classifier Loss 0.13813528418540955, Total Loss 48.73210144042969\n",
      "4: Encoding Loss 5.590770244598389, Transition Loss -1.6031219959259033, Classifier Loss 0.1471293866634369, Total Loss 48.256919860839844\n",
      "4: Encoding Loss 3.2023212909698486, Transition Loss -1.140379786491394, Classifier Loss 0.11402229964733124, Total Loss 30.61570167541504\n",
      "4: Encoding Loss 3.6018662452697754, Transition Loss -0.5500935316085815, Classifier Loss 0.10117096453905106, Total Loss 31.72807502746582\n",
      "4: Encoding Loss 4.518097400665283, Transition Loss -0.7573550939559937, Classifier Loss 0.13252319395542145, Total Loss 40.36060333251953\n",
      "4: Encoding Loss 4.2160844802856445, Transition Loss -2.312851905822754, Classifier Loss 0.09874628484249115, Total Loss 35.17020797729492\n",
      "4: Encoding Loss 5.646205425262451, Transition Loss -1.4036850929260254, Classifier Loss 0.1593286693096161, Total Loss 49.80954360961914\n",
      "4: Encoding Loss 5.225058555603027, Transition Loss -1.415024757385254, Classifier Loss 0.10465867072343826, Total Loss 41.81565475463867\n",
      "4: Encoding Loss 3.6173062324523926, Transition Loss -0.548066258430481, Classifier Loss 0.10880859941244125, Total Loss 32.58448028564453\n",
      "4: Encoding Loss 4.370652198791504, Transition Loss -1.6966829299926758, Classifier Loss 0.08100676536560059, Total Loss 34.323909759521484\n",
      "4: Encoding Loss 6.2160539627075195, Transition Loss -1.0874955654144287, Classifier Loss 0.15859660506248474, Total Loss 53.15555191040039\n",
      "4: Encoding Loss 5.437802314758301, Transition Loss -0.06723350286483765, Classifier Loss 0.07831917703151703, Total Loss 40.45870590209961\n",
      "4: Encoding Loss 7.426915645599365, Transition Loss -0.07678619027137756, Classifier Loss 0.12464594095945358, Total Loss 57.02606201171875\n",
      "4: Encoding Loss 6.2180376052856445, Transition Loss -1.4544823169708252, Classifier Loss 0.3004813492298126, Total Loss 67.35578155517578\n",
      "4: Encoding Loss 6.294808387756348, Transition Loss -1.5274229049682617, Classifier Loss 0.0947241485118866, Total Loss 47.240657806396484\n",
      "4: Encoding Loss 5.279810905456543, Transition Loss -1.346684217453003, Classifier Loss 0.13516269624233246, Total Loss 45.19459915161133\n",
      "4: Encoding Loss 4.176018714904785, Transition Loss -1.095202922821045, Classifier Loss 0.10940075665712357, Total Loss 35.995750427246094\n",
      "4: Encoding Loss 6.064140796661377, Transition Loss -1.810494303703308, Classifier Loss 0.22464238107204437, Total Loss 58.848358154296875\n",
      "4: Encoding Loss 4.62457275390625, Transition Loss -0.9683206081390381, Classifier Loss 0.10642403364181519, Total Loss 38.38945388793945\n",
      "4: Encoding Loss 6.692198753356934, Transition Loss -1.8491239547729492, Classifier Loss 0.14043249189853668, Total Loss 54.195701599121094\n",
      "4: Encoding Loss 5.8483757972717285, Transition Loss -1.2721447944641113, Classifier Loss 0.1511579006910324, Total Loss 50.20553970336914\n",
      "4: Encoding Loss 5.929556846618652, Transition Loss -1.586310625076294, Classifier Loss 0.08791409432888031, Total Loss 44.36811828613281\n",
      "4: Encoding Loss 6.647747039794922, Transition Loss -0.8488368391990662, Classifier Loss 0.16635192930698395, Total Loss 56.52133560180664\n",
      "4: Encoding Loss 2.687645435333252, Transition Loss -1.6689540147781372, Classifier Loss 0.1204957515001297, Total Loss 28.174781799316406\n",
      "4: Encoding Loss 2.8327033519744873, Transition Loss -0.6911267638206482, Classifier Loss 0.08293815702199936, Total Loss 25.28976058959961\n",
      "4: Encoding Loss 6.538957118988037, Transition Loss -0.5133124589920044, Classifier Loss 0.13014574348926544, Total Loss 52.24811553955078\n",
      "4: Encoding Loss 5.510824680328369, Transition Loss 0.22118830680847168, Classifier Loss 0.11887861788272858, Total Loss 45.04128646850586\n",
      "4: Encoding Loss 5.537571907043457, Transition Loss -0.8805582523345947, Classifier Loss 0.18077410757541656, Total Loss 51.302494049072266\n",
      "4: Encoding Loss 5.662359714508057, Transition Loss -1.658180594444275, Classifier Loss 0.0974414125084877, Total Loss 43.71763610839844\n",
      "4: Encoding Loss 3.7268691062927246, Transition Loss -2.123844861984253, Classifier Loss 0.13701193034648895, Total Loss 36.06155776977539\n",
      "4: Encoding Loss 3.0879359245300293, Transition Loss -1.2909777164459229, Classifier Loss 0.10164690762758255, Total Loss 28.691789627075195\n",
      "4: Encoding Loss 8.063881874084473, Transition Loss -1.189070463180542, Classifier Loss 0.17166444659233093, Total Loss 65.54926300048828\n",
      "4: Encoding Loss 4.874892234802246, Transition Loss -0.17359767854213715, Classifier Loss 0.1761617511510849, Total Loss 46.86546325683594\n",
      "4: Encoding Loss 4.776232719421387, Transition Loss -1.4333953857421875, Classifier Loss 0.06998580694198608, Total Loss 35.6554069519043\n",
      "4: Encoding Loss 5.637312889099121, Transition Loss -0.0904979556798935, Classifier Loss 0.13816842436790466, Total Loss 47.64068603515625\n",
      "4: Encoding Loss 3.219761610031128, Transition Loss 0.33276697993278503, Classifier Loss 0.09324143081903458, Total Loss 28.775821685791016\n",
      "4: Encoding Loss 3.7707695960998535, Transition Loss -1.1706621646881104, Classifier Loss 0.12454532086849213, Total Loss 35.07868194580078\n",
      "4: Encoding Loss 6.0178728103637695, Transition Loss -0.9407604336738586, Classifier Loss 0.11939377337694168, Total Loss 48.04623794555664\n",
      "4: Encoding Loss 4.136341571807861, Transition Loss -1.0143778324127197, Classifier Loss 0.1352442502975464, Total Loss 38.342071533203125\n",
      "4: Encoding Loss 6.986410617828369, Transition Loss -0.9392848014831543, Classifier Loss 0.16056908667087555, Total Loss 57.974998474121094\n",
      "4: Encoding Loss 7.057713508605957, Transition Loss -1.3623661994934082, Classifier Loss 0.17621773481369019, Total Loss 59.96751022338867\n",
      "4: Encoding Loss 4.704285144805908, Transition Loss -0.9365172386169434, Classifier Loss 0.0708126500248909, Total Loss 35.306602478027344\n",
      "4: Encoding Loss 4.703178882598877, Transition Loss -0.6044003963470459, Classifier Loss 0.17850306630134583, Total Loss 46.06914138793945\n",
      "4: Encoding Loss 6.416621208190918, Transition Loss -2.765958786010742, Classifier Loss 0.19964085519313812, Total Loss 58.46270751953125\n",
      "4: Encoding Loss 6.767416954040527, Transition Loss -1.390610694885254, Classifier Loss 0.09435859322547913, Total Loss 50.0398063659668\n",
      "4: Encoding Loss 5.292217254638672, Transition Loss -1.1095865964889526, Classifier Loss 0.1134258359670639, Total Loss 43.0954475402832\n",
      "4: Encoding Loss 6.467360496520996, Transition Loss -2.0401782989501953, Classifier Loss 0.15456178784370422, Total Loss 54.25952911376953\n",
      "4: Encoding Loss 5.3308024406433105, Transition Loss -0.498343825340271, Classifier Loss 0.08939745277166367, Total Loss 40.92436218261719\n",
      "4: Encoding Loss 2.8604393005371094, Transition Loss -1.4334200620651245, Classifier Loss 0.11215188354253769, Total Loss 28.377248764038086\n",
      "4: Encoding Loss 6.077939033508301, Transition Loss -1.6326136589050293, Classifier Loss 0.15861746668815613, Total Loss 52.328731536865234\n",
      "4: Encoding Loss 8.679309844970703, Transition Loss -2.917478322982788, Classifier Loss 0.2002793550491333, Total Loss 72.10263061523438\n",
      "4: Encoding Loss 5.353909969329834, Transition Loss -1.3868342638015747, Classifier Loss 0.08529268205165863, Total Loss 40.65217971801758\n",
      "4: Encoding Loss 5.146897315979004, Transition Loss -1.0838974714279175, Classifier Loss 0.09034284204244614, Total Loss 39.91523361206055\n",
      "4: Encoding Loss 5.875386714935303, Transition Loss -0.5214477181434631, Classifier Loss 0.1724257618188858, Total Loss 52.49468994140625\n",
      "4: Encoding Loss 6.2438812255859375, Transition Loss -0.9313706159591675, Classifier Loss 0.2708919644355774, Total Loss 64.55210876464844\n",
      "4: Encoding Loss 6.127345561981201, Transition Loss -1.4378361701965332, Classifier Loss 0.19858761131763458, Total Loss 56.62226104736328\n",
      "4: Encoding Loss 6.306684970855713, Transition Loss -1.7833449840545654, Classifier Loss 0.16039970517158508, Total Loss 53.87936782836914\n",
      "4: Encoding Loss 5.309476375579834, Transition Loss -1.36509108543396, Classifier Loss 0.13573475182056427, Total Loss 45.42979049682617\n",
      "4: Encoding Loss 5.472707271575928, Transition Loss -0.4618438184261322, Classifier Loss 0.166798397898674, Total Loss 49.51590347290039\n",
      "4: Encoding Loss 6.655417442321777, Transition Loss -0.6408172845840454, Classifier Loss 0.1737084984779358, Total Loss 57.3031005859375\n",
      "4: Encoding Loss 5.056735038757324, Transition Loss -0.5801974534988403, Classifier Loss 0.08678136765956879, Total Loss 39.018314361572266\n",
      "4: Encoding Loss 8.25123119354248, Transition Loss -1.9662604331970215, Classifier Loss 0.10137024521827698, Total Loss 59.64362716674805\n",
      "4: Encoding Loss 6.30512809753418, Transition Loss -0.89143967628479, Classifier Loss 0.13780486583709717, Total Loss 51.61090087890625\n",
      "4: Encoding Loss 4.337082862854004, Transition Loss -1.3775384426116943, Classifier Loss 0.11734243482351303, Total Loss 37.75619125366211\n",
      "4: Encoding Loss 5.236838340759277, Transition Loss -0.4960305392742157, Classifier Loss 0.19272561371326447, Total Loss 50.693397521972656\n",
      "4: Encoding Loss 4.996607780456543, Transition Loss -1.244724154472351, Classifier Loss 0.11805836856365204, Total Loss 41.78498458862305\n",
      "4: Encoding Loss 4.89370059967041, Transition Loss -1.4332596063613892, Classifier Loss 0.17307834327220917, Total Loss 46.669464111328125\n",
      "4: Encoding Loss 11.544271469116211, Transition Loss -1.8824489116668701, Classifier Loss 0.1449548453092575, Total Loss 83.76036071777344\n",
      "4: Encoding Loss 8.390168190002441, Transition Loss -0.8757834434509277, Classifier Loss 0.08253379166126251, Total Loss 58.59403991699219\n",
      "4: Encoding Loss 6.523571968078613, Transition Loss 0.4608498215675354, Classifier Loss 0.08701127022504807, Total Loss 48.02690124511719\n",
      "4: Encoding Loss 4.768319129943848, Transition Loss -2.075087070465088, Classifier Loss 0.12809881567955017, Total Loss 41.41896438598633\n",
      "4: Encoding Loss 6.157021522521973, Transition Loss -1.5404460430145264, Classifier Loss 0.13810625672340393, Total Loss 50.752140045166016\n",
      "4: Encoding Loss 4.04755163192749, Transition Loss -0.43096667528152466, Classifier Loss 0.12322866916656494, Total Loss 36.60800552368164\n",
      "4: Encoding Loss 5.226644992828369, Transition Loss -1.2497174739837646, Classifier Loss 0.1989024132490158, Total Loss 51.24961471557617\n",
      "4: Encoding Loss 9.467364311218262, Transition Loss -1.2855796813964844, Classifier Loss 0.11391358077526093, Total Loss 68.19503784179688\n",
      "4: Encoding Loss 5.684608459472656, Transition Loss -1.635049819946289, Classifier Loss 0.12203453481197357, Total Loss 46.31045150756836\n",
      "4: Encoding Loss 5.861366271972656, Transition Loss -1.1333701610565186, Classifier Loss 0.09955812245607376, Total Loss 45.12355422973633\n",
      "4: Encoding Loss 5.9466962814331055, Transition Loss -1.9194226264953613, Classifier Loss 0.0807327926158905, Total Loss 43.75269317626953\n",
      "4: Encoding Loss 5.09952449798584, Transition Loss -1.855109453201294, Classifier Loss 0.08356805145740509, Total Loss 38.953208923339844\n",
      "4: Encoding Loss 4.902705669403076, Transition Loss -1.0345594882965088, Classifier Loss 0.0615224689245224, Total Loss 35.56806945800781\n",
      "4: Encoding Loss 4.236330986022949, Transition Loss -1.9943647384643555, Classifier Loss 0.13870184123516083, Total Loss 39.28737258911133\n",
      "4: Encoding Loss 6.596177577972412, Transition Loss -0.4767919182777405, Classifier Loss 0.12442873418331146, Total Loss 52.019752502441406\n",
      "4: Encoding Loss 5.7633562088012695, Transition Loss -0.8087205290794373, Classifier Loss 0.10128448903560638, Total Loss 44.7082633972168\n",
      "4: Encoding Loss 5.210446357727051, Transition Loss -2.69010853767395, Classifier Loss 0.12710760533809662, Total Loss 43.97236251831055\n",
      "4: Encoding Loss 7.893812656402588, Transition Loss -2.558598041534424, Classifier Loss 0.13910725712776184, Total Loss 61.272579193115234\n",
      "4: Encoding Loss 6.024898529052734, Transition Loss -0.16919732093811035, Classifier Loss 0.0830865204334259, Total Loss 44.45797348022461\n",
      "4: Encoding Loss 4.916040897369385, Transition Loss -1.6504197120666504, Classifier Loss 0.17545241117477417, Total Loss 47.040828704833984\n",
      "4: Encoding Loss 4.971012592315674, Transition Loss -1.285294532775879, Classifier Loss 0.10951059311628342, Total Loss 40.7766227722168\n",
      "4: Encoding Loss 3.6994781494140625, Transition Loss -1.689763069152832, Classifier Loss 0.09678662568330765, Total Loss 31.874855041503906\n",
      "4: Encoding Loss 4.8291120529174805, Transition Loss -2.372016191482544, Classifier Loss 0.15823844075202942, Total Loss 44.797569274902344\n",
      "4: Encoding Loss 6.794804573059082, Transition Loss -1.9205329418182373, Classifier Loss 0.11704473942518234, Total Loss 52.472537994384766\n",
      "4: Encoding Loss 6.579302787780762, Transition Loss -2.246246576309204, Classifier Loss 0.13186894357204437, Total Loss 52.66181182861328\n",
      "4: Encoding Loss 7.325270652770996, Transition Loss -0.960827648639679, Classifier Loss 0.15781648457050323, Total Loss 59.73289108276367\n",
      "4: Encoding Loss 4.826435089111328, Transition Loss 0.25056010484695435, Classifier Loss 0.13470250368118286, Total Loss 42.52908706665039\n",
      "4: Encoding Loss 4.801587104797363, Transition Loss -2.1964149475097656, Classifier Loss 0.11772347241640091, Total Loss 40.58099365234375\n",
      "4: Encoding Loss 5.24735164642334, Transition Loss -1.6830589771270752, Classifier Loss 0.1433144360780716, Total Loss 45.814884185791016\n",
      "4: Encoding Loss 4.622367858886719, Transition Loss -1.5045127868652344, Classifier Loss 0.0980936661362648, Total Loss 37.542972564697266\n",
      "4: Encoding Loss 4.141819953918457, Transition Loss -1.8132909536361694, Classifier Loss 0.09813939779996872, Total Loss 34.66413497924805\n",
      "4: Encoding Loss 3.6817314624786377, Transition Loss -2.070356845855713, Classifier Loss 0.08587269484996796, Total Loss 30.676830291748047\n",
      "4: Encoding Loss 3.858569383621216, Transition Loss -0.5528692603111267, Classifier Loss 0.14063677191734314, Total Loss 37.214874267578125\n",
      "4: Encoding Loss 4.580563545227051, Transition Loss -0.8764177560806274, Classifier Loss 0.14522072672843933, Total Loss 42.005104064941406\n",
      "4: Encoding Loss 6.408052921295166, Transition Loss -0.40719518065452576, Classifier Loss 0.125584676861763, Total Loss 51.006622314453125\n",
      "4: Encoding Loss 4.749754428863525, Transition Loss -0.8040446639060974, Classifier Loss 0.12083229422569275, Total Loss 40.58143615722656\n",
      "4: Encoding Loss 7.751850128173828, Transition Loss -1.168276071548462, Classifier Loss 0.13468711078166962, Total Loss 59.97934341430664\n",
      "4: Encoding Loss 6.982257843017578, Transition Loss -0.8110759258270264, Classifier Loss 0.1249179020524025, Total Loss 54.385013580322266\n",
      "4: Encoding Loss 5.1916913986206055, Transition Loss -1.544368028640747, Classifier Loss 0.18578235805034637, Total Loss 49.72776794433594\n",
      "4: Encoding Loss 5.1723833084106445, Transition Loss -0.6487013697624207, Classifier Loss 0.12537388503551483, Total Loss 43.57143020629883\n",
      "4: Encoding Loss 7.792169094085693, Transition Loss -2.0113186836242676, Classifier Loss 0.18517592549324036, Total Loss 65.26981353759766\n",
      "4: Encoding Loss 5.441215991973877, Transition Loss -1.4841408729553223, Classifier Loss 0.08572765439748764, Total Loss 41.21946716308594\n",
      "4: Encoding Loss 8.129476547241211, Transition Loss -0.8365408182144165, Classifier Loss 0.20834967494010925, Total Loss 69.61149597167969\n",
      "4: Encoding Loss 6.251672744750977, Transition Loss -0.6309557557106018, Classifier Loss 0.1667667031288147, Total Loss 54.18645477294922\n",
      "4: Encoding Loss 4.859755992889404, Transition Loss -1.5772030353546143, Classifier Loss 0.14519177377223969, Total Loss 43.677085876464844\n",
      "4: Encoding Loss 4.091835021972656, Transition Loss -2.469937562942505, Classifier Loss 0.17429549992084503, Total Loss 41.97957229614258\n",
      "4: Encoding Loss 6.776904106140137, Transition Loss -1.7194019556045532, Classifier Loss 0.16248035430908203, Total Loss 56.908775329589844\n",
      "4: Encoding Loss 4.7080278396606445, Transition Loss -0.8046112060546875, Classifier Loss 0.12062104046344757, Total Loss 40.30995178222656\n",
      "4: Encoding Loss 2.8079705238342285, Transition Loss -1.098493218421936, Classifier Loss 0.1009553074836731, Total Loss 26.942916870117188\n",
      "4: Encoding Loss 3.024940013885498, Transition Loss -1.5375477075576782, Classifier Loss 0.10804054886102676, Total Loss 28.953083038330078\n",
      "4: Encoding Loss 3.8136470317840576, Transition Loss -1.2974778413772583, Classifier Loss 0.11671368032693863, Total Loss 34.552734375\n",
      "4: Encoding Loss 9.190470695495605, Transition Loss 0.3146982192993164, Classifier Loss 0.13030460476875305, Total Loss 68.29916381835938\n",
      "4: Encoding Loss 6.353625297546387, Transition Loss 0.05138060450553894, Classifier Loss 0.16115136444568634, Total Loss 54.2574462890625\n",
      "4: Encoding Loss 4.570980072021484, Transition Loss 0.3096408247947693, Classifier Loss 0.09349598735570908, Total Loss 36.89933776855469\n",
      "4: Encoding Loss 7.911840915679932, Transition Loss -0.5386471748352051, Classifier Loss 0.26187464594841003, Total Loss 73.65829467773438\n",
      "4: Encoding Loss 5.237274646759033, Transition Loss -1.3881789445877075, Classifier Loss 0.08394778519868851, Total Loss 39.81787109375\n",
      "4: Encoding Loss 5.102462291717529, Transition Loss -2.0127556324005127, Classifier Loss 0.1431756466627121, Total Loss 44.93153381347656\n",
      "4: Encoding Loss 5.918211936950684, Transition Loss -1.1327327489852905, Classifier Loss 0.15877476334571838, Total Loss 51.386295318603516\n",
      "4: Encoding Loss 5.202543258666992, Transition Loss -1.3479132652282715, Classifier Loss 0.0732484832406044, Total Loss 38.53956985473633\n",
      "4: Encoding Loss 4.6417717933654785, Transition Loss -0.6201462745666504, Classifier Loss 0.0902576595544815, Total Loss 36.87614822387695\n",
      "4: Encoding Loss 6.166629314422607, Transition Loss -0.7716528177261353, Classifier Loss 0.14464330673217773, Total Loss 51.463802337646484\n",
      "4: Encoding Loss 4.161776542663574, Transition Loss -0.18657031655311584, Classifier Loss 0.07698000967502594, Total Loss 32.66858673095703\n",
      "4: Encoding Loss 7.254932403564453, Transition Loss 0.26090091466903687, Classifier Loss 0.09843890368938446, Total Loss 53.47784423828125\n",
      "4: Encoding Loss 5.952374458312988, Transition Loss -1.7744617462158203, Classifier Loss 0.1316825896501541, Total Loss 48.881797790527344\n",
      "4: Encoding Loss 4.565754413604736, Transition Loss -1.5082356929779053, Classifier Loss 0.1587558090686798, Total Loss 43.26950454711914\n",
      "4: Encoding Loss 6.368076801300049, Transition Loss -1.699185848236084, Classifier Loss 0.0977921336889267, Total Loss 47.986995697021484\n",
      "4: Encoding Loss 3.916079044342041, Transition Loss -1.119030475616455, Classifier Loss 0.1398514360189438, Total Loss 37.48117446899414\n",
      "4: Encoding Loss 2.254455089569092, Transition Loss -1.4743413925170898, Classifier Loss 0.12384991347789764, Total Loss 25.911134719848633\n",
      "4: Encoding Loss 2.8607189655303955, Transition Loss -1.13905930519104, Classifier Loss 0.08094634860754013, Total Loss 25.258493423461914\n",
      "4: Encoding Loss 8.173059463500977, Transition Loss -1.9600937366485596, Classifier Loss 0.13851827383041382, Total Loss 62.88939666748047\n",
      "4: Encoding Loss 5.452253818511963, Transition Loss -1.5772770643234253, Classifier Loss 0.08227786421775818, Total Loss 40.94068145751953\n",
      "4: Encoding Loss 2.5384092330932617, Transition Loss -1.8002190589904785, Classifier Loss 0.09126366674900055, Total Loss 24.356101989746094\n",
      "4: Encoding Loss 6.644346714019775, Transition Loss -1.2706708908081055, Classifier Loss 0.12447352707386017, Total Loss 52.31292724609375\n",
      "4: Encoding Loss 4.24902868270874, Transition Loss -0.9031902551651001, Classifier Loss 0.10320848226547241, Total Loss 35.814659118652344\n",
      "4: Encoding Loss 5.670626163482666, Transition Loss -0.8431427478790283, Classifier Loss 0.13101984560489655, Total Loss 47.12540817260742\n",
      "4: Encoding Loss 5.549058437347412, Transition Loss -0.6814678907394409, Classifier Loss 0.09705092012882233, Total Loss 42.999176025390625\n",
      "4: Encoding Loss 5.53134822845459, Transition Loss -2.3917665481567383, Classifier Loss 0.16103580594062805, Total Loss 49.290714263916016\n",
      "4: Encoding Loss 5.431697845458984, Transition Loss -0.6106711030006409, Classifier Loss 0.13789048790931702, Total Loss 46.378990173339844\n",
      "4: Encoding Loss 5.105587959289551, Transition Loss -1.3325493335723877, Classifier Loss 0.20431074500083923, Total Loss 51.06406784057617\n",
      "4: Encoding Loss 4.639410972595215, Transition Loss -2.298264980316162, Classifier Loss 0.10559496283531189, Total Loss 38.39504623413086\n",
      "4: Encoding Loss 3.586487293243408, Transition Loss -1.1476755142211914, Classifier Loss 0.06908009946346283, Total Loss 28.426475524902344\n",
      "4: Encoding Loss 7.966086387634277, Transition Loss -0.9775230288505554, Classifier Loss 0.1919301599264145, Total Loss 66.98914337158203\n",
      "4: Encoding Loss 7.44403600692749, Transition Loss -0.9267374277114868, Classifier Loss 0.18056319653987885, Total Loss 62.72016906738281\n",
      "4: Encoding Loss 5.268853187561035, Transition Loss -1.1191813945770264, Classifier Loss 0.18205410242080688, Total Loss 49.81808090209961\n",
      "4: Encoding Loss 3.57018780708313, Transition Loss -1.5580565929412842, Classifier Loss 0.09769096225500107, Total Loss 31.189599990844727\n",
      "4: Encoding Loss 11.841337203979492, Transition Loss -1.0957374572753906, Classifier Loss 0.1805039346218109, Total Loss 89.09798431396484\n",
      "4: Encoding Loss 9.67492961883545, Transition Loss -2.36427640914917, Classifier Loss 0.10447791963815689, Total Loss 68.49642944335938\n",
      "4: Encoding Loss 5.88118839263916, Transition Loss -1.786208152770996, Classifier Loss 0.16405043005943298, Total Loss 51.691463470458984\n",
      "4: Encoding Loss 6.480111598968506, Transition Loss -1.8759009838104248, Classifier Loss 0.16820460557937622, Total Loss 55.700382232666016\n",
      "4: Encoding Loss 7.224353790283203, Transition Loss -1.4389408826828003, Classifier Loss 0.1465224027633667, Total Loss 57.99778747558594\n",
      "4: Encoding Loss 5.668656349182129, Transition Loss -0.7426512837409973, Classifier Loss 0.09591062366962433, Total Loss 43.60270690917969\n",
      "4: Encoding Loss 7.555790901184082, Transition Loss -2.2351136207580566, Classifier Loss 0.10742837935686111, Total Loss 56.076690673828125\n",
      "4: Encoding Loss 7.232093811035156, Transition Loss -1.342644453048706, Classifier Loss 0.21795877814292908, Total Loss 65.18790435791016\n",
      "4: Encoding Loss 7.384066581726074, Transition Loss -1.9584300518035889, Classifier Loss 0.09112229943275452, Total Loss 53.41585159301758\n",
      "4: Encoding Loss 6.707108974456787, Transition Loss -0.47761666774749756, Classifier Loss 0.21387037634849548, Total Loss 61.62950134277344\n",
      "4: Encoding Loss 6.054730415344238, Transition Loss -1.6861555576324463, Classifier Loss 0.08676806837320328, Total Loss 45.0045166015625\n",
      "4: Encoding Loss 5.189782619476318, Transition Loss -1.2617348432540894, Classifier Loss 0.09106706827878952, Total Loss 40.24489974975586\n",
      "4: Encoding Loss 4.193186283111572, Transition Loss -2.1605424880981445, Classifier Loss 0.08850842714309692, Total Loss 34.00909423828125\n",
      "4: Encoding Loss 4.314202308654785, Transition Loss -0.985267162322998, Classifier Loss 0.12989161908626556, Total Loss 38.87398147583008\n",
      "4: Encoding Loss 3.8230013847351074, Transition Loss -2.563786029815674, Classifier Loss 0.11172597110271454, Total Loss 34.109580993652344\n",
      "4: Encoding Loss 8.315414428710938, Transition Loss -0.9050655961036682, Classifier Loss 0.1223011240363121, Total Loss 62.12224197387695\n",
      "4: Encoding Loss 5.366429328918457, Transition Loss -1.607995867729187, Classifier Loss 0.12950453162193298, Total Loss 45.14838790893555\n",
      "4: Encoding Loss 3.0424880981445312, Transition Loss -1.0630629062652588, Classifier Loss 0.07948858290910721, Total Loss 26.20336151123047\n",
      "4: Encoding Loss 7.5679216384887695, Transition Loss -2.0132265090942383, Classifier Loss 0.09613803029060364, Total Loss 55.020530700683594\n",
      "4: Encoding Loss 8.346409797668457, Transition Loss -1.3515456914901733, Classifier Loss 0.10134757310152054, Total Loss 60.212677001953125\n",
      "4: Encoding Loss 6.456136226654053, Transition Loss -0.5766793489456177, Classifier Loss 0.15114715695381165, Total Loss 53.8513069152832\n",
      "4: Encoding Loss 6.058278560638428, Transition Loss -1.526726245880127, Classifier Loss 0.06228949874639511, Total Loss 42.5780143737793\n",
      "4: Encoding Loss 5.020831108093262, Transition Loss -0.9904382228851318, Classifier Loss 0.13005493581295013, Total Loss 43.13008499145508\n",
      "4: Encoding Loss 6.248777866363525, Transition Loss -1.5297393798828125, Classifier Loss 0.1078246608376503, Total Loss 48.2745246887207\n",
      "4: Encoding Loss 6.381673812866211, Transition Loss -1.0874860286712646, Classifier Loss 0.11841665208339691, Total Loss 50.13127136230469\n",
      "4: Encoding Loss 4.368897438049316, Transition Loss -0.9371696710586548, Classifier Loss 0.11507288366556168, Total Loss 37.720298767089844\n",
      "4: Encoding Loss 6.017331600189209, Transition Loss -1.7894515991210938, Classifier Loss 0.1834261268377304, Total Loss 54.44588851928711\n",
      "4: Encoding Loss 4.094865322113037, Transition Loss -1.0864697694778442, Classifier Loss 0.10907910764217377, Total Loss 35.47666931152344\n",
      "4: Encoding Loss 5.00607442855835, Transition Loss -0.45572543144226074, Classifier Loss 0.0671374574303627, Total Loss 36.7500114440918\n",
      "4: Encoding Loss 6.9602813720703125, Transition Loss -1.3039369583129883, Classifier Loss 0.19101007282733917, Total Loss 60.8621711730957\n",
      "4: Encoding Loss 5.3327531814575195, Transition Loss -0.9252300262451172, Classifier Loss 0.09375631809234619, Total Loss 41.37178039550781\n",
      "4: Encoding Loss 5.560591697692871, Transition Loss -3.149038314819336, Classifier Loss 0.09491360187530518, Total Loss 42.85365295410156\n",
      "4: Encoding Loss 4.265288352966309, Transition Loss -1.6271406412124634, Classifier Loss 0.08190055936574936, Total Loss 33.78113555908203\n",
      "4: Encoding Loss 5.416388511657715, Transition Loss -0.26851460337638855, Classifier Loss 0.17087620496749878, Total Loss 49.585845947265625\n",
      "4: Encoding Loss 3.600659132003784, Transition Loss -1.3038969039916992, Classifier Loss 0.12774299085140228, Total Loss 34.37773132324219\n",
      "4: Encoding Loss 8.807717323303223, Transition Loss -1.7708938121795654, Classifier Loss 0.18927821516990662, Total Loss 71.7734146118164\n",
      "4: Encoding Loss 6.951908111572266, Transition Loss -0.8083702921867371, Classifier Loss 0.0998033955693245, Total Loss 51.691463470458984\n",
      "4: Encoding Loss 5.526095390319824, Transition Loss -2.330116033554077, Classifier Loss 0.1262524127960205, Total Loss 45.7808837890625\n",
      "4: Encoding Loss 4.794624328613281, Transition Loss -2.428110361099243, Classifier Loss 0.15302295982837677, Total Loss 44.06907272338867\n",
      "4: Encoding Loss 4.806760787963867, Transition Loss -1.5719857215881348, Classifier Loss 0.13601623475551605, Total Loss 42.44156265258789\n",
      "4: Encoding Loss 5.828562259674072, Transition Loss -1.2415413856506348, Classifier Loss 0.11663764715194702, Total Loss 46.6346435546875\n",
      "4: Encoding Loss 5.211061954498291, Transition Loss -1.7383921146392822, Classifier Loss 0.19281519949436188, Total Loss 50.54719924926758\n",
      "4: Encoding Loss 6.085219383239746, Transition Loss -1.2576525211334229, Classifier Loss 0.06794692575931549, Total Loss 43.30550765991211\n",
      "4: Encoding Loss 4.900901794433594, Transition Loss 0.1319860816001892, Classifier Loss 0.08726751059293747, Total Loss 38.184959411621094\n",
      "4: Encoding Loss 5.590450286865234, Transition Loss -1.7783217430114746, Classifier Loss 0.09503333270549774, Total Loss 43.045326232910156\n",
      "4: Encoding Loss 4.258257865905762, Transition Loss -1.4520386457443237, Classifier Loss 0.10024301707744598, Total Loss 35.573272705078125\n",
      "4: Encoding Loss 7.118642330169678, Transition Loss -0.5128065347671509, Classifier Loss 0.14996425807476044, Total Loss 57.70807647705078\n",
      "4: Encoding Loss 5.735464572906494, Transition Loss -2.2543811798095703, Classifier Loss 0.07082603871822357, Total Loss 41.49449157714844\n",
      "4: Encoding Loss 7.7166008949279785, Transition Loss -1.5499296188354492, Classifier Loss 0.23483408987522125, Total Loss 69.78240203857422\n",
      "4: Encoding Loss 6.1665754318237305, Transition Loss -1.025413155555725, Classifier Loss 0.13496063649654388, Total Loss 50.4951057434082\n",
      "4: Encoding Loss 5.329974174499512, Transition Loss -1.6768426895141602, Classifier Loss 0.09267206490039825, Total Loss 41.24638366699219\n",
      "4: Encoding Loss 5.473752021789551, Transition Loss -0.18149206042289734, Classifier Loss 0.10801798105239868, Total Loss 43.64423751831055\n",
      "4: Encoding Loss 5.4062018394470215, Transition Loss -1.1560720205307007, Classifier Loss 0.0779554471373558, Total Loss 40.232295989990234\n",
      "4: Encoding Loss 5.903733253479004, Transition Loss -0.47383323311805725, Classifier Loss 0.1398583948612213, Total Loss 49.408050537109375\n",
      "4: Encoding Loss 7.191778659820557, Transition Loss -0.4461459219455719, Classifier Loss 0.08130552619695663, Total Loss 51.28104782104492\n",
      "4: Encoding Loss 4.633685111999512, Transition Loss -1.4837138652801514, Classifier Loss 0.1878778487443924, Total Loss 46.58930206298828\n",
      "4: Encoding Loss 5.624728202819824, Transition Loss -0.5072617530822754, Classifier Loss 0.07852625101804733, Total Loss 41.60079574584961\n",
      "4: Encoding Loss 4.618201732635498, Transition Loss -0.5249862670898438, Classifier Loss 0.11910500377416611, Total Loss 39.619503021240234\n",
      "4: Encoding Loss 4.773181915283203, Transition Loss -1.5926098823547363, Classifier Loss 0.07131917029619217, Total Loss 35.7703742980957\n",
      "4: Encoding Loss 5.147408962249756, Transition Loss -1.8272911310195923, Classifier Loss 0.18898668885231018, Total Loss 49.78239059448242\n",
      "4: Encoding Loss 4.9522929191589355, Transition Loss -1.840844750404358, Classifier Loss 0.14999186992645264, Total Loss 44.71220779418945\n",
      "4: Encoding Loss 3.7415690422058105, Transition Loss -2.495513916015625, Classifier Loss 0.11019402742385864, Total Loss 33.46781921386719\n",
      "4: Encoding Loss 3.8687584400177, Transition Loss -0.7646103501319885, Classifier Loss 0.14424052834510803, Total Loss 37.63629913330078\n",
      "4: Encoding Loss 6.173663139343262, Transition Loss -0.6504993438720703, Classifier Loss 0.13273808360099792, Total Loss 50.315528869628906\n",
      "4: Encoding Loss 4.267604827880859, Transition Loss -2.0425729751586914, Classifier Loss 0.08869302272796631, Total Loss 34.474117279052734\n",
      "4: Encoding Loss 5.22784948348999, Transition Loss -0.04924771189689636, Classifier Loss 0.07072856277227402, Total Loss 38.43993377685547\n",
      "4: Encoding Loss 5.742926597595215, Transition Loss -2.0611932277679443, Classifier Loss 0.10534942895174026, Total Loss 44.99168014526367\n",
      "4: Encoding Loss 4.494106769561768, Transition Loss -0.8981883525848389, Classifier Loss 0.08236267417669296, Total Loss 35.2005500793457\n",
      "4: Encoding Loss 8.537862777709961, Transition Loss 0.1551060676574707, Classifier Loss 0.24398081004619598, Total Loss 75.68730163574219\n",
      "4: Encoding Loss 6.52908182144165, Transition Loss -1.494282841682434, Classifier Loss 0.08964186906814575, Total Loss 48.13808059692383\n",
      "4: Encoding Loss 7.355949401855469, Transition Loss 0.18763160705566406, Classifier Loss 0.10232681035995483, Total Loss 54.44343185424805\n",
      "4: Encoding Loss 4.915510654449463, Transition Loss -1.5713804960250854, Classifier Loss 0.1473086178302765, Total Loss 44.223297119140625\n",
      "4: Encoding Loss 3.1700046062469482, Transition Loss -1.0344606637954712, Classifier Loss 0.1303110271692276, Total Loss 32.05072021484375\n",
      "4: Encoding Loss 6.761260986328125, Transition Loss -1.5440473556518555, Classifier Loss 0.13755133748054504, Total Loss 54.32208251953125\n",
      "4: Encoding Loss 9.3247652053833, Transition Loss -2.5750555992126465, Classifier Loss 0.2112753838300705, Total Loss 77.07510375976562\n",
      "4: Encoding Loss 7.7338738441467285, Transition Loss -1.3484783172607422, Classifier Loss 0.2899065613746643, Total Loss 75.39335632324219\n",
      "4: Encoding Loss 4.132246494293213, Transition Loss -1.232598066329956, Classifier Loss 0.13749170303344727, Total Loss 38.54215621948242\n",
      "4: Encoding Loss 8.250335693359375, Transition Loss 0.23994019627571106, Classifier Loss 0.12510836124420166, Total Loss 62.10883331298828\n",
      "4: Encoding Loss 8.633130073547363, Transition Loss -1.27854323387146, Classifier Loss 0.32306361198425293, Total Loss 84.1046371459961\n",
      "4: Encoding Loss 6.866008758544922, Transition Loss -0.7758268117904663, Classifier Loss 0.1458626091480255, Total Loss 55.782005310058594\n",
      "4: Encoding Loss 7.4199419021606445, Transition Loss -0.31325671076774597, Classifier Loss 0.14740529656410217, Total Loss 59.26005935668945\n",
      "4: Encoding Loss 7.300515651702881, Transition Loss -0.5461483001708984, Classifier Loss 0.13217520713806152, Total Loss 57.02040100097656\n",
      "4: Encoding Loss 5.424363613128662, Transition Loss -1.7650749683380127, Classifier Loss 0.11466799676418304, Total Loss 44.01227951049805\n",
      "4: Encoding Loss 4.733602046966553, Transition Loss -1.6884257793426514, Classifier Loss 0.13235843181610107, Total Loss 41.636783599853516\n",
      "4: Encoding Loss 6.3388447761535645, Transition Loss -2.951482057571411, Classifier Loss 0.14695148169994354, Total Loss 52.7270393371582\n",
      "4: Encoding Loss 5.022215843200684, Transition Loss -2.046713352203369, Classifier Loss 0.11232561618089676, Total Loss 41.36503982543945\n",
      "4: Encoding Loss 4.501638412475586, Transition Loss -1.1437110900878906, Classifier Loss 0.08781906962394714, Total Loss 35.791282653808594\n",
      "4: Encoding Loss 5.064901828765869, Transition Loss -1.636042594909668, Classifier Loss 0.16949747502803802, Total Loss 47.3385009765625\n",
      "4: Encoding Loss 3.3139114379882812, Transition Loss -1.6205312013626099, Classifier Loss 0.11367281526327133, Total Loss 31.250102996826172\n",
      "4: Encoding Loss 4.865319728851318, Transition Loss -1.4803038835525513, Classifier Loss 0.13281786441802979, Total Loss 42.473114013671875\n",
      "4: Encoding Loss 6.137153625488281, Transition Loss -1.4225212335586548, Classifier Loss 0.22069083154201508, Total Loss 58.89143753051758\n",
      "4: Encoding Loss 5.348440170288086, Transition Loss -0.4410538971424103, Classifier Loss 0.16788820922374725, Total Loss 48.87928771972656\n",
      "4: Encoding Loss 4.316896915435791, Transition Loss -1.650526523590088, Classifier Loss 0.15324604511260986, Total Loss 41.22532653808594\n",
      "4: Encoding Loss 5.901968955993652, Transition Loss -1.911460280418396, Classifier Loss 0.11041413992643356, Total Loss 46.45246887207031\n",
      "4: Encoding Loss 4.406201362609863, Transition Loss -0.601762056350708, Classifier Loss 0.08618073910474777, Total Loss 35.0550422668457\n",
      "4: Encoding Loss 5.701244354248047, Transition Loss -1.3651092052459717, Classifier Loss 0.0686560794711113, Total Loss 41.07252883911133\n",
      "4: Encoding Loss 4.650967597961426, Transition Loss -1.420672059059143, Classifier Loss 0.07296016067266464, Total Loss 35.20125198364258\n",
      "4: Encoding Loss 3.8692843914031982, Transition Loss -0.7236670255661011, Classifier Loss 0.15965455770492554, Total Loss 39.18087387084961\n",
      "4: Encoding Loss 6.405428886413574, Transition Loss -0.757229745388031, Classifier Loss 0.15903253853321075, Total Loss 54.33552932739258\n",
      "4: Encoding Loss 6.770787239074707, Transition Loss -0.3745664954185486, Classifier Loss 0.14419183135032654, Total Loss 55.043758392333984\n",
      "4: Encoding Loss 6.680279731750488, Transition Loss -0.8467903137207031, Classifier Loss 0.12872274219989777, Total Loss 52.953617095947266\n",
      "4: Encoding Loss 3.127641439437866, Transition Loss -2.038835287094116, Classifier Loss 0.1248711347579956, Total Loss 31.252147674560547\n",
      "4: Encoding Loss 7.933380126953125, Transition Loss -1.527707576751709, Classifier Loss 0.16794940829277039, Total Loss 64.39460754394531\n",
      "4: Encoding Loss 5.832756042480469, Transition Loss -2.2541089057922363, Classifier Loss 0.1220492571592331, Total Loss 47.2005615234375\n",
      "4: Encoding Loss 5.888146877288818, Transition Loss -1.0148762464523315, Classifier Loss 0.11626677215099335, Total Loss 46.95515823364258\n",
      "4: Encoding Loss 6.884852409362793, Transition Loss -1.9658763408660889, Classifier Loss 0.19681355357170105, Total Loss 60.98968505859375\n",
      "4: Encoding Loss 5.643094539642334, Transition Loss -1.9021387100219727, Classifier Loss 0.17042046785354614, Total Loss 50.89986038208008\n",
      "4: Encoding Loss 7.764286994934082, Transition Loss -0.3585699200630188, Classifier Loss 0.14892840385437012, Total Loss 61.47842025756836\n",
      "4: Encoding Loss 5.393022537231445, Transition Loss 0.04399225115776062, Classifier Loss 0.12852704524993896, Total Loss 45.22843551635742\n",
      "4: Encoding Loss 5.141265869140625, Transition Loss -1.8840219974517822, Classifier Loss 0.10825610160827637, Total Loss 41.67245101928711\n",
      "4: Encoding Loss 6.896930694580078, Transition Loss -0.8012958765029907, Classifier Loss 0.15884509682655334, Total Loss 57.26577377319336\n",
      "4: Encoding Loss 6.893774509429932, Transition Loss -1.2236981391906738, Classifier Loss 0.12775245308876038, Total Loss 54.13740539550781\n",
      "4: Encoding Loss 5.966530799865723, Transition Loss -0.8841868042945862, Classifier Loss 0.12513387203216553, Total Loss 48.31222152709961\n",
      "4: Encoding Loss 4.061250686645508, Transition Loss 0.03978380560874939, Classifier Loss 0.13181909918785095, Total Loss 37.565330505371094\n",
      "4: Encoding Loss 3.4009480476379395, Transition Loss -0.9019710421562195, Classifier Loss 0.09774411469697952, Total Loss 30.179738998413086\n",
      "4: Encoding Loss 5.650177001953125, Transition Loss -1.7593998908996582, Classifier Loss 0.09729903936386108, Total Loss 43.63026428222656\n",
      "4: Encoding Loss 4.7599592208862305, Transition Loss -1.3114380836486816, Classifier Loss 0.1073913425207138, Total Loss 39.298362731933594\n",
      "4: Encoding Loss 4.879451274871826, Transition Loss -0.8168350458145142, Classifier Loss 0.2155522108078003, Total Loss 50.83160400390625\n",
      "4: Encoding Loss 4.709327220916748, Transition Loss -2.1058554649353027, Classifier Loss 0.06076846271753311, Total Loss 34.331966400146484\n",
      "4: Encoding Loss 4.418781757354736, Transition Loss -2.3918232917785645, Classifier Loss 0.13607317209243774, Total Loss 40.11905288696289\n",
      "4: Encoding Loss 2.2907092571258545, Transition Loss -0.7598912119865417, Classifier Loss 0.0932542011141777, Total Loss 23.069372177124023\n",
      "4: Encoding Loss 5.24887752532959, Transition Loss -0.8306689858436584, Classifier Loss 0.08886519074440002, Total Loss 40.37945556640625\n",
      "4: Encoding Loss 4.679401874542236, Transition Loss -1.3166110515594482, Classifier Loss 0.16148178279399872, Total Loss 44.224063873291016\n",
      "4: Encoding Loss 3.6333136558532715, Transition Loss -0.8755478858947754, Classifier Loss 0.08605741709470749, Total Loss 30.4052734375\n",
      "4: Encoding Loss 5.292623043060303, Transition Loss -0.3842076063156128, Classifier Loss 0.06526635587215424, Total Loss 38.282222747802734\n",
      "4: Encoding Loss 7.1389617919921875, Transition Loss -1.8221533298492432, Classifier Loss 0.22034139931201935, Total Loss 64.86717987060547\n",
      "4: Encoding Loss 4.827733516693115, Transition Loss -0.586145281791687, Classifier Loss 0.20747625827789307, Total Loss 49.71379470825195\n",
      "4: Encoding Loss 5.5110063552856445, Transition Loss -1.0300211906433105, Classifier Loss 0.21508067846298218, Total Loss 54.57369613647461\n",
      "4: Encoding Loss 5.034239292144775, Transition Loss -1.6146870851516724, Classifier Loss 0.11979898065328598, Total Loss 42.184688568115234\n",
      "4: Encoding Loss 6.343954563140869, Transition Loss -1.5327829122543335, Classifier Loss 0.16885340213775635, Total Loss 54.94845199584961\n",
      "4: Encoding Loss 5.812829971313477, Transition Loss -0.9842919111251831, Classifier Loss 0.12391545623540878, Total Loss 47.268131256103516\n",
      "4: Encoding Loss 5.808055877685547, Transition Loss -0.5729726552963257, Classifier Loss 0.09334561228752136, Total Loss 44.18266677856445\n",
      "4: Encoding Loss 4.230143070220947, Transition Loss -1.7030301094055176, Classifier Loss 0.12079185992479324, Total Loss 37.4593620300293\n",
      "4: Encoding Loss 6.4951581954956055, Transition Loss -0.18321895599365234, Classifier Loss 0.11030440032482147, Total Loss 50.001319885253906\n",
      "4: Encoding Loss 5.217376232147217, Transition Loss -1.2937707901000977, Classifier Loss 0.14496220648288727, Total Loss 45.79996109008789\n",
      "4: Encoding Loss 5.724887847900391, Transition Loss -1.3564095497131348, Classifier Loss 0.1269143968820572, Total Loss 47.040225982666016\n",
      "4: Encoding Loss 6.193877220153809, Transition Loss -0.6125750541687012, Classifier Loss 0.11714943498373032, Total Loss 48.87796401977539\n",
      "4: Encoding Loss 4.780928134918213, Transition Loss -1.7053401470184326, Classifier Loss 0.07647205889225006, Total Loss 36.33209228515625\n",
      "4: Encoding Loss 4.079523086547852, Transition Loss -1.2436444759368896, Classifier Loss 0.12050091475248337, Total Loss 36.526737213134766\n",
      "4: Encoding Loss 4.023433208465576, Transition Loss -1.247956395149231, Classifier Loss 0.10882440954446793, Total Loss 35.02254104614258\n",
      "4: Encoding Loss 7.450578212738037, Transition Loss -2.162336587905884, Classifier Loss 0.11519487202167511, Total Loss 56.22209548950195\n",
      "4: Encoding Loss 4.274209976196289, Transition Loss -3.0011048316955566, Classifier Loss 0.09313250333070755, Total Loss 34.95730972290039\n",
      "4: Encoding Loss 3.0188190937042236, Transition Loss -1.1055545806884766, Classifier Loss 0.11933919042348862, Total Loss 30.046390533447266\n",
      "4: Encoding Loss 10.45805549621582, Transition Loss 0.12330786883831024, Classifier Loss 0.1965886652469635, Total Loss 82.4565200805664\n",
      "4: Encoding Loss 7.451100826263428, Transition Loss -0.26107215881347656, Classifier Loss 0.14385901391506195, Total Loss 59.0924072265625\n",
      "4: Encoding Loss 4.838518142700195, Transition Loss -2.331069231033325, Classifier Loss 0.10137232393026352, Total Loss 39.16741180419922\n",
      "4: Encoding Loss 4.142404556274414, Transition Loss -0.646652340888977, Classifier Loss 0.09928276389837265, Total Loss 34.782447814941406\n",
      "4: Encoding Loss 7.312426567077637, Transition Loss -2.239229440689087, Classifier Loss 0.22064198553562164, Total Loss 65.9378662109375\n",
      "4: Encoding Loss 5.100079536437988, Transition Loss -1.095084547996521, Classifier Loss 0.09767114371061325, Total Loss 40.36715316772461\n",
      "4: Encoding Loss 4.515206336975098, Transition Loss -1.1030642986297607, Classifier Loss 0.09515827894210815, Total Loss 36.606624603271484\n",
      "4: Encoding Loss 4.648289680480957, Transition Loss -1.429701328277588, Classifier Loss 0.15297150611877441, Total Loss 43.186317443847656\n",
      "4: Encoding Loss 6.250670433044434, Transition Loss -0.7512010931968689, Classifier Loss 0.06363002210855484, Total Loss 43.86672592163086\n",
      "4: Encoding Loss 4.97486686706543, Transition Loss -1.507555365562439, Classifier Loss 0.10709578543901443, Total Loss 40.55817794799805\n",
      "4: Encoding Loss 7.396546363830566, Transition Loss -1.862486720085144, Classifier Loss 0.1086910218000412, Total Loss 55.24763870239258\n",
      "4: Encoding Loss 6.263310432434082, Transition Loss -0.6291056275367737, Classifier Loss 0.09965361654758453, Total Loss 47.54497528076172\n",
      "4: Encoding Loss 5.618128776550293, Transition Loss -0.44256794452667236, Classifier Loss 0.14403964579105377, Total Loss 48.11256408691406\n",
      "4: Encoding Loss 7.513464450836182, Transition Loss -1.066192388534546, Classifier Loss 0.08240541815757751, Total Loss 53.320899963378906\n",
      "4: Encoding Loss 6.111907005310059, Transition Loss -1.0996177196502686, Classifier Loss 0.20734956860542297, Total Loss 57.40596389770508\n",
      "4: Encoding Loss 5.17654275894165, Transition Loss -0.5934460163116455, Classifier Loss 0.12215850502252579, Total Loss 43.274871826171875\n",
      "4: Encoding Loss 3.5087242126464844, Transition Loss -1.0909373760223389, Classifier Loss 0.08925921469926834, Total Loss 29.97783088684082\n",
      "4: Encoding Loss 4.989838600158691, Transition Loss -1.475703239440918, Classifier Loss 0.10612180829048157, Total Loss 40.550621032714844\n",
      "4: Encoding Loss 5.208736419677734, Transition Loss -0.34032750129699707, Classifier Loss 0.11048244684934616, Total Loss 42.30052947998047\n",
      "4: Encoding Loss 4.646660327911377, Transition Loss 0.18674449622631073, Classifier Loss 0.13510571420192719, Total Loss 41.465232849121094\n",
      "4: Encoding Loss 4.441697120666504, Transition Loss -1.595263123512268, Classifier Loss 0.09559380263090134, Total Loss 36.208927154541016\n",
      "4: Encoding Loss 3.295771837234497, Transition Loss -2.4163835048675537, Classifier Loss 0.0909186378121376, Total Loss 28.865528106689453\n",
      "4: Encoding Loss 6.623781681060791, Transition Loss -1.8185384273529053, Classifier Loss 0.1303827464580536, Total Loss 52.78023910522461\n",
      "4: Encoding Loss 5.92864990234375, Transition Loss -1.9219461679458618, Classifier Loss 0.18960237503051758, Total Loss 54.53136444091797\n",
      "4: Encoding Loss 5.612458229064941, Transition Loss -0.18635980784893036, Classifier Loss 0.13212259113788605, Total Loss 46.886932373046875\n",
      "4: Encoding Loss 5.281186103820801, Transition Loss -0.9379177093505859, Classifier Loss 0.1408744603395462, Total Loss 45.77418899536133\n",
      "4: Encoding Loss 6.24746036529541, Transition Loss -0.6802917718887329, Classifier Loss 0.18597844243049622, Total Loss 56.082340240478516\n",
      "4: Encoding Loss 5.651159763336182, Transition Loss -0.7347930669784546, Classifier Loss 0.14692144095897675, Total Loss 48.598812103271484\n",
      "4: Encoding Loss 4.758608341217041, Transition Loss -1.0893973112106323, Classifier Loss 0.10552975535392761, Total Loss 39.104190826416016\n",
      "4: Encoding Loss 5.620319843292236, Transition Loss -2.5267388820648193, Classifier Loss 0.1123039722442627, Total Loss 44.9513053894043\n",
      "4: Encoding Loss 4.564464092254639, Transition Loss -1.0195748805999756, Classifier Loss 0.07726571708917618, Total Loss 35.11294937133789\n",
      "4: Encoding Loss 5.226199150085449, Transition Loss -1.434242606163025, Classifier Loss 0.1339016854763031, Total Loss 44.74679183959961\n",
      "4: Encoding Loss 6.1721086502075195, Transition Loss -2.6397013664245605, Classifier Loss 0.0743928775191307, Total Loss 44.47088623046875\n",
      "4: Encoding Loss 6.940550327301025, Transition Loss -0.866640031337738, Classifier Loss 0.08423452079296112, Total Loss 50.066410064697266\n",
      "4: Encoding Loss 3.5487945079803467, Transition Loss -0.753589391708374, Classifier Loss 0.09038285166025162, Total Loss 30.330753326416016\n",
      "4: Encoding Loss 6.339376926422119, Transition Loss -1.416746973991394, Classifier Loss 0.08093024790287018, Total Loss 46.128719329833984\n",
      "4: Encoding Loss 6.370700359344482, Transition Loss -1.196549892425537, Classifier Loss 0.09177865087985992, Total Loss 47.40159225463867\n",
      "4: Encoding Loss 4.20658016204834, Transition Loss -0.652285099029541, Classifier Loss 0.056553248316049576, Total Loss 30.894546508789062\n",
      "4: Encoding Loss 5.285409927368164, Transition Loss -2.4180829524993896, Classifier Loss 0.14574261009693146, Total Loss 46.28575134277344\n",
      "4: Encoding Loss 6.787332057952881, Transition Loss -0.42554226517677307, Classifier Loss 0.10917431861162186, Total Loss 51.64125442504883\n",
      "4: Encoding Loss 6.5453643798828125, Transition Loss -1.011944055557251, Classifier Loss 0.14376969635486603, Total Loss 53.64875030517578\n",
      "4: Encoding Loss 5.277438163757324, Transition Loss -1.3125578165054321, Classifier Loss 0.095601886510849, Total Loss 41.22429275512695\n",
      "4: Encoding Loss 5.307591915130615, Transition Loss -1.4416661262512207, Classifier Loss 0.10745862126350403, Total Loss 42.59083938598633\n",
      "4: Encoding Loss 5.247292518615723, Transition Loss -0.7600796818733215, Classifier Loss 0.18676206469535828, Total Loss 50.1596565246582\n",
      "4: Encoding Loss 6.214450836181641, Transition Loss -1.5166454315185547, Classifier Loss 0.08941864222288132, Total Loss 46.227962493896484\n",
      "4: Encoding Loss 3.417973756790161, Transition Loss -1.1551998853683472, Classifier Loss 0.1439734548330307, Total Loss 34.904727935791016\n",
      "4: Encoding Loss 8.856687545776367, Transition Loss 0.16579383611679077, Classifier Loss 0.13664564490318298, Total Loss 66.87100982666016\n",
      "4: Encoding Loss 5.7295379638671875, Transition Loss -1.0881884098052979, Classifier Loss 0.08082346618175507, Total Loss 42.45914077758789\n",
      "4: Encoding Loss 3.233966827392578, Transition Loss -2.124268054962158, Classifier Loss 0.09512001276016235, Total Loss 28.914953231811523\n",
      "4: Encoding Loss 6.131512641906738, Transition Loss -0.28655391931533813, Classifier Loss 0.07948420941829681, Total Loss 44.73738479614258\n",
      "4: Encoding Loss 3.110393762588501, Transition Loss -0.87473464012146, Classifier Loss 0.05836279317736626, Total Loss 24.498292922973633\n",
      "4: Encoding Loss 7.814550876617432, Transition Loss -1.6236876249313354, Classifier Loss 0.15545260906219482, Total Loss 62.43191909790039\n",
      "4: Encoding Loss 7.917880058288574, Transition Loss -1.0542783737182617, Classifier Loss 0.1355801820755005, Total Loss 61.064876556396484\n",
      "4: Encoding Loss 6.240618705749512, Transition Loss -1.897397518157959, Classifier Loss 0.09475777298212051, Total Loss 46.918731689453125\n",
      "4: Encoding Loss 4.463135242462158, Transition Loss -1.0504727363586426, Classifier Loss 0.14588278532028198, Total Loss 41.36667251586914\n",
      "4: Encoding Loss 4.408520221710205, Transition Loss -1.5051926374435425, Classifier Loss 0.11751066893339157, Total Loss 38.20158767700195\n",
      "4: Encoding Loss 5.681324005126953, Transition Loss -1.5108723640441895, Classifier Loss 0.10952115803956985, Total Loss 45.039459228515625\n",
      "4: Encoding Loss 3.7193665504455566, Transition Loss -0.8320939540863037, Classifier Loss 0.10947676748037338, Total Loss 33.263545989990234\n",
      "4: Encoding Loss 6.771054267883301, Transition Loss -1.9609706401824951, Classifier Loss 0.18385876715183258, Total Loss 59.01142120361328\n",
      "4: Encoding Loss 4.634222507476807, Transition Loss -1.747721791267395, Classifier Loss 0.09116537123918533, Total Loss 36.92117691040039\n",
      "4: Encoding Loss 3.492344617843628, Transition Loss -0.7229571342468262, Classifier Loss 0.09075285494327545, Total Loss 30.029064178466797\n",
      "4: Encoding Loss 5.858059883117676, Transition Loss -0.038590848445892334, Classifier Loss 0.16183020174503326, Total Loss 51.33136749267578\n",
      "4: Encoding Loss 5.315893650054932, Transition Loss -1.6609094142913818, Classifier Loss 0.09060075134038925, Total Loss 40.95477294921875\n",
      "4: Encoding Loss 7.562209129333496, Transition Loss -0.6803176403045654, Classifier Loss 0.12612324953079224, Total Loss 57.98530960083008\n",
      "4: Encoding Loss 4.126429557800293, Transition Loss -2.203329563140869, Classifier Loss 0.1997264325618744, Total Loss 44.730342864990234\n",
      "4: Encoding Loss 7.060860633850098, Transition Loss -1.073226809501648, Classifier Loss 0.1138545498251915, Total Loss 53.75019073486328\n",
      "4: Encoding Loss 5.110946178436279, Transition Loss -1.1297653913497925, Classifier Loss 0.06608831882476807, Total Loss 37.2740592956543\n",
      "4: Encoding Loss 8.308793067932129, Transition Loss -0.41647252440452576, Classifier Loss 0.15207348763942719, Total Loss 65.05994415283203\n",
      "4: Encoding Loss 5.606196403503418, Transition Loss -1.218996524810791, Classifier Loss 0.1490222066640854, Total Loss 48.53891372680664\n",
      "4: Encoding Loss 4.41259241104126, Transition Loss -1.0508030652999878, Classifier Loss 0.10769717395305634, Total Loss 37.24485397338867\n",
      "4: Encoding Loss 5.234227180480957, Transition Loss -2.5666379928588867, Classifier Loss 0.0996464267373085, Total Loss 41.368980407714844\n",
      "4: Encoding Loss 3.7906570434570312, Transition Loss 0.3910250663757324, Classifier Loss 0.1325078308582306, Total Loss 36.1511344909668\n",
      "4: Encoding Loss 3.5222744941711426, Transition Loss -0.33587977290153503, Classifier Loss 0.06889282166957855, Total Loss 28.022796630859375\n",
      "4: Encoding Loss 4.716216087341309, Transition Loss -2.255321502685547, Classifier Loss 0.16267700493335724, Total Loss 44.5640983581543\n",
      "4: Encoding Loss 5.7033514976501465, Transition Loss -1.5828421115875244, Classifier Loss 0.175384059548378, Total Loss 51.75788497924805\n",
      "4: Encoding Loss 4.359525680541992, Transition Loss -0.5959854125976562, Classifier Loss 0.11588228493928909, Total Loss 37.745147705078125\n",
      "4: Encoding Loss 4.6645050048828125, Transition Loss -0.5794069170951843, Classifier Loss 0.09717654436826706, Total Loss 37.70445251464844\n",
      "4: Encoding Loss 4.614640235900879, Transition Loss -1.1182332038879395, Classifier Loss 0.07102034986019135, Total Loss 34.7894287109375\n",
      "4: Encoding Loss 7.0248308181762695, Transition Loss -0.19206121563911438, Classifier Loss 0.24439847469329834, Total Loss 66.58876037597656\n",
      "4: Encoding Loss 5.500814914703369, Transition Loss -1.2017239332199097, Classifier Loss 0.10572558641433716, Total Loss 43.576969146728516\n",
      "4: Encoding Loss 7.199887275695801, Transition Loss -1.4611563682556152, Classifier Loss 0.09707563370466232, Total Loss 52.906307220458984\n",
      "4: Encoding Loss 4.424694061279297, Transition Loss -0.1710917353630066, Classifier Loss 0.08948898315429688, Total Loss 35.49699401855469\n",
      "4: Encoding Loss 5.780422210693359, Transition Loss -1.5332789421081543, Classifier Loss 0.07232633978128433, Total Loss 41.914554595947266\n",
      "4: Encoding Loss 5.173228740692139, Transition Loss -0.7978190183639526, Classifier Loss 0.11829216033220291, Total Loss 42.86827087402344\n",
      "4: Encoding Loss 4.9274115562438965, Transition Loss -1.2857887744903564, Classifier Loss 0.2243366688489914, Total Loss 51.997623443603516\n",
      "4: Encoding Loss 8.616497039794922, Transition Loss -0.41554874181747437, Classifier Loss 0.23182645440101624, Total Loss 74.88146209716797\n",
      "4: Encoding Loss 7.397924900054932, Transition Loss -1.2978684902191162, Classifier Loss 0.1057770848274231, Total Loss 54.96474075317383\n",
      "4: Encoding Loss 6.796395778656006, Transition Loss -1.2449827194213867, Classifier Loss 0.18592318892478943, Total Loss 59.37019729614258\n",
      "4: Encoding Loss 5.067620754241943, Transition Loss -1.6497011184692383, Classifier Loss 0.07085881382226944, Total Loss 37.49094772338867\n",
      "4: Encoding Loss 5.45330810546875, Transition Loss -1.1270805597305298, Classifier Loss 0.10435739159584045, Total Loss 43.15513610839844\n",
      "4: Encoding Loss 7.063172817230225, Transition Loss -1.0909470319747925, Classifier Loss 0.11506129801273346, Total Loss 53.884735107421875\n",
      "4: Encoding Loss 5.577410697937012, Transition Loss -1.3186595439910889, Classifier Loss 0.13271671533584595, Total Loss 46.73561096191406\n",
      "4: Encoding Loss 3.5172832012176514, Transition Loss 0.18573376536369324, Classifier Loss 0.06486910581588745, Total Loss 27.66490364074707\n",
      "4: Encoding Loss 4.154801368713379, Transition Loss -0.7612618803977966, Classifier Loss 0.10153239220380783, Total Loss 35.08174133300781\n",
      "4: Encoding Loss 4.716341018676758, Transition Loss 0.06055450439453125, Classifier Loss 0.1034240573644638, Total Loss 38.664676666259766\n",
      "4: Encoding Loss 4.711180210113525, Transition Loss -1.3544294834136963, Classifier Loss 0.07963672280311584, Total Loss 36.2302131652832\n",
      "4: Encoding Loss 7.276278495788574, Transition Loss -0.4378741979598999, Classifier Loss 0.21454641222953796, Total Loss 65.11213684082031\n",
      "4: Encoding Loss 5.560114860534668, Transition Loss -1.6518577337265015, Classifier Loss 0.09889053553342819, Total Loss 43.24908447265625\n",
      "4: Encoding Loss 8.57404613494873, Transition Loss -2.607102155685425, Classifier Loss 0.1261300891637802, Total Loss 64.05624389648438\n",
      "4: Encoding Loss 6.278919696807861, Transition Loss -1.5320628881454468, Classifier Loss 0.09520203620195389, Total Loss 47.19310760498047\n",
      "4: Encoding Loss 3.715435743331909, Transition Loss -1.0374499559402466, Classifier Loss 0.10503973066806793, Total Loss 32.796173095703125\n",
      "4: Encoding Loss 4.187344074249268, Transition Loss -0.418940007686615, Classifier Loss 0.04545621946454048, Total Loss 29.669519424438477\n",
      "4: Encoding Loss 6.428104400634766, Transition Loss -2.7741410732269287, Classifier Loss 0.2541201412677765, Total Loss 63.979530334472656\n",
      "4: Encoding Loss 5.32943058013916, Transition Loss -1.3889853954315186, Classifier Loss 0.16027067601680756, Total Loss 48.00309371948242\n",
      "4: Encoding Loss 3.723651170730591, Transition Loss 0.6412916779518127, Classifier Loss 0.0782051607966423, Total Loss 30.418941497802734\n",
      "4: Encoding Loss 5.743269920349121, Transition Loss -0.9564458131790161, Classifier Loss 0.2397651970386505, Total Loss 58.435760498046875\n",
      "4: Encoding Loss 4.731960296630859, Transition Loss -1.681879997253418, Classifier Loss 0.14535042643547058, Total Loss 42.9261360168457\n",
      "4: Encoding Loss 6.315126419067383, Transition Loss -0.8568325638771057, Classifier Loss 0.17214299738407135, Total Loss 55.104713439941406\n",
      "4: Encoding Loss 3.4630255699157715, Transition Loss -0.5012720227241516, Classifier Loss 0.08644741028547287, Total Loss 29.42269515991211\n",
      "4: Encoding Loss 3.1721291542053223, Transition Loss -0.7183785438537598, Classifier Loss 0.1699592024087906, Total Loss 36.02840805053711\n",
      "4: Encoding Loss 5.854066848754883, Transition Loss -2.1970293521881104, Classifier Loss 0.1291087418794632, Total Loss 48.03439712524414\n",
      "4: Encoding Loss 5.283015251159668, Transition Loss -1.4399139881134033, Classifier Loss 0.17453250288963318, Total Loss 49.1507682800293\n",
      "4: Encoding Loss 5.783278465270996, Transition Loss -2.0817599296569824, Classifier Loss 0.1551256626844406, Total Loss 50.21140670776367\n",
      "4: Encoding Loss 5.764364719390869, Transition Loss -1.6551315784454346, Classifier Loss 0.07029364258050919, Total Loss 41.614891052246094\n",
      "4: Encoding Loss 5.610103607177734, Transition Loss -2.438528537750244, Classifier Loss 0.13234040141105652, Total Loss 46.89368438720703\n",
      "4: Encoding Loss 7.16095495223999, Transition Loss -1.1096322536468506, Classifier Loss 0.15894851088523865, Total Loss 58.86014175415039\n",
      "4: Encoding Loss 6.445556640625, Transition Loss -1.5084965229034424, Classifier Loss 0.1841951310634613, Total Loss 57.09225082397461\n",
      "4: Encoding Loss 6.105106353759766, Transition Loss -0.8735460042953491, Classifier Loss 0.09538698941469193, Total Loss 46.16898727416992\n",
      "4: Encoding Loss 4.124176979064941, Transition Loss -2.054091691970825, Classifier Loss 0.10542362183332443, Total Loss 35.28660202026367\n",
      "4: Encoding Loss 6.624701023101807, Transition Loss -1.5150606632232666, Classifier Loss 0.11658593267202377, Total Loss 51.406192779541016\n",
      "4: Encoding Loss 4.6974592208862305, Transition Loss -2.0435051918029785, Classifier Loss 0.09938374161720276, Total Loss 38.122314453125\n",
      "4: Encoding Loss 5.7823028564453125, Transition Loss -1.6906083822250366, Classifier Loss 0.07633903622627258, Total Loss 42.32704544067383\n",
      "4: Encoding Loss 4.81020450592041, Transition Loss -0.9715000987052917, Classifier Loss 0.08527449518442154, Total Loss 37.38828659057617\n",
      "4: Encoding Loss 5.3287553787231445, Transition Loss -1.5626691579818726, Classifier Loss 0.162019744515419, Total Loss 48.17388153076172\n",
      "4: Encoding Loss 4.166147708892822, Transition Loss 0.03211948275566101, Classifier Loss 0.07130687683820724, Total Loss 32.14042282104492\n",
      "4: Encoding Loss 8.975601196289062, Transition Loss -1.2724380493164062, Classifier Loss 0.1840141862630844, Total Loss 72.2545166015625\n",
      "4: Encoding Loss 8.157332420349121, Transition Loss -1.9329452514648438, Classifier Loss 0.17235374450683594, Total Loss 66.17859649658203\n",
      "4: Encoding Loss 7.502439022064209, Transition Loss -1.0922303199768066, Classifier Loss 0.19146296381950378, Total Loss 64.1604995727539\n",
      "4: Encoding Loss 6.114613056182861, Transition Loss -0.3094301223754883, Classifier Loss 0.1529548168182373, Total Loss 51.98303985595703\n",
      "4: Encoding Loss 7.067983150482178, Transition Loss -0.24184924364089966, Classifier Loss 0.23283761739730835, Total Loss 65.69156646728516\n",
      "4: Encoding Loss 4.540918827056885, Transition Loss 0.0378166139125824, Classifier Loss 0.07864530384540558, Total Loss 35.12517166137695\n",
      "4: Encoding Loss 3.7957749366760254, Transition Loss -0.9552536010742188, Classifier Loss 0.12310073524713516, Total Loss 35.08434295654297\n",
      "4: Encoding Loss 2.83964204788208, Transition Loss 0.15711137652397156, Classifier Loss 0.12007514387369156, Total Loss 29.108213424682617\n",
      "4: Encoding Loss 5.45658016204834, Transition Loss -0.6839520931243896, Classifier Loss 0.14971193671226501, Total Loss 47.71040344238281\n",
      "4: Encoding Loss 6.146798610687256, Transition Loss -1.6474908590316772, Classifier Loss 0.12262528389692307, Total Loss 49.142662048339844\n",
      "4: Encoding Loss 7.179729461669922, Transition Loss -1.4609465599060059, Classifier Loss 0.20220790803432465, Total Loss 63.298583984375\n",
      "4: Encoding Loss 4.274718284606934, Transition Loss -1.5353800058364868, Classifier Loss 0.094511479139328, Total Loss 35.098846435546875\n",
      "4: Encoding Loss 6.408790588378906, Transition Loss -2.130464792251587, Classifier Loss 0.17344363033771515, Total Loss 55.7962532043457\n",
      "4: Encoding Loss 4.954455852508545, Transition Loss -1.1304380893707275, Classifier Loss 0.09157868474721909, Total Loss 38.884151458740234\n",
      "4: Encoding Loss 5.1583781242370605, Transition Loss -1.678964376449585, Classifier Loss 0.1138862892985344, Total Loss 42.338226318359375\n",
      "4: Encoding Loss 4.6472697257995605, Transition Loss -2.899570941925049, Classifier Loss 0.0938837081193924, Total Loss 37.27082824707031\n",
      "4: Encoding Loss 2.682267427444458, Transition Loss -1.6252248287200928, Classifier Loss 0.09913276880979538, Total Loss 26.0062313079834\n",
      "4: Encoding Loss 6.08837890625, Transition Loss -1.6482365131378174, Classifier Loss 0.11305349320173264, Total Loss 47.834964752197266\n",
      "4: Encoding Loss 5.553036689758301, Transition Loss -1.7402077913284302, Classifier Loss 0.18809586763381958, Total Loss 52.127113342285156\n",
      "4: Encoding Loss 3.315903902053833, Transition Loss -1.6020456552505493, Classifier Loss 0.09587041288614273, Total Loss 29.481822967529297\n",
      "4: Encoding Loss 7.208477973937988, Transition Loss -2.142594575881958, Classifier Loss 0.1227390244603157, Total Loss 55.5239143371582\n",
      "4: Encoding Loss 5.100639343261719, Transition Loss -0.9524186849594116, Classifier Loss 0.11775302141904831, Total Loss 42.37875747680664\n",
      "4: Encoding Loss 6.748362064361572, Transition Loss -1.074512004852295, Classifier Loss 0.1251584142446518, Total Loss 53.005584716796875\n",
      "4: Encoding Loss 7.169861793518066, Transition Loss -1.4103978872299194, Classifier Loss 0.1690225601196289, Total Loss 59.92086410522461\n",
      "4: Encoding Loss 6.608707427978516, Transition Loss -0.8172896504402161, Classifier Loss 0.13635532557964325, Total Loss 53.28744888305664\n",
      "4: Encoding Loss 4.850171089172363, Transition Loss -1.4850385189056396, Classifier Loss 0.11313528567552567, Total Loss 40.41395950317383\n",
      "4: Encoding Loss 6.072482109069824, Transition Loss -1.7032325267791748, Classifier Loss 0.1378910392522812, Total Loss 50.22331619262695\n",
      "4: Encoding Loss 5.264586448669434, Transition Loss -1.482776165008545, Classifier Loss 0.09851279854774475, Total Loss 41.438209533691406\n",
      "4: Encoding Loss 2.825024127960205, Transition Loss -0.2969036102294922, Classifier Loss 0.053764499723911285, Total Loss 22.32647705078125\n",
      "4: Encoding Loss 7.850852966308594, Transition Loss -0.35557711124420166, Classifier Loss 0.10362902283668518, Total Loss 57.46788024902344\n",
      "4: Encoding Loss 6.236109733581543, Transition Loss -1.2764126062393188, Classifier Loss 0.09746471792459488, Total Loss 47.162620544433594\n",
      "4: Encoding Loss 5.885189056396484, Transition Loss -1.1051417589187622, Classifier Loss 0.05304650962352753, Total Loss 40.61534118652344\n",
      "4: Encoding Loss 4.247986793518066, Transition Loss -0.2370314747095108, Classifier Loss 0.07454874366521835, Total Loss 32.94269943237305\n",
      "4: Encoding Loss 4.23565673828125, Transition Loss -1.914528250694275, Classifier Loss 0.0593663826584816, Total Loss 31.34981346130371\n",
      "4: Encoding Loss 4.247317314147949, Transition Loss -1.6803635358810425, Classifier Loss 0.14613483846187592, Total Loss 40.096717834472656\n",
      "4: Encoding Loss 5.787060737609863, Transition Loss -0.4332576394081116, Classifier Loss 0.0812777429819107, Total Loss 42.84996795654297\n",
      "4: Encoding Loss 4.806461334228516, Transition Loss -1.021193504333496, Classifier Loss 0.07485579699277878, Total Loss 36.32394027709961\n",
      "4: Encoding Loss 4.709217548370361, Transition Loss -0.5900986194610596, Classifier Loss 0.0709342435002327, Total Loss 35.34849548339844\n",
      "4: Encoding Loss 3.683472156524658, Transition Loss -0.9190645813941956, Classifier Loss 0.09476590156555176, Total Loss 31.577054977416992\n",
      "4: Encoding Loss 6.409398078918457, Transition Loss -1.3250776529312134, Classifier Loss 0.10072479397058487, Total Loss 48.52833938598633\n",
      "4: Encoding Loss 4.622720241546631, Transition Loss -1.7537333965301514, Classifier Loss 0.10253965109586716, Total Loss 37.989585876464844\n",
      "4: Encoding Loss 3.9839069843292236, Transition Loss -2.198493480682373, Classifier Loss 0.08043587952852249, Total Loss 31.946149826049805\n",
      "4: Encoding Loss 3.7103030681610107, Transition Loss -1.3194549083709717, Classifier Loss 0.13673178851604462, Total Loss 35.934471130371094\n",
      "4: Encoding Loss 6.265534400939941, Transition Loss -0.8124829530715942, Classifier Loss 0.11909491568803787, Total Loss 49.502376556396484\n",
      "4: Encoding Loss 3.8531627655029297, Transition Loss -0.9426576495170593, Classifier Loss 0.08624769002199173, Total Loss 31.74336814880371\n",
      "4: Encoding Loss 4.538729667663574, Transition Loss -1.0832958221435547, Classifier Loss 0.11721426248550415, Total Loss 38.953369140625\n",
      "4: Encoding Loss 4.9873857498168945, Transition Loss -2.4664340019226074, Classifier Loss 0.11401202529668808, Total Loss 41.324527740478516\n",
      "4: Encoding Loss 4.34149694442749, Transition Loss -1.1446001529693604, Classifier Loss 0.0603833869099617, Total Loss 32.08686447143555\n",
      "4: Encoding Loss 3.6607799530029297, Transition Loss -0.8327379822731018, Classifier Loss 0.09165814518928528, Total Loss 31.130159378051758\n",
      "4: Encoding Loss 4.666805267333984, Transition Loss -1.5979442596435547, Classifier Loss 0.17817570269107819, Total Loss 45.81776428222656\n",
      "4: Encoding Loss 3.561647891998291, Transition Loss -2.2363955974578857, Classifier Loss 0.10181380808353424, Total Loss 31.55037498474121\n",
      "4: Encoding Loss 3.6987576484680176, Transition Loss -0.940365731716156, Classifier Loss 0.09705697745084763, Total Loss 31.897869110107422\n",
      "4: Encoding Loss 4.6298394203186035, Transition Loss -2.135671854019165, Classifier Loss 0.10948881506919861, Total Loss 38.72706604003906\n",
      "4: Encoding Loss 5.373383045196533, Transition Loss -1.0395303964614868, Classifier Loss 0.17593058943748474, Total Loss 49.832942962646484\n",
      "4: Encoding Loss 5.765743255615234, Transition Loss -1.43362295627594, Classifier Loss 0.12902936339378357, Total Loss 47.496822357177734\n",
      "4: Encoding Loss 3.581061601638794, Transition Loss -1.8720436096191406, Classifier Loss 0.10023477673530579, Total Loss 31.509098052978516\n",
      "4: Encoding Loss 6.615971565246582, Transition Loss -1.1284409761428833, Classifier Loss 0.18962351977825165, Total Loss 58.65773391723633\n",
      "4: Encoding Loss 7.414468288421631, Transition Loss -1.7285687923431396, Classifier Loss 0.2677196264266968, Total Loss 71.2580795288086\n",
      "4: Encoding Loss 7.39696741104126, Transition Loss -1.639961838722229, Classifier Loss 0.15231797099113464, Total Loss 59.612945556640625\n",
      "4: Encoding Loss 7.229234218597412, Transition Loss -2.2144644260406494, Classifier Loss 0.14211976528167725, Total Loss 57.58650207519531\n",
      "4: Encoding Loss 4.4129719734191895, Transition Loss -2.642897605895996, Classifier Loss 0.10333682596683502, Total Loss 36.81045913696289\n",
      "4: Encoding Loss 8.164647102355957, Transition Loss -2.0965449810028076, Classifier Loss 0.11054247617721558, Total Loss 60.04129409790039\n",
      "4: Encoding Loss 8.243277549743652, Transition Loss -0.2576811611652374, Classifier Loss 0.14225925505161285, Total Loss 63.685489654541016\n",
      "4: Encoding Loss 6.936487674713135, Transition Loss -2.263537883758545, Classifier Loss 0.26088279485702515, Total Loss 67.706298828125\n",
      "4: Encoding Loss 7.668905735015869, Transition Loss -1.7354977130889893, Classifier Loss 0.16718411445617676, Total Loss 62.73115158081055\n",
      "4: Encoding Loss 6.646777153015137, Transition Loss 0.2631289064884186, Classifier Loss 0.08952834457159042, Total Loss 48.938751220703125\n",
      "4: Encoding Loss 5.414068698883057, Transition Loss -1.5611176490783691, Classifier Loss 0.06315766274929047, Total Loss 38.79955291748047\n",
      "4: Encoding Loss 2.6847431659698486, Transition Loss -1.3100732564926147, Classifier Loss 0.11768437176942825, Total Loss 27.876371383666992\n",
      "4: Encoding Loss 4.584508895874023, Transition Loss -0.7006605267524719, Classifier Loss 0.10871592164039612, Total Loss 38.37836837768555\n",
      "4: Encoding Loss 3.834132671356201, Transition Loss 0.40067601203918457, Classifier Loss 0.09882786124944687, Total Loss 33.0478515625\n",
      "4: Encoding Loss 2.1808650493621826, Transition Loss -1.2192732095718384, Classifier Loss 0.06620817631483078, Total Loss 19.705520629882812\n",
      "4: Encoding Loss 4.434505462646484, Transition Loss -0.9717172980308533, Classifier Loss 0.17809951305389404, Total Loss 44.416595458984375\n",
      "4: Encoding Loss 4.090927600860596, Transition Loss -0.852199912071228, Classifier Loss 0.16138257086277008, Total Loss 40.6834831237793\n",
      "4: Encoding Loss 6.6892523765563965, Transition Loss -0.36343106627464294, Classifier Loss 0.08158749341964722, Total Loss 48.29412078857422\n",
      "4: Encoding Loss 5.713818550109863, Transition Loss -2.027292013168335, Classifier Loss 0.09766732901334763, Total Loss 44.048831939697266\n",
      "4: Encoding Loss 7.698919773101807, Transition Loss -1.2372043132781982, Classifier Loss 0.30056360363960266, Total Loss 76.24938201904297\n",
      "4: Encoding Loss 5.514344692230225, Transition Loss -2.2167305946350098, Classifier Loss 0.1662425845861435, Total Loss 49.709442138671875\n",
      "4: Encoding Loss 4.531374931335449, Transition Loss -1.3812401294708252, Classifier Loss 0.07588142901659012, Total Loss 34.775840759277344\n",
      "4: Encoding Loss 6.732013702392578, Transition Loss -1.6496800184249878, Classifier Loss 0.16958828270435333, Total Loss 57.350250244140625\n",
      "4: Encoding Loss 6.6380934715271, Transition Loss -1.563339114189148, Classifier Loss 0.13382385671138763, Total Loss 53.210323333740234\n",
      "4: Encoding Loss 5.193231105804443, Transition Loss -0.0003205835819244385, Classifier Loss 0.14319606125354767, Total Loss 45.4789924621582\n",
      "4: Encoding Loss 4.404426574707031, Transition Loss 0.5080279111862183, Classifier Loss 0.10871095955371857, Total Loss 37.50086975097656\n",
      "4: Encoding Loss 5.335627555847168, Transition Loss -1.9099483489990234, Classifier Loss 0.11338914185762405, Total Loss 43.3519172668457\n",
      "4: Encoding Loss 6.784085273742676, Transition Loss -1.429607629776001, Classifier Loss 0.10878864675760269, Total Loss 51.58280563354492\n",
      "4: Encoding Loss 5.3479695320129395, Transition Loss -0.41475099325180054, Classifier Loss 0.1482371836900711, Total Loss 46.911373138427734\n",
      "4: Encoding Loss 3.266489028930664, Transition Loss -0.47377991676330566, Classifier Loss 0.06667827069759369, Total Loss 26.266572952270508\n",
      "4: Encoding Loss 4.873461723327637, Transition Loss -1.8148361444473267, Classifier Loss 0.13976162672042847, Total Loss 43.216209411621094\n",
      "4: Encoding Loss 3.972209930419922, Transition Loss 0.6259584426879883, Classifier Loss 0.11962605267763138, Total Loss 36.04624938964844\n",
      "4: Encoding Loss 6.363763332366943, Transition Loss -1.6740257740020752, Classifier Loss 0.12199243158102036, Total Loss 50.38115692138672\n",
      "4: Encoding Loss 4.126173973083496, Transition Loss -2.0554823875427246, Classifier Loss 0.10219886153936386, Total Loss 34.97610855102539\n",
      "4: Encoding Loss 6.347487449645996, Transition Loss -1.1568238735198975, Classifier Loss 0.11836137622594833, Total Loss 49.92060470581055\n",
      "4: Encoding Loss 4.885973930358887, Transition Loss -1.9051357507705688, Classifier Loss 0.08739287406206131, Total Loss 38.05437088012695\n",
      "4: Encoding Loss 5.2735772132873535, Transition Loss -1.3614933490753174, Classifier Loss 0.12862560153007507, Total Loss 44.50347900390625\n",
      "4: Encoding Loss 5.625349998474121, Transition Loss -1.9376647472381592, Classifier Loss 0.13144974410533905, Total Loss 46.89630126953125\n",
      "4: Encoding Loss 4.195939064025879, Transition Loss 0.5812144875526428, Classifier Loss 0.11784770339727402, Total Loss 37.19289016723633\n",
      "4: Encoding Loss 1.9143569469451904, Transition Loss -0.767861008644104, Classifier Loss 0.08995721489191055, Total Loss 20.481557846069336\n",
      "4: Encoding Loss 11.792316436767578, Transition Loss -1.7456154823303223, Classifier Loss 0.1348264217376709, Total Loss 84.23583984375\n",
      "4: Encoding Loss 10.61658763885498, Transition Loss -1.4063220024108887, Classifier Loss 0.1119396835565567, Total Loss 74.89292907714844\n",
      "4: Encoding Loss 7.21307897567749, Transition Loss -0.8590559959411621, Classifier Loss 0.125440776348114, Total Loss 55.82221221923828\n",
      "4: Encoding Loss 5.48046350479126, Transition Loss -1.1331546306610107, Classifier Loss 0.09179198741912842, Total Loss 42.061527252197266\n",
      "4: Encoding Loss 3.917327404022217, Transition Loss -1.0990331172943115, Classifier Loss 0.14424890279769897, Total Loss 37.92841720581055\n",
      "4: Encoding Loss 5.368327617645264, Transition Loss -1.5370397567749023, Classifier Loss 0.16590547561645508, Total Loss 48.79990005493164\n",
      "4: Encoding Loss 3.524653673171997, Transition Loss 0.08554673194885254, Classifier Loss 0.07261484861373901, Total Loss 28.443626403808594\n",
      "4: Encoding Loss 6.635536193847656, Transition Loss -2.1650168895721436, Classifier Loss 0.07808364927768707, Total Loss 47.6207160949707\n",
      "4: Encoding Loss 5.528369903564453, Transition Loss -1.187448263168335, Classifier Loss 0.056177541613578796, Total Loss 38.78749465942383\n",
      "4: Encoding Loss 5.1314496994018555, Transition Loss -1.8747048377990723, Classifier Loss 0.10734083503484726, Total Loss 41.522029876708984\n",
      "4: Encoding Loss 4.152927875518799, Transition Loss -0.9478130340576172, Classifier Loss 0.061881810426712036, Total Loss 31.105369567871094\n",
      "4: Encoding Loss 4.262907028198242, Transition Loss 0.011544972658157349, Classifier Loss 0.16014313697814941, Total Loss 41.596378326416016\n",
      "4: Encoding Loss 5.025638103485107, Transition Loss -1.1578044891357422, Classifier Loss 0.06917442381381989, Total Loss 37.0708122253418\n",
      "4: Encoding Loss 3.8362762928009033, Transition Loss -2.361180543899536, Classifier Loss 0.1220025047659874, Total Loss 35.21696472167969\n",
      "4: Encoding Loss 5.160041809082031, Transition Loss -1.560041069984436, Classifier Loss 0.2151099592447281, Total Loss 52.47062301635742\n",
      "4: Encoding Loss 5.315177917480469, Transition Loss -1.2908167839050293, Classifier Loss 0.12133733183145523, Total Loss 44.024288177490234\n",
      "4: Encoding Loss 7.61472225189209, Transition Loss -1.2922558784484863, Classifier Loss 0.12015227973461151, Total Loss 57.70304489135742\n",
      "4: Encoding Loss 5.132299423217773, Transition Loss -0.5026125907897949, Classifier Loss 0.1136055663228035, Total Loss 42.154151916503906\n",
      "4: Encoding Loss 4.1185688972473145, Transition Loss 0.015107780694961548, Classifier Loss 0.12948660552501678, Total Loss 37.66611862182617\n",
      "4: Encoding Loss 7.61425256729126, Transition Loss -1.3432568311691284, Classifier Loss 0.10725072771310806, Total Loss 56.41005325317383\n",
      "4: Encoding Loss 5.720808982849121, Transition Loss -2.0441770553588867, Classifier Loss 0.16821573674678802, Total Loss 51.14561462402344\n",
      "4: Encoding Loss 7.4528703689575195, Transition Loss -2.018467426300049, Classifier Loss 0.1370713710784912, Total Loss 58.423553466796875\n",
      "4: Encoding Loss 5.500625133514404, Transition Loss -1.9982715845108032, Classifier Loss 0.07594828307628632, Total Loss 40.597782135009766\n",
      "4: Encoding Loss 3.8732943534851074, Transition Loss -1.9939711093902588, Classifier Loss 0.1441870629787445, Total Loss 37.657676696777344\n",
      "4: Encoding Loss 6.795114994049072, Transition Loss -1.0732405185699463, Classifier Loss 0.11609698086977005, Total Loss 52.37995910644531\n",
      "4: Encoding Loss 8.632162094116211, Transition Loss -1.3832193613052368, Classifier Loss 0.16625364124774933, Total Loss 68.41778564453125\n",
      "4: Encoding Loss 6.402696132659912, Transition Loss -1.3346587419509888, Classifier Loss 0.07104291021823883, Total Loss 45.519935607910156\n",
      "4: Encoding Loss 4.1367974281311035, Transition Loss -0.3638346195220947, Classifier Loss 0.08664272725582123, Total Loss 33.48491287231445\n",
      "4: Encoding Loss 3.6031413078308105, Transition Loss -0.9776541590690613, Classifier Loss 0.11235535144805908, Total Loss 32.8539924621582\n",
      "4: Encoding Loss 6.116469383239746, Transition Loss -1.372786045074463, Classifier Loss 0.09529980272054672, Total Loss 46.228248596191406\n",
      "4: Encoding Loss 4.507436275482178, Transition Loss -1.1389591693878174, Classifier Loss 0.05335334315896034, Total Loss 32.37949752807617\n",
      "4: Encoding Loss 6.592216491699219, Transition Loss -1.924619197845459, Classifier Loss 0.06613396108150482, Total Loss 46.165924072265625\n",
      "4: Encoding Loss 3.7774548530578613, Transition Loss -1.751842975616455, Classifier Loss 0.10931231081485748, Total Loss 33.59526062011719\n",
      "4: Encoding Loss 8.399965286254883, Transition Loss -0.09588664770126343, Classifier Loss 0.2560662627220154, Total Loss 76.00637817382812\n",
      "4: Encoding Loss 5.90886116027832, Transition Loss -1.0892597436904907, Classifier Loss 0.1375453770160675, Total Loss 49.20726776123047\n",
      "4: Encoding Loss 6.478396415710449, Transition Loss -1.2506824731826782, Classifier Loss 0.12026723474264145, Total Loss 50.896602630615234\n",
      "4: Encoding Loss 5.7289252281188965, Transition Loss 0.0068293362855911255, Classifier Loss 0.11203031241893768, Total Loss 45.579315185546875\n",
      "4: Encoding Loss 6.305821418762207, Transition Loss -0.605164647102356, Classifier Loss 0.19876916706562042, Total Loss 57.711605072021484\n",
      "4: Encoding Loss 6.160994529724121, Transition Loss -1.3150039911270142, Classifier Loss 0.17231258749961853, Total Loss 54.19670104980469\n",
      "4: Encoding Loss 4.358202934265137, Transition Loss -0.7027631998062134, Classifier Loss 0.09987711161375046, Total Loss 36.13665008544922\n",
      "4: Encoding Loss 4.839015007019043, Transition Loss -1.9105556011199951, Classifier Loss 0.14688558876514435, Total Loss 43.721885681152344\n",
      "4: Encoding Loss 4.945487976074219, Transition Loss -0.7399460673332214, Classifier Loss 0.1483435332775116, Total Loss 44.50698471069336\n",
      "4: Encoding Loss 5.866785049438477, Transition Loss -0.3633112907409668, Classifier Loss 0.12545408308506012, Total Loss 47.7459716796875\n",
      "4: Encoding Loss 3.5102202892303467, Transition Loss -1.0148639678955078, Classifier Loss 0.06970859318971634, Total Loss 28.031776428222656\n",
      "4: Encoding Loss 4.007941722869873, Transition Loss -0.9503610730171204, Classifier Loss 0.12308387458324432, Total Loss 36.35565948486328\n",
      "4: Encoding Loss 3.8134353160858154, Transition Loss -1.6568642854690552, Classifier Loss 0.10178428888320923, Total Loss 33.058380126953125\n",
      "4: Encoding Loss 4.93437385559082, Transition Loss -0.7803161144256592, Classifier Loss 0.07741937041282654, Total Loss 37.347869873046875\n",
      "4: Encoding Loss 4.992588996887207, Transition Loss -1.066514492034912, Classifier Loss 0.13768933713436127, Total Loss 43.72404098510742\n",
      "4: Encoding Loss 3.8905842304229736, Transition Loss -2.1162502765655518, Classifier Loss 0.09991210699081421, Total Loss 33.33386993408203\n",
      "4: Encoding Loss 5.085715293884277, Transition Loss -1.2905306816101074, Classifier Loss 0.15884892642498016, Total Loss 46.3986701965332\n",
      "4: Encoding Loss 3.9818034172058105, Transition Loss -0.27608630061149597, Classifier Loss 0.06983586400747299, Total Loss 30.874298095703125\n",
      "4: Encoding Loss 3.320465087890625, Transition Loss -0.4486132264137268, Classifier Loss 0.08241541683673859, Total Loss 28.164154052734375\n",
      "4: Encoding Loss 4.850879669189453, Transition Loss -0.617499828338623, Classifier Loss 0.22770294547080994, Total Loss 51.87532424926758\n",
      "4: Encoding Loss 5.991662502288818, Transition Loss -1.5976824760437012, Classifier Loss 0.20500345528125763, Total Loss 56.449684143066406\n",
      "4: Encoding Loss 4.352532386779785, Transition Loss -0.7512770891189575, Classifier Loss 0.14818553626537323, Total Loss 40.93344497680664\n",
      "4: Encoding Loss 6.227146148681641, Transition Loss -1.6337671279907227, Classifier Loss 0.16014154255390167, Total Loss 53.37637710571289\n",
      "4: Encoding Loss 4.213515281677246, Transition Loss -1.981263279914856, Classifier Loss 0.09945680946111679, Total Loss 35.225982666015625\n",
      "4: Encoding Loss 6.206506252288818, Transition Loss -1.6491981744766235, Classifier Loss 0.14284490048885345, Total Loss 51.52286911010742\n",
      "4: Encoding Loss 5.373843193054199, Transition Loss -1.0476446151733398, Classifier Loss 0.06769857555627823, Total Loss 39.01249694824219\n",
      "4: Encoding Loss 3.200237989425659, Transition Loss -1.2064279317855835, Classifier Loss 0.12169022113084793, Total Loss 31.369970321655273\n",
      "4: Encoding Loss 3.8509624004364014, Transition Loss -2.125889778137207, Classifier Loss 0.08093350380659103, Total Loss 31.19827651977539\n",
      "4: Encoding Loss 3.600280284881592, Transition Loss -0.3703880310058594, Classifier Loss 0.17411674559116364, Total Loss 39.01321029663086\n",
      "4: Encoding Loss 7.14618444442749, Transition Loss -0.8490031361579895, Classifier Loss 0.08351888507604599, Total Loss 51.22865676879883\n",
      "4: Encoding Loss 5.751864433288574, Transition Loss -1.2671120166778564, Classifier Loss 0.2552744150161743, Total Loss 60.03812026977539\n",
      "4: Encoding Loss 6.640055179595947, Transition Loss -0.24491672217845917, Classifier Loss 0.15857428312301636, Total Loss 55.697662353515625\n",
      "4: Encoding Loss 5.512809753417969, Transition Loss -2.095693588256836, Classifier Loss 0.16705726087093353, Total Loss 49.78174591064453\n",
      "4: Encoding Loss 4.961850166320801, Transition Loss -0.9523330926895142, Classifier Loss 0.08094475418329239, Total Loss 37.865196228027344\n",
      "4: Encoding Loss 3.159681797027588, Transition Loss -2.1768381595611572, Classifier Loss 0.08856048434972763, Total Loss 27.813268661499023\n",
      "4: Encoding Loss 2.7820327281951904, Transition Loss -1.1903949975967407, Classifier Loss 0.08859170973300934, Total Loss 25.550891876220703\n",
      "4: Encoding Loss 3.8646926879882812, Transition Loss -2.3488476276397705, Classifier Loss 0.09738238155841827, Total Loss 32.92545700073242\n",
      "4: Encoding Loss 6.485400199890137, Transition Loss -0.029284581542015076, Classifier Loss 0.07451117783784866, Total Loss 46.36351013183594\n",
      "4: Encoding Loss 5.5492658615112305, Transition Loss -0.3433600068092346, Classifier Loss 0.12367165088653564, Total Loss 45.66262435913086\n",
      "4: Encoding Loss 6.735997200012207, Transition Loss -2.94718599319458, Classifier Loss 0.09499771893024445, Total Loss 49.91457748413086\n",
      "4: Encoding Loss 5.5174241065979, Transition Loss -1.780003309249878, Classifier Loss 0.11562507599592209, Total Loss 44.66633987426758\n",
      "4: Encoding Loss 4.585827827453613, Transition Loss -0.1812605857849121, Classifier Loss 0.12367186695337296, Total Loss 39.882080078125\n",
      "4: Encoding Loss 4.867603302001953, Transition Loss -2.151446580886841, Classifier Loss 0.06681223958730698, Total Loss 35.885982513427734\n",
      "4: Encoding Loss 7.280392646789551, Transition Loss -1.0859899520874023, Classifier Loss 0.14314021170139313, Total Loss 57.99594497680664\n",
      "4: Encoding Loss 6.1894097328186035, Transition Loss -2.4470481872558594, Classifier Loss 0.18621470034122467, Total Loss 55.7569465637207\n",
      "4: Encoding Loss 5.960478782653809, Transition Loss -1.6319729089736938, Classifier Loss 0.11595755070447922, Total Loss 47.357975006103516\n",
      "4: Encoding Loss 6.349061012268066, Transition Loss -0.6440668106079102, Classifier Loss 0.18472032248973846, Total Loss 56.566139221191406\n",
      "4: Encoding Loss 6.220003128051758, Transition Loss -1.6966060400009155, Classifier Loss 0.053420357406139374, Total Loss 42.661376953125\n",
      "4: Encoding Loss 5.453404426574707, Transition Loss -1.526678442955017, Classifier Loss 0.1956586390733719, Total Loss 52.285682678222656\n",
      "4: Encoding Loss 4.044657230377197, Transition Loss -0.6663626432418823, Classifier Loss 0.09487079828977585, Total Loss 33.754756927490234\n",
      "4: Encoding Loss 6.756115913391113, Transition Loss -1.9595073461532593, Classifier Loss 0.11666582524776459, Total Loss 52.20249557495117\n",
      "4: Encoding Loss 5.836377143859863, Transition Loss -0.7282495498657227, Classifier Loss 0.12474644184112549, Total Loss 47.492618560791016\n",
      "4: Encoding Loss 5.088385581970215, Transition Loss -0.6800084710121155, Classifier Loss 0.0982174426317215, Total Loss 40.35178756713867\n",
      "4: Encoding Loss 4.495047569274902, Transition Loss -1.1485590934753418, Classifier Loss 0.13911393284797668, Total Loss 40.881221771240234\n",
      "4: Encoding Loss 5.028649806976318, Transition Loss -0.9082326292991638, Classifier Loss 0.1524682492017746, Total Loss 45.41836166381836\n",
      "4: Encoding Loss 5.654098033905029, Transition Loss -0.5737886428833008, Classifier Loss 0.14431144297122955, Total Loss 48.355506896972656\n",
      "4: Encoding Loss 5.1908650398254395, Transition Loss -2.689058780670166, Classifier Loss 0.07694496959447861, Total Loss 38.8386116027832\n",
      "4: Encoding Loss 2.795100688934326, Transition Loss -1.499247670173645, Classifier Loss 0.11429022252559662, Total Loss 28.19902801513672\n",
      "4: Encoding Loss 9.805302619934082, Transition Loss -2.0101170539855957, Classifier Loss 0.1712953746318817, Total Loss 75.96055603027344\n",
      "4: Encoding Loss 7.663869380950928, Transition Loss -0.18038606643676758, Classifier Loss 0.060280416160821915, Total Loss 52.01118850708008\n",
      "4: Encoding Loss 5.857585430145264, Transition Loss -0.7929193377494812, Classifier Loss 0.18345478177070618, Total Loss 53.49067687988281\n",
      "4: Encoding Loss 5.130756378173828, Transition Loss -1.032058835029602, Classifier Loss 0.16514180600643158, Total Loss 47.298309326171875\n",
      "4: Encoding Loss 6.157594203948975, Transition Loss -0.8568999767303467, Classifier Loss 0.08434600383043289, Total Loss 45.379825592041016\n",
      "4: Encoding Loss 6.977374076843262, Transition Loss -1.4272894859313965, Classifier Loss 0.265312135219574, Total Loss 68.39488983154297\n",
      "4: Encoding Loss 5.129245281219482, Transition Loss -0.42390620708465576, Classifier Loss 0.10965792834758759, Total Loss 41.74109649658203\n",
      "4: Encoding Loss 5.049539089202881, Transition Loss 0.3124268352985382, Classifier Loss 0.12998950481414795, Total Loss 43.42115783691406\n",
      "4: Encoding Loss 5.265993118286133, Transition Loss -0.6437622308731079, Classifier Loss 0.10687460005283356, Total Loss 42.28316116333008\n",
      "4: Encoding Loss 3.9978370666503906, Transition Loss 0.13098914921283722, Classifier Loss 0.04269567131996155, Total Loss 28.308984756469727\n",
      "4: Encoding Loss 6.868261814117432, Transition Loss -1.1578090190887451, Classifier Loss 0.10573994368314743, Total Loss 51.783103942871094\n",
      "4: Encoding Loss 7.004344940185547, Transition Loss -1.610271692276001, Classifier Loss 0.19491803646087646, Total Loss 61.51723098754883\n",
      "4: Encoding Loss 4.7169904708862305, Transition Loss -1.0323249101638794, Classifier Loss 0.10195916891098022, Total Loss 38.49745178222656\n",
      "4: Encoding Loss 3.7480645179748535, Transition Loss -1.7573115825653076, Classifier Loss 0.09973295032978058, Total Loss 32.46098327636719\n",
      "4: Encoding Loss 4.077000617980957, Transition Loss -1.1824266910552979, Classifier Loss 0.09437737613916397, Total Loss 33.899269104003906\n",
      "4: Encoding Loss 4.89401388168335, Transition Loss -0.4639202952384949, Classifier Loss 0.06339327245950699, Total Loss 35.703224182128906\n",
      "4: Encoding Loss 5.57814359664917, Transition Loss -1.0170577764511108, Classifier Loss 0.17512831091880798, Total Loss 50.98128890991211\n",
      "4: Encoding Loss 3.5298359394073486, Transition Loss -0.44716379046440125, Classifier Loss 0.15460015833377838, Total Loss 36.638851165771484\n",
      "4: Encoding Loss 5.728834629058838, Transition Loss -1.9956581592559814, Classifier Loss 0.100082166492939, Total Loss 44.380428314208984\n",
      "4: Encoding Loss 3.7697665691375732, Transition Loss -0.9545137882232666, Classifier Loss 0.06294122338294983, Total Loss 28.912342071533203\n",
      "4: Encoding Loss 4.816924095153809, Transition Loss -2.4779891967773438, Classifier Loss 0.06315559893846512, Total Loss 35.21611404418945\n",
      "4: Encoding Loss 2.4641947746276855, Transition Loss -1.4269720315933228, Classifier Loss 0.05297224223613739, Total Loss 20.081823348999023\n",
      "4: Encoding Loss 6.677783966064453, Transition Loss -1.5145909786224365, Classifier Loss 0.15294402837753296, Total Loss 55.36050033569336\n",
      "4: Encoding Loss 4.566546440124512, Transition Loss -1.6478419303894043, Classifier Loss 0.10785362124443054, Total Loss 38.183982849121094\n",
      "4: Encoding Loss 4.1105427742004395, Transition Loss -0.6448919773101807, Classifier Loss 0.08700042963027954, Total Loss 33.363040924072266\n",
      "4: Encoding Loss 5.599154472351074, Transition Loss 0.16868890821933746, Classifier Loss 0.1273212432861328, Total Loss 46.394527435302734\n",
      "4: Encoding Loss 6.270886421203613, Transition Loss -1.6448768377304077, Classifier Loss 0.15808464586734772, Total Loss 53.433128356933594\n",
      "4: Encoding Loss 6.460217475891113, Transition Loss -3.1848013401031494, Classifier Loss 0.1571526676416397, Total Loss 54.47529983520508\n",
      "4: Encoding Loss 4.69558572769165, Transition Loss -1.5498976707458496, Classifier Loss 0.10859857499599457, Total Loss 39.03274917602539\n",
      "4: Encoding Loss 5.5413641929626465, Transition Loss -2.1256866455078125, Classifier Loss 0.06483392417430878, Total Loss 39.73072814941406\n",
      "4: Encoding Loss 2.7650856971740723, Transition Loss -2.4838802814483643, Classifier Loss 0.06266088038682938, Total Loss 22.855609893798828\n",
      "4: Encoding Loss 5.390374183654785, Transition Loss -2.637023687362671, Classifier Loss 0.20289088785648346, Total Loss 52.630279541015625\n",
      "4: Encoding Loss 5.012923717498779, Transition Loss -0.7495810985565186, Classifier Loss 0.19594606757164001, Total Loss 49.67184829711914\n",
      "4: Encoding Loss 7.378730773925781, Transition Loss -0.4863184094429016, Classifier Loss 0.11337496340274811, Total Loss 55.60968780517578\n",
      "4: Encoding Loss 4.2151055335998535, Transition Loss -1.090006709098816, Classifier Loss 0.07292440533638, Total Loss 32.5826416015625\n",
      "4: Encoding Loss 3.6899044513702393, Transition Loss -1.6456410884857178, Classifier Loss 0.09120972454547882, Total Loss 31.259740829467773\n",
      "4: Encoding Loss 5.732333183288574, Transition Loss -1.4373186826705933, Classifier Loss 0.058411914855241776, Total Loss 40.234615325927734\n",
      "4: Encoding Loss 4.881142616271973, Transition Loss -0.9032397270202637, Classifier Loss 0.07764691114425659, Total Loss 37.051185607910156\n",
      "4: Encoding Loss 4.848755836486816, Transition Loss -1.3757736682891846, Classifier Loss 0.09403569251298904, Total Loss 38.49555587768555\n",
      "4: Encoding Loss 5.10185432434082, Transition Loss -1.4050652980804443, Classifier Loss 0.0768384039402008, Total Loss 38.29440689086914\n",
      "4: Encoding Loss 5.948676586151123, Transition Loss -1.344950795173645, Classifier Loss 0.07222899794578552, Total Loss 42.914424896240234\n",
      "4: Encoding Loss 4.755530834197998, Transition Loss -1.2268253564834595, Classifier Loss 0.1375073939561844, Total Loss 42.28343200683594\n",
      "4: Encoding Loss 4.315552711486816, Transition Loss -0.8757774829864502, Classifier Loss 0.10465557128190994, Total Loss 36.3585205078125\n",
      "4: Encoding Loss 5.549653053283691, Transition Loss 0.18085899949073792, Classifier Loss 0.1258019208908081, Total Loss 45.95045471191406\n",
      "4: Encoding Loss 4.523951530456543, Transition Loss -1.7577943801879883, Classifier Loss 0.08733536303043365, Total Loss 35.87654495239258\n",
      "4: Encoding Loss 5.097550392150879, Transition Loss -1.762137532234192, Classifier Loss 0.09117718786001205, Total Loss 39.70231628417969\n",
      "4: Encoding Loss 4.511638164520264, Transition Loss -0.8003213405609131, Classifier Loss 0.16229979693889618, Total Loss 43.29949188232422\n",
      "4: Encoding Loss 4.067974090576172, Transition Loss -0.28493428230285645, Classifier Loss 0.0631723552942276, Total Loss 30.72496795654297\n",
      "4: Encoding Loss 5.199265003204346, Transition Loss -1.9841980934143066, Classifier Loss 0.08442093431949615, Total Loss 39.63689041137695\n",
      "4: Encoding Loss 4.432708263397217, Transition Loss -0.9353234767913818, Classifier Loss 0.06569403409957886, Total Loss 33.165279388427734\n",
      "4: Encoding Loss 6.368165493011475, Transition Loss -0.14329881966114044, Classifier Loss 0.21140675246715546, Total Loss 59.349613189697266\n",
      "4: Encoding Loss 6.038527965545654, Transition Loss -1.6423656940460205, Classifier Loss 0.0964181199669838, Total Loss 45.8723258972168\n",
      "4: Encoding Loss 6.4110283851623535, Transition Loss -0.6162934899330139, Classifier Loss 0.17109942436218262, Total Loss 55.57586669921875\n",
      "4: Encoding Loss 4.5468831062316895, Transition Loss -1.0594148635864258, Classifier Loss 0.0480671226978302, Total Loss 32.087589263916016\n",
      "4: Encoding Loss 4.891002655029297, Transition Loss -0.9930210709571838, Classifier Loss 0.14781635999679565, Total Loss 44.12725830078125\n",
      "4: Encoding Loss 2.8577466011047363, Transition Loss -1.34592866897583, Classifier Loss 0.12777790427207947, Total Loss 29.92373275756836\n",
      "4: Encoding Loss 6.098441123962402, Transition Loss -0.45538434386253357, Classifier Loss 0.1536126285791397, Total Loss 51.95172882080078\n",
      "4: Encoding Loss 5.603298664093018, Transition Loss -1.323465347290039, Classifier Loss 0.12042984366416931, Total Loss 45.66224670410156\n",
      "4: Encoding Loss 3.9181931018829346, Transition Loss -0.5387907028198242, Classifier Loss 0.1361423134803772, Total Loss 37.12317657470703\n",
      "4: Encoding Loss 5.0533037185668945, Transition Loss -1.162932276725769, Classifier Loss 0.10570251941680908, Total Loss 40.889610290527344\n",
      "4: Encoding Loss 6.30370569229126, Transition Loss -0.5943176746368408, Classifier Loss 0.09178201854228973, Total Loss 47.00019836425781\n",
      "4: Encoding Loss 3.9527854919433594, Transition Loss 0.05253557860851288, Classifier Loss 0.08860847353935242, Total Loss 32.598575592041016\n",
      "4: Encoding Loss 6.37774133682251, Transition Loss -0.1964743435382843, Classifier Loss 0.14520424604415894, Total Loss 52.78679275512695\n",
      "4: Encoding Loss 5.6400299072265625, Transition Loss -1.1457468271255493, Classifier Loss 0.16389459371566772, Total Loss 50.22917938232422\n",
      "4: Encoding Loss 4.457863807678223, Transition Loss -1.3307000398635864, Classifier Loss 0.09425631910562515, Total Loss 36.172279357910156\n",
      "4: Encoding Loss 4.191648960113525, Transition Loss -1.4333809614181519, Classifier Loss 0.169359028339386, Total Loss 42.085227966308594\n",
      "4: Encoding Loss 6.0580573081970215, Transition Loss -0.2129550576210022, Classifier Loss 0.14144864678382874, Total Loss 50.493125915527344\n",
      "4: Encoding Loss 5.2026262283325195, Transition Loss -1.353977918624878, Classifier Loss 0.14948396384716034, Total Loss 46.163612365722656\n",
      "4: Encoding Loss 5.817885398864746, Transition Loss -1.1588654518127441, Classifier Loss 0.17958834767341614, Total Loss 52.865684509277344\n",
      "4: Encoding Loss 5.233440399169922, Transition Loss -1.1652002334594727, Classifier Loss 0.09921624511480331, Total Loss 41.32180404663086\n",
      "4: Encoding Loss 5.870752334594727, Transition Loss -1.4865734577178955, Classifier Loss 0.12511537969112396, Total Loss 47.73545837402344\n",
      "4: Encoding Loss 5.60804557800293, Transition Loss -0.8879276514053345, Classifier Loss 0.1133088544011116, Total Loss 44.97880554199219\n",
      "4: Encoding Loss 5.002982139587402, Transition Loss -1.0984845161437988, Classifier Loss 0.114505335688591, Total Loss 41.46799087524414\n",
      "4: Encoding Loss 4.947343826293945, Transition Loss -0.18956613540649414, Classifier Loss 0.07370289415121078, Total Loss 37.05427932739258\n",
      "4: Encoding Loss 3.8089237213134766, Transition Loss -0.36375367641448975, Classifier Loss 0.0714646726846695, Total Loss 29.99986457824707\n",
      "4: Encoding Loss 7.495567321777344, Transition Loss -1.459798812866211, Classifier Loss 0.18094369769096375, Total Loss 63.06718826293945\n",
      "4: Encoding Loss 7.555458068847656, Transition Loss -1.6987111568450928, Classifier Loss 0.13532741367816925, Total Loss 58.864810943603516\n",
      "4: Encoding Loss 4.261789798736572, Transition Loss -0.8702247142791748, Classifier Loss 0.1065998449921608, Total Loss 36.230377197265625\n",
      "4: Encoding Loss 6.783268928527832, Transition Loss -1.537327766418457, Classifier Loss 0.11484688520431519, Total Loss 52.18368911743164\n",
      "4: Encoding Loss 5.06439733505249, Transition Loss -0.9547728300094604, Classifier Loss 0.09515772014856339, Total Loss 39.90177536010742\n",
      "4: Encoding Loss 4.617103099822998, Transition Loss -0.9977295398712158, Classifier Loss 0.06896143406629562, Total Loss 34.59836196899414\n",
      "4: Encoding Loss 6.654036045074463, Transition Loss -1.796272873878479, Classifier Loss 0.10823425650596619, Total Loss 50.746925354003906\n",
      "4: Encoding Loss 6.072452545166016, Transition Loss -1.0253400802612305, Classifier Loss 0.20563259720802307, Total Loss 56.997562408447266\n",
      "4: Encoding Loss 5.518064975738525, Transition Loss -1.2323625087738037, Classifier Loss 0.10655347257852554, Total Loss 43.76324462890625\n",
      "4: Encoding Loss 4.473182678222656, Transition Loss -0.18273639678955078, Classifier Loss 0.11271768808364868, Total Loss 38.11079406738281\n",
      "4: Encoding Loss 5.53592586517334, Transition Loss -2.168469190597534, Classifier Loss 0.09833225607872009, Total Loss 43.047916412353516\n",
      "4: Encoding Loss 5.101752281188965, Transition Loss -1.7319446802139282, Classifier Loss 0.1017255112528801, Total Loss 40.782371520996094\n",
      "4: Encoding Loss 5.82623815536499, Transition Loss 0.13145285844802856, Classifier Loss 0.1377677470445633, Total Loss 48.78678894042969\n",
      "4: Encoding Loss 7.78560733795166, Transition Loss -1.3780567646026611, Classifier Loss 0.14557434618473053, Total Loss 61.270530700683594\n",
      "4: Encoding Loss 5.349394798278809, Transition Loss -2.0370845794677734, Classifier Loss 0.06810819357633591, Total Loss 38.9063720703125\n",
      "4: Encoding Loss 6.0330915451049805, Transition Loss -1.0338338613510132, Classifier Loss 0.07615022361278534, Total Loss 43.81315994262695\n",
      "4: Encoding Loss 5.493875026702881, Transition Loss -0.7302919626235962, Classifier Loss 0.17194825410842896, Total Loss 50.15778732299805\n",
      "4: Encoding Loss 5.241582870483398, Transition Loss -1.133597731590271, Classifier Loss 0.06133127585053444, Total Loss 37.58217239379883\n",
      "5: Encoding Loss 3.954667329788208, Transition Loss -1.1353806257247925, Classifier Loss 0.10142407566308975, Total Loss 33.869956970214844\n",
      "5: Encoding Loss 6.418112754821777, Transition Loss -0.32342106103897095, Classifier Loss 0.111562080681324, Total Loss 49.664756774902344\n",
      "5: Encoding Loss 5.496707916259766, Transition Loss -1.611257791519165, Classifier Loss 0.10359963774681091, Total Loss 43.33956527709961\n",
      "5: Encoding Loss 6.351731300354004, Transition Loss -1.0925509929656982, Classifier Loss 0.09110099077224731, Total Loss 47.22005081176758\n",
      "5: Encoding Loss 7.285876274108887, Transition Loss 0.19006621837615967, Classifier Loss 0.19913476705551147, Total Loss 63.70476531982422\n",
      "5: Encoding Loss 6.68036413192749, Transition Loss -2.577827215194702, Classifier Loss 0.07619163393974304, Total Loss 47.700321197509766\n",
      "5: Encoding Loss 4.208638668060303, Transition Loss -1.8861446380615234, Classifier Loss 0.14931824803352356, Total Loss 40.18290328979492\n",
      "5: Encoding Loss 4.087920188903809, Transition Loss -1.5044655799865723, Classifier Loss 0.14060667157173157, Total Loss 38.58758544921875\n",
      "5: Encoding Loss 5.491011142730713, Transition Loss -1.152391791343689, Classifier Loss 0.16391630470752716, Total Loss 49.33723831176758\n",
      "5: Encoding Loss 3.736402988433838, Transition Loss -0.7278125286102295, Classifier Loss 0.10137380659580231, Total Loss 32.555511474609375\n",
      "5: Encoding Loss 6.036070823669434, Transition Loss -1.300685167312622, Classifier Loss 0.17163124680519104, Total Loss 53.379032135009766\n",
      "5: Encoding Loss 3.991924285888672, Transition Loss -1.2141627073287964, Classifier Loss 0.0903591737151146, Total Loss 32.98698043823242\n",
      "5: Encoding Loss 2.660208225250244, Transition Loss -0.57073974609375, Classifier Loss 0.06212335452437401, Total Loss 22.173355102539062\n",
      "5: Encoding Loss 4.91661262512207, Transition Loss -2.0659594535827637, Classifier Loss 0.08738096803426743, Total Loss 38.23694610595703\n",
      "5: Encoding Loss 3.6084609031677246, Transition Loss -1.9223649501800537, Classifier Loss 0.10563322901725769, Total Loss 32.21331787109375\n",
      "5: Encoding Loss 4.6595377922058105, Transition Loss -1.319161295890808, Classifier Loss 0.10025042295455933, Total Loss 37.98174285888672\n",
      "5: Encoding Loss 4.137454032897949, Transition Loss -1.690442442893982, Classifier Loss 0.09095124155282974, Total Loss 33.91917419433594\n",
      "5: Encoding Loss 3.0918076038360596, Transition Loss 0.2322257161140442, Classifier Loss 0.09380902349948883, Total Loss 28.02463722229004\n",
      "5: Encoding Loss 7.3807477951049805, Transition Loss -2.3717150688171387, Classifier Loss 0.12619540095329285, Total Loss 56.903079986572266\n",
      "5: Encoding Loss 4.9620866775512695, Transition Loss -0.7549746036529541, Classifier Loss 0.20631681382656097, Total Loss 50.403900146484375\n",
      "5: Encoding Loss 6.7200212478637695, Transition Loss -2.307408332824707, Classifier Loss 0.23247390985488892, Total Loss 63.56659698486328\n",
      "5: Encoding Loss 5.549214839935303, Transition Loss -1.1841576099395752, Classifier Loss 0.13351193070411682, Total Loss 46.64601135253906\n",
      "5: Encoding Loss 6.548567295074463, Transition Loss -0.5716678500175476, Classifier Loss 0.11718638241291046, Total Loss 51.00981521606445\n",
      "5: Encoding Loss 6.393242359161377, Transition Loss -0.2435072660446167, Classifier Loss 0.11556409299373627, Total Loss 49.91576385498047\n",
      "5: Encoding Loss 5.533543586730957, Transition Loss 0.28390419483184814, Classifier Loss 0.07791443914175034, Total Loss 41.10626983642578\n",
      "5: Encoding Loss 5.457751750946045, Transition Loss -1.8206579685211182, Classifier Loss 0.0974467471241951, Total Loss 42.49045944213867\n",
      "5: Encoding Loss 3.5529141426086426, Transition Loss -0.40527254343032837, Classifier Loss 0.09111087769269943, Total Loss 30.42841148376465\n",
      "5: Encoding Loss 8.24455451965332, Transition Loss -1.4690479040145874, Classifier Loss 0.17188116908073425, Total Loss 66.65485382080078\n",
      "5: Encoding Loss 4.733382701873779, Transition Loss -2.155468463897705, Classifier Loss 0.0885237604379654, Total Loss 37.25181198120117\n",
      "5: Encoding Loss 3.95344877243042, Transition Loss -1.3664969205856323, Classifier Loss 0.16617834568023682, Total Loss 40.337982177734375\n",
      "5: Encoding Loss 4.716418266296387, Transition Loss -1.7091617584228516, Classifier Loss 0.10305778682231903, Total Loss 38.603607177734375\n",
      "5: Encoding Loss 4.851205348968506, Transition Loss 0.5207874774932861, Classifier Loss 0.07923218607902527, Total Loss 37.23876953125\n",
      "5: Encoding Loss 6.212069034576416, Transition Loss -1.0664302110671997, Classifier Loss 0.1723150908946991, Total Loss 54.50349426269531\n",
      "5: Encoding Loss 4.273266792297363, Transition Loss -1.5605380535125732, Classifier Loss 0.09347938746213913, Total Loss 34.986915588378906\n",
      "5: Encoding Loss 4.1817474365234375, Transition Loss -1.2514005899429321, Classifier Loss 0.14861027896404266, Total Loss 39.95101547241211\n",
      "5: Encoding Loss 5.285810947418213, Transition Loss -1.378476619720459, Classifier Loss 0.26080694794654846, Total Loss 57.79500961303711\n",
      "5: Encoding Loss 5.1357645988464355, Transition Loss -1.3036139011383057, Classifier Loss 0.1027713418006897, Total Loss 41.0911979675293\n",
      "5: Encoding Loss 4.756411552429199, Transition Loss -0.33908364176750183, Classifier Loss 0.08993133902549744, Total Loss 37.53146743774414\n",
      "5: Encoding Loss 8.69720458984375, Transition Loss 0.06938484311103821, Classifier Loss 0.2021721601486206, Total Loss 72.42820739746094\n",
      "5: Encoding Loss 7.688478946685791, Transition Loss -1.6553853750228882, Classifier Loss 0.1261584609746933, Total Loss 58.746055603027344\n",
      "5: Encoding Loss 5.425337314605713, Transition Loss -1.131492018699646, Classifier Loss 0.14122425019741058, Total Loss 46.67399597167969\n",
      "5: Encoding Loss 6.261597633361816, Transition Loss -1.1591973304748535, Classifier Loss 0.13895460963249207, Total Loss 51.46458435058594\n",
      "5: Encoding Loss 5.706104278564453, Transition Loss -1.150862216949463, Classifier Loss 0.15047115087509155, Total Loss 49.28327941894531\n",
      "5: Encoding Loss 8.914525032043457, Transition Loss -2.2632832527160645, Classifier Loss 0.09626734256744385, Total Loss 63.112979888916016\n",
      "5: Encoding Loss 5.782865047454834, Transition Loss -1.659045696258545, Classifier Loss 0.12273765355348587, Total Loss 46.97029495239258\n",
      "5: Encoding Loss 3.4977848529815674, Transition Loss 0.10625576972961426, Classifier Loss 0.08017627894878387, Total Loss 29.046838760375977\n",
      "5: Encoding Loss 4.281479835510254, Transition Loss -0.9791784286499023, Classifier Loss 0.1231837049126625, Total Loss 38.00685501098633\n",
      "5: Encoding Loss 6.878337860107422, Transition Loss -0.02332766354084015, Classifier Loss 0.11477944999933243, Total Loss 52.747962951660156\n",
      "5: Encoding Loss 7.193039894104004, Transition Loss -0.37230581045150757, Classifier Loss 0.2218238264322281, Total Loss 65.34046936035156\n",
      "5: Encoding Loss 4.353482723236084, Transition Loss -0.34919512271881104, Classifier Loss 0.08692584931850433, Total Loss 34.8133430480957\n",
      "5: Encoding Loss 4.8290300369262695, Transition Loss -1.4922046661376953, Classifier Loss 0.15023228526115417, Total Loss 43.9968147277832\n",
      "5: Encoding Loss 3.5918736457824707, Transition Loss -1.700168490409851, Classifier Loss 0.0567847341299057, Total Loss 27.229036331176758\n",
      "5: Encoding Loss 8.065840721130371, Transition Loss -1.099603295326233, Classifier Loss 0.10150104761123657, Total Loss 58.54471206665039\n",
      "5: Encoding Loss 5.84346342086792, Transition Loss 0.05421343445777893, Classifier Loss 0.1706053912639618, Total Loss 52.143009185791016\n",
      "5: Encoding Loss 3.179199457168579, Transition Loss -1.7075468301773071, Classifier Loss 0.12920424342155457, Total Loss 31.994937896728516\n",
      "5: Encoding Loss 7.683287143707275, Transition Loss -0.28597578406333923, Classifier Loss 0.08337279409170151, Total Loss 54.4368896484375\n",
      "5: Encoding Loss 6.91464900970459, Transition Loss -0.593660831451416, Classifier Loss 0.17634059488773346, Total Loss 59.12171936035156\n",
      "5: Encoding Loss 4.229806900024414, Transition Loss -1.066246509552002, Classifier Loss 0.07534727454185486, Total Loss 32.913143157958984\n",
      "5: Encoding Loss 10.81609058380127, Transition Loss -1.9500229358673096, Classifier Loss 0.20080217719078064, Total Loss 84.97598266601562\n",
      "5: Encoding Loss 8.425585746765137, Transition Loss -1.5323735475540161, Classifier Loss 0.06260517984628677, Total Loss 56.813419342041016\n",
      "5: Encoding Loss 6.591507911682129, Transition Loss -0.8314645886421204, Classifier Loss 0.10693788528442383, Total Loss 50.24250411987305\n",
      "5: Encoding Loss 4.834122657775879, Transition Loss -2.26379656791687, Classifier Loss 0.10173320025205612, Total Loss 39.17715072631836\n",
      "5: Encoding Loss 3.9123311042785645, Transition Loss -2.184504985809326, Classifier Loss 0.14031529426574707, Total Loss 37.504642486572266\n",
      "5: Encoding Loss 5.096749305725098, Transition Loss -3.76404070854187, Classifier Loss 0.12284020334482193, Total Loss 42.86301040649414\n",
      "5: Encoding Loss 5.013148307800293, Transition Loss -0.8316985368728638, Classifier Loss 0.13531085848808289, Total Loss 43.60964584350586\n",
      "5: Encoding Loss 5.680622100830078, Transition Loss -1.9134644269943237, Classifier Loss 0.21823135018348694, Total Loss 55.90610122680664\n",
      "5: Encoding Loss 3.5034828186035156, Transition Loss -1.1763074398040771, Classifier Loss 0.21088221669197083, Total Loss 42.10865020751953\n",
      "5: Encoding Loss 5.998354911804199, Transition Loss -1.5150188207626343, Classifier Loss 0.10591165721416473, Total Loss 46.580692291259766\n",
      "5: Encoding Loss 4.51528787612915, Transition Loss -0.13984932005405426, Classifier Loss 0.11241552978754044, Total Loss 38.33322525024414\n",
      "5: Encoding Loss 2.9761974811553955, Transition Loss -0.10355260968208313, Classifier Loss 0.114974744617939, Total Loss 29.354618072509766\n",
      "5: Encoding Loss 5.168665409088135, Transition Loss -1.9667686223983765, Classifier Loss 0.1476103961467743, Total Loss 45.772247314453125\n",
      "5: Encoding Loss 3.114471912384033, Transition Loss -1.435036301612854, Classifier Loss 0.1273161768913269, Total Loss 31.417875289916992\n",
      "5: Encoding Loss 2.625197172164917, Transition Loss -2.0441043376922607, Classifier Loss 0.11076227575540543, Total Loss 26.82659339904785\n",
      "5: Encoding Loss 7.247354984283447, Transition Loss -0.35699427127838135, Classifier Loss 0.05925329029560089, Total Loss 49.40932083129883\n",
      "5: Encoding Loss 4.940323829650879, Transition Loss -1.4571895599365234, Classifier Loss 0.07759227603673935, Total Loss 37.40058517456055\n",
      "5: Encoding Loss 5.456789493560791, Transition Loss -1.6618472337722778, Classifier Loss 0.15092432498931885, Total Loss 47.83250427246094\n",
      "5: Encoding Loss 4.248927116394043, Transition Loss -1.8370113372802734, Classifier Loss 0.05643533915281296, Total Loss 31.136363983154297\n",
      "5: Encoding Loss 6.764091491699219, Transition Loss -1.5183579921722412, Classifier Loss 0.11217454075813293, Total Loss 51.801395416259766\n",
      "5: Encoding Loss 5.029228687286377, Transition Loss -1.478729486465454, Classifier Loss 0.16743436455726624, Total Loss 46.918216705322266\n",
      "5: Encoding Loss 3.071530818939209, Transition Loss -1.3869493007659912, Classifier Loss 0.08690960705280304, Total Loss 27.119592666625977\n",
      "5: Encoding Loss 8.226244926452637, Transition Loss -0.8801779747009277, Classifier Loss 0.08512534946203232, Total Loss 57.86965560913086\n",
      "5: Encoding Loss 6.382882118225098, Transition Loss -1.063279628753662, Classifier Loss 0.17860348522663116, Total Loss 56.15721893310547\n",
      "5: Encoding Loss 4.092530250549316, Transition Loss -0.2712482213973999, Classifier Loss 0.17818982899188995, Total Loss 42.37405776977539\n",
      "5: Encoding Loss 4.893639087677002, Transition Loss -1.0869077444076538, Classifier Loss 0.1368168294429779, Total Loss 43.04308319091797\n",
      "5: Encoding Loss 5.517961025238037, Transition Loss -1.2490437030792236, Classifier Loss 0.14011161029338837, Total Loss 47.118431091308594\n",
      "5: Encoding Loss 4.933191299438477, Transition Loss -2.30177640914917, Classifier Loss 0.12795507907867432, Total Loss 42.39373779296875\n",
      "5: Encoding Loss 4.585093975067139, Transition Loss 0.7789473533630371, Classifier Loss 0.08319231867790222, Total Loss 36.14137649536133\n",
      "5: Encoding Loss 5.708779335021973, Transition Loss -1.5206859111785889, Classifier Loss 0.176932230591774, Total Loss 51.94529342651367\n",
      "5: Encoding Loss 5.338566303253174, Transition Loss -2.26674485206604, Classifier Loss 0.05477645620703697, Total Loss 37.50813674926758\n",
      "5: Encoding Loss 4.774933815002441, Transition Loss -1.633623719215393, Classifier Loss 0.14997446537017822, Total Loss 43.64639663696289\n",
      "5: Encoding Loss 3.9471237659454346, Transition Loss 0.7675919532775879, Classifier Loss 0.12423726916313171, Total Loss 36.41350555419922\n",
      "5: Encoding Loss 2.896402597427368, Transition Loss -1.2531533241271973, Classifier Loss 0.08304852992296219, Total Loss 25.682767868041992\n",
      "5: Encoding Loss 6.053282737731934, Transition Loss -1.7753219604492188, Classifier Loss 0.09567426890134811, Total Loss 45.88641357421875\n",
      "5: Encoding Loss 5.15302848815918, Transition Loss 0.8094125986099243, Classifier Loss 0.09383044391870499, Total Loss 40.62498092651367\n",
      "5: Encoding Loss 3.6873931884765625, Transition Loss -1.702345848083496, Classifier Loss 0.07116526365280151, Total Loss 29.240205764770508\n",
      "5: Encoding Loss 2.8595221042633057, Transition Loss -1.277353048324585, Classifier Loss 0.12224062532186508, Total Loss 29.38068389892578\n",
      "5: Encoding Loss 3.476945400238037, Transition Loss 0.14851999282836914, Classifier Loss 0.07774648815393448, Total Loss 28.695730209350586\n",
      "5: Encoding Loss 10.519845962524414, Transition Loss 0.09755873680114746, Classifier Loss 0.2553786337375641, Total Loss 88.69596099853516\n",
      "5: Encoding Loss 8.112455368041992, Transition Loss -1.2146644592285156, Classifier Loss 0.11782892048358917, Total Loss 60.4571418762207\n",
      "5: Encoding Loss 8.621659278869629, Transition Loss -0.9856384992599487, Classifier Loss 0.16426673531532288, Total Loss 68.15623474121094\n",
      "5: Encoding Loss 6.464687347412109, Transition Loss -1.5622644424438477, Classifier Loss 0.07703733444213867, Total Loss 46.491233825683594\n",
      "5: Encoding Loss 5.221771240234375, Transition Loss -1.789259672164917, Classifier Loss 0.07159074395895004, Total Loss 38.48898696899414\n",
      "5: Encoding Loss 4.809353828430176, Transition Loss -0.4765513241291046, Classifier Loss 0.14667539298534393, Total Loss 43.52347183227539\n",
      "5: Encoding Loss 4.647132873535156, Transition Loss -1.9189223051071167, Classifier Loss 0.1150248721241951, Total Loss 39.384517669677734\n",
      "5: Encoding Loss 3.046896457672119, Transition Loss -0.23413950204849243, Classifier Loss 0.1500178724527359, Total Loss 33.2830696105957\n",
      "5: Encoding Loss 3.0310733318328857, Transition Loss 0.6238450407981873, Classifier Loss 0.0738547146320343, Total Loss 25.82145118713379\n",
      "5: Encoding Loss 6.5373969078063965, Transition Loss -0.037397727370262146, Classifier Loss 0.1056399941444397, Total Loss 49.788368225097656\n",
      "5: Encoding Loss 4.616971015930176, Transition Loss -2.169912815093994, Classifier Loss 0.05864162743091583, Total Loss 33.565120697021484\n",
      "5: Encoding Loss 9.500065803527832, Transition Loss -2.044891595840454, Classifier Loss 0.13186171650886536, Total Loss 70.18575286865234\n",
      "5: Encoding Loss 7.211338043212891, Transition Loss -0.9879022836685181, Classifier Loss 0.16852976381778717, Total Loss 60.12060546875\n",
      "5: Encoding Loss 4.716290473937988, Transition Loss -1.5512616634368896, Classifier Loss 0.08042602241039276, Total Loss 36.339725494384766\n",
      "5: Encoding Loss 6.54610538482666, Transition Loss -0.3926543891429901, Classifier Loss 0.11891096830368042, Total Loss 51.16757583618164\n",
      "5: Encoding Loss 6.605212211608887, Transition Loss -0.9708657264709473, Classifier Loss 0.08575524389743805, Total Loss 48.2064094543457\n",
      "5: Encoding Loss 6.008603572845459, Transition Loss -1.0517122745513916, Classifier Loss 0.06330519914627075, Total Loss 42.3817253112793\n",
      "5: Encoding Loss 7.0210442543029785, Transition Loss -1.2961502075195312, Classifier Loss 0.08786005526781082, Total Loss 50.9117546081543\n",
      "5: Encoding Loss 4.369810104370117, Transition Loss -2.4135239124298096, Classifier Loss 0.10322879254817963, Total Loss 36.540775299072266\n",
      "5: Encoding Loss 4.121757984161377, Transition Loss -0.13505107164382935, Classifier Loss 0.10341502726078033, Total Loss 35.071998596191406\n",
      "5: Encoding Loss 2.9639925956726074, Transition Loss -0.955268919467926, Classifier Loss 0.08263710141181946, Total Loss 26.047285079956055\n",
      "5: Encoding Loss 4.530544281005859, Transition Loss -1.2054059505462646, Classifier Loss 0.11221353709697723, Total Loss 38.40414047241211\n",
      "5: Encoding Loss 6.030277252197266, Transition Loss -0.131447434425354, Classifier Loss 0.12046612054109573, Total Loss 48.22822189331055\n",
      "5: Encoding Loss 6.523155212402344, Transition Loss -0.819425642490387, Classifier Loss 0.20396271347999573, Total Loss 59.534873962402344\n",
      "5: Encoding Loss 4.996784210205078, Transition Loss -1.6528409719467163, Classifier Loss 0.15364454686641693, Total Loss 45.34450149536133\n",
      "5: Encoding Loss 6.722240447998047, Transition Loss -1.3893396854400635, Classifier Loss 0.10198475420475006, Total Loss 50.5313606262207\n",
      "5: Encoding Loss 4.704182147979736, Transition Loss -1.626725673675537, Classifier Loss 0.08999238163232803, Total Loss 37.22367858886719\n",
      "5: Encoding Loss 2.6090075969696045, Transition Loss -1.2692184448242188, Classifier Loss 0.08945286273956299, Total Loss 24.59882354736328\n",
      "5: Encoding Loss 6.308598518371582, Transition Loss -2.125718116760254, Classifier Loss 0.08065532892942429, Total Loss 45.91627502441406\n",
      "5: Encoding Loss 7.005490303039551, Transition Loss -2.116342067718506, Classifier Loss 0.216827392578125, Total Loss 63.71483612060547\n",
      "5: Encoding Loss 5.3252716064453125, Transition Loss -2.0845658779144287, Classifier Loss 0.057628341019153595, Total Loss 37.71363067626953\n",
      "5: Encoding Loss 2.936983108520508, Transition Loss -1.040252685546875, Classifier Loss 0.07164623588323593, Total Loss 24.78610610961914\n",
      "5: Encoding Loss 2.629436492919922, Transition Loss -3.140017032623291, Classifier Loss 0.06881070137023926, Total Loss 22.65643310546875\n",
      "5: Encoding Loss 1.2504746913909912, Transition Loss -1.1111383438110352, Classifier Loss 0.1484038084745407, Total Loss 22.342784881591797\n",
      "5: Encoding Loss 5.876589775085449, Transition Loss -1.1537859439849854, Classifier Loss 0.113999143242836, Total Loss 46.658992767333984\n",
      "5: Encoding Loss 5.269950866699219, Transition Loss -1.4787166118621826, Classifier Loss 0.11764197051525116, Total Loss 43.3833122253418\n",
      "5: Encoding Loss 4.341050148010254, Transition Loss -1.4191898107528687, Classifier Loss 0.07304412871599197, Total Loss 33.35014724731445\n",
      "5: Encoding Loss 6.056897163391113, Transition Loss -1.2372509241104126, Classifier Loss 0.12964418530464172, Total Loss 49.30530548095703\n",
      "5: Encoding Loss 6.258846759796143, Transition Loss -1.0764163732528687, Classifier Loss 0.20202897489070892, Total Loss 57.755550384521484\n",
      "5: Encoding Loss 4.689593315124512, Transition Loss 0.06818774342536926, Classifier Loss 0.09182970970869064, Total Loss 37.347808837890625\n",
      "5: Encoding Loss 5.726006031036377, Transition Loss 0.11796639114618301, Classifier Loss 0.11678457260131836, Total Loss 46.08168029785156\n",
      "5: Encoding Loss 3.8927371501922607, Transition Loss -1.7136846780776978, Classifier Loss 0.0884731337428093, Total Loss 32.20304870605469\n",
      "5: Encoding Loss 8.264007568359375, Transition Loss -0.0016605108976364136, Classifier Loss 0.07384166866540909, Total Loss 56.96821594238281\n",
      "5: Encoding Loss 6.560657501220703, Transition Loss -1.4266407489776611, Classifier Loss 0.06950186938047409, Total Loss 46.313560485839844\n",
      "5: Encoding Loss 6.439774990081787, Transition Loss -0.5080925822257996, Classifier Loss 0.12478229403495789, Total Loss 51.11668014526367\n",
      "5: Encoding Loss 5.383091449737549, Transition Loss -0.899636447429657, Classifier Loss 0.0642024502158165, Total Loss 38.71843719482422\n",
      "5: Encoding Loss 5.165627479553223, Transition Loss -0.15105147659778595, Classifier Loss 0.10793168097734451, Total Loss 41.78687286376953\n",
      "5: Encoding Loss 3.8637566566467285, Transition Loss -1.1586154699325562, Classifier Loss 0.09398046880960464, Total Loss 32.58012771606445\n",
      "5: Encoding Loss 5.234853267669678, Transition Loss -0.9725526571273804, Classifier Loss 0.17314894497394562, Total Loss 48.72362518310547\n",
      "5: Encoding Loss 2.9501256942749023, Transition Loss -1.5459825992584229, Classifier Loss 0.07276742160320282, Total Loss 24.976879119873047\n",
      "5: Encoding Loss 7.268422603607178, Transition Loss -1.1141932010650635, Classifier Loss 0.3043416440486908, Total Loss 74.04425811767578\n",
      "5: Encoding Loss 6.548291206359863, Transition Loss -0.6922045350074768, Classifier Loss 0.1981000006198883, Total Loss 59.09946823120117\n",
      "5: Encoding Loss 4.452342510223389, Transition Loss -1.5031296014785767, Classifier Loss 0.14291158318519592, Total Loss 41.00461196899414\n",
      "5: Encoding Loss 5.58409309387207, Transition Loss -1.9264039993286133, Classifier Loss 0.1025366485118866, Total Loss 43.75745391845703\n",
      "5: Encoding Loss 6.616418361663818, Transition Loss -0.4381856918334961, Classifier Loss 0.1001288965344429, Total Loss 49.71122741699219\n",
      "5: Encoding Loss 6.898955345153809, Transition Loss -0.8074955344200134, Classifier Loss 0.17477746307849884, Total Loss 58.871158599853516\n",
      "5: Encoding Loss 5.298437595367432, Transition Loss -1.721683382987976, Classifier Loss 0.0852457657456398, Total Loss 40.314510345458984\n",
      "5: Encoding Loss 4.5003132820129395, Transition Loss -3.0328056812286377, Classifier Loss 0.10338185727596283, Total Loss 37.33885192871094\n",
      "5: Encoding Loss 7.393641471862793, Transition Loss -2.220769166946411, Classifier Loss 0.16551648080348969, Total Loss 60.9126091003418\n",
      "5: Encoding Loss 6.123849391937256, Transition Loss -0.8037433624267578, Classifier Loss 0.09193194657564163, Total Loss 45.93597412109375\n",
      "5: Encoding Loss 4.0667829513549805, Transition Loss -2.1600849628448486, Classifier Loss 0.10315998643636703, Total Loss 34.7158317565918\n",
      "5: Encoding Loss 2.6808583736419678, Transition Loss -0.8825973272323608, Classifier Loss 0.09713219106197357, Total Loss 25.798017501831055\n",
      "5: Encoding Loss 7.921393394470215, Transition Loss -1.9716894626617432, Classifier Loss 0.12184630334377289, Total Loss 59.71220397949219\n",
      "5: Encoding Loss 5.298428058624268, Transition Loss -0.021007969975471497, Classifier Loss 0.10321854054927826, Total Loss 42.1124153137207\n",
      "5: Encoding Loss 6.1216139793396, Transition Loss -0.6734994053840637, Classifier Loss 0.0830044373869896, Total Loss 45.02985763549805\n",
      "5: Encoding Loss 4.062697410583496, Transition Loss -0.9073077440261841, Classifier Loss 0.06818534433841705, Total Loss 31.194358825683594\n",
      "5: Encoding Loss 4.12063455581665, Transition Loss -1.4357225894927979, Classifier Loss 0.08901099860668182, Total Loss 33.624332427978516\n",
      "5: Encoding Loss 8.422554969787598, Transition Loss 0.07135060429573059, Classifier Loss 0.16796089708805084, Total Loss 67.3599624633789\n",
      "5: Encoding Loss 6.259800910949707, Transition Loss -1.2788195610046387, Classifier Loss 0.19120223820209503, Total Loss 56.67852020263672\n",
      "5: Encoding Loss 5.603657245635986, Transition Loss -2.834244966506958, Classifier Loss 0.18964645266532898, Total Loss 52.58545684814453\n",
      "5: Encoding Loss 3.8541171550750732, Transition Loss -1.895530104637146, Classifier Loss 0.07737482339143753, Total Loss 30.861427307128906\n",
      "5: Encoding Loss 7.818144798278809, Transition Loss -1.8177871704101562, Classifier Loss 0.1617722511291504, Total Loss 63.08536911010742\n",
      "5: Encoding Loss 6.19417667388916, Transition Loss -0.6997852325439453, Classifier Loss 0.0789872333407402, Total Loss 45.063507080078125\n",
      "5: Encoding Loss 4.952101230621338, Transition Loss -1.3361694812774658, Classifier Loss 0.08954472839832306, Total Loss 38.66654586791992\n",
      "5: Encoding Loss 3.873858690261841, Transition Loss -1.8936386108398438, Classifier Loss 0.05428338423371315, Total Loss 28.670734405517578\n",
      "5: Encoding Loss 5.310908794403076, Transition Loss -1.2716211080551147, Classifier Loss 0.19027432799339294, Total Loss 50.89237976074219\n",
      "5: Encoding Loss 6.376862525939941, Transition Loss -2.7954189777374268, Classifier Loss 0.1354161500930786, Total Loss 51.801673889160156\n",
      "5: Encoding Loss 4.393517971038818, Transition Loss -0.8962001800537109, Classifier Loss 0.10760501772165298, Total Loss 37.121253967285156\n",
      "5: Encoding Loss 6.741970062255859, Transition Loss -0.2566247582435608, Classifier Loss 0.17036078870296478, Total Loss 57.487796783447266\n",
      "5: Encoding Loss 6.806388854980469, Transition Loss -1.4462192058563232, Classifier Loss 0.1468094289302826, Total Loss 55.51869583129883\n",
      "5: Encoding Loss 5.598689079284668, Transition Loss -2.4309587478637695, Classifier Loss 0.10460469126701355, Total Loss 44.051631927490234\n",
      "5: Encoding Loss 6.5066070556640625, Transition Loss -1.1039142608642578, Classifier Loss 0.06729773432016373, Total Loss 45.76897430419922\n",
      "5: Encoding Loss 5.32440185546875, Transition Loss 0.240708589553833, Classifier Loss 0.10653051733970642, Total Loss 42.69574737548828\n",
      "5: Encoding Loss 5.553580284118652, Transition Loss -0.7412300109863281, Classifier Loss 0.14565767347812653, Total Loss 47.88695526123047\n",
      "5: Encoding Loss 7.082240104675293, Transition Loss -0.7811760902404785, Classifier Loss 0.06821578741073608, Total Loss 49.3147087097168\n",
      "5: Encoding Loss 5.244051933288574, Transition Loss -2.026707649230957, Classifier Loss 0.0663958489894867, Total Loss 38.103084564208984\n",
      "5: Encoding Loss 6.937004089355469, Transition Loss -1.472386121749878, Classifier Loss 0.15625591576099396, Total Loss 57.24702835083008\n",
      "5: Encoding Loss 5.411194324493408, Transition Loss -0.9919328689575195, Classifier Loss 0.19863960146903992, Total Loss 52.33073043823242\n",
      "5: Encoding Loss 5.521760940551758, Transition Loss -2.478653907775879, Classifier Loss 0.1313522458076477, Total Loss 46.26479721069336\n",
      "5: Encoding Loss 3.361823320388794, Transition Loss 0.3117634057998657, Classifier Loss 0.11304481327533722, Total Loss 31.600128173828125\n",
      "5: Encoding Loss 3.1287684440612793, Transition Loss -0.5633055567741394, Classifier Loss 0.0968140959739685, Total Loss 28.45379638671875\n",
      "5: Encoding Loss 2.438317060470581, Transition Loss -1.118198275566101, Classifier Loss 0.08230587840080261, Total Loss 22.860042572021484\n",
      "5: Encoding Loss 8.121503829956055, Transition Loss -1.91270112991333, Classifier Loss 0.17894107103347778, Total Loss 66.62237548828125\n",
      "5: Encoding Loss 5.7213053703308105, Transition Loss -1.4514672756195068, Classifier Loss 0.12353312969207764, Total Loss 46.68056869506836\n",
      "5: Encoding Loss 6.609330177307129, Transition Loss -1.5270638465881348, Classifier Loss 0.11980438232421875, Total Loss 51.63581085205078\n",
      "5: Encoding Loss 4.464378356933594, Transition Loss -1.0420629978179932, Classifier Loss 0.14143186807632446, Total Loss 40.92904281616211\n",
      "5: Encoding Loss 4.269312381744385, Transition Loss -1.6691040992736816, Classifier Loss 0.1329086273908615, Total Loss 38.906070709228516\n",
      "5: Encoding Loss 4.216036319732666, Transition Loss -1.6306414604187012, Classifier Loss 0.1547575294971466, Total Loss 40.77132034301758\n",
      "5: Encoding Loss 4.67869758605957, Transition Loss -1.3041967153549194, Classifier Loss 0.1250673234462738, Total Loss 40.57839584350586\n",
      "5: Encoding Loss 6.4782538414001465, Transition Loss -0.926133394241333, Classifier Loss 0.14626553654670715, Total Loss 53.49570846557617\n",
      "5: Encoding Loss 4.962677955627441, Transition Loss -0.6817302703857422, Classifier Loss 0.2485278993844986, Total Loss 54.62858963012695\n",
      "5: Encoding Loss 7.1893110275268555, Transition Loss -1.6716810464859009, Classifier Loss 0.1088746041059494, Total Loss 54.02266311645508\n",
      "5: Encoding Loss 4.828874111175537, Transition Loss -1.8500884771347046, Classifier Loss 0.09283458441495895, Total Loss 38.25596618652344\n",
      "5: Encoding Loss 6.153743743896484, Transition Loss -1.5170135498046875, Classifier Loss 0.09502255171537399, Total Loss 46.424110412597656\n",
      "5: Encoding Loss 4.471539497375488, Transition Loss -0.8760207891464233, Classifier Loss 0.09778140485286713, Total Loss 36.607025146484375\n",
      "5: Encoding Loss 4.462617874145508, Transition Loss -0.9194010496139526, Classifier Loss 0.07179533690214157, Total Loss 33.95487594604492\n",
      "5: Encoding Loss 3.1689162254333496, Transition Loss -0.5145699977874756, Classifier Loss 0.10053171962499619, Total Loss 29.066463470458984\n",
      "5: Encoding Loss 5.87093448638916, Transition Loss -0.9231675863265991, Classifier Loss 0.0938408300280571, Total Loss 44.60932159423828\n",
      "5: Encoding Loss 6.258448123931885, Transition Loss -0.9345676898956299, Classifier Loss 0.17317037284374237, Total Loss 54.86735534667969\n",
      "5: Encoding Loss 3.693255662918091, Transition Loss -1.4984016418457031, Classifier Loss 0.1780070960521698, Total Loss 39.95964431762695\n",
      "5: Encoding Loss 3.9240212440490723, Transition Loss -0.21852615475654602, Classifier Loss 0.11804146319627762, Total Loss 35.34818649291992\n",
      "5: Encoding Loss 7.221907615661621, Transition Loss -1.4230836629867554, Classifier Loss 0.20261046290397644, Total Loss 63.591922760009766\n",
      "5: Encoding Loss 6.158558368682861, Transition Loss -0.4844679832458496, Classifier Loss 0.1001657247543335, Total Loss 46.96773147583008\n",
      "5: Encoding Loss 5.2640156745910645, Transition Loss -0.8809277415275574, Classifier Loss 0.0655955970287323, Total Loss 38.14330291748047\n",
      "5: Encoding Loss 6.9142165184021, Transition Loss -0.49964141845703125, Classifier Loss 0.1331813633441925, Total Loss 54.80323791503906\n",
      "5: Encoding Loss 5.098785400390625, Transition Loss -2.8740346431732178, Classifier Loss 0.14115138351917267, Total Loss 44.706703186035156\n",
      "5: Encoding Loss 3.452385663986206, Transition Loss 0.7798175811767578, Classifier Loss 0.09750667959451675, Total Loss 30.77691078186035\n",
      "5: Encoding Loss 3.4862091541290283, Transition Loss -2.1737220287323, Classifier Loss 0.09481959789991379, Total Loss 30.398345947265625\n",
      "5: Encoding Loss 7.51094913482666, Transition Loss -2.082207679748535, Classifier Loss 0.056435082107782364, Total Loss 50.7083740234375\n",
      "5: Encoding Loss 8.339397430419922, Transition Loss -1.7681320905685425, Classifier Loss 0.09095263481140137, Total Loss 59.13094711303711\n",
      "5: Encoding Loss 8.015976905822754, Transition Loss -1.351320505142212, Classifier Loss 0.07489669322967529, Total Loss 55.584991455078125\n",
      "5: Encoding Loss 5.006701946258545, Transition Loss -0.8870264291763306, Classifier Loss 0.1312265843153, Total Loss 43.16251754760742\n",
      "5: Encoding Loss 5.1673665046691895, Transition Loss -0.7585805654525757, Classifier Loss 0.06474368274211884, Total Loss 37.47826385498047\n",
      "5: Encoding Loss 3.1933560371398926, Transition Loss -0.7745453119277954, Classifier Loss 0.10075639188289642, Total Loss 29.23546600341797\n",
      "5: Encoding Loss 7.434683322906494, Transition Loss -2.164994478225708, Classifier Loss 0.08663661032915115, Total Loss 53.270896911621094\n",
      "5: Encoding Loss 5.516928672790527, Transition Loss -1.321354627609253, Classifier Loss 0.07496929913759232, Total Loss 40.59797286987305\n",
      "5: Encoding Loss 6.072506904602051, Transition Loss -1.5854188203811646, Classifier Loss 0.15473274886608124, Total Loss 51.907684326171875\n",
      "5: Encoding Loss 4.495039939880371, Transition Loss -1.3976082801818848, Classifier Loss 0.04458039253950119, Total Loss 31.42772102355957\n",
      "5: Encoding Loss 5.712163925170898, Transition Loss -0.32344263792037964, Classifier Loss 0.07471946626901627, Total Loss 41.74480056762695\n",
      "5: Encoding Loss 4.913138389587402, Transition Loss -1.3769971132278442, Classifier Loss 0.14437156915664673, Total Loss 43.91543960571289\n",
      "5: Encoding Loss 4.514693737030029, Transition Loss -1.3187156915664673, Classifier Loss 0.08576475083827972, Total Loss 35.66411209106445\n",
      "5: Encoding Loss 3.0682311058044434, Transition Loss -2.1413793563842773, Classifier Loss 0.08067208528518677, Total Loss 26.475740432739258\n",
      "5: Encoding Loss 5.830029010772705, Transition Loss -3.10485577583313, Classifier Loss 0.07910624891519547, Total Loss 42.889556884765625\n",
      "5: Encoding Loss 4.173428058624268, Transition Loss -0.7080485820770264, Classifier Loss 0.16026456654071808, Total Loss 41.066741943359375\n",
      "5: Encoding Loss 4.834656715393066, Transition Loss -0.905720591545105, Classifier Loss 0.18517881631851196, Total Loss 47.52545928955078\n",
      "5: Encoding Loss 5.53577184677124, Transition Loss -0.687268078327179, Classifier Loss 0.11190668493509293, Total Loss 44.405029296875\n",
      "5: Encoding Loss 3.911548614501953, Transition Loss -0.4263509213924408, Classifier Loss 0.06785295903682709, Total Loss 30.254417419433594\n",
      "5: Encoding Loss 8.536516189575195, Transition Loss -0.17724096775054932, Classifier Loss 0.15445706248283386, Total Loss 66.66473388671875\n",
      "5: Encoding Loss 4.905917167663574, Transition Loss -0.979791522026062, Classifier Loss 0.10192645341157913, Total Loss 39.62775802612305\n",
      "5: Encoding Loss 7.509453773498535, Transition Loss -0.3765471577644348, Classifier Loss 0.19950537383556366, Total Loss 65.00711059570312\n",
      "5: Encoding Loss 5.853500843048096, Transition Loss -2.487090587615967, Classifier Loss 0.1875087320804596, Total Loss 53.87088394165039\n",
      "5: Encoding Loss 5.573261737823486, Transition Loss -1.0603227615356445, Classifier Loss 0.12635089457035065, Total Loss 46.07423782348633\n",
      "5: Encoding Loss 5.645462512969971, Transition Loss -1.6948132514953613, Classifier Loss 0.15460045635700226, Total Loss 49.3321418762207\n",
      "5: Encoding Loss 3.2173972129821777, Transition Loss -1.2338303327560425, Classifier Loss 0.09951964765787125, Total Loss 29.2558536529541\n",
      "5: Encoding Loss 3.7255587577819824, Transition Loss -0.48284637928009033, Classifier Loss 0.09519343823194504, Total Loss 31.87250328063965\n",
      "5: Encoding Loss 4.357825756072998, Transition Loss -0.7554900646209717, Classifier Loss 0.12224490940570831, Total Loss 38.37114334106445\n",
      "5: Encoding Loss 4.301474571228027, Transition Loss -2.509896755218506, Classifier Loss 0.08760732412338257, Total Loss 34.56857681274414\n",
      "5: Encoding Loss 5.506058692932129, Transition Loss -1.4805734157562256, Classifier Loss 0.1425241380929947, Total Loss 47.288177490234375\n",
      "5: Encoding Loss 4.9859466552734375, Transition Loss -1.4443392753601074, Classifier Loss 0.10055986046791077, Total Loss 39.971092224121094\n",
      "5: Encoding Loss 3.538156032562256, Transition Loss -0.45919883251190186, Classifier Loss 0.10522106289863586, Total Loss 31.7508602142334\n",
      "5: Encoding Loss 4.188076972961426, Transition Loss -1.6804373264312744, Classifier Loss 0.06588678061962128, Total Loss 31.716468811035156\n",
      "5: Encoding Loss 6.306308269500732, Transition Loss -1.1497759819030762, Classifier Loss 0.14454595744609833, Total Loss 52.291988372802734\n",
      "5: Encoding Loss 5.2010698318481445, Transition Loss 0.027262866497039795, Classifier Loss 0.0662994384765625, Total Loss 37.84727096557617\n",
      "5: Encoding Loss 7.07227087020874, Transition Loss 0.015071600675582886, Classifier Loss 0.10769308358430862, Total Loss 53.208961486816406\n",
      "5: Encoding Loss 6.419034481048584, Transition Loss -1.5614018440246582, Classifier Loss 0.3008347749710083, Total Loss 68.59706115722656\n",
      "5: Encoding Loss 6.118807315826416, Transition Loss -1.6344071626663208, Classifier Loss 0.07075883448123932, Total Loss 43.7880744934082\n",
      "5: Encoding Loss 5.238129615783691, Transition Loss -1.3495509624481201, Classifier Loss 0.1272716373205185, Total Loss 44.155399322509766\n",
      "5: Encoding Loss 4.5100250244140625, Transition Loss -1.174721360206604, Classifier Loss 0.09399840235710144, Total Loss 36.45952224731445\n",
      "5: Encoding Loss 5.934574127197266, Transition Loss -1.9357521533966064, Classifier Loss 0.2212524265050888, Total Loss 57.73191452026367\n",
      "5: Encoding Loss 4.3687849044799805, Transition Loss -0.9853437542915344, Classifier Loss 0.08862438797950745, Total Loss 35.07475662231445\n",
      "5: Encoding Loss 6.8372979164123535, Transition Loss -1.9763609170913696, Classifier Loss 0.12558862566947937, Total Loss 53.58185958862305\n",
      "5: Encoding Loss 6.031632423400879, Transition Loss -1.348418951034546, Classifier Loss 0.14317655563354492, Total Loss 50.50691604614258\n",
      "5: Encoding Loss 5.84725284576416, Transition Loss -1.6974753141403198, Classifier Loss 0.06909899413585663, Total Loss 41.992740631103516\n",
      "5: Encoding Loss 7.0650506019592285, Transition Loss -0.854163408279419, Classifier Loss 0.1578972041606903, Total Loss 58.17967987060547\n",
      "5: Encoding Loss 2.7470061779022217, Transition Loss -1.8153200149536133, Classifier Loss 0.0867481604218483, Total Loss 25.1561279296875\n",
      "5: Encoding Loss 2.680723190307617, Transition Loss -0.6633284687995911, Classifier Loss 0.07291988283395767, Total Loss 23.376062393188477\n",
      "5: Encoding Loss 6.41409158706665, Transition Loss -0.4638059735298157, Classifier Loss 0.11300324648618698, Total Loss 49.78468704223633\n",
      "5: Encoding Loss 5.125605583190918, Transition Loss 0.38619840145111084, Classifier Loss 0.10142279416322708, Total Loss 41.05039596557617\n",
      "5: Encoding Loss 5.532151222229004, Transition Loss -0.9154261350631714, Classifier Loss 0.17079126834869385, Total Loss 50.27166748046875\n",
      "5: Encoding Loss 5.503106594085693, Transition Loss -1.7693604230880737, Classifier Loss 0.08008535206317902, Total Loss 41.026466369628906\n",
      "5: Encoding Loss 3.577885389328003, Transition Loss -2.385576009750366, Classifier Loss 0.12062335759401321, Total Loss 33.52869415283203\n",
      "5: Encoding Loss 3.277932643890381, Transition Loss -1.380308985710144, Classifier Loss 0.08773760497570038, Total Loss 28.440805435180664\n",
      "5: Encoding Loss 8.042057991027832, Transition Loss -1.3082587718963623, Classifier Loss 0.17455145716667175, Total Loss 65.70697021484375\n",
      "5: Encoding Loss 4.616535663604736, Transition Loss -0.1785728633403778, Classifier Loss 0.17394445836544037, Total Loss 45.09358596801758\n",
      "5: Encoding Loss 4.79373836517334, Transition Loss -1.4784841537475586, Classifier Loss 0.0652821883559227, Total Loss 35.29005813598633\n",
      "5: Encoding Loss 5.576341152191162, Transition Loss 0.042452603578567505, Classifier Loss 0.13009041547775269, Total Loss 46.48406982421875\n",
      "5: Encoding Loss 3.18156361579895, Transition Loss 0.4840732216835022, Classifier Loss 0.07609061151742935, Total Loss 26.892074584960938\n",
      "5: Encoding Loss 3.736435651779175, Transition Loss -1.305821418762207, Classifier Loss 0.10968900471925735, Total Loss 33.386993408203125\n",
      "5: Encoding Loss 6.462890625, Transition Loss -0.9402098059654236, Classifier Loss 0.11078554391860962, Total Loss 49.85551834106445\n",
      "5: Encoding Loss 4.070455551147461, Transition Loss -1.0014728307724, Classifier Loss 0.1286248117685318, Total Loss 37.28481674194336\n",
      "5: Encoding Loss 6.424250602722168, Transition Loss -0.9558830261230469, Classifier Loss 0.15917427837848663, Total Loss 54.46255111694336\n",
      "5: Encoding Loss 6.6331024169921875, Transition Loss -1.4071571826934814, Classifier Loss 0.17780126631259918, Total Loss 57.57817840576172\n",
      "5: Encoding Loss 4.697300434112549, Transition Loss -0.9944111704826355, Classifier Loss 0.05026811733841896, Total Loss 33.21022033691406\n",
      "5: Encoding Loss 4.799060821533203, Transition Loss -0.5364417433738708, Classifier Loss 0.1776590198278427, Total Loss 46.560054779052734\n",
      "5: Encoding Loss 6.453444480895996, Transition Loss -3.0351336002349854, Classifier Loss 0.19381967186927795, Total Loss 58.10142517089844\n",
      "5: Encoding Loss 6.75986385345459, Transition Loss -1.4901747703552246, Classifier Loss 0.07635155320167542, Total Loss 48.19374465942383\n",
      "5: Encoding Loss 5.199248790740967, Transition Loss -1.2016408443450928, Classifier Loss 0.09518374502658844, Total Loss 40.71338653564453\n",
      "5: Encoding Loss 6.511496543884277, Transition Loss -2.1542599201202393, Classifier Loss 0.14983153343200684, Total Loss 54.051273345947266\n",
      "5: Encoding Loss 5.304876804351807, Transition Loss -0.4603041410446167, Classifier Loss 0.07875838875770569, Total Loss 39.704917907714844\n",
      "5: Encoding Loss 2.932473659515381, Transition Loss -1.4739843606948853, Classifier Loss 0.09205295145511627, Total Loss 26.799549102783203\n",
      "5: Encoding Loss 5.9285664558410645, Transition Loss -1.7625324726104736, Classifier Loss 0.14528125524520874, Total Loss 50.098819732666016\n",
      "5: Encoding Loss 8.83287239074707, Transition Loss -3.171916961669922, Classifier Loss 0.18772932887077332, Total Loss 71.7688980102539\n",
      "5: Encoding Loss 5.43074369430542, Transition Loss -1.4240009784698486, Classifier Loss 0.07673812657594681, Total Loss 40.25770950317383\n",
      "5: Encoding Loss 5.323702812194824, Transition Loss -1.0893490314483643, Classifier Loss 0.07258698344230652, Total Loss 39.20048141479492\n",
      "5: Encoding Loss 5.942717552185059, Transition Loss -0.3694227635860443, Classifier Loss 0.17031441628932953, Total Loss 52.687599182128906\n",
      "5: Encoding Loss 6.103589057922363, Transition Loss -0.9298848509788513, Classifier Loss 0.27291664481163025, Total Loss 63.91282653808594\n",
      "5: Encoding Loss 6.102200031280518, Transition Loss -1.5038976669311523, Classifier Loss 0.1898622065782547, Total Loss 55.598819732666016\n",
      "5: Encoding Loss 6.387983798980713, Transition Loss -1.9505870342254639, Classifier Loss 0.16032713651657104, Total Loss 54.35983657836914\n",
      "5: Encoding Loss 5.274258613586426, Transition Loss -1.3568150997161865, Classifier Loss 0.13105744123458862, Total Loss 44.750755310058594\n",
      "5: Encoding Loss 5.462975025177002, Transition Loss -0.44622600078582764, Classifier Loss 0.16173453629016876, Total Loss 48.95112609863281\n",
      "5: Encoding Loss 6.6906418800354, Transition Loss -0.6833822727203369, Classifier Loss 0.1641826331615448, Total Loss 56.56184005737305\n",
      "5: Encoding Loss 4.826766014099121, Transition Loss -0.5620397329330444, Classifier Loss 0.0750105008482933, Total Loss 36.461421966552734\n",
      "5: Encoding Loss 7.750861167907715, Transition Loss -2.054967164993286, Classifier Loss 0.0865786224603653, Total Loss 55.16221237182617\n",
      "5: Encoding Loss 5.520278453826904, Transition Loss -0.9172275066375732, Classifier Loss 0.1211199015378952, Total Loss 45.233299255371094\n",
      "5: Encoding Loss 4.1513800621032715, Transition Loss -1.5624079704284668, Classifier Loss 0.09758345037698746, Total Loss 34.66600036621094\n",
      "5: Encoding Loss 5.510651588439941, Transition Loss -0.45124149322509766, Classifier Loss 0.1889542043209076, Total Loss 51.95915222167969\n",
      "5: Encoding Loss 4.8764543533325195, Transition Loss -1.3402835130691528, Classifier Loss 0.09273044764995575, Total Loss 38.53123092651367\n",
      "5: Encoding Loss 4.835609436035156, Transition Loss -1.450124740600586, Classifier Loss 0.1726084053516388, Total Loss 46.27391815185547\n",
      "5: Encoding Loss 11.04096794128418, Transition Loss -2.0268983840942383, Classifier Loss 0.12257195264101028, Total Loss 78.502197265625\n",
      "5: Encoding Loss 7.2932448387146, Transition Loss -0.7639807462692261, Classifier Loss 0.07861156761646271, Total Loss 51.620323181152344\n",
      "5: Encoding Loss 6.647345542907715, Transition Loss 0.6218998432159424, Classifier Loss 0.08147787302732468, Total Loss 48.28062057495117\n",
      "5: Encoding Loss 4.664816856384277, Transition Loss -2.1875836849212646, Classifier Loss 0.11879121512174606, Total Loss 39.86715316772461\n",
      "5: Encoding Loss 6.202227592468262, Transition Loss -1.559841275215149, Classifier Loss 0.13044089078903198, Total Loss 50.256832122802734\n",
      "5: Encoding Loss 3.930192470550537, Transition Loss -0.49876782298088074, Classifier Loss 0.10970829427242279, Total Loss 34.551788330078125\n",
      "5: Encoding Loss 5.282187461853027, Transition Loss -1.3531372547149658, Classifier Loss 0.2036815881729126, Total Loss 52.06074523925781\n",
      "5: Encoding Loss 9.311399459838867, Transition Loss -1.333174705505371, Classifier Loss 0.10680025815963745, Total Loss 66.54788970947266\n",
      "5: Encoding Loss 5.2361063957214355, Transition Loss -1.7466620206832886, Classifier Loss 0.1057383194565773, Total Loss 41.98977279663086\n",
      "5: Encoding Loss 5.645145416259766, Transition Loss -1.084434986114502, Classifier Loss 0.08590755611658096, Total Loss 42.4611930847168\n",
      "5: Encoding Loss 5.9086833000183105, Transition Loss -2.028083324432373, Classifier Loss 0.0669197291135788, Total Loss 42.14326095581055\n",
      "5: Encoding Loss 5.123459815979004, Transition Loss -2.0424625873565674, Classifier Loss 0.06842248141765594, Total Loss 37.582191467285156\n",
      "5: Encoding Loss 4.680220127105713, Transition Loss -1.0758297443389893, Classifier Loss 0.04861157387495041, Total Loss 32.942047119140625\n",
      "5: Encoding Loss 4.3069257736206055, Transition Loss -2.2204644680023193, Classifier Loss 0.12669482827186584, Total Loss 38.51015090942383\n",
      "5: Encoding Loss 6.032217979431152, Transition Loss -0.526138186454773, Classifier Loss 0.11487004905939102, Total Loss 47.68010330200195\n",
      "5: Encoding Loss 5.7706193923950195, Transition Loss -0.8170627951622009, Classifier Loss 0.08273016661405563, Total Loss 42.89640808105469\n",
      "5: Encoding Loss 5.532433986663818, Transition Loss -2.883108615875244, Classifier Loss 0.12158799171447754, Total Loss 45.35225296020508\n",
      "5: Encoding Loss 7.237011909484863, Transition Loss -2.801135540008545, Classifier Loss 0.13041725754737854, Total Loss 56.462677001953125\n",
      "5: Encoding Loss 5.342628002166748, Transition Loss -0.02960243821144104, Classifier Loss 0.071538046002388, Total Loss 39.209564208984375\n",
      "5: Encoding Loss 4.950586795806885, Transition Loss -1.7482993602752686, Classifier Loss 0.16548587381839752, Total Loss 46.25141143798828\n",
      "5: Encoding Loss 4.977059841156006, Transition Loss -1.3172866106033325, Classifier Loss 0.09555310755968094, Total Loss 39.417144775390625\n",
      "5: Encoding Loss 3.6383209228515625, Transition Loss -1.803751826286316, Classifier Loss 0.09128530323505402, Total Loss 30.957735061645508\n",
      "5: Encoding Loss 4.835740089416504, Transition Loss -2.553989887237549, Classifier Loss 0.15158022940158844, Total Loss 44.17144012451172\n",
      "5: Encoding Loss 6.642051696777344, Transition Loss -2.06917405128479, Classifier Loss 0.10726646333932877, Total Loss 50.578128814697266\n",
      "5: Encoding Loss 6.570889472961426, Transition Loss -2.5060434341430664, Classifier Loss 0.12539458274841309, Total Loss 51.96379470825195\n",
      "5: Encoding Loss 7.0679450035095215, Transition Loss -0.9968529939651489, Classifier Loss 0.1653616577386856, Total Loss 58.94343948364258\n",
      "5: Encoding Loss 4.952970504760742, Transition Loss 0.308489054441452, Classifier Loss 0.11962439119815826, Total Loss 41.80365753173828\n",
      "5: Encoding Loss 4.798017978668213, Transition Loss -2.421149253845215, Classifier Loss 0.10010599344968796, Total Loss 38.79773712158203\n",
      "5: Encoding Loss 5.243896484375, Transition Loss -1.7747290134429932, Classifier Loss 0.13290216028690338, Total Loss 44.75288772583008\n",
      "5: Encoding Loss 4.512026786804199, Transition Loss -1.6092026233673096, Classifier Loss 0.09147724509239197, Total Loss 36.219242095947266\n",
      "5: Encoding Loss 4.270567417144775, Transition Loss -1.9037237167358398, Classifier Loss 0.09606648981571198, Total Loss 35.22929382324219\n",
      "5: Encoding Loss 3.7328453063964844, Transition Loss -2.1710071563720703, Classifier Loss 0.07538563013076782, Total Loss 29.93476676940918\n",
      "5: Encoding Loss 3.8463428020477295, Transition Loss -0.5736556053161621, Classifier Loss 0.1252688467502594, Total Loss 35.604713439941406\n",
      "5: Encoding Loss 4.816871643066406, Transition Loss -0.8738791942596436, Classifier Loss 0.13694484531879425, Total Loss 42.595367431640625\n",
      "5: Encoding Loss 6.334230899810791, Transition Loss -0.37009724974632263, Classifier Loss 0.11880888789892197, Total Loss 49.88612747192383\n",
      "5: Encoding Loss 4.5466389656066895, Transition Loss -0.762010931968689, Classifier Loss 0.1077573373913765, Total Loss 38.05526351928711\n",
      "5: Encoding Loss 7.287092685699463, Transition Loss -1.2084904909133911, Classifier Loss 0.12407258152961731, Total Loss 56.129329681396484\n",
      "5: Encoding Loss 6.632353782653809, Transition Loss -0.8457896113395691, Classifier Loss 0.12635844945907593, Total Loss 52.429630279541016\n",
      "5: Encoding Loss 4.92809534072876, Transition Loss -1.618949294090271, Classifier Loss 0.16998936235904694, Total Loss 46.56686019897461\n",
      "5: Encoding Loss 5.21923303604126, Transition Loss -0.6528214812278748, Classifier Loss 0.11927863955497742, Total Loss 43.243003845214844\n",
      "5: Encoding Loss 7.646862983703613, Transition Loss -2.1357200145721436, Classifier Loss 0.17751777172088623, Total Loss 63.632102966308594\n",
      "5: Encoding Loss 5.264992713928223, Transition Loss -1.6047511100769043, Classifier Loss 0.06894072145223618, Total Loss 38.4833869934082\n",
      "5: Encoding Loss 7.43801736831665, Transition Loss -0.95442134141922, Classifier Loss 0.19897989928722382, Total Loss 64.52571105957031\n",
      "5: Encoding Loss 5.9337873458862305, Transition Loss -0.592038094997406, Classifier Loss 0.1727035641670227, Total Loss 52.87284851074219\n",
      "5: Encoding Loss 4.96567440032959, Transition Loss -1.7016867399215698, Classifier Loss 0.1307028830051422, Total Loss 42.8636589050293\n",
      "5: Encoding Loss 4.087326526641846, Transition Loss -2.6621663570404053, Classifier Loss 0.16883094608783722, Total Loss 41.40599060058594\n",
      "5: Encoding Loss 6.9950852394104, Transition Loss -1.9068703651428223, Classifier Loss 0.14538514614105225, Total Loss 56.508262634277344\n",
      "5: Encoding Loss 4.503431797027588, Transition Loss -0.810746967792511, Classifier Loss 0.10919946432113647, Total Loss 37.94021224975586\n",
      "5: Encoding Loss 2.9570746421813965, Transition Loss -1.1250592470169067, Classifier Loss 0.08806491643190384, Total Loss 26.548490524291992\n",
      "5: Encoding Loss 2.7104578018188477, Transition Loss -1.6511212587356567, Classifier Loss 0.09787290543317795, Total Loss 26.04937744140625\n",
      "5: Encoding Loss 3.6045570373535156, Transition Loss -1.402753233909607, Classifier Loss 0.10206402838230133, Total Loss 31.83318328857422\n",
      "5: Encoding Loss 8.729639053344727, Transition Loss 0.4713650643825531, Classifier Loss 0.11627151817083359, Total Loss 64.19353485107422\n",
      "5: Encoding Loss 5.955153942108154, Transition Loss 0.10398495197296143, Classifier Loss 0.15826204419136047, Total Loss 51.598724365234375\n",
      "5: Encoding Loss 4.255861282348633, Transition Loss 0.40705305337905884, Classifier Loss 0.08016093820333481, Total Loss 33.71408462524414\n",
      "5: Encoding Loss 8.064577102661133, Transition Loss -0.5365440845489502, Classifier Loss 0.2563874423503876, Total Loss 74.02599334716797\n",
      "5: Encoding Loss 5.254881858825684, Transition Loss -1.5178086757659912, Classifier Loss 0.06852658092975616, Total Loss 38.381343841552734\n",
      "5: Encoding Loss 5.0217413902282715, Transition Loss -2.187793493270874, Classifier Loss 0.13816042244434357, Total Loss 43.94561767578125\n",
      "5: Encoding Loss 5.812757968902588, Transition Loss -1.1717252731323242, Classifier Loss 0.15146201848983765, Total Loss 50.022281646728516\n",
      "5: Encoding Loss 5.202483654022217, Transition Loss -1.4450011253356934, Classifier Loss 0.06805717945098877, Total Loss 38.020042419433594\n",
      "5: Encoding Loss 4.612557411193848, Transition Loss -0.6571074724197388, Classifier Loss 0.07883917540311813, Total Loss 35.558998107910156\n",
      "5: Encoding Loss 6.188173294067383, Transition Loss -0.8044050335884094, Classifier Loss 0.1258133053779602, Total Loss 49.71004867553711\n",
      "5: Encoding Loss 3.9681694507598877, Transition Loss -0.05302390456199646, Classifier Loss 0.057211603969335556, Total Loss 29.5301570892334\n",
      "5: Encoding Loss 7.026055335998535, Transition Loss 0.3704245090484619, Classifier Loss 0.09225405007600784, Total Loss 51.5299072265625\n",
      "5: Encoding Loss 5.59430456161499, Transition Loss -1.9325225353240967, Classifier Loss 0.11778192967176437, Total Loss 45.3432502746582\n",
      "5: Encoding Loss 4.066208362579346, Transition Loss -1.612696886062622, Classifier Loss 0.14605146646499634, Total Loss 39.00175094604492\n",
      "5: Encoding Loss 6.236090183258057, Transition Loss -1.839949607849121, Classifier Loss 0.08362923562526703, Total Loss 45.77872848510742\n",
      "5: Encoding Loss 3.7899975776672363, Transition Loss -1.1343530416488647, Classifier Loss 0.127975732088089, Total Loss 35.537105560302734\n",
      "5: Encoding Loss 2.2197327613830566, Transition Loss -1.5435028076171875, Classifier Loss 0.11752527207136154, Total Loss 25.07030487060547\n",
      "5: Encoding Loss 2.617405414581299, Transition Loss -1.1900006532669067, Classifier Loss 0.07579033076763153, Total Loss 23.282989501953125\n",
      "5: Encoding Loss 8.606660842895508, Transition Loss -2.0635323524475098, Classifier Loss 0.1265704333782196, Total Loss 64.29618835449219\n",
      "5: Encoding Loss 5.30424690246582, Transition Loss -1.6674984693527222, Classifier Loss 0.0649227723479271, Total Loss 38.31709289550781\n",
      "5: Encoding Loss 2.69492506980896, Transition Loss -1.8925526142120361, Classifier Loss 0.0833125114440918, Total Loss 24.500045776367188\n",
      "5: Encoding Loss 6.651491641998291, Transition Loss -1.2991684675216675, Classifier Loss 0.1267993301153183, Total Loss 52.58836364746094\n",
      "5: Encoding Loss 3.8923068046569824, Transition Loss -0.9903192520141602, Classifier Loss 0.08537657558917999, Total Loss 31.891101837158203\n",
      "5: Encoding Loss 5.395051002502441, Transition Loss -0.8184441924095154, Classifier Loss 0.11563224345445633, Total Loss 43.933204650878906\n",
      "5: Encoding Loss 5.188336372375488, Transition Loss -0.6048455238342285, Classifier Loss 0.08996018767356873, Total Loss 40.125797271728516\n",
      "5: Encoding Loss 5.498243808746338, Transition Loss -2.6040291786193848, Classifier Loss 0.14638544619083405, Total Loss 47.62696838378906\n",
      "5: Encoding Loss 5.409291744232178, Transition Loss -0.6072527170181274, Classifier Loss 0.12897448241710663, Total Loss 45.35295867919922\n",
      "5: Encoding Loss 5.263023376464844, Transition Loss -1.4567112922668457, Classifier Loss 0.1841595470905304, Total Loss 49.99351119995117\n",
      "5: Encoding Loss 4.524871349334717, Transition Loss -2.4509849548339844, Classifier Loss 0.1073540598154068, Total Loss 37.8836555480957\n",
      "5: Encoding Loss 3.6752707958221436, Transition Loss -1.2151134014129639, Classifier Loss 0.054326076060533524, Total Loss 27.483747482299805\n",
      "5: Encoding Loss 7.694445610046387, Transition Loss -0.9712119698524475, Classifier Loss 0.17901664972305298, Total Loss 64.06795501708984\n",
      "5: Encoding Loss 7.022662162780762, Transition Loss -0.9128868579864502, Classifier Loss 0.17276042699813843, Total Loss 59.411651611328125\n",
      "5: Encoding Loss 5.395349979400635, Transition Loss -1.1304149627685547, Classifier Loss 0.17747889459133148, Total Loss 50.11953353881836\n",
      "5: Encoding Loss 3.543923854827881, Transition Loss -1.6664258241653442, Classifier Loss 0.07597474753856659, Total Loss 28.860353469848633\n",
      "5: Encoding Loss 11.057013511657715, Transition Loss -1.1380393505096436, Classifier Loss 0.17521196603775024, Total Loss 83.86282348632812\n",
      "5: Encoding Loss 8.020098686218262, Transition Loss -2.5035593509674072, Classifier Loss 0.09029725193977356, Total Loss 57.149314880371094\n",
      "5: Encoding Loss 5.375092029571533, Transition Loss -1.8878099918365479, Classifier Loss 0.16034169495105743, Total Loss 48.283966064453125\n",
      "5: Encoding Loss 6.5288405418396, Transition Loss -2.013493299484253, Classifier Loss 0.1656568944454193, Total Loss 55.73793029785156\n",
      "5: Encoding Loss 7.030519008636475, Transition Loss -1.5121431350708008, Classifier Loss 0.13889963924884796, Total Loss 56.07247543334961\n",
      "5: Encoding Loss 5.541757583618164, Transition Loss -0.6681930422782898, Classifier Loss 0.09105999022722244, Total Loss 42.35627746582031\n",
      "5: Encoding Loss 7.357634544372559, Transition Loss -2.4838356971740723, Classifier Loss 0.10325559973716736, Total Loss 54.470375061035156\n",
      "5: Encoding Loss 7.252607822418213, Transition Loss -1.3884493112564087, Classifier Loss 0.21087366342544556, Total Loss 64.60245513916016\n",
      "5: Encoding Loss 7.400813102722168, Transition Loss -2.142503261566162, Classifier Loss 0.0717293918132782, Total Loss 51.576961517333984\n",
      "5: Encoding Loss 6.787428379058838, Transition Loss -0.4623962640762329, Classifier Loss 0.21716415882110596, Total Loss 62.44080352783203\n",
      "5: Encoding Loss 5.991464614868164, Transition Loss -1.7816097736358643, Classifier Loss 0.07544388622045517, Total Loss 43.492462158203125\n",
      "5: Encoding Loss 5.214100360870361, Transition Loss -1.3300294876098633, Classifier Loss 0.08531135320663452, Total Loss 39.815208435058594\n",
      "5: Encoding Loss 4.041626930236816, Transition Loss -2.3453948497772217, Classifier Loss 0.07620139420032501, Total Loss 31.86896324157715\n",
      "5: Encoding Loss 4.390118598937988, Transition Loss -1.0369863510131836, Classifier Loss 0.11085788905620575, Total Loss 37.426082611083984\n",
      "5: Encoding Loss 3.9094595909118652, Transition Loss -2.7697947025299072, Classifier Loss 0.09031949937343597, Total Loss 32.48760223388672\n",
      "5: Encoding Loss 7.900338649749756, Transition Loss -0.9381563663482666, Classifier Loss 0.11837179213762283, Total Loss 59.23883819580078\n",
      "5: Encoding Loss 4.767213344573975, Transition Loss -1.7531960010528564, Classifier Loss 0.11579562723636627, Total Loss 40.18214416503906\n",
      "5: Encoding Loss 2.9845194816589355, Transition Loss -1.1484591960906982, Classifier Loss 0.06264036893844604, Total Loss 24.17069435119629\n",
      "5: Encoding Loss 7.464559078216553, Transition Loss -2.1505284309387207, Classifier Loss 0.0865706205368042, Total Loss 53.44356155395508\n",
      "5: Encoding Loss 8.195743560791016, Transition Loss -1.40818452835083, Classifier Loss 0.09002400934696198, Total Loss 58.176300048828125\n",
      "5: Encoding Loss 6.242331504821777, Transition Loss -0.5170839428901672, Classifier Loss 0.14134028553962708, Total Loss 51.58781433105469\n",
      "5: Encoding Loss 5.9904937744140625, Transition Loss -1.6407443284988403, Classifier Loss 0.051115572452545166, Total Loss 41.053863525390625\n",
      "5: Encoding Loss 4.966887950897217, Transition Loss -1.0203001499176025, Classifier Loss 0.12952691316604614, Total Loss 42.75361251831055\n",
      "5: Encoding Loss 5.6579742431640625, Transition Loss -1.5649281740188599, Classifier Loss 0.10432431846857071, Total Loss 44.37965393066406\n",
      "5: Encoding Loss 6.212545394897461, Transition Loss -1.1236544847488403, Classifier Loss 0.10871874541044235, Total Loss 48.146697998046875\n",
      "5: Encoding Loss 4.123634338378906, Transition Loss -0.930798351764679, Classifier Loss 0.09918811172246933, Total Loss 34.66024398803711\n",
      "5: Encoding Loss 6.076962947845459, Transition Loss -1.8897581100463867, Classifier Loss 0.1734686642885208, Total Loss 53.807891845703125\n",
      "5: Encoding Loss 3.8219845294952393, Transition Loss -1.1227383613586426, Classifier Loss 0.09418901056051254, Total Loss 32.35035705566406\n",
      "5: Encoding Loss 5.061192512512207, Transition Loss -0.462847501039505, Classifier Loss 0.06087283045053482, Total Loss 36.45425033569336\n",
      "5: Encoding Loss 6.996270656585693, Transition Loss -1.419750452041626, Classifier Loss 0.1929553598165512, Total Loss 61.2725944519043\n",
      "5: Encoding Loss 5.29645299911499, Transition Loss -0.9762195348739624, Classifier Loss 0.07552006095647812, Total Loss 39.33033752441406\n",
      "5: Encoding Loss 5.478834629058838, Transition Loss -3.436262369155884, Classifier Loss 0.08462104946374893, Total Loss 41.333740234375\n",
      "5: Encoding Loss 4.3471198081970215, Transition Loss -1.7194868326187134, Classifier Loss 0.0640045702457428, Total Loss 32.48249053955078\n",
      "5: Encoding Loss 5.587880611419678, Transition Loss -0.26158812642097473, Classifier Loss 0.17566488683223724, Total Loss 51.09366989135742\n",
      "5: Encoding Loss 3.659858226776123, Transition Loss -1.3600399494171143, Classifier Loss 0.11147947609424591, Total Loss 33.10655212402344\n",
      "5: Encoding Loss 8.30578327178955, Transition Loss -1.8774100542068481, Classifier Loss 0.18008551001548767, Total Loss 67.8425064086914\n",
      "5: Encoding Loss 6.043774604797363, Transition Loss -0.8163456916809082, Classifier Loss 0.08625722676515579, Total Loss 44.88804626464844\n",
      "5: Encoding Loss 5.137012958526611, Transition Loss -2.4945907592773438, Classifier Loss 0.12014073133468628, Total Loss 42.83515167236328\n",
      "5: Encoding Loss 4.680905342102051, Transition Loss -2.663097381591797, Classifier Loss 0.1394340693950653, Total Loss 42.027774810791016\n",
      "5: Encoding Loss 4.62901496887207, Transition Loss -1.5761992931365967, Classifier Loss 0.13116399943828583, Total Loss 40.889862060546875\n",
      "5: Encoding Loss 5.710019588470459, Transition Loss -1.328850507736206, Classifier Loss 0.10863414406776428, Total Loss 45.12300491333008\n",
      "5: Encoding Loss 5.100629806518555, Transition Loss -1.8963788747787476, Classifier Loss 0.1884373426437378, Total Loss 49.44675827026367\n",
      "5: Encoding Loss 6.055746555328369, Transition Loss -1.3255726099014282, Classifier Loss 0.05252287536859512, Total Loss 41.586238861083984\n",
      "5: Encoding Loss 4.778882026672363, Transition Loss 0.2205336093902588, Classifier Loss 0.07341401278972626, Total Loss 36.102909088134766\n",
      "5: Encoding Loss 5.63291072845459, Transition Loss -1.872894287109375, Classifier Loss 0.07517295330762863, Total Loss 41.31401443481445\n",
      "5: Encoding Loss 4.139198303222656, Transition Loss -1.479917049407959, Classifier Loss 0.08809249103069305, Total Loss 33.64384841918945\n",
      "5: Encoding Loss 6.969184875488281, Transition Loss -0.5081998705863953, Classifier Loss 0.14231660962104797, Total Loss 56.046566009521484\n",
      "5: Encoding Loss 5.3964691162109375, Transition Loss -2.3692824840545654, Classifier Loss 0.05759453773498535, Total Loss 38.13732147216797\n",
      "5: Encoding Loss 7.578152656555176, Transition Loss -1.6160507202148438, Classifier Loss 0.22647316753864288, Total Loss 68.11558532714844\n",
      "5: Encoding Loss 5.826674938201904, Transition Loss -1.0870730876922607, Classifier Loss 0.13232512772083282, Total Loss 48.19213104248047\n",
      "5: Encoding Loss 5.4382500648498535, Transition Loss -1.78044855594635, Classifier Loss 0.07383886724710464, Total Loss 40.01267623901367\n",
      "5: Encoding Loss 5.285861968994141, Transition Loss -0.18616537749767303, Classifier Loss 0.09136790782213211, Total Loss 40.851890563964844\n",
      "5: Encoding Loss 5.265665531158447, Transition Loss -1.1761716604232788, Classifier Loss 0.0717075914144516, Total Loss 38.764286041259766\n",
      "5: Encoding Loss 5.767277240753174, Transition Loss -0.4043341279029846, Classifier Loss 0.1380426585674286, Total Loss 48.40776824951172\n",
      "5: Encoding Loss 7.277608394622803, Transition Loss -0.4482082426548004, Classifier Loss 0.06983622908592224, Total Loss 50.64909744262695\n",
      "5: Encoding Loss 4.65879487991333, Transition Loss -1.6151951551437378, Classifier Loss 0.18631350994110107, Total Loss 46.58347702026367\n",
      "5: Encoding Loss 5.3712053298950195, Transition Loss -0.5242815017700195, Classifier Loss 0.07816001772880554, Total Loss 40.043025970458984\n",
      "5: Encoding Loss 4.5214409828186035, Transition Loss -0.5407268404960632, Classifier Loss 0.11731718480587006, Total Loss 38.86014938354492\n",
      "5: Encoding Loss 4.844653129577637, Transition Loss -1.6957303285598755, Classifier Loss 0.0528755821287632, Total Loss 34.354801177978516\n",
      "5: Encoding Loss 4.928995132446289, Transition Loss -1.9544979333877563, Classifier Loss 0.1879233717918396, Total Loss 48.36552810668945\n",
      "5: Encoding Loss 5.103810787200928, Transition Loss -2.0131921768188477, Classifier Loss 0.1493118703365326, Total Loss 45.55324935913086\n",
      "5: Encoding Loss 3.6872477531433105, Transition Loss -2.6956846714019775, Classifier Loss 0.09745816141366959, Total Loss 31.868227005004883\n",
      "5: Encoding Loss 3.94126558303833, Transition Loss -0.7020559906959534, Classifier Loss 0.1374700367450714, Total Loss 37.394317626953125\n",
      "5: Encoding Loss 5.881596565246582, Transition Loss -0.6360452175140381, Classifier Loss 0.1253642588853836, Total Loss 47.82575225830078\n",
      "5: Encoding Loss 3.9673526287078857, Transition Loss -2.1741795539855957, Classifier Loss 0.07654392719268799, Total Loss 31.457639694213867\n",
      "5: Encoding Loss 5.088254928588867, Transition Loss 0.0261879563331604, Classifier Loss 0.055377356708049774, Total Loss 36.07774353027344\n",
      "5: Encoding Loss 5.7331695556640625, Transition Loss -2.199941635131836, Classifier Loss 0.09784819185733795, Total Loss 44.18295669555664\n",
      "5: Encoding Loss 4.275527000427246, Transition Loss -0.885543704032898, Classifier Loss 0.07726366817951202, Total Loss 33.37917709350586\n",
      "5: Encoding Loss 8.429496765136719, Transition Loss 0.34067344665527344, Classifier Loss 0.25589224696159363, Total Loss 76.30247497558594\n",
      "5: Encoding Loss 6.4846391677856445, Transition Loss -1.5435163974761963, Classifier Loss 0.07257672399282455, Total Loss 46.16489028930664\n",
      "5: Encoding Loss 7.378249168395996, Transition Loss 0.2909386157989502, Classifier Loss 0.09716577082872391, Total Loss 54.10245132446289\n",
      "5: Encoding Loss 5.146834373474121, Transition Loss -1.6502645015716553, Classifier Loss 0.15441395342350006, Total Loss 46.32174301147461\n",
      "5: Encoding Loss 3.4685397148132324, Transition Loss -1.1007733345031738, Classifier Loss 0.12300094962120056, Total Loss 33.110897064208984\n",
      "5: Encoding Loss 6.567080497741699, Transition Loss -1.6353776454925537, Classifier Loss 0.12633398175239563, Total Loss 52.03522872924805\n",
      "5: Encoding Loss 9.248148918151855, Transition Loss -2.7443509101867676, Classifier Loss 0.20772990584373474, Total Loss 76.26078796386719\n",
      "5: Encoding Loss 7.451103210449219, Transition Loss -1.3964154720306396, Classifier Loss 0.29651784896850586, Total Loss 74.35784912109375\n",
      "5: Encoding Loss 4.1746826171875, Transition Loss -1.276944875717163, Classifier Loss 0.12679968774318695, Total Loss 37.72755432128906\n",
      "5: Encoding Loss 8.066614151000977, Transition Loss 0.33268624544143677, Classifier Loss 0.11555270105600357, Total Loss 60.08803176879883\n",
      "5: Encoding Loss 8.751373291015625, Transition Loss -1.3505988121032715, Classifier Loss 0.321092814207077, Total Loss 84.61698150634766\n",
      "5: Encoding Loss 6.9198408126831055, Transition Loss -0.8073782920837402, Classifier Loss 0.13540953397750854, Total Loss 55.05967712402344\n",
      "5: Encoding Loss 7.410162925720215, Transition Loss -0.2974502444267273, Classifier Loss 0.14262571930885315, Total Loss 58.72343444824219\n",
      "5: Encoding Loss 7.40899133682251, Transition Loss -0.5564311146736145, Classifier Loss 0.11369924992322922, Total Loss 55.82365417480469\n",
      "5: Encoding Loss 5.351530075073242, Transition Loss -1.909557580947876, Classifier Loss 0.10837636888027191, Total Loss 42.94605255126953\n",
      "5: Encoding Loss 4.714293479919434, Transition Loss -1.8086936473846436, Classifier Loss 0.12767940759658813, Total Loss 41.052978515625\n",
      "5: Encoding Loss 6.0983476638793945, Transition Loss -3.1631226539611816, Classifier Loss 0.14523209631443024, Total Loss 51.112030029296875\n",
      "5: Encoding Loss 4.991398811340332, Transition Loss -2.1699578762054443, Classifier Loss 0.10065647959709167, Total Loss 40.0131721496582\n",
      "5: Encoding Loss 4.391873359680176, Transition Loss -1.2010575532913208, Classifier Loss 0.07691730558872223, Total Loss 34.04248809814453\n",
      "5: Encoding Loss 4.963479042053223, Transition Loss -1.7348096370697021, Classifier Loss 0.1628895103931427, Total Loss 46.069129943847656\n",
      "5: Encoding Loss 3.1678223609924316, Transition Loss -1.8018643856048584, Classifier Loss 0.09976550936698914, Total Loss 28.982765197753906\n",
      "5: Encoding Loss 4.518811225891113, Transition Loss -1.5518156290054321, Classifier Loss 0.13083221018314362, Total Loss 40.19546890258789\n",
      "5: Encoding Loss 6.304747104644775, Transition Loss -1.5579105615615845, Classifier Loss 0.22364267706871033, Total Loss 60.1921272277832\n",
      "5: Encoding Loss 5.600676536560059, Transition Loss -0.4703753590583801, Classifier Loss 0.16333550214767456, Total Loss 49.93742370605469\n",
      "5: Encoding Loss 4.298924922943115, Transition Loss -1.7883996963500977, Classifier Loss 0.1466309130191803, Total Loss 40.45592498779297\n",
      "5: Encoding Loss 5.615807056427002, Transition Loss -2.0976760387420654, Classifier Loss 0.09372695535421371, Total Loss 43.06669998168945\n",
      "5: Encoding Loss 4.299495220184326, Transition Loss -0.6107755303382874, Classifier Loss 0.07423556596040726, Total Loss 33.22028350830078\n",
      "5: Encoding Loss 5.549193382263184, Transition Loss -1.3996126651763916, Classifier Loss 0.057111311703920364, Total Loss 39.005733489990234\n",
      "5: Encoding Loss 4.495112419128418, Transition Loss -1.4411150217056274, Classifier Loss 0.058489855378866196, Total Loss 32.819087982177734\n",
      "5: Encoding Loss 3.762709140777588, Transition Loss -0.6452310681343079, Classifier Loss 0.1521592140197754, Total Loss 37.79191589355469\n",
      "5: Encoding Loss 6.374065399169922, Transition Loss -0.7517824172973633, Classifier Loss 0.1535111665725708, Total Loss 53.59520721435547\n",
      "5: Encoding Loss 6.873388767242432, Transition Loss -0.3090190291404724, Classifier Loss 0.14261917769908905, Total Loss 55.50212860107422\n",
      "5: Encoding Loss 6.744103908538818, Transition Loss -0.8073515892028809, Classifier Loss 0.1108747273683548, Total Loss 51.55177688598633\n",
      "5: Encoding Loss 3.159787654876709, Transition Loss -2.1756155490875244, Classifier Loss 0.11214841902256012, Total Loss 30.172698974609375\n",
      "5: Encoding Loss 7.648087978363037, Transition Loss -1.5502196550369263, Classifier Loss 0.16564533114433289, Total Loss 62.45244216918945\n",
      "5: Encoding Loss 5.282047748565674, Transition Loss -2.3752877712249756, Classifier Loss 0.1130753606557846, Total Loss 42.99887466430664\n",
      "5: Encoding Loss 5.864928245544434, Transition Loss -1.0106494426727295, Classifier Loss 0.10662823170423508, Total Loss 45.85198974609375\n",
      "5: Encoding Loss 7.143052577972412, Transition Loss -2.0528032779693604, Classifier Loss 0.18756985664367676, Total Loss 61.61448287963867\n",
      "5: Encoding Loss 5.604996681213379, Transition Loss -2.009718894958496, Classifier Loss 0.16133582592010498, Total Loss 49.762760162353516\n",
      "5: Encoding Loss 7.513404846191406, Transition Loss -0.2587263584136963, Classifier Loss 0.13774780929088593, Total Loss 58.855106353759766\n",
      "5: Encoding Loss 5.192858695983887, Transition Loss 0.1030282974243164, Classifier Loss 0.11273814737796783, Total Loss 42.4721794128418\n",
      "5: Encoding Loss 5.1096906661987305, Transition Loss -1.9770402908325195, Classifier Loss 0.10020614415407181, Total Loss 40.67797088623047\n",
      "5: Encoding Loss 7.025266170501709, Transition Loss -0.7811578512191772, Classifier Loss 0.15125559270381927, Total Loss 57.27684783935547\n",
      "5: Encoding Loss 6.690670967102051, Transition Loss -1.2870795726776123, Classifier Loss 0.12062466144561768, Total Loss 52.20597839355469\n",
      "5: Encoding Loss 5.8812432289123535, Transition Loss -0.8765734434127808, Classifier Loss 0.12475050240755081, Total Loss 47.76216125488281\n",
      "5: Encoding Loss 4.125039100646973, Transition Loss 0.11386072635650635, Classifier Loss 0.1309029459953308, Total Loss 37.88607406616211\n",
      "5: Encoding Loss 3.215203046798706, Transition Loss -0.8819011449813843, Classifier Loss 0.07821004837751389, Total Loss 27.11187171936035\n",
      "5: Encoding Loss 5.661870002746582, Transition Loss -1.825149416923523, Classifier Loss 0.08329097926616669, Total Loss 42.299591064453125\n",
      "5: Encoding Loss 4.708662986755371, Transition Loss -1.3260295391082764, Classifier Loss 0.1031244695186615, Total Loss 38.56389617919922\n",
      "5: Encoding Loss 4.565742015838623, Transition Loss -0.7960983514785767, Classifier Loss 0.2123054414987564, Total Loss 48.62468338012695\n",
      "5: Encoding Loss 4.4209065437316895, Transition Loss -2.1624295711517334, Classifier Loss 0.052938513457775116, Total Loss 31.81842803955078\n",
      "5: Encoding Loss 4.393321514129639, Transition Loss -2.5327553749084473, Classifier Loss 0.13420145213603973, Total Loss 39.77906036376953\n",
      "5: Encoding Loss 2.073133945465088, Transition Loss -0.7574845552444458, Classifier Loss 0.07902360707521439, Total Loss 20.34086036682129\n",
      "5: Encoding Loss 5.604826927185059, Transition Loss -0.8106012344360352, Classifier Loss 0.07957060635089874, Total Loss 41.58570098876953\n",
      "5: Encoding Loss 4.578030586242676, Transition Loss -1.399526834487915, Classifier Loss 0.1524403989315033, Total Loss 42.71166229248047\n",
      "5: Encoding Loss 3.68756365776062, Transition Loss -0.8074862360954285, Classifier Loss 0.07605066150426865, Total Loss 29.730127334594727\n",
      "5: Encoding Loss 5.216357231140137, Transition Loss -0.2926267385482788, Classifier Loss 0.05039774626493454, Total Loss 36.33780288696289\n",
      "5: Encoding Loss 7.362729549407959, Transition Loss -1.9337728023529053, Classifier Loss 0.21163541078567505, Total Loss 65.33914947509766\n",
      "5: Encoding Loss 4.942540645599365, Transition Loss -0.5650030374526978, Classifier Loss 0.20575924217700958, Total Loss 50.2309455871582\n",
      "5: Encoding Loss 5.416864395141602, Transition Loss -1.068685531616211, Classifier Loss 0.21029403805732727, Total Loss 53.5301628112793\n",
      "5: Encoding Loss 4.591635704040527, Transition Loss -1.7057929039001465, Classifier Loss 0.10169794410467148, Total Loss 37.718929290771484\n",
      "5: Encoding Loss 6.59130859375, Transition Loss -1.6086862087249756, Classifier Loss 0.16151981055736542, Total Loss 55.699188232421875\n",
      "5: Encoding Loss 5.9141130447387695, Transition Loss -1.02105712890625, Classifier Loss 0.11090430617332458, Total Loss 46.574703216552734\n",
      "5: Encoding Loss 5.637176990509033, Transition Loss -0.5500500202178955, Classifier Loss 0.0862678810954094, Total Loss 42.44963073730469\n",
      "5: Encoding Loss 4.262870788574219, Transition Loss -1.8263564109802246, Classifier Loss 0.11422335356473923, Total Loss 36.99882888793945\n",
      "5: Encoding Loss 6.384398460388184, Transition Loss -0.12853360176086426, Classifier Loss 0.10258571803569794, Total Loss 48.56491470336914\n",
      "5: Encoding Loss 4.973388671875, Transition Loss -1.4274510145187378, Classifier Loss 0.11198793351650238, Total Loss 41.03855514526367\n",
      "5: Encoding Loss 5.787679672241211, Transition Loss -1.3808777332305908, Classifier Loss 0.11508991569280624, Total Loss 46.23451614379883\n",
      "5: Encoding Loss 6.203604698181152, Transition Loss -0.6205627918243408, Classifier Loss 0.10285595804452896, Total Loss 47.50697708129883\n",
      "5: Encoding Loss 4.806674480438232, Transition Loss -1.7470227479934692, Classifier Loss 0.06454083323478699, Total Loss 35.293434143066406\n",
      "5: Encoding Loss 4.133129596710205, Transition Loss -1.2247536182403564, Classifier Loss 0.10939313471317291, Total Loss 35.73760223388672\n",
      "5: Encoding Loss 3.7841179370880127, Transition Loss -1.2041256427764893, Classifier Loss 0.10273316502571106, Total Loss 32.977542877197266\n",
      "5: Encoding Loss 7.5795440673828125, Transition Loss -2.3009865283966064, Classifier Loss 0.09915812313556671, Total Loss 55.39215850830078\n",
      "5: Encoding Loss 4.3001933097839355, Transition Loss -3.2014689445495605, Classifier Loss 0.07734064012765884, Total Loss 33.53394317626953\n",
      "5: Encoding Loss 3.0826008319854736, Transition Loss -1.1357700824737549, Classifier Loss 0.11454940587282181, Total Loss 29.950092315673828\n",
      "5: Encoding Loss 10.016592979431152, Transition Loss 0.2565811276435852, Classifier Loss 0.19425976276397705, Total Loss 79.62816619873047\n",
      "5: Encoding Loss 7.15105676651001, Transition Loss -0.20673403143882751, Classifier Loss 0.12683191895484924, Total Loss 55.58945083618164\n",
      "5: Encoding Loss 4.8206257820129395, Transition Loss -2.4697537422180176, Classifier Loss 0.09294170886278152, Total Loss 38.21693801879883\n",
      "5: Encoding Loss 4.207290172576904, Transition Loss -0.6654391884803772, Classifier Loss 0.09052597731351852, Total Loss 34.29607391357422\n",
      "5: Encoding Loss 7.094300270080566, Transition Loss -2.375842571258545, Classifier Loss 0.2082192599773407, Total Loss 63.38677978515625\n",
      "5: Encoding Loss 4.900078296661377, Transition Loss -1.0811786651611328, Classifier Loss 0.08613840490579605, Total Loss 38.01388168334961\n",
      "5: Encoding Loss 4.356048107147217, Transition Loss -1.0774626731872559, Classifier Loss 0.08771821856498718, Total Loss 34.90768051147461\n",
      "5: Encoding Loss 4.520515441894531, Transition Loss -1.4752073287963867, Classifier Loss 0.144651859998703, Total Loss 41.58768844604492\n",
      "5: Encoding Loss 6.235886096954346, Transition Loss -0.6603325009346008, Classifier Loss 0.0560130849480629, Total Loss 43.016361236572266\n",
      "5: Encoding Loss 4.730518341064453, Transition Loss -1.4645880460739136, Classifier Loss 0.09681650251150131, Total Loss 38.06417465209961\n",
      "5: Encoding Loss 7.047338962554932, Transition Loss -1.9320064783096313, Classifier Loss 0.1039731353521347, Total Loss 52.680572509765625\n",
      "5: Encoding Loss 6.040775299072266, Transition Loss -0.6065016984939575, Classifier Loss 0.09003432095050812, Total Loss 45.247840881347656\n",
      "5: Encoding Loss 5.6054253578186035, Transition Loss -0.37723127007484436, Classifier Loss 0.14862097799777985, Total Loss 48.49449920654297\n",
      "5: Encoding Loss 7.359808444976807, Transition Loss -1.0413038730621338, Classifier Loss 0.07049356400966644, Total Loss 51.20779037475586\n",
      "5: Encoding Loss 5.952258110046387, Transition Loss -1.1162819862365723, Classifier Loss 0.19310423731803894, Total Loss 55.02352523803711\n",
      "5: Encoding Loss 5.1322221755981445, Transition Loss -0.5764744281768799, Classifier Loss 0.12372434139251709, Total Loss 43.1655387878418\n",
      "5: Encoding Loss 3.3295066356658936, Transition Loss -1.020762324333191, Classifier Loss 0.07523397356271744, Total Loss 27.500030517578125\n",
      "5: Encoding Loss 4.838413238525391, Transition Loss -1.5175375938415527, Classifier Loss 0.09179583191871643, Total Loss 38.20945739746094\n",
      "5: Encoding Loss 5.214905261993408, Transition Loss -0.34144043922424316, Classifier Loss 0.10359075665473938, Total Loss 41.64836883544922\n",
      "5: Encoding Loss 4.5252299308776855, Transition Loss 0.28827354311943054, Classifier Loss 0.12413573265075684, Total Loss 39.68026351928711\n",
      "5: Encoding Loss 4.325813293457031, Transition Loss -1.664546251296997, Classifier Loss 0.08478514850139618, Total Loss 34.4327278137207\n",
      "5: Encoding Loss 3.205341339111328, Transition Loss -2.585580825805664, Classifier Loss 0.08954760432243347, Total Loss 28.185775756835938\n",
      "5: Encoding Loss 6.428248405456543, Transition Loss -1.901277780532837, Classifier Loss 0.11806381493806839, Total Loss 50.37511444091797\n",
      "5: Encoding Loss 5.521974086761475, Transition Loss -2.0568454265594482, Classifier Loss 0.17378400266170502, Total Loss 50.509422302246094\n",
      "5: Encoding Loss 5.631857872009277, Transition Loss -0.10757741332054138, Classifier Loss 0.11747517436742783, Total Loss 45.53862380981445\n",
      "5: Encoding Loss 5.287823677062988, Transition Loss -0.9746602177619934, Classifier Loss 0.12915751338005066, Total Loss 44.642303466796875\n",
      "5: Encoding Loss 6.269282817840576, Transition Loss -0.618299126625061, Classifier Loss 0.1729264259338379, Total Loss 54.9080924987793\n",
      "5: Encoding Loss 5.510258674621582, Transition Loss -0.6576378345489502, Classifier Loss 0.1391003429889679, Total Loss 46.9713249206543\n",
      "5: Encoding Loss 4.948923110961914, Transition Loss -1.1081597805023193, Classifier Loss 0.10116829723119736, Total Loss 39.80992889404297\n",
      "5: Encoding Loss 5.232814788818359, Transition Loss -2.7257823944091797, Classifier Loss 0.09436193108558655, Total Loss 40.831993103027344\n",
      "5: Encoding Loss 4.473719120025635, Transition Loss -0.9407093524932861, Classifier Loss 0.06859679520130157, Total Loss 33.70161819458008\n",
      "5: Encoding Loss 5.072776794433594, Transition Loss -1.5068931579589844, Classifier Loss 0.12424714863300323, Total Loss 42.860774993896484\n",
      "5: Encoding Loss 5.946120738983154, Transition Loss -2.7367351055145264, Classifier Loss 0.06754960119724274, Total Loss 42.43059158325195\n",
      "5: Encoding Loss 6.82781982421875, Transition Loss -0.8388074636459351, Classifier Loss 0.06897515803575516, Total Loss 47.864097595214844\n",
      "5: Encoding Loss 3.445044755935669, Transition Loss -0.7646182775497437, Classifier Loss 0.08261828869581223, Total Loss 28.931793212890625\n",
      "5: Encoding Loss 6.007850170135498, Transition Loss -1.4266808032989502, Classifier Loss 0.07016462087631226, Total Loss 43.06299591064453\n",
      "5: Encoding Loss 5.987391948699951, Transition Loss -1.2374558448791504, Classifier Loss 0.0809936448931694, Total Loss 44.023223876953125\n",
      "5: Encoding Loss 4.118170738220215, Transition Loss -0.6746585369110107, Classifier Loss 0.04446808993816376, Total Loss 29.15556526184082\n",
      "5: Encoding Loss 5.2225847244262695, Transition Loss -2.6207706928253174, Classifier Loss 0.14159703254699707, Total Loss 45.494163513183594\n",
      "5: Encoding Loss 6.973321437835693, Transition Loss -0.3867413401603699, Classifier Loss 0.10473841428756714, Total Loss 52.31361770629883\n",
      "5: Encoding Loss 6.5920305252075195, Transition Loss -1.0299079418182373, Classifier Loss 0.1402113437652588, Total Loss 53.572906494140625\n",
      "5: Encoding Loss 5.29447078704834, Transition Loss -1.4055099487304688, Classifier Loss 0.08136864751577377, Total Loss 39.90312957763672\n",
      "5: Encoding Loss 5.265165328979492, Transition Loss -1.5255509614944458, Classifier Loss 0.09747222065925598, Total Loss 41.33760452270508\n",
      "5: Encoding Loss 5.2692952156066895, Transition Loss -0.7565754652023315, Classifier Loss 0.1823878288269043, Total Loss 49.854251861572266\n",
      "5: Encoding Loss 6.245748043060303, Transition Loss -1.5866999626159668, Classifier Loss 0.08001717925071716, Total Loss 45.4755744934082\n",
      "5: Encoding Loss 3.329833984375, Transition Loss -1.1816487312316895, Classifier Loss 0.14336515963077545, Total Loss 34.31504821777344\n",
      "5: Encoding Loss 8.481630325317383, Transition Loss 0.2696610391139984, Classifier Loss 0.14572255313396454, Total Loss 65.56990051269531\n",
      "5: Encoding Loss 5.336899757385254, Transition Loss -1.0815348625183105, Classifier Loss 0.0598783865571022, Total Loss 38.00880813598633\n",
      "5: Encoding Loss 3.1235649585723877, Transition Loss -2.2243459224700928, Classifier Loss 0.07725831866264343, Total Loss 26.466333389282227\n",
      "5: Encoding Loss 6.322458744049072, Transition Loss -0.2524220645427704, Classifier Loss 0.06068333610892296, Total Loss 44.002986907958984\n",
      "5: Encoding Loss 3.0132689476013184, Transition Loss -0.8885853886604309, Classifier Loss 0.05219445750117302, Total Loss 23.2987060546875\n",
      "5: Encoding Loss 7.341780185699463, Transition Loss -1.6661670207977295, Classifier Loss 0.14547428488731384, Total Loss 58.597442626953125\n",
      "5: Encoding Loss 7.309237480163574, Transition Loss -1.0615496635437012, Classifier Loss 0.12026090919971466, Total Loss 55.8810920715332\n",
      "5: Encoding Loss 6.048731803894043, Transition Loss -2.038628339767456, Classifier Loss 0.0810682401061058, Total Loss 44.398399353027344\n",
      "5: Encoding Loss 4.0590901374816895, Transition Loss -1.0757765769958496, Classifier Loss 0.13644981384277344, Total Loss 37.99909210205078\n",
      "5: Encoding Loss 4.449082851409912, Transition Loss -1.5249576568603516, Classifier Loss 0.10996061563491821, Total Loss 37.68994903564453\n",
      "5: Encoding Loss 5.549141883850098, Transition Loss -1.5863815546035767, Classifier Loss 0.09288059920072556, Total Loss 42.582279205322266\n",
      "5: Encoding Loss 3.6183433532714844, Transition Loss -0.8382527828216553, Classifier Loss 0.1005098968744278, Total Loss 31.76071548461914\n",
      "5: Encoding Loss 6.495033264160156, Transition Loss -2.059603214263916, Classifier Loss 0.18413135409355164, Total Loss 57.382511138916016\n",
      "5: Encoding Loss 4.328453540802002, Transition Loss -1.8694822788238525, Classifier Loss 0.07954001426696777, Total Loss 33.92397689819336\n",
      "5: Encoding Loss 3.4797282218933105, Transition Loss -0.7223626375198364, Classifier Loss 0.07854299247264862, Total Loss 28.73238182067871\n",
      "5: Encoding Loss 5.785403728485107, Transition Loss -0.03406152129173279, Classifier Loss 0.15274330973625183, Total Loss 49.98674011230469\n",
      "5: Encoding Loss 5.058506011962891, Transition Loss -1.692710041999817, Classifier Loss 0.09393642097711563, Total Loss 39.7440071105957\n",
      "5: Encoding Loss 7.302573204040527, Transition Loss -0.6731144189834595, Classifier Loss 0.11135336011648178, Total Loss 54.950504302978516\n",
      "5: Encoding Loss 4.035796165466309, Transition Loss -2.3896172046661377, Classifier Loss 0.19960878789424896, Total Loss 44.17470169067383\n",
      "5: Encoding Loss 6.803804874420166, Transition Loss -1.1204142570495605, Classifier Loss 0.1124434843659401, Total Loss 52.06673049926758\n",
      "5: Encoding Loss 4.767714023590088, Transition Loss -1.1934986114501953, Classifier Loss 0.06097376346588135, Total Loss 34.70318603515625\n",
      "5: Encoding Loss 8.36277961730957, Transition Loss -0.3348337411880493, Classifier Loss 0.1527012139558792, Total Loss 65.44666290283203\n",
      "5: Encoding Loss 5.454392433166504, Transition Loss -1.2383942604064941, Classifier Loss 0.143796905875206, Total Loss 47.105552673339844\n",
      "5: Encoding Loss 4.351838111877441, Transition Loss -1.0525048971176147, Classifier Loss 0.10297401994466782, Total Loss 36.40801239013672\n",
      "5: Encoding Loss 5.196592807769775, Transition Loss -2.6661672592163086, Classifier Loss 0.09792550653219223, Total Loss 40.971038818359375\n",
      "5: Encoding Loss 3.6327731609344482, Transition Loss 0.4762050211429596, Classifier Loss 0.12552742660045624, Total Loss 34.53986740112305\n",
      "5: Encoding Loss 3.5642826557159424, Transition Loss -0.28950807452201843, Classifier Loss 0.06066019460558891, Total Loss 27.45159912109375\n",
      "5: Encoding Loss 4.8995256423950195, Transition Loss -2.413405418395996, Classifier Loss 0.15509653091430664, Total Loss 44.90584182739258\n",
      "5: Encoding Loss 5.649536609649658, Transition Loss -1.6924943923950195, Classifier Loss 0.1655750870704651, Total Loss 50.45405197143555\n",
      "5: Encoding Loss 4.288100242614746, Transition Loss -0.6399826407432556, Classifier Loss 0.10203548520803452, Total Loss 35.9318962097168\n",
      "5: Encoding Loss 4.600262641906738, Transition Loss -0.5907836556434631, Classifier Loss 0.08841920644044876, Total Loss 36.443260192871094\n",
      "5: Encoding Loss 4.303717136383057, Transition Loss -1.175469160079956, Classifier Loss 0.056000445038080215, Total Loss 31.421876907348633\n",
      "5: Encoding Loss 7.06881046295166, Transition Loss -0.1695062518119812, Classifier Loss 0.2505025267601013, Total Loss 67.46305084228516\n",
      "5: Encoding Loss 5.336486339569092, Transition Loss -1.27497398853302, Classifier Loss 0.08612410724163055, Total Loss 40.630821228027344\n",
      "5: Encoding Loss 7.002254486083984, Transition Loss -1.5418680906295776, Classifier Loss 0.08927024155855179, Total Loss 50.93993377685547\n",
      "5: Encoding Loss 4.21334171295166, Transition Loss -0.121318519115448, Classifier Loss 0.07301945984363556, Total Loss 32.581947326660156\n",
      "5: Encoding Loss 5.7801923751831055, Transition Loss -1.5798276662826538, Classifier Loss 0.05267738550901413, Total Loss 39.94826126098633\n",
      "5: Encoding Loss 5.2070698738098145, Transition Loss -0.8620443344116211, Classifier Loss 0.10089714080095291, Total Loss 41.331790924072266\n",
      "5: Encoding Loss 4.9286932945251465, Transition Loss -1.320077657699585, Classifier Loss 0.21409928798675537, Total Loss 50.981563568115234\n",
      "5: Encoding Loss 8.647725105285645, Transition Loss -0.3673739433288574, Classifier Loss 0.22784289717674255, Total Loss 74.67049407958984\n",
      "5: Encoding Loss 7.0587029457092285, Transition Loss -1.3316956758499146, Classifier Loss 0.10093113034963608, Total Loss 52.44479751586914\n",
      "5: Encoding Loss 6.808976173400879, Transition Loss -1.3098163604736328, Classifier Loss 0.17993715405464172, Total Loss 58.847049713134766\n",
      "5: Encoding Loss 4.883578300476074, Transition Loss -1.7611420154571533, Classifier Loss 0.05610648915171623, Total Loss 34.911415100097656\n",
      "5: Encoding Loss 5.469070911407471, Transition Loss -1.132563591003418, Classifier Loss 0.0967869982123375, Total Loss 42.492671966552734\n",
      "5: Encoding Loss 6.911994934082031, Transition Loss -1.1106069087982178, Classifier Loss 0.1102370023727417, Total Loss 52.4952278137207\n",
      "5: Encoding Loss 5.333049774169922, Transition Loss -1.354262113571167, Classifier Loss 0.12902317941188812, Total Loss 44.90007781982422\n",
      "5: Encoding Loss 3.3539295196533203, Transition Loss 0.28420308232307434, Classifier Loss 0.05327838286757469, Total Loss 25.56509780883789\n",
      "5: Encoding Loss 4.071166038513184, Transition Loss -0.765374481678009, Classifier Loss 0.0964992418885231, Total Loss 34.07661819458008\n",
      "5: Encoding Loss 4.631272792816162, Transition Loss 0.11893083155155182, Classifier Loss 0.09386417269706726, Total Loss 37.22163009643555\n",
      "5: Encoding Loss 5.02350378036499, Transition Loss -1.3963457345962524, Classifier Loss 0.06848188489675522, Total Loss 36.98865509033203\n",
      "5: Encoding Loss 6.9224138259887695, Transition Loss -0.40097588300704956, Classifier Loss 0.20665086805820465, Total Loss 62.19940948486328\n",
      "5: Encoding Loss 5.295559883117676, Transition Loss -1.7131097316741943, Classifier Loss 0.0867047905921936, Total Loss 40.443153381347656\n",
      "5: Encoding Loss 8.480876922607422, Transition Loss -2.7336010932922363, Classifier Loss 0.12841378152370453, Total Loss 63.725547790527344\n",
      "5: Encoding Loss 6.126990795135498, Transition Loss -1.587895393371582, Classifier Loss 0.0936923399567604, Total Loss 46.13054275512695\n",
      "5: Encoding Loss 3.6457667350769043, Transition Loss -1.0758146047592163, Classifier Loss 0.09930838644504547, Total Loss 31.805007934570312\n",
      "5: Encoding Loss 3.9548065662384033, Transition Loss -0.422071635723114, Classifier Loss 0.03599872440099716, Total Loss 27.328542709350586\n",
      "5: Encoding Loss 6.390347003936768, Transition Loss -2.9464519023895264, Classifier Loss 0.25173595547676086, Total Loss 63.51449966430664\n",
      "5: Encoding Loss 5.282929420471191, Transition Loss -1.4475356340408325, Classifier Loss 0.1560320258140564, Total Loss 47.300201416015625\n",
      "5: Encoding Loss 3.728776454925537, Transition Loss 0.7317512631416321, Classifier Loss 0.06629446893930435, Total Loss 29.2948055267334\n",
      "5: Encoding Loss 5.516068458557129, Transition Loss -0.9814795255661011, Classifier Loss 0.23716583847999573, Total Loss 56.81260299682617\n",
      "5: Encoding Loss 4.439539909362793, Transition Loss -1.6931945085525513, Classifier Loss 0.1374230682849884, Total Loss 40.378868103027344\n",
      "5: Encoding Loss 6.204327583312988, Transition Loss -0.8286999464035034, Classifier Loss 0.1708066463470459, Total Loss 54.30630111694336\n",
      "5: Encoding Loss 3.2279655933380127, Transition Loss -0.46723660826683044, Classifier Loss 0.0703551173210144, Total Loss 26.403118133544922\n",
      "5: Encoding Loss 3.076780080795288, Transition Loss -0.6949453353881836, Classifier Loss 0.16323697566986084, Total Loss 34.78409957885742\n",
      "5: Encoding Loss 5.79921293258667, Transition Loss -2.2829599380493164, Classifier Loss 0.12266434729099274, Total Loss 47.0608024597168\n",
      "5: Encoding Loss 5.0714111328125, Transition Loss -1.486221432685852, Classifier Loss 0.16842465102672577, Total Loss 47.27033996582031\n",
      "5: Encoding Loss 5.853323459625244, Transition Loss -2.17299222946167, Classifier Loss 0.14897821843624115, Total Loss 50.01689147949219\n",
      "5: Encoding Loss 5.773503303527832, Transition Loss -1.7287901639938354, Classifier Loss 0.05747847259044647, Total Loss 40.388179779052734\n",
      "5: Encoding Loss 5.477855682373047, Transition Loss -2.5877902507781982, Classifier Loss 0.12078963220119476, Total Loss 44.945064544677734\n",
      "5: Encoding Loss 7.146299839019775, Transition Loss -1.1192355155944824, Classifier Loss 0.1646525263786316, Total Loss 59.34260559082031\n",
      "5: Encoding Loss 6.2559814453125, Transition Loss -1.6066845655441284, Classifier Loss 0.168767049908638, Total Loss 54.41195297241211\n",
      "5: Encoding Loss 6.228724479675293, Transition Loss -0.8810250759124756, Classifier Loss 0.08773726224899292, Total Loss 46.14572525024414\n",
      "5: Encoding Loss 4.045497894287109, Transition Loss -2.221978187561035, Classifier Loss 0.09205775707960129, Total Loss 33.477874755859375\n",
      "5: Encoding Loss 6.624384880065918, Transition Loss -1.5711686611175537, Classifier Loss 0.11157923191785812, Total Loss 50.90360641479492\n",
      "5: Encoding Loss 4.572077751159668, Transition Loss -2.1656360626220703, Classifier Loss 0.09019278734922409, Total Loss 36.45088195800781\n",
      "5: Encoding Loss 5.842240810394287, Transition Loss -1.7976633310317993, Classifier Loss 0.057313788682222366, Total Loss 40.78411102294922\n",
      "5: Encoding Loss 4.642535209655762, Transition Loss -1.000241756439209, Classifier Loss 0.0863654762506485, Total Loss 36.49135971069336\n",
      "5: Encoding Loss 5.3531599044799805, Transition Loss -1.5924618244171143, Classifier Loss 0.1459086537361145, Total Loss 46.709190368652344\n",
      "5: Encoding Loss 4.088624954223633, Transition Loss 0.10496750473976135, Classifier Loss 0.06567294150590897, Total Loss 31.14103126525879\n",
      "5: Encoding Loss 8.674955368041992, Transition Loss -1.304283857345581, Classifier Loss 0.18012405931949615, Total Loss 70.0616226196289\n",
      "5: Encoding Loss 8.086660385131836, Transition Loss -2.0800161361694336, Classifier Loss 0.16860726475715637, Total Loss 65.3798599243164\n",
      "5: Encoding Loss 7.460142612457275, Transition Loss -1.1592378616333008, Classifier Loss 0.1902141571044922, Total Loss 63.78180694580078\n",
      "5: Encoding Loss 6.115357398986816, Transition Loss -0.32649749517440796, Classifier Loss 0.14214909076690674, Total Loss 50.906925201416016\n",
      "5: Encoding Loss 7.349542140960693, Transition Loss -0.23960447311401367, Classifier Loss 0.23433326184749603, Total Loss 67.53047943115234\n",
      "5: Encoding Loss 4.507790565490723, Transition Loss 0.10304887592792511, Classifier Loss 0.06664372980594635, Total Loss 33.75233459472656\n",
      "5: Encoding Loss 3.7687652111053467, Transition Loss -0.92168790102005, Classifier Loss 0.11367218941450119, Total Loss 33.97944259643555\n",
      "5: Encoding Loss 2.652505874633789, Transition Loss 0.28453758358955383, Classifier Loss 0.10493520647287369, Total Loss 26.52237319946289\n",
      "5: Encoding Loss 5.359745502471924, Transition Loss -0.6894130706787109, Classifier Loss 0.14229333400726318, Total Loss 46.387535095214844\n",
      "5: Encoding Loss 6.114610195159912, Transition Loss -1.6981942653656006, Classifier Loss 0.11213476955890656, Total Loss 47.90046310424805\n",
      "5: Encoding Loss 7.035032749176025, Transition Loss -1.5976353883743286, Classifier Loss 0.19628757238388062, Total Loss 61.838314056396484\n",
      "5: Encoding Loss 4.30405855178833, Transition Loss -1.6140730381011963, Classifier Loss 0.08705178648233414, Total Loss 34.52888488769531\n",
      "5: Encoding Loss 6.492642402648926, Transition Loss -2.277378559112549, Classifier Loss 0.16853757202625275, Total Loss 55.8087043762207\n",
      "5: Encoding Loss 4.808399677276611, Transition Loss -1.150168776512146, Classifier Loss 0.0853772684931755, Total Loss 37.387664794921875\n",
      "5: Encoding Loss 5.040141582489014, Transition Loss -1.6692577600479126, Classifier Loss 0.11086766421794891, Total Loss 41.32695007324219\n",
      "5: Encoding Loss 4.572442054748535, Transition Loss -3.085460662841797, Classifier Loss 0.08555937558412552, Total Loss 35.98935317993164\n",
      "5: Encoding Loss 2.66291880607605, Transition Loss -1.7667508125305176, Classifier Loss 0.0852716937661171, Total Loss 24.50397491455078\n",
      "5: Encoding Loss 5.705145835876465, Transition Loss -1.6913836002349854, Classifier Loss 0.09839888662099838, Total Loss 44.070091247558594\n",
      "5: Encoding Loss 5.3774590492248535, Transition Loss -1.851304292678833, Classifier Loss 0.16119679808616638, Total Loss 48.383697509765625\n",
      "5: Encoding Loss 3.154212236404419, Transition Loss -1.649041771888733, Classifier Loss 0.08625262975692749, Total Loss 27.549877166748047\n",
      "5: Encoding Loss 7.299421787261963, Transition Loss -2.2242209911346436, Classifier Loss 0.09235228598117828, Total Loss 53.0308723449707\n",
      "5: Encoding Loss 4.9979705810546875, Transition Loss -0.9494287371635437, Classifier Loss 0.09434995800256729, Total Loss 39.42243957519531\n",
      "5: Encoding Loss 6.69616174697876, Transition Loss -1.1195075511932373, Classifier Loss 0.125728040933609, Total Loss 52.74932861328125\n",
      "5: Encoding Loss 6.930445194244385, Transition Loss -1.4958025217056274, Classifier Loss 0.16860705614089966, Total Loss 58.442779541015625\n",
      "5: Encoding Loss 6.457884311676025, Transition Loss -0.8437391519546509, Classifier Loss 0.12271612882614136, Total Loss 51.018585205078125\n",
      "5: Encoding Loss 4.829556465148926, Transition Loss -1.5358142852783203, Classifier Loss 0.12192182242870331, Total Loss 41.168907165527344\n",
      "5: Encoding Loss 6.162510871887207, Transition Loss -1.7714601755142212, Classifier Loss 0.12252376973628998, Total Loss 49.22673416137695\n",
      "5: Encoding Loss 5.153118133544922, Transition Loss -1.5728518962860107, Classifier Loss 0.08708492666482925, Total Loss 39.6265754699707\n",
      "5: Encoding Loss 2.714669704437256, Transition Loss -0.24710270762443542, Classifier Loss 0.04886214807629585, Total Loss 21.174135208129883\n",
      "5: Encoding Loss 7.605430603027344, Transition Loss -0.27047741413116455, Classifier Loss 0.09761565178632736, Total Loss 55.39404296875\n",
      "5: Encoding Loss 6.196889877319336, Transition Loss -1.2955026626586914, Classifier Loss 0.0883893370628357, Total Loss 46.019752502441406\n",
      "5: Encoding Loss 6.184578895568848, Transition Loss -1.038590908050537, Classifier Loss 0.04618779942393303, Total Loss 41.7258415222168\n",
      "5: Encoding Loss 4.245425701141357, Transition Loss -0.1634076088666916, Classifier Loss 0.06582973897457123, Total Loss 32.05546569824219\n",
      "5: Encoding Loss 4.290307998657227, Transition Loss -2.0397801399230957, Classifier Loss 0.049685291945934296, Total Loss 30.709562301635742\n",
      "5: Encoding Loss 4.199471473693848, Transition Loss -1.6893281936645508, Classifier Loss 0.14200375974178314, Total Loss 39.39653015136719\n",
      "5: Encoding Loss 5.856971263885498, Transition Loss -0.3751845359802246, Classifier Loss 0.06605701893568039, Total Loss 41.74738311767578\n",
      "5: Encoding Loss 4.781653881072998, Transition Loss -1.048030138015747, Classifier Loss 0.06390449404716492, Total Loss 35.0799560546875\n",
      "5: Encoding Loss 4.447640895843506, Transition Loss -0.6020883917808533, Classifier Loss 0.06196650490164757, Total Loss 32.88225555419922\n",
      "5: Encoding Loss 3.7185447216033936, Transition Loss -0.9164966344833374, Classifier Loss 0.08246541768312454, Total Loss 30.557445526123047\n",
      "5: Encoding Loss 6.259631156921387, Transition Loss -1.407951831817627, Classifier Loss 0.09068616479635239, Total Loss 46.62583923339844\n",
      "5: Encoding Loss 4.45020055770874, Transition Loss -1.8068923950195312, Classifier Loss 0.09501651674509048, Total Loss 36.2021369934082\n",
      "5: Encoding Loss 4.017329216003418, Transition Loss -2.3423171043395996, Classifier Loss 0.0707501694560051, Total Loss 31.178056716918945\n",
      "5: Encoding Loss 3.7324378490448, Transition Loss -1.3914018869400024, Classifier Loss 0.1259414702653885, Total Loss 34.98822021484375\n",
      "5: Encoding Loss 6.415369510650635, Transition Loss -0.8098512887954712, Classifier Loss 0.1153385266661644, Total Loss 50.0257453918457\n",
      "5: Encoding Loss 3.7497153282165527, Transition Loss -0.8972020745277405, Classifier Loss 0.07203841209411621, Total Loss 29.70177459716797\n",
      "5: Encoding Loss 4.45035457611084, Transition Loss -1.1353309154510498, Classifier Loss 0.11253925412893295, Total Loss 37.95560073852539\n",
      "5: Encoding Loss 5.023872375488281, Transition Loss -2.655714988708496, Classifier Loss 0.09975792467594147, Total Loss 40.11796951293945\n",
      "5: Encoding Loss 4.533716201782227, Transition Loss -1.115615725517273, Classifier Loss 0.05599001795053482, Total Loss 32.80085372924805\n",
      "5: Encoding Loss 3.6626744270324707, Transition Loss -0.7778232097625732, Classifier Loss 0.07539970427751541, Total Loss 29.51570701599121\n",
      "5: Encoding Loss 4.554844856262207, Transition Loss -1.6544156074523926, Classifier Loss 0.16995196044445038, Total Loss 44.323604583740234\n",
      "5: Encoding Loss 3.356156587600708, Transition Loss -2.283531904220581, Classifier Loss 0.0925801619887352, Total Loss 29.39404296875\n",
      "5: Encoding Loss 3.574047803878784, Transition Loss -0.9425902366638184, Classifier Loss 0.0918751135468483, Total Loss 30.63142204284668\n",
      "5: Encoding Loss 4.9783830642700195, Transition Loss -2.2945077419281006, Classifier Loss 0.09042131155729294, Total Loss 38.9115104675293\n",
      "5: Encoding Loss 5.476630210876465, Transition Loss -1.0497068166732788, Classifier Loss 0.16707175970077515, Total Loss 49.5665397644043\n",
      "5: Encoding Loss 5.766366004943848, Transition Loss -1.528009295463562, Classifier Loss 0.12154962122440338, Total Loss 46.75254821777344\n",
      "5: Encoding Loss 3.451204776763916, Transition Loss -2.021683692932129, Classifier Loss 0.09532913565635681, Total Loss 30.239334106445312\n",
      "5: Encoding Loss 6.527334690093994, Transition Loss -1.1608365774154663, Classifier Loss 0.18707303702831268, Total Loss 57.870849609375\n",
      "5: Encoding Loss 7.175743103027344, Transition Loss -1.837107539176941, Classifier Loss 0.2534046471118927, Total Loss 68.39419555664062\n",
      "5: Encoding Loss 7.4486403465271, Transition Loss -1.732853889465332, Classifier Loss 0.14391888678073883, Total Loss 59.083038330078125\n",
      "5: Encoding Loss 7.211331367492676, Transition Loss -2.2824623584747314, Classifier Loss 0.1341599076986313, Total Loss 56.68307113647461\n",
      "5: Encoding Loss 4.5167999267578125, Transition Loss -2.812272548675537, Classifier Loss 0.09736019372940063, Total Loss 36.835697174072266\n",
      "5: Encoding Loss 7.957171440124512, Transition Loss -2.28971529006958, Classifier Loss 0.10034799575805664, Total Loss 57.77691650390625\n",
      "5: Encoding Loss 7.948057651519775, Transition Loss -0.28108111023902893, Classifier Loss 0.13987930119037628, Total Loss 61.67616653442383\n",
      "5: Encoding Loss 6.932129859924316, Transition Loss -2.4171857833862305, Classifier Loss 0.2482844740152359, Total Loss 66.42025756835938\n",
      "5: Encoding Loss 7.463076591491699, Transition Loss -1.8525315523147583, Classifier Loss 0.16074596345424652, Total Loss 60.852317810058594\n",
      "5: Encoding Loss 6.544863700866699, Transition Loss 0.30319875478744507, Classifier Loss 0.07269034534692764, Total Loss 46.65950012207031\n",
      "5: Encoding Loss 5.342978000640869, Transition Loss -1.6359741687774658, Classifier Loss 0.049586959183216095, Total Loss 37.015907287597656\n",
      "5: Encoding Loss 2.7104172706604004, Transition Loss -1.3306858539581299, Classifier Loss 0.10380882769823074, Total Loss 26.642854690551758\n",
      "5: Encoding Loss 4.633179187774658, Transition Loss -0.6611326336860657, Classifier Loss 0.09439829736948013, Total Loss 37.238643646240234\n",
      "5: Encoding Loss 4.040417671203613, Transition Loss 0.4725044369697571, Classifier Loss 0.08226349204778671, Total Loss 32.657859802246094\n",
      "5: Encoding Loss 2.10573148727417, Transition Loss -1.2178359031677246, Classifier Loss 0.04912344738841057, Total Loss 17.546247482299805\n",
      "5: Encoding Loss 4.174215316772461, Transition Loss -0.9963377714157104, Classifier Loss 0.17193035781383514, Total Loss 42.23793029785156\n",
      "5: Encoding Loss 4.169792652130127, Transition Loss -0.8958777189254761, Classifier Loss 0.15790006518363953, Total Loss 40.80840301513672\n",
      "5: Encoding Loss 6.802445411682129, Transition Loss -0.2951582074165344, Classifier Loss 0.07269298285245895, Total Loss 48.08385467529297\n",
      "5: Encoding Loss 5.706259727478027, Transition Loss -2.151716709136963, Classifier Loss 0.08802638202905655, Total Loss 43.039337158203125\n",
      "5: Encoding Loss 7.992458343505859, Transition Loss -1.249664545059204, Classifier Loss 0.3061589002609253, Total Loss 78.57013702392578\n",
      "5: Encoding Loss 5.408461570739746, Transition Loss -2.3462820053100586, Classifier Loss 0.15550220012664795, Total Loss 48.00005340576172\n",
      "5: Encoding Loss 4.5195817947387695, Transition Loss -1.4150792360305786, Classifier Loss 0.06058202683925629, Total Loss 33.17512893676758\n",
      "5: Encoding Loss 6.623326301574707, Transition Loss -1.7314271926879883, Classifier Loss 0.16061371564865112, Total Loss 55.800636291503906\n",
      "5: Encoding Loss 6.7558913230896, Transition Loss -1.6686327457427979, Classifier Loss 0.12633147835731506, Total Loss 53.16783142089844\n",
      "5: Encoding Loss 5.194927215576172, Transition Loss 0.05877766013145447, Classifier Loss 0.14009077847003937, Total Loss 45.202152252197266\n",
      "5: Encoding Loss 4.374432563781738, Transition Loss 0.5664479732513428, Classifier Loss 0.09581993520259857, Total Loss 36.05516815185547\n",
      "5: Encoding Loss 5.247381687164307, Transition Loss -2.0481207370758057, Classifier Loss 0.10412769019603729, Total Loss 41.896240234375\n",
      "5: Encoding Loss 6.826170921325684, Transition Loss -1.5254244804382324, Classifier Loss 0.10689006745815277, Total Loss 51.645423889160156\n",
      "5: Encoding Loss 5.280730724334717, Transition Loss -0.38452985882759094, Classifier Loss 0.145497664809227, Total Loss 46.23400115966797\n",
      "5: Encoding Loss 3.208843946456909, Transition Loss -0.4545712172985077, Classifier Loss 0.061231423169374466, Total Loss 25.376026153564453\n",
      "5: Encoding Loss 4.977021217346191, Transition Loss -1.8974496126174927, Classifier Loss 0.1468208283185959, Total Loss 44.543453216552734\n",
      "5: Encoding Loss 3.5823376178741455, Transition Loss 0.6914511322975159, Classifier Loss 0.10977022349834442, Total Loss 32.74762725830078\n",
      "5: Encoding Loss 6.643672943115234, Transition Loss -1.7699471712112427, Classifier Loss 0.1003710925579071, Total Loss 49.8984375\n",
      "5: Encoding Loss 4.111162185668945, Transition Loss -2.1912522315979004, Classifier Loss 0.08870469033718109, Total Loss 33.53656768798828\n",
      "5: Encoding Loss 6.350430965423584, Transition Loss -1.23349130153656, Classifier Loss 0.1237848550081253, Total Loss 50.48058319091797\n",
      "5: Encoding Loss 4.622095108032227, Transition Loss -2.025034189224243, Classifier Loss 0.07893862575292587, Total Loss 35.62562561035156\n",
      "5: Encoding Loss 5.557629585266113, Transition Loss -1.416808843612671, Classifier Loss 0.12376902997493744, Total Loss 45.72211456298828\n",
      "5: Encoding Loss 5.487695693969727, Transition Loss -2.031344175338745, Classifier Loss 0.13123030960559845, Total Loss 46.04839324951172\n",
      "5: Encoding Loss 4.0581374168396, Transition Loss 0.6929784417152405, Classifier Loss 0.10874761641025543, Total Loss 35.50077819824219\n",
      "5: Encoding Loss 2.0310771465301514, Transition Loss -0.7718271613121033, Classifier Loss 0.08400072902441025, Total Loss 20.586227416992188\n",
      "5: Encoding Loss 10.976949691772461, Transition Loss -1.8977534770965576, Classifier Loss 0.12133096903562546, Total Loss 77.9940414428711\n",
      "5: Encoding Loss 8.692829132080078, Transition Loss -1.4478930234909058, Classifier Loss 0.09835104644298553, Total Loss 61.99150466918945\n",
      "5: Encoding Loss 6.143327713012695, Transition Loss -0.9035450220108032, Classifier Loss 0.11900555342435837, Total Loss 48.76015853881836\n",
      "5: Encoding Loss 4.882193565368652, Transition Loss -1.2046434879302979, Classifier Loss 0.07573701441287994, Total Loss 36.86638259887695\n",
      "5: Encoding Loss 3.9133689403533936, Transition Loss -1.1392037868499756, Classifier Loss 0.13166460394859314, Total Loss 36.64622116088867\n",
      "5: Encoding Loss 5.042599678039551, Transition Loss -1.6544103622436523, Classifier Loss 0.16677379608154297, Total Loss 46.932315826416016\n",
      "5: Encoding Loss 3.4207088947296143, Transition Loss 0.15901216864585876, Classifier Loss 0.06875184923410416, Total Loss 27.463043212890625\n",
      "5: Encoding Loss 6.35770320892334, Transition Loss -2.245727300643921, Classifier Loss 0.07141423970460892, Total Loss 45.286746978759766\n",
      "5: Encoding Loss 5.579703330993652, Transition Loss -1.1549320220947266, Classifier Loss 0.04570050910115242, Total Loss 38.04780960083008\n",
      "5: Encoding Loss 5.107176303863525, Transition Loss -1.9113364219665527, Classifier Loss 0.09315483272075653, Total Loss 39.95777893066406\n",
      "5: Encoding Loss 4.174610614776611, Transition Loss -0.9474363327026367, Classifier Loss 0.05151176080107689, Total Loss 30.198461532592773\n",
      "5: Encoding Loss 4.236947536468506, Transition Loss 0.056443795561790466, Classifier Loss 0.15038415789604187, Total Loss 40.48268127441406\n",
      "5: Encoding Loss 5.002041816711426, Transition Loss -1.1305276155471802, Classifier Loss 0.06666441261768341, Total Loss 36.67823791503906\n",
      "5: Encoding Loss 3.67533802986145, Transition Loss -2.4741039276123047, Classifier Loss 0.11498891562223434, Total Loss 33.549930572509766\n",
      "5: Encoding Loss 5.296777725219727, Transition Loss -1.5855052471160889, Classifier Loss 0.21595005691051483, Total Loss 53.375038146972656\n",
      "5: Encoding Loss 5.280774116516113, Transition Loss -1.2824524641036987, Classifier Loss 0.12070880830287933, Total Loss 43.75501251220703\n",
      "5: Encoding Loss 7.3612565994262695, Transition Loss -1.2828571796417236, Classifier Loss 0.1131996363401413, Total Loss 55.48698806762695\n",
      "5: Encoding Loss 5.074673652648926, Transition Loss -0.4561832547187805, Classifier Loss 0.11300177127122879, Total Loss 41.7480354309082\n",
      "5: Encoding Loss 4.278891563415527, Transition Loss 0.07659706473350525, Classifier Loss 0.11888357251882553, Total Loss 37.59234619140625\n",
      "5: Encoding Loss 7.132622718811035, Transition Loss -1.40235435962677, Classifier Loss 0.10864679515361786, Total Loss 53.659854888916016\n",
      "5: Encoding Loss 5.449411392211914, Transition Loss -2.158865451812744, Classifier Loss 0.16395656764507294, Total Loss 49.09126281738281\n",
      "5: Encoding Loss 7.293324947357178, Transition Loss -2.062718391418457, Classifier Loss 0.12906603515148163, Total Loss 56.665733337402344\n",
      "5: Encoding Loss 5.266412734985352, Transition Loss -2.093623399734497, Classifier Loss 0.06515271216630936, Total Loss 38.112911224365234\n",
      "5: Encoding Loss 3.8956122398376465, Transition Loss -2.1082162857055664, Classifier Loss 0.1386096030473709, Total Loss 37.23379135131836\n",
      "5: Encoding Loss 6.605146884918213, Transition Loss -1.080285906791687, Classifier Loss 0.10791987925767899, Total Loss 50.42243957519531\n",
      "5: Encoding Loss 8.608121871948242, Transition Loss -1.3814691305160522, Classifier Loss 0.15750758349895477, Total Loss 67.39894104003906\n",
      "5: Encoding Loss 6.211403846740723, Transition Loss -1.4116322994232178, Classifier Loss 0.0611913725733757, Total Loss 43.38699722290039\n",
      "5: Encoding Loss 4.2168049812316895, Transition Loss -0.2920224368572235, Classifier Loss 0.07642710953950882, Total Loss 32.943424224853516\n",
      "5: Encoding Loss 3.5286660194396973, Transition Loss -1.0029857158660889, Classifier Loss 0.1055915504693985, Total Loss 31.730751037597656\n",
      "5: Encoding Loss 5.970739364624023, Transition Loss -1.4302680492401123, Classifier Loss 0.08738662302494049, Total Loss 44.56252670288086\n",
      "5: Encoding Loss 4.451085567474365, Transition Loss -1.1110234260559082, Classifier Loss 0.04251043125987053, Total Loss 30.95711326599121\n",
      "5: Encoding Loss 6.542169094085693, Transition Loss -2.000685691833496, Classifier Loss 0.05579382926225662, Total Loss 44.831600189208984\n",
      "5: Encoding Loss 3.6356747150421143, Transition Loss -1.8697192668914795, Classifier Loss 0.09851241856813431, Total Loss 31.66454315185547\n",
      "5: Encoding Loss 8.300427436828613, Transition Loss -0.012675344944000244, Classifier Loss 0.25979724526405334, Total Loss 75.78227996826172\n",
      "5: Encoding Loss 5.649125099182129, Transition Loss -1.0717837810516357, Classifier Loss 0.13309071958065033, Total Loss 47.20339584350586\n",
      "5: Encoding Loss 6.323658466339111, Transition Loss -1.2430589199066162, Classifier Loss 0.10797050595283508, Total Loss 48.73850631713867\n",
      "5: Encoding Loss 5.7446746826171875, Transition Loss 0.0879761278629303, Classifier Loss 0.10848414152860641, Total Loss 45.351654052734375\n",
      "5: Encoding Loss 6.34726619720459, Transition Loss -0.5747847557067871, Classifier Loss 0.20121906697750092, Total Loss 58.20527648925781\n",
      "5: Encoding Loss 5.852521896362305, Transition Loss -1.34782075881958, Classifier Loss 0.1776471585035324, Total Loss 52.87930679321289\n",
      "5: Encoding Loss 4.301480770111084, Transition Loss -0.6430215835571289, Classifier Loss 0.09423985332250595, Total Loss 35.23261642456055\n",
      "5: Encoding Loss 4.72356653213501, Transition Loss -2.017620325088501, Classifier Loss 0.13905423879623413, Total Loss 42.24601745605469\n",
      "5: Encoding Loss 5.044731140136719, Transition Loss -0.746664822101593, Classifier Loss 0.13523295521736145, Total Loss 43.791385650634766\n",
      "5: Encoding Loss 5.974124908447266, Transition Loss -0.3365418314933777, Classifier Loss 0.12160134315490723, Total Loss 48.0047492980957\n",
      "5: Encoding Loss 3.4747421741485596, Transition Loss -0.9943082332611084, Classifier Loss 0.06810382753610611, Total Loss 27.658437728881836\n",
      "5: Encoding Loss 3.9974048137664795, Transition Loss -0.9268128871917725, Classifier Loss 0.11728992313146591, Total Loss 35.71305465698242\n",
      "5: Encoding Loss 3.7800731658935547, Transition Loss -1.7390397787094116, Classifier Loss 0.08963619917631149, Total Loss 31.643362045288086\n",
      "5: Encoding Loss 4.875389575958252, Transition Loss -0.7418481111526489, Classifier Loss 0.06358596682548523, Total Loss 35.61063766479492\n",
      "5: Encoding Loss 5.359903812408447, Transition Loss -1.0433886051177979, Classifier Loss 0.12895336747169495, Total Loss 45.054344177246094\n",
      "5: Encoding Loss 4.144802093505859, Transition Loss -2.192781686782837, Classifier Loss 0.09548384696245193, Total Loss 34.41632080078125\n",
      "5: Encoding Loss 4.934592247009277, Transition Loss -1.3385796546936035, Classifier Loss 0.145849347114563, Total Loss 44.19195556640625\n",
      "5: Encoding Loss 3.812453508377075, Transition Loss -0.2750597894191742, Classifier Loss 0.056382738053798676, Total Loss 28.51288414001465\n",
      "5: Encoding Loss 3.3056907653808594, Transition Loss -0.36983317136764526, Classifier Loss 0.06930525600910187, Total Loss 26.764522552490234\n",
      "5: Encoding Loss 4.976198673248291, Transition Loss -0.5823136568069458, Classifier Loss 0.22023531794548035, Total Loss 51.880489349365234\n",
      "5: Encoding Loss 6.009291172027588, Transition Loss -1.6411350965499878, Classifier Loss 0.2002870738506317, Total Loss 56.08380126953125\n",
      "5: Encoding Loss 4.335712432861328, Transition Loss -0.6822370290756226, Classifier Loss 0.13888341188430786, Total Loss 39.90234375\n",
      "5: Encoding Loss 6.146268367767334, Transition Loss -1.6914596557617188, Classifier Loss 0.15086033940315247, Total Loss 51.96297073364258\n",
      "5: Encoding Loss 4.025359153747559, Transition Loss -2.0209903717041016, Classifier Loss 0.09310886263847351, Total Loss 33.46223449707031\n",
      "5: Encoding Loss 5.920117378234863, Transition Loss -1.6472928524017334, Classifier Loss 0.13811467587947845, Total Loss 49.331512451171875\n",
      "5: Encoding Loss 5.334324359893799, Transition Loss -1.0153014659881592, Classifier Loss 0.05788407847285271, Total Loss 37.793949127197266\n",
      "5: Encoding Loss 3.1390140056610107, Transition Loss -1.2241839170455933, Classifier Loss 0.11454782634973526, Total Loss 30.28837776184082\n",
      "5: Encoding Loss 3.9154467582702637, Transition Loss -2.1825318336486816, Classifier Loss 0.07270515710115433, Total Loss 30.7623233795166\n",
      "5: Encoding Loss 3.881183624267578, Transition Loss -0.3096935749053955, Classifier Loss 0.1706385314464569, Total Loss 40.350830078125\n",
      "5: Encoding Loss 7.235908508300781, Transition Loss -0.8457161784172058, Classifier Loss 0.07485035061836243, Total Loss 50.900146484375\n",
      "5: Encoding Loss 5.654683589935303, Transition Loss -1.3288211822509766, Classifier Loss 0.24567446112632751, Total Loss 58.49502182006836\n",
      "5: Encoding Loss 6.4788923263549805, Transition Loss -0.23024466633796692, Classifier Loss 0.14892590045928955, Total Loss 53.76585388183594\n",
      "5: Encoding Loss 5.600879669189453, Transition Loss -2.2151172161102295, Classifier Loss 0.15914767980575562, Total Loss 49.519161224365234\n",
      "5: Encoding Loss 4.958281993865967, Transition Loss -0.9017409086227417, Classifier Loss 0.07089589536190033, Total Loss 36.83892059326172\n",
      "5: Encoding Loss 3.1468796730041504, Transition Loss -2.274197816848755, Classifier Loss 0.08016205579042435, Total Loss 26.896574020385742\n",
      "5: Encoding Loss 2.7553138732910156, Transition Loss -1.2070232629776, Classifier Loss 0.07713325321674347, Total Loss 24.244726181030273\n",
      "5: Encoding Loss 3.7915520668029785, Transition Loss -2.4819250106811523, Classifier Loss 0.08932890743017197, Total Loss 31.68121337890625\n",
      "5: Encoding Loss 6.621729850769043, Transition Loss 0.053601786494255066, Classifier Loss 0.057713139802217484, Total Loss 45.523136138916016\n",
      "5: Encoding Loss 5.560214519500732, Transition Loss -0.3387308418750763, Classifier Loss 0.11819919943809509, Total Loss 45.18107223510742\n",
      "5: Encoding Loss 6.556501865386963, Transition Loss -3.2032723426818848, Classifier Loss 0.08323922753334045, Total Loss 47.661651611328125\n",
      "5: Encoding Loss 5.391053676605225, Transition Loss -1.8673558235168457, Classifier Loss 0.11424899846315384, Total Loss 43.770477294921875\n",
      "5: Encoding Loss 4.254326343536377, Transition Loss -0.11010867357254028, Classifier Loss 0.11592337489128113, Total Loss 37.11825180053711\n",
      "5: Encoding Loss 4.538548469543457, Transition Loss -2.265004873275757, Classifier Loss 0.052940141409635544, Total Loss 32.52439880371094\n",
      "5: Encoding Loss 7.259235382080078, Transition Loss -1.06749427318573, Classifier Loss 0.1335824877023697, Total Loss 56.91323471069336\n",
      "5: Encoding Loss 6.092806816101074, Transition Loss -2.600158214569092, Classifier Loss 0.18314746022224426, Total Loss 54.870548248291016\n",
      "5: Encoding Loss 5.847418308258057, Transition Loss -1.667499303817749, Classifier Loss 0.10584891587495804, Total Loss 45.66873550415039\n",
      "5: Encoding Loss 5.929973602294922, Transition Loss -0.6443219184875488, Classifier Loss 0.17964057624340057, Total Loss 53.54364013671875\n",
      "5: Encoding Loss 6.279423713684082, Transition Loss -1.732105016708374, Classifier Loss 0.04223613813519478, Total Loss 41.89946365356445\n",
      "5: Encoding Loss 5.175191879272461, Transition Loss -1.5795719623565674, Classifier Loss 0.1968158632516861, Total Loss 50.73210906982422\n",
      "5: Encoding Loss 4.110118865966797, Transition Loss -0.6821379661560059, Classifier Loss 0.08751504868268967, Total Loss 33.41194534301758\n",
      "5: Encoding Loss 6.92959451675415, Transition Loss -2.0277533531188965, Classifier Loss 0.12292000651359558, Total Loss 53.86875534057617\n",
      "5: Encoding Loss 5.756929874420166, Transition Loss -0.6968598365783691, Classifier Loss 0.11899680644273758, Total Loss 46.440982818603516\n",
      "5: Encoding Loss 4.759836196899414, Transition Loss -0.6044164896011353, Classifier Loss 0.0874079018831253, Total Loss 37.29956817626953\n",
      "5: Encoding Loss 4.545199394226074, Transition Loss -1.1780133247375488, Classifier Loss 0.13073086738586426, Total Loss 40.34381103515625\n",
      "5: Encoding Loss 5.007486820220947, Transition Loss -0.8483381271362305, Classifier Loss 0.14776067435741425, Total Loss 44.82065200805664\n",
      "5: Encoding Loss 5.525522708892822, Transition Loss -0.5235757827758789, Classifier Loss 0.13620464503765106, Total Loss 46.77339172363281\n",
      "5: Encoding Loss 5.09172248840332, Transition Loss -2.81300950050354, Classifier Loss 0.06562461704015732, Total Loss 37.11167526245117\n",
      "5: Encoding Loss 2.80141019821167, Transition Loss -1.6214336156845093, Classifier Loss 0.10345528274774551, Total Loss 27.153343200683594\n",
      "5: Encoding Loss 9.916813850402832, Transition Loss -2.1162984371185303, Classifier Loss 0.16857022047042847, Total Loss 76.35706329345703\n",
      "5: Encoding Loss 7.1105499267578125, Transition Loss -0.15619340538978577, Classifier Loss 0.04871517792344093, Total Loss 47.53475570678711\n",
      "5: Encoding Loss 5.8051533699035645, Transition Loss -0.8270484209060669, Classifier Loss 0.17601843178272247, Total Loss 52.432430267333984\n",
      "5: Encoding Loss 5.147616863250732, Transition Loss -1.030451774597168, Classifier Loss 0.16315220296382904, Total Loss 47.20050811767578\n",
      "5: Encoding Loss 6.335949897766113, Transition Loss -0.8719193339347839, Classifier Loss 0.07297029346227646, Total Loss 45.312381744384766\n",
      "5: Encoding Loss 6.89888858795166, Transition Loss -1.5068936347961426, Classifier Loss 0.2673988938331604, Total Loss 68.13262176513672\n",
      "5: Encoding Loss 5.041375160217285, Transition Loss -0.3910183906555176, Classifier Loss 0.09863536059856415, Total Loss 40.111629486083984\n",
      "5: Encoding Loss 5.079476833343506, Transition Loss 0.38017919659614563, Classifier Loss 0.12927094101905823, Total Loss 43.5560302734375\n",
      "5: Encoding Loss 5.41428279876709, Transition Loss -0.64518141746521, Classifier Loss 0.10496450215578079, Total Loss 42.98188781738281\n",
      "5: Encoding Loss 4.049708843231201, Transition Loss 0.1997195929288864, Classifier Loss 0.03536354750394821, Total Loss 27.91449546813965\n",
      "5: Encoding Loss 6.712132453918457, Transition Loss -1.2440098524093628, Classifier Loss 0.09568328410387039, Total Loss 49.84062957763672\n",
      "5: Encoding Loss 7.211779594421387, Transition Loss -1.6878595352172852, Classifier Loss 0.2012689709663391, Total Loss 63.39690017700195\n",
      "5: Encoding Loss 4.831114292144775, Transition Loss -1.030653715133667, Classifier Loss 0.08948579430580139, Total Loss 37.934852600097656\n",
      "5: Encoding Loss 3.8032009601593018, Transition Loss -1.8465538024902344, Classifier Loss 0.08689676225185394, Total Loss 31.50814437866211\n",
      "5: Encoding Loss 4.285403251647949, Transition Loss -1.2113027572631836, Classifier Loss 0.08681794255971909, Total Loss 34.39373016357422\n",
      "5: Encoding Loss 4.947951316833496, Transition Loss -0.4188002645969391, Classifier Loss 0.05668359622359276, Total Loss 35.35590362548828\n",
      "5: Encoding Loss 5.609602928161621, Transition Loss -0.988548219203949, Classifier Loss 0.17076840996742249, Total Loss 50.73406219482422\n",
      "5: Encoding Loss 3.4188637733459473, Transition Loss -0.39361700415611267, Classifier Loss 0.14934612810611725, Total Loss 35.44763946533203\n",
      "5: Encoding Loss 5.770319938659668, Transition Loss -2.0472304821014404, Classifier Loss 0.09474924206733704, Total Loss 44.09602737426758\n",
      "5: Encoding Loss 3.6536953449249268, Transition Loss -0.9309147596359253, Classifier Loss 0.05506370961666107, Total Loss 27.428171157836914\n",
      "5: Encoding Loss 4.873970985412598, Transition Loss -2.5960934162139893, Classifier Loss 0.05284322798252106, Total Loss 34.5271110534668\n",
      "5: Encoding Loss 2.4276299476623535, Transition Loss -1.4762085676193237, Classifier Loss 0.048661187291145325, Total Loss 19.431306838989258\n",
      "5: Encoding Loss 6.198746681213379, Transition Loss -1.5895111560821533, Classifier Loss 0.14407795667648315, Total Loss 51.599639892578125\n",
      "5: Encoding Loss 4.35395622253418, Transition Loss -1.761165738105774, Classifier Loss 0.1084030419588089, Total Loss 36.96333694458008\n",
      "5: Encoding Loss 3.946279525756836, Transition Loss -0.6548013687133789, Classifier Loss 0.07997942715883255, Total Loss 31.67535972595215\n",
      "5: Encoding Loss 5.640976905822754, Transition Loss 0.2682031989097595, Classifier Loss 0.10957971960306168, Total Loss 44.91111373901367\n",
      "5: Encoding Loss 6.430239200592041, Transition Loss -1.70624840259552, Classifier Loss 0.1612347811460495, Total Loss 54.70423126220703\n",
      "5: Encoding Loss 6.681875705718994, Transition Loss -3.461048126220703, Classifier Loss 0.1545470505952835, Total Loss 55.54457473754883\n",
      "5: Encoding Loss 4.740274906158447, Transition Loss -1.6108474731445312, Classifier Loss 0.1003013476729393, Total Loss 38.47114181518555\n",
      "5: Encoding Loss 5.456518173217773, Transition Loss -2.22318959236145, Classifier Loss 0.0512554794549942, Total Loss 37.86376953125\n",
      "5: Encoding Loss 2.7468273639678955, Transition Loss -2.6260581016540527, Classifier Loss 0.05870778113603592, Total Loss 22.350690841674805\n",
      "5: Encoding Loss 5.0621490478515625, Transition Loss -2.796922206878662, Classifier Loss 0.19735606014728546, Total Loss 50.107383728027344\n",
      "5: Encoding Loss 5.070318222045898, Transition Loss -0.787830650806427, Classifier Loss 0.19245117902755737, Total Loss 49.66671371459961\n",
      "5: Encoding Loss 6.95537805557251, Transition Loss -0.46429243683815, Classifier Loss 0.10910341888666153, Total Loss 52.642425537109375\n",
      "5: Encoding Loss 4.18515682220459, Transition Loss -1.0542371273040771, Classifier Loss 0.060867585241794586, Total Loss 31.19727897644043\n",
      "5: Encoding Loss 3.829662322998047, Transition Loss -1.6632558107376099, Classifier Loss 0.07861359417438507, Total Loss 30.838666915893555\n",
      "5: Encoding Loss 5.568087577819824, Transition Loss -1.4908853769302368, Classifier Loss 0.04575945436954498, Total Loss 37.98387908935547\n",
      "5: Encoding Loss 4.728266716003418, Transition Loss -0.9338722229003906, Classifier Loss 0.0659184381365776, Total Loss 34.9610710144043\n",
      "5: Encoding Loss 4.760867118835449, Transition Loss -1.4170011281967163, Classifier Loss 0.08436831831932068, Total Loss 37.001468658447266\n",
      "5: Encoding Loss 5.0528998374938965, Transition Loss -1.4409971237182617, Classifier Loss 0.06462673842906952, Total Loss 36.77949905395508\n",
      "5: Encoding Loss 5.621530055999756, Transition Loss -1.381117582321167, Classifier Loss 0.06011217460036278, Total Loss 39.73984909057617\n",
      "5: Encoding Loss 4.7850141525268555, Transition Loss -1.2409815788269043, Classifier Loss 0.13389527797698975, Total Loss 42.09912109375\n",
      "5: Encoding Loss 4.024754524230957, Transition Loss -0.8861159086227417, Classifier Loss 0.09850984811782837, Total Loss 33.9991569519043\n",
      "5: Encoding Loss 5.596638202667236, Transition Loss 0.2738810181617737, Classifier Loss 0.1184072345495224, Total Loss 45.53010940551758\n",
      "5: Encoding Loss 4.6121134757995605, Transition Loss -1.779569387435913, Classifier Loss 0.08196339756250381, Total Loss 35.868309020996094\n",
      "5: Encoding Loss 4.857884883880615, Transition Loss -1.8978362083435059, Classifier Loss 0.08106798678636551, Total Loss 37.25334930419922\n",
      "5: Encoding Loss 4.4249491691589355, Transition Loss -0.762657880783081, Classifier Loss 0.1545497179031372, Total Loss 42.004364013671875\n",
      "5: Encoding Loss 3.9698989391326904, Transition Loss -0.27793967723846436, Classifier Loss 0.05578671395778656, Total Loss 29.39795684814453\n",
      "5: Encoding Loss 5.078400611877441, Transition Loss -2.0264015197753906, Classifier Loss 0.08532145619392395, Total Loss 39.001739501953125\n",
      "5: Encoding Loss 4.3072590827941895, Transition Loss -0.9357280135154724, Classifier Loss 0.05144665017724037, Total Loss 30.98784637451172\n",
      "5: Encoding Loss 6.240313529968262, Transition Loss -0.05560247600078583, Classifier Loss 0.20967453718185425, Total Loss 58.4093132019043\n",
      "5: Encoding Loss 5.981884002685547, Transition Loss -1.675074577331543, Classifier Loss 0.09514147788286209, Total Loss 45.404781341552734\n",
      "5: Encoding Loss 6.256977081298828, Transition Loss -0.5605201721191406, Classifier Loss 0.16694355010986328, Total Loss 54.235992431640625\n",
      "5: Encoding Loss 4.750588417053223, Transition Loss -1.087148666381836, Classifier Loss 0.038506992161273956, Total Loss 32.35379409790039\n",
      "5: Encoding Loss 4.929882049560547, Transition Loss -1.0207256078720093, Classifier Loss 0.15161047875881195, Total Loss 44.739933013916016\n",
      "5: Encoding Loss 2.8725273609161377, Transition Loss -1.4427324533462524, Classifier Loss 0.11822021007537842, Total Loss 29.056608200073242\n",
      "5: Encoding Loss 5.848249912261963, Transition Loss -0.4207962453365326, Classifier Loss 0.14217956364154816, Total Loss 49.307289123535156\n",
      "5: Encoding Loss 5.705000400543213, Transition Loss -1.373481273651123, Classifier Loss 0.10971786081790924, Total Loss 45.20124053955078\n",
      "5: Encoding Loss 3.8855769634246826, Transition Loss -0.5089341998100281, Classifier Loss 0.12550649046897888, Total Loss 35.86391067504883\n",
      "5: Encoding Loss 4.900677680969238, Transition Loss -1.2475591897964478, Classifier Loss 0.09921777248382568, Total Loss 39.32534408569336\n",
      "5: Encoding Loss 6.175227642059326, Transition Loss -0.5412948131561279, Classifier Loss 0.07994028180837631, Total Loss 45.0451774597168\n",
      "5: Encoding Loss 3.907510280609131, Transition Loss 0.08939394354820251, Classifier Loss 0.08345267176628113, Total Loss 31.826086044311523\n",
      "5: Encoding Loss 6.228501796722412, Transition Loss -0.12530001997947693, Classifier Loss 0.1577233225107193, Total Loss 53.14329528808594\n",
      "5: Encoding Loss 5.758228302001953, Transition Loss -1.129765510559082, Classifier Loss 0.16048288345336914, Total Loss 50.597206115722656\n",
      "5: Encoding Loss 4.492842674255371, Transition Loss -1.3748481273651123, Classifier Loss 0.08722150325775146, Total Loss 35.67865753173828\n",
      "5: Encoding Loss 4.074324607849121, Transition Loss -1.4829221963882446, Classifier Loss 0.17006109654903412, Total Loss 41.45146560668945\n",
      "5: Encoding Loss 6.231047630310059, Transition Loss -0.14654290676116943, Classifier Loss 0.13886448740959167, Total Loss 51.27267837524414\n",
      "5: Encoding Loss 5.379354953765869, Transition Loss -1.4573638439178467, Classifier Loss 0.1491243988275528, Total Loss 47.18798828125\n",
      "5: Encoding Loss 5.807659149169922, Transition Loss -1.1921261548995972, Classifier Loss 0.16859102249145508, Total Loss 51.7045783996582\n",
      "5: Encoding Loss 5.059962272644043, Transition Loss -1.1823755502700806, Classifier Loss 0.09398815035820007, Total Loss 39.75811767578125\n",
      "5: Encoding Loss 5.710467338562012, Transition Loss -1.5507838726043701, Classifier Loss 0.11525120586156845, Total Loss 45.78730392456055\n",
      "5: Encoding Loss 5.656301498413086, Transition Loss -0.8976767063140869, Classifier Loss 0.10987567901611328, Total Loss 44.925018310546875\n",
      "5: Encoding Loss 4.994299411773682, Transition Loss -1.1511929035186768, Classifier Loss 0.10467278212308884, Total Loss 40.432613372802734\n",
      "5: Encoding Loss 4.68816614151001, Transition Loss -0.12130150198936462, Classifier Loss 0.0558798685669899, Total Loss 33.71693420410156\n",
      "5: Encoding Loss 3.7075493335723877, Transition Loss -0.3214111328125, Classifier Loss 0.06344714760780334, Total Loss 28.58988380432129\n",
      "5: Encoding Loss 7.3025712966918945, Transition Loss -1.5007861852645874, Classifier Loss 0.18058374524116516, Total Loss 61.87320327758789\n",
      "5: Encoding Loss 6.835702419281006, Transition Loss -1.7724117040634155, Classifier Loss 0.1256190836429596, Total Loss 53.575416564941406\n",
      "5: Encoding Loss 3.952930450439453, Transition Loss -0.8947305679321289, Classifier Loss 0.11421921849250793, Total Loss 35.13914489746094\n",
      "5: Encoding Loss 6.819016933441162, Transition Loss -1.5548596382141113, Classifier Loss 0.10440956056118011, Total Loss 51.35443878173828\n",
      "5: Encoding Loss 4.8904523849487305, Transition Loss -0.9579046964645386, Classifier Loss 0.08742612600326538, Total Loss 38.08494567871094\n",
      "5: Encoding Loss 4.50393009185791, Transition Loss -0.9561147689819336, Classifier Loss 0.058389805257320404, Total Loss 32.862178802490234\n",
      "5: Encoding Loss 6.504290580749512, Transition Loss -1.8599262237548828, Classifier Loss 0.10847663879394531, Total Loss 49.87266540527344\n",
      "5: Encoding Loss 5.976864337921143, Transition Loss -1.0390982627868652, Classifier Loss 0.1981869786977768, Total Loss 55.67947006225586\n",
      "5: Encoding Loss 5.4952192306518555, Transition Loss -1.2230095863342285, Classifier Loss 0.09311924129724503, Total Loss 42.282752990722656\n",
      "5: Encoding Loss 4.592617034912109, Transition Loss -0.13810309767723083, Classifier Loss 0.09117105603218079, Total Loss 36.67275619506836\n",
      "5: Encoding Loss 5.4094743728637695, Transition Loss -2.229262590408325, Classifier Loss 0.08956831693649292, Total Loss 41.41278839111328\n",
      "5: Encoding Loss 5.080423355102539, Transition Loss -1.8319777250289917, Classifier Loss 0.09215722233057022, Total Loss 39.697532653808594\n",
      "5: Encoding Loss 5.670389652252197, Transition Loss 0.18193167448043823, Classifier Loss 0.12834429740905762, Total Loss 46.929542541503906\n",
      "5: Encoding Loss 7.660300254821777, Transition Loss -1.3322474956512451, Classifier Loss 0.14522181451320648, Total Loss 60.48345184326172\n",
      "5: Encoding Loss 5.055891513824463, Transition Loss -2.1421456336975098, Classifier Loss 0.0646609365940094, Total Loss 36.80058670043945\n",
      "5: Encoding Loss 6.152242183685303, Transition Loss -0.9980480074882507, Classifier Loss 0.0631100982427597, Total Loss 43.224063873291016\n",
      "5: Encoding Loss 5.482069492340088, Transition Loss -0.7595102787017822, Classifier Loss 0.1654217392206192, Total Loss 49.434288024902344\n",
      "5: Encoding Loss 4.821080207824707, Transition Loss -1.107948899269104, Classifier Loss 0.05346490442752838, Total Loss 34.27252960205078\n",
      "6: Encoding Loss 3.9138343334198, Transition Loss -1.1570422649383545, Classifier Loss 0.08211483061313629, Total Loss 31.694026947021484\n",
      "6: Encoding Loss 6.305637836456299, Transition Loss -0.27641409635543823, Classifier Loss 0.10218769311904907, Total Loss 48.052486419677734\n",
      "6: Encoding Loss 5.27718448638916, Transition Loss -1.6276462078094482, Classifier Loss 0.10735069215297699, Total Loss 42.397525787353516\n",
      "6: Encoding Loss 6.145418643951416, Transition Loss -1.1171914339065552, Classifier Loss 0.08049433678388596, Total Loss 44.92150115966797\n",
      "6: Encoding Loss 7.574016571044922, Transition Loss 0.294426828622818, Classifier Loss 0.19739480316638947, Total Loss 65.30134582519531\n",
      "6: Encoding Loss 6.551552772521973, Transition Loss -2.6862475872039795, Classifier Loss 0.07029742002487183, Total Loss 46.33798599243164\n",
      "6: Encoding Loss 4.157101631164551, Transition Loss -1.9664254188537598, Classifier Loss 0.14340291917324066, Total Loss 39.2821159362793\n",
      "6: Encoding Loss 3.9905803203582764, Transition Loss -1.5588135719299316, Classifier Loss 0.1303023248910904, Total Loss 36.97309494018555\n",
      "6: Encoding Loss 5.490968227386475, Transition Loss -1.1129083633422852, Classifier Loss 0.16394829750061035, Total Loss 49.34019470214844\n",
      "6: Encoding Loss 3.522568464279175, Transition Loss -0.6996957063674927, Classifier Loss 0.0902673602104187, Total Loss 30.161867141723633\n",
      "6: Encoding Loss 5.874558925628662, Transition Loss -1.331000566482544, Classifier Loss 0.157555490732193, Total Loss 51.00237274169922\n",
      "6: Encoding Loss 3.9449398517608643, Transition Loss -1.2336987257003784, Classifier Loss 0.08713158965110779, Total Loss 32.38230514526367\n",
      "6: Encoding Loss 2.655910015106201, Transition Loss -0.506227433681488, Classifier Loss 0.060090985149145126, Total Loss 21.94435691833496\n",
      "6: Encoding Loss 4.60062313079834, Transition Loss -2.146207809448242, Classifier Loss 0.08507845550775528, Total Loss 36.1107292175293\n",
      "6: Encoding Loss 3.5070204734802246, Transition Loss -2.044062376022339, Classifier Loss 0.08933361619710922, Total Loss 29.974668502807617\n",
      "6: Encoding Loss 4.673764228820801, Transition Loss -1.3473329544067383, Classifier Loss 0.09550857543945312, Total Loss 37.5929069519043\n",
      "6: Encoding Loss 4.005403518676758, Transition Loss -1.7974729537963867, Classifier Loss 0.08140520751476288, Total Loss 32.17222595214844\n",
      "6: Encoding Loss 3.025407314300537, Transition Loss 0.3469882607460022, Classifier Loss 0.08101867884397507, Total Loss 26.393108367919922\n",
      "6: Encoding Loss 7.209619998931885, Transition Loss -2.454704523086548, Classifier Loss 0.1157451793551445, Total Loss 54.83125686645508\n",
      "6: Encoding Loss 4.643248081207275, Transition Loss -0.7421317100524902, Classifier Loss 0.20153608918190002, Total Loss 48.01280212402344\n",
      "6: Encoding Loss 6.591465950012207, Transition Loss -2.3856582641601562, Classifier Loss 0.23702162504196167, Total Loss 63.25000762939453\n",
      "6: Encoding Loss 5.618246078491211, Transition Loss -1.2486915588378906, Classifier Loss 0.13279646635055542, Total Loss 46.988624572753906\n",
      "6: Encoding Loss 6.547600746154785, Transition Loss -0.5529346466064453, Classifier Loss 0.11100858449935913, Total Loss 50.38624572753906\n",
      "6: Encoding Loss 6.383488655090332, Transition Loss -0.18508800864219666, Classifier Loss 0.10856764018535614, Total Loss 49.15762710571289\n",
      "6: Encoding Loss 5.391395568847656, Transition Loss 0.3224409222602844, Classifier Loss 0.0709623247385025, Total Loss 39.57358169555664\n",
      "6: Encoding Loss 5.037434101104736, Transition Loss -1.90891695022583, Classifier Loss 0.09486034512519836, Total Loss 39.709877014160156\n",
      "6: Encoding Loss 3.4957146644592285, Transition Loss -0.37791353464126587, Classifier Loss 0.07635792344808578, Total Loss 28.60993003845215\n",
      "6: Encoding Loss 7.882759094238281, Transition Loss -1.5756946802139282, Classifier Loss 0.16655954718589783, Total Loss 63.95187759399414\n",
      "6: Encoding Loss 4.289664268493652, Transition Loss -2.332233190536499, Classifier Loss 0.08003270626068115, Total Loss 33.74032211303711\n",
      "6: Encoding Loss 3.8134498596191406, Transition Loss -1.4107625484466553, Classifier Loss 0.1598324030637741, Total Loss 38.863372802734375\n",
      "6: Encoding Loss 4.8909502029418945, Transition Loss -1.7459437847137451, Classifier Loss 0.09637980163097382, Total Loss 38.98298263549805\n",
      "6: Encoding Loss 4.544887065887451, Transition Loss 0.5528700351715088, Classifier Loss 0.0682978555560112, Total Loss 34.32025909423828\n",
      "6: Encoding Loss 6.1978936195373535, Transition Loss -1.1240371465682983, Classifier Loss 0.16563159227371216, Total Loss 53.75007247924805\n",
      "6: Encoding Loss 4.285298824310303, Transition Loss -1.560375452041626, Classifier Loss 0.08661071956157684, Total Loss 34.37223815917969\n",
      "6: Encoding Loss 4.055137634277344, Transition Loss -1.2898160219192505, Classifier Loss 0.13572227954864502, Total Loss 37.90253829956055\n",
      "6: Encoding Loss 5.359620094299316, Transition Loss -1.4281659126281738, Classifier Loss 0.251539945602417, Total Loss 57.3111457824707\n",
      "6: Encoding Loss 4.815437316894531, Transition Loss -1.3116949796676636, Classifier Loss 0.10555367171764374, Total Loss 39.44746780395508\n",
      "6: Encoding Loss 4.8550872802734375, Transition Loss -0.3302699327468872, Classifier Loss 0.08122202008962631, Total Loss 37.252593994140625\n",
      "6: Encoding Loss 8.73040771484375, Transition Loss 0.0841471254825592, Classifier Loss 0.20626384019851685, Total Loss 73.04249572753906\n",
      "6: Encoding Loss 7.593406677246094, Transition Loss -1.6600003242492676, Classifier Loss 0.11927325278520584, Total Loss 57.48710250854492\n",
      "6: Encoding Loss 5.553887844085693, Transition Loss -1.1341142654418945, Classifier Loss 0.13613706827163696, Total Loss 46.936580657958984\n",
      "6: Encoding Loss 6.395554065704346, Transition Loss -1.1757723093032837, Classifier Loss 0.14580431580543518, Total Loss 52.95328903198242\n",
      "6: Encoding Loss 5.729897499084473, Transition Loss -1.142558217048645, Classifier Loss 0.1431548297405243, Total Loss 48.69441223144531\n",
      "6: Encoding Loss 8.613718032836914, Transition Loss -2.3323798179626465, Classifier Loss 0.09041468054056168, Total Loss 60.722843170166016\n",
      "6: Encoding Loss 5.5385332107543945, Transition Loss -1.7464574575424194, Classifier Loss 0.12056893855333328, Total Loss 45.28739547729492\n",
      "6: Encoding Loss 3.3796133995056152, Transition Loss 0.11662814021110535, Classifier Loss 0.0687170997262001, Total Loss 27.196043014526367\n",
      "6: Encoding Loss 4.129167079925537, Transition Loss -1.0349855422973633, Classifier Loss 0.11930382251739502, Total Loss 36.7049674987793\n",
      "6: Encoding Loss 7.0482940673828125, Transition Loss 0.0010174214839935303, Classifier Loss 0.10186175256967545, Total Loss 52.476348876953125\n",
      "6: Encoding Loss 7.159787654876709, Transition Loss -0.37117400765419006, Classifier Loss 0.21815355122089386, Total Loss 64.77394104003906\n",
      "6: Encoding Loss 4.192381381988525, Transition Loss -0.30748438835144043, Classifier Loss 0.07557108998298645, Total Loss 32.71127700805664\n",
      "6: Encoding Loss 4.737973690032959, Transition Loss -1.5390299558639526, Classifier Loss 0.14744225144386292, Total Loss 43.17145538330078\n",
      "6: Encoding Loss 3.3468177318573, Transition Loss -1.7744495868682861, Classifier Loss 0.04867378994822502, Total Loss 24.94757843017578\n",
      "6: Encoding Loss 7.9471116065979, Transition Loss -1.056056261062622, Classifier Loss 0.09731203317642212, Total Loss 57.413448333740234\n",
      "6: Encoding Loss 5.516168117523193, Transition Loss 0.0693197250366211, Classifier Loss 0.16941887140274048, Total Loss 50.066627502441406\n",
      "6: Encoding Loss 3.104315757751465, Transition Loss -1.794764757156372, Classifier Loss 0.1306362748146057, Total Loss 31.688804626464844\n",
      "6: Encoding Loss 7.685076713562012, Transition Loss -0.27318504452705383, Classifier Loss 0.07521553337574005, Total Loss 53.63190460205078\n",
      "6: Encoding Loss 6.63817834854126, Transition Loss -0.5440250635147095, Classifier Loss 0.1666904091835022, Total Loss 56.497894287109375\n",
      "6: Encoding Loss 4.0402069091796875, Transition Loss -1.0741941928863525, Classifier Loss 0.07181520015001297, Total Loss 31.422334671020508\n",
      "6: Encoding Loss 10.809000015258789, Transition Loss -2.0261034965515137, Classifier Loss 0.2146121710538864, Total Loss 86.31441497802734\n",
      "6: Encoding Loss 7.7712578773498535, Transition Loss -1.563910961151123, Classifier Loss 0.05347999185323715, Total Loss 51.97492218017578\n",
      "6: Encoding Loss 6.349860191345215, Transition Loss -0.8358078002929688, Classifier Loss 0.09939324855804443, Total Loss 48.03815460205078\n",
      "6: Encoding Loss 4.603347301483154, Transition Loss -2.3026320934295654, Classifier Loss 0.09196768701076508, Total Loss 36.81593322753906\n",
      "6: Encoding Loss 3.882913589477539, Transition Loss -2.276602268218994, Classifier Loss 0.1329444795846939, Total Loss 36.59101867675781\n",
      "6: Encoding Loss 5.2739033699035645, Transition Loss -4.000432968139648, Classifier Loss 0.11119304597377777, Total Loss 42.76112747192383\n",
      "6: Encoding Loss 4.812523365020752, Transition Loss -0.8271432518959045, Classifier Loss 0.13251495361328125, Total Loss 42.126304626464844\n",
      "6: Encoding Loss 5.815272331237793, Transition Loss -1.987626552581787, Classifier Loss 0.21280837059020996, Total Loss 56.17168045043945\n",
      "6: Encoding Loss 3.5081992149353027, Transition Loss -1.2279698848724365, Classifier Loss 0.2241496592760086, Total Loss 43.46367263793945\n",
      "6: Encoding Loss 5.851078987121582, Transition Loss -1.5698177814483643, Classifier Loss 0.0948292687535286, Total Loss 44.58877182006836\n",
      "6: Encoding Loss 4.354971408843994, Transition Loss -0.058414027094841, Classifier Loss 0.10809671878814697, Total Loss 36.939476013183594\n",
      "6: Encoding Loss 2.8444526195526123, Transition Loss -0.08248350024223328, Classifier Loss 0.11137636005878448, Total Loss 28.204320907592773\n",
      "6: Encoding Loss 4.943819522857666, Transition Loss -2.0621745586395264, Classifier Loss 0.14443069696426392, Total Loss 44.10516357421875\n",
      "6: Encoding Loss 2.9144043922424316, Transition Loss -1.4721379280090332, Classifier Loss 0.11870242655277252, Total Loss 29.356081008911133\n",
      "6: Encoding Loss 2.421276807785034, Transition Loss -2.1262645721435547, Classifier Loss 0.10781709849834442, Total Loss 25.308521270751953\n",
      "6: Encoding Loss 7.333138942718506, Transition Loss -0.34249603748321533, Classifier Loss 0.04946991056203842, Total Loss 48.9456901550293\n",
      "6: Encoding Loss 4.736393928527832, Transition Loss -1.519529938697815, Classifier Loss 0.0660858079791069, Total Loss 35.026336669921875\n",
      "6: Encoding Loss 5.497608661651611, Transition Loss -1.732409119606018, Classifier Loss 0.1583225578069687, Total Loss 48.81721496582031\n",
      "6: Encoding Loss 4.090932369232178, Transition Loss -1.8993512392044067, Classifier Loss 0.049890607595443726, Total Loss 29.533897399902344\n",
      "6: Encoding Loss 7.053664207458496, Transition Loss -1.5561299324035645, Classifier Loss 0.10116013884544373, Total Loss 52.4373779296875\n",
      "6: Encoding Loss 4.8198137283325195, Transition Loss -1.5049467086791992, Classifier Loss 0.15896543860435486, Total Loss 44.814823150634766\n",
      "6: Encoding Loss 2.9794037342071533, Transition Loss -1.4101976156234741, Classifier Loss 0.08061866462230682, Total Loss 25.937725067138672\n",
      "6: Encoding Loss 8.03471565246582, Transition Loss -0.8939566612243652, Classifier Loss 0.08035348355770111, Total Loss 56.243282318115234\n",
      "6: Encoding Loss 6.108371734619141, Transition Loss -1.085017204284668, Classifier Loss 0.17929765582084656, Total Loss 54.579559326171875\n",
      "6: Encoding Loss 4.001419544219971, Transition Loss -0.25577110052108765, Classifier Loss 0.16811256110668182, Total Loss 40.819671630859375\n",
      "6: Encoding Loss 4.797107696533203, Transition Loss -1.0827033519744873, Classifier Loss 0.13394610583782196, Total Loss 42.176822662353516\n",
      "6: Encoding Loss 5.139739513397217, Transition Loss -1.2340837717056274, Classifier Loss 0.14213748276233673, Total Loss 45.051692962646484\n",
      "6: Encoding Loss 4.885032653808594, Transition Loss -2.3725929260253906, Classifier Loss 0.12593130767345428, Total Loss 41.90237808227539\n",
      "6: Encoding Loss 4.51033878326416, Transition Loss 0.8751009702682495, Classifier Loss 0.07768043875694275, Total Loss 35.180118560791016\n",
      "6: Encoding Loss 5.370344638824463, Transition Loss -1.528751254081726, Classifier Loss 0.1543038785457611, Total Loss 47.65184783935547\n",
      "6: Encoding Loss 5.311419486999512, Transition Loss -2.363701820373535, Classifier Loss 0.045490287244319916, Total Loss 36.416603088378906\n",
      "6: Encoding Loss 4.704789161682129, Transition Loss -1.7027132511138916, Classifier Loss 0.1463611125946045, Total Loss 42.86416244506836\n",
      "6: Encoding Loss 3.883784055709839, Transition Loss 0.8281912803649902, Classifier Loss 0.11709034442901611, Total Loss 35.343017578125\n",
      "6: Encoding Loss 2.9457523822784424, Transition Loss -1.288909673690796, Classifier Loss 0.07835061848163605, Total Loss 25.509061813354492\n",
      "6: Encoding Loss 5.82626485824585, Transition Loss -1.8201314210891724, Classifier Loss 0.08832386136054993, Total Loss 43.789249420166016\n",
      "6: Encoding Loss 4.83182430267334, Transition Loss 0.9198600649833679, Classifier Loss 0.086717888712883, Total Loss 38.030677795410156\n",
      "6: Encoding Loss 3.605571746826172, Transition Loss -1.761622667312622, Classifier Loss 0.06256852298974991, Total Loss 27.889577865600586\n",
      "6: Encoding Loss 2.8642361164093018, Transition Loss -1.3244359493255615, Classifier Loss 0.12162978202104568, Total Loss 29.34786605834961\n",
      "6: Encoding Loss 3.5506789684295654, Transition Loss 0.20956075191497803, Classifier Loss 0.07200415432453156, Total Loss 28.588314056396484\n",
      "6: Encoding Loss 10.63975715637207, Transition Loss 0.1266777217388153, Classifier Loss 0.2567996680736542, Total Loss 89.56918334960938\n",
      "6: Encoding Loss 7.7274956703186035, Transition Loss -1.2090296745300293, Classifier Loss 0.10666508972644806, Total Loss 57.03099822998047\n",
      "6: Encoding Loss 8.436662673950195, Transition Loss -1.0127595663070679, Classifier Loss 0.16801804304122925, Total Loss 67.42137908935547\n",
      "6: Encoding Loss 6.390782356262207, Transition Loss -1.614885926246643, Classifier Loss 0.06763989478349686, Total Loss 45.10803985595703\n",
      "6: Encoding Loss 5.497363090515137, Transition Loss -1.829490065574646, Classifier Loss 0.06467700004577637, Total Loss 39.451148986816406\n",
      "6: Encoding Loss 4.737517356872559, Transition Loss -0.4470216631889343, Classifier Loss 0.15002219378948212, Total Loss 43.427146911621094\n",
      "6: Encoding Loss 4.648204326629639, Transition Loss -2.044741630554199, Classifier Loss 0.10753767937421799, Total Loss 38.64217758178711\n",
      "6: Encoding Loss 2.9563496112823486, Transition Loss -0.2115071415901184, Classifier Loss 0.14333347976207733, Total Loss 32.07136154174805\n",
      "6: Encoding Loss 2.903503894805908, Transition Loss 0.7135090827941895, Classifier Loss 0.058320462703704834, Total Loss 23.538475036621094\n",
      "6: Encoding Loss 6.582937717437744, Transition Loss 0.06754696369171143, Classifier Loss 0.10667520761489868, Total Loss 50.19216537475586\n",
      "6: Encoding Loss 4.382603645324707, Transition Loss -2.2107861042022705, Classifier Loss 0.04782934859395027, Total Loss 31.07767105102539\n",
      "6: Encoding Loss 9.548280715942383, Transition Loss -2.0594234466552734, Classifier Loss 0.13700895011425018, Total Loss 70.98975372314453\n",
      "6: Encoding Loss 7.183828830718994, Transition Loss -1.004301905632019, Classifier Loss 0.18250547349452972, Total Loss 61.35312271118164\n",
      "6: Encoding Loss 4.583674430847168, Transition Loss -1.5817792415618896, Classifier Loss 0.07528644800186157, Total Loss 35.030059814453125\n",
      "6: Encoding Loss 6.383680820465088, Transition Loss -0.3382525146007538, Classifier Loss 0.10745134949684143, Total Loss 49.047088623046875\n",
      "6: Encoding Loss 6.406055450439453, Transition Loss -0.9694694876670837, Classifier Loss 0.07352945953607559, Total Loss 45.78889083862305\n",
      "6: Encoding Loss 5.884167194366455, Transition Loss -1.0640920400619507, Classifier Loss 0.04625551402568817, Total Loss 39.93013000488281\n",
      "6: Encoding Loss 7.172807216644287, Transition Loss -1.2899881601333618, Classifier Loss 0.08466966450214386, Total Loss 51.503299713134766\n",
      "6: Encoding Loss 4.399881839752197, Transition Loss -2.5134520530700684, Classifier Loss 0.09602203220129013, Total Loss 36.00048828125\n",
      "6: Encoding Loss 4.2197465896606445, Transition Loss -0.09336404502391815, Classifier Loss 0.09949503093957901, Total Loss 35.2679443359375\n",
      "6: Encoding Loss 2.917745351791382, Transition Loss -0.941493809223175, Classifier Loss 0.06986251473426819, Total Loss 24.49234962463379\n",
      "6: Encoding Loss 4.3796610832214355, Transition Loss -1.2164592742919922, Classifier Loss 0.10135610401630402, Total Loss 36.413089752197266\n",
      "6: Encoding Loss 5.9977569580078125, Transition Loss -0.12376159429550171, Classifier Loss 0.12197404354810715, Total Loss 48.183895111083984\n",
      "6: Encoding Loss 6.304181098937988, Transition Loss -0.8561785817146301, Classifier Loss 0.206848606467247, Total Loss 58.509605407714844\n",
      "6: Encoding Loss 4.916534423828125, Transition Loss -1.7519407272338867, Classifier Loss 0.14877468347549438, Total Loss 44.3759765625\n",
      "6: Encoding Loss 6.680661678314209, Transition Loss -1.4445730447769165, Classifier Loss 0.1030411422252655, Total Loss 50.38751220703125\n",
      "6: Encoding Loss 4.555281162261963, Transition Loss -1.7072687149047852, Classifier Loss 0.07378089427947998, Total Loss 34.7090950012207\n",
      "6: Encoding Loss 2.583688974380493, Transition Loss -1.3152334690093994, Classifier Loss 0.0804322212934494, Total Loss 23.544830322265625\n",
      "6: Encoding Loss 6.120244026184082, Transition Loss -2.1983449459075928, Classifier Loss 0.07791052013635635, Total Loss 44.51163864135742\n",
      "6: Encoding Loss 7.0758771896362305, Transition Loss -2.211116075515747, Classifier Loss 0.21291956305503845, Total Loss 63.746337890625\n",
      "6: Encoding Loss 5.336620807647705, Transition Loss -2.1228580474853516, Classifier Loss 0.05071019008755684, Total Loss 37.08989334106445\n",
      "6: Encoding Loss 2.8945393562316895, Transition Loss -1.0886187553405762, Classifier Loss 0.06264379620552063, Total Loss 23.631181716918945\n",
      "6: Encoding Loss 2.5657432079315186, Transition Loss -3.3689863681793213, Classifier Loss 0.06652295589447021, Total Loss 22.045406341552734\n",
      "6: Encoding Loss 1.298363447189331, Transition Loss -1.0998762845993042, Classifier Loss 0.1445847898721695, Total Loss 22.248220443725586\n",
      "6: Encoding Loss 5.80403470993042, Transition Loss -1.1949466466903687, Classifier Loss 0.09519878029823303, Total Loss 44.34361267089844\n",
      "6: Encoding Loss 5.369918346405029, Transition Loss -1.5680829286575317, Classifier Loss 0.11413762718439102, Total Loss 43.632652282714844\n",
      "6: Encoding Loss 4.2068939208984375, Transition Loss -1.4551469087600708, Classifier Loss 0.06872310489416122, Total Loss 32.113094329833984\n",
      "6: Encoding Loss 6.342866897583008, Transition Loss -1.2531392574310303, Classifier Loss 0.12150875478982925, Total Loss 50.207576751708984\n",
      "6: Encoding Loss 6.16835355758667, Transition Loss -1.1021835803985596, Classifier Loss 0.1988411396741867, Total Loss 56.893795013427734\n",
      "6: Encoding Loss 4.8628740310668945, Transition Loss 0.10993817448616028, Classifier Loss 0.07993252575397491, Total Loss 37.214473724365234\n",
      "6: Encoding Loss 5.4351654052734375, Transition Loss 0.14775246381759644, Classifier Loss 0.1066637858748436, Total Loss 43.33647155761719\n",
      "6: Encoding Loss 3.8146331310272217, Transition Loss -1.7813915014266968, Classifier Loss 0.06852163374423981, Total Loss 29.73925018310547\n",
      "6: Encoding Loss 7.984901428222656, Transition Loss 0.04677461087703705, Classifier Loss 0.06564192473888397, Total Loss 54.492313385009766\n",
      "6: Encoding Loss 6.197987079620361, Transition Loss -1.4799244403839111, Classifier Loss 0.0676523894071579, Total Loss 43.952571868896484\n",
      "6: Encoding Loss 6.469429969787598, Transition Loss -0.5064451098442078, Classifier Loss 0.11991792172193527, Total Loss 50.808170318603516\n",
      "6: Encoding Loss 5.324878692626953, Transition Loss -0.9363151788711548, Classifier Loss 0.05532977730035782, Total Loss 37.481876373291016\n",
      "6: Encoding Loss 5.209354877471924, Transition Loss -0.12622258067131042, Classifier Loss 0.102413110435009, Total Loss 41.49739074707031\n",
      "6: Encoding Loss 3.6525509357452393, Transition Loss -1.1912126541137695, Classifier Loss 0.0896121934056282, Total Loss 30.876049041748047\n",
      "6: Encoding Loss 5.353873252868652, Transition Loss -0.9245389699935913, Classifier Loss 0.17603537440299988, Total Loss 49.72640609741211\n",
      "6: Encoding Loss 3.071974515914917, Transition Loss -1.627519965171814, Classifier Loss 0.06244376674294472, Total Loss 24.675575256347656\n",
      "6: Encoding Loss 6.704325199127197, Transition Loss -1.1822789907455444, Classifier Loss 0.2959176003932953, Total Loss 69.8172378540039\n",
      "6: Encoding Loss 6.494040012359619, Transition Loss -0.7062381505966187, Classifier Loss 0.20203587412834167, Total Loss 59.167545318603516\n",
      "6: Encoding Loss 4.208142280578613, Transition Loss -1.5562388896942139, Classifier Loss 0.14750629663467407, Total Loss 39.99885940551758\n",
      "6: Encoding Loss 5.429583549499512, Transition Loss -1.9741990566253662, Classifier Loss 0.08236832916736603, Total Loss 40.81354522705078\n",
      "6: Encoding Loss 6.819612979888916, Transition Loss -0.35376980900764465, Classifier Loss 0.09737032651901245, Total Loss 50.654571533203125\n",
      "6: Encoding Loss 7.014585971832275, Transition Loss -0.7488662004470825, Classifier Loss 0.1760125458240509, Total Loss 59.688472747802734\n",
      "6: Encoding Loss 5.0841288566589355, Transition Loss -1.8240119218826294, Classifier Loss 0.07976152747869492, Total Loss 38.48019790649414\n",
      "6: Encoding Loss 4.5333428382873535, Transition Loss -3.1772119998931885, Classifier Loss 0.09795718640089035, Total Loss 36.9945068359375\n",
      "6: Encoding Loss 7.0989089012146, Transition Loss -2.230623483657837, Classifier Loss 0.15984834730625153, Total Loss 58.57740020751953\n",
      "6: Encoding Loss 5.861901760101318, Transition Loss -0.7964475154876709, Classifier Loss 0.09121749550104141, Total Loss 44.292842864990234\n",
      "6: Encoding Loss 3.855139970779419, Transition Loss -2.2775425910949707, Classifier Loss 0.09766879677772522, Total Loss 32.89680862426758\n",
      "6: Encoding Loss 2.5867786407470703, Transition Loss -0.9120806455612183, Classifier Loss 0.09568912535905838, Total Loss 25.08922004699707\n",
      "6: Encoding Loss 7.744833469390869, Transition Loss -2.031921625137329, Classifier Loss 0.1278393417596817, Total Loss 59.25212478637695\n",
      "6: Encoding Loss 5.0506792068481445, Transition Loss 0.019329726696014404, Classifier Loss 0.09643027931451797, Total Loss 39.954837799072266\n",
      "6: Encoding Loss 6.064418792724609, Transition Loss -0.6723355054855347, Classifier Loss 0.0781087726354599, Total Loss 44.19712448120117\n",
      "6: Encoding Loss 3.953251361846924, Transition Loss -0.9357735514640808, Classifier Loss 0.054361388087272644, Total Loss 29.1552734375\n",
      "6: Encoding Loss 4.143690586090088, Transition Loss -1.4562532901763916, Classifier Loss 0.082217276096344, Total Loss 33.08328628540039\n",
      "6: Encoding Loss 8.185998916625977, Transition Loss 0.11411917209625244, Classifier Loss 0.16632798314094543, Total Loss 65.79444122314453\n",
      "6: Encoding Loss 5.982672691345215, Transition Loss -1.3314247131347656, Classifier Loss 0.18332991003990173, Total Loss 54.228492736816406\n",
      "6: Encoding Loss 5.515486717224121, Transition Loss -2.9782745838165283, Classifier Loss 0.1712680608034134, Total Loss 50.218536376953125\n",
      "6: Encoding Loss 3.706871509552002, Transition Loss -1.9940874576568604, Classifier Loss 0.06999223679304123, Total Loss 29.239656448364258\n",
      "6: Encoding Loss 7.962737083435059, Transition Loss -1.8827751874923706, Classifier Loss 0.1560448408126831, Total Loss 63.380157470703125\n",
      "6: Encoding Loss 5.8656487464904785, Transition Loss -0.7089394330978394, Classifier Loss 0.06753590703010559, Total Loss 41.947200775146484\n",
      "6: Encoding Loss 4.76747989654541, Transition Loss -1.378718614578247, Classifier Loss 0.08092050999403, Total Loss 36.69637680053711\n",
      "6: Encoding Loss 3.9070427417755127, Transition Loss -1.9843429327011108, Classifier Loss 0.049599532037973404, Total Loss 28.401416778564453\n",
      "6: Encoding Loss 5.468850135803223, Transition Loss -1.299921989440918, Classifier Loss 0.1842351257801056, Total Loss 51.23609924316406\n",
      "6: Encoding Loss 6.356566429138184, Transition Loss -2.986579418182373, Classifier Loss 0.13114912807941437, Total Loss 51.25312042236328\n",
      "6: Encoding Loss 4.3650641441345215, Transition Loss -0.8803633451461792, Classifier Loss 0.10004968196153641, Total Loss 36.195003509521484\n",
      "6: Encoding Loss 6.853588104248047, Transition Loss -0.2149524837732315, Classifier Loss 0.1640656739473343, Total Loss 57.52800750732422\n",
      "6: Encoding Loss 6.881003379821777, Transition Loss -1.4324616193771362, Classifier Loss 0.14110234379768372, Total Loss 55.39568328857422\n",
      "6: Encoding Loss 5.492092132568359, Transition Loss -2.530348539352417, Classifier Loss 0.10892832279205322, Total Loss 43.8443717956543\n",
      "6: Encoding Loss 6.536813259124756, Transition Loss -1.108860731124878, Classifier Loss 0.05958452820777893, Total Loss 45.17889404296875\n",
      "6: Encoding Loss 5.299846649169922, Transition Loss 0.3189884424209595, Classifier Loss 0.10299193859100342, Total Loss 42.225868225097656\n",
      "6: Encoding Loss 5.458994388580322, Transition Loss -0.7513689994812012, Classifier Loss 0.13758738338947296, Total Loss 46.51240539550781\n",
      "6: Encoding Loss 7.217455863952637, Transition Loss -0.7996923923492432, Classifier Loss 0.06305205076932907, Total Loss 49.609622955322266\n",
      "6: Encoding Loss 5.176109313964844, Transition Loss -2.1506145000457764, Classifier Loss 0.05983620136976242, Total Loss 37.0394172668457\n",
      "6: Encoding Loss 7.05987024307251, Transition Loss -1.5231165885925293, Classifier Loss 0.14952711760997772, Total Loss 57.31132507324219\n",
      "6: Encoding Loss 5.502359867095947, Transition Loss -0.9862094521522522, Classifier Loss 0.20477697253227234, Total Loss 53.4914665222168\n",
      "6: Encoding Loss 5.53818416595459, Transition Loss -2.5892577171325684, Classifier Loss 0.12447462230920792, Total Loss 45.67552947998047\n",
      "6: Encoding Loss 3.4197020530700684, Transition Loss 0.3745286464691162, Classifier Loss 0.10405542701482773, Total Loss 31.073566436767578\n",
      "6: Encoding Loss 2.9963302612304688, Transition Loss -0.5891709923744202, Classifier Loss 0.08521635085344315, Total Loss 26.499380111694336\n",
      "6: Encoding Loss 2.4747390747070312, Transition Loss -1.1153395175933838, Classifier Loss 0.07906460016965866, Total Loss 22.75444793701172\n",
      "6: Encoding Loss 8.02882194519043, Transition Loss -2.0005013942718506, Classifier Loss 0.16808053851127625, Total Loss 64.98018646240234\n",
      "6: Encoding Loss 5.317482948303223, Transition Loss -1.4810627698898315, Classifier Loss 0.11764170229434967, Total Loss 43.66847610473633\n",
      "6: Encoding Loss 6.641656398773193, Transition Loss -1.5720510482788086, Classifier Loss 0.11186953634023666, Total Loss 51.0362663269043\n",
      "6: Encoding Loss 4.528993129730225, Transition Loss -1.0831540822982788, Classifier Loss 0.13496346771717072, Total Loss 40.66987228393555\n",
      "6: Encoding Loss 4.4640679359436035, Transition Loss -1.734050989151001, Classifier Loss 0.12896014750003815, Total Loss 39.67972946166992\n",
      "6: Encoding Loss 4.165753364562988, Transition Loss -1.69801664352417, Classifier Loss 0.15098536014556885, Total Loss 40.092376708984375\n",
      "6: Encoding Loss 4.538441181182861, Transition Loss -1.3798398971557617, Classifier Loss 0.11794137954711914, Total Loss 39.024234771728516\n",
      "6: Encoding Loss 6.460230350494385, Transition Loss -1.0115660429000854, Classifier Loss 0.13908465206623077, Total Loss 52.6694450378418\n",
      "6: Encoding Loss 4.9545979499816895, Transition Loss -0.7290539145469666, Classifier Loss 0.24979400634765625, Total Loss 54.70669937133789\n",
      "6: Encoding Loss 7.103267669677734, Transition Loss -1.7644317150115967, Classifier Loss 0.10069169104099274, Total Loss 52.68806838989258\n",
      "6: Encoding Loss 4.634438991546631, Transition Loss -1.9184476137161255, Classifier Loss 0.08484601974487305, Total Loss 36.290470123291016\n",
      "6: Encoding Loss 6.290370464324951, Transition Loss -1.572272777557373, Classifier Loss 0.09032946079969406, Total Loss 46.774539947509766\n",
      "6: Encoding Loss 4.504790306091309, Transition Loss -0.9271752238273621, Classifier Loss 0.09728232026100159, Total Loss 36.75660705566406\n",
      "6: Encoding Loss 4.3950910568237305, Transition Loss -0.9312565326690674, Classifier Loss 0.06487614661455154, Total Loss 32.8577880859375\n",
      "6: Encoding Loss 3.165445566177368, Transition Loss -0.4671177864074707, Classifier Loss 0.0942051112651825, Total Loss 28.41299819946289\n",
      "6: Encoding Loss 5.685502052307129, Transition Loss -0.9628655910491943, Classifier Loss 0.08018587529659271, Total Loss 42.1312141418457\n",
      "6: Encoding Loss 5.765453338623047, Transition Loss -0.8477901220321655, Classifier Loss 0.16926787793636322, Total Loss 51.519168853759766\n",
      "6: Encoding Loss 3.6541450023651123, Transition Loss -1.4974993467330933, Classifier Loss 0.1895444691181183, Total Loss 40.878719329833984\n",
      "6: Encoding Loss 3.9430012702941895, Transition Loss -0.12593622505664825, Classifier Loss 0.11806866526603699, Total Loss 35.46482467651367\n",
      "6: Encoding Loss 7.043992519378662, Transition Loss -1.4100823402404785, Classifier Loss 0.2041913866996765, Total Loss 62.682533264160156\n",
      "6: Encoding Loss 6.023308753967285, Transition Loss -0.3832177221775055, Classifier Loss 0.0986105278134346, Total Loss 46.000755310058594\n",
      "6: Encoding Loss 5.1976752281188965, Transition Loss -0.8631737232208252, Classifier Loss 0.05504488945007324, Total Loss 36.69019317626953\n",
      "6: Encoding Loss 6.738569736480713, Transition Loss -0.4816877245903015, Classifier Loss 0.12532472610473633, Total Loss 52.96369552612305\n",
      "6: Encoding Loss 4.932969570159912, Transition Loss -2.9780843257904053, Classifier Loss 0.13286802172660828, Total Loss 42.88343048095703\n",
      "6: Encoding Loss 3.3295750617980957, Transition Loss 0.9415568709373474, Classifier Loss 0.0873885527253151, Total Loss 29.09292984008789\n",
      "6: Encoding Loss 3.5169780254364014, Transition Loss -2.2615082263946533, Classifier Loss 0.08621751517057419, Total Loss 29.72271728515625\n",
      "6: Encoding Loss 6.806585788726807, Transition Loss -2.1485595703125, Classifier Loss 0.044278450310230255, Total Loss 45.266502380371094\n",
      "6: Encoding Loss 7.5777435302734375, Transition Loss -1.733319878578186, Classifier Loss 0.0794854685664177, Total Loss 53.41431427001953\n",
      "6: Encoding Loss 7.070611476898193, Transition Loss -1.2869300842285156, Classifier Loss 0.06714558601379395, Total Loss 49.13771438598633\n",
      "6: Encoding Loss 4.63870906829834, Transition Loss -0.8622356653213501, Classifier Loss 0.1175185963511467, Total Loss 39.583770751953125\n",
      "6: Encoding Loss 5.202826023101807, Transition Loss -0.7480852603912354, Classifier Loss 0.05825290083885193, Total Loss 37.04195022583008\n",
      "6: Encoding Loss 3.0674781799316406, Transition Loss -0.7923667430877686, Classifier Loss 0.09777747839689255, Total Loss 28.182300567626953\n",
      "6: Encoding Loss 7.245020389556885, Transition Loss -2.30513858795166, Classifier Loss 0.08225401490926743, Total Loss 51.694602966308594\n",
      "6: Encoding Loss 5.297907829284668, Transition Loss -1.4169095754623413, Classifier Loss 0.06839770078659058, Total Loss 38.626651763916016\n",
      "6: Encoding Loss 6.177009582519531, Transition Loss -1.6371142864227295, Classifier Loss 0.15457837283611298, Total Loss 52.51923751831055\n",
      "6: Encoding Loss 4.522706985473633, Transition Loss -1.4505788087844849, Classifier Loss 0.03855044022202492, Total Loss 30.990707397460938\n",
      "6: Encoding Loss 5.757931232452393, Transition Loss -0.2749086618423462, Classifier Loss 0.07005099952220917, Total Loss 41.55257797241211\n",
      "6: Encoding Loss 5.069840908050537, Transition Loss -1.406691074371338, Classifier Loss 0.1393306404352188, Total Loss 44.35154724121094\n",
      "6: Encoding Loss 4.456114768981934, Transition Loss -1.37416672706604, Classifier Loss 0.08454043418169022, Total Loss 35.190185546875\n",
      "6: Encoding Loss 2.9978458881378174, Transition Loss -2.2885360717773438, Classifier Loss 0.06945422291755676, Total Loss 24.931583404541016\n",
      "6: Encoding Loss 5.760857105255127, Transition Loss -3.2872776985168457, Classifier Loss 0.07337851077318192, Total Loss 41.90167999267578\n",
      "6: Encoding Loss 3.977567195892334, Transition Loss -0.685647189617157, Classifier Loss 0.15364757180213928, Total Loss 39.229888916015625\n",
      "6: Encoding Loss 4.8562421798706055, Transition Loss -0.9067801833152771, Classifier Loss 0.1709892451763153, Total Loss 46.236019134521484\n",
      "6: Encoding Loss 5.6577277183532715, Transition Loss -0.6069587469100952, Classifier Loss 0.10604580491781235, Total Loss 44.55070495605469\n",
      "6: Encoding Loss 3.913670063018799, Transition Loss -0.39380913972854614, Classifier Loss 0.060199134051799774, Total Loss 29.50177574157715\n",
      "6: Encoding Loss 8.76624584197998, Transition Loss -0.15823635458946228, Classifier Loss 0.15088015794754028, Total Loss 67.68543243408203\n",
      "6: Encoding Loss 4.7529096603393555, Transition Loss -0.9654077291488647, Classifier Loss 0.09756729006767273, Total Loss 38.2738037109375\n",
      "6: Encoding Loss 7.646428108215332, Transition Loss -0.39620083570480347, Classifier Loss 0.1980750560760498, Total Loss 65.68592071533203\n",
      "6: Encoding Loss 5.9086151123046875, Transition Loss -2.5793559551239014, Classifier Loss 0.18258242309093475, Total Loss 53.708900451660156\n",
      "6: Encoding Loss 5.83521032333374, Transition Loss -1.0594141483306885, Classifier Loss 0.11975601315498352, Total Loss 46.98644256591797\n",
      "6: Encoding Loss 5.732975959777832, Transition Loss -1.787644624710083, Classifier Loss 0.15136881172657013, Total Loss 49.53402328491211\n",
      "6: Encoding Loss 3.263873338699341, Transition Loss -1.2974035739898682, Classifier Loss 0.09907881915569305, Total Loss 29.490604400634766\n",
      "6: Encoding Loss 3.5987422466278076, Transition Loss -0.4445536136627197, Classifier Loss 0.08662857860326767, Total Loss 30.255136489868164\n",
      "6: Encoding Loss 4.4248552322387695, Transition Loss -0.7966426610946655, Classifier Loss 0.10747502744197845, Total Loss 37.29631423950195\n",
      "6: Encoding Loss 4.581221103668213, Transition Loss -2.6753604412078857, Classifier Loss 0.08373209834098816, Total Loss 35.859466552734375\n",
      "6: Encoding Loss 5.546582221984863, Transition Loss -1.526423454284668, Classifier Loss 0.14076030254364014, Total Loss 47.354915618896484\n",
      "6: Encoding Loss 4.907070159912109, Transition Loss -1.4484214782714844, Classifier Loss 0.09891217947006226, Total Loss 39.33306121826172\n",
      "6: Encoding Loss 3.485727310180664, Transition Loss -0.4296469986438751, Classifier Loss 0.10110419988632202, Total Loss 31.024612426757812\n",
      "6: Encoding Loss 4.126742362976074, Transition Loss -1.6509504318237305, Classifier Loss 0.06083569675683975, Total Loss 30.843364715576172\n",
      "6: Encoding Loss 6.176169395446777, Transition Loss -1.2123180627822876, Classifier Loss 0.13509584963321686, Total Loss 50.56612014770508\n",
      "6: Encoding Loss 5.008500576019287, Transition Loss 0.06662988662719727, Classifier Loss 0.06065681576728821, Total Loss 36.14333724975586\n",
      "6: Encoding Loss 6.856146812438965, Transition Loss 0.054459989070892334, Classifier Loss 0.10946507751941681, Total Loss 52.10517501831055\n",
      "6: Encoding Loss 6.343164443969727, Transition Loss -1.6241270303726196, Classifier Loss 0.30008289217948914, Total Loss 68.0666275024414\n",
      "6: Encoding Loss 6.04775333404541, Transition Loss -1.6783512830734253, Classifier Loss 0.0651366263628006, Total Loss 42.79951477050781\n",
      "6: Encoding Loss 5.200681686401367, Transition Loss -1.361588954925537, Classifier Loss 0.11464519798755646, Total Loss 42.66806411743164\n",
      "6: Encoding Loss 4.458353519439697, Transition Loss -1.2275547981262207, Classifier Loss 0.07854475826025009, Total Loss 34.60410690307617\n",
      "6: Encoding Loss 5.881308555603027, Transition Loss -2.023069381713867, Classifier Loss 0.21187657117843628, Total Loss 56.474700927734375\n",
      "6: Encoding Loss 4.162583351135254, Transition Loss -1.0079866647720337, Classifier Loss 0.07896143943071365, Total Loss 32.871238708496094\n",
      "6: Encoding Loss 6.935458183288574, Transition Loss -2.103447198867798, Classifier Loss 0.11924830079078674, Total Loss 53.536739349365234\n",
      "6: Encoding Loss 5.8663010597229, Transition Loss -1.4022610187530518, Classifier Loss 0.1433548480272293, Total Loss 49.53273010253906\n",
      "6: Encoding Loss 6.027707576751709, Transition Loss -1.8117449283599854, Classifier Loss 0.06255128234624863, Total Loss 42.420650482177734\n",
      "6: Encoding Loss 6.515347480773926, Transition Loss -0.8585797548294067, Classifier Loss 0.1533079594373703, Total Loss 54.42253875732422\n",
      "6: Encoding Loss 2.478888511657715, Transition Loss -1.9460337162017822, Classifier Loss 0.08091143518686295, Total Loss 22.96369743347168\n",
      "6: Encoding Loss 2.705756902694702, Transition Loss -0.6962043046951294, Classifier Loss 0.0662139356136322, Total Loss 22.85565757751465\n",
      "6: Encoding Loss 6.370027542114258, Transition Loss -0.45365363359451294, Classifier Loss 0.10426024347543716, Total Loss 48.6460075378418\n",
      "6: Encoding Loss 4.545656681060791, Transition Loss 0.4546607732772827, Classifier Loss 0.08626927435398102, Total Loss 36.082733154296875\n",
      "6: Encoding Loss 5.669002532958984, Transition Loss -0.9237276911735535, Classifier Loss 0.16821619868278503, Total Loss 50.835262298583984\n",
      "6: Encoding Loss 5.400942802429199, Transition Loss -1.8471882343292236, Classifier Loss 0.07504637539386749, Total Loss 39.9095573425293\n",
      "6: Encoding Loss 3.6648216247558594, Transition Loss -2.481336832046509, Classifier Loss 0.11492080986499786, Total Loss 33.480018615722656\n",
      "6: Encoding Loss 3.0938353538513184, Transition Loss -1.4441486597061157, Classifier Loss 0.08328677713871002, Total Loss 26.89111328125\n",
      "6: Encoding Loss 8.087955474853516, Transition Loss -1.3703534603118896, Classifier Loss 0.1631826013326645, Total Loss 64.84544372558594\n",
      "6: Encoding Loss 4.493473529815674, Transition Loss -0.16453030705451965, Classifier Loss 0.1799757480621338, Total Loss 44.958351135253906\n",
      "6: Encoding Loss 4.743173122406006, Transition Loss -1.4540334939956665, Classifier Loss 0.05958293005824089, Total Loss 34.416751861572266\n",
      "6: Encoding Loss 5.51413106918335, Transition Loss 0.09123280644416809, Classifier Loss 0.12652966380119324, Total Loss 45.77424621582031\n",
      "6: Encoding Loss 3.1634066104888916, Transition Loss 0.5619160532951355, Classifier Loss 0.07453033328056335, Total Loss 26.658239364624023\n",
      "6: Encoding Loss 3.76763653755188, Transition Loss -1.3906606435775757, Classifier Loss 0.10162730515003204, Total Loss 32.76799392700195\n",
      "6: Encoding Loss 6.354903221130371, Transition Loss -0.8996186256408691, Classifier Loss 0.10679949820041656, Total Loss 48.80901336669922\n",
      "6: Encoding Loss 3.948218584060669, Transition Loss -0.9744843244552612, Classifier Loss 0.11652513593435287, Total Loss 35.34143829345703\n",
      "6: Encoding Loss 6.506514549255371, Transition Loss -0.9709131717681885, Classifier Loss 0.14864815771579742, Total Loss 53.90351486206055\n",
      "6: Encoding Loss 6.7564897537231445, Transition Loss -1.4126774072647095, Classifier Loss 0.1907518208026886, Total Loss 59.613555908203125\n",
      "6: Encoding Loss 4.725497245788574, Transition Loss -1.043076515197754, Classifier Loss 0.0461103618144989, Total Loss 32.96360397338867\n",
      "6: Encoding Loss 4.538918495178223, Transition Loss -0.4692471921443939, Classifier Loss 0.1729876846075058, Total Loss 44.5320930480957\n",
      "6: Encoding Loss 6.182448863983154, Transition Loss -3.179043769836426, Classifier Loss 0.19256539642810822, Total Loss 56.3499641418457\n",
      "6: Encoding Loss 6.677865028381348, Transition Loss -1.5601038932800293, Classifier Loss 0.07592172920703888, Total Loss 47.65874099731445\n",
      "6: Encoding Loss 5.2291460037231445, Transition Loss -1.2729402780532837, Classifier Loss 0.08882183581590652, Total Loss 40.256553649902344\n",
      "6: Encoding Loss 6.260979652404785, Transition Loss -2.28206205368042, Classifier Loss 0.14401496946811676, Total Loss 51.96646499633789\n",
      "6: Encoding Loss 5.152127265930176, Transition Loss -0.49785080552101135, Classifier Loss 0.07674287259578705, Total Loss 38.58685302734375\n",
      "6: Encoding Loss 2.8809638023376465, Transition Loss -1.5357451438903809, Classifier Loss 0.09208359569311142, Total Loss 26.4935302734375\n",
      "6: Encoding Loss 5.429997444152832, Transition Loss -1.8570529222488403, Classifier Loss 0.13526156544685364, Total Loss 46.10540008544922\n",
      "6: Encoding Loss 9.012792587280273, Transition Loss -3.2621967792510986, Classifier Loss 0.17762452363967896, Total Loss 71.83790588378906\n",
      "6: Encoding Loss 5.417398452758789, Transition Loss -1.4008338451385498, Classifier Loss 0.06334658712148666, Total Loss 38.8384895324707\n",
      "6: Encoding Loss 5.544251441955566, Transition Loss -1.021159052848816, Classifier Loss 0.05936546251177788, Total Loss 39.2016487121582\n",
      "6: Encoding Loss 6.028944492340088, Transition Loss -0.3026793599128723, Classifier Loss 0.16781224310398102, Total Loss 52.95477294921875\n",
      "6: Encoding Loss 6.092865943908691, Transition Loss -0.9253717660903931, Classifier Loss 0.2641664445400238, Total Loss 62.973472595214844\n",
      "6: Encoding Loss 6.17016077041626, Transition Loss -1.5657949447631836, Classifier Loss 0.18590591847896576, Total Loss 55.610931396484375\n",
      "6: Encoding Loss 6.31266450881958, Transition Loss -2.0542967319488525, Classifier Loss 0.1531011462211609, Total Loss 53.18528366088867\n",
      "6: Encoding Loss 5.285893440246582, Transition Loss -1.3725316524505615, Classifier Loss 0.12427760660648346, Total Loss 44.14257049560547\n",
      "6: Encoding Loss 5.425087928771973, Transition Loss -0.47518882155418396, Classifier Loss 0.15710830688476562, Total Loss 48.26116943359375\n",
      "6: Encoding Loss 6.776170253753662, Transition Loss -0.7157235145568848, Classifier Loss 0.17467115819454193, Total Loss 58.12385177612305\n",
      "6: Encoding Loss 4.746795654296875, Transition Loss -0.5776435136795044, Classifier Loss 0.06531976163387299, Total Loss 35.01251983642578\n",
      "6: Encoding Loss 7.925378322601318, Transition Loss -2.1233935356140137, Classifier Loss 0.07965419441461563, Total Loss 55.516841888427734\n",
      "6: Encoding Loss 5.4132304191589355, Transition Loss -0.946354866027832, Classifier Loss 0.11437903344631195, Total Loss 43.91691207885742\n",
      "6: Encoding Loss 4.255271911621094, Transition Loss -1.6555352210998535, Classifier Loss 0.0943487212061882, Total Loss 34.965843200683594\n",
      "6: Encoding Loss 5.473816871643066, Transition Loss -0.4669124484062195, Classifier Loss 0.18474100530147552, Total Loss 51.31681442260742\n",
      "6: Encoding Loss 5.046379089355469, Transition Loss -1.3749492168426514, Classifier Loss 0.08885234594345093, Total Loss 39.1629638671875\n",
      "6: Encoding Loss 5.099752902984619, Transition Loss -1.4837822914123535, Classifier Loss 0.15506456792354584, Total Loss 46.10437774658203\n",
      "6: Encoding Loss 10.857019424438477, Transition Loss -2.082400321960449, Classifier Loss 0.12857438623905182, Total Loss 77.99872589111328\n",
      "6: Encoding Loss 6.711733818054199, Transition Loss -0.7356942296028137, Classifier Loss 0.0709434375166893, Total Loss 47.36445617675781\n",
      "6: Encoding Loss 6.494287967681885, Transition Loss 0.6848691701889038, Classifier Loss 0.0772867500782013, Total Loss 46.968353271484375\n",
      "6: Encoding Loss 4.613621711730957, Transition Loss -2.249917507171631, Classifier Loss 0.114650659263134, Total Loss 39.145896911621094\n",
      "6: Encoding Loss 6.135245323181152, Transition Loss -1.55059814453125, Classifier Loss 0.12897302210330963, Total Loss 49.70815658569336\n",
      "6: Encoding Loss 3.818387269973755, Transition Loss -0.5447131395339966, Classifier Loss 0.10375859588384628, Total Loss 33.28596496582031\n",
      "6: Encoding Loss 5.2572526931762695, Transition Loss -1.4215028285980225, Classifier Loss 0.19603660702705383, Total Loss 51.146610260009766\n",
      "6: Encoding Loss 9.291557312011719, Transition Loss -1.3479626178741455, Classifier Loss 0.11913172155618668, Total Loss 67.66197967529297\n",
      "6: Encoding Loss 4.953871250152588, Transition Loss -1.8012397289276123, Classifier Loss 0.10106495767831802, Total Loss 39.829002380371094\n",
      "6: Encoding Loss 5.4963226318359375, Transition Loss -1.0895226001739502, Classifier Loss 0.07989323139190674, Total Loss 40.96682357788086\n",
      "6: Encoding Loss 5.719748020172119, Transition Loss -2.073786973953247, Classifier Loss 0.061643730849027634, Total Loss 40.482032775878906\n",
      "6: Encoding Loss 5.16253137588501, Transition Loss -2.128101348876953, Classifier Loss 0.057304173707962036, Total Loss 36.70475387573242\n",
      "6: Encoding Loss 4.586066246032715, Transition Loss -1.0714625120162964, Classifier Loss 0.039499830454587936, Total Loss 31.465953826904297\n",
      "6: Encoding Loss 4.091074466705322, Transition Loss -2.3489279747009277, Classifier Loss 0.11476624011993408, Total Loss 36.022132873535156\n",
      "6: Encoding Loss 6.251215934753418, Transition Loss -0.5298160314559937, Classifier Loss 0.11996977031230927, Total Loss 49.504058837890625\n",
      "6: Encoding Loss 5.722656726837158, Transition Loss -0.7969402074813843, Classifier Loss 0.07660217583179474, Total Loss 41.9958381652832\n",
      "6: Encoding Loss 5.5718488693237305, Transition Loss -3.015852451324463, Classifier Loss 0.1074104979634285, Total Loss 44.17094039916992\n",
      "6: Encoding Loss 6.979763984680176, Transition Loss -2.9611053466796875, Classifier Loss 0.13091899454593658, Total Loss 54.969303131103516\n",
      "6: Encoding Loss 4.8560357093811035, Transition Loss -0.0011434108018875122, Classifier Loss 0.06578125804662704, Total Loss 35.71434020996094\n",
      "6: Encoding Loss 4.710277080535889, Transition Loss -1.8196699619293213, Classifier Loss 0.16000878810882568, Total Loss 44.26181411743164\n",
      "6: Encoding Loss 4.984488010406494, Transition Loss -1.3837186098098755, Classifier Loss 0.09209942817687988, Total Loss 39.11631774902344\n",
      "6: Encoding Loss 3.6137497425079346, Transition Loss -1.875383973121643, Classifier Loss 0.08062124252319336, Total Loss 29.743873596191406\n",
      "6: Encoding Loss 5.125378131866455, Transition Loss -2.6741700172424316, Classifier Loss 0.14842195808887482, Total Loss 45.59339904785156\n",
      "6: Encoding Loss 6.73553466796875, Transition Loss -2.16161847114563, Classifier Loss 0.10130183398723602, Total Loss 50.54252624511719\n",
      "6: Encoding Loss 6.3846940994262695, Transition Loss -2.645998001098633, Classifier Loss 0.113289974629879, Total Loss 49.6361083984375\n",
      "6: Encoding Loss 7.183851718902588, Transition Loss -1.023803949356079, Classifier Loss 0.1612224280834198, Total Loss 59.224945068359375\n",
      "6: Encoding Loss 4.984972953796387, Transition Loss 0.34824809432029724, Classifier Loss 0.11493389308452606, Total Loss 41.54252624511719\n",
      "6: Encoding Loss 4.9222869873046875, Transition Loss -2.538400888442993, Classifier Loss 0.09136085957288742, Total Loss 38.668792724609375\n",
      "6: Encoding Loss 5.1206374168396, Transition Loss -1.8385484218597412, Classifier Loss 0.1325642466545105, Total Loss 43.979515075683594\n",
      "6: Encoding Loss 4.464245796203613, Transition Loss -1.6504682302474976, Classifier Loss 0.0883866474032402, Total Loss 35.623477935791016\n",
      "6: Encoding Loss 3.9120545387268066, Transition Loss -1.9491496086120605, Classifier Loss 0.08766913414001465, Total Loss 32.23846435546875\n",
      "6: Encoding Loss 3.601907253265381, Transition Loss -2.2383041381835938, Classifier Loss 0.06801100075244904, Total Loss 28.411649703979492\n",
      "6: Encoding Loss 3.6437408924102783, Transition Loss -0.6327067613601685, Classifier Loss 0.1131507083773613, Total Loss 33.17726516723633\n",
      "6: Encoding Loss 4.811212539672852, Transition Loss -0.9055368304252625, Classifier Loss 0.13155922293663025, Total Loss 42.0228385925293\n",
      "6: Encoding Loss 6.3486104011535645, Transition Loss -0.37065571546554565, Classifier Loss 0.10860177129507065, Total Loss 48.951690673828125\n",
      "6: Encoding Loss 4.361388206481934, Transition Loss -0.7331814169883728, Classifier Loss 0.10154750943183899, Total Loss 36.32278823852539\n",
      "6: Encoding Loss 7.032086372375488, Transition Loss -1.2138434648513794, Classifier Loss 0.12146031856536865, Total Loss 54.338069915771484\n",
      "6: Encoding Loss 6.549081325531006, Transition Loss -0.857197642326355, Classifier Loss 0.12375779449939728, Total Loss 51.669925689697266\n",
      "6: Encoding Loss 4.896926403045654, Transition Loss -1.698051929473877, Classifier Loss 0.16838553547859192, Total Loss 46.21943664550781\n",
      "6: Encoding Loss 5.211611747741699, Transition Loss -0.6547154188156128, Classifier Loss 0.11024688929319382, Total Loss 42.294097900390625\n",
      "6: Encoding Loss 7.697399139404297, Transition Loss -2.190582752227783, Classifier Loss 0.17380815744400024, Total Loss 63.5643310546875\n",
      "6: Encoding Loss 5.1219072341918945, Transition Loss -1.6859344244003296, Classifier Loss 0.059124309569597244, Total Loss 36.6431999206543\n",
      "6: Encoding Loss 7.338296413421631, Transition Loss -0.9468111991882324, Classifier Loss 0.19608670473098755, Total Loss 63.6380729675293\n",
      "6: Encoding Loss 5.8842549324035645, Transition Loss -0.5608431100845337, Classifier Loss 0.15015649795532227, Total Loss 50.32095718383789\n",
      "6: Encoding Loss 4.961512565612793, Transition Loss -1.745640516281128, Classifier Loss 0.1219271719455719, Total Loss 41.961097717285156\n",
      "6: Encoding Loss 4.679885387420654, Transition Loss -2.714526653289795, Classifier Loss 0.16346709430217743, Total Loss 44.4249382019043\n",
      "6: Encoding Loss 6.723687171936035, Transition Loss -1.9973217248916626, Classifier Loss 0.14604946970939636, Total Loss 54.94627380371094\n",
      "6: Encoding Loss 4.517287731170654, Transition Loss -0.8615435361862183, Classifier Loss 0.0960504338145256, Total Loss 36.70842742919922\n",
      "6: Encoding Loss 2.9115591049194336, Transition Loss -1.1951754093170166, Classifier Loss 0.08237620443105698, Total Loss 25.70649528503418\n",
      "6: Encoding Loss 2.6875433921813965, Transition Loss -1.7350720167160034, Classifier Loss 0.08721991628408432, Total Loss 24.8465576171875\n",
      "6: Encoding Loss 3.6429152488708496, Transition Loss -1.4600615501403809, Classifier Loss 0.10057768225669861, Total Loss 31.914676666259766\n",
      "6: Encoding Loss 8.829994201660156, Transition Loss 0.5338248014450073, Classifier Loss 0.1176563948392868, Total Loss 64.95913696289062\n",
      "6: Encoding Loss 6.073240756988525, Transition Loss 0.14349693059921265, Classifier Loss 0.15296293795108795, Total Loss 51.79314041137695\n",
      "6: Encoding Loss 4.419905662536621, Transition Loss 0.4981212615966797, Classifier Loss 0.07312147319316864, Total Loss 34.03083038330078\n",
      "6: Encoding Loss 7.8510541915893555, Transition Loss -0.48158687353134155, Classifier Loss 0.2675171196460724, Total Loss 73.85784912109375\n",
      "6: Encoding Loss 4.951732158660889, Transition Loss -1.5785099267959595, Classifier Loss 0.06780026853084564, Total Loss 36.48978805541992\n",
      "6: Encoding Loss 4.765791416168213, Transition Loss -2.259078025817871, Classifier Loss 0.13142569363117218, Total Loss 41.73641586303711\n",
      "6: Encoding Loss 5.962898254394531, Transition Loss -1.1967835426330566, Classifier Loss 0.1506851464509964, Total Loss 50.845428466796875\n",
      "6: Encoding Loss 5.0189127922058105, Transition Loss -1.4963222742080688, Classifier Loss 0.061319898813962936, Total Loss 36.244869232177734\n",
      "6: Encoding Loss 4.394597053527832, Transition Loss -0.711219072341919, Classifier Loss 0.07676563411951065, Total Loss 34.04385757446289\n",
      "6: Encoding Loss 6.1065287590026855, Transition Loss -0.8257116079330444, Classifier Loss 0.1205008253455162, Total Loss 48.688926696777344\n",
      "6: Encoding Loss 3.800668239593506, Transition Loss 0.06482528150081635, Classifier Loss 0.050211306661367416, Total Loss 27.851072311401367\n",
      "6: Encoding Loss 7.211509704589844, Transition Loss 0.4677216410636902, Classifier Loss 0.09364738315343857, Total Loss 52.820884704589844\n",
      "6: Encoding Loss 5.609185218811035, Transition Loss -1.9640406370162964, Classifier Loss 0.112869493663311, Total Loss 44.94127655029297\n",
      "6: Encoding Loss 4.2440948486328125, Transition Loss -1.6344716548919678, Classifier Loss 0.1434357911348343, Total Loss 39.807498931884766\n",
      "6: Encoding Loss 6.310666084289551, Transition Loss -1.8881672620773315, Classifier Loss 0.08029082417488098, Total Loss 45.89232635498047\n",
      "6: Encoding Loss 3.639951705932617, Transition Loss -1.130426287651062, Classifier Loss 0.11871125549077988, Total Loss 33.71038055419922\n",
      "6: Encoding Loss 2.258225679397583, Transition Loss -1.5958415269851685, Classifier Loss 0.11395908892154694, Total Loss 24.944623947143555\n",
      "6: Encoding Loss 2.4510865211486816, Transition Loss -1.1785484552383423, Classifier Loss 0.07340684533119202, Total Loss 22.04673194885254\n",
      "6: Encoding Loss 8.728962898254395, Transition Loss -2.10487699508667, Classifier Loss 0.12481304258108139, Total Loss 64.854248046875\n",
      "6: Encoding Loss 5.108681678771973, Transition Loss -1.722963809967041, Classifier Loss 0.0589553602039814, Total Loss 36.54693603515625\n",
      "6: Encoding Loss 2.664584159851074, Transition Loss -1.9311552047729492, Classifier Loss 0.07993044704198837, Total Loss 23.979778289794922\n",
      "6: Encoding Loss 6.521481990814209, Transition Loss -1.2980371713638306, Classifier Loss 0.11955751478672028, Total Loss 51.084129333496094\n",
      "6: Encoding Loss 3.63232684135437, Transition Loss -1.0289069414138794, Classifier Loss 0.07828329503536224, Total Loss 29.62187957763672\n",
      "6: Encoding Loss 5.328512668609619, Transition Loss -0.8028765916824341, Classifier Loss 0.10489801317453384, Total Loss 42.46055603027344\n",
      "6: Encoding Loss 5.4381422996521, Transition Loss -0.5779920220375061, Classifier Loss 0.08368907868862152, Total Loss 40.99753189086914\n",
      "6: Encoding Loss 5.406956672668457, Transition Loss -2.715608596801758, Classifier Loss 0.14150844514369965, Total Loss 46.59149932861328\n",
      "6: Encoding Loss 5.440112113952637, Transition Loss -0.5601279735565186, Classifier Loss 0.13083988428115845, Total Loss 45.72443771362305\n",
      "6: Encoding Loss 4.871111869812012, Transition Loss -1.4972434043884277, Classifier Loss 0.17857232689857483, Total Loss 47.083309173583984\n",
      "6: Encoding Loss 4.602477550506592, Transition Loss -2.51979923248291, Classifier Loss 0.10317289084196091, Total Loss 37.931148529052734\n",
      "6: Encoding Loss 3.512885332107544, Transition Loss -1.234196662902832, Classifier Loss 0.04690132290124893, Total Loss 25.766950607299805\n",
      "6: Encoding Loss 7.683442115783691, Transition Loss -1.0499153137207031, Classifier Loss 0.16688789427280426, Total Loss 62.789024353027344\n",
      "6: Encoding Loss 6.9716620445251465, Transition Loss -0.9392627477645874, Classifier Loss 0.16756758093833923, Total Loss 58.58635711669922\n",
      "6: Encoding Loss 5.385442733764648, Transition Loss -1.182990312576294, Classifier Loss 0.17241878807544708, Total Loss 49.55406188964844\n",
      "6: Encoding Loss 3.4846673011779785, Transition Loss -1.7459125518798828, Classifier Loss 0.07379453629255295, Total Loss 28.286760330200195\n",
      "6: Encoding Loss 10.838756561279297, Transition Loss -1.1730833053588867, Classifier Loss 0.17211544513702393, Total Loss 82.24360656738281\n",
      "6: Encoding Loss 7.244383335113525, Transition Loss -2.572084903717041, Classifier Loss 0.07887090742588043, Total Loss 51.35236358642578\n",
      "6: Encoding Loss 5.175760746002197, Transition Loss -1.930725336074829, Classifier Loss 0.1571868658065796, Total Loss 46.772483825683594\n",
      "6: Encoding Loss 6.432711124420166, Transition Loss -2.0870254039764404, Classifier Loss 0.17555969953536987, Total Loss 56.15140151977539\n",
      "6: Encoding Loss 6.9737868309021, Transition Loss -1.4836097955703735, Classifier Loss 0.13155317306518555, Total Loss 54.99744415283203\n",
      "6: Encoding Loss 5.53341817855835, Transition Loss -0.6528969407081604, Classifier Loss 0.09648546576499939, Total Loss 42.84880065917969\n",
      "6: Encoding Loss 7.410869598388672, Transition Loss -2.5647480487823486, Classifier Loss 0.10173443704843521, Total Loss 54.63763427734375\n",
      "6: Encoding Loss 7.299923896789551, Transition Loss -1.3989338874816895, Classifier Loss 0.20831404626369476, Total Loss 64.6303939819336\n",
      "6: Encoding Loss 7.357375144958496, Transition Loss -2.2154717445373535, Classifier Loss 0.07073869556188583, Total Loss 51.21723937988281\n",
      "6: Encoding Loss 6.619607448577881, Transition Loss -0.49986886978149414, Classifier Loss 0.2110837697982788, Total Loss 60.825828552246094\n",
      "6: Encoding Loss 6.017779350280762, Transition Loss -1.8715944290161133, Classifier Loss 0.06591352075338364, Total Loss 42.69728088378906\n",
      "6: Encoding Loss 5.215996265411377, Transition Loss -1.361600637435913, Classifier Loss 0.08139752596616745, Total Loss 39.435184478759766\n",
      "6: Encoding Loss 4.083179473876953, Transition Loss -2.4697656631469727, Classifier Loss 0.06923743337392807, Total Loss 31.421833038330078\n",
      "6: Encoding Loss 4.237401008605957, Transition Loss -1.0498507022857666, Classifier Loss 0.1049850657582283, Total Loss 35.92249298095703\n",
      "6: Encoding Loss 4.045241832733154, Transition Loss -2.8822033405303955, Classifier Loss 0.0844528079032898, Total Loss 32.715579986572266\n",
      "6: Encoding Loss 7.6848039627075195, Transition Loss -0.9371132850646973, Classifier Loss 0.11120714992284775, Total Loss 57.22916793823242\n",
      "6: Encoding Loss 4.632933616638184, Transition Loss -1.7643238306045532, Classifier Loss 0.1165560930967331, Total Loss 39.45250701904297\n",
      "6: Encoding Loss 2.8960630893707275, Transition Loss -1.2034027576446533, Classifier Loss 0.057653795927762985, Total Loss 23.141277313232422\n",
      "6: Encoding Loss 7.2596588134765625, Transition Loss -2.211071491241455, Classifier Loss 0.08313828706741333, Total Loss 51.87089538574219\n",
      "6: Encoding Loss 7.865052700042725, Transition Loss -1.4130159616470337, Classifier Loss 0.09130106121301651, Total Loss 56.319862365722656\n",
      "6: Encoding Loss 6.106228828430176, Transition Loss -0.5501552820205688, Classifier Loss 0.14213325083255768, Total Loss 50.85047912597656\n",
      "6: Encoding Loss 6.035384178161621, Transition Loss -1.7160937786102295, Classifier Loss 0.04580308496952057, Total Loss 40.791927337646484\n",
      "6: Encoding Loss 4.942835330963135, Transition Loss -1.0505003929138184, Classifier Loss 0.11530962586402893, Total Loss 41.187557220458984\n",
      "6: Encoding Loss 5.8141608238220215, Transition Loss -1.6126060485839844, Classifier Loss 0.10109643638134003, Total Loss 44.99396896362305\n",
      "6: Encoding Loss 5.818572044372559, Transition Loss -1.1656010150909424, Classifier Loss 0.10531137883663177, Total Loss 45.442108154296875\n",
      "6: Encoding Loss 4.1104044914245605, Transition Loss -0.9480518102645874, Classifier Loss 0.09173435717821121, Total Loss 33.835487365722656\n",
      "6: Encoding Loss 5.917551517486572, Transition Loss -2.0163235664367676, Classifier Loss 0.1691906601190567, Total Loss 52.4235725402832\n",
      "6: Encoding Loss 3.8872058391571045, Transition Loss -1.1631546020507812, Classifier Loss 0.08479714393615723, Total Loss 31.802486419677734\n",
      "6: Encoding Loss 5.019481658935547, Transition Loss -0.47677817940711975, Classifier Loss 0.053875453770160675, Total Loss 35.50424575805664\n",
      "6: Encoding Loss 6.952325344085693, Transition Loss -1.418236255645752, Classifier Loss 0.19432298839092255, Total Loss 61.145687103271484\n",
      "6: Encoding Loss 5.293560981750488, Transition Loss -0.9785971641540527, Classifier Loss 0.07226435840129852, Total Loss 38.98740768432617\n",
      "6: Encoding Loss 5.684096336364746, Transition Loss -3.5758461952209473, Classifier Loss 0.08595263212919235, Total Loss 42.69841384887695\n",
      "6: Encoding Loss 4.382168292999268, Transition Loss -1.7339537143707275, Classifier Loss 0.0620679035782814, Total Loss 32.499107360839844\n",
      "6: Encoding Loss 5.446865558624268, Transition Loss -0.27830713987350464, Classifier Loss 0.1707649976015091, Total Loss 49.75758361816406\n",
      "6: Encoding Loss 3.756809711456299, Transition Loss -1.3734549283981323, Classifier Loss 0.10859754681587219, Total Loss 33.400062561035156\n",
      "6: Encoding Loss 8.184517860412598, Transition Loss -1.954521894454956, Classifier Loss 0.17420662939548492, Total Loss 66.52699279785156\n",
      "6: Encoding Loss 5.741446495056152, Transition Loss -0.8356119394302368, Classifier Loss 0.07605314254760742, Total Loss 42.05365753173828\n",
      "6: Encoding Loss 5.172476768493652, Transition Loss -2.5976362228393555, Classifier Loss 0.10914398729801178, Total Loss 41.94822311401367\n",
      "6: Encoding Loss 4.661341190338135, Transition Loss -2.8014981746673584, Classifier Loss 0.13603150844573975, Total Loss 41.57007598876953\n",
      "6: Encoding Loss 4.719322681427002, Transition Loss -1.6066827774047852, Classifier Loss 0.13780196011066437, Total Loss 42.09549331665039\n",
      "6: Encoding Loss 5.750704288482666, Transition Loss -1.3558512926101685, Classifier Loss 0.10223422199487686, Total Loss 44.727108001708984\n",
      "6: Encoding Loss 5.077731132507324, Transition Loss -1.9132174253463745, Classifier Loss 0.1867765635251999, Total Loss 49.14327621459961\n",
      "6: Encoding Loss 6.10510778427124, Transition Loss -1.355457067489624, Classifier Loss 0.0443030446767807, Total Loss 41.0604133605957\n",
      "6: Encoding Loss 4.768467426300049, Transition Loss 0.30593016743659973, Classifier Loss 0.06497795879840851, Total Loss 35.23097229003906\n",
      "6: Encoding Loss 5.7130608558654785, Transition Loss -1.9390783309936523, Classifier Loss 0.06695723533630371, Total Loss 40.97331619262695\n",
      "6: Encoding Loss 4.511979103088379, Transition Loss -1.5241135358810425, Classifier Loss 0.08485846221446991, Total Loss 35.55710983276367\n",
      "6: Encoding Loss 6.725809574127197, Transition Loss -0.44835028052330017, Classifier Loss 0.13376222550868988, Total Loss 53.730899810791016\n",
      "6: Encoding Loss 5.167679786682129, Transition Loss -2.3934128284454346, Classifier Loss 0.05276162922382355, Total Loss 36.28128433227539\n",
      "6: Encoding Loss 7.284800052642822, Transition Loss -1.6271955966949463, Classifier Loss 0.22529667615890503, Total Loss 66.2378158569336\n",
      "6: Encoding Loss 5.528048515319824, Transition Loss -1.1424446105957031, Classifier Loss 0.1263761669397354, Total Loss 45.805450439453125\n",
      "6: Encoding Loss 5.342081546783447, Transition Loss -1.8753496408462524, Classifier Loss 0.07379981130361557, Total Loss 39.43172073364258\n",
      "6: Encoding Loss 5.2518205642700195, Transition Loss -0.1503390222787857, Classifier Loss 0.08384300023317337, Total Loss 39.895164489746094\n",
      "6: Encoding Loss 5.336968421936035, Transition Loss -1.1921172142028809, Classifier Loss 0.06432164460420609, Total Loss 38.45349884033203\n",
      "6: Encoding Loss 5.6347174644470215, Transition Loss -0.37641191482543945, Classifier Loss 0.1245039850473404, Total Loss 46.2585563659668\n",
      "6: Encoding Loss 6.982231140136719, Transition Loss -0.44445857405662537, Classifier Loss 0.05695340037345886, Total Loss 47.58854675292969\n",
      "6: Encoding Loss 4.426011085510254, Transition Loss -1.6555848121643066, Classifier Loss 0.18966978788375854, Total Loss 45.52238082885742\n",
      "6: Encoding Loss 5.29705810546875, Transition Loss -0.46671390533447266, Classifier Loss 0.08683974295854568, Total Loss 40.46613693237305\n",
      "6: Encoding Loss 4.339292526245117, Transition Loss -0.47862085700035095, Classifier Loss 0.10538050532341003, Total Loss 36.57361602783203\n",
      "6: Encoding Loss 4.714276313781738, Transition Loss -1.697561264038086, Classifier Loss 0.04873085021972656, Total Loss 33.15806579589844\n",
      "6: Encoding Loss 4.901495933532715, Transition Loss -2.0044350624084473, Classifier Loss 0.18353892862796783, Total Loss 47.76206970214844\n",
      "6: Encoding Loss 4.90101432800293, Transition Loss -2.0885157585144043, Classifier Loss 0.14431579411029816, Total Loss 43.83683395385742\n",
      "6: Encoding Loss 3.616163969039917, Transition Loss -2.7887024879455566, Classifier Loss 0.08717477321624756, Total Loss 30.413347244262695\n",
      "6: Encoding Loss 4.044224739074707, Transition Loss -0.7004889249801636, Classifier Loss 0.13536468148231506, Total Loss 37.80154037475586\n",
      "6: Encoding Loss 5.711039066314697, Transition Loss -0.6289345026016235, Classifier Loss 0.1276562213897705, Total Loss 47.0316047668457\n",
      "6: Encoding Loss 3.6422314643859863, Transition Loss -2.214888095855713, Classifier Loss 0.07492551952600479, Total Loss 29.345056533813477\n",
      "6: Encoding Loss 5.041782379150391, Transition Loss 0.09882265329360962, Classifier Loss 0.05094068497419357, Total Loss 35.38429260253906\n",
      "6: Encoding Loss 5.854101657867432, Transition Loss -2.2339413166046143, Classifier Loss 0.09061551094055176, Total Loss 44.18526840209961\n",
      "6: Encoding Loss 4.206216335296631, Transition Loss -0.8946486711502075, Classifier Loss 0.06856571137905121, Total Loss 32.09351348876953\n",
      "6: Encoding Loss 8.259553909301758, Transition Loss 0.44758737087249756, Classifier Loss 0.2518494129180908, Total Loss 74.92129516601562\n",
      "6: Encoding Loss 6.154105186462402, Transition Loss -1.600029706954956, Classifier Loss 0.07079711556434631, Total Loss 44.00370407104492\n",
      "6: Encoding Loss 7.390878200531006, Transition Loss 0.33706891536712646, Classifier Loss 0.08322829008102417, Total Loss 52.80292892456055\n",
      "6: Encoding Loss 5.1066975593566895, Transition Loss -1.6904628276824951, Classifier Loss 0.15678218007087708, Total Loss 46.31772994995117\n",
      "6: Encoding Loss 3.507852554321289, Transition Loss -1.1703715324401855, Classifier Loss 0.10996823757886887, Total Loss 32.0434684753418\n",
      "6: Encoding Loss 6.439836025238037, Transition Loss -1.6876320838928223, Classifier Loss 0.12936556339263916, Total Loss 51.57490158081055\n",
      "6: Encoding Loss 9.20397663116455, Transition Loss -2.835491180419922, Classifier Loss 0.19687588512897491, Total Loss 74.91031646728516\n",
      "6: Encoding Loss 7.297617435455322, Transition Loss -1.5035576820373535, Classifier Loss 0.29209765791893005, Total Loss 72.99486541748047\n",
      "6: Encoding Loss 4.22174072265625, Transition Loss -1.3858082294464111, Classifier Loss 0.12228379398584366, Total Loss 37.55827331542969\n",
      "6: Encoding Loss 7.855225563049316, Transition Loss 0.2914004921913147, Classifier Loss 0.10656610131263733, Total Loss 57.90452575683594\n",
      "6: Encoding Loss 8.895946502685547, Transition Loss -1.4633878469467163, Classifier Loss 0.3332201838493347, Total Loss 86.69711303710938\n",
      "6: Encoding Loss 6.783329010009766, Transition Loss -0.8675644397735596, Classifier Loss 0.13050474226474762, Total Loss 53.75010299682617\n",
      "6: Encoding Loss 7.42012882232666, Transition Loss -0.34377753734588623, Classifier Loss 0.14027567207813263, Total Loss 58.54820251464844\n",
      "6: Encoding Loss 7.323982238769531, Transition Loss -0.5704502463340759, Classifier Loss 0.11429514735937119, Total Loss 55.3731803894043\n",
      "6: Encoding Loss 5.297998905181885, Transition Loss -1.9711196422576904, Classifier Loss 0.10821634531021118, Total Loss 42.60883712768555\n",
      "6: Encoding Loss 4.852022171020508, Transition Loss -1.8940246105194092, Classifier Loss 0.13057288527488708, Total Loss 42.168663024902344\n",
      "6: Encoding Loss 6.070677757263184, Transition Loss -3.3480629920959473, Classifier Loss 0.1415795236825943, Total Loss 50.58068084716797\n",
      "6: Encoding Loss 4.907508850097656, Transition Loss -2.233426570892334, Classifier Loss 0.08957479894161224, Total Loss 38.401641845703125\n",
      "6: Encoding Loss 4.3883256912231445, Transition Loss -1.2565338611602783, Classifier Loss 0.06858407706022263, Total Loss 33.18785858154297\n",
      "6: Encoding Loss 5.154173851013184, Transition Loss -1.7881370782852173, Classifier Loss 0.1697736382484436, Total Loss 47.901695251464844\n",
      "6: Encoding Loss 3.303401231765747, Transition Loss -1.8441152572631836, Classifier Loss 0.10156085342168808, Total Loss 29.97575569152832\n",
      "6: Encoding Loss 4.44276237487793, Transition Loss -1.5547935962677002, Classifier Loss 0.1246781051158905, Total Loss 39.12376403808594\n",
      "6: Encoding Loss 6.335200309753418, Transition Loss -1.6285299062728882, Classifier Loss 0.21833346784114838, Total Loss 59.84389877319336\n",
      "6: Encoding Loss 5.505326271057129, Transition Loss -0.4891691207885742, Classifier Loss 0.16353462636470795, Total Loss 49.38522720336914\n",
      "6: Encoding Loss 4.085339546203613, Transition Loss -1.8633928298950195, Classifier Loss 0.1361425220966339, Total Loss 38.125545501708984\n",
      "6: Encoding Loss 5.736295700073242, Transition Loss -2.2086434364318848, Classifier Loss 0.09279734641313553, Total Loss 43.696624755859375\n",
      "6: Encoding Loss 4.183470726013184, Transition Loss -0.6696463823318481, Classifier Loss 0.06580843031406403, Total Loss 31.6814022064209\n",
      "6: Encoding Loss 5.7609171867370605, Transition Loss -1.413129448890686, Classifier Loss 0.050569597631692886, Total Loss 39.62190246582031\n",
      "6: Encoding Loss 4.471370697021484, Transition Loss -1.4574627876281738, Classifier Loss 0.056995924562215805, Total Loss 32.5272331237793\n",
      "6: Encoding Loss 3.809786319732666, Transition Loss -0.6209205985069275, Classifier Loss 0.14402736723423004, Total Loss 37.261207580566406\n",
      "6: Encoding Loss 6.312352180480957, Transition Loss -0.778902530670166, Classifier Loss 0.14920920133590698, Total Loss 52.79472351074219\n",
      "6: Encoding Loss 6.498129844665527, Transition Loss -0.25836804509162903, Classifier Loss 0.14235259592533112, Total Loss 53.22393798828125\n",
      "6: Encoding Loss 6.521764278411865, Transition Loss -0.8335673213005066, Classifier Loss 0.10960172861814499, Total Loss 50.09042739868164\n",
      "6: Encoding Loss 3.0552167892456055, Transition Loss -2.269815444946289, Classifier Loss 0.10388194024562836, Total Loss 28.718585968017578\n",
      "6: Encoding Loss 7.496543884277344, Transition Loss -1.5743815898895264, Classifier Loss 0.16529804468154907, Total Loss 61.50843811035156\n",
      "6: Encoding Loss 4.879085063934326, Transition Loss -2.432704210281372, Classifier Loss 0.11296101659536362, Total Loss 40.56964111328125\n",
      "6: Encoding Loss 6.0138654708862305, Transition Loss -1.006643533706665, Classifier Loss 0.1041933074593544, Total Loss 46.50212097167969\n",
      "6: Encoding Loss 7.255988121032715, Transition Loss -2.1057426929473877, Classifier Loss 0.19309908151626587, Total Loss 62.84499740600586\n",
      "6: Encoding Loss 5.689692497253418, Transition Loss -2.094261407852173, Classifier Loss 0.15845073759555817, Total Loss 49.982391357421875\n",
      "6: Encoding Loss 7.4484076499938965, Transition Loss -0.24936865270137787, Classifier Loss 0.1491445004940033, Total Loss 59.60479736328125\n",
      "6: Encoding Loss 5.0413031578063965, Transition Loss 0.07924105226993561, Classifier Loss 0.11024598032236099, Total Loss 41.304115295410156\n",
      "6: Encoding Loss 5.0083327293396, Transition Loss -2.039966583251953, Classifier Loss 0.09584546089172363, Total Loss 39.63372802734375\n",
      "6: Encoding Loss 7.015421390533447, Transition Loss -0.8096892237663269, Classifier Loss 0.15075649321079254, Total Loss 57.16785430908203\n",
      "6: Encoding Loss 6.532132148742676, Transition Loss -1.3492939472198486, Classifier Loss 0.1129232719540596, Total Loss 50.48458480834961\n",
      "6: Encoding Loss 5.94098424911499, Transition Loss -0.9026790857315063, Classifier Loss 0.12291881442070007, Total Loss 47.93742752075195\n",
      "6: Encoding Loss 4.2201409339904785, Transition Loss 0.10021725296974182, Classifier Loss 0.12155871093273163, Total Loss 37.516807556152344\n",
      "6: Encoding Loss 3.224803924560547, Transition Loss -0.9045782089233398, Classifier Loss 0.0721874013543129, Total Loss 26.567201614379883\n",
      "6: Encoding Loss 5.632321357727051, Transition Loss -1.8293898105621338, Classifier Loss 0.0823875293135643, Total Loss 42.031951904296875\n",
      "6: Encoding Loss 4.912848949432373, Transition Loss -1.3638049364089966, Classifier Loss 0.10833977907896042, Total Loss 40.31052780151367\n",
      "6: Encoding Loss 4.8163743019104, Transition Loss -0.7740748524665833, Classifier Loss 0.21084189414978027, Total Loss 49.98212814331055\n",
      "6: Encoding Loss 4.380949020385742, Transition Loss -2.222658395767212, Classifier Loss 0.05633533000946045, Total Loss 31.9183406829834\n",
      "6: Encoding Loss 4.348247528076172, Transition Loss -2.619847536087036, Classifier Loss 0.12584808468818665, Total Loss 38.67324447631836\n",
      "6: Encoding Loss 1.8786853551864624, Transition Loss -0.7394107580184937, Classifier Loss 0.07283607125282288, Total Loss 18.555423736572266\n",
      "6: Encoding Loss 5.213623046875, Transition Loss -0.8336605429649353, Classifier Loss 0.0713045597076416, Total Loss 38.411865234375\n",
      "6: Encoding Loss 4.593465328216553, Transition Loss -1.4598495960235596, Classifier Loss 0.14886921644210815, Total Loss 42.4471321105957\n",
      "6: Encoding Loss 3.617577314376831, Transition Loss -0.778435230255127, Classifier Loss 0.06818675249814987, Total Loss 28.523828506469727\n",
      "6: Encoding Loss 5.005253314971924, Transition Loss -0.25866222381591797, Classifier Loss 0.04011347144842148, Total Loss 34.04276657104492\n",
      "6: Encoding Loss 7.434046745300293, Transition Loss -2.0796573162078857, Classifier Loss 0.20919309556484222, Total Loss 65.52275848388672\n",
      "6: Encoding Loss 5.049973487854004, Transition Loss -0.6251857280731201, Classifier Loss 0.20031802356243134, Total Loss 50.331390380859375\n",
      "6: Encoding Loss 5.087014198303223, Transition Loss -1.1234952211380005, Classifier Loss 0.20663554966449738, Total Loss 51.18518829345703\n",
      "6: Encoding Loss 4.61453914642334, Transition Loss -1.7848968505859375, Classifier Loss 0.09705866873264313, Total Loss 37.392391204833984\n",
      "6: Encoding Loss 6.403700351715088, Transition Loss -1.6679229736328125, Classifier Loss 0.1582995504140854, Total Loss 54.25149154663086\n",
      "6: Encoding Loss 5.660025596618652, Transition Loss -1.0527124404907227, Classifier Loss 0.11574103683233261, Total Loss 45.53384017944336\n",
      "6: Encoding Loss 5.967838287353516, Transition Loss -0.5676457285881042, Classifier Loss 0.08070702105760574, Total Loss 43.87750244140625\n",
      "6: Encoding Loss 4.227649688720703, Transition Loss -1.9127726554870605, Classifier Loss 0.11242114752531052, Total Loss 36.60725021362305\n",
      "6: Encoding Loss 6.391404628753662, Transition Loss -0.12705859541893005, Classifier Loss 0.09734316915273666, Total Loss 48.082698822021484\n",
      "6: Encoding Loss 4.864319324493408, Transition Loss -1.553269863128662, Classifier Loss 0.10561755299568176, Total Loss 39.74705123901367\n",
      "6: Encoding Loss 5.787062168121338, Transition Loss -1.4054371118545532, Classifier Loss 0.11260780692100525, Total Loss 45.98259353637695\n",
      "6: Encoding Loss 6.223584175109863, Transition Loss -0.6213339567184448, Classifier Loss 0.09439612925052643, Total Loss 46.7808723449707\n",
      "6: Encoding Loss 4.798335075378418, Transition Loss -1.8088632822036743, Classifier Loss 0.06006808578968048, Total Loss 34.79609680175781\n",
      "6: Encoding Loss 4.040088176727295, Transition Loss -1.2852658033370972, Classifier Loss 0.10373028367757797, Total Loss 34.61304473876953\n",
      "6: Encoding Loss 3.687690258026123, Transition Loss -1.215599775314331, Classifier Loss 0.09728605300188065, Total Loss 31.85426139831543\n",
      "6: Encoding Loss 7.4831061363220215, Transition Loss -2.402637481689453, Classifier Loss 0.10661792010068893, Total Loss 55.559471130371094\n",
      "6: Encoding Loss 4.217405796051025, Transition Loss -3.3217391967773438, Classifier Loss 0.0713573694229126, Total Loss 32.438846588134766\n",
      "6: Encoding Loss 3.1039233207702637, Transition Loss -1.1759920120239258, Classifier Loss 0.10683789104223251, Total Loss 29.306859970092773\n",
      "6: Encoding Loss 9.697867393493652, Transition Loss 0.3010752201080322, Classifier Loss 0.18171702325344086, Total Loss 76.47933959960938\n",
      "6: Encoding Loss 6.696664333343506, Transition Loss -0.1789679229259491, Classifier Loss 0.13049957156181335, Total Loss 53.22987365722656\n",
      "6: Encoding Loss 4.664865970611572, Transition Loss -2.577117919921875, Classifier Loss 0.0872940719127655, Total Loss 36.71757507324219\n",
      "6: Encoding Loss 4.083619594573975, Transition Loss -0.6773613691329956, Classifier Loss 0.0874466598033905, Total Loss 33.24611282348633\n",
      "6: Encoding Loss 6.841292381286621, Transition Loss -2.474782943725586, Classifier Loss 0.20534302294254303, Total Loss 61.58106994628906\n",
      "6: Encoding Loss 4.830045223236084, Transition Loss -1.1404671669006348, Classifier Loss 0.08617942780256271, Total Loss 37.597755432128906\n",
      "6: Encoding Loss 4.546942234039307, Transition Loss -1.1270654201507568, Classifier Loss 0.08241353929042816, Total Loss 35.522560119628906\n",
      "6: Encoding Loss 4.372928619384766, Transition Loss -1.5430805683135986, Classifier Loss 0.145145982503891, Total Loss 40.75155258178711\n",
      "6: Encoding Loss 6.2166876792907715, Transition Loss -0.6712979078292847, Classifier Loss 0.052420806139707565, Total Loss 42.54194259643555\n",
      "6: Encoding Loss 4.807839393615723, Transition Loss -1.4913605451583862, Classifier Loss 0.08869294822216034, Total Loss 37.715736389160156\n",
      "6: Encoding Loss 6.930254936218262, Transition Loss -2.0078418254852295, Classifier Loss 0.09050631523132324, Total Loss 50.6313591003418\n",
      "6: Encoding Loss 5.740079879760742, Transition Loss -0.6088536977767944, Classifier Loss 0.09069190919399261, Total Loss 43.50942611694336\n",
      "6: Encoding Loss 5.583688259124756, Transition Loss -0.34581229090690613, Classifier Loss 0.13666002452373505, Total Loss 47.167999267578125\n",
      "6: Encoding Loss 7.326425075531006, Transition Loss -1.065521478652954, Classifier Loss 0.05704360827803612, Total Loss 49.6624870300293\n",
      "6: Encoding Loss 5.9296345710754395, Transition Loss -1.116541862487793, Classifier Loss 0.19095829129219055, Total Loss 54.67319107055664\n",
      "6: Encoding Loss 5.214280128479004, Transition Loss -0.5696669220924377, Classifier Loss 0.11073850095272064, Total Loss 42.35930252075195\n",
      "6: Encoding Loss 3.2943930625915527, Transition Loss -1.0194774866104126, Classifier Loss 0.07676640152931213, Total Loss 27.442590713500977\n",
      "6: Encoding Loss 4.8787736892700195, Transition Loss -1.5809036493301392, Classifier Loss 0.09196865558624268, Total Loss 38.4688720703125\n",
      "6: Encoding Loss 5.110708713531494, Transition Loss -0.32928723096847534, Classifier Loss 0.09491443634033203, Total Loss 40.15556335449219\n",
      "6: Encoding Loss 4.533298492431641, Transition Loss 0.3215067982673645, Classifier Loss 0.11834404617547989, Total Loss 39.16279983520508\n",
      "6: Encoding Loss 4.329191207885742, Transition Loss -1.7270681858062744, Classifier Loss 0.0860331803560257, Total Loss 34.57777786254883\n",
      "6: Encoding Loss 3.2558605670928955, Transition Loss -2.6857094764709473, Classifier Loss 0.08060669898986816, Total Loss 27.594758987426758\n",
      "6: Encoding Loss 6.255507469177246, Transition Loss -1.9813586473464966, Classifier Loss 0.11326868832111359, Total Loss 48.85912322998047\n",
      "6: Encoding Loss 5.305995941162109, Transition Loss -2.1515674591064453, Classifier Loss 0.17046570777893066, Total Loss 48.881683349609375\n",
      "6: Encoding Loss 5.354743957519531, Transition Loss -0.07750898599624634, Classifier Loss 0.11149168014526367, Total Loss 43.27760314941406\n",
      "6: Encoding Loss 5.262242317199707, Transition Loss -1.007996678352356, Classifier Loss 0.13935619592666626, Total Loss 45.5086669921875\n",
      "6: Encoding Loss 6.326966285705566, Transition Loss -0.6222356557846069, Classifier Loss 0.1733465939760208, Total Loss 55.29621124267578\n",
      "6: Encoding Loss 5.4517951011657715, Transition Loss -0.6646037101745605, Classifier Loss 0.13463249802589417, Total Loss 46.17375564575195\n",
      "6: Encoding Loss 4.739877700805664, Transition Loss -1.158767819404602, Classifier Loss 0.09803147614002228, Total Loss 38.24195098876953\n",
      "6: Encoding Loss 5.403237819671631, Transition Loss -2.850043773651123, Classifier Loss 0.09127230942249298, Total Loss 41.5455207824707\n",
      "6: Encoding Loss 4.763611793518066, Transition Loss -0.9297648072242737, Classifier Loss 0.06625761091709137, Total Loss 35.207061767578125\n",
      "6: Encoding Loss 4.958122730255127, Transition Loss -1.475208044052124, Classifier Loss 0.11697874218225479, Total Loss 41.44601821899414\n",
      "6: Encoding Loss 5.971170902252197, Transition Loss -2.8362064361572266, Classifier Loss 0.06124260276556015, Total Loss 41.95015335083008\n",
      "6: Encoding Loss 6.627098083496094, Transition Loss -0.8581703901290894, Classifier Loss 0.06201683729887009, Total Loss 45.96392822265625\n",
      "6: Encoding Loss 3.240187406539917, Transition Loss -0.7774706482887268, Classifier Loss 0.07661975920200348, Total Loss 27.10279083251953\n",
      "6: Encoding Loss 5.741979122161865, Transition Loss -1.444832444190979, Classifier Loss 0.0587407723069191, Total Loss 40.325374603271484\n",
      "6: Encoding Loss 6.013644218444824, Transition Loss -1.257922649383545, Classifier Loss 0.07410456240177155, Total Loss 43.4918212890625\n",
      "6: Encoding Loss 4.027740478515625, Transition Loss -0.6451054811477661, Classifier Loss 0.0417766198515892, Total Loss 28.343849182128906\n",
      "6: Encoding Loss 5.091702461242676, Transition Loss -2.7354884147644043, Classifier Loss 0.1355358511209488, Total Loss 44.10270309448242\n",
      "6: Encoding Loss 6.905035495758057, Transition Loss -0.3324289917945862, Classifier Loss 0.10160262882709503, Total Loss 51.5903434753418\n",
      "6: Encoding Loss 6.705157279968262, Transition Loss -1.0248264074325562, Classifier Loss 0.13710032403469086, Total Loss 53.94057083129883\n",
      "6: Encoding Loss 5.368000030517578, Transition Loss -1.4617488384246826, Classifier Loss 0.08078480511903763, Total Loss 40.28589630126953\n",
      "6: Encoding Loss 5.253836631774902, Transition Loss -1.5583462715148926, Classifier Loss 0.09244959056377411, Total Loss 40.76736068725586\n",
      "6: Encoding Loss 5.234266757965088, Transition Loss -0.7502208948135376, Classifier Loss 0.17757558822631836, Total Loss 49.16286087036133\n",
      "6: Encoding Loss 6.283124923706055, Transition Loss -1.658815860748291, Classifier Loss 0.07638505101203918, Total Loss 45.33658981323242\n",
      "6: Encoding Loss 3.471348524093628, Transition Loss -1.2076902389526367, Classifier Loss 0.1441574990749359, Total Loss 35.24335861206055\n",
      "6: Encoding Loss 8.242696762084961, Transition Loss 0.3375246226787567, Classifier Loss 0.13650140166282654, Total Loss 63.2413330078125\n",
      "6: Encoding Loss 5.049229621887207, Transition Loss -1.128572940826416, Classifier Loss 0.059418559074401855, Total Loss 36.23678207397461\n",
      "6: Encoding Loss 3.1075491905212402, Transition Loss -2.2959487438201904, Classifier Loss 0.07428781688213348, Total Loss 26.07316017150879\n",
      "6: Encoding Loss 6.176231384277344, Transition Loss -0.24355091154575348, Classifier Loss 0.06048447638750076, Total Loss 43.105735778808594\n",
      "6: Encoding Loss 2.7706611156463623, Transition Loss -0.9448891878128052, Classifier Loss 0.04828398674726486, Total Loss 21.451988220214844\n",
      "6: Encoding Loss 7.138855934143066, Transition Loss -1.7366011142730713, Classifier Loss 0.13955947756767273, Total Loss 56.78839111328125\n",
      "6: Encoding Loss 6.991915225982666, Transition Loss -1.077052116394043, Classifier Loss 0.11919547617435455, Total Loss 53.870609283447266\n",
      "6: Encoding Loss 5.645931243896484, Transition Loss -2.1352505683898926, Classifier Loss 0.07474662363529205, Total Loss 41.349395751953125\n",
      "6: Encoding Loss 3.9760653972625732, Transition Loss -1.1027344465255737, Classifier Loss 0.1347534954547882, Total Loss 37.331298828125\n",
      "6: Encoding Loss 4.5467119216918945, Transition Loss -1.5609952211380005, Classifier Loss 0.10485606640577316, Total Loss 37.76525115966797\n",
      "6: Encoding Loss 5.379661560058594, Transition Loss -1.637535810470581, Classifier Loss 0.09710771590471268, Total Loss 41.98808288574219\n",
      "6: Encoding Loss 3.5787854194641113, Transition Loss -0.8487423658370972, Classifier Loss 0.09676235914230347, Total Loss 31.148609161376953\n",
      "6: Encoding Loss 6.319538116455078, Transition Loss -2.13992977142334, Classifier Loss 0.18657159805297852, Total Loss 56.57353210449219\n",
      "6: Encoding Loss 4.241130828857422, Transition Loss -1.9599601030349731, Classifier Loss 0.07017102837562561, Total Loss 32.463104248046875\n",
      "6: Encoding Loss 3.4864661693573, Transition Loss -0.7353586554527283, Classifier Loss 0.07635937631130219, Total Loss 28.554443359375\n",
      "6: Encoding Loss 5.550593376159668, Transition Loss -0.019757062196731567, Classifier Loss 0.14411766827106476, Total Loss 47.7153205871582\n",
      "6: Encoding Loss 4.873089790344238, Transition Loss -1.7531882524490356, Classifier Loss 0.08676557242870331, Total Loss 37.91439437866211\n",
      "6: Encoding Loss 7.515503883361816, Transition Loss -0.6659097075462341, Classifier Loss 0.11459213495254517, Total Loss 56.551971435546875\n",
      "6: Encoding Loss 4.063608646392822, Transition Loss -2.492656707763672, Classifier Loss 0.1960468590259552, Total Loss 43.98534393310547\n",
      "6: Encoding Loss 6.454519748687744, Transition Loss -1.1620025634765625, Classifier Loss 0.10691246390342712, Total Loss 49.41790008544922\n",
      "6: Encoding Loss 4.523952484130859, Transition Loss -1.2371330261230469, Classifier Loss 0.05866546928882599, Total Loss 33.009769439697266\n",
      "6: Encoding Loss 8.367759704589844, Transition Loss -0.31358885765075684, Classifier Loss 0.15422087907791138, Total Loss 65.62852478027344\n",
      "6: Encoding Loss 5.359145164489746, Transition Loss -1.2671633958816528, Classifier Loss 0.136734277009964, Total Loss 45.82779312133789\n",
      "6: Encoding Loss 4.395086288452148, Transition Loss -1.049523949623108, Classifier Loss 0.09446414560079575, Total Loss 35.81651306152344\n",
      "6: Encoding Loss 5.011691093444824, Transition Loss -2.7413382530212402, Classifier Loss 0.0972142145037651, Total Loss 39.79047393798828\n",
      "6: Encoding Loss 3.599807024002075, Transition Loss 0.566611647605896, Classifier Loss 0.12639807164669037, Total Loss 34.465293884277344\n",
      "6: Encoding Loss 3.490675687789917, Transition Loss -0.2406323254108429, Classifier Loss 0.05714327096939087, Total Loss 26.658287048339844\n",
      "6: Encoding Loss 4.763380527496338, Transition Loss -2.4789602756500244, Classifier Loss 0.15420949459075928, Total Loss 44.000244140625\n",
      "6: Encoding Loss 5.776881217956543, Transition Loss -1.745261788368225, Classifier Loss 0.1605318933725357, Total Loss 50.71377944946289\n",
      "6: Encoding Loss 4.347652912139893, Transition Loss -0.6925358772277832, Classifier Loss 0.09881994873285294, Total Loss 35.96763610839844\n",
      "6: Encoding Loss 4.6789350509643555, Transition Loss -0.6129017472267151, Classifier Loss 0.08763471245765686, Total Loss 36.83683776855469\n",
      "6: Encoding Loss 4.302133083343506, Transition Loss -1.1945948600769043, Classifier Loss 0.04901563748717308, Total Loss 30.713884353637695\n",
      "6: Encoding Loss 7.083161354064941, Transition Loss -0.17053762078285217, Classifier Loss 0.2499682903289795, Total Loss 67.49573516845703\n",
      "6: Encoding Loss 5.108233451843262, Transition Loss -1.2462191581726074, Classifier Loss 0.08572874218225479, Total Loss 39.22177505493164\n",
      "6: Encoding Loss 6.7613372802734375, Transition Loss -1.5406091213226318, Classifier Loss 0.08981873095035553, Total Loss 49.549278259277344\n",
      "6: Encoding Loss 4.13963508605957, Transition Loss -0.1083594262599945, Classifier Loss 0.06711126863956451, Total Loss 31.54889488220215\n",
      "6: Encoding Loss 5.61993408203125, Transition Loss -1.621049404144287, Classifier Loss 0.04292789101600647, Total Loss 38.01174545288086\n",
      "6: Encoding Loss 4.85257625579834, Transition Loss -0.8823353052139282, Classifier Loss 0.09869218617677689, Total Loss 38.98432540893555\n",
      "6: Encoding Loss 4.902736663818359, Transition Loss -1.3938393592834473, Classifier Loss 0.21182787418365479, Total Loss 50.59865188598633\n",
      "6: Encoding Loss 8.593497276306152, Transition Loss -0.3859422206878662, Classifier Loss 0.22750771045684814, Total Loss 74.31159973144531\n",
      "6: Encoding Loss 6.7895307540893555, Transition Loss -1.3608016967773438, Classifier Loss 0.09152691066265106, Total Loss 49.88933181762695\n",
      "6: Encoding Loss 6.777446746826172, Transition Loss -1.3509793281555176, Classifier Loss 0.17393802106380463, Total Loss 58.05794143676758\n",
      "6: Encoding Loss 4.90043830871582, Transition Loss -1.8448669910430908, Classifier Loss 0.0463993214070797, Total Loss 34.04182815551758\n",
      "6: Encoding Loss 5.6074628829956055, Transition Loss -1.1956329345703125, Classifier Loss 0.08136075735092163, Total Loss 41.78037643432617\n",
      "6: Encoding Loss 6.8257293701171875, Transition Loss -1.114546775817871, Classifier Loss 0.10436194390058517, Total Loss 51.3901252746582\n",
      "6: Encoding Loss 5.219171047210693, Transition Loss -1.3807815313339233, Classifier Loss 0.12683336436748505, Total Loss 43.99781036376953\n",
      "6: Encoding Loss 3.5415656566619873, Transition Loss 0.35510164499282837, Classifier Loss 0.04966738820075989, Total Loss 26.358173370361328\n",
      "6: Encoding Loss 3.9101099967956543, Transition Loss -0.7012447118759155, Classifier Loss 0.08771784603595734, Total Loss 32.23216247558594\n",
      "6: Encoding Loss 4.586960792541504, Transition Loss 0.16711416840553284, Classifier Loss 0.08463691920042038, Total Loss 36.052303314208984\n",
      "6: Encoding Loss 4.991152286529541, Transition Loss -1.4132285118103027, Classifier Loss 0.065575011074543, Total Loss 36.50385284423828\n",
      "6: Encoding Loss 7.123028755187988, Transition Loss -0.38675063848495483, Classifier Loss 0.19891509413719177, Total Loss 62.6295280456543\n",
      "6: Encoding Loss 5.479196071624756, Transition Loss -1.7785722017288208, Classifier Loss 0.08732795715332031, Total Loss 41.60726547241211\n",
      "6: Encoding Loss 8.313998222351074, Transition Loss -2.8007092475891113, Classifier Loss 0.12457386404275894, Total Loss 62.34025573730469\n",
      "6: Encoding Loss 6.0276594161987305, Transition Loss -1.6430773735046387, Classifier Loss 0.08793486654758453, Total Loss 44.95878982543945\n",
      "6: Encoding Loss 3.653186082839966, Transition Loss -1.1254429817199707, Classifier Loss 0.09832315146923065, Total Loss 31.7509822845459\n",
      "6: Encoding Loss 3.992678165435791, Transition Loss -0.42027637362480164, Classifier Loss 0.02578078769147396, Total Loss 26.533981323242188\n",
      "6: Encoding Loss 6.579676628112793, Transition Loss -3.0577316284179688, Classifier Loss 0.249098002910614, Total Loss 64.38664245605469\n",
      "6: Encoding Loss 5.359885215759277, Transition Loss -1.4508583545684814, Classifier Loss 0.14943601191043854, Total Loss 47.102333068847656\n",
      "6: Encoding Loss 3.780701160430908, Transition Loss 0.7758321762084961, Classifier Loss 0.05782104656100273, Total Loss 28.77664566040039\n",
      "6: Encoding Loss 5.49037504196167, Transition Loss -1.0222777128219604, Classifier Loss 0.23351910710334778, Total Loss 56.293758392333984\n",
      "6: Encoding Loss 4.3063836097717285, Transition Loss -1.7109867334365845, Classifier Loss 0.12198206782341003, Total Loss 38.03582763671875\n",
      "6: Encoding Loss 6.273895740509033, Transition Loss -0.8254071474075317, Classifier Loss 0.1872815042734146, Total Loss 56.37119674682617\n",
      "6: Encoding Loss 3.2054717540740967, Transition Loss -0.4657402038574219, Classifier Loss 0.06365357339382172, Total Loss 25.598003387451172\n",
      "6: Encoding Loss 3.025902509689331, Transition Loss -0.6607784032821655, Classifier Loss 0.1626332849264145, Total Loss 34.41848373413086\n",
      "6: Encoding Loss 5.737309455871582, Transition Loss -2.3210110664367676, Classifier Loss 0.11484819650650024, Total Loss 45.90774917602539\n",
      "6: Encoding Loss 4.994615077972412, Transition Loss -1.5474588871002197, Classifier Loss 0.1667024791240692, Total Loss 46.63732147216797\n",
      "6: Encoding Loss 5.464217185974121, Transition Loss -2.248051166534424, Classifier Loss 0.13839562237262726, Total Loss 46.623966217041016\n",
      "6: Encoding Loss 5.758217811584473, Transition Loss -1.747816562652588, Classifier Loss 0.053118348121643066, Total Loss 39.86044692993164\n",
      "6: Encoding Loss 5.5578460693359375, Transition Loss -2.720655918121338, Classifier Loss 0.11656934022903442, Total Loss 45.00292205810547\n",
      "6: Encoding Loss 7.209273338317871, Transition Loss -1.1418588161468506, Classifier Loss 0.16161620616912842, Total Loss 59.416805267333984\n",
      "6: Encoding Loss 6.215476989746094, Transition Loss -1.6803243160247803, Classifier Loss 0.16358163952827454, Total Loss 53.650352478027344\n",
      "6: Encoding Loss 5.818356513977051, Transition Loss -0.8760318756103516, Classifier Loss 0.07734499126672745, Total Loss 42.644287109375\n",
      "6: Encoding Loss 4.022315979003906, Transition Loss -2.2256178855895996, Classifier Loss 0.08805440366268158, Total Loss 32.93844985961914\n",
      "6: Encoding Loss 6.506609916687012, Transition Loss -1.580003023147583, Classifier Loss 0.10616520047187805, Total Loss 49.655548095703125\n",
      "6: Encoding Loss 4.515122890472412, Transition Loss -2.2283220291137695, Classifier Loss 0.08440279960632324, Total Loss 35.53012466430664\n",
      "6: Encoding Loss 5.749824523925781, Transition Loss -1.8909331560134888, Classifier Loss 0.055537886917591095, Total Loss 40.051979064941406\n",
      "6: Encoding Loss 4.698182582855225, Transition Loss -1.0248667001724243, Classifier Loss 0.07473978400230408, Total Loss 35.66266632080078\n",
      "6: Encoding Loss 5.3838067054748535, Transition Loss -1.6065263748168945, Classifier Loss 0.13796328008174896, Total Loss 46.09852600097656\n",
      "6: Encoding Loss 3.9238059520721436, Transition Loss 0.14859536290168762, Classifier Loss 0.06196897476911545, Total Loss 29.79917335510254\n",
      "6: Encoding Loss 8.4883394241333, Transition Loss -1.3323752880096436, Classifier Loss 0.1835489422082901, Total Loss 69.2844009399414\n",
      "6: Encoding Loss 7.933483123779297, Transition Loss -2.1561148166656494, Classifier Loss 0.1623586118221283, Total Loss 63.835899353027344\n",
      "6: Encoding Loss 7.432580471038818, Transition Loss -1.1396996974945068, Classifier Loss 0.19056329131126404, Total Loss 63.65135955810547\n",
      "6: Encoding Loss 6.153859615325928, Transition Loss -0.32035374641418457, Classifier Loss 0.12607063353061676, Total Loss 49.530094146728516\n",
      "6: Encoding Loss 7.3649702072143555, Transition Loss -0.25180506706237793, Classifier Loss 0.22995103895664215, Total Loss 67.18482971191406\n",
      "6: Encoding Loss 4.593888759613037, Transition Loss 0.15497224032878876, Classifier Loss 0.0629582554101944, Total Loss 33.921146392822266\n",
      "6: Encoding Loss 3.7020390033721924, Transition Loss -0.9373124837875366, Classifier Loss 0.10863403975963593, Total Loss 33.07526397705078\n",
      "6: Encoding Loss 2.7353601455688477, Transition Loss 0.33141350746154785, Classifier Loss 0.10107138752937317, Total Loss 26.65186309814453\n",
      "6: Encoding Loss 5.223597049713135, Transition Loss -0.722328782081604, Classifier Loss 0.1393517553806305, Total Loss 45.27647018432617\n",
      "6: Encoding Loss 6.164596080780029, Transition Loss -1.7605831623077393, Classifier Loss 0.1109536811709404, Total Loss 48.08224105834961\n",
      "6: Encoding Loss 7.015509605407715, Transition Loss -1.7388707399368286, Classifier Loss 0.19211901724338531, Total Loss 61.30426788330078\n",
      "6: Encoding Loss 4.28521203994751, Transition Loss -1.7311824560165405, Classifier Loss 0.07457840442657471, Total Loss 33.168418884277344\n",
      "6: Encoding Loss 6.354552268981934, Transition Loss -2.3310813903808594, Classifier Loss 0.17706015706062317, Total Loss 55.832401275634766\n",
      "6: Encoding Loss 4.7887043952941895, Transition Loss -1.155768632888794, Classifier Loss 0.07958763092756271, Total Loss 36.690528869628906\n",
      "6: Encoding Loss 4.774828910827637, Transition Loss -1.620396614074707, Classifier Loss 0.10546403378248215, Total Loss 39.19472885131836\n",
      "6: Encoding Loss 4.489270210266113, Transition Loss -3.1875107288360596, Classifier Loss 0.08877778053283691, Total Loss 35.81212615966797\n",
      "6: Encoding Loss 2.6261534690856934, Transition Loss -1.8404170274734497, Classifier Loss 0.07749947905540466, Total Loss 23.506134033203125\n",
      "6: Encoding Loss 5.892401695251465, Transition Loss -1.7373411655426025, Classifier Loss 0.1004590094089508, Total Loss 45.399620056152344\n",
      "6: Encoding Loss 5.223371505737305, Transition Loss -1.9447784423828125, Classifier Loss 0.15892469882965088, Total Loss 47.2319221496582\n",
      "6: Encoding Loss 3.19775652885437, Transition Loss -1.7021992206573486, Classifier Loss 0.08180616050958633, Total Loss 27.36647605895996\n",
      "6: Encoding Loss 6.845428466796875, Transition Loss -2.3122622966766357, Classifier Loss 0.08814379572868347, Total Loss 49.88602828979492\n",
      "6: Encoding Loss 4.705935001373291, Transition Loss -0.9942963719367981, Classifier Loss 0.08520882576704025, Total Loss 36.75609588623047\n",
      "6: Encoding Loss 6.5337018966674805, Transition Loss -1.1712809801101685, Classifier Loss 0.12034783512353897, Total Loss 51.23652648925781\n",
      "6: Encoding Loss 7.003238677978516, Transition Loss -1.5297009944915771, Classifier Loss 0.15536129474639893, Total Loss 57.55495071411133\n",
      "6: Encoding Loss 6.389502048492432, Transition Loss -0.8831605911254883, Classifier Loss 0.10887123644351959, Total Loss 49.22378158569336\n",
      "6: Encoding Loss 4.8588690757751465, Transition Loss -1.5764671564102173, Classifier Loss 0.11692117899656296, Total Loss 40.844703674316406\n",
      "6: Encoding Loss 5.967233180999756, Transition Loss -1.8472658395767212, Classifier Loss 0.12011232227087021, Total Loss 47.81389617919922\n",
      "6: Encoding Loss 5.261693000793457, Transition Loss -1.6101654767990112, Classifier Loss 0.08296936005353928, Total Loss 39.866451263427734\n",
      "6: Encoding Loss 2.788229465484619, Transition Loss -0.20130425691604614, Classifier Loss 0.04571345075964928, Total Loss 21.300642013549805\n",
      "6: Encoding Loss 7.2565598487854, Transition Loss -0.25675728917121887, Classifier Loss 0.09127046167850494, Total Loss 52.66630172729492\n",
      "6: Encoding Loss 6.495446681976318, Transition Loss -1.3364367485046387, Classifier Loss 0.08480294793844223, Total Loss 47.45244598388672\n",
      "6: Encoding Loss 6.544663906097412, Transition Loss -1.0379332304000854, Classifier Loss 0.043354183435440063, Total Loss 43.602989196777344\n",
      "6: Encoding Loss 4.234731197357178, Transition Loss -0.11919593811035156, Classifier Loss 0.06393375247716904, Total Loss 31.801715850830078\n",
      "6: Encoding Loss 4.296835899353027, Transition Loss -2.1060380935668945, Classifier Loss 0.0473899319767952, Total Loss 30.519166946411133\n",
      "6: Encoding Loss 4.0146307945251465, Transition Loss -1.6987323760986328, Classifier Loss 0.14837417006492615, Total Loss 38.924522399902344\n",
      "6: Encoding Loss 5.501967906951904, Transition Loss -0.45857352018356323, Classifier Loss 0.0577506497502327, Total Loss 38.78669357299805\n",
      "6: Encoding Loss 4.672337532043457, Transition Loss -1.0976977348327637, Classifier Loss 0.05466940626502037, Total Loss 33.500526428222656\n",
      "6: Encoding Loss 4.381804466247559, Transition Loss -0.5987531542778015, Classifier Loss 0.05058467388153076, Total Loss 31.349056243896484\n",
      "6: Encoding Loss 3.668652296066284, Transition Loss -0.9417507648468018, Classifier Loss 0.07667821645736694, Total Loss 29.67936134338379\n",
      "6: Encoding Loss 6.228822231292725, Transition Loss -1.4527387619018555, Classifier Loss 0.08730914443731308, Total Loss 46.103271484375\n",
      "6: Encoding Loss 4.440797328948975, Transition Loss -1.8736732006072998, Classifier Loss 0.08290085941553116, Total Loss 34.93412399291992\n",
      "6: Encoding Loss 3.892981767654419, Transition Loss -2.4733076095581055, Classifier Loss 0.06289036571979523, Total Loss 29.645936965942383\n",
      "6: Encoding Loss 3.810288190841675, Transition Loss -1.4737141132354736, Classifier Loss 0.12203057110309601, Total Loss 35.0641975402832\n",
      "6: Encoding Loss 6.11867618560791, Transition Loss -0.8416368961334229, Classifier Loss 0.11012788116931915, Total Loss 47.724510192871094\n",
      "6: Encoding Loss 3.681337833404541, Transition Loss -0.9084323048591614, Classifier Loss 0.06276999413967133, Total Loss 28.36466407775879\n",
      "6: Encoding Loss 4.524028301239014, Transition Loss -1.159440279006958, Classifier Loss 0.10022373497486115, Total Loss 37.16607666015625\n",
      "6: Encoding Loss 4.924574851989746, Transition Loss -2.7833094596862793, Classifier Loss 0.08873631805181503, Total Loss 38.41996765136719\n",
      "6: Encoding Loss 4.332742214202881, Transition Loss -1.1396667957305908, Classifier Loss 0.051110971719026566, Total Loss 31.10709571838379\n",
      "6: Encoding Loss 3.5362887382507324, Transition Loss -0.7805709838867188, Classifier Loss 0.06598490476608276, Total Loss 27.81591033935547\n",
      "6: Encoding Loss 4.640254020690918, Transition Loss -1.721551775932312, Classifier Loss 0.15631769597530365, Total Loss 43.47260665893555\n",
      "6: Encoding Loss 3.398909330368042, Transition Loss -2.315023899078369, Classifier Loss 0.0936637744307518, Total Loss 29.758909225463867\n",
      "6: Encoding Loss 3.673656940460205, Transition Loss -0.9110152125358582, Classifier Loss 0.09136401861906052, Total Loss 31.177980422973633\n",
      "6: Encoding Loss 5.04421329498291, Transition Loss -2.330510139465332, Classifier Loss 0.0875156968832016, Total Loss 39.01591873168945\n",
      "6: Encoding Loss 5.6201887130737305, Transition Loss -1.0419845581054688, Classifier Loss 0.16290119290351868, Total Loss 50.01083755493164\n",
      "6: Encoding Loss 5.749505996704102, Transition Loss -1.5750041007995605, Classifier Loss 0.11578263342380524, Total Loss 46.074668884277344\n",
      "6: Encoding Loss 3.455937147140503, Transition Loss -2.101167917251587, Classifier Loss 0.09558074921369553, Total Loss 30.292858123779297\n",
      "6: Encoding Loss 6.336267948150635, Transition Loss -1.173596978187561, Classifier Loss 0.1863776296377182, Total Loss 56.654903411865234\n",
      "6: Encoding Loss 7.4229254722595215, Transition Loss -1.903207540512085, Classifier Loss 0.2501707077026367, Total Loss 69.55386352539062\n",
      "6: Encoding Loss 7.334092140197754, Transition Loss -1.828883409500122, Classifier Loss 0.14297963678836823, Total Loss 58.301788330078125\n",
      "6: Encoding Loss 7.157412528991699, Transition Loss -2.352911949157715, Classifier Loss 0.129355788230896, Total Loss 55.87911605834961\n",
      "6: Encoding Loss 4.357203483581543, Transition Loss -2.9026153087615967, Classifier Loss 0.09385602176189423, Total Loss 35.52766418457031\n",
      "6: Encoding Loss 7.702228546142578, Transition Loss -2.4160025119781494, Classifier Loss 0.09368690103292465, Total Loss 55.58109664916992\n",
      "6: Encoding Loss 7.9593915939331055, Transition Loss -0.27735695242881775, Classifier Loss 0.1271171271800995, Total Loss 60.467952728271484\n",
      "6: Encoding Loss 6.805083274841309, Transition Loss -2.5052220821380615, Classifier Loss 0.251710444688797, Total Loss 66.00054931640625\n",
      "6: Encoding Loss 7.601894378662109, Transition Loss -1.9415390491485596, Classifier Loss 0.16187001764774323, Total Loss 61.79759216308594\n",
      "6: Encoding Loss 6.6649980545043945, Transition Loss 0.35249823331832886, Classifier Loss 0.07338493317365646, Total Loss 47.469482421875\n",
      "6: Encoding Loss 5.405473709106445, Transition Loss -1.670253038406372, Classifier Loss 0.04432693123817444, Total Loss 36.8648681640625\n",
      "6: Encoding Loss 2.8156046867370605, Transition Loss -1.3323698043823242, Classifier Loss 0.10408024489879608, Total Loss 27.301122665405273\n",
      "6: Encoding Loss 4.252135276794434, Transition Loss -0.7626143097877502, Classifier Loss 0.08587083220481873, Total Loss 34.09959030151367\n",
      "6: Encoding Loss 3.8365566730499268, Transition Loss 0.503983736038208, Classifier Loss 0.07223465293645859, Total Loss 30.444398880004883\n",
      "6: Encoding Loss 2.1348280906677246, Transition Loss -1.235969066619873, Classifier Loss 0.04938709735870361, Total Loss 17.74718475341797\n",
      "6: Encoding Loss 4.259835243225098, Transition Loss -1.010826826095581, Classifier Loss 0.16395555436611176, Total Loss 41.95416259765625\n",
      "6: Encoding Loss 4.061326503753662, Transition Loss -0.9457535147666931, Classifier Loss 0.1600293517112732, Total Loss 40.37051773071289\n",
      "6: Encoding Loss 6.780726432800293, Transition Loss -0.2501554489135742, Classifier Loss 0.06608830392360687, Total Loss 47.2930908203125\n",
      "6: Encoding Loss 5.638030529022217, Transition Loss -2.261812925338745, Classifier Loss 0.0866398960351944, Total Loss 42.49127197265625\n",
      "6: Encoding Loss 7.945701599121094, Transition Loss -1.3122341632843018, Classifier Loss 0.29343903064727783, Total Loss 77.01758575439453\n",
      "6: Encoding Loss 5.331267833709717, Transition Loss -2.4309120178222656, Classifier Loss 0.14955218136310577, Total Loss 46.94185256958008\n",
      "6: Encoding Loss 4.496624946594238, Transition Loss -1.4447977542877197, Classifier Loss 0.05909687653183937, Total Loss 32.88886260986328\n",
      "6: Encoding Loss 6.548558235168457, Transition Loss -1.806805968284607, Classifier Loss 0.1584911197423935, Total Loss 55.13974380493164\n",
      "6: Encoding Loss 6.682125091552734, Transition Loss -1.6925519704818726, Classifier Loss 0.12218037992715836, Total Loss 52.31011199951172\n",
      "6: Encoding Loss 5.129510402679443, Transition Loss 0.07131174206733704, Classifier Loss 0.140426903963089, Total Loss 44.84828186035156\n",
      "6: Encoding Loss 4.309935569763184, Transition Loss 0.6108258962631226, Classifier Loss 0.08740347623825073, Total Loss 34.844295501708984\n",
      "6: Encoding Loss 5.138951301574707, Transition Loss -2.129589080810547, Classifier Loss 0.09302074462175369, Total Loss 40.13492965698242\n",
      "6: Encoding Loss 7.032763481140137, Transition Loss -1.5746771097183228, Classifier Loss 0.10233616828918457, Total Loss 52.429569244384766\n",
      "6: Encoding Loss 5.310766696929932, Transition Loss -0.3464168608188629, Classifier Loss 0.13794127106666565, Total Loss 45.658592224121094\n",
      "6: Encoding Loss 3.2433700561523438, Transition Loss -0.47110456228256226, Classifier Loss 0.053331196308135986, Total Loss 24.793149948120117\n",
      "6: Encoding Loss 4.856528282165527, Transition Loss -1.9756747484207153, Classifier Loss 0.1401151567697525, Total Loss 43.149898529052734\n",
      "6: Encoding Loss 3.754558563232422, Transition Loss 0.7338885068893433, Classifier Loss 0.1033475324511528, Total Loss 33.155662536621094\n",
      "6: Encoding Loss 6.673179626464844, Transition Loss -1.8548460006713867, Classifier Loss 0.10650370270013809, Total Loss 50.6887092590332\n",
      "6: Encoding Loss 4.060091972351074, Transition Loss -2.306607246398926, Classifier Loss 0.09091519564390182, Total Loss 33.451148986816406\n",
      "6: Encoding Loss 6.157230377197266, Transition Loss -1.2872647047042847, Classifier Loss 0.1162339597940445, Total Loss 48.56626510620117\n",
      "6: Encoding Loss 4.468997001647949, Transition Loss -2.1164658069610596, Classifier Loss 0.07024438679218292, Total Loss 33.83757781982422\n",
      "6: Encoding Loss 5.315463066101074, Transition Loss -1.459620714187622, Classifier Loss 0.11889412999153137, Total Loss 43.78160858154297\n",
      "6: Encoding Loss 5.502294540405273, Transition Loss -2.104074478149414, Classifier Loss 0.1201121136546135, Total Loss 45.02413558959961\n",
      "6: Encoding Loss 4.035998821258545, Transition Loss 0.7679382562637329, Classifier Loss 0.10526213049888611, Total Loss 35.049381256103516\n",
      "6: Encoding Loss 1.9009227752685547, Transition Loss -0.8015506267547607, Classifier Loss 0.08010727912187576, Total Loss 19.415943145751953\n",
      "6: Encoding Loss 10.529190063476562, Transition Loss -1.9863919019699097, Classifier Loss 0.11666037142276764, Total Loss 74.84038543701172\n",
      "6: Encoding Loss 7.653814792633057, Transition Loss -1.4762341976165771, Classifier Loss 0.09520325064659119, Total Loss 55.442623138427734\n",
      "6: Encoding Loss 5.68737268447876, Transition Loss -0.9186463952064514, Classifier Loss 0.10995237529277802, Total Loss 45.119110107421875\n",
      "6: Encoding Loss 4.790038108825684, Transition Loss -1.238208293914795, Classifier Loss 0.07470717281103134, Total Loss 36.210453033447266\n",
      "6: Encoding Loss 3.696164608001709, Transition Loss -1.1458338499069214, Classifier Loss 0.1404743194580078, Total Loss 36.223960876464844\n",
      "6: Encoding Loss 4.981293678283691, Transition Loss -1.6936073303222656, Classifier Loss 0.16302989423274994, Total Loss 46.19007110595703\n",
      "6: Encoding Loss 3.4470016956329346, Transition Loss 0.18020065128803253, Classifier Loss 0.06530442088842392, Total Loss 27.28453254699707\n",
      "6: Encoding Loss 6.055668830871582, Transition Loss -2.2999062538146973, Classifier Loss 0.06598524004220963, Total Loss 42.93162155151367\n",
      "6: Encoding Loss 5.4691877365112305, Transition Loss -1.1593862771987915, Classifier Loss 0.04079052060842514, Total Loss 36.893714904785156\n",
      "6: Encoding Loss 5.372490882873535, Transition Loss -1.969490885734558, Classifier Loss 0.08708851784467697, Total Loss 40.94300842285156\n",
      "6: Encoding Loss 4.082563877105713, Transition Loss -0.950127124786377, Classifier Loss 0.04472534731030464, Total Loss 28.967538833618164\n",
      "6: Encoding Loss 4.341619968414307, Transition Loss 0.09741100668907166, Classifier Loss 0.15258586406707764, Total Loss 41.34727096557617\n",
      "6: Encoding Loss 4.8571882247924805, Transition Loss -1.14658522605896, Classifier Loss 0.05438162013888359, Total Loss 34.580833435058594\n",
      "6: Encoding Loss 3.6664810180664062, Transition Loss -2.5894742012023926, Classifier Loss 0.10825904458761215, Total Loss 32.823753356933594\n",
      "6: Encoding Loss 5.27992057800293, Transition Loss -1.6249637603759766, Classifier Loss 0.21390685439109802, Total Loss 53.06956481933594\n",
      "6: Encoding Loss 5.138430118560791, Transition Loss -1.3133691549301147, Classifier Loss 0.11197233200073242, Total Loss 42.02729034423828\n",
      "6: Encoding Loss 7.2541656494140625, Transition Loss -1.2992703914642334, Classifier Loss 0.10445616394281387, Total Loss 53.9700927734375\n",
      "6: Encoding Loss 4.944952964782715, Transition Loss -0.4651082754135132, Classifier Loss 0.10352249443531036, Total Loss 40.02178192138672\n",
      "6: Encoding Loss 4.250733375549316, Transition Loss 0.08929532766342163, Classifier Loss 0.1158275306224823, Total Loss 37.12287139892578\n",
      "6: Encoding Loss 7.21997594833374, Transition Loss -1.4408984184265137, Classifier Loss 0.09811855107545853, Total Loss 53.13113784790039\n",
      "6: Encoding Loss 5.4729509353637695, Transition Loss -2.2454373836517334, Classifier Loss 0.15619410574436188, Total Loss 48.45622253417969\n",
      "6: Encoding Loss 7.199865818023682, Transition Loss -2.0950002670288086, Classifier Loss 0.1220693588256836, Total Loss 55.40529251098633\n",
      "6: Encoding Loss 5.373563289642334, Transition Loss -2.152880907058716, Classifier Loss 0.06312856823205948, Total Loss 38.553375244140625\n",
      "6: Encoding Loss 3.780282974243164, Transition Loss -2.2313365936279297, Classifier Loss 0.13784974813461304, Total Loss 36.465782165527344\n",
      "6: Encoding Loss 6.7203779220581055, Transition Loss -1.1131280660629272, Classifier Loss 0.10202322900295258, Total Loss 50.524147033691406\n",
      "6: Encoding Loss 8.630853652954102, Transition Loss -1.4288209676742554, Classifier Loss 0.1538960486650467, Total Loss 67.17415618896484\n",
      "6: Encoding Loss 6.053279876708984, Transition Loss -1.4805729389190674, Classifier Loss 0.055440135300159454, Total Loss 41.863101959228516\n",
      "6: Encoding Loss 4.167853355407715, Transition Loss -0.28588640689849854, Classifier Loss 0.06756431609392166, Total Loss 31.763439178466797\n",
      "6: Encoding Loss 3.5424118041992188, Transition Loss -1.0497817993164062, Classifier Loss 0.09778255224227905, Total Loss 31.032306671142578\n",
      "6: Encoding Loss 5.737979888916016, Transition Loss -1.491248607635498, Classifier Loss 0.07353033125400543, Total Loss 41.78031921386719\n",
      "6: Encoding Loss 4.213095188140869, Transition Loss -1.1269053220748901, Classifier Loss 0.03839373216032982, Total Loss 29.117494583129883\n",
      "6: Encoding Loss 6.439959526062012, Transition Loss -2.071338176727295, Classifier Loss 0.05462246015667915, Total Loss 44.10117721557617\n",
      "6: Encoding Loss 3.5221924781799316, Transition Loss -1.9525527954101562, Classifier Loss 0.09598235785961151, Total Loss 30.73061180114746\n",
      "6: Encoding Loss 8.120767593383789, Transition Loss 0.036949098110198975, Classifier Loss 0.25765374302864075, Total Loss 74.5047607421875\n",
      "6: Encoding Loss 5.346397876739502, Transition Loss -1.127166748046875, Classifier Loss 0.13073855638504028, Total Loss 45.15179443359375\n",
      "6: Encoding Loss 6.1392998695373535, Transition Loss -1.2826368808746338, Classifier Loss 0.1078655868768692, Total Loss 47.621849060058594\n",
      "6: Encoding Loss 5.728302955627441, Transition Loss 0.08782985806465149, Classifier Loss 0.10083957761526108, Total Loss 44.48891067504883\n",
      "6: Encoding Loss 6.53235387802124, Transition Loss -0.5934630632400513, Classifier Loss 0.21622608602046967, Total Loss 60.816497802734375\n",
      "6: Encoding Loss 5.947493076324463, Transition Loss -1.396558403968811, Classifier Loss 0.17913949489593506, Total Loss 53.598350524902344\n",
      "6: Encoding Loss 4.391828536987305, Transition Loss -0.6199237108230591, Classifier Loss 0.09054829925298691, Total Loss 35.405555725097656\n",
      "6: Encoding Loss 4.799081325531006, Transition Loss -2.119861602783203, Classifier Loss 0.13140995800495148, Total Loss 41.93463897705078\n",
      "6: Encoding Loss 5.184515476226807, Transition Loss -0.7166646122932434, Classifier Loss 0.1325390636920929, Total Loss 44.360713958740234\n",
      "6: Encoding Loss 5.817464351654053, Transition Loss -0.32849612832069397, Classifier Loss 0.1179702877998352, Total Loss 46.70168685913086\n",
      "6: Encoding Loss 3.458636522293091, Transition Loss -0.9990988969802856, Classifier Loss 0.06389797478914261, Total Loss 27.141216278076172\n",
      "6: Encoding Loss 4.024842262268066, Transition Loss -0.9294725656509399, Classifier Loss 0.1187475249171257, Total Loss 36.023433685302734\n",
      "6: Encoding Loss 3.759453296661377, Transition Loss -1.8106406927108765, Classifier Loss 0.09226115047931671, Total Loss 31.78211212158203\n",
      "6: Encoding Loss 4.944357872009277, Transition Loss -0.7393933534622192, Classifier Loss 0.05487988144159317, Total Loss 35.153839111328125\n",
      "6: Encoding Loss 5.096386432647705, Transition Loss -1.0880751609802246, Classifier Loss 0.1251780092716217, Total Loss 43.09568786621094\n",
      "6: Encoding Loss 4.0754499435424805, Transition Loss -2.266899347305298, Classifier Loss 0.09483751654624939, Total Loss 33.935546875\n",
      "6: Encoding Loss 4.7899603843688965, Transition Loss -1.3799651861190796, Classifier Loss 0.14052072167396545, Total Loss 42.791282653808594\n",
      "6: Encoding Loss 3.8386948108673096, Transition Loss -0.25496676564216614, Classifier Loss 0.053121183067560196, Total Loss 28.344186782836914\n",
      "6: Encoding Loss 3.3947157859802246, Transition Loss -0.3656536340713501, Classifier Loss 0.06843329221010208, Total Loss 27.211477279663086\n",
      "6: Encoding Loss 4.884217262268066, Transition Loss -0.5814085602760315, Classifier Loss 0.22119851410388947, Total Loss 51.424922943115234\n",
      "6: Encoding Loss 5.833384037017822, Transition Loss -1.65913987159729, Classifier Loss 0.20370358228683472, Total Loss 55.369998931884766\n",
      "6: Encoding Loss 4.131996154785156, Transition Loss -0.642834484577179, Classifier Loss 0.1383471041917801, Total Loss 38.626434326171875\n",
      "6: Encoding Loss 6.150374412536621, Transition Loss -1.713030219078064, Classifier Loss 0.15492865443229675, Total Loss 52.39442825317383\n",
      "6: Encoding Loss 4.054317474365234, Transition Loss -2.0687456130981445, Classifier Loss 0.08828053623437881, Total Loss 33.153133392333984\n",
      "6: Encoding Loss 5.803654670715332, Transition Loss -1.696592092514038, Classifier Loss 0.14194439351558685, Total Loss 49.015689849853516\n",
      "6: Encoding Loss 5.256843090057373, Transition Loss -1.083472490310669, Classifier Loss 0.05365472286939621, Total Loss 36.906097412109375\n",
      "6: Encoding Loss 3.1525957584381104, Transition Loss -1.274830937385559, Classifier Loss 0.11436805874109268, Total Loss 30.351871490478516\n",
      "6: Encoding Loss 3.8359344005584717, Transition Loss -2.2356510162353516, Classifier Loss 0.06910347193479538, Total Loss 29.925060272216797\n",
      "6: Encoding Loss 3.729360580444336, Transition Loss -0.30637073516845703, Classifier Loss 0.16534423828125, Total Loss 38.910465240478516\n",
      "6: Encoding Loss 6.752579689025879, Transition Loss -0.8170177340507507, Classifier Loss 0.06851164996623993, Total Loss 47.36631774902344\n",
      "6: Encoding Loss 5.638300895690918, Transition Loss -1.3909330368041992, Classifier Loss 0.24333855509757996, Total Loss 58.16310501098633\n",
      "6: Encoding Loss 6.456118106842041, Transition Loss -0.27006804943084717, Classifier Loss 0.14880113303661346, Total Loss 53.61671447753906\n",
      "6: Encoding Loss 5.3194146156311035, Transition Loss -2.2767772674560547, Classifier Loss 0.15328779816627502, Total Loss 47.24435806274414\n",
      "6: Encoding Loss 4.8149237632751465, Transition Loss -0.9442943334579468, Classifier Loss 0.06538441777229309, Total Loss 35.427608489990234\n",
      "6: Encoding Loss 2.994359254837036, Transition Loss -2.331392526626587, Classifier Loss 0.07164113223552704, Total Loss 25.129335403442383\n",
      "6: Encoding Loss 2.7775747776031494, Transition Loss -1.2818056344985962, Classifier Loss 0.07801522314548492, Total Loss 24.466459274291992\n",
      "6: Encoding Loss 4.071515083312988, Transition Loss -2.5716590881347656, Classifier Loss 0.07724952697753906, Total Loss 32.15301513671875\n",
      "6: Encoding Loss 6.288430213928223, Transition Loss 0.09680727124214172, Classifier Loss 0.0510723814368248, Total Loss 42.87654495239258\n",
      "6: Encoding Loss 5.358067989349365, Transition Loss -0.3169742524623871, Classifier Loss 0.1177862361073494, Total Loss 43.92690658569336\n",
      "6: Encoding Loss 6.6471357345581055, Transition Loss -3.3222930431365967, Classifier Loss 0.08014212548732758, Total Loss 47.89570236206055\n",
      "6: Encoding Loss 5.492760181427002, Transition Loss -1.905775547027588, Classifier Loss 0.10587412118911743, Total Loss 43.543212890625\n",
      "6: Encoding Loss 4.342950820922852, Transition Loss -0.029096335172653198, Classifier Loss 0.11679361015558243, Total Loss 37.737056732177734\n",
      "6: Encoding Loss 4.483088493347168, Transition Loss -2.3217933177948, Classifier Loss 0.05111564323306084, Total Loss 32.00917053222656\n",
      "6: Encoding Loss 7.4016594886779785, Transition Loss -1.067240834236145, Classifier Loss 0.1268703043460846, Total Loss 57.096561431884766\n",
      "6: Encoding Loss 6.2349090576171875, Transition Loss -2.693024158477783, Classifier Loss 0.18062584102153778, Total Loss 55.47096252441406\n",
      "6: Encoding Loss 5.857300281524658, Transition Loss -1.6936290264129639, Classifier Loss 0.10737989097833633, Total Loss 45.88111114501953\n",
      "6: Encoding Loss 6.210740089416504, Transition Loss -0.6327178478240967, Classifier Loss 0.1661916822195053, Total Loss 53.883358001708984\n",
      "6: Encoding Loss 6.149827003479004, Transition Loss -1.7305705547332764, Classifier Loss 0.03688778355717659, Total Loss 40.58705139160156\n",
      "6: Encoding Loss 5.181360721588135, Transition Loss -1.5884877443313599, Classifier Loss 0.1895107924938202, Total Loss 50.03860855102539\n",
      "6: Encoding Loss 4.064218521118164, Transition Loss -0.6883661150932312, Classifier Loss 0.08770249783992767, Total Loss 33.15528869628906\n",
      "6: Encoding Loss 6.5488386154174805, Transition Loss -2.0300285816192627, Classifier Loss 0.12477655708789825, Total Loss 51.76987838745117\n",
      "6: Encoding Loss 5.782428741455078, Transition Loss -0.7127619981765747, Classifier Loss 0.1141311526298523, Total Loss 46.10740280151367\n",
      "6: Encoding Loss 4.763943195343018, Transition Loss -0.5841275453567505, Classifier Loss 0.08947435021400452, Total Loss 37.53086471557617\n",
      "6: Encoding Loss 4.544899940490723, Transition Loss -1.1592044830322266, Classifier Loss 0.12543021142482758, Total Loss 39.811954498291016\n",
      "6: Encoding Loss 4.8824567794799805, Transition Loss -0.8254117965698242, Classifier Loss 0.14347980916500092, Total Loss 43.642391204833984\n",
      "6: Encoding Loss 5.69748592376709, Transition Loss -0.5127466917037964, Classifier Loss 0.12894019484519958, Total Loss 47.078731536865234\n",
      "6: Encoding Loss 5.009063243865967, Transition Loss -2.9026522636413574, Classifier Loss 0.0654577985405922, Total Loss 36.5989990234375\n",
      "6: Encoding Loss 2.8738389015197754, Transition Loss -1.6344033479690552, Classifier Loss 0.0998825654387474, Total Loss 27.230636596679688\n",
      "6: Encoding Loss 9.418021202087402, Transition Loss -2.1929898262023926, Classifier Loss 0.1637231409549713, Total Loss 72.87957000732422\n",
      "6: Encoding Loss 6.810188293457031, Transition Loss -0.15676072239875793, Classifier Loss 0.04193419590592384, Total Loss 45.05448913574219\n",
      "6: Encoding Loss 5.772225379943848, Transition Loss -0.8358538150787354, Classifier Loss 0.1770058423280716, Total Loss 52.33360290527344\n",
      "6: Encoding Loss 5.110756874084473, Transition Loss -1.034105658531189, Classifier Loss 0.15488874912261963, Total Loss 46.15300369262695\n",
      "6: Encoding Loss 6.050795555114746, Transition Loss -0.8819580078125, Classifier Loss 0.06596823036670685, Total Loss 42.901248931884766\n",
      "6: Encoding Loss 6.7849297523498535, Transition Loss -1.5300745964050293, Classifier Loss 0.26213157176971436, Total Loss 66.92212677001953\n",
      "6: Encoding Loss 5.026001453399658, Transition Loss -0.34436607360839844, Classifier Loss 0.09445451200008392, Total Loss 39.601322174072266\n",
      "6: Encoding Loss 5.311174392700195, Transition Loss 0.42238637804985046, Classifier Loss 0.13342726230621338, Total Loss 45.378726959228516\n",
      "6: Encoding Loss 5.3917036056518555, Transition Loss -0.602922797203064, Classifier Loss 0.1017284020781517, Total Loss 42.522823333740234\n",
      "6: Encoding Loss 4.025633335113525, Transition Loss 0.22301724553108215, Classifier Loss 0.03529975190758705, Total Loss 27.77298355102539\n",
      "6: Encoding Loss 6.579624176025391, Transition Loss -1.2598463296890259, Classifier Loss 0.0918847918510437, Total Loss 48.66572189331055\n",
      "6: Encoding Loss 7.164412021636963, Transition Loss -1.7605454921722412, Classifier Loss 0.20343992114067078, Total Loss 63.32976150512695\n",
      "6: Encoding Loss 4.547031402587891, Transition Loss -1.064571738243103, Classifier Loss 0.08307919651269913, Total Loss 35.589683532714844\n",
      "6: Encoding Loss 3.770576000213623, Transition Loss -1.908016562461853, Classifier Loss 0.07741808146238327, Total Loss 30.364501953125\n",
      "6: Encoding Loss 4.259683132171631, Transition Loss -1.2658253908157349, Classifier Loss 0.08276307582855225, Total Loss 33.833900451660156\n",
      "6: Encoding Loss 5.072256088256836, Transition Loss -0.4184873700141907, Classifier Loss 0.05489017069339752, Total Loss 35.922386169433594\n",
      "6: Encoding Loss 5.644714832305908, Transition Loss -1.0279998779296875, Classifier Loss 0.16726116836071014, Total Loss 50.593994140625\n",
      "6: Encoding Loss 3.286381244659424, Transition Loss -0.369925856590271, Classifier Loss 0.1429043412208557, Total Loss 34.00857162475586\n",
      "6: Encoding Loss 5.7467875480651855, Transition Loss -2.0963592529296875, Classifier Loss 0.09544433653354645, Total Loss 44.024322509765625\n",
      "6: Encoding Loss 3.7402706146240234, Transition Loss -0.9740222692489624, Classifier Loss 0.05089123547077179, Total Loss 27.530357360839844\n",
      "6: Encoding Loss 4.822149753570557, Transition Loss -2.672778606414795, Classifier Loss 0.05024270713329315, Total Loss 33.95610046386719\n",
      "6: Encoding Loss 2.254009246826172, Transition Loss -1.4989724159240723, Classifier Loss 0.04220528155565262, Total Loss 17.743986129760742\n",
      "6: Encoding Loss 6.177121639251709, Transition Loss -1.6710115671157837, Classifier Loss 0.13386370241641998, Total Loss 50.44843673706055\n",
      "6: Encoding Loss 4.304245948791504, Transition Loss -1.7776165008544922, Classifier Loss 0.10603761672973633, Total Loss 36.42852783203125\n",
      "6: Encoding Loss 3.871718645095825, Transition Loss -0.660692572593689, Classifier Loss 0.07847939431667328, Total Loss 31.077985763549805\n",
      "6: Encoding Loss 5.56636905670166, Transition Loss 0.3064740300178528, Classifier Loss 0.10391320288181305, Total Loss 43.91212463378906\n",
      "6: Encoding Loss 6.098154067993164, Transition Loss -1.7891987562179565, Classifier Loss 0.15845945477485657, Total Loss 52.43415069580078\n",
      "6: Encoding Loss 6.639993190765381, Transition Loss -3.5508487224578857, Classifier Loss 0.14069043099880219, Total Loss 53.90758514404297\n",
      "6: Encoding Loss 4.667834758758545, Transition Loss -1.6296712160110474, Classifier Loss 0.09743275493383408, Total Loss 37.7496337890625\n",
      "6: Encoding Loss 5.496922492980957, Transition Loss -2.2562527656555176, Classifier Loss 0.04951820150017738, Total Loss 37.93245315551758\n",
      "6: Encoding Loss 2.78743839263916, Transition Loss -2.6941301822662354, Classifier Loss 0.05530408024787903, Total Loss 22.25396156311035\n",
      "6: Encoding Loss 5.093690395355225, Transition Loss -2.8783938884735107, Classifier Loss 0.1865130066871643, Total Loss 49.2122917175293\n",
      "6: Encoding Loss 4.8956618309021, Transition Loss -0.8151627779006958, Classifier Loss 0.19081084430217743, Total Loss 48.45473098754883\n",
      "6: Encoding Loss 7.359317779541016, Transition Loss -0.46560168266296387, Classifier Loss 0.10475664585828781, Total Loss 54.63138198852539\n",
      "6: Encoding Loss 4.209147930145264, Transition Loss -1.1357181072235107, Classifier Loss 0.06792186945676804, Total Loss 32.0466194152832\n",
      "6: Encoding Loss 3.863579273223877, Transition Loss -1.6941192150115967, Classifier Loss 0.07121390104293823, Total Loss 30.30219078063965\n",
      "6: Encoding Loss 5.441340446472168, Transition Loss -1.5189220905303955, Classifier Loss 0.03930403292179108, Total Loss 36.577842712402344\n",
      "6: Encoding Loss 4.708085060119629, Transition Loss -0.8769243359565735, Classifier Loss 0.0635981336236, Total Loss 34.60797119140625\n",
      "6: Encoding Loss 4.773287773132324, Transition Loss -1.4116203784942627, Classifier Loss 0.0784432664513588, Total Loss 36.483489990234375\n",
      "6: Encoding Loss 4.916482925415039, Transition Loss -1.4235026836395264, Classifier Loss 0.06012365221977234, Total Loss 35.51069641113281\n",
      "6: Encoding Loss 5.249545097351074, Transition Loss -1.36968994140625, Classifier Loss 0.05430922284722328, Total Loss 36.92764663696289\n",
      "6: Encoding Loss 4.666199207305908, Transition Loss -1.2454707622528076, Classifier Loss 0.12902487814426422, Total Loss 40.89918518066406\n",
      "6: Encoding Loss 4.246966361999512, Transition Loss -0.8986477851867676, Classifier Loss 0.08674736320972443, Total Loss 34.15617752075195\n",
      "6: Encoding Loss 5.715149879455566, Transition Loss 0.3438549339771271, Classifier Loss 0.12205374985933304, Total Loss 46.633819580078125\n",
      "6: Encoding Loss 4.5044965744018555, Transition Loss -1.833252191543579, Classifier Loss 0.07600273936986923, Total Loss 34.626522064208984\n",
      "6: Encoding Loss 4.706035137176514, Transition Loss -1.9770022630691528, Classifier Loss 0.07399031519889832, Total Loss 35.63445281982422\n",
      "6: Encoding Loss 4.512731552124023, Transition Loss -0.7404314279556274, Classifier Loss 0.15306268632411957, Total Loss 42.382362365722656\n",
      "6: Encoding Loss 3.9244487285614014, Transition Loss -0.22764405608177185, Classifier Loss 0.04782917723059654, Total Loss 28.329519271850586\n",
      "6: Encoding Loss 5.0606207847595215, Transition Loss -2.092625856399536, Classifier Loss 0.08116835355758667, Total Loss 38.4797248840332\n",
      "6: Encoding Loss 4.189872741699219, Transition Loss -0.9334313869476318, Classifier Loss 0.0463486947119236, Total Loss 29.77373504638672\n",
      "6: Encoding Loss 6.280459403991699, Transition Loss 0.03117266297340393, Classifier Loss 0.2064388245344162, Total Loss 58.339111328125\n",
      "6: Encoding Loss 5.879389762878418, Transition Loss -1.7312885522842407, Classifier Loss 0.09260015189647675, Total Loss 44.53565979003906\n",
      "6: Encoding Loss 6.427082061767578, Transition Loss -0.5618124008178711, Classifier Loss 0.16222333908081055, Total Loss 54.784603118896484\n",
      "6: Encoding Loss 4.4812912940979, Transition Loss -1.1366949081420898, Classifier Loss 0.03361830487847328, Total Loss 30.24912452697754\n",
      "6: Encoding Loss 4.937602519989014, Transition Loss -1.0483391284942627, Classifier Loss 0.14890332520008087, Total Loss 44.51552963256836\n",
      "6: Encoding Loss 3.03579044342041, Transition Loss -1.4878567457199097, Classifier Loss 0.11592703312635422, Total Loss 29.80685043334961\n",
      "6: Encoding Loss 5.786550045013428, Transition Loss -0.4030245840549469, Classifier Loss 0.13262592256069183, Total Loss 47.98173522949219\n",
      "6: Encoding Loss 5.693720817565918, Transition Loss -1.3742759227752686, Classifier Loss 0.10186261683702469, Total Loss 44.34803771972656\n",
      "6: Encoding Loss 4.0667219161987305, Transition Loss -0.5327498316764832, Classifier Loss 0.12556007504463196, Total Loss 36.95612716674805\n",
      "6: Encoding Loss 4.9964494705200195, Transition Loss -1.333580732345581, Classifier Loss 0.08804938942193985, Total Loss 38.783103942871094\n",
      "6: Encoding Loss 6.198454856872559, Transition Loss -0.546509325504303, Classifier Loss 0.07899389415979385, Total Loss 45.089900970458984\n",
      "6: Encoding Loss 3.9108142852783203, Transition Loss 0.08688530325889587, Classifier Loss 0.07392106205224991, Total Loss 30.89174461364746\n",
      "6: Encoding Loss 6.501225471496582, Transition Loss -0.08821460604667664, Classifier Loss 0.15674462914466858, Total Loss 54.68178176879883\n",
      "6: Encoding Loss 5.663972854614258, Transition Loss -1.1488763093948364, Classifier Loss 0.16412225365638733, Total Loss 50.395606994628906\n",
      "6: Encoding Loss 4.5354766845703125, Transition Loss -1.4009945392608643, Classifier Loss 0.08606968820095062, Total Loss 35.819271087646484\n",
      "6: Encoding Loss 4.052252769470215, Transition Loss -1.5043855905532837, Classifier Loss 0.16693705320358276, Total Loss 41.006622314453125\n",
      "6: Encoding Loss 6.3221659660339355, Transition Loss -0.12670403718948364, Classifier Loss 0.14312395453453064, Total Loss 52.24534225463867\n",
      "6: Encoding Loss 5.304357528686523, Transition Loss -1.5169167518615723, Classifier Loss 0.14379851520061493, Total Loss 46.20539093017578\n",
      "6: Encoding Loss 5.728052616119385, Transition Loss -1.2143381834030151, Classifier Loss 0.15840938687324524, Total Loss 50.208770751953125\n",
      "6: Encoding Loss 5.107673168182373, Transition Loss -1.2209177017211914, Classifier Loss 0.08656390756368637, Total Loss 39.30194091796875\n",
      "6: Encoding Loss 5.879816055297852, Transition Loss -1.566491961479187, Classifier Loss 0.12114998698234558, Total Loss 47.39326858520508\n",
      "6: Encoding Loss 5.530900001525879, Transition Loss -0.8985804319381714, Classifier Loss 0.10310715436935425, Total Loss 43.495758056640625\n",
      "6: Encoding Loss 4.666727542877197, Transition Loss -1.2186894416809082, Classifier Loss 0.10055838525295258, Total Loss 38.05571746826172\n",
      "6: Encoding Loss 4.848443031311035, Transition Loss -0.10374267399311066, Classifier Loss 0.05288512632250786, Total Loss 34.379127502441406\n",
      "6: Encoding Loss 3.9701457023620605, Transition Loss -0.29662278294563293, Classifier Loss 0.06372091174125671, Total Loss 30.192848205566406\n",
      "6: Encoding Loss 7.023648262023926, Transition Loss -1.5149445533752441, Classifier Loss 0.17401307821273804, Total Loss 59.5425910949707\n",
      "6: Encoding Loss 6.367213249206543, Transition Loss -1.810977578163147, Classifier Loss 0.12664619088172913, Total Loss 50.8671760559082\n",
      "6: Encoding Loss 3.600078582763672, Transition Loss -0.942363440990448, Classifier Loss 0.09641247987747192, Total Loss 31.24134063720703\n",
      "6: Encoding Loss 7.063436985015869, Transition Loss -1.6055527925491333, Classifier Loss 0.09589725732803345, Total Loss 51.96970748901367\n",
      "6: Encoding Loss 4.945061683654785, Transition Loss -0.9346974492073059, Classifier Loss 0.08372879773378372, Total Loss 38.042877197265625\n",
      "6: Encoding Loss 4.545027732849121, Transition Loss -0.977033793926239, Classifier Loss 0.052288349717855453, Total Loss 32.49861526489258\n",
      "6: Encoding Loss 6.424266815185547, Transition Loss -1.9408396482467651, Classifier Loss 0.11390913277864456, Total Loss 49.93573760986328\n",
      "6: Encoding Loss 5.9696149826049805, Transition Loss -1.0539371967315674, Classifier Loss 0.19839246571063995, Total Loss 55.65651321411133\n",
      "6: Encoding Loss 5.676024436950684, Transition Loss -1.2786788940429688, Classifier Loss 0.09640296548604965, Total Loss 43.6959342956543\n",
      "6: Encoding Loss 4.601962089538574, Transition Loss -0.11334656178951263, Classifier Loss 0.08880104869604111, Total Loss 36.4918327331543\n",
      "6: Encoding Loss 5.412961006164551, Transition Loss -2.2748875617980957, Classifier Loss 0.09258235991001129, Total Loss 41.73509216308594\n",
      "6: Encoding Loss 5.092404365539551, Transition Loss -1.8589508533477783, Classifier Loss 0.09314502775669098, Total Loss 39.86818313598633\n",
      "6: Encoding Loss 5.926434516906738, Transition Loss 0.2144758254289627, Classifier Loss 0.12278292328119278, Total Loss 47.922691345214844\n",
      "6: Encoding Loss 7.637835502624512, Transition Loss -1.356091856956482, Classifier Loss 0.14078158140182495, Total Loss 59.904632568359375\n",
      "6: Encoding Loss 5.054594993591309, Transition Loss -2.206702947616577, Classifier Loss 0.06309258192777634, Total Loss 36.635948181152344\n",
      "6: Encoding Loss 5.907564640045166, Transition Loss -1.0501506328582764, Classifier Loss 0.05626337602734566, Total Loss 41.07130813598633\n",
      "6: Encoding Loss 5.250002861022949, Transition Loss -0.7860056161880493, Classifier Loss 0.15834221243858337, Total Loss 47.333927154541016\n",
      "6: Encoding Loss 4.819211006164551, Transition Loss -1.125133752822876, Classifier Loss 0.04259008914232254, Total Loss 33.173824310302734\n",
      "7: Encoding Loss 3.7411179542541504, Transition Loss -1.232290506362915, Classifier Loss 0.08326403051614761, Total Loss 30.772619247436523\n",
      "7: Encoding Loss 6.503355503082275, Transition Loss -0.2238873541355133, Classifier Loss 0.11588738858699799, Total Loss 50.60878372192383\n",
      "7: Encoding Loss 5.384078025817871, Transition Loss -1.6626932621002197, Classifier Loss 0.09820938855409622, Total Loss 42.1247444152832\n",
      "7: Encoding Loss 6.199102878570557, Transition Loss -1.1393136978149414, Classifier Loss 0.07778294384479523, Total Loss 44.97245788574219\n",
      "7: Encoding Loss 7.372236728668213, Transition Loss 0.3290342092514038, Classifier Loss 0.2022596150636673, Total Loss 64.59099578857422\n",
      "7: Encoding Loss 6.600213050842285, Transition Loss -2.802351474761963, Classifier Loss 0.06586327403783798, Total Loss 46.186485290527344\n",
      "7: Encoding Loss 4.272429466247559, Transition Loss -2.0310449600219727, Classifier Loss 0.14804087579250336, Total Loss 40.4378547668457\n",
      "7: Encoding Loss 4.239285469055176, Transition Loss -1.6530756950378418, Classifier Loss 0.12089598178863525, Total Loss 37.52465057373047\n",
      "7: Encoding Loss 5.235464096069336, Transition Loss -1.1392967700958252, Classifier Loss 0.16139920055866241, Total Loss 47.552249908447266\n",
      "7: Encoding Loss 3.378258466720581, Transition Loss -0.6995373964309692, Classifier Loss 0.08756408095359802, Total Loss 29.025678634643555\n",
      "7: Encoding Loss 5.938243389129639, Transition Loss -1.3919620513916016, Classifier Loss 0.15881429612636566, Total Loss 51.510337829589844\n",
      "7: Encoding Loss 3.856025218963623, Transition Loss -1.2229783535003662, Classifier Loss 0.08621785044670105, Total Loss 31.757450103759766\n",
      "7: Encoding Loss 2.5465376377105713, Transition Loss -0.4396034777164459, Classifier Loss 0.0539390929043293, Total Loss 20.67296028137207\n",
      "7: Encoding Loss 4.613152503967285, Transition Loss -2.168738603591919, Classifier Loss 0.08115047216415405, Total Loss 35.793094635009766\n",
      "7: Encoding Loss 3.5096144676208496, Transition Loss -2.105682611465454, Classifier Loss 0.08787699788808823, Total Loss 29.84454345703125\n",
      "7: Encoding Loss 4.537632942199707, Transition Loss -1.3549938201904297, Classifier Loss 0.08730492740869522, Total Loss 35.95574951171875\n",
      "7: Encoding Loss 3.899178981781006, Transition Loss -1.8591570854187012, Classifier Loss 0.07228071242570877, Total Loss 30.62240219116211\n",
      "7: Encoding Loss 2.8425395488739014, Transition Loss 0.45193716883659363, Classifier Loss 0.08044248074293137, Total Loss 25.280261993408203\n",
      "7: Encoding Loss 7.237296104431152, Transition Loss -2.5309038162231445, Classifier Loss 0.10758266597986221, Total Loss 54.181034088134766\n",
      "7: Encoding Loss 4.45444917678833, Transition Loss -0.708926260471344, Classifier Loss 0.205438494682312, Total Loss 47.270263671875\n",
      "7: Encoding Loss 6.635522365570068, Transition Loss -2.4396562576293945, Classifier Loss 0.23591601848602295, Total Loss 63.40376281738281\n",
      "7: Encoding Loss 5.640820503234863, Transition Loss -1.281752109527588, Classifier Loss 0.12038858234882355, Total Loss 45.883270263671875\n",
      "7: Encoding Loss 6.4057817459106445, Transition Loss -0.5578909516334534, Classifier Loss 0.10604310780763626, Total Loss 49.038780212402344\n",
      "7: Encoding Loss 6.332021236419678, Transition Loss -0.16645067930221558, Classifier Loss 0.10444319248199463, Total Loss 48.43638229370117\n",
      "7: Encoding Loss 5.525916576385498, Transition Loss 0.33351168036460876, Classifier Loss 0.06642273813486099, Total Loss 39.93117904663086\n",
      "7: Encoding Loss 5.037816524505615, Transition Loss -1.9640991687774658, Classifier Loss 0.09130546450614929, Total Loss 39.356658935546875\n",
      "7: Encoding Loss 3.5376968383789062, Transition Loss -0.41573402285575867, Classifier Loss 0.07854890823364258, Total Loss 29.08090591430664\n",
      "7: Encoding Loss 7.7374982833862305, Transition Loss -1.6154228448867798, Classifier Loss 0.15696029365062714, Total Loss 62.12037658691406\n",
      "7: Encoding Loss 3.955676317214966, Transition Loss -2.392735481262207, Classifier Loss 0.07206662744283676, Total Loss 30.93976402282715\n",
      "7: Encoding Loss 3.6566245555877686, Transition Loss -1.4264227151870728, Classifier Loss 0.16207340359687805, Total Loss 38.146514892578125\n",
      "7: Encoding Loss 4.965339183807373, Transition Loss -1.8097736835479736, Classifier Loss 0.09304992109537125, Total Loss 39.0963020324707\n",
      "7: Encoding Loss 4.573380470275879, Transition Loss 0.6044527292251587, Classifier Loss 0.06356874108314514, Total Loss 34.038936614990234\n",
      "7: Encoding Loss 6.035738468170166, Transition Loss -1.1657540798187256, Classifier Loss 0.16330569982528687, Total Loss 52.54453659057617\n",
      "7: Encoding Loss 4.139019966125488, Transition Loss -1.5423598289489746, Classifier Loss 0.07915674149990082, Total Loss 32.749176025390625\n",
      "7: Encoding Loss 4.143311023712158, Transition Loss -1.3177192211151123, Classifier Loss 0.13229060173034668, Total Loss 38.088401794433594\n",
      "7: Encoding Loss 5.220785140991211, Transition Loss -1.4480007886886597, Classifier Loss 0.24640904366970062, Total Loss 55.96503448486328\n",
      "7: Encoding Loss 4.783308029174805, Transition Loss -1.368645191192627, Classifier Loss 0.09775815904140472, Total Loss 38.47511672973633\n",
      "7: Encoding Loss 4.746748447418213, Transition Loss -0.33032476902008057, Classifier Loss 0.07438920438289642, Total Loss 35.91927719116211\n",
      "7: Encoding Loss 8.359426498413086, Transition Loss 0.05436426401138306, Classifier Loss 0.20454749464988708, Total Loss 70.633056640625\n",
      "7: Encoding Loss 7.390074729919434, Transition Loss -1.6853023767471313, Classifier Loss 0.11817754805088043, Total Loss 56.157527923583984\n",
      "7: Encoding Loss 5.363986968994141, Transition Loss -1.1587762832641602, Classifier Loss 0.13339778780937195, Total Loss 45.52323532104492\n",
      "7: Encoding Loss 6.095047473907471, Transition Loss -1.2020903825759888, Classifier Loss 0.12563255429267883, Total Loss 49.133060455322266\n",
      "7: Encoding Loss 5.769785404205322, Transition Loss -1.190927505493164, Classifier Loss 0.13857689499855042, Total Loss 48.47592544555664\n",
      "7: Encoding Loss 8.640735626220703, Transition Loss -2.399285078048706, Classifier Loss 0.08367327600717545, Total Loss 60.210784912109375\n",
      "7: Encoding Loss 5.306951522827148, Transition Loss -1.8210190534591675, Classifier Loss 0.11610656976699829, Total Loss 43.45164108276367\n",
      "7: Encoding Loss 3.42744779586792, Transition Loss 0.09000444412231445, Classifier Loss 0.06070750579237938, Total Loss 26.67144012451172\n",
      "7: Encoding Loss 4.250172138214111, Transition Loss -1.0776740312576294, Classifier Loss 0.10685371607542038, Total Loss 36.18597412109375\n",
      "7: Encoding Loss 7.117579460144043, Transition Loss 0.02680644392967224, Classifier Loss 0.09695614129304886, Total Loss 52.41181564331055\n",
      "7: Encoding Loss 7.198964595794678, Transition Loss -0.3514944016933441, Classifier Loss 0.22325663268566132, Total Loss 65.51931762695312\n",
      "7: Encoding Loss 4.084827899932861, Transition Loss -0.3152371644973755, Classifier Loss 0.07136176526546478, Total Loss 31.64501953125\n",
      "7: Encoding Loss 4.693564414978027, Transition Loss -1.5956907272338867, Classifier Loss 0.14872650802135468, Total Loss 43.03340148925781\n",
      "7: Encoding Loss 3.482405185699463, Transition Loss -1.846414566040039, Classifier Loss 0.0425075963139534, Total Loss 25.144453048706055\n",
      "7: Encoding Loss 7.88363790512085, Transition Loss -1.049337387084961, Classifier Loss 0.09101691842079163, Total Loss 56.40310287475586\n",
      "7: Encoding Loss 5.305744647979736, Transition Loss 0.06169062852859497, Classifier Loss 0.1637643575668335, Total Loss 48.2355842590332\n",
      "7: Encoding Loss 2.979248523712158, Transition Loss -1.8403631448745728, Classifier Loss 0.12236730754375458, Total Loss 30.111486434936523\n",
      "7: Encoding Loss 7.872100830078125, Transition Loss -0.24873985350131989, Classifier Loss 0.06642643362283707, Total Loss 53.87514877319336\n",
      "7: Encoding Loss 7.004339694976807, Transition Loss -0.546192467212677, Classifier Loss 0.16375136375427246, Total Loss 58.40095901489258\n",
      "7: Encoding Loss 4.019157886505127, Transition Loss -1.136775255203247, Classifier Loss 0.06895653158426285, Total Loss 31.010147094726562\n",
      "7: Encoding Loss 10.455078125, Transition Loss -2.1429715156555176, Classifier Loss 0.21178822219371796, Total Loss 83.90843963623047\n",
      "7: Encoding Loss 7.168813228607178, Transition Loss -1.6703147888183594, Classifier Loss 0.048204455524683, Total Loss 47.83266067504883\n",
      "7: Encoding Loss 6.1968183517456055, Transition Loss -0.8607546091079712, Classifier Loss 0.0962793305516243, Total Loss 46.808502197265625\n",
      "7: Encoding Loss 4.390923500061035, Transition Loss -2.3770346641540527, Classifier Loss 0.08646076917648315, Total Loss 34.99066925048828\n",
      "7: Encoding Loss 3.8762054443359375, Transition Loss -2.360692024230957, Classifier Loss 0.13192111253738403, Total Loss 36.44839859008789\n",
      "7: Encoding Loss 5.152745723724365, Transition Loss -4.177332878112793, Classifier Loss 0.10515638440847397, Total Loss 41.430442810058594\n",
      "7: Encoding Loss 4.793423652648926, Transition Loss -0.8634635210037231, Classifier Loss 0.12585581839084625, Total Loss 41.34577560424805\n",
      "7: Encoding Loss 5.905879974365234, Transition Loss -1.9669591188430786, Classifier Loss 0.21747387945652008, Total Loss 57.181884765625\n",
      "7: Encoding Loss 3.5505614280700684, Transition Loss -1.2550127506256104, Classifier Loss 0.2208390235900879, Total Loss 43.38676834106445\n",
      "7: Encoding Loss 5.687765121459961, Transition Loss -1.6561527252197266, Classifier Loss 0.08605668693780899, Total Loss 42.731597900390625\n",
      "7: Encoding Loss 4.331277370452881, Transition Loss -0.05243402719497681, Classifier Loss 0.10167019069194794, Total Loss 36.154666900634766\n",
      "7: Encoding Loss 2.857294797897339, Transition Loss -0.09398286044597626, Classifier Loss 0.10390470921993256, Total Loss 27.534202575683594\n",
      "7: Encoding Loss 4.82637882232666, Transition Loss -2.151919364929199, Classifier Loss 0.14777125418186188, Total Loss 43.734535217285156\n",
      "7: Encoding Loss 2.8535873889923096, Transition Loss -1.4837895631790161, Classifier Loss 0.1125737652182579, Total Loss 28.37830924987793\n",
      "7: Encoding Loss 2.3298466205596924, Transition Loss -2.1797947883605957, Classifier Loss 0.10068462044000626, Total Loss 24.04667091369629\n",
      "7: Encoding Loss 7.34650182723999, Transition Loss -0.39162757992744446, Classifier Loss 0.05077866092324257, Total Loss 49.15672302246094\n",
      "7: Encoding Loss 4.6177449226379395, Transition Loss -1.572792887687683, Classifier Loss 0.06304152309894562, Total Loss 34.00999450683594\n",
      "7: Encoding Loss 5.429233551025391, Transition Loss -1.8054249286651611, Classifier Loss 0.15403446555137634, Total Loss 47.978126525878906\n",
      "7: Encoding Loss 4.118285179138184, Transition Loss -1.912785291671753, Classifier Loss 0.04812806472182274, Total Loss 29.52175521850586\n",
      "7: Encoding Loss 6.866079807281494, Transition Loss -1.5822014808654785, Classifier Loss 0.09831860661506653, Total Loss 51.027706146240234\n",
      "7: Encoding Loss 4.778262138366699, Transition Loss -1.4954535961151123, Classifier Loss 0.1529935598373413, Total Loss 43.96833419799805\n",
      "7: Encoding Loss 2.9447574615478516, Transition Loss -1.3610384464263916, Classifier Loss 0.07984928786754608, Total Loss 25.652929306030273\n",
      "7: Encoding Loss 7.264618873596191, Transition Loss -0.966776967048645, Classifier Loss 0.07415392994880676, Total Loss 51.002723693847656\n",
      "7: Encoding Loss 5.59417724609375, Transition Loss -1.0791071653366089, Classifier Loss 0.1711997091770172, Total Loss 50.68460464477539\n",
      "7: Encoding Loss 3.758230686187744, Transition Loss -0.24842825531959534, Classifier Loss 0.1638745814561844, Total Loss 38.936744689941406\n",
      "7: Encoding Loss 4.802853584289551, Transition Loss -1.1224040985107422, Classifier Loss 0.12760844826698303, Total Loss 41.5775146484375\n",
      "7: Encoding Loss 5.555327892303467, Transition Loss -1.2423814535140991, Classifier Loss 0.14110592007637024, Total Loss 47.44206619262695\n",
      "7: Encoding Loss 5.003426551818848, Transition Loss -2.479536771774292, Classifier Loss 0.1197693943977356, Total Loss 41.99650573730469\n",
      "7: Encoding Loss 4.4829206466674805, Transition Loss 0.9065166711807251, Classifier Loss 0.07024979591369629, Total Loss 34.28511047363281\n",
      "7: Encoding Loss 5.212122917175293, Transition Loss -1.6071829795837402, Classifier Loss 0.14566268026828766, Total Loss 45.83836364746094\n",
      "7: Encoding Loss 5.277287483215332, Transition Loss -2.467378616333008, Classifier Loss 0.04240427166223526, Total Loss 35.90316390991211\n",
      "7: Encoding Loss 4.5829057693481445, Transition Loss -1.7686102390289307, Classifier Loss 0.13837695121765137, Total Loss 41.33442306518555\n",
      "7: Encoding Loss 3.8286819458007812, Transition Loss 0.849807858467102, Classifier Loss 0.1143481582403183, Total Loss 34.746829986572266\n",
      "7: Encoding Loss 2.919226884841919, Transition Loss -1.333402395248413, Classifier Loss 0.07122506201267242, Total Loss 24.637332916259766\n",
      "7: Encoding Loss 5.76064920425415, Transition Loss -1.8352057933807373, Classifier Loss 0.08293060958385468, Total Loss 42.856224060058594\n",
      "7: Encoding Loss 4.744290351867676, Transition Loss 1.0098865032196045, Classifier Loss 0.07776270061731339, Total Loss 36.645965576171875\n",
      "7: Encoding Loss 3.7861361503601074, Transition Loss -1.7989985942840576, Classifier Loss 0.060892440378665924, Total Loss 28.805341720581055\n",
      "7: Encoding Loss 2.8949790000915527, Transition Loss -1.330028772354126, Classifier Loss 0.1195634976029396, Total Loss 29.325693130493164\n",
      "7: Encoding Loss 3.537471055984497, Transition Loss 0.23219114542007446, Classifier Loss 0.07185396552085876, Total Loss 28.50309944152832\n",
      "7: Encoding Loss 10.688491821289062, Transition Loss 0.1841704249382019, Classifier Loss 0.24840489029884338, Total Loss 89.04510498046875\n",
      "7: Encoding Loss 7.271292209625244, Transition Loss -1.2290778160095215, Classifier Loss 0.10683383047580719, Total Loss 54.310646057128906\n",
      "7: Encoding Loss 8.455202102661133, Transition Loss -1.0552480220794678, Classifier Loss 0.1679566651582718, Total Loss 67.52645874023438\n",
      "7: Encoding Loss 6.412896633148193, Transition Loss -1.7031899690628052, Classifier Loss 0.06049662083387375, Total Loss 44.526363372802734\n",
      "7: Encoding Loss 5.359667778015137, Transition Loss -1.8982856273651123, Classifier Loss 0.06350953131914139, Total Loss 38.508201599121094\n",
      "7: Encoding Loss 4.708895683288574, Transition Loss -0.47503912448883057, Classifier Loss 0.1449585109949112, Total Loss 42.74903869628906\n",
      "7: Encoding Loss 4.786294460296631, Transition Loss -2.143247604370117, Classifier Loss 0.11534185707569122, Total Loss 40.251094818115234\n",
      "7: Encoding Loss 3.0038135051727295, Transition Loss -0.20275092124938965, Classifier Loss 0.14422304928302765, Total Loss 32.445106506347656\n",
      "7: Encoding Loss 2.9745380878448486, Transition Loss 0.7429255247116089, Classifier Loss 0.05652705952525139, Total Loss 23.79710578918457\n",
      "7: Encoding Loss 6.4989118576049805, Transition Loss 0.0883910059928894, Classifier Loss 0.09983660280704498, Total Loss 49.012489318847656\n",
      "7: Encoding Loss 4.134062767028809, Transition Loss -2.289234161376953, Classifier Loss 0.04530535265803337, Total Loss 29.33399772644043\n",
      "7: Encoding Loss 9.234247207641602, Transition Loss -2.130136013031006, Classifier Loss 0.13603375852108002, Total Loss 69.00800323486328\n",
      "7: Encoding Loss 6.75902795791626, Transition Loss -1.1148171424865723, Classifier Loss 0.17480221390724182, Total Loss 58.03394317626953\n",
      "7: Encoding Loss 4.257277011871338, Transition Loss -1.6748231649398804, Classifier Loss 0.07659438997507095, Total Loss 33.202430725097656\n",
      "7: Encoding Loss 6.3814191818237305, Transition Loss -0.40243130922317505, Classifier Loss 0.10289615392684937, Total Loss 48.577972412109375\n",
      "7: Encoding Loss 6.44317626953125, Transition Loss -0.9949191212654114, Classifier Loss 0.07128637284040451, Total Loss 45.78730010986328\n",
      "7: Encoding Loss 5.83613395690918, Transition Loss -1.1018694639205933, Classifier Loss 0.04398571327328682, Total Loss 39.41493225097656\n",
      "7: Encoding Loss 7.053840637207031, Transition Loss -1.3256754875183105, Classifier Loss 0.07740646600723267, Total Loss 50.06315994262695\n",
      "7: Encoding Loss 4.350008964538574, Transition Loss -2.6013731956481934, Classifier Loss 0.09173450618982315, Total Loss 35.272464752197266\n",
      "7: Encoding Loss 4.019681930541992, Transition Loss -0.07217149436473846, Classifier Loss 0.09502163529396057, Total Loss 33.6202278137207\n",
      "7: Encoding Loss 3.0470175743103027, Transition Loss -0.9474865794181824, Classifier Loss 0.06514435261487961, Total Loss 24.796161651611328\n",
      "7: Encoding Loss 4.110930919647217, Transition Loss -1.2270838022232056, Classifier Loss 0.09831283986568451, Total Loss 34.49637985229492\n",
      "7: Encoding Loss 5.987923622131348, Transition Loss -0.1659313142299652, Classifier Loss 0.11264652013778687, Total Loss 47.19213104248047\n",
      "7: Encoding Loss 6.218173503875732, Transition Loss -0.9077473878860474, Classifier Loss 0.20184296369552612, Total Loss 57.492977142333984\n",
      "7: Encoding Loss 4.878580570220947, Transition Loss -1.828727126121521, Classifier Loss 0.1464809775352478, Total Loss 43.91884994506836\n",
      "7: Encoding Loss 6.875677585601807, Transition Loss -1.501037359237671, Classifier Loss 0.09887067973613739, Total Loss 51.14053726196289\n",
      "7: Encoding Loss 4.664145469665527, Transition Loss -1.7869361639022827, Classifier Loss 0.07150088995695114, Total Loss 35.13425064086914\n",
      "7: Encoding Loss 2.6517014503479004, Transition Loss -1.3461946249008179, Classifier Loss 0.07494601607322693, Total Loss 23.404272079467773\n",
      "7: Encoding Loss 6.141838073730469, Transition Loss -2.2803335189819336, Classifier Loss 0.0696200430393219, Total Loss 43.8121223449707\n",
      "7: Encoding Loss 6.95400857925415, Transition Loss -2.3591582775115967, Classifier Loss 0.2056637853384018, Total Loss 62.289485931396484\n",
      "7: Encoding Loss 5.207329750061035, Transition Loss -2.1903274059295654, Classifier Loss 0.04752739518880844, Total Loss 35.99584197998047\n",
      "7: Encoding Loss 3.088407278060913, Transition Loss -1.1233206987380981, Classifier Loss 0.056659724563360214, Total Loss 24.195966720581055\n",
      "7: Encoding Loss 2.5442955493927, Transition Loss -3.5161991119384766, Classifier Loss 0.06319110095500946, Total Loss 21.583478927612305\n",
      "7: Encoding Loss 1.2185003757476807, Transition Loss -1.0413577556610107, Classifier Loss 0.14526745676994324, Total Loss 21.837331771850586\n",
      "7: Encoding Loss 5.867068767547607, Transition Loss -1.2429823875427246, Classifier Loss 0.09286186844110489, Total Loss 44.48810577392578\n",
      "7: Encoding Loss 5.324191570281982, Transition Loss -1.6267476081848145, Classifier Loss 0.10788234323263168, Total Loss 42.732730865478516\n",
      "7: Encoding Loss 3.918534755706787, Transition Loss -1.4760987758636475, Classifier Loss 0.06523741781711578, Total Loss 30.034358978271484\n",
      "7: Encoding Loss 6.268089771270752, Transition Loss -1.2437694072723389, Classifier Loss 0.12238898128271103, Total Loss 49.84694290161133\n",
      "7: Encoding Loss 6.038106441497803, Transition Loss -1.1125292778015137, Classifier Loss 0.1948099136352539, Total Loss 55.70918655395508\n",
      "7: Encoding Loss 4.720816612243652, Transition Loss 0.1020650565624237, Classifier Loss 0.07606007158756256, Total Loss 35.97173309326172\n",
      "7: Encoding Loss 5.393064975738525, Transition Loss 0.18210048973560333, Classifier Loss 0.10449983924627304, Total Loss 42.8812141418457\n",
      "7: Encoding Loss 3.574650287628174, Transition Loss -1.7897268533706665, Classifier Loss 0.07040143758058548, Total Loss 28.48733139038086\n",
      "7: Encoding Loss 8.07465934753418, Transition Loss 0.08469396829605103, Classifier Loss 0.062448497861623764, Total Loss 54.726688385009766\n",
      "7: Encoding Loss 6.117791175842285, Transition Loss -1.4805617332458496, Classifier Loss 0.0618915930390358, Total Loss 42.89531707763672\n",
      "7: Encoding Loss 6.228196144104004, Transition Loss -0.517188310623169, Classifier Loss 0.10868080705404282, Total Loss 48.23705291748047\n",
      "7: Encoding Loss 5.29672908782959, Transition Loss -0.9717666506767273, Classifier Loss 0.05348757654428482, Total Loss 37.128746032714844\n",
      "7: Encoding Loss 5.234590530395508, Transition Loss -0.1735905408859253, Classifier Loss 0.08867363631725311, Total Loss 40.27484130859375\n",
      "7: Encoding Loss 3.49919056892395, Transition Loss -1.2145333290100098, Classifier Loss 0.08423221856355667, Total Loss 29.417879104614258\n",
      "7: Encoding Loss 5.196753978729248, Transition Loss -0.9013283252716064, Classifier Loss 0.18185818195343018, Total Loss 49.3659782409668\n",
      "7: Encoding Loss 2.857028007507324, Transition Loss -1.6491384506225586, Classifier Loss 0.0641401931643486, Total Loss 23.555526733398438\n",
      "7: Encoding Loss 6.856581211090088, Transition Loss -1.2121284008026123, Classifier Loss 0.2936459183692932, Total Loss 70.50359344482422\n",
      "7: Encoding Loss 6.513943672180176, Transition Loss -0.7137048840522766, Classifier Loss 0.20376603305339813, Total Loss 59.45998001098633\n",
      "7: Encoding Loss 4.120556831359863, Transition Loss -1.5834592580795288, Classifier Loss 0.14341911673545837, Total Loss 39.06462097167969\n",
      "7: Encoding Loss 5.684915542602539, Transition Loss -2.0249745845794678, Classifier Loss 0.08190090209245682, Total Loss 42.29877471923828\n",
      "7: Encoding Loss 6.792324066162109, Transition Loss -0.3328300416469574, Classifier Loss 0.09169749170541763, Total Loss 49.923561096191406\n",
      "7: Encoding Loss 7.238409996032715, Transition Loss -0.707146167755127, Classifier Loss 0.17413808405399323, Total Loss 60.84398651123047\n",
      "7: Encoding Loss 5.235183238983154, Transition Loss -1.9020243883132935, Classifier Loss 0.07738563418388367, Total Loss 39.14890670776367\n",
      "7: Encoding Loss 4.521273136138916, Transition Loss -3.31355357170105, Classifier Loss 0.09372244775295258, Total Loss 36.49856185913086\n",
      "7: Encoding Loss 6.926922798156738, Transition Loss -2.23740816116333, Classifier Loss 0.1582488864660263, Total Loss 57.38553237915039\n",
      "7: Encoding Loss 5.878248691558838, Transition Loss -0.806414783000946, Classifier Loss 0.07643534243106842, Total Loss 42.91270446777344\n",
      "7: Encoding Loss 3.8376357555389404, Transition Loss -2.344700813293457, Classifier Loss 0.09793727844953537, Total Loss 32.818603515625\n",
      "7: Encoding Loss 2.5705132484436035, Transition Loss -0.9114717245101929, Classifier Loss 0.09638743847608566, Total Loss 25.061458587646484\n",
      "7: Encoding Loss 7.476385593414307, Transition Loss -2.0710997581481934, Classifier Loss 0.11497703194618225, Total Loss 56.35519027709961\n",
      "7: Encoding Loss 4.991550922393799, Transition Loss 0.021299153566360474, Classifier Loss 0.09257213771343231, Total Loss 39.21503829956055\n",
      "7: Encoding Loss 6.045220375061035, Transition Loss -0.7289603352546692, Classifier Loss 0.07787938416004181, Total Loss 44.0589714050293\n",
      "7: Encoding Loss 4.0145697593688965, Transition Loss -1.0271066427230835, Classifier Loss 0.05141168832778931, Total Loss 29.228178024291992\n",
      "7: Encoding Loss 4.10598087310791, Transition Loss -1.5033763647079468, Classifier Loss 0.07731465250253677, Total Loss 32.36674880981445\n",
      "7: Encoding Loss 8.209695816040039, Transition Loss 0.13448208570480347, Classifier Loss 0.16770361363887787, Total Loss 66.08232879638672\n",
      "7: Encoding Loss 5.844289302825928, Transition Loss -1.387295126914978, Classifier Loss 0.18426935374736786, Total Loss 53.492122650146484\n",
      "7: Encoding Loss 5.549478054046631, Transition Loss -3.0796053409576416, Classifier Loss 0.1739964783191681, Total Loss 50.69528579711914\n",
      "7: Encoding Loss 3.710850954055786, Transition Loss -2.0418267250061035, Classifier Loss 0.07109227031469345, Total Loss 29.373516082763672\n",
      "7: Encoding Loss 7.575874328613281, Transition Loss -1.9195048809051514, Classifier Loss 0.15066131949424744, Total Loss 60.52061080932617\n",
      "7: Encoding Loss 5.568116188049316, Transition Loss -0.7374682426452637, Classifier Loss 0.06364791840314865, Total Loss 39.773197174072266\n",
      "7: Encoding Loss 4.696660995483398, Transition Loss -1.4608176946640015, Classifier Loss 0.07581645995378494, Total Loss 35.76102828979492\n",
      "7: Encoding Loss 3.696812391281128, Transition Loss -2.013963460922241, Classifier Loss 0.04661088064312935, Total Loss 26.84115982055664\n",
      "7: Encoding Loss 5.463603496551514, Transition Loss -1.3392127752304077, Classifier Loss 0.18418501317501068, Total Loss 51.19959259033203\n",
      "7: Encoding Loss 6.380889415740967, Transition Loss -3.1145074367523193, Classifier Loss 0.12302597612142563, Total Loss 50.58668899536133\n",
      "7: Encoding Loss 4.332162857055664, Transition Loss -0.8262065649032593, Classifier Loss 0.09902442991733551, Total Loss 35.89509201049805\n",
      "7: Encoding Loss 6.489212989807129, Transition Loss -0.17085018754005432, Classifier Loss 0.1562250852584839, Total Loss 54.55772018432617\n",
      "7: Encoding Loss 6.867318630218506, Transition Loss -1.461808681488037, Classifier Loss 0.14017964899539948, Total Loss 55.22129440307617\n",
      "7: Encoding Loss 5.3795576095581055, Transition Loss -2.6177287101745605, Classifier Loss 0.10280930995941162, Total Loss 42.55723571777344\n",
      "7: Encoding Loss 6.3598480224609375, Transition Loss -1.1577274799346924, Classifier Loss 0.05616997554898262, Total Loss 43.7756233215332\n",
      "7: Encoding Loss 5.3032941818237305, Transition Loss 0.34397417306900024, Classifier Loss 0.0957331657409668, Total Loss 41.530670166015625\n",
      "7: Encoding Loss 5.606315612792969, Transition Loss -0.7185316681861877, Classifier Loss 0.13443602621555328, Total Loss 47.08121109008789\n",
      "7: Encoding Loss 7.031181335449219, Transition Loss -0.8021385073661804, Classifier Loss 0.05852484330534935, Total Loss 48.03925323486328\n",
      "7: Encoding Loss 5.077581405639648, Transition Loss -2.2179906368255615, Classifier Loss 0.05622340738773346, Total Loss 36.08694076538086\n",
      "7: Encoding Loss 6.986135959625244, Transition Loss -1.5618029832839966, Classifier Loss 0.15135908126831055, Total Loss 57.052101135253906\n",
      "7: Encoding Loss 5.231506824493408, Transition Loss -0.976926326751709, Classifier Loss 0.20685668289661407, Total Loss 52.07432174682617\n",
      "7: Encoding Loss 5.492997169494629, Transition Loss -2.6386642456054688, Classifier Loss 0.12449214607477188, Total Loss 45.40614318847656\n",
      "7: Encoding Loss 3.4133827686309814, Transition Loss 0.3861548900604248, Classifier Loss 0.10108161717653275, Total Loss 30.742921829223633\n",
      "7: Encoding Loss 3.149801731109619, Transition Loss -0.6091400980949402, Classifier Loss 0.0745447650551796, Total Loss 26.353042602539062\n",
      "7: Encoding Loss 2.6194238662719727, Transition Loss -1.133628010749817, Classifier Loss 0.07976038008928299, Total Loss 23.692127227783203\n",
      "7: Encoding Loss 7.978137969970703, Transition Loss -2.068678379058838, Classifier Loss 0.16153059899806976, Total Loss 64.02106475830078\n",
      "7: Encoding Loss 5.101865768432617, Transition Loss -1.5082299709320068, Classifier Loss 0.11543667316436768, Total Loss 42.154258728027344\n",
      "7: Encoding Loss 6.395749568939209, Transition Loss -1.6092710494995117, Classifier Loss 0.1033722311258316, Total Loss 48.71107864379883\n",
      "7: Encoding Loss 4.493584632873535, Transition Loss -1.1135004758834839, Classifier Loss 0.1305084228515625, Total Loss 40.011905670166016\n",
      "7: Encoding Loss 4.396551132202148, Transition Loss -1.7706376314163208, Classifier Loss 0.12566596269607544, Total Loss 38.945194244384766\n",
      "7: Encoding Loss 4.202726364135742, Transition Loss -1.750489354133606, Classifier Loss 0.14725655317306519, Total Loss 39.941314697265625\n",
      "7: Encoding Loss 4.406161308288574, Transition Loss -1.4031150341033936, Classifier Loss 0.11138544976711273, Total Loss 37.574954986572266\n",
      "7: Encoding Loss 6.598994255065918, Transition Loss -1.058755874633789, Classifier Loss 0.13075131177902222, Total Loss 52.66867446899414\n",
      "7: Encoding Loss 4.906655311584473, Transition Loss -0.7481518983840942, Classifier Loss 0.24621453881263733, Total Loss 54.06108856201172\n",
      "7: Encoding Loss 7.07330846786499, Transition Loss -1.8131979703903198, Classifier Loss 0.10560393333435059, Total Loss 52.9995231628418\n",
      "7: Encoding Loss 4.636303901672363, Transition Loss -1.943316102027893, Classifier Loss 0.0830317959189415, Total Loss 36.12022399902344\n",
      "7: Encoding Loss 6.274282932281494, Transition Loss -1.5511213541030884, Classifier Loss 0.08949242532253265, Total Loss 46.59431838989258\n",
      "7: Encoding Loss 4.536317825317383, Transition Loss -0.9655609130859375, Classifier Loss 0.0888151004910469, Total Loss 36.09903335571289\n",
      "7: Encoding Loss 4.445029258728027, Transition Loss -0.9803782105445862, Classifier Loss 0.06450186669826508, Total Loss 33.119972229003906\n",
      "7: Encoding Loss 3.1954970359802246, Transition Loss -0.4811314344406128, Classifier Loss 0.08234784752130508, Total Loss 27.407575607299805\n",
      "7: Encoding Loss 5.513318061828613, Transition Loss -1.0224179029464722, Classifier Loss 0.07637261599302292, Total Loss 40.71676254272461\n",
      "7: Encoding Loss 5.853180885314941, Transition Loss -0.8166275024414062, Classifier Loss 0.170780748128891, Total Loss 52.196834564208984\n",
      "7: Encoding Loss 3.5756919384002686, Transition Loss -1.5238789319992065, Classifier Loss 0.1766229122877121, Total Loss 39.1158332824707\n",
      "7: Encoding Loss 3.9656245708465576, Transition Loss -0.09085388481616974, Classifier Loss 0.11763343214988708, Total Loss 35.55705261230469\n",
      "7: Encoding Loss 6.924168586730957, Transition Loss -1.4082263708114624, Classifier Loss 0.2042529284954071, Total Loss 61.96974182128906\n",
      "7: Encoding Loss 5.984179496765137, Transition Loss -0.38687264919281006, Classifier Loss 0.09714473783969879, Total Loss 45.6193962097168\n",
      "7: Encoding Loss 5.152709484100342, Transition Loss -0.8708750605583191, Classifier Loss 0.047955118119716644, Total Loss 35.711421966552734\n",
      "7: Encoding Loss 6.60146427154541, Transition Loss -0.4947216510772705, Classifier Loss 0.12037546932697296, Total Loss 51.64613723754883\n",
      "7: Encoding Loss 4.83902645111084, Transition Loss -3.085918426513672, Classifier Loss 0.13255371153354645, Total Loss 42.28829574584961\n",
      "7: Encoding Loss 3.4043707847595215, Transition Loss 0.9977296590805054, Classifier Loss 0.07872022688388824, Total Loss 28.69734001159668\n",
      "7: Encoding Loss 3.4457271099090576, Transition Loss -2.35262393951416, Classifier Loss 0.08365129679441452, Total Loss 29.03855323791504\n",
      "7: Encoding Loss 6.25431489944458, Transition Loss -2.1910226345062256, Classifier Loss 0.04423445463180542, Total Loss 41.94845962524414\n",
      "7: Encoding Loss 7.034937858581543, Transition Loss -1.7523753643035889, Classifier Loss 0.07078979164361954, Total Loss 49.287906646728516\n",
      "7: Encoding Loss 6.54378604888916, Transition Loss -1.2811142206192017, Classifier Loss 0.06112829968333244, Total Loss 45.375038146972656\n",
      "7: Encoding Loss 4.515280246734619, Transition Loss -0.8622503280639648, Classifier Loss 0.11776114255189896, Total Loss 38.867454528808594\n",
      "7: Encoding Loss 4.948007106781006, Transition Loss -0.7874118685722351, Classifier Loss 0.05284399911761284, Total Loss 34.97212600708008\n",
      "7: Encoding Loss 2.9964141845703125, Transition Loss -0.7944023609161377, Classifier Loss 0.09333391487598419, Total Loss 27.31155776977539\n",
      "7: Encoding Loss 7.032869815826416, Transition Loss -2.393617868423462, Classifier Loss 0.07905766367912292, Total Loss 50.102027893066406\n",
      "7: Encoding Loss 5.094099998474121, Transition Loss -1.4511682987213135, Classifier Loss 0.06487607210874557, Total Loss 37.05162811279297\n",
      "7: Encoding Loss 6.316413402557373, Transition Loss -1.6708675622940063, Classifier Loss 0.15444958209991455, Total Loss 53.3427734375\n",
      "7: Encoding Loss 4.267493724822998, Transition Loss -1.4995434284210205, Classifier Loss 0.03355681896209717, Total Loss 28.960046768188477\n",
      "7: Encoding Loss 5.734357833862305, Transition Loss -0.2670213282108307, Classifier Loss 0.07263989001512527, Total Loss 41.67002868652344\n",
      "7: Encoding Loss 5.05505895614624, Transition Loss -1.4389748573303223, Classifier Loss 0.1355160027742386, Total Loss 43.881378173828125\n",
      "7: Encoding Loss 4.502894401550293, Transition Loss -1.4402979612350464, Classifier Loss 0.08042120933532715, Total Loss 35.05891418457031\n",
      "7: Encoding Loss 2.966338634490967, Transition Loss -2.3751254081726074, Classifier Loss 0.06795787811279297, Total Loss 24.592870712280273\n",
      "7: Encoding Loss 5.579983711242676, Transition Loss -3.3844377994537354, Classifier Loss 0.06960231810808182, Total Loss 40.43878173828125\n",
      "7: Encoding Loss 3.7898025512695312, Transition Loss -0.6751308441162109, Classifier Loss 0.15672098100185394, Total Loss 38.410640716552734\n",
      "7: Encoding Loss 4.8882012367248535, Transition Loss -0.9060487747192383, Classifier Loss 0.17343828082084656, Total Loss 46.672672271728516\n",
      "7: Encoding Loss 5.636480331420898, Transition Loss -0.619712769985199, Classifier Loss 0.09976878017187119, Total Loss 43.79551315307617\n",
      "7: Encoding Loss 3.9407284259796143, Transition Loss -0.4380565881729126, Classifier Loss 0.056252747774124146, Total Loss 29.26947021484375\n",
      "7: Encoding Loss 8.751346588134766, Transition Loss -0.16152054071426392, Classifier Loss 0.1476108729839325, Total Loss 67.26911163330078\n",
      "7: Encoding Loss 4.762894630432129, Transition Loss -0.9731173515319824, Classifier Loss 0.09512215852737427, Total Loss 38.089195251464844\n",
      "7: Encoding Loss 7.727303504943848, Transition Loss -0.4171949028968811, Classifier Loss 0.1943179816007614, Total Loss 65.79545593261719\n",
      "7: Encoding Loss 5.878026485443115, Transition Loss -2.681924819946289, Classifier Loss 0.17722250521183014, Total Loss 52.98933792114258\n",
      "7: Encoding Loss 5.899294853210449, Transition Loss -1.1001408100128174, Classifier Loss 0.11743335425853729, Total Loss 47.138668060302734\n",
      "7: Encoding Loss 5.687936782836914, Transition Loss -1.8486875295639038, Classifier Loss 0.1482122838497162, Total Loss 48.9481086730957\n",
      "7: Encoding Loss 3.265110731124878, Transition Loss -1.3370258808135986, Classifier Loss 0.08854302763938904, Total Loss 28.444435119628906\n",
      "7: Encoding Loss 3.656343936920166, Transition Loss -0.45044180750846863, Classifier Loss 0.08216462284326553, Total Loss 30.154346466064453\n",
      "7: Encoding Loss 4.473293781280518, Transition Loss -0.8037658929824829, Classifier Loss 0.10590266436338425, Total Loss 37.429710388183594\n",
      "7: Encoding Loss 4.418381214141846, Transition Loss -2.744239330291748, Classifier Loss 0.08021047711372375, Total Loss 34.530235290527344\n",
      "7: Encoding Loss 5.515546798706055, Transition Loss -1.565401315689087, Classifier Loss 0.1362033635377884, Total Loss 46.712989807128906\n",
      "7: Encoding Loss 4.989514350891113, Transition Loss -1.4563872814178467, Classifier Loss 0.09358162432909012, Total Loss 39.2946662902832\n",
      "7: Encoding Loss 3.5913071632385254, Transition Loss -0.38522106409072876, Classifier Loss 0.09829331934452057, Total Loss 31.37702178955078\n",
      "7: Encoding Loss 4.13961124420166, Transition Loss -1.6741944551467896, Classifier Loss 0.05521587282419205, Total Loss 30.358585357666016\n",
      "7: Encoding Loss 6.129426002502441, Transition Loss -1.2495254278182983, Classifier Loss 0.13198420405387878, Total Loss 49.97447967529297\n",
      "7: Encoding Loss 4.972512722015381, Transition Loss 0.12215137481689453, Classifier Loss 0.06323160231113434, Total Loss 36.20709991455078\n",
      "7: Encoding Loss 6.592625617980957, Transition Loss 0.07595887780189514, Classifier Loss 0.10402384400367737, Total Loss 49.988525390625\n",
      "7: Encoding Loss 6.500255584716797, Transition Loss -1.7067228555679321, Classifier Loss 0.29207539558410645, Total Loss 68.2083969116211\n",
      "7: Encoding Loss 6.089097023010254, Transition Loss -1.7428710460662842, Classifier Loss 0.06397043168544769, Total Loss 42.93092727661133\n",
      "7: Encoding Loss 5.18056058883667, Transition Loss -1.3645662069320679, Classifier Loss 0.10793710500001907, Total Loss 41.876529693603516\n",
      "7: Encoding Loss 4.385985374450684, Transition Loss -1.2586246728897095, Classifier Loss 0.08503912389278412, Total Loss 34.81932067871094\n",
      "7: Encoding Loss 5.8010406494140625, Transition Loss -2.0937414169311523, Classifier Loss 0.20782211422920227, Total Loss 55.587615966796875\n",
      "7: Encoding Loss 4.024062633514404, Transition Loss -1.0339720249176025, Classifier Loss 0.07547113299369812, Total Loss 31.691076278686523\n",
      "7: Encoding Loss 6.834928035736084, Transition Loss -2.1765995025634766, Classifier Loss 0.11302328109741211, Total Loss 52.31102752685547\n",
      "7: Encoding Loss 5.751716613769531, Transition Loss -1.4502108097076416, Classifier Loss 0.13807682693004608, Total Loss 48.31740188598633\n",
      "7: Encoding Loss 6.0295233726501465, Transition Loss -1.8704454898834229, Classifier Loss 0.05949331820011139, Total Loss 42.125728607177734\n",
      "7: Encoding Loss 6.732368469238281, Transition Loss -0.8759923577308655, Classifier Loss 0.14935898780822754, Total Loss 55.32975769042969\n",
      "7: Encoding Loss 2.4807422161102295, Transition Loss -1.9967114925384521, Classifier Loss 0.07875721156597137, Total Loss 22.759374618530273\n",
      "7: Encoding Loss 2.702483892440796, Transition Loss -0.6809402108192444, Classifier Loss 0.06081832945346832, Total Loss 22.296464920043945\n",
      "7: Encoding Loss 6.211991310119629, Transition Loss -0.42212802171707153, Classifier Loss 0.09921640157699585, Total Loss 47.19342041015625\n",
      "7: Encoding Loss 4.372400283813477, Transition Loss 0.4777395725250244, Classifier Loss 0.09530283510684967, Total Loss 35.95578384399414\n",
      "7: Encoding Loss 5.67703914642334, Transition Loss -0.9379264712333679, Classifier Loss 0.16835372149944305, Total Loss 50.89723587036133\n",
      "7: Encoding Loss 5.4639129638671875, Transition Loss -1.8991472721099854, Classifier Loss 0.06967845559120178, Total Loss 39.75056457519531\n",
      "7: Encoding Loss 3.6689038276672363, Transition Loss -2.544536590576172, Classifier Loss 0.10208377987146378, Total Loss 32.22078323364258\n",
      "7: Encoding Loss 3.021801233291626, Transition Loss -1.485417127609253, Classifier Loss 0.07970394194126129, Total Loss 26.10060691833496\n",
      "7: Encoding Loss 7.835970878601074, Transition Loss -1.4211221933364868, Classifier Loss 0.17032697796821594, Total Loss 64.0479507446289\n",
      "7: Encoding Loss 4.292588233947754, Transition Loss -0.18332073092460632, Classifier Loss 0.1796664148569107, Total Loss 43.72209930419922\n",
      "7: Encoding Loss 4.715493679046631, Transition Loss -1.51735258102417, Classifier Loss 0.06093929708003998, Total Loss 34.386287689208984\n",
      "7: Encoding Loss 5.40590238571167, Transition Loss 0.11829960346221924, Classifier Loss 0.13067342340946198, Total Loss 45.55008316040039\n",
      "7: Encoding Loss 3.1533522605895996, Transition Loss 0.582883358001709, Classifier Loss 0.06553049385547638, Total Loss 25.706317901611328\n",
      "7: Encoding Loss 3.6708202362060547, Transition Loss -1.4695061445236206, Classifier Loss 0.09686887264251709, Total Loss 31.71122169494629\n",
      "7: Encoding Loss 6.132211208343506, Transition Loss -0.9357171058654785, Classifier Loss 0.09978911280632019, Total Loss 46.77180862426758\n",
      "7: Encoding Loss 3.855419158935547, Transition Loss -0.9741247892379761, Classifier Loss 0.11888959258794785, Total Loss 35.02108383178711\n",
      "7: Encoding Loss 6.565681457519531, Transition Loss -0.9854516983032227, Classifier Loss 0.15127882361412048, Total Loss 54.52157974243164\n",
      "7: Encoding Loss 6.784250736236572, Transition Loss -1.423566222190857, Classifier Loss 0.19230704009532928, Total Loss 59.93564224243164\n",
      "7: Encoding Loss 4.667019844055176, Transition Loss -1.1213963031768799, Classifier Loss 0.038216184824705124, Total Loss 31.82328987121582\n",
      "7: Encoding Loss 4.650076866149902, Transition Loss -0.44275858998298645, Classifier Loss 0.1679655760526657, Total Loss 44.69684600830078\n",
      "7: Encoding Loss 6.210332870483398, Transition Loss -3.3017361164093018, Classifier Loss 0.1835910975933075, Total Loss 55.619789123535156\n",
      "7: Encoding Loss 6.637770652770996, Transition Loss -1.585829496383667, Classifier Loss 0.0645003616809845, Total Loss 46.27602767944336\n",
      "7: Encoding Loss 5.124571323394775, Transition Loss -1.2952656745910645, Classifier Loss 0.0815989077091217, Total Loss 38.90679931640625\n",
      "7: Encoding Loss 6.341318607330322, Transition Loss -2.2542409896850586, Classifier Loss 0.14397361874580383, Total Loss 52.444374084472656\n",
      "7: Encoding Loss 5.201009750366211, Transition Loss -0.48307669162750244, Classifier Loss 0.06544631719589233, Total Loss 37.7504997253418\n",
      "7: Encoding Loss 2.9498789310455322, Transition Loss -1.5581727027893066, Classifier Loss 0.08623901754617691, Total Loss 26.322551727294922\n",
      "7: Encoding Loss 5.360447883605957, Transition Loss -1.907754898071289, Classifier Loss 0.1302868276834488, Total Loss 45.190608978271484\n",
      "7: Encoding Loss 9.08549690246582, Transition Loss -3.264089822769165, Classifier Loss 0.17939229309558868, Total Loss 72.4509048461914\n",
      "7: Encoding Loss 5.353342533111572, Transition Loss -1.3929133415222168, Classifier Loss 0.0602106899023056, Total Loss 38.140567779541016\n",
      "7: Encoding Loss 5.259760856628418, Transition Loss -1.0069804191589355, Classifier Loss 0.04860827326774597, Total Loss 36.41899108886719\n",
      "7: Encoding Loss 6.018366813659668, Transition Loss -0.2625819444656372, Classifier Loss 0.16709795594215393, Total Loss 52.81989288330078\n",
      "7: Encoding Loss 6.024290084838867, Transition Loss -0.9065881371498108, Classifier Loss 0.2786659896373749, Total Loss 64.01197052001953\n",
      "7: Encoding Loss 6.111220359802246, Transition Loss -1.6189523935317993, Classifier Loss 0.18377889692783356, Total Loss 55.04456329345703\n",
      "7: Encoding Loss 6.317694187164307, Transition Loss -2.1401517391204834, Classifier Loss 0.14960002899169922, Total Loss 52.86531448364258\n",
      "7: Encoding Loss 5.230729579925537, Transition Loss -1.410037875175476, Classifier Loss 0.12378107011318207, Total Loss 43.76192092895508\n",
      "7: Encoding Loss 5.484167575836182, Transition Loss -0.5218511819839478, Classifier Loss 0.15366242825984955, Total Loss 48.27103805541992\n",
      "7: Encoding Loss 6.620969295501709, Transition Loss -0.7207410335540771, Classifier Loss 0.17227505147457123, Total Loss 56.953033447265625\n",
      "7: Encoding Loss 4.903945446014404, Transition Loss -0.5568708181381226, Classifier Loss 0.06195636838674545, Total Loss 35.61908721923828\n",
      "7: Encoding Loss 7.664597988128662, Transition Loss -2.1087193489074707, Classifier Loss 0.0785093605518341, Total Loss 53.837684631347656\n",
      "7: Encoding Loss 4.960404396057129, Transition Loss -0.9895696640014648, Classifier Loss 0.11173883080482483, Total Loss 40.9359130859375\n",
      "7: Encoding Loss 4.076786041259766, Transition Loss -1.6800616979599, Classifier Loss 0.09304390102624893, Total Loss 33.764434814453125\n",
      "7: Encoding Loss 5.428661346435547, Transition Loss -0.4626966714859009, Classifier Loss 0.1841212958097458, Total Loss 50.98391342163086\n",
      "7: Encoding Loss 4.949365615844727, Transition Loss -1.4233858585357666, Classifier Loss 0.07873544096946716, Total Loss 37.56917190551758\n",
      "7: Encoding Loss 5.279691696166992, Transition Loss -1.4892927408218384, Classifier Loss 0.15382079780101776, Total Loss 47.05963897705078\n",
      "7: Encoding Loss 10.798158645629883, Transition Loss -2.180847406387329, Classifier Loss 0.12045089155435562, Total Loss 76.83317565917969\n",
      "7: Encoding Loss 6.449054718017578, Transition Loss -0.6987437605857849, Classifier Loss 0.0679311603307724, Total Loss 45.48716735839844\n",
      "7: Encoding Loss 6.720621585845947, Transition Loss 0.7383000254631042, Classifier Loss 0.06815415620803833, Total Loss 47.43446350097656\n",
      "7: Encoding Loss 4.544164180755615, Transition Loss -2.266580581665039, Classifier Loss 0.11095423251390457, Total Loss 38.359500885009766\n",
      "7: Encoding Loss 6.143423080444336, Transition Loss -1.5611237287521362, Classifier Loss 0.12668034434318542, Total Loss 49.52794647216797\n",
      "7: Encoding Loss 3.844839572906494, Transition Loss -0.5918146967887878, Classifier Loss 0.09950827062129974, Total Loss 33.019630432128906\n",
      "7: Encoding Loss 5.367068290710449, Transition Loss -1.3959020376205444, Classifier Loss 0.20457568764686584, Total Loss 52.659423828125\n",
      "7: Encoding Loss 9.001419067382812, Transition Loss -1.334606409072876, Classifier Loss 0.11541730910539627, Total Loss 65.54971313476562\n",
      "7: Encoding Loss 4.810783386230469, Transition Loss -1.7888455390930176, Classifier Loss 0.096773162484169, Total Loss 38.54130172729492\n",
      "7: Encoding Loss 5.64058256149292, Transition Loss -1.0434023141860962, Classifier Loss 0.07914978265762329, Total Loss 41.758060455322266\n",
      "7: Encoding Loss 5.952000617980957, Transition Loss -2.079437732696533, Classifier Loss 0.057648058980703354, Total Loss 41.47597885131836\n",
      "7: Encoding Loss 5.1094183921813965, Transition Loss -2.1827831268310547, Classifier Loss 0.057216957211494446, Total Loss 36.37733459472656\n",
      "7: Encoding Loss 4.589430809020996, Transition Loss -1.071089267730713, Classifier Loss 0.038153160363435745, Total Loss 31.351472854614258\n",
      "7: Encoding Loss 4.181147575378418, Transition Loss -2.418853282928467, Classifier Loss 0.11333601176738739, Total Loss 36.419517517089844\n",
      "7: Encoding Loss 6.17280912399292, Transition Loss -0.5521414875984192, Classifier Loss 0.1177232638001442, Total Loss 48.808963775634766\n",
      "7: Encoding Loss 5.637158393859863, Transition Loss -0.7909897565841675, Classifier Loss 0.07147227227687836, Total Loss 40.96986389160156\n",
      "7: Encoding Loss 5.445746898651123, Transition Loss -3.0605223178863525, Classifier Loss 0.10891248285770416, Total Loss 43.564510345458984\n",
      "7: Encoding Loss 6.951167106628418, Transition Loss -3.057328701019287, Classifier Loss 0.12620168924331665, Total Loss 54.32594680786133\n",
      "7: Encoding Loss 4.776278018951416, Transition Loss 0.058371931314468384, Classifier Loss 0.06503676623106003, Total Loss 35.184696197509766\n",
      "7: Encoding Loss 4.8586015701293945, Transition Loss -1.8625839948654175, Classifier Loss 0.14861075580120087, Total Loss 44.011940002441406\n",
      "7: Encoding Loss 5.082759857177734, Transition Loss -1.3899568319320679, Classifier Loss 0.08069998770952225, Total Loss 38.566001892089844\n",
      "7: Encoding Loss 3.614335536956787, Transition Loss -1.909867763519287, Classifier Loss 0.07263074070215225, Total Loss 28.94832420349121\n",
      "7: Encoding Loss 5.254624366760254, Transition Loss -2.741914749145508, Classifier Loss 0.15376782417297363, Total Loss 46.9034309387207\n",
      "7: Encoding Loss 6.64291524887085, Transition Loss -2.2118794918060303, Classifier Loss 0.09890293329954147, Total Loss 49.74690246582031\n",
      "7: Encoding Loss 6.401430606842041, Transition Loss -2.747969627380371, Classifier Loss 0.11039259284734726, Total Loss 49.446746826171875\n",
      "7: Encoding Loss 7.114285469055176, Transition Loss -1.0427683591842651, Classifier Loss 0.1551939696073532, Total Loss 58.20469665527344\n",
      "7: Encoding Loss 5.031767845153809, Transition Loss 0.3490583300590515, Classifier Loss 0.10699035227298737, Total Loss 41.029266357421875\n",
      "7: Encoding Loss 4.921487331390381, Transition Loss -2.625844955444336, Classifier Loss 0.08558949083089828, Total Loss 38.08682632446289\n",
      "7: Encoding Loss 5.0804643630981445, Transition Loss -1.8658796548843384, Classifier Loss 0.12545964121818542, Total Loss 43.02800369262695\n",
      "7: Encoding Loss 4.409082412719727, Transition Loss -1.6956228017807007, Classifier Loss 0.08345500379800797, Total Loss 34.79931640625\n",
      "7: Encoding Loss 4.268113136291504, Transition Loss -1.9575862884521484, Classifier Loss 0.09086811542510986, Total Loss 34.694705963134766\n",
      "7: Encoding Loss 3.4631404876708984, Transition Loss -2.255570888519287, Classifier Loss 0.059709127992391586, Total Loss 26.74885368347168\n",
      "7: Encoding Loss 3.6293022632598877, Transition Loss -0.6613063812255859, Classifier Loss 0.11125285178422928, Total Loss 32.90083694458008\n",
      "7: Encoding Loss 4.943982124328613, Transition Loss -0.9037906527519226, Classifier Loss 0.12927885353565216, Total Loss 42.59141540527344\n",
      "7: Encoding Loss 6.314202785491943, Transition Loss -0.3506760001182556, Classifier Loss 0.1055208146572113, Total Loss 48.43716049194336\n",
      "7: Encoding Loss 4.314757347106934, Transition Loss -0.68777996301651, Classifier Loss 0.10657429695129395, Total Loss 36.54570007324219\n",
      "7: Encoding Loss 7.0103912353515625, Transition Loss -1.2238184213638306, Classifier Loss 0.1194664016366005, Total Loss 54.00849914550781\n",
      "7: Encoding Loss 6.51317834854126, Transition Loss -0.8622430562973022, Classifier Loss 0.11988695710897446, Total Loss 51.06742477416992\n",
      "7: Encoding Loss 4.791772365570068, Transition Loss -1.7628082036972046, Classifier Loss 0.15679892897605896, Total Loss 44.4298210144043\n",
      "7: Encoding Loss 5.267043113708496, Transition Loss -0.6592021584510803, Classifier Loss 0.10892108082771301, Total Loss 42.49410629272461\n",
      "7: Encoding Loss 7.649228572845459, Transition Loss -2.2455501556396484, Classifier Loss 0.16789203882217407, Total Loss 62.68368148803711\n",
      "7: Encoding Loss 5.035266399383545, Transition Loss -1.7442939281463623, Classifier Loss 0.057246532291173935, Total Loss 35.93555450439453\n",
      "7: Encoding Loss 7.307884216308594, Transition Loss -0.9736822843551636, Classifier Loss 0.19686703383922577, Total Loss 63.53361892700195\n",
      "7: Encoding Loss 5.798836708068848, Transition Loss -0.552236795425415, Classifier Loss 0.1490321308374405, Total Loss 49.696014404296875\n",
      "7: Encoding Loss 4.897040367126465, Transition Loss -1.8133103847503662, Classifier Loss 0.12030063569545746, Total Loss 41.411582946777344\n",
      "7: Encoding Loss 4.302099704742432, Transition Loss -2.7601866722106934, Classifier Loss 0.1565796434879303, Total Loss 41.46946334838867\n",
      "7: Encoding Loss 6.591108798980713, Transition Loss -2.0548603534698486, Classifier Loss 0.13642795383930206, Total Loss 53.188629150390625\n",
      "7: Encoding Loss 4.601582050323486, Transition Loss -0.8090778589248657, Classifier Loss 0.09462261199951172, Total Loss 37.07143020629883\n",
      "7: Encoding Loss 2.905005931854248, Transition Loss -1.1954052448272705, Classifier Loss 0.07550572603940964, Total Loss 24.980131149291992\n",
      "7: Encoding Loss 2.7623696327209473, Transition Loss -1.7510572671890259, Classifier Loss 0.08376295864582062, Total Loss 24.94981575012207\n",
      "7: Encoding Loss 3.5855727195739746, Transition Loss -1.4739503860473633, Classifier Loss 0.0996420606970787, Total Loss 31.477052688598633\n",
      "7: Encoding Loss 8.692024230957031, Transition Loss 0.5774171352386475, Classifier Loss 0.11800479143857956, Total Loss 64.18359375\n",
      "7: Encoding Loss 5.8792595863342285, Transition Loss 0.16361212730407715, Classifier Loss 0.1477799266576767, Total Loss 50.118995666503906\n",
      "7: Encoding Loss 4.0820631980896, Transition Loss 0.55418461561203, Classifier Loss 0.07108861207962036, Total Loss 31.82291603088379\n",
      "7: Encoding Loss 7.8339667320251465, Transition Loss -0.4555051028728485, Classifier Loss 0.2628718614578247, Total Loss 73.29080200195312\n",
      "7: Encoding Loss 4.878588676452637, Transition Loss -1.5992076396942139, Classifier Loss 0.058950275182724, Total Loss 35.16592025756836\n",
      "7: Encoding Loss 4.633688449859619, Transition Loss -2.276001453399658, Classifier Loss 0.12541478872299194, Total Loss 40.34270095825195\n",
      "7: Encoding Loss 5.728705406188965, Transition Loss -1.2179393768310547, Classifier Loss 0.14329075813293457, Total Loss 48.70082092285156\n",
      "7: Encoding Loss 5.029852867126465, Transition Loss -1.4829819202423096, Classifier Loss 0.058178968727588654, Total Loss 35.996421813964844\n",
      "7: Encoding Loss 4.56102180480957, Transition Loss -0.7394739389419556, Classifier Loss 0.07669421285390854, Total Loss 35.035255432128906\n",
      "7: Encoding Loss 6.016477108001709, Transition Loss -0.8526864647865295, Classifier Loss 0.11792554706335068, Total Loss 47.89107894897461\n",
      "7: Encoding Loss 3.7654242515563965, Transition Loss 0.08024333417415619, Classifier Loss 0.04576379060745239, Total Loss 27.20102310180664\n",
      "7: Encoding Loss 7.048670768737793, Transition Loss 0.4876282811164856, Classifier Loss 0.08780688047409058, Total Loss 51.26776885986328\n",
      "7: Encoding Loss 5.42464542388916, Transition Loss -2.0194263458251953, Classifier Loss 0.11514975130558014, Total Loss 44.062042236328125\n",
      "7: Encoding Loss 4.120769500732422, Transition Loss -1.6636419296264648, Classifier Loss 0.1394766867160797, Total Loss 38.67162322998047\n",
      "7: Encoding Loss 6.348448753356934, Transition Loss -1.9434705972671509, Classifier Loss 0.06785593926906586, Total Loss 44.875511169433594\n",
      "7: Encoding Loss 3.6914215087890625, Transition Loss -1.128697156906128, Classifier Loss 0.1207754984498024, Total Loss 34.22562789916992\n",
      "7: Encoding Loss 2.2909939289093018, Transition Loss -1.6279993057250977, Classifier Loss 0.10207854211330414, Total Loss 23.953168869018555\n",
      "7: Encoding Loss 2.325700044631958, Transition Loss -1.1610524654388428, Classifier Loss 0.06728319823741913, Total Loss 20.682056427001953\n",
      "7: Encoding Loss 8.71679401397705, Transition Loss -2.129514694213867, Classifier Loss 0.12481347471475601, Total Loss 64.78125762939453\n",
      "7: Encoding Loss 4.936584949493408, Transition Loss -1.7098499536514282, Classifier Loss 0.05674372985959053, Total Loss 35.2932014465332\n",
      "7: Encoding Loss 2.6993327140808105, Transition Loss -1.9655455350875854, Classifier Loss 0.07883023470640182, Total Loss 24.078235626220703\n",
      "7: Encoding Loss 6.406708717346191, Transition Loss -1.3146599531173706, Classifier Loss 0.11680607497692108, Total Loss 50.12033462524414\n",
      "7: Encoding Loss 3.459334135055542, Transition Loss -1.0635101795196533, Classifier Loss 0.08124838769435883, Total Loss 28.88041877746582\n",
      "7: Encoding Loss 5.386713981628418, Transition Loss -0.7808061838150024, Classifier Loss 0.11067643761634827, Total Loss 43.38761901855469\n",
      "7: Encoding Loss 5.5055766105651855, Transition Loss -0.5763289928436279, Classifier Loss 0.08099757879972458, Total Loss 41.132991790771484\n",
      "7: Encoding Loss 5.247145652770996, Transition Loss -2.8589558601379395, Classifier Loss 0.13843008875846863, Total Loss 45.32474136352539\n",
      "7: Encoding Loss 5.522531509399414, Transition Loss -0.5588349103927612, Classifier Loss 0.1262413114309311, Total Loss 45.75909423828125\n",
      "7: Encoding Loss 4.870194435119629, Transition Loss -1.5518734455108643, Classifier Loss 0.17213353514671326, Total Loss 46.43389892578125\n",
      "7: Encoding Loss 4.432147026062012, Transition Loss -2.588183641433716, Classifier Loss 0.0940595492720604, Total Loss 35.997806549072266\n",
      "7: Encoding Loss 3.55291485786438, Transition Loss -1.2850680351257324, Classifier Loss 0.047156162559986115, Total Loss 26.0325927734375\n",
      "7: Encoding Loss 7.634072780609131, Transition Loss -1.054543137550354, Classifier Loss 0.16680890321731567, Total Loss 62.48490524291992\n",
      "7: Encoding Loss 6.818946838378906, Transition Loss -0.9239542484283447, Classifier Loss 0.16444221138954163, Total Loss 57.3575325012207\n",
      "7: Encoding Loss 5.343601226806641, Transition Loss -1.2065925598144531, Classifier Loss 0.16805775463581085, Total Loss 48.86689758300781\n",
      "7: Encoding Loss 3.5652976036071777, Transition Loss -1.802685022354126, Classifier Loss 0.07296939194202423, Total Loss 28.688005447387695\n",
      "7: Encoding Loss 10.577241897583008, Transition Loss -1.192392349243164, Classifier Loss 0.17347010970115662, Total Loss 80.80998229980469\n",
      "7: Encoding Loss 6.664784908294678, Transition Loss -2.6220688819885254, Classifier Loss 0.0720488429069519, Total Loss 47.19254684448242\n",
      "7: Encoding Loss 4.987478733062744, Transition Loss -1.9952764511108398, Classifier Loss 0.14904944598674774, Total Loss 44.82902145385742\n",
      "7: Encoding Loss 6.351994514465332, Transition Loss -2.1692326068878174, Classifier Loss 0.169903963804245, Total Loss 55.101497650146484\n",
      "7: Encoding Loss 6.850798606872559, Transition Loss -1.4744328260421753, Classifier Loss 0.1370987445116043, Total Loss 54.8140754699707\n",
      "7: Encoding Loss 5.4922027587890625, Transition Loss -0.6367437839508057, Classifier Loss 0.09209838509559631, Total Loss 42.16279983520508\n",
      "7: Encoding Loss 7.209251403808594, Transition Loss -2.65486478805542, Classifier Loss 0.09876549243927002, Total Loss 53.13099670410156\n",
      "7: Encoding Loss 7.301889896392822, Transition Loss -1.426259994506836, Classifier Loss 0.20727038383483887, Total Loss 64.53780364990234\n",
      "7: Encoding Loss 7.560049533843994, Transition Loss -2.230346202850342, Classifier Loss 0.06428737193346024, Total Loss 51.788143157958984\n",
      "7: Encoding Loss 6.641944885253906, Transition Loss -0.5126577019691467, Classifier Loss 0.21222759783267975, Total Loss 61.074222564697266\n",
      "7: Encoding Loss 6.0735039710998535, Transition Loss -1.893352746963501, Classifier Loss 0.06555932760238647, Total Loss 42.99619674682617\n",
      "7: Encoding Loss 5.209446907043457, Transition Loss -1.3670320510864258, Classifier Loss 0.07397301495075226, Total Loss 38.653438568115234\n",
      "7: Encoding Loss 4.063022136688232, Transition Loss -2.515157699584961, Classifier Loss 0.06941857188940048, Total Loss 31.318986892700195\n",
      "7: Encoding Loss 4.27761173248291, Transition Loss -1.0492146015167236, Classifier Loss 0.10361739248037338, Total Loss 36.0269889831543\n",
      "7: Encoding Loss 3.97078013420105, Transition Loss -2.937310218811035, Classifier Loss 0.08289651572704315, Total Loss 32.1131591796875\n",
      "7: Encoding Loss 7.565443992614746, Transition Loss -0.9323004484176636, Classifier Loss 0.10640154033899307, Total Loss 56.032447814941406\n",
      "7: Encoding Loss 4.487643241882324, Transition Loss -1.832731008529663, Classifier Loss 0.10644359886646271, Total Loss 37.569488525390625\n",
      "7: Encoding Loss 2.8996353149414062, Transition Loss -1.2264306545257568, Classifier Loss 0.05094686523079872, Total Loss 22.492008209228516\n",
      "7: Encoding Loss 7.433826446533203, Transition Loss -2.251643180847168, Classifier Loss 0.07398448139429092, Total Loss 52.00050735473633\n",
      "7: Encoding Loss 7.921291351318359, Transition Loss -1.4626456499099731, Classifier Loss 0.08660449832677841, Total Loss 56.18761444091797\n",
      "7: Encoding Loss 5.952404499053955, Transition Loss -0.5350853204727173, Classifier Loss 0.1345701962709427, Total Loss 49.171234130859375\n",
      "7: Encoding Loss 5.967870712280273, Transition Loss -1.741428256034851, Classifier Loss 0.044154342263936996, Total Loss 40.221961975097656\n",
      "7: Encoding Loss 4.777272701263428, Transition Loss -1.0602433681488037, Classifier Loss 0.11841292679309845, Total Loss 40.5045051574707\n",
      "7: Encoding Loss 5.6499342918396, Transition Loss -1.6384190320968628, Classifier Loss 0.09882421046495438, Total Loss 43.7813720703125\n",
      "7: Encoding Loss 5.876018524169922, Transition Loss -1.1858594417572021, Classifier Loss 0.1088126078248024, Total Loss 46.136898040771484\n",
      "7: Encoding Loss 4.027066230773926, Transition Loss -0.9349076747894287, Classifier Loss 0.08942464739084244, Total Loss 33.104488372802734\n",
      "7: Encoding Loss 6.086756706237793, Transition Loss -2.0482428073883057, Classifier Loss 0.1597844362258911, Total Loss 52.498165130615234\n",
      "7: Encoding Loss 3.9281272888183594, Transition Loss -1.1633373498916626, Classifier Loss 0.08155539631843567, Total Loss 31.723838806152344\n",
      "7: Encoding Loss 5.098141670227051, Transition Loss -0.4278855323791504, Classifier Loss 0.046654459089040756, Total Loss 35.25412368774414\n",
      "7: Encoding Loss 7.096358299255371, Transition Loss -1.452433705329895, Classifier Loss 0.20182472467422485, Total Loss 62.76004409790039\n",
      "7: Encoding Loss 5.313338756561279, Transition Loss -0.9623792171478271, Classifier Loss 0.06043031066656113, Total Loss 37.92267990112305\n",
      "7: Encoding Loss 5.720337867736816, Transition Loss -3.679342746734619, Classifier Loss 0.07027500122785568, Total Loss 41.34805679321289\n",
      "7: Encoding Loss 4.440843105316162, Transition Loss -1.7553173303604126, Classifier Loss 0.057718969881534576, Total Loss 32.416255950927734\n",
      "7: Encoding Loss 5.339059352874756, Transition Loss -0.28689923882484436, Classifier Loss 0.17550288140773773, Total Loss 49.58453369140625\n",
      "7: Encoding Loss 3.773768901824951, Transition Loss -1.4003586769104004, Classifier Loss 0.09969940036535263, Total Loss 32.611995697021484\n",
      "7: Encoding Loss 8.196737289428711, Transition Loss -1.9801291227340698, Classifier Loss 0.1689716875553131, Total Loss 66.0768051147461\n",
      "7: Encoding Loss 5.505613803863525, Transition Loss -0.8258413672447205, Classifier Loss 0.07330557703971863, Total Loss 40.36391067504883\n",
      "7: Encoding Loss 5.272881984710693, Transition Loss -2.6490368843078613, Classifier Loss 0.10853518545627594, Total Loss 42.489749908447266\n",
      "7: Encoding Loss 4.639704704284668, Transition Loss -2.906425714492798, Classifier Loss 0.13484257459640503, Total Loss 41.32132339477539\n",
      "7: Encoding Loss 4.754810333251953, Transition Loss -1.6072041988372803, Classifier Loss 0.133257657289505, Total Loss 41.85398483276367\n",
      "7: Encoding Loss 5.6730241775512695, Transition Loss -1.3826954364776611, Classifier Loss 0.10218414664268494, Total Loss 44.25600814819336\n",
      "7: Encoding Loss 5.192810535430908, Transition Loss -1.955629587173462, Classifier Loss 0.18143996596336365, Total Loss 49.300079345703125\n",
      "7: Encoding Loss 6.194526672363281, Transition Loss -1.3939722776412964, Classifier Loss 0.04144999384880066, Total Loss 41.31160354614258\n",
      "7: Encoding Loss 4.680418968200684, Transition Loss 0.3337104618549347, Classifier Loss 0.05907623469829559, Total Loss 34.12362289428711\n",
      "7: Encoding Loss 5.779419422149658, Transition Loss -1.9674406051635742, Classifier Loss 0.06417550891637802, Total Loss 41.093284606933594\n",
      "7: Encoding Loss 4.117889881134033, Transition Loss -1.5325011014938354, Classifier Loss 0.08134030550718307, Total Loss 32.840755462646484\n",
      "7: Encoding Loss 6.7955827713012695, Transition Loss -0.4600275158882141, Classifier Loss 0.12716497480869293, Total Loss 53.48981475830078\n",
      "7: Encoding Loss 5.0175299644470215, Transition Loss -2.4003541469573975, Classifier Loss 0.04691101610660553, Total Loss 34.79532241821289\n",
      "7: Encoding Loss 7.308068752288818, Transition Loss -1.6541166305541992, Classifier Loss 0.22288793325424194, Total Loss 66.13654327392578\n",
      "7: Encoding Loss 5.609250545501709, Transition Loss -1.139114499092102, Classifier Loss 0.12786132097244263, Total Loss 46.441184997558594\n",
      "7: Encoding Loss 5.257422924041748, Transition Loss -1.8717968463897705, Classifier Loss 0.06515707075595856, Total Loss 38.05949783325195\n",
      "7: Encoding Loss 5.156558990478516, Transition Loss -0.11808685958385468, Classifier Loss 0.07375921308994293, Total Loss 38.31523132324219\n",
      "7: Encoding Loss 5.312622547149658, Transition Loss -1.1751816272735596, Classifier Loss 0.06788591295480728, Total Loss 38.66386032104492\n",
      "7: Encoding Loss 5.389996528625488, Transition Loss -0.35989999771118164, Classifier Loss 0.12065309286117554, Total Loss 44.40514373779297\n",
      "7: Encoding Loss 7.179643154144287, Transition Loss -0.4403478503227234, Classifier Loss 0.055147506296634674, Total Loss 48.592437744140625\n",
      "7: Encoding Loss 4.46054220199585, Transition Loss -1.7039821147918701, Classifier Loss 0.18433205783367157, Total Loss 45.195777893066406\n",
      "7: Encoding Loss 5.395508766174316, Transition Loss -0.43529078364372253, Classifier Loss 0.07916528731584549, Total Loss 40.289405822753906\n",
      "7: Encoding Loss 4.347445964813232, Transition Loss -0.4828178286552429, Classifier Loss 0.1059601679444313, Total Loss 36.68050003051758\n",
      "7: Encoding Loss 4.704514026641846, Transition Loss -1.7417235374450684, Classifier Loss 0.044050201773643494, Total Loss 32.63140869140625\n",
      "7: Encoding Loss 4.716032028198242, Transition Loss -2.0424506664276123, Classifier Loss 0.17992711067199707, Total Loss 46.2880859375\n",
      "7: Encoding Loss 4.891800403594971, Transition Loss -2.136284112930298, Classifier Loss 0.14563870429992676, Total Loss 43.913818359375\n",
      "7: Encoding Loss 3.624931812286377, Transition Loss -2.8754842281341553, Classifier Loss 0.08968263119459152, Total Loss 30.716703414916992\n",
      "7: Encoding Loss 3.9399447441101074, Transition Loss -0.685718297958374, Classifier Loss 0.13064095377922058, Total Loss 36.7034912109375\n",
      "7: Encoding Loss 5.6660051345825195, Transition Loss -0.632867693901062, Classifier Loss 0.1327401101589203, Total Loss 47.26979064941406\n",
      "7: Encoding Loss 3.581541061401367, Transition Loss -2.2682857513427734, Classifier Loss 0.0634155422449112, Total Loss 27.829893112182617\n",
      "7: Encoding Loss 5.013424396514893, Transition Loss 0.14130905270576477, Classifier Loss 0.04031902179121971, Total Loss 34.16897201538086\n",
      "7: Encoding Loss 5.784457683563232, Transition Loss -2.254276752471924, Classifier Loss 0.08860448002815247, Total Loss 43.5662956237793\n",
      "7: Encoding Loss 4.114596366882324, Transition Loss -0.8800624012947083, Classifier Loss 0.06424042582511902, Total Loss 31.111268997192383\n",
      "7: Encoding Loss 8.269266128540039, Transition Loss 0.5144481062889099, Classifier Loss 0.2582117021083832, Total Loss 75.64254760742188\n",
      "7: Encoding Loss 6.037961006164551, Transition Loss -1.6097819805145264, Classifier Loss 0.06233620271086693, Total Loss 42.46074295043945\n",
      "7: Encoding Loss 7.119392395019531, Transition Loss 0.38294386863708496, Classifier Loss 0.0808565765619278, Total Loss 50.95519256591797\n",
      "7: Encoding Loss 5.076223850250244, Transition Loss -1.7116479873657227, Classifier Loss 0.15088671445846558, Total Loss 45.54533386230469\n",
      "7: Encoding Loss 3.517200469970703, Transition Loss -1.1721590757369995, Classifier Loss 0.10193095356225967, Total Loss 31.29582977294922\n",
      "7: Encoding Loss 6.404748439788818, Transition Loss -1.7024109363555908, Classifier Loss 0.12375867366790771, Total Loss 50.80367660522461\n",
      "7: Encoding Loss 9.212724685668945, Transition Loss -2.875018358230591, Classifier Loss 0.19975288212299347, Total Loss 75.25048065185547\n",
      "7: Encoding Loss 7.233529090881348, Transition Loss -1.5058531761169434, Classifier Loss 0.29579609632492065, Total Loss 72.98018646240234\n",
      "7: Encoding Loss 4.243053913116455, Transition Loss -1.3782731294631958, Classifier Loss 0.11138468980789185, Total Loss 36.59624099731445\n",
      "7: Encoding Loss 7.751783847808838, Transition Loss 0.3304533362388611, Classifier Loss 0.09871245175600052, Total Loss 56.51413345336914\n",
      "7: Encoding Loss 8.972574234008789, Transition Loss -1.48465096950531, Classifier Loss 0.32974115014076233, Total Loss 86.8089599609375\n",
      "7: Encoding Loss 6.733752250671387, Transition Loss -0.8492001295089722, Classifier Loss 0.12853096425533295, Total Loss 53.255271911621094\n",
      "7: Encoding Loss 7.356176853179932, Transition Loss -0.26464182138442993, Classifier Loss 0.1422325223684311, Total Loss 58.360206604003906\n",
      "7: Encoding Loss 7.3393096923828125, Transition Loss -0.589516818523407, Classifier Loss 0.10576526820659637, Total Loss 54.61214828491211\n",
      "7: Encoding Loss 5.340837478637695, Transition Loss -2.0072379112243652, Classifier Loss 0.11076585203409195, Total Loss 43.12080764770508\n",
      "7: Encoding Loss 4.843255043029785, Transition Loss -1.9163919687271118, Classifier Loss 0.1303195357322693, Total Loss 42.09071731567383\n",
      "7: Encoding Loss 5.93979549407959, Transition Loss -3.3897125720977783, Classifier Loss 0.13643789291381836, Total Loss 49.28120803833008\n",
      "7: Encoding Loss 4.778237819671631, Transition Loss -2.256457805633545, Classifier Loss 0.08380894362926483, Total Loss 37.04941940307617\n",
      "7: Encoding Loss 4.253294944763184, Transition Loss -1.2712223529815674, Classifier Loss 0.06805820018053055, Total Loss 32.3250846862793\n",
      "7: Encoding Loss 5.233329772949219, Transition Loss -1.8184157609939575, Classifier Loss 0.1756446212530136, Total Loss 48.963714599609375\n",
      "7: Encoding Loss 3.27128529548645, Transition Loss -1.9157934188842773, Classifier Loss 0.09743569046258926, Total Loss 29.370513916015625\n",
      "7: Encoding Loss 4.331027984619141, Transition Loss -1.5554157495498657, Classifier Loss 0.12773005664348602, Total Loss 38.75855255126953\n",
      "7: Encoding Loss 6.36936092376709, Transition Loss -1.6496542692184448, Classifier Loss 0.2167845219373703, Total Loss 59.893959045410156\n",
      "7: Encoding Loss 5.470883369445801, Transition Loss -0.45100927352905273, Classifier Loss 0.15761123597621918, Total Loss 48.586246490478516\n",
      "7: Encoding Loss 4.093996524810791, Transition Loss -1.8882946968078613, Classifier Loss 0.13463769853115082, Total Loss 38.02699279785156\n",
      "7: Encoding Loss 5.6020941734313965, Transition Loss -2.2417798042297363, Classifier Loss 0.08527544140815735, Total Loss 42.139217376708984\n",
      "7: Encoding Loss 4.067414283752441, Transition Loss -0.6570711731910706, Classifier Loss 0.0631856620311737, Total Loss 30.722789764404297\n",
      "7: Encoding Loss 5.596285343170166, Transition Loss -1.4174840450286865, Classifier Loss 0.046188972890377045, Total Loss 38.196041107177734\n",
      "7: Encoding Loss 4.42574405670166, Transition Loss -1.4433908462524414, Classifier Loss 0.05569363385438919, Total Loss 32.123252868652344\n",
      "7: Encoding Loss 3.84968638420105, Transition Loss -0.5721345543861389, Classifier Loss 0.13555388152599335, Total Loss 36.65327835083008\n",
      "7: Encoding Loss 6.024113655090332, Transition Loss -0.7773175835609436, Classifier Loss 0.13850893080234528, Total Loss 49.995262145996094\n",
      "7: Encoding Loss 6.946773529052734, Transition Loss -0.22872045636177063, Classifier Loss 0.12805843353271484, Total Loss 54.486392974853516\n",
      "7: Encoding Loss 6.525321006774902, Transition Loss -0.7571353316307068, Classifier Loss 0.11117192357778549, Total Loss 50.26881790161133\n",
      "7: Encoding Loss 3.0536739826202393, Transition Loss -2.290341854095459, Classifier Loss 0.09884292632341385, Total Loss 28.205421447753906\n",
      "7: Encoding Loss 7.341675758361816, Transition Loss -1.5388765335083008, Classifier Loss 0.16115859150886536, Total Loss 60.16530227661133\n",
      "7: Encoding Loss 4.602999687194824, Transition Loss -2.4847583770751953, Classifier Loss 0.1062254011631012, Total Loss 38.23954391479492\n",
      "7: Encoding Loss 5.974146842956543, Transition Loss -1.0008779764175415, Classifier Loss 0.09672309458255768, Total Loss 45.51679229736328\n",
      "7: Encoding Loss 7.204801082611084, Transition Loss -2.101735830307007, Classifier Loss 0.19827419519424438, Total Loss 63.055389404296875\n",
      "7: Encoding Loss 5.685291767120361, Transition Loss -2.0682666301727295, Classifier Loss 0.156968355178833, Total Loss 49.80775833129883\n",
      "7: Encoding Loss 7.53973913192749, Transition Loss -0.21903136372566223, Classifier Loss 0.14532923698425293, Total Loss 59.77127456665039\n",
      "7: Encoding Loss 4.842999458312988, Transition Loss 0.115608811378479, Classifier Loss 0.10175583511590958, Total Loss 39.279823303222656\n",
      "7: Encoding Loss 5.049222946166992, Transition Loss -2.016324996948242, Classifier Loss 0.09082584083080292, Total Loss 39.37711715698242\n",
      "7: Encoding Loss 6.924442291259766, Transition Loss -0.7514249086380005, Classifier Loss 0.14742323756217957, Total Loss 56.28867721557617\n",
      "7: Encoding Loss 6.557806968688965, Transition Loss -1.3341186046600342, Classifier Loss 0.11343462020158768, Total Loss 50.68977355957031\n",
      "7: Encoding Loss 5.861798286437988, Transition Loss -0.8493661284446716, Classifier Loss 0.12459824979305267, Total Loss 47.63027572631836\n",
      "7: Encoding Loss 4.212603569030762, Transition Loss 0.1523960828781128, Classifier Loss 0.11771988123655319, Total Loss 37.10857009887695\n",
      "7: Encoding Loss 3.3226287364959717, Transition Loss -0.8856599926948547, Classifier Loss 0.06700822710990906, Total Loss 26.636241912841797\n",
      "7: Encoding Loss 5.554678440093994, Transition Loss -1.8039698600769043, Classifier Loss 0.07989917695522308, Total Loss 41.31726837158203\n",
      "7: Encoding Loss 4.602488040924072, Transition Loss -1.3329994678497314, Classifier Loss 0.1032354086637497, Total Loss 37.93793487548828\n",
      "7: Encoding Loss 4.770918369293213, Transition Loss -0.7557286024093628, Classifier Loss 0.2053840160369873, Total Loss 49.163612365722656\n",
      "7: Encoding Loss 4.457856178283691, Transition Loss -2.2266411781311035, Classifier Loss 0.04992212727665901, Total Loss 31.73845863342285\n",
      "7: Encoding Loss 4.6370038986206055, Transition Loss -2.6867170333862305, Classifier Loss 0.12601317465305328, Total Loss 40.42226791381836\n",
      "7: Encoding Loss 1.9751970767974854, Transition Loss -0.7233529090881348, Classifier Loss 0.06519169360399246, Total Loss 18.37006187438965\n",
      "7: Encoding Loss 5.190559387207031, Transition Loss -0.8237717151641846, Classifier Loss 0.06535698473453522, Total Loss 37.67873001098633\n",
      "7: Encoding Loss 4.807551860809326, Transition Loss -1.4730417728424072, Classifier Loss 0.14548656344413757, Total Loss 43.39337921142578\n",
      "7: Encoding Loss 3.830878734588623, Transition Loss -0.7326196432113647, Classifier Loss 0.06536906212568283, Total Loss 29.52188491821289\n",
      "7: Encoding Loss 4.736111640930176, Transition Loss -0.19790077209472656, Classifier Loss 0.03578651696443558, Total Loss 31.995241165161133\n",
      "7: Encoding Loss 7.469420433044434, Transition Loss -2.1188411712646484, Classifier Loss 0.20428036153316498, Total Loss 65.24371337890625\n",
      "7: Encoding Loss 5.101825714111328, Transition Loss -0.6228516101837158, Classifier Loss 0.20080403983592987, Total Loss 50.69111251831055\n",
      "7: Encoding Loss 4.942233085632324, Transition Loss -1.1503276824951172, Classifier Loss 0.20448820292949677, Total Loss 50.10175704956055\n",
      "7: Encoding Loss 4.630860805511475, Transition Loss -1.7881776094436646, Classifier Loss 0.09276267141103745, Total Loss 37.06071472167969\n",
      "7: Encoding Loss 6.504944801330566, Transition Loss -1.702580451965332, Classifier Loss 0.15537630021572113, Total Loss 54.56661605834961\n",
      "7: Encoding Loss 5.673050880432129, Transition Loss -1.0401554107666016, Classifier Loss 0.10480305552482605, Total Loss 44.51819610595703\n",
      "7: Encoding Loss 5.8381242752075195, Transition Loss -0.5233016014099121, Classifier Loss 0.07637618482112885, Total Loss 42.66615676879883\n",
      "7: Encoding Loss 4.220104694366455, Transition Loss -1.9337975978851318, Classifier Loss 0.11254062503576279, Total Loss 36.573917388916016\n",
      "7: Encoding Loss 6.302927494049072, Transition Loss -0.07055547833442688, Classifier Loss 0.09190689772367477, Total Loss 47.00822830200195\n",
      "7: Encoding Loss 4.74846076965332, Transition Loss -1.5959954261779785, Classifier Loss 0.09918622672557831, Total Loss 38.40875244140625\n",
      "7: Encoding Loss 5.788488388061523, Transition Loss -1.437960147857666, Classifier Loss 0.11722704023122787, Total Loss 46.453060150146484\n",
      "7: Encoding Loss 6.323056697845459, Transition Loss -0.6457958221435547, Classifier Loss 0.09080227464437485, Total Loss 47.018310546875\n",
      "7: Encoding Loss 4.797023773193359, Transition Loss -1.8477482795715332, Classifier Loss 0.05634421855211258, Total Loss 34.41582489013672\n",
      "7: Encoding Loss 4.046391487121582, Transition Loss -1.2912474870681763, Classifier Loss 0.09677520394325256, Total Loss 33.95535659790039\n",
      "7: Encoding Loss 3.6955783367156982, Transition Loss -1.2194929122924805, Classifier Loss 0.09119889885187149, Total Loss 31.29287338256836\n",
      "7: Encoding Loss 7.665637016296387, Transition Loss -2.4604294300079346, Classifier Loss 0.09383415430784225, Total Loss 55.37625503540039\n",
      "7: Encoding Loss 4.16389274597168, Transition Loss -3.399014472961426, Classifier Loss 0.06665728241205215, Total Loss 31.64772605895996\n",
      "7: Encoding Loss 3.0620534420013428, Transition Loss -1.1727581024169922, Classifier Loss 0.10524609684944153, Total Loss 28.896461486816406\n",
      "7: Encoding Loss 9.886144638061523, Transition Loss 0.34303802251815796, Classifier Loss 0.1833970546722412, Total Loss 77.79379272460938\n",
      "7: Encoding Loss 6.7663984298706055, Transition Loss -0.14184261858463287, Classifier Loss 0.13518117368221283, Total Loss 54.116451263427734\n",
      "7: Encoding Loss 4.678581237792969, Transition Loss -2.609781503677368, Classifier Loss 0.0812886655330658, Total Loss 36.199310302734375\n",
      "7: Encoding Loss 4.0653228759765625, Transition Loss -0.67640221118927, Classifier Loss 0.08165408670902252, Total Loss 32.55707931518555\n",
      "7: Encoding Loss 7.1381940841674805, Transition Loss -2.527693033218384, Classifier Loss 0.20286741852760315, Total Loss 63.11489486694336\n",
      "7: Encoding Loss 4.769564151763916, Transition Loss -1.1396522521972656, Classifier Loss 0.07956475764513016, Total Loss 36.573402404785156\n",
      "7: Encoding Loss 4.419802188873291, Transition Loss -1.1336885690689087, Classifier Loss 0.07876092940568924, Total Loss 34.39445114135742\n",
      "7: Encoding Loss 4.209589958190918, Transition Loss -1.577542781829834, Classifier Loss 0.13599136471748352, Total Loss 38.856048583984375\n",
      "7: Encoding Loss 6.284163475036621, Transition Loss -0.6313580274581909, Classifier Loss 0.04935332015156746, Total Loss 42.64006423950195\n",
      "7: Encoding Loss 4.793195724487305, Transition Loss -1.4604454040527344, Classifier Loss 0.08388330042362213, Total Loss 37.14692306518555\n",
      "7: Encoding Loss 6.7797441482543945, Transition Loss -2.0260579586029053, Classifier Loss 0.10695625096559525, Total Loss 51.37328338623047\n",
      "7: Encoding Loss 5.646134853363037, Transition Loss -0.6108740568161011, Classifier Loss 0.08497664332389832, Total Loss 42.37423324584961\n",
      "7: Encoding Loss 5.674400329589844, Transition Loss -0.2906937599182129, Classifier Loss 0.14328205585479736, Total Loss 48.37449264526367\n",
      "7: Encoding Loss 7.397520542144775, Transition Loss -1.0507152080535889, Classifier Loss 0.0574520118534565, Total Loss 50.129905700683594\n",
      "7: Encoding Loss 5.884350299835205, Transition Loss -1.1153936386108398, Classifier Loss 0.18919779360294342, Total Loss 54.22543716430664\n",
      "7: Encoding Loss 5.076793193817139, Transition Loss -0.5486902594566345, Classifier Loss 0.11360402405261993, Total Loss 41.82094192504883\n",
      "7: Encoding Loss 3.335803508758545, Transition Loss -0.9913269281387329, Classifier Loss 0.07328560948371887, Total Loss 27.342987060546875\n",
      "7: Encoding Loss 5.235578536987305, Transition Loss -1.5660535097122192, Classifier Loss 0.08679892122745514, Total Loss 40.09273910522461\n",
      "7: Encoding Loss 5.121421813964844, Transition Loss -0.36659935116767883, Classifier Loss 0.09325841069221497, Total Loss 40.054229736328125\n",
      "7: Encoding Loss 4.5291008949279785, Transition Loss 0.34830573201179504, Classifier Loss 0.10618845373392105, Total Loss 37.93277359008789\n",
      "7: Encoding Loss 4.298731803894043, Transition Loss -1.758487343788147, Classifier Loss 0.07992890477180481, Total Loss 33.78458023071289\n",
      "7: Encoding Loss 3.150702953338623, Transition Loss -2.7334771156311035, Classifier Loss 0.08134636282920837, Total Loss 27.037761688232422\n",
      "7: Encoding Loss 6.198117256164551, Transition Loss -2.020712375640869, Classifier Loss 0.11500848829746246, Total Loss 48.688743591308594\n",
      "7: Encoding Loss 5.220391273498535, Transition Loss -2.183037042617798, Classifier Loss 0.1631343513727188, Total Loss 47.63490676879883\n",
      "7: Encoding Loss 5.342818737030029, Transition Loss -0.025364011526107788, Classifier Loss 0.10978202521800995, Total Loss 43.03510665893555\n",
      "7: Encoding Loss 5.249436855316162, Transition Loss -0.9775779247283936, Classifier Loss 0.13378503918647766, Total Loss 44.874732971191406\n",
      "7: Encoding Loss 6.288800239562988, Transition Loss -0.5612380504608154, Classifier Loss 0.16947561502456665, Total Loss 54.68014144897461\n",
      "7: Encoding Loss 5.406394004821777, Transition Loss -0.6136536002159119, Classifier Loss 0.13337725400924683, Total Loss 45.775848388671875\n",
      "7: Encoding Loss 5.0732340812683105, Transition Loss -1.150928258895874, Classifier Loss 0.09568072110414505, Total Loss 40.007015228271484\n",
      "7: Encoding Loss 5.314096450805664, Transition Loss -2.9322667121887207, Classifier Loss 0.08776897937059402, Total Loss 40.660308837890625\n",
      "7: Encoding Loss 4.583603858947754, Transition Loss -0.90708327293396, Classifier Loss 0.06282186508178711, Total Loss 33.783447265625\n",
      "7: Encoding Loss 5.093019962310791, Transition Loss -1.4994932413101196, Classifier Loss 0.11368988454341888, Total Loss 41.926509857177734\n",
      "7: Encoding Loss 5.719684600830078, Transition Loss -2.866154432296753, Classifier Loss 0.0595005638897419, Total Loss 40.26701736450195\n",
      "7: Encoding Loss 6.854432106018066, Transition Loss -0.8173805475234985, Classifier Loss 0.05945688858628273, Total Loss 47.071956634521484\n",
      "7: Encoding Loss 3.324237823486328, Transition Loss -0.7607795000076294, Classifier Loss 0.07454198598861694, Total Loss 27.399320602416992\n",
      "7: Encoding Loss 5.57699728012085, Transition Loss -1.4536768198013306, Classifier Loss 0.059498388320207596, Total Loss 39.41124725341797\n",
      "7: Encoding Loss 5.915609836578369, Transition Loss -1.2326397895812988, Classifier Loss 0.07203665375709534, Total Loss 42.696834564208984\n",
      "7: Encoding Loss 3.853106737136841, Transition Loss -0.5932968854904175, Classifier Loss 0.037067681550979614, Total Loss 26.825172424316406\n",
      "7: Encoding Loss 5.172807216644287, Transition Loss -2.7686004638671875, Classifier Loss 0.13580797612667084, Total Loss 44.61653518676758\n",
      "7: Encoding Loss 6.727959632873535, Transition Loss -0.29470294713974, Classifier Loss 0.10100321471691132, Total Loss 50.46796417236328\n",
      "7: Encoding Loss 6.661205291748047, Transition Loss -1.0062565803527832, Classifier Loss 0.1394498497247696, Total Loss 53.91181182861328\n",
      "7: Encoding Loss 5.4413580894470215, Transition Loss -1.4373271465301514, Classifier Loss 0.07468679547309875, Total Loss 40.11625289916992\n",
      "7: Encoding Loss 5.334053993225098, Transition Loss -1.5722788572311401, Classifier Loss 0.08915944397449493, Total Loss 40.919639587402344\n",
      "7: Encoding Loss 5.292357444763184, Transition Loss -0.7559212446212769, Classifier Loss 0.17534367740154266, Total Loss 49.288211822509766\n",
      "7: Encoding Loss 6.181225776672363, Transition Loss -1.677658200263977, Classifier Loss 0.07264728844165802, Total Loss 44.35141372680664\n",
      "7: Encoding Loss 3.403125286102295, Transition Loss -1.2218767404556274, Classifier Loss 0.14456358551979065, Total Loss 34.8746223449707\n",
      "7: Encoding Loss 8.555344581604004, Transition Loss 0.3823682367801666, Classifier Loss 0.13896985352039337, Total Loss 65.38200378417969\n",
      "7: Encoding Loss 5.002516269683838, Transition Loss -1.129153847694397, Classifier Loss 0.051234468817710876, Total Loss 35.13809585571289\n",
      "7: Encoding Loss 3.151421546936035, Transition Loss -2.3318777084350586, Classifier Loss 0.07236381620168686, Total Loss 26.143978118896484\n",
      "7: Encoding Loss 6.056042671203613, Transition Loss -0.20893536508083344, Classifier Loss 0.056873224675655365, Total Loss 42.02349853515625\n",
      "7: Encoding Loss 2.635129928588867, Transition Loss -0.9523400068283081, Classifier Loss 0.0465259812772274, Total Loss 20.462997436523438\n",
      "7: Encoding Loss 6.960679054260254, Transition Loss -1.752571940422058, Classifier Loss 0.1360621601343155, Total Loss 55.369590759277344\n",
      "7: Encoding Loss 6.584665775299072, Transition Loss -1.0436090230941772, Classifier Loss 0.11080896854400635, Total Loss 50.58847427368164\n",
      "7: Encoding Loss 5.626132011413574, Transition Loss -2.151745080947876, Classifier Loss 0.07742559164762497, Total Loss 41.49848937988281\n",
      "7: Encoding Loss 3.7871310710906982, Transition Loss -1.0825424194335938, Classifier Loss 0.12845741212368011, Total Loss 35.568092346191406\n",
      "7: Encoding Loss 4.515423774719238, Transition Loss -1.5334571599960327, Classifier Loss 0.09948351979255676, Total Loss 37.040279388427734\n",
      "7: Encoding Loss 5.503983974456787, Transition Loss -1.6019443273544312, Classifier Loss 0.08831488341093063, Total Loss 41.85475540161133\n",
      "7: Encoding Loss 3.8727023601531982, Transition Loss -0.7945461273193359, Classifier Loss 0.0949411615729332, Total Loss 32.73001480102539\n",
      "7: Encoding Loss 6.22125768661499, Transition Loss -2.1282076835632324, Classifier Loss 0.1865358054637909, Total Loss 55.98027801513672\n",
      "7: Encoding Loss 4.2565765380859375, Transition Loss -1.9941788911819458, Classifier Loss 0.07098138332366943, Total Loss 32.636802673339844\n",
      "7: Encoding Loss 3.448406219482422, Transition Loss -0.7215631008148193, Classifier Loss 0.07306239008903503, Total Loss 27.996389389038086\n",
      "7: Encoding Loss 5.535513877868652, Transition Loss 0.014097511768341064, Classifier Loss 0.14594817161560059, Total Loss 47.813541412353516\n",
      "7: Encoding Loss 4.625890731811523, Transition Loss -1.7441482543945312, Classifier Loss 0.08368737250566483, Total Loss 36.12338638305664\n",
      "7: Encoding Loss 7.916965484619141, Transition Loss -0.6504100561141968, Classifier Loss 0.1052495539188385, Total Loss 58.0264892578125\n",
      "7: Encoding Loss 4.0720343589782715, Transition Loss -2.570420026779175, Classifier Loss 0.201851949095726, Total Loss 44.616371154785156\n",
      "7: Encoding Loss 6.1146559715271, Transition Loss -1.1659892797470093, Classifier Loss 0.09479770809412003, Total Loss 46.16724395751953\n",
      "7: Encoding Loss 4.209824085235596, Transition Loss -1.2564818859100342, Classifier Loss 0.0504634827375412, Total Loss 30.304790496826172\n",
      "7: Encoding Loss 8.416450500488281, Transition Loss -0.26764190196990967, Classifier Loss 0.1523343175649643, Total Loss 65.7320327758789\n",
      "7: Encoding Loss 5.316439151763916, Transition Loss -1.2759534120559692, Classifier Loss 0.13461744785308838, Total Loss 45.35987091064453\n",
      "7: Encoding Loss 4.445317268371582, Transition Loss -1.0763089656829834, Classifier Loss 0.08661744743585587, Total Loss 35.33321762084961\n",
      "7: Encoding Loss 5.064464092254639, Transition Loss -2.791893482208252, Classifier Loss 0.09562599658966064, Total Loss 39.94826889038086\n",
      "7: Encoding Loss 3.6094672679901123, Transition Loss 0.6627700328826904, Classifier Loss 0.12468013912439346, Total Loss 34.389923095703125\n",
      "7: Encoding Loss 3.5289721488952637, Transition Loss -0.25849637389183044, Classifier Loss 0.051074497401714325, Total Loss 26.28118133544922\n",
      "7: Encoding Loss 4.463767051696777, Transition Loss -2.5330545902252197, Classifier Loss 0.15078961849212646, Total Loss 41.86054992675781\n",
      "7: Encoding Loss 5.687094688415527, Transition Loss -1.7829467058181763, Classifier Loss 0.14644832909107208, Total Loss 48.76668930053711\n",
      "7: Encoding Loss 4.296521186828613, Transition Loss -0.7010237574577332, Classifier Loss 0.09927116334438324, Total Loss 35.705963134765625\n",
      "7: Encoding Loss 4.575808048248291, Transition Loss -0.6122481226921082, Classifier Loss 0.07508223503828049, Total Loss 34.96282958984375\n",
      "7: Encoding Loss 4.368936061859131, Transition Loss -1.2040473222732544, Classifier Loss 0.04415905475616455, Total Loss 30.629039764404297\n",
      "7: Encoding Loss 6.921356201171875, Transition Loss -0.18262478709220886, Classifier Loss 0.25039929151535034, Total Loss 66.5679931640625\n",
      "7: Encoding Loss 4.733770847320557, Transition Loss -1.2814449071884155, Classifier Loss 0.07878785580396652, Total Loss 36.28089904785156\n",
      "7: Encoding Loss 6.919300079345703, Transition Loss -1.5806055068969727, Classifier Loss 0.08705652505159378, Total Loss 50.22081756591797\n",
      "7: Encoding Loss 4.247938632965088, Transition Loss -0.07156428694725037, Classifier Loss 0.06240462511777878, Total Loss 31.72806739807129\n",
      "7: Encoding Loss 5.879677772521973, Transition Loss -1.6532319784164429, Classifier Loss 0.03755071386694908, Total Loss 39.03247833251953\n",
      "7: Encoding Loss 5.154996395111084, Transition Loss -0.9038623571395874, Classifier Loss 0.09175261110067368, Total Loss 40.10487747192383\n",
      "7: Encoding Loss 5.099337577819824, Transition Loss -1.3711252212524414, Classifier Loss 0.21163944900035858, Total Loss 51.759422302246094\n",
      "7: Encoding Loss 8.330343246459961, Transition Loss -0.3517371416091919, Classifier Loss 0.22729675471782684, Total Loss 72.71160125732422\n",
      "7: Encoding Loss 6.5453033447265625, Transition Loss -1.3258507251739502, Classifier Loss 0.08655592799186707, Total Loss 47.926883697509766\n",
      "7: Encoding Loss 6.672649383544922, Transition Loss -1.3495142459869385, Classifier Loss 0.17134243249893188, Total Loss 57.16959762573242\n",
      "7: Encoding Loss 4.868818283081055, Transition Loss -1.8586809635162354, Classifier Loss 0.046051986515522, Total Loss 33.81736755371094\n",
      "7: Encoding Loss 5.555843353271484, Transition Loss -1.2055679559707642, Classifier Loss 0.08339975029230118, Total Loss 41.67455291748047\n",
      "7: Encoding Loss 6.704087734222412, Transition Loss -1.1057640314102173, Classifier Loss 0.10308461636304855, Total Loss 50.53254699707031\n",
      "7: Encoding Loss 5.122169494628906, Transition Loss -1.3861970901489258, Classifier Loss 0.12584014236927032, Total Loss 43.31647872924805\n",
      "7: Encoding Loss 3.4078280925750732, Transition Loss 0.38764387369155884, Classifier Loss 0.0456552729010582, Total Loss 25.16755485534668\n",
      "7: Encoding Loss 3.98528790473938, Transition Loss -0.6910072565078735, Classifier Loss 0.08868438005447388, Total Loss 32.77989196777344\n",
      "7: Encoding Loss 4.569977760314941, Transition Loss 0.20907950401306152, Classifier Loss 0.0780630111694336, Total Loss 35.30979919433594\n",
      "7: Encoding Loss 5.175856590270996, Transition Loss -1.4034535884857178, Classifier Loss 0.0676160678267479, Total Loss 37.81618881225586\n",
      "7: Encoding Loss 6.88815450668335, Transition Loss -0.35445892810821533, Classifier Loss 0.20088595151901245, Total Loss 61.41738510131836\n",
      "7: Encoding Loss 5.31464147567749, Transition Loss -1.8001794815063477, Classifier Loss 0.08127909153699875, Total Loss 40.015037536621094\n",
      "7: Encoding Loss 8.226916313171387, Transition Loss -2.7924792766571045, Classifier Loss 0.13059383630752563, Total Loss 62.41976547241211\n",
      "7: Encoding Loss 5.8887038230896, Transition Loss -1.6336783170700073, Classifier Loss 0.08650945872068405, Total Loss 43.98251724243164\n",
      "7: Encoding Loss 3.702500581741333, Transition Loss -1.1498796939849854, Classifier Loss 0.09371252357959747, Total Loss 31.585796356201172\n",
      "7: Encoding Loss 3.9565842151641846, Transition Loss -0.4163188934326172, Classifier Loss 0.023805126547813416, Total Loss 26.11985206604004\n",
      "7: Encoding Loss 6.5472822189331055, Transition Loss -3.0894570350646973, Classifier Loss 0.24748481810092926, Total Loss 64.03094482421875\n",
      "7: Encoding Loss 5.248978614807129, Transition Loss -1.438918113708496, Classifier Loss 0.14147941768169403, Total Loss 45.641239166259766\n",
      "7: Encoding Loss 3.847449779510498, Transition Loss 0.8445050120353699, Classifier Loss 0.05351299047470093, Total Loss 28.773801803588867\n",
      "7: Encoding Loss 5.454935550689697, Transition Loss -1.0092415809631348, Classifier Loss 0.23195448517799377, Total Loss 55.924659729003906\n",
      "7: Encoding Loss 4.212924003601074, Transition Loss -1.6547906398773193, Classifier Loss 0.12183767557144165, Total Loss 37.46064758300781\n",
      "7: Encoding Loss 6.339236259460449, Transition Loss -0.8033663034439087, Classifier Loss 0.18421222269535065, Total Loss 56.456321716308594\n",
      "7: Encoding Loss 3.317725658416748, Transition Loss -0.4019912779331207, Classifier Loss 0.05771265923976898, Total Loss 25.677459716796875\n",
      "7: Encoding Loss 3.0939652919769287, Transition Loss -0.6053294539451599, Classifier Loss 0.1633044183254242, Total Loss 34.89399337768555\n",
      "7: Encoding Loss 5.627997398376465, Transition Loss -2.310636520385742, Classifier Loss 0.11661205440759659, Total Loss 45.42826843261719\n",
      "7: Encoding Loss 4.768147945404053, Transition Loss -1.5714986324310303, Classifier Loss 0.1654171645641327, Total Loss 45.14997482299805\n",
      "7: Encoding Loss 5.601210594177246, Transition Loss -2.2594666481018066, Classifier Loss 0.13421005010604858, Total Loss 47.027366638183594\n",
      "7: Encoding Loss 5.662043571472168, Transition Loss -1.7578493356704712, Classifier Loss 0.05004774034023285, Total Loss 38.97633361816406\n",
      "7: Encoding Loss 5.454232215881348, Transition Loss -2.7515246868133545, Classifier Loss 0.11287329345941544, Total Loss 44.01162338256836\n",
      "7: Encoding Loss 7.1483354568481445, Transition Loss -1.129183292388916, Classifier Loss 0.155046284198761, Total Loss 58.39419174194336\n",
      "7: Encoding Loss 6.124452114105225, Transition Loss -1.7079792022705078, Classifier Loss 0.15418651700019836, Total Loss 52.1646842956543\n",
      "7: Encoding Loss 5.905512809753418, Transition Loss -0.8685078024864197, Classifier Loss 0.08029982447624207, Total Loss 43.46271514892578\n",
      "7: Encoding Loss 3.8884003162384033, Transition Loss -2.3022055625915527, Classifier Loss 0.08520758897066116, Total Loss 31.85024070739746\n",
      "7: Encoding Loss 6.686764717102051, Transition Loss -1.6034376621246338, Classifier Loss 0.10256393253803253, Total Loss 50.3763427734375\n",
      "7: Encoding Loss 4.5519819259643555, Transition Loss -2.2603158950805664, Classifier Loss 0.07908785343170166, Total Loss 35.21977615356445\n",
      "7: Encoding Loss 5.861678123474121, Transition Loss -1.8846442699432373, Classifier Loss 0.05191231518983841, Total Loss 40.36054611206055\n",
      "7: Encoding Loss 4.765515327453613, Transition Loss -1.0355441570281982, Classifier Loss 0.07099276036024094, Total Loss 35.691951751708984\n",
      "7: Encoding Loss 5.314077854156494, Transition Loss -1.6323515176773071, Classifier Loss 0.1329328864812851, Total Loss 45.17710494995117\n",
      "7: Encoding Loss 3.7571959495544434, Transition Loss 0.18176454305648804, Classifier Loss 0.058547016233205795, Total Loss 28.470584869384766\n",
      "7: Encoding Loss 8.522098541259766, Transition Loss -1.3246220350265503, Classifier Loss 0.17552493512630463, Total Loss 68.68456268310547\n",
      "7: Encoding Loss 7.977252006530762, Transition Loss -2.202993631362915, Classifier Loss 0.1588207185268402, Total Loss 63.74470520019531\n",
      "7: Encoding Loss 7.444003105163574, Transition Loss -1.1034859418869019, Classifier Loss 0.19222207367420197, Total Loss 63.88578796386719\n",
      "7: Encoding Loss 6.063345909118652, Transition Loss -0.2954670786857605, Classifier Loss 0.12310204654932022, Total Loss 48.690162658691406\n",
      "7: Encoding Loss 7.3454389572143555, Transition Loss -0.23960697650909424, Classifier Loss 0.2306116819381714, Total Loss 67.13370513916016\n",
      "7: Encoding Loss 4.563951015472412, Transition Loss 0.2283131182193756, Classifier Loss 0.05602547153830528, Total Loss 33.077579498291016\n",
      "7: Encoding Loss 3.7869210243225098, Transition Loss -0.9233161807060242, Classifier Loss 0.10671759396791458, Total Loss 33.39291763305664\n",
      "7: Encoding Loss 2.680161952972412, Transition Loss 0.39166784286499023, Classifier Loss 0.10567427426576614, Total Loss 26.80506706237793\n",
      "7: Encoding Loss 5.1592512130737305, Transition Loss -0.6849774718284607, Classifier Loss 0.13591331243515015, Total Loss 44.546566009521484\n",
      "7: Encoding Loss 5.942296028137207, Transition Loss -1.7638270854949951, Classifier Loss 0.1076975017786026, Total Loss 46.42282485961914\n",
      "7: Encoding Loss 7.068735599517822, Transition Loss -1.6988952159881592, Classifier Loss 0.19393974542617798, Total Loss 61.80570983886719\n",
      "7: Encoding Loss 4.223689079284668, Transition Loss -1.731999397277832, Classifier Loss 0.07657124102115631, Total Loss 32.998565673828125\n",
      "7: Encoding Loss 6.374146461486816, Transition Loss -2.3676772117614746, Classifier Loss 0.18041539192199707, Total Loss 56.28547286987305\n",
      "7: Encoding Loss 4.71895170211792, Transition Loss -1.13807213306427, Classifier Loss 0.0808015912771225, Total Loss 36.39341735839844\n",
      "7: Encoding Loss 4.764889717102051, Transition Loss -1.594175100326538, Classifier Loss 0.0986894965171814, Total Loss 38.4576530456543\n",
      "7: Encoding Loss 4.57808780670166, Transition Loss -3.1935875415802, Classifier Loss 0.08443395793437958, Total Loss 35.91064453125\n",
      "7: Encoding Loss 2.6082587242126465, Transition Loss -1.8492697477340698, Classifier Loss 0.07207341492176056, Total Loss 22.856155395507812\n",
      "7: Encoding Loss 5.820676326751709, Transition Loss -1.739174246788025, Classifier Loss 0.09549251943826675, Total Loss 44.472618103027344\n",
      "7: Encoding Loss 5.283679008483887, Transition Loss -1.9616413116455078, Classifier Loss 0.1611633598804474, Total Loss 47.817626953125\n",
      "7: Encoding Loss 3.1653716564178467, Transition Loss -1.6810213327407837, Classifier Loss 0.07523557543754578, Total Loss 26.51511573791504\n",
      "7: Encoding Loss 6.816501617431641, Transition Loss -2.3132264614105225, Classifier Loss 0.09101534634828568, Total Loss 49.99961853027344\n",
      "7: Encoding Loss 4.486105918884277, Transition Loss -0.9934749007225037, Classifier Loss 0.07972904294729233, Total Loss 34.88914489746094\n",
      "7: Encoding Loss 6.399603366851807, Transition Loss -1.150588035583496, Classifier Loss 0.10590514540672302, Total Loss 48.987674713134766\n",
      "7: Encoding Loss 6.952718257904053, Transition Loss -1.5418444871902466, Classifier Loss 0.15200302004814148, Total Loss 56.91599655151367\n",
      "7: Encoding Loss 6.475379943847656, Transition Loss -0.8876993656158447, Classifier Loss 0.10719764977693558, Total Loss 49.57168960571289\n",
      "7: Encoding Loss 4.787659168243408, Transition Loss -1.582008957862854, Classifier Loss 0.11485468596220016, Total Loss 40.210792541503906\n",
      "7: Encoding Loss 5.990926742553711, Transition Loss -1.8279107809066772, Classifier Loss 0.10190607607364655, Total Loss 46.13543701171875\n",
      "7: Encoding Loss 5.177456855773926, Transition Loss -1.6398552656173706, Classifier Loss 0.0774843692779541, Total Loss 38.812522888183594\n",
      "7: Encoding Loss 2.7433454990386963, Transition Loss -0.14273619651794434, Classifier Loss 0.04452813044190407, Total Loss 20.912830352783203\n",
      "7: Encoding Loss 7.3215742111206055, Transition Loss -0.22370880842208862, Classifier Loss 0.08574990183115005, Total Loss 52.50434875488281\n",
      "7: Encoding Loss 6.553857326507568, Transition Loss -1.3165830373764038, Classifier Loss 0.07798900455236435, Total Loss 47.12152099609375\n",
      "7: Encoding Loss 6.583564758300781, Transition Loss -1.0048011541366577, Classifier Loss 0.04098676145076752, Total Loss 43.59966278076172\n",
      "7: Encoding Loss 4.154633522033691, Transition Loss -0.07957179844379425, Classifier Loss 0.06160518527030945, Total Loss 31.088287353515625\n",
      "7: Encoding Loss 4.14679479598999, Transition Loss -2.1370811462402344, Classifier Loss 0.04004770517349243, Total Loss 28.884685516357422\n",
      "7: Encoding Loss 3.816524028778076, Transition Loss -1.696833610534668, Classifier Loss 0.1342715620994568, Total Loss 36.32562255859375\n",
      "7: Encoding Loss 5.5338568687438965, Transition Loss -0.41307973861694336, Classifier Loss 0.054887235164642334, Total Loss 38.69170379638672\n",
      "7: Encoding Loss 4.570725440979004, Transition Loss -1.116844892501831, Classifier Loss 0.048371460288763046, Total Loss 32.261051177978516\n",
      "7: Encoding Loss 4.355744361877441, Transition Loss -0.5668279528617859, Classifier Loss 0.049080826342105865, Total Loss 31.042322158813477\n",
      "7: Encoding Loss 3.6853885650634766, Transition Loss -0.8980765342712402, Classifier Loss 0.07677437365055084, Total Loss 29.789409637451172\n",
      "7: Encoding Loss 6.19685697555542, Transition Loss -1.4391555786132812, Classifier Loss 0.07917279750108719, Total Loss 45.09784698486328\n",
      "7: Encoding Loss 4.434613227844238, Transition Loss -1.870267391204834, Classifier Loss 0.08096945285797119, Total Loss 34.70387649536133\n",
      "7: Encoding Loss 3.945596694946289, Transition Loss -2.4919254779815674, Classifier Loss 0.05470969155430794, Total Loss 29.143552780151367\n",
      "7: Encoding Loss 3.713531017303467, Transition Loss -1.484278678894043, Classifier Loss 0.1116468608379364, Total Loss 33.44527816772461\n",
      "7: Encoding Loss 6.311070442199707, Transition Loss -0.8000086545944214, Classifier Loss 0.10593903809785843, Total Loss 48.46000671386719\n",
      "7: Encoding Loss 3.6368508338928223, Transition Loss -0.8845452070236206, Classifier Loss 0.060034118592739105, Total Loss 27.82416343688965\n",
      "7: Encoding Loss 4.399118900299072, Transition Loss -1.170190691947937, Classifier Loss 0.1079060435295105, Total Loss 37.18484878540039\n",
      "7: Encoding Loss 4.804786205291748, Transition Loss -2.842426300048828, Classifier Loss 0.09070926904678345, Total Loss 37.89850616455078\n",
      "7: Encoding Loss 4.084515571594238, Transition Loss -1.1245478391647339, Classifier Loss 0.04307422786951065, Total Loss 28.81406593322754\n",
      "7: Encoding Loss 3.4540748596191406, Transition Loss -0.7556520700454712, Classifier Loss 0.06310711801052094, Total Loss 27.03485870361328\n",
      "7: Encoding Loss 4.62399435043335, Transition Loss -1.7459955215454102, Classifier Loss 0.16312383115291595, Total Loss 44.0556526184082\n",
      "7: Encoding Loss 3.4069199562072754, Transition Loss -2.34417724609375, Classifier Loss 0.08886105567216873, Total Loss 29.32668685913086\n",
      "7: Encoding Loss 3.6460788249969482, Transition Loss -0.9293781518936157, Classifier Loss 0.08785533159971237, Total Loss 30.66163444519043\n",
      "7: Encoding Loss 4.703312873840332, Transition Loss -2.4054160118103027, Classifier Loss 0.07802436500787735, Total Loss 36.0213508605957\n",
      "7: Encoding Loss 5.656044960021973, Transition Loss -1.0450234413146973, Classifier Loss 0.1655350923538208, Total Loss 50.48936080932617\n",
      "7: Encoding Loss 5.49924898147583, Transition Loss -1.5964246988296509, Classifier Loss 0.11328868567943573, Total Loss 44.323726654052734\n",
      "7: Encoding Loss 3.459453582763672, Transition Loss -2.1319241523742676, Classifier Loss 0.0886378213763237, Total Loss 29.61964988708496\n",
      "7: Encoding Loss 6.177518844604492, Transition Loss -1.1988303661346436, Classifier Loss 0.18822434544563293, Total Loss 55.88706970214844\n",
      "7: Encoding Loss 7.168137550354004, Transition Loss -1.9717981815338135, Classifier Loss 0.24423420429229736, Total Loss 67.43146514892578\n",
      "7: Encoding Loss 7.511180877685547, Transition Loss -1.878682017326355, Classifier Loss 0.1399919092655182, Total Loss 59.06552505493164\n",
      "7: Encoding Loss 7.280457496643066, Transition Loss -2.315183639526367, Classifier Loss 0.13144075870513916, Total Loss 56.82589340209961\n",
      "7: Encoding Loss 4.449429988861084, Transition Loss -2.9158430099487305, Classifier Loss 0.08641055971384048, Total Loss 35.33647155761719\n",
      "7: Encoding Loss 7.469794273376465, Transition Loss -2.4324283599853516, Classifier Loss 0.09109630435705185, Total Loss 53.927425384521484\n",
      "7: Encoding Loss 7.8073248863220215, Transition Loss -0.24702191352844238, Classifier Loss 0.13480156660079956, Total Loss 60.32400894165039\n",
      "7: Encoding Loss 6.863492488861084, Transition Loss -2.571662664413452, Classifier Loss 0.2427564561367035, Total Loss 65.45557403564453\n",
      "7: Encoding Loss 7.516100883483887, Transition Loss -1.9782499074935913, Classifier Loss 0.16109512746334076, Total Loss 61.20532989501953\n",
      "7: Encoding Loss 6.687464237213135, Transition Loss 0.3763881325721741, Classifier Loss 0.06608787178993225, Total Loss 46.88412857055664\n",
      "7: Encoding Loss 5.551323890686035, Transition Loss -1.6859039068222046, Classifier Loss 0.03956093639135361, Total Loss 37.263362884521484\n",
      "7: Encoding Loss 2.875260591506958, Transition Loss -1.3320086002349854, Classifier Loss 0.10265886783599854, Total Loss 27.516918182373047\n",
      "7: Encoding Loss 4.433249473571777, Transition Loss -0.7668214440345764, Classifier Loss 0.08785288780927658, Total Loss 35.384483337402344\n",
      "7: Encoding Loss 4.053744792938232, Transition Loss 0.5448657274246216, Classifier Loss 0.07081189751625061, Total Loss 31.621606826782227\n",
      "7: Encoding Loss 2.2285280227661133, Transition Loss -1.239159107208252, Classifier Loss 0.04164883866906166, Total Loss 17.53555679321289\n",
      "7: Encoding Loss 4.299088001251221, Transition Loss -1.0041495561599731, Classifier Loss 0.16591672599315643, Total Loss 42.385799407958984\n",
      "7: Encoding Loss 3.9403762817382812, Transition Loss -0.922086238861084, Classifier Loss 0.149025559425354, Total Loss 38.5444450378418\n",
      "7: Encoding Loss 6.819960594177246, Transition Loss -0.20339757204055786, Classifier Loss 0.06343021243810654, Total Loss 47.2627067565918\n",
      "7: Encoding Loss 5.391592979431152, Transition Loss -2.326531410217285, Classifier Loss 0.0810849592089653, Total Loss 40.457122802734375\n",
      "7: Encoding Loss 8.076093673706055, Transition Loss -1.2847061157226562, Classifier Loss 0.2934109568595886, Total Loss 77.79714965820312\n",
      "7: Encoding Loss 5.3134942054748535, Transition Loss -2.475064516067505, Classifier Loss 0.14729264378547668, Total Loss 46.60923767089844\n",
      "7: Encoding Loss 4.719435691833496, Transition Loss -1.4571053981781006, Classifier Loss 0.05596912279725075, Total Loss 33.91294479370117\n",
      "7: Encoding Loss 6.237115383148193, Transition Loss -1.7960119247436523, Classifier Loss 0.16229279339313507, Total Loss 53.6512565612793\n",
      "7: Encoding Loss 6.817079067230225, Transition Loss -1.7355302572250366, Classifier Loss 0.11740825325250626, Total Loss 52.642608642578125\n",
      "7: Encoding Loss 5.18118143081665, Transition Loss 0.10220390558242798, Classifier Loss 0.1369083672761917, Total Loss 44.818809509277344\n",
      "7: Encoding Loss 4.132037162780762, Transition Loss 0.6627234220504761, Classifier Loss 0.08700884878635406, Total Loss 33.758201599121094\n",
      "7: Encoding Loss 5.112326145172119, Transition Loss -2.152195453643799, Classifier Loss 0.08834592252969742, Total Loss 39.507686614990234\n",
      "7: Encoding Loss 6.877641201019287, Transition Loss -1.5850622653961182, Classifier Loss 0.09757938981056213, Total Loss 51.023155212402344\n",
      "7: Encoding Loss 5.297411918640137, Transition Loss -0.3151908218860626, Classifier Loss 0.13837403059005737, Total Loss 45.62174987792969\n",
      "7: Encoding Loss 3.323514461517334, Transition Loss -0.44494491815567017, Classifier Loss 0.04754653200507164, Total Loss 24.6955623626709\n",
      "7: Encoding Loss 4.847934722900391, Transition Loss -1.9837086200714111, Classifier Loss 0.13724955916404724, Total Loss 42.811771392822266\n",
      "7: Encoding Loss 3.7947967052459717, Transition Loss 0.7891730070114136, Classifier Loss 0.10501938313245773, Total Loss 33.58639144897461\n",
      "7: Encoding Loss 6.610818386077881, Transition Loss -1.8851958513259888, Classifier Loss 0.10602827370166779, Total Loss 50.26698303222656\n",
      "7: Encoding Loss 3.958373546600342, Transition Loss -2.3329715728759766, Classifier Loss 0.08535238355398178, Total Loss 32.2845458984375\n",
      "7: Encoding Loss 6.274346351623535, Transition Loss -1.2929970026016235, Classifier Loss 0.12272194027900696, Total Loss 49.917755126953125\n",
      "7: Encoding Loss 4.586573600769043, Transition Loss -2.14902400970459, Classifier Loss 0.06932240724563599, Total Loss 34.45082473754883\n",
      "7: Encoding Loss 5.259823799133301, Transition Loss -1.4595569372177124, Classifier Loss 0.11846597492694855, Total Loss 43.40495681762695\n",
      "7: Encoding Loss 5.640682220458984, Transition Loss -2.1283230781555176, Classifier Loss 0.11213990300893784, Total Loss 45.05723190307617\n",
      "7: Encoding Loss 4.226251602172852, Transition Loss 0.8422431349754333, Classifier Loss 0.09955432265996933, Total Loss 35.64984130859375\n",
      "7: Encoding Loss 1.9880796670913696, Transition Loss -0.805978536605835, Classifier Loss 0.07737977057695389, Total Loss 19.6661319732666\n",
      "7: Encoding Loss 10.182069778442383, Transition Loss -2.0168323516845703, Classifier Loss 0.11600061506032944, Total Loss 72.6916732788086\n",
      "7: Encoding Loss 6.8398895263671875, Transition Loss -1.4691739082336426, Classifier Loss 0.09206510335206985, Total Loss 50.245262145996094\n",
      "7: Encoding Loss 5.346970558166504, Transition Loss -0.9340217709541321, Classifier Loss 0.10720809549093246, Total Loss 42.80226135253906\n",
      "7: Encoding Loss 4.767187595367432, Transition Loss -1.2325220108032227, Classifier Loss 0.06446085125207901, Total Loss 35.0487174987793\n",
      "7: Encoding Loss 3.519937038421631, Transition Loss -1.1329890489578247, Classifier Loss 0.13458696007728577, Total Loss 34.57786560058594\n",
      "7: Encoding Loss 5.051772117614746, Transition Loss -1.7320289611816406, Classifier Loss 0.15630695223808289, Total Loss 45.940635681152344\n",
      "7: Encoding Loss 3.507561206817627, Transition Loss 0.18276146054267883, Classifier Loss 0.05801299214363098, Total Loss 26.91977310180664\n",
      "7: Encoding Loss 6.2309160232543945, Transition Loss -2.328760862350464, Classifier Loss 0.0675593763589859, Total Loss 44.1405029296875\n",
      "7: Encoding Loss 5.536806106567383, Transition Loss -1.1226996183395386, Classifier Loss 0.039901405572891235, Total Loss 37.21052551269531\n",
      "7: Encoding Loss 5.212432861328125, Transition Loss -1.961322546005249, Classifier Loss 0.08341474831104279, Total Loss 39.61528778076172\n",
      "7: Encoding Loss 4.078799247741699, Transition Loss -0.9380808472633362, Classifier Loss 0.04028529301285744, Total Loss 28.500951766967773\n",
      "7: Encoding Loss 4.313976287841797, Transition Loss 0.15266796946525574, Classifier Loss 0.14907926321029663, Total Loss 40.85285186767578\n",
      "7: Encoding Loss 5.061100959777832, Transition Loss -1.0900784730911255, Classifier Loss 0.06359174847602844, Total Loss 36.725345611572266\n",
      "7: Encoding Loss 3.6490485668182373, Transition Loss -2.623363494873047, Classifier Loss 0.10770317912101746, Total Loss 32.6635627746582\n",
      "7: Encoding Loss 5.330562591552734, Transition Loss -1.6328272819519043, Classifier Loss 0.20909497141838074, Total Loss 52.8922233581543\n",
      "7: Encoding Loss 5.0717668533325195, Transition Loss -1.2930465936660767, Classifier Loss 0.10609081387519836, Total Loss 41.039161682128906\n",
      "7: Encoding Loss 7.621377468109131, Transition Loss -1.2832919359207153, Classifier Loss 0.10346230864524841, Total Loss 56.07398223876953\n",
      "7: Encoding Loss 4.846699237823486, Transition Loss -0.43511492013931274, Classifier Loss 0.09721598029136658, Total Loss 38.80162048339844\n",
      "7: Encoding Loss 4.242302894592285, Transition Loss 0.13725443184375763, Classifier Loss 0.10986284911632538, Total Loss 36.49500274658203\n",
      "7: Encoding Loss 7.05251407623291, Transition Loss -1.458115577697754, Classifier Loss 0.09456774592399597, Total Loss 51.771278381347656\n",
      "7: Encoding Loss 5.436680793762207, Transition Loss -2.2834815979003906, Classifier Loss 0.1467490792274475, Total Loss 47.29408264160156\n",
      "7: Encoding Loss 7.210656642913818, Transition Loss -2.096909284591675, Classifier Loss 0.12283261120319366, Total Loss 55.546363830566406\n",
      "7: Encoding Loss 5.375429630279541, Transition Loss -2.195387840270996, Classifier Loss 0.0639791190624237, Total Loss 38.64961242675781\n",
      "7: Encoding Loss 3.8114326000213623, Transition Loss -2.2923688888549805, Classifier Loss 0.13209490478038788, Total Loss 36.077171325683594\n",
      "7: Encoding Loss 6.699516296386719, Transition Loss -1.111572265625, Classifier Loss 0.09686248004436493, Total Loss 49.88290023803711\n",
      "7: Encoding Loss 8.512831687927246, Transition Loss -1.4190258979797363, Classifier Loss 0.15238720178604126, Total Loss 66.31514739990234\n",
      "7: Encoding Loss 5.839518070220947, Transition Loss -1.4957711696624756, Classifier Loss 0.05609431490302086, Total Loss 40.64594268798828\n",
      "7: Encoding Loss 4.317554950714111, Transition Loss -0.24623127281665802, Classifier Loss 0.06102832406759262, Total Loss 32.00806427001953\n",
      "7: Encoding Loss 3.563286542892456, Transition Loss -1.0489014387130737, Classifier Loss 0.09339618682861328, Total Loss 30.71891975402832\n",
      "7: Encoding Loss 5.763373851776123, Transition Loss -1.478200912475586, Classifier Loss 0.07413745671510696, Total Loss 41.99340057373047\n",
      "7: Encoding Loss 4.175050735473633, Transition Loss -1.0867135524749756, Classifier Loss 0.04269494488835335, Total Loss 29.319366455078125\n",
      "7: Encoding Loss 6.183806419372559, Transition Loss -2.068871021270752, Classifier Loss 0.05205059051513672, Total Loss 42.307071685791016\n",
      "7: Encoding Loss 3.4237396717071533, Transition Loss -1.9836325645446777, Classifier Loss 0.09088580310344696, Total Loss 29.630226135253906\n",
      "7: Encoding Loss 8.092610359191895, Transition Loss 0.11651092767715454, Classifier Loss 0.26073798537254333, Total Loss 74.67607116699219\n",
      "7: Encoding Loss 5.253439903259277, Transition Loss -1.1462898254394531, Classifier Loss 0.1251024305820465, Total Loss 44.030426025390625\n",
      "7: Encoding Loss 6.133359432220459, Transition Loss -1.2820805311203003, Classifier Loss 0.09944625198841095, Total Loss 46.7442741394043\n",
      "7: Encoding Loss 5.79993200302124, Transition Loss 0.13718390464782715, Classifier Loss 0.09968853741884232, Total Loss 44.82332229614258\n",
      "7: Encoding Loss 6.541196346282959, Transition Loss -0.562211811542511, Classifier Loss 0.21053262054920197, Total Loss 60.30022048950195\n",
      "7: Encoding Loss 6.09591007232666, Transition Loss -1.3989782333374023, Classifier Loss 0.17359134554862976, Total Loss 53.93403625488281\n",
      "7: Encoding Loss 4.377707481384277, Transition Loss -0.5939202308654785, Classifier Loss 0.08966761827468872, Total Loss 35.23277282714844\n",
      "7: Encoding Loss 4.612973213195801, Transition Loss -2.1558313369750977, Classifier Loss 0.13478229939937592, Total Loss 41.15520477294922\n",
      "7: Encoding Loss 5.039918422698975, Transition Loss -0.7285865545272827, Classifier Loss 0.1250133365392685, Total Loss 42.74055480957031\n",
      "7: Encoding Loss 5.922664642333984, Transition Loss -0.3136255145072937, Classifier Loss 0.11216307431459427, Total Loss 46.75217056274414\n",
      "7: Encoding Loss 3.2986724376678467, Transition Loss -0.9532627463340759, Classifier Loss 0.06028857082128525, Total Loss 25.820510864257812\n",
      "7: Encoding Loss 3.834080457687378, Transition Loss -0.9093868732452393, Classifier Loss 0.10866084694862366, Total Loss 33.87020492553711\n",
      "7: Encoding Loss 3.6833393573760986, Transition Loss -1.8564090728759766, Classifier Loss 0.08863912522792816, Total Loss 30.963207244873047\n",
      "7: Encoding Loss 4.922574520111084, Transition Loss -0.7192241549491882, Classifier Loss 0.051807843148708344, Total Loss 34.715946197509766\n",
      "7: Encoding Loss 5.125929355621338, Transition Loss -1.0811705589294434, Classifier Loss 0.1295904964208603, Total Loss 43.714195251464844\n",
      "7: Encoding Loss 3.957845687866211, Transition Loss -2.277918815612793, Classifier Loss 0.0927106961607933, Total Loss 33.01723098754883\n",
      "7: Encoding Loss 4.945681571960449, Transition Loss -1.3739649057388306, Classifier Loss 0.13472694158554077, Total Loss 43.146236419677734\n",
      "7: Encoding Loss 3.8391270637512207, Transition Loss -0.1815999150276184, Classifier Loss 0.043084125965833664, Total Loss 27.343103408813477\n",
      "7: Encoding Loss 3.4529528617858887, Transition Loss -0.311777263879776, Classifier Loss 0.06243973225355148, Total Loss 26.961566925048828\n",
      "7: Encoding Loss 4.875287055969238, Transition Loss -0.5673012137413025, Classifier Loss 0.21141915023326874, Total Loss 50.39341354370117\n",
      "7: Encoding Loss 5.87343692779541, Transition Loss -1.6823780536651611, Classifier Loss 0.20186947286128998, Total Loss 55.42689895629883\n",
      "7: Encoding Loss 4.044111251831055, Transition Loss -0.5965532064437866, Classifier Loss 0.13558506965637207, Total Loss 37.82293701171875\n",
      "7: Encoding Loss 6.264971733093262, Transition Loss -1.6774628162384033, Classifier Loss 0.1484648734331131, Total Loss 52.435646057128906\n",
      "7: Encoding Loss 3.9573700428009033, Transition Loss -2.062246084213257, Classifier Loss 0.08244197815656662, Total Loss 31.987594604492188\n",
      "7: Encoding Loss 5.706838130950928, Transition Loss -1.6817293167114258, Classifier Loss 0.12765911221504211, Total Loss 47.00627136230469\n",
      "7: Encoding Loss 5.266242980957031, Transition Loss -1.0354843139648438, Classifier Loss 0.05247805640101433, Total Loss 36.8448486328125\n",
      "7: Encoding Loss 3.1712021827697754, Transition Loss -1.2699410915374756, Classifier Loss 0.10736428201198578, Total Loss 29.763134002685547\n",
      "7: Encoding Loss 3.764453411102295, Transition Loss -2.236708402633667, Classifier Loss 0.0668923631310463, Total Loss 29.275062561035156\n",
      "7: Encoding Loss 3.800130844116211, Transition Loss -0.26765286922454834, Classifier Loss 0.15549319982528687, Total Loss 38.349998474121094\n",
      "7: Encoding Loss 6.806855201721191, Transition Loss -0.7937435507774353, Classifier Loss 0.06522271782159805, Total Loss 47.36308670043945\n",
      "7: Encoding Loss 5.6245927810668945, Transition Loss -1.4321584701538086, Classifier Loss 0.23607559502124786, Total Loss 57.35454559326172\n",
      "7: Encoding Loss 6.408461570739746, Transition Loss -0.24250486493110657, Classifier Loss 0.14536599814891815, Total Loss 52.98727798461914\n",
      "7: Encoding Loss 5.467907428741455, Transition Loss -2.2713489532470703, Classifier Loss 0.15231281518936157, Total Loss 48.037818908691406\n",
      "7: Encoding Loss 4.885193824768066, Transition Loss -0.9229216575622559, Classifier Loss 0.06349550187587738, Total Loss 35.660343170166016\n",
      "7: Encoding Loss 2.941509485244751, Transition Loss -2.3637290000915527, Classifier Loss 0.06682922691106796, Total Loss 24.33103370666504\n",
      "7: Encoding Loss 2.8058226108551025, Transition Loss -1.2901593446731567, Classifier Loss 0.07368224114179611, Total Loss 24.2026424407959\n",
      "7: Encoding Loss 3.78715181350708, Transition Loss -2.6160411834716797, Classifier Loss 0.07199431955814362, Total Loss 29.921297073364258\n",
      "7: Encoding Loss 6.290518760681152, Transition Loss 0.12782007455825806, Classifier Loss 0.046768154948949814, Total Loss 42.4710578918457\n",
      "7: Encoding Loss 5.262564659118652, Transition Loss -0.30413979291915894, Classifier Loss 0.11538195610046387, Total Loss 43.11346435546875\n",
      "7: Encoding Loss 6.610428810119629, Transition Loss -3.4435033798217773, Classifier Loss 0.07447691261768341, Total Loss 47.108890533447266\n",
      "7: Encoding Loss 5.494232177734375, Transition Loss -1.9315087795257568, Classifier Loss 0.10237079858779907, Total Loss 43.201698303222656\n",
      "7: Encoding Loss 4.259159088134766, Transition Loss 0.02436840534210205, Classifier Loss 0.1090533435344696, Total Loss 36.470035552978516\n",
      "7: Encoding Loss 4.374736785888672, Transition Loss -2.3285534381866455, Classifier Loss 0.04552491009235382, Total Loss 30.799983978271484\n",
      "7: Encoding Loss 7.412962913513184, Transition Loss -1.0570727586746216, Classifier Loss 0.12616164982318878, Total Loss 57.09352111816406\n",
      "7: Encoding Loss 6.122865676879883, Transition Loss -2.7374110221862793, Classifier Loss 0.17316024005413055, Total Loss 54.0521240234375\n",
      "7: Encoding Loss 5.7278337478637695, Transition Loss -1.718099594116211, Classifier Loss 0.101193867623806, Total Loss 44.48570251464844\n",
      "7: Encoding Loss 6.154722213745117, Transition Loss -0.5952275991439819, Classifier Loss 0.16899338364601135, Total Loss 53.82743453979492\n",
      "7: Encoding Loss 6.221109867095947, Transition Loss -1.7192906141281128, Classifier Loss 0.031488943845033646, Total Loss 40.47486877441406\n",
      "7: Encoding Loss 5.1393890380859375, Transition Loss -1.6013100147247314, Classifier Loss 0.1854788064956665, Total Loss 49.383575439453125\n",
      "7: Encoding Loss 4.097784519195557, Transition Loss -0.6903116703033447, Classifier Loss 0.07695920765399933, Total Loss 32.28235626220703\n",
      "7: Encoding Loss 6.669577121734619, Transition Loss -2.0214762687683105, Classifier Loss 0.11853385716676712, Total Loss 51.87004089355469\n",
      "7: Encoding Loss 5.703200817108154, Transition Loss -0.6787351369857788, Classifier Loss 0.11060064285993576, Total Loss 45.27900314331055\n",
      "7: Encoding Loss 4.833035469055176, Transition Loss -0.5693633556365967, Classifier Loss 0.08065953105688095, Total Loss 37.06393814086914\n",
      "7: Encoding Loss 4.440297603607178, Transition Loss -1.1531314849853516, Classifier Loss 0.12088551372289658, Total Loss 38.72987747192383\n",
      "7: Encoding Loss 4.818454265594482, Transition Loss -0.7768323421478271, Classifier Loss 0.1468788981437683, Total Loss 43.59830856323242\n",
      "7: Encoding Loss 5.59232234954834, Transition Loss -0.4806368947029114, Classifier Loss 0.12893354892730713, Total Loss 46.44709777832031\n",
      "7: Encoding Loss 5.201777458190918, Transition Loss -2.9756150245666504, Classifier Loss 0.0645952895283699, Total Loss 37.66900634765625\n",
      "7: Encoding Loss 2.555793523788452, Transition Loss -1.7931904792785645, Classifier Loss 0.09846203774213791, Total Loss 25.180248260498047\n",
      "7: Encoding Loss 9.971988677978516, Transition Loss -2.2367663383483887, Classifier Loss 0.16071197390556335, Total Loss 75.9022445678711\n",
      "7: Encoding Loss 6.695883750915527, Transition Loss -0.1276320517063141, Classifier Loss 0.04235977679491043, Total Loss 44.411231994628906\n",
      "7: Encoding Loss 5.655996799468994, Transition Loss -0.8419820070266724, Classifier Loss 0.18226514756679535, Total Loss 52.16216278076172\n",
      "7: Encoding Loss 5.083664894104004, Transition Loss -1.0544077157974243, Classifier Loss 0.1504276543855667, Total Loss 45.54433059692383\n",
      "7: Encoding Loss 6.378852367401123, Transition Loss -0.926895022392273, Classifier Loss 0.0653802677989006, Total Loss 44.81077194213867\n",
      "7: Encoding Loss 6.9397735595703125, Transition Loss -1.58012056350708, Classifier Loss 0.2664594352245331, Total Loss 68.28395080566406\n",
      "7: Encoding Loss 4.867250919342041, Transition Loss -0.31791988015174866, Classifier Loss 0.09211388230323792, Total Loss 38.41476821899414\n",
      "7: Encoding Loss 5.186922073364258, Transition Loss 0.4601541757583618, Classifier Loss 0.11929246783256531, Total Loss 43.23484420776367\n",
      "7: Encoding Loss 5.43782901763916, Transition Loss -0.5782660841941833, Classifier Loss 0.09105802327394485, Total Loss 41.732547760009766\n",
      "7: Encoding Loss 3.990788698196411, Transition Loss 0.24691754579544067, Classifier Loss 0.030741773545742035, Total Loss 27.11767578125\n",
      "7: Encoding Loss 6.589771270751953, Transition Loss -1.2984912395477295, Classifier Loss 0.0900610014796257, Total Loss 48.54420852661133\n",
      "7: Encoding Loss 7.148694038391113, Transition Loss -1.7856428623199463, Classifier Loss 0.20376187562942505, Total Loss 63.267642974853516\n",
      "7: Encoding Loss 4.580594539642334, Transition Loss -1.045077919960022, Classifier Loss 0.08158587664365768, Total Loss 35.6417350769043\n",
      "7: Encoding Loss 3.761671781539917, Transition Loss -1.9317445755004883, Classifier Loss 0.0750131830573082, Total Loss 30.07057762145996\n",
      "7: Encoding Loss 4.205387592315674, Transition Loss -1.2459628582000732, Classifier Loss 0.08199743926525116, Total Loss 33.43157196044922\n",
      "7: Encoding Loss 4.884436130523682, Transition Loss -0.4009903073310852, Classifier Loss 0.052166979759931564, Total Loss 34.523155212402344\n",
      "7: Encoding Loss 5.717827796936035, Transition Loss -1.0026311874389648, Classifier Loss 0.17226767539978027, Total Loss 51.5333366394043\n",
      "7: Encoding Loss 3.3046352863311768, Transition Loss -0.32319405674934387, Classifier Loss 0.1392120122909546, Total Loss 33.74888610839844\n",
      "7: Encoding Loss 5.697108268737793, Transition Loss -2.085980176925659, Classifier Loss 0.08968475461006165, Total Loss 43.150291442871094\n",
      "7: Encoding Loss 3.7280635833740234, Transition Loss -0.997117817401886, Classifier Loss 0.05078091099858284, Total Loss 27.446073532104492\n",
      "7: Encoding Loss 4.691153526306152, Transition Loss -2.726876974105835, Classifier Loss 0.046213649213314056, Total Loss 32.76719665527344\n",
      "7: Encoding Loss 2.2699320316314697, Transition Loss -1.5080103874206543, Classifier Loss 0.04200788959860802, Total Loss 17.819778442382812\n",
      "7: Encoding Loss 6.184438228607178, Transition Loss -1.6589326858520508, Classifier Loss 0.13381533324718475, Total Loss 50.48750305175781\n",
      "7: Encoding Loss 4.2118048667907715, Transition Loss -1.8142558336257935, Classifier Loss 0.10038261115550995, Total Loss 35.30836486816406\n",
      "7: Encoding Loss 3.7877261638641357, Transition Loss -0.6743445992469788, Classifier Loss 0.0765000507235527, Total Loss 30.376094818115234\n",
      "7: Encoding Loss 5.4409403800964355, Transition Loss 0.33419501781463623, Classifier Loss 0.10074444860219955, Total Loss 42.85376739501953\n",
      "7: Encoding Loss 6.261638641357422, Transition Loss -1.803979516029358, Classifier Loss 0.1552245318889618, Total Loss 53.0915641784668\n",
      "7: Encoding Loss 6.6314802169799805, Transition Loss -3.695801258087158, Classifier Loss 0.14767606556415558, Total Loss 54.55500793457031\n",
      "7: Encoding Loss 4.695630073547363, Transition Loss -1.644685983657837, Classifier Loss 0.08719418942928314, Total Loss 36.89254379272461\n",
      "7: Encoding Loss 5.472153663635254, Transition Loss -2.2870426177978516, Classifier Loss 0.04625819995999336, Total Loss 37.457828521728516\n",
      "7: Encoding Loss 2.7732045650482178, Transition Loss -2.7439522743225098, Classifier Loss 0.04842215031385422, Total Loss 21.4803466796875\n",
      "7: Encoding Loss 4.87992000579834, Transition Loss -2.91788387298584, Classifier Loss 0.1792812943458557, Total Loss 47.20648193359375\n",
      "7: Encoding Loss 4.871866703033447, Transition Loss -0.85234135389328, Classifier Loss 0.19202947616577148, Total Loss 48.43381118774414\n",
      "7: Encoding Loss 7.266326904296875, Transition Loss -0.4477294087409973, Classifier Loss 0.09993323683738708, Total Loss 53.59110641479492\n",
      "7: Encoding Loss 4.240710258483887, Transition Loss -1.141099452972412, Classifier Loss 0.05302463471889496, Total Loss 30.74627113342285\n",
      "7: Encoding Loss 3.866774320602417, Transition Loss -1.7063692808151245, Classifier Loss 0.06508173048496246, Total Loss 29.70813751220703\n",
      "7: Encoding Loss 5.452922344207764, Transition Loss -1.5577137470245361, Classifier Loss 0.039378572255373, Total Loss 36.6547737121582\n",
      "7: Encoding Loss 4.5657830238342285, Transition Loss -0.9169868230819702, Classifier Loss 0.05979291722178459, Total Loss 33.373626708984375\n",
      "7: Encoding Loss 4.740414619445801, Transition Loss -1.4288716316223145, Classifier Loss 0.0760049819946289, Total Loss 36.04241180419922\n",
      "7: Encoding Loss 4.99583625793457, Transition Loss -1.4414951801300049, Classifier Loss 0.05762539058923721, Total Loss 35.73698043823242\n",
      "7: Encoding Loss 5.318848133087158, Transition Loss -1.3823603391647339, Classifier Loss 0.04936970770359039, Total Loss 36.84950637817383\n",
      "7: Encoding Loss 4.6252641677856445, Transition Loss -1.2538152933120728, Classifier Loss 0.12584751844406128, Total Loss 40.335838317871094\n",
      "7: Encoding Loss 4.039408206939697, Transition Loss -0.8979529738426208, Classifier Loss 0.08777321875095367, Total Loss 33.01341247558594\n",
      "7: Encoding Loss 5.477863311767578, Transition Loss 0.3365945816040039, Classifier Loss 0.11626515537500381, Total Loss 44.628334045410156\n",
      "7: Encoding Loss 4.466207981109619, Transition Loss -1.850165605545044, Classifier Loss 0.07013358175754547, Total Loss 33.80986785888672\n",
      "7: Encoding Loss 4.686143398284912, Transition Loss -2.024768829345703, Classifier Loss 0.0703122541308403, Total Loss 35.14727783203125\n",
      "7: Encoding Loss 4.4076762199401855, Transition Loss -0.733883798122406, Classifier Loss 0.1502547711133957, Total Loss 41.47124099731445\n",
      "7: Encoding Loss 3.9893264770507812, Transition Loss -0.27764052152633667, Classifier Loss 0.04493694752454758, Total Loss 28.429542541503906\n",
      "7: Encoding Loss 5.134060859680176, Transition Loss -2.155134916305542, Classifier Loss 0.07201267033815384, Total Loss 38.00476837158203\n",
      "7: Encoding Loss 4.10400915145874, Transition Loss -0.9264717102050781, Classifier Loss 0.04392053186893463, Total Loss 29.01573944091797\n",
      "7: Encoding Loss 6.318617343902588, Transition Loss 0.06633469462394714, Classifier Loss 0.20264704525470734, Total Loss 58.20294189453125\n",
      "7: Encoding Loss 5.913356781005859, Transition Loss -1.7229254245758057, Classifier Loss 0.0878344401717186, Total Loss 44.26289367675781\n",
      "7: Encoding Loss 6.405152797698975, Transition Loss -0.5013494491577148, Classifier Loss 0.1669054627418518, Total Loss 55.12126541137695\n",
      "7: Encoding Loss 4.599363327026367, Transition Loss -1.1412289142608643, Classifier Loss 0.03079291060566902, Total Loss 30.675016403198242\n",
      "7: Encoding Loss 4.832700252532959, Transition Loss -1.0951169729232788, Classifier Loss 0.14684072136878967, Total Loss 43.67983627319336\n",
      "7: Encoding Loss 2.989915370941162, Transition Loss -1.530131220817566, Classifier Loss 0.11432279646396637, Total Loss 29.37116050720215\n",
      "7: Encoding Loss 5.724621772766113, Transition Loss -0.3803800046443939, Classifier Loss 0.13398100435733795, Total Loss 47.74568176269531\n",
      "7: Encoding Loss 5.789320945739746, Transition Loss -1.3504149913787842, Classifier Loss 0.09952833503484726, Total Loss 44.68821716308594\n",
      "7: Encoding Loss 3.916018009185791, Transition Loss -0.49662846326828003, Classifier Loss 0.12121850252151489, Total Loss 35.617759704589844\n",
      "7: Encoding Loss 4.763874530792236, Transition Loss -1.3462510108947754, Classifier Loss 0.09143614023923874, Total Loss 37.726322174072266\n",
      "7: Encoding Loss 5.992347717285156, Transition Loss -0.4969315826892853, Classifier Loss 0.07388566434383392, Total Loss 43.34245300292969\n",
      "7: Encoding Loss 3.9157745838165283, Transition Loss 0.09607289731502533, Classifier Loss 0.0663205087184906, Total Loss 30.165128707885742\n",
      "7: Encoding Loss 6.368697166442871, Transition Loss -0.06893107295036316, Classifier Loss 0.16946831345558167, Total Loss 55.15898895263672\n",
      "7: Encoding Loss 5.6315226554870605, Transition Loss -1.111158847808838, Classifier Loss 0.16439998149871826, Total Loss 50.22869110107422\n",
      "7: Encoding Loss 4.541870594024658, Transition Loss -1.397313117980957, Classifier Loss 0.08894383162260056, Total Loss 36.14504623413086\n",
      "7: Encoding Loss 4.001859188079834, Transition Loss -1.500087022781372, Classifier Loss 0.16517125070095062, Total Loss 40.52768325805664\n",
      "7: Encoding Loss 6.1778154373168945, Transition Loss -0.07323014736175537, Classifier Loss 0.13664424419403076, Total Loss 50.73128890991211\n",
      "7: Encoding Loss 5.2181267738342285, Transition Loss -1.542952299118042, Classifier Loss 0.14168106019496918, Total Loss 45.47624969482422\n",
      "7: Encoding Loss 5.756708145141602, Transition Loss -1.2555798292160034, Classifier Loss 0.15898028016090393, Total Loss 50.437774658203125\n",
      "7: Encoding Loss 5.156228065490723, Transition Loss -1.2325453758239746, Classifier Loss 0.08244556188583374, Total Loss 39.18143081665039\n",
      "7: Encoding Loss 5.835899353027344, Transition Loss -1.5952398777008057, Classifier Loss 0.11573837697505951, Total Loss 46.58859634399414\n",
      "7: Encoding Loss 5.572752475738525, Transition Loss -0.8747856616973877, Classifier Loss 0.10348701477050781, Total Loss 43.78486633300781\n",
      "7: Encoding Loss 4.689174652099609, Transition Loss -1.2316542863845825, Classifier Loss 0.09444400668144226, Total Loss 37.57896041870117\n",
      "7: Encoding Loss 4.560906410217285, Transition Loss -0.018403291702270508, Classifier Loss 0.04916072636842728, Total Loss 32.28150177001953\n",
      "7: Encoding Loss 3.8786532878875732, Transition Loss -0.2862566113471985, Classifier Loss 0.06312616914510727, Total Loss 29.584423065185547\n",
      "7: Encoding Loss 7.0623602867126465, Transition Loss -1.5447512865066528, Classifier Loss 0.1713264435529709, Total Loss 59.50619125366211\n",
      "7: Encoding Loss 6.157977104187012, Transition Loss -1.8190661668777466, Classifier Loss 0.12433958053588867, Total Loss 49.3810920715332\n",
      "7: Encoding Loss 3.44592022895813, Transition Loss -0.9775152802467346, Classifier Loss 0.08678749948740005, Total Loss 29.353879928588867\n",
      "7: Encoding Loss 7.013535499572754, Transition Loss -1.6127326488494873, Classifier Loss 0.09631293267011642, Total Loss 51.71186447143555\n",
      "7: Encoding Loss 4.741444110870361, Transition Loss -0.908784806728363, Classifier Loss 0.07930943369865417, Total Loss 36.37924575805664\n",
      "7: Encoding Loss 4.521193027496338, Transition Loss -0.9550230503082275, Classifier Loss 0.05279598385095596, Total Loss 32.406375885009766\n",
      "7: Encoding Loss 6.353858470916748, Transition Loss -1.9087016582489014, Classifier Loss 0.1106405183672905, Total Loss 49.18644332885742\n",
      "7: Encoding Loss 5.9193010330200195, Transition Loss -1.0489054918289185, Classifier Loss 0.1941637098789215, Total Loss 54.931758880615234\n",
      "7: Encoding Loss 5.568644046783447, Transition Loss -1.315532922744751, Classifier Loss 0.08661191910505295, Total Loss 42.072532653808594\n",
      "7: Encoding Loss 4.612002372741699, Transition Loss -0.0745786726474762, Classifier Loss 0.07587072253227234, Total Loss 35.259056091308594\n",
      "7: Encoding Loss 5.150962829589844, Transition Loss -2.3182852268218994, Classifier Loss 0.08863680064678192, Total Loss 39.768531799316406\n",
      "7: Encoding Loss 5.270138740539551, Transition Loss -1.8867319822311401, Classifier Loss 0.09758361428976059, Total Loss 41.378440856933594\n",
      "7: Encoding Loss 5.741565704345703, Transition Loss 0.21953850984573364, Classifier Loss 0.12909704446792603, Total Loss 47.44691467285156\n",
      "7: Encoding Loss 7.543253421783447, Transition Loss -1.355790615081787, Classifier Loss 0.13631321489810944, Total Loss 58.89030075073242\n",
      "7: Encoding Loss 4.877632141113281, Transition Loss -2.256906509399414, Classifier Loss 0.05726338177919388, Total Loss 34.99123001098633\n",
      "7: Encoding Loss 6.227509498596191, Transition Loss -0.9938023090362549, Classifier Loss 0.053306400775909424, Total Loss 42.6953010559082\n",
      "7: Encoding Loss 5.306885719299316, Transition Loss -0.7919979095458984, Classifier Loss 0.15551820397377014, Total Loss 47.392818450927734\n",
      "7: Encoding Loss 4.814873695373535, Transition Loss -1.1100659370422363, Classifier Loss 0.04109498858451843, Total Loss 32.99829864501953\n",
      "8: Encoding Loss 3.8688735961914062, Transition Loss -1.15964674949646, Classifier Loss 0.07401927560567856, Total Loss 30.61470603942871\n",
      "8: Encoding Loss 6.189531326293945, Transition Loss -0.1987958699464798, Classifier Loss 0.1012897714972496, Total Loss 47.26608657836914\n",
      "8: Encoding Loss 5.091236114501953, Transition Loss -1.626270055770874, Classifier Loss 0.08763135969638824, Total Loss 39.30990219116211\n",
      "8: Encoding Loss 6.015322208404541, Transition Loss -1.1191011667251587, Classifier Loss 0.07152845710515976, Total Loss 43.24433517456055\n",
      "8: Encoding Loss 7.7226080894470215, Transition Loss 0.3876308500766754, Classifier Loss 0.1981489211320877, Total Loss 66.30559539794922\n",
      "8: Encoding Loss 6.546830654144287, Transition Loss -2.836803674697876, Classifier Loss 0.059295836836099625, Total Loss 45.20943832397461\n",
      "8: Encoding Loss 4.2786946296691895, Transition Loss -2.0817372798919678, Classifier Loss 0.14995595812797546, Total Loss 40.66693115234375\n",
      "8: Encoding Loss 4.08986234664917, Transition Loss -1.6467769145965576, Classifier Loss 0.11555950343608856, Total Loss 36.09446334838867\n",
      "8: Encoding Loss 5.0555925369262695, Transition Loss -1.1553795337677002, Classifier Loss 0.1623842567205429, Total Loss 46.5715217590332\n",
      "8: Encoding Loss 3.214339017868042, Transition Loss -0.6680653095245361, Classifier Loss 0.08134228736162186, Total Loss 27.419998168945312\n",
      "8: Encoding Loss 5.899405479431152, Transition Loss -1.4129581451416016, Classifier Loss 0.1519726812839508, Total Loss 50.5931396484375\n",
      "8: Encoding Loss 3.9074463844299316, Transition Loss -1.2183926105499268, Classifier Loss 0.08073804527521133, Total Loss 31.517995834350586\n",
      "8: Encoding Loss 2.58489727973938, Transition Loss -0.38230377435684204, Classifier Loss 0.04592599347233772, Total Loss 20.101831436157227\n",
      "8: Encoding Loss 4.52169942855835, Transition Loss -2.1806087493896484, Classifier Loss 0.0831523984670639, Total Loss 35.44456481933594\n",
      "8: Encoding Loss 3.4773285388946533, Transition Loss -2.127793312072754, Classifier Loss 0.0819304957985878, Total Loss 29.056171417236328\n",
      "8: Encoding Loss 4.558810234069824, Transition Loss -1.3939793109893799, Classifier Loss 0.07924767583608627, Total Loss 35.27707290649414\n",
      "8: Encoding Loss 3.852020740509033, Transition Loss -1.8929928541183472, Classifier Loss 0.06935697793960571, Total Loss 30.04706573486328\n",
      "8: Encoding Loss 2.8557212352752686, Transition Loss 0.49880439043045044, Classifier Loss 0.07898063957691193, Total Loss 25.231914520263672\n",
      "8: Encoding Loss 7.081050872802734, Transition Loss -2.5458271503448486, Classifier Loss 0.1021113321185112, Total Loss 52.69641876220703\n",
      "8: Encoding Loss 4.286803245544434, Transition Loss -0.6441194415092468, Classifier Loss 0.1992163509130478, Total Loss 45.64219665527344\n",
      "8: Encoding Loss 6.6157426834106445, Transition Loss -2.4261085987091064, Classifier Loss 0.23701292276382446, Total Loss 63.39478302001953\n",
      "8: Encoding Loss 5.628264427185059, Transition Loss -1.294332504272461, Classifier Loss 0.12547576427459717, Total Loss 46.316646575927734\n",
      "8: Encoding Loss 6.354169845581055, Transition Loss -0.5432669520378113, Classifier Loss 0.11396650969982147, Total Loss 49.521453857421875\n",
      "8: Encoding Loss 6.295396327972412, Transition Loss -0.12754613161087036, Classifier Loss 0.10360943526029587, Total Loss 48.13327407836914\n",
      "8: Encoding Loss 5.421196937561035, Transition Loss 0.3543098270893097, Classifier Loss 0.06295377761125565, Total Loss 38.96428680419922\n",
      "8: Encoding Loss 5.027420997619629, Transition Loss -1.989542841911316, Classifier Loss 0.08346730470657349, Total Loss 38.510459899902344\n",
      "8: Encoding Loss 3.550840377807617, Transition Loss -0.3625568151473999, Classifier Loss 0.07670220732688904, Total Loss 28.97511863708496\n",
      "8: Encoding Loss 7.875174045562744, Transition Loss -1.5255091190338135, Classifier Loss 0.1628267616033554, Total Loss 63.533111572265625\n",
      "8: Encoding Loss 3.91850209236145, Transition Loss -2.3803582191467285, Classifier Loss 0.06724011898040771, Total Loss 30.234073638916016\n",
      "8: Encoding Loss 3.6558616161346436, Transition Loss -1.4050219058990479, Classifier Loss 0.16238535940647125, Total Loss 38.17314529418945\n",
      "8: Encoding Loss 4.887217044830322, Transition Loss -1.8248035907745361, Classifier Loss 0.0865185558795929, Total Loss 37.974430084228516\n",
      "8: Encoding Loss 4.583186149597168, Transition Loss 0.6413198709487915, Classifier Loss 0.06097522750496864, Total Loss 33.85316848754883\n",
      "8: Encoding Loss 6.144582748413086, Transition Loss -1.1841849088668823, Classifier Loss 0.15729370713233948, Total Loss 52.59639358520508\n",
      "8: Encoding Loss 4.228651523590088, Transition Loss -1.524083137512207, Classifier Loss 0.07621655613183975, Total Loss 32.99295425415039\n",
      "8: Encoding Loss 4.111238479614258, Transition Loss -1.3165581226348877, Classifier Loss 0.1349487155675888, Total Loss 38.16177749633789\n",
      "8: Encoding Loss 5.249381065368652, Transition Loss -1.470793604850769, Classifier Loss 0.24325615167617798, Total Loss 55.82131576538086\n",
      "8: Encoding Loss 4.819612503051758, Transition Loss -1.3516169786453247, Classifier Loss 0.10756158083677292, Total Loss 39.67329406738281\n",
      "8: Encoding Loss 4.7389702796936035, Transition Loss -0.33495762944221497, Classifier Loss 0.06967654824256897, Total Loss 35.401344299316406\n",
      "8: Encoding Loss 8.480928421020508, Transition Loss 0.07999566197395325, Classifier Loss 0.20482869446277618, Total Loss 71.40043640136719\n",
      "8: Encoding Loss 7.530272006988525, Transition Loss -1.6866285800933838, Classifier Loss 0.11572467535734177, Total Loss 56.75342559814453\n",
      "8: Encoding Loss 5.27839994430542, Transition Loss -1.165658712387085, Classifier Loss 0.13086161017417908, Total Loss 44.75609588623047\n",
      "8: Encoding Loss 6.374094486236572, Transition Loss -1.2042419910430908, Classifier Loss 0.12451217323541641, Total Loss 50.69530487060547\n",
      "8: Encoding Loss 5.7176618576049805, Transition Loss -1.1821973323822021, Classifier Loss 0.14060690999031067, Total Loss 48.36619186401367\n",
      "8: Encoding Loss 8.503419876098633, Transition Loss -2.404322624206543, Classifier Loss 0.07867062091827393, Total Loss 58.886619567871094\n",
      "8: Encoding Loss 5.147336959838867, Transition Loss -1.8404550552368164, Classifier Loss 0.11688646674156189, Total Loss 42.57193374633789\n",
      "8: Encoding Loss 3.438258409500122, Transition Loss 0.12245211005210876, Classifier Loss 0.06078071892261505, Total Loss 26.756603240966797\n",
      "8: Encoding Loss 4.361395835876465, Transition Loss -1.0901225805282593, Classifier Loss 0.10189115256071091, Total Loss 36.3570556640625\n",
      "8: Encoding Loss 7.112925052642822, Transition Loss 0.04435637593269348, Classifier Loss 0.09350458532571793, Total Loss 52.04574966430664\n",
      "8: Encoding Loss 7.385260105133057, Transition Loss -0.3351108729839325, Classifier Loss 0.2167927473783493, Total Loss 65.9906997680664\n",
      "8: Encoding Loss 4.1362409591674805, Transition Loss -0.2712266147136688, Classifier Loss 0.06966175138950348, Total Loss 31.78351402282715\n",
      "8: Encoding Loss 4.778055667877197, Transition Loss -1.6086050271987915, Classifier Loss 0.14703050255775452, Total Loss 43.3707389831543\n",
      "8: Encoding Loss 3.4864814281463623, Transition Loss -1.8978137969970703, Classifier Loss 0.038910917937755585, Total Loss 24.809223175048828\n",
      "8: Encoding Loss 7.69635009765625, Transition Loss -1.0052505731582642, Classifier Loss 0.08662724494934082, Total Loss 54.840423583984375\n",
      "8: Encoding Loss 5.224364280700684, Transition Loss 0.05067409574985504, Classifier Loss 0.15858492255210876, Total Loss 47.224952697753906\n",
      "8: Encoding Loss 2.9980742931365967, Transition Loss -1.8415281772613525, Classifier Loss 0.11991503834724426, Total Loss 29.97921371459961\n",
      "8: Encoding Loss 7.406766414642334, Transition Loss -0.2581922709941864, Classifier Loss 0.0633680447936058, Total Loss 50.77730178833008\n",
      "8: Encoding Loss 7.061702728271484, Transition Loss -0.5180701613426208, Classifier Loss 0.15812602639198303, Total Loss 58.182613372802734\n",
      "8: Encoding Loss 4.010664939880371, Transition Loss -1.118941307067871, Classifier Loss 0.06229960173368454, Total Loss 30.293502807617188\n",
      "8: Encoding Loss 10.23516845703125, Transition Loss -2.1402087211608887, Classifier Loss 0.19546063244342804, Total Loss 80.95622253417969\n",
      "8: Encoding Loss 6.756163120269775, Transition Loss -1.6767529249191284, Classifier Loss 0.04603692516684532, Total Loss 45.13999938964844\n",
      "8: Encoding Loss 5.856307029724121, Transition Loss -0.8568909168243408, Classifier Loss 0.09445725381374359, Total Loss 44.58322525024414\n",
      "8: Encoding Loss 4.218165397644043, Transition Loss -2.3967270851135254, Classifier Loss 0.08424170315265656, Total Loss 33.732208251953125\n",
      "8: Encoding Loss 3.8223862648010254, Transition Loss -2.3718268871307373, Classifier Loss 0.12862667441368103, Total Loss 35.79603576660156\n",
      "8: Encoding Loss 5.182692527770996, Transition Loss -4.238177299499512, Classifier Loss 0.10411503165960312, Total Loss 41.50596618652344\n",
      "8: Encoding Loss 4.711459636688232, Transition Loss -0.8565461039543152, Classifier Loss 0.12330015748739243, Total Loss 40.59843063354492\n",
      "8: Encoding Loss 5.908574104309082, Transition Loss -2.089515447616577, Classifier Loss 0.2131989449262619, Total Loss 56.770503997802734\n",
      "8: Encoding Loss 3.4725348949432373, Transition Loss -1.2543727159500122, Classifier Loss 0.2178449034690857, Total Loss 42.619197845458984\n",
      "8: Encoding Loss 5.871560096740723, Transition Loss -1.662209391593933, Classifier Loss 0.08219384402036667, Total Loss 43.448081970214844\n",
      "8: Encoding Loss 4.349174976348877, Transition Loss -0.0077044665813446045, Classifier Loss 0.09377742558717728, Total Loss 35.4727897644043\n",
      "8: Encoding Loss 2.8313724994659424, Transition Loss -0.05375717580318451, Classifier Loss 0.09785178303718567, Total Loss 26.773393630981445\n",
      "8: Encoding Loss 4.846541404724121, Transition Loss -2.1905534267425537, Classifier Loss 0.14179833233356476, Total Loss 43.25820541381836\n",
      "8: Encoding Loss 2.8796799182891846, Transition Loss -1.489704966545105, Classifier Loss 0.1107778325676918, Total Loss 28.355268478393555\n",
      "8: Encoding Loss 2.360360622406006, Transition Loss -2.1917829513549805, Classifier Loss 0.10260127484798431, Total Loss 24.42141342163086\n",
      "8: Encoding Loss 7.32210636138916, Transition Loss -0.3628024458885193, Classifier Loss 0.04318646341562271, Total Loss 48.25114059448242\n",
      "8: Encoding Loss 4.407576560974121, Transition Loss -1.5633103847503662, Classifier Loss 0.06152903661131859, Total Loss 32.597740173339844\n",
      "8: Encoding Loss 5.358409881591797, Transition Loss -1.8310645818710327, Classifier Loss 0.15120942890644073, Total Loss 47.270668029785156\n",
      "8: Encoding Loss 4.018982887268066, Transition Loss -1.9623398780822754, Classifier Loss 0.044268131256103516, Total Loss 28.53992462158203\n",
      "8: Encoding Loss 6.81015157699585, Transition Loss -1.595403790473938, Classifier Loss 0.09462273120880127, Total Loss 50.322547912597656\n",
      "8: Encoding Loss 4.855632781982422, Transition Loss -1.4862377643585205, Classifier Loss 0.14771468937397003, Total Loss 43.90467071533203\n",
      "8: Encoding Loss 2.923177480697632, Transition Loss -1.318528413772583, Classifier Loss 0.07399937510490417, Total Loss 24.938474655151367\n",
      "8: Encoding Loss 7.043515205383301, Transition Loss -0.9526839256286621, Classifier Loss 0.07251179963350296, Total Loss 49.51189041137695\n",
      "8: Encoding Loss 5.416496276855469, Transition Loss -1.0565670728683472, Classifier Loss 0.1745513677597046, Total Loss 49.95369338989258\n",
      "8: Encoding Loss 3.6870601177215576, Transition Loss -0.21509075164794922, Classifier Loss 0.15475739538669586, Total Loss 37.59801483154297\n",
      "8: Encoding Loss 4.81183385848999, Transition Loss -1.0950255393981934, Classifier Loss 0.12113972753286362, Total Loss 40.98453903198242\n",
      "8: Encoding Loss 5.4976806640625, Transition Loss -1.196272611618042, Classifier Loss 0.14638882875442505, Total Loss 47.624488830566406\n",
      "8: Encoding Loss 4.9679155349731445, Transition Loss -2.492220163345337, Classifier Loss 0.11430319398641586, Total Loss 41.23681640625\n",
      "8: Encoding Loss 4.481493949890137, Transition Loss 0.9267836213111877, Classifier Loss 0.06687508523464203, Total Loss 33.94718551635742\n",
      "8: Encoding Loss 5.2405595779418945, Transition Loss -1.6293084621429443, Classifier Loss 0.15669390559196472, Total Loss 47.11209487915039\n",
      "8: Encoding Loss 5.326539993286133, Transition Loss -2.5078587532043457, Classifier Loss 0.042701855301856995, Total Loss 36.228424072265625\n",
      "8: Encoding Loss 4.5777812004089355, Transition Loss -1.8002300262451172, Classifier Loss 0.1436418741941452, Total Loss 41.83015441894531\n",
      "8: Encoding Loss 3.737764358520508, Transition Loss 0.8586788773536682, Classifier Loss 0.11785154044628143, Total Loss 34.55521011352539\n",
      "8: Encoding Loss 2.9675369262695312, Transition Loss -1.3346047401428223, Classifier Loss 0.07375257462263107, Total Loss 25.17994499206543\n",
      "8: Encoding Loss 5.803128242492676, Transition Loss -1.8539257049560547, Classifier Loss 0.08077029883861542, Total Loss 42.89506149291992\n",
      "8: Encoding Loss 4.857685565948486, Transition Loss 1.0371137857437134, Classifier Loss 0.07942865043878555, Total Loss 37.503822326660156\n",
      "8: Encoding Loss 3.6920249462127686, Transition Loss -1.8539296388626099, Classifier Loss 0.05835829675197601, Total Loss 27.98723793029785\n",
      "8: Encoding Loss 2.8144168853759766, Transition Loss -1.3784065246582031, Classifier Loss 0.11070684343576431, Total Loss 27.956634521484375\n",
      "8: Encoding Loss 3.4934744834899902, Transition Loss 0.23276326060295105, Classifier Loss 0.059325553476810455, Total Loss 26.986507415771484\n",
      "8: Encoding Loss 10.635274887084961, Transition Loss 0.19059377908706665, Classifier Loss 0.2490745633840561, Total Loss 88.79534912109375\n",
      "8: Encoding Loss 7.064603328704834, Transition Loss -1.219435453414917, Classifier Loss 0.09956762939691544, Total Loss 52.34389877319336\n",
      "8: Encoding Loss 8.222211837768555, Transition Loss -1.061712384223938, Classifier Loss 0.1640862226486206, Total Loss 65.74147033691406\n",
      "8: Encoding Loss 6.27841854095459, Transition Loss -1.69406259059906, Classifier Loss 0.06060552969574928, Total Loss 43.73038864135742\n",
      "8: Encoding Loss 5.201831817626953, Transition Loss -1.9066576957702637, Classifier Loss 0.05595177412033081, Total Loss 36.8054084777832\n",
      "8: Encoding Loss 4.61501932144165, Transition Loss -0.4560665190219879, Classifier Loss 0.13928382098674774, Total Loss 41.618316650390625\n",
      "8: Encoding Loss 4.640408039093018, Transition Loss -2.167574882507324, Classifier Loss 0.10338208824396133, Total Loss 38.17979049682617\n",
      "8: Encoding Loss 2.9708733558654785, Transition Loss -0.20281124114990234, Classifier Loss 0.13871388137340546, Total Loss 31.69654655456543\n",
      "8: Encoding Loss 3.0435662269592285, Transition Loss 0.7801190614700317, Classifier Loss 0.05370093882083893, Total Loss 23.943540573120117\n",
      "8: Encoding Loss 6.562483787536621, Transition Loss 0.11568194627761841, Classifier Loss 0.1039648950099945, Total Loss 49.817665100097656\n",
      "8: Encoding Loss 4.069523811340332, Transition Loss -2.254302740097046, Classifier Loss 0.043339990079402924, Total Loss 28.750240325927734\n",
      "8: Encoding Loss 9.250271797180176, Transition Loss -2.102952718734741, Classifier Loss 0.1382620930671692, Total Loss 69.3270034790039\n",
      "8: Encoding Loss 6.985969543457031, Transition Loss -1.0994796752929688, Classifier Loss 0.17298133671283722, Total Loss 59.2135124206543\n",
      "8: Encoding Loss 4.265846252441406, Transition Loss -1.6469110250473022, Classifier Loss 0.06526638567447662, Total Loss 32.12105941772461\n",
      "8: Encoding Loss 6.251890659332275, Transition Loss -0.35476505756378174, Classifier Loss 0.09975669533014297, Total Loss 47.486873626708984\n",
      "8: Encoding Loss 6.459244728088379, Transition Loss -0.9788909554481506, Classifier Loss 0.07350073754787445, Total Loss 46.10515213012695\n",
      "8: Encoding Loss 5.746771812438965, Transition Loss -1.069615364074707, Classifier Loss 0.04038524627685547, Total Loss 38.51873016357422\n",
      "8: Encoding Loss 6.834046840667725, Transition Loss -1.3082165718078613, Classifier Loss 0.07152561843395233, Total Loss 48.15632247924805\n",
      "8: Encoding Loss 4.397026062011719, Transition Loss -2.601147413253784, Classifier Loss 0.0894714891910553, Total Loss 35.32826614379883\n",
      "8: Encoding Loss 4.02065896987915, Transition Loss -0.014763906598091125, Classifier Loss 0.09258506447076797, Total Loss 33.38245391845703\n",
      "8: Encoding Loss 2.9314286708831787, Transition Loss -0.9287938475608826, Classifier Loss 0.06305450201034546, Total Loss 23.893651962280273\n",
      "8: Encoding Loss 4.0370259284973145, Transition Loss -1.2204221487045288, Classifier Loss 0.09060566127300262, Total Loss 33.28223419189453\n",
      "8: Encoding Loss 5.8760666847229, Transition Loss -0.17333605885505676, Classifier Loss 0.11483142524957657, Total Loss 46.73947525024414\n",
      "8: Encoding Loss 6.20418643951416, Transition Loss -0.9080156087875366, Classifier Loss 0.20029664039611816, Total Loss 57.25442123413086\n",
      "8: Encoding Loss 4.829129219055176, Transition Loss -1.8701794147491455, Classifier Loss 0.13997159898281097, Total Loss 42.971187591552734\n",
      "8: Encoding Loss 6.7452263832092285, Transition Loss -1.5185935497283936, Classifier Loss 0.10266949981451035, Total Loss 50.737701416015625\n",
      "8: Encoding Loss 4.5132951736450195, Transition Loss -1.8218635320663452, Classifier Loss 0.06935521215200424, Total Loss 34.014564514160156\n",
      "8: Encoding Loss 2.6793951988220215, Transition Loss -1.3575503826141357, Classifier Loss 0.07244332879781723, Total Loss 23.320161819458008\n",
      "8: Encoding Loss 5.992732048034668, Transition Loss -2.3034698963165283, Classifier Loss 0.06554529070854187, Total Loss 42.51000213623047\n",
      "8: Encoding Loss 6.804041862487793, Transition Loss -2.409376621246338, Classifier Loss 0.19680920243263245, Total Loss 60.504207611083984\n",
      "8: Encoding Loss 5.102316379547119, Transition Loss -2.2006313800811768, Classifier Loss 0.04327279329299927, Total Loss 34.9402961730957\n",
      "8: Encoding Loss 3.0044054985046387, Transition Loss -1.1447813510894775, Classifier Loss 0.054736312478780746, Total Loss 23.49960708618164\n",
      "8: Encoding Loss 2.443373680114746, Transition Loss -3.593301296234131, Classifier Loss 0.06346694380044937, Total Loss 21.0054988861084\n",
      "8: Encoding Loss 1.2696985006332397, Transition Loss -1.0579957962036133, Classifier Loss 0.14008186757564545, Total Loss 21.625953674316406\n",
      "8: Encoding Loss 5.633092403411865, Transition Loss -1.259442925453186, Classifier Loss 0.08600180596113205, Total Loss 42.39823532104492\n",
      "8: Encoding Loss 5.200634002685547, Transition Loss -1.654165267944336, Classifier Loss 0.10589949786663055, Total Loss 41.793094635009766\n",
      "8: Encoding Loss 3.999014377593994, Transition Loss -1.464956521987915, Classifier Loss 0.06257005780935287, Total Loss 30.250507354736328\n",
      "8: Encoding Loss 6.210724830627441, Transition Loss -1.230157494544983, Classifier Loss 0.11736150085926056, Total Loss 49.00000762939453\n",
      "8: Encoding Loss 6.137457847595215, Transition Loss -1.1223218441009521, Classifier Loss 0.19764482975006104, Total Loss 56.588783264160156\n",
      "8: Encoding Loss 4.7893218994140625, Transition Loss 0.11667844653129578, Classifier Loss 0.070813849568367, Total Loss 35.863990783691406\n",
      "8: Encoding Loss 5.322379112243652, Transition Loss 0.1850554347038269, Classifier Loss 0.10912581533193588, Total Loss 42.92087936401367\n",
      "8: Encoding Loss 3.63086199760437, Transition Loss -1.8044664859771729, Classifier Loss 0.0657641813158989, Total Loss 28.360870361328125\n",
      "8: Encoding Loss 7.925403594970703, Transition Loss 0.13821235299110413, Classifier Loss 0.06011383235454559, Total Loss 53.61909103393555\n",
      "8: Encoding Loss 6.04196310043335, Transition Loss -1.4814578294754028, Classifier Loss 0.05819263681769371, Total Loss 42.07045364379883\n",
      "8: Encoding Loss 6.254103660583496, Transition Loss -0.5361276865005493, Classifier Loss 0.11046815663576126, Total Loss 48.57122802734375\n",
      "8: Encoding Loss 5.311869144439697, Transition Loss -0.9893075227737427, Classifier Loss 0.050129618495702744, Total Loss 36.88378143310547\n",
      "8: Encoding Loss 5.230867862701416, Transition Loss -0.14739039540290833, Classifier Loss 0.08256799727678299, Total Loss 39.64194869995117\n",
      "8: Encoding Loss 3.829530715942383, Transition Loss -1.2225897312164307, Classifier Loss 0.08132453262805939, Total Loss 31.109149932861328\n",
      "8: Encoding Loss 5.329871654510498, Transition Loss -0.8902562260627747, Classifier Loss 0.18228162825107574, Total Loss 50.20703887939453\n",
      "8: Encoding Loss 2.922290563583374, Transition Loss -1.6592872142791748, Classifier Loss 0.05789061635732651, Total Loss 23.3221435546875\n",
      "8: Encoding Loss 6.6415886878967285, Transition Loss -1.2492858171463013, Classifier Loss 0.28892308473587036, Total Loss 68.74134826660156\n",
      "8: Encoding Loss 6.358829021453857, Transition Loss -0.6924132108688354, Classifier Loss 0.20618757605552673, Total Loss 58.77145767211914\n",
      "8: Encoding Loss 4.0456743240356445, Transition Loss -1.618110179901123, Classifier Loss 0.13995274901390076, Total Loss 38.268672943115234\n",
      "8: Encoding Loss 5.65521764755249, Transition Loss -2.017051935195923, Classifier Loss 0.07388095557689667, Total Loss 41.31859588623047\n",
      "8: Encoding Loss 6.8516035079956055, Transition Loss -0.30660730600357056, Classifier Loss 0.09460490942001343, Total Loss 50.56999206542969\n",
      "8: Encoding Loss 7.2707319259643555, Transition Loss -0.6862343549728394, Classifier Loss 0.1648612916469574, Total Loss 60.11024856567383\n",
      "8: Encoding Loss 5.075532913208008, Transition Loss -1.8503040075302124, Classifier Loss 0.07624030858278275, Total Loss 38.07649230957031\n",
      "8: Encoding Loss 4.549018383026123, Transition Loss -3.3492074012756348, Classifier Loss 0.08598994463682175, Total Loss 35.89176559448242\n",
      "8: Encoding Loss 7.072293281555176, Transition Loss -2.237495183944702, Classifier Loss 0.15526774525642395, Total Loss 57.95964050292969\n",
      "8: Encoding Loss 5.677427291870117, Transition Loss -0.7930972576141357, Classifier Loss 0.07657923549413681, Total Loss 41.722171783447266\n",
      "8: Encoding Loss 3.7718100547790527, Transition Loss -2.358919620513916, Classifier Loss 0.09443722665309906, Total Loss 32.07364273071289\n",
      "8: Encoding Loss 2.6130199432373047, Transition Loss -0.891755223274231, Classifier Loss 0.08583103865385056, Total Loss 24.260868072509766\n",
      "8: Encoding Loss 7.431795597076416, Transition Loss -2.060027599334717, Classifier Loss 0.12067962437868118, Total Loss 56.65791320800781\n",
      "8: Encoding Loss 4.713534832000732, Transition Loss 0.05012708902359009, Classifier Loss 0.08700501173734665, Total Loss 37.00176239013672\n",
      "8: Encoding Loss 6.1530680656433105, Transition Loss -0.6814526319503784, Classifier Loss 0.07392407953739166, Total Loss 44.310546875\n",
      "8: Encoding Loss 4.063657283782959, Transition Loss -0.9845550060272217, Classifier Loss 0.048084378242492676, Total Loss 29.18998908996582\n",
      "8: Encoding Loss 4.086812973022461, Transition Loss -1.4757534265518188, Classifier Loss 0.07364766299724579, Total Loss 31.88505744934082\n",
      "8: Encoding Loss 7.7661452293396, Transition Loss 0.17142194509506226, Classifier Loss 0.16531099379062653, Total Loss 63.1965446472168\n",
      "8: Encoding Loss 5.741028785705566, Transition Loss -1.3775479793548584, Classifier Loss 0.1686931550502777, Total Loss 51.31494140625\n",
      "8: Encoding Loss 5.486026763916016, Transition Loss -3.1118717193603516, Classifier Loss 0.16472865641117096, Total Loss 49.38778305053711\n",
      "8: Encoding Loss 3.678992509841919, Transition Loss -2.047607898712158, Classifier Loss 0.06749125570058823, Total Loss 28.822263717651367\n",
      "8: Encoding Loss 7.780198097229004, Transition Loss -1.897629976272583, Classifier Loss 0.14626064896583557, Total Loss 61.306495666503906\n",
      "8: Encoding Loss 5.488646507263184, Transition Loss -0.6916857957839966, Classifier Loss 0.05727069079875946, Total Loss 38.65867233276367\n",
      "8: Encoding Loss 4.761667728424072, Transition Loss -1.4097375869750977, Classifier Loss 0.07409853488206863, Total Loss 35.97929763793945\n",
      "8: Encoding Loss 3.999812364578247, Transition Loss -1.9907313585281372, Classifier Loss 0.040368035435676575, Total Loss 28.034883499145508\n",
      "8: Encoding Loss 5.355856895446777, Transition Loss -1.2938413619995117, Classifier Loss 0.1753198802471161, Total Loss 49.66661071777344\n",
      "8: Encoding Loss 6.040154457092285, Transition Loss -3.1680335998535156, Classifier Loss 0.12194669246673584, Total Loss 48.43433380126953\n",
      "8: Encoding Loss 4.069688320159912, Transition Loss -0.7933928370475769, Classifier Loss 0.09580839425325394, Total Loss 33.998653411865234\n",
      "8: Encoding Loss 6.467583656311035, Transition Loss -0.1445099264383316, Classifier Loss 0.15367372334003448, Total Loss 54.17281723022461\n",
      "8: Encoding Loss 6.862756252288818, Transition Loss -1.397068738937378, Classifier Loss 0.1361328512430191, Total Loss 54.789268493652344\n",
      "8: Encoding Loss 5.502381324768066, Transition Loss -2.632561445236206, Classifier Loss 0.10337372124195099, Total Loss 43.350608825683594\n",
      "8: Encoding Loss 6.325355529785156, Transition Loss -1.139338493347168, Classifier Loss 0.04992377758026123, Total Loss 42.94405746459961\n",
      "8: Encoding Loss 5.2692766189575195, Transition Loss 0.39620232582092285, Classifier Loss 0.09656541049480438, Total Loss 41.43068313598633\n",
      "8: Encoding Loss 5.525338172912598, Transition Loss -0.6871888637542725, Classifier Loss 0.12827639281749725, Total Loss 45.97939682006836\n",
      "8: Encoding Loss 7.104445457458496, Transition Loss -0.7858210802078247, Classifier Loss 0.05349431931972504, Total Loss 47.975791931152344\n",
      "8: Encoding Loss 5.04638147354126, Transition Loss -2.243234157562256, Classifier Loss 0.05700170248746872, Total Loss 35.977561950683594\n",
      "8: Encoding Loss 6.99676513671875, Transition Loss -1.541710376739502, Classifier Loss 0.14684061706066132, Total Loss 56.66403579711914\n",
      "8: Encoding Loss 5.190239429473877, Transition Loss -0.9436343908309937, Classifier Loss 0.20196105539798737, Total Loss 51.33716583251953\n",
      "8: Encoding Loss 5.428175926208496, Transition Loss -2.6565518379211426, Classifier Loss 0.11542466282844543, Total Loss 44.11045837402344\n",
      "8: Encoding Loss 3.328214168548584, Transition Loss 0.4573817253112793, Classifier Loss 0.09907577931880951, Total Loss 30.059816360473633\n",
      "8: Encoding Loss 3.0998144149780273, Transition Loss -0.5915912985801697, Classifier Loss 0.07383716106414795, Total Loss 25.98236656188965\n",
      "8: Encoding Loss 2.4409823417663574, Transition Loss -1.0933479070663452, Classifier Loss 0.07417145371437073, Total Loss 22.062602996826172\n",
      "8: Encoding Loss 7.808451175689697, Transition Loss -2.1381940841674805, Classifier Loss 0.15225555002689362, Total Loss 62.075408935546875\n",
      "8: Encoding Loss 4.848517417907715, Transition Loss -1.483302354812622, Classifier Loss 0.10973221808671951, Total Loss 40.0637321472168\n",
      "8: Encoding Loss 6.336418628692627, Transition Loss -1.5938396453857422, Classifier Loss 0.09949573874473572, Total Loss 47.96744918823242\n",
      "8: Encoding Loss 4.48772668838501, Transition Loss -1.1077107191085815, Classifier Loss 0.12753240764141083, Total Loss 39.679161071777344\n",
      "8: Encoding Loss 4.303215980529785, Transition Loss -1.791347861289978, Classifier Loss 0.12415334582328796, Total Loss 38.23391342163086\n",
      "8: Encoding Loss 4.061174392700195, Transition Loss -1.7606149911880493, Classifier Loss 0.14346109330654144, Total Loss 38.71245193481445\n",
      "8: Encoding Loss 4.393259048461914, Transition Loss -1.3788609504699707, Classifier Loss 0.10842978954315186, Total Loss 37.20198440551758\n",
      "8: Encoding Loss 6.538932800292969, Transition Loss -1.0721070766448975, Classifier Loss 0.12605369091033936, Total Loss 51.838539123535156\n",
      "8: Encoding Loss 4.933423042297363, Transition Loss -0.7178609371185303, Classifier Loss 0.24121293425559998, Total Loss 53.7215461730957\n",
      "8: Encoding Loss 6.930690765380859, Transition Loss -1.8105485439300537, Classifier Loss 0.09378905594348907, Total Loss 50.96232604980469\n",
      "8: Encoding Loss 4.465137958526611, Transition Loss -1.9272046089172363, Classifier Loss 0.07917871326208115, Total Loss 34.70793151855469\n",
      "8: Encoding Loss 6.002385139465332, Transition Loss -1.5354175567626953, Classifier Loss 0.0894390195608139, Total Loss 44.95759963989258\n",
      "8: Encoding Loss 4.507411003112793, Transition Loss -0.930573582649231, Classifier Loss 0.08797544240951538, Total Loss 35.841636657714844\n",
      "8: Encoding Loss 4.332151412963867, Transition Loss -0.9204052686691284, Classifier Loss 0.05513203516602516, Total Loss 31.505746841430664\n",
      "8: Encoding Loss 3.1199488639831543, Transition Loss -0.43696409463882446, Classifier Loss 0.07661037892103195, Total Loss 26.380556106567383\n",
      "8: Encoding Loss 5.60066032409668, Transition Loss -1.012437105178833, Classifier Loss 0.07742775976657867, Total Loss 41.34633255004883\n",
      "8: Encoding Loss 5.716490268707275, Transition Loss -0.7757365703582764, Classifier Loss 0.16724726557731628, Total Loss 51.02335739135742\n",
      "8: Encoding Loss 3.6018645763397217, Transition Loss -1.5360865592956543, Classifier Loss 0.16937430202960968, Total Loss 38.548004150390625\n",
      "8: Encoding Loss 3.9985694885253906, Transition Loss -0.07021327316761017, Classifier Loss 0.11472901701927185, Total Loss 35.464290618896484\n",
      "8: Encoding Loss 7.142270565032959, Transition Loss -1.380307674407959, Classifier Loss 0.19987809658050537, Total Loss 62.840885162353516\n",
      "8: Encoding Loss 5.97724723815918, Transition Loss -0.30565255880355835, Classifier Loss 0.09789154678583145, Total Loss 45.65251541137695\n",
      "8: Encoding Loss 5.159384250640869, Transition Loss -0.8306357860565186, Classifier Loss 0.04745092615485191, Total Loss 35.70106887817383\n",
      "8: Encoding Loss 6.6488447189331055, Transition Loss -0.44500231742858887, Classifier Loss 0.1183651015162468, Total Loss 51.729400634765625\n",
      "8: Encoding Loss 4.7255144119262695, Transition Loss -3.093797206878662, Classifier Loss 0.12703578174114227, Total Loss 41.05542755126953\n",
      "8: Encoding Loss 3.3265323638916016, Transition Loss 1.0880411863327026, Classifier Loss 0.08237095922231674, Total Loss 28.631505966186523\n",
      "8: Encoding Loss 3.64178466796875, Transition Loss -2.357494354248047, Classifier Loss 0.0787772610783577, Total Loss 29.72749137878418\n",
      "8: Encoding Loss 6.165630340576172, Transition Loss -2.149122714996338, Classifier Loss 0.04053713381290436, Total Loss 41.04663848876953\n",
      "8: Encoding Loss 6.975409030914307, Transition Loss -1.6952646970748901, Classifier Loss 0.07272912561893463, Total Loss 49.12468719482422\n",
      "8: Encoding Loss 6.444288730621338, Transition Loss -1.2425053119659424, Classifier Loss 0.05896259844303131, Total Loss 44.56149673461914\n",
      "8: Encoding Loss 4.398126125335693, Transition Loss -0.8200278878211975, Classifier Loss 0.10615764558315277, Total Loss 37.00419616699219\n",
      "8: Encoding Loss 5.056895732879639, Transition Loss -0.7431145906448364, Classifier Loss 0.050958745181560516, Total Loss 35.43695068359375\n",
      "8: Encoding Loss 3.0531399250030518, Transition Loss -0.7726991176605225, Classifier Loss 0.09401483833789825, Total Loss 27.720014572143555\n",
      "8: Encoding Loss 6.87026834487915, Transition Loss -2.398685932159424, Classifier Loss 0.07141918689012527, Total Loss 48.36256790161133\n",
      "8: Encoding Loss 4.927199363708496, Transition Loss -1.4279756546020508, Classifier Loss 0.06365357339382172, Total Loss 35.927982330322266\n",
      "8: Encoding Loss 6.240426063537598, Transition Loss -1.6531693935394287, Classifier Loss 0.15220347046852112, Total Loss 52.66224670410156\n",
      "8: Encoding Loss 4.405857086181641, Transition Loss -1.4725892543792725, Classifier Loss 0.0276104137301445, Total Loss 29.19559669494629\n",
      "8: Encoding Loss 5.7142462730407715, Transition Loss -0.1909428834915161, Classifier Loss 0.07231971621513367, Total Loss 41.51737594604492\n",
      "8: Encoding Loss 5.040221214294434, Transition Loss -1.3994556665420532, Classifier Loss 0.13384383916854858, Total Loss 43.625152587890625\n",
      "8: Encoding Loss 4.496598243713379, Transition Loss -1.4277567863464355, Classifier Loss 0.07740425318479538, Total Loss 34.719444274902344\n",
      "8: Encoding Loss 2.8933088779449463, Transition Loss -2.3969743251800537, Classifier Loss 0.06138867884874344, Total Loss 23.49776268005371\n",
      "8: Encoding Loss 5.5525617599487305, Transition Loss -3.415442705154419, Classifier Loss 0.061854176223278046, Total Loss 39.49942398071289\n",
      "8: Encoding Loss 3.7018444538116455, Transition Loss -0.6491942405700684, Classifier Loss 0.1453757882118225, Total Loss 36.74838638305664\n",
      "8: Encoding Loss 4.887484073638916, Transition Loss -0.8960383534431458, Classifier Loss 0.16359269618988037, Total Loss 45.683815002441406\n",
      "8: Encoding Loss 5.648737907409668, Transition Loss -0.553130030632019, Classifier Loss 0.10183974355459213, Total Loss 44.0761833190918\n",
      "8: Encoding Loss 3.9970695972442627, Transition Loss -0.415371298789978, Classifier Loss 0.051898229867219925, Total Loss 29.172075271606445\n",
      "8: Encoding Loss 8.409296989440918, Transition Loss -0.14277490973472595, Classifier Loss 0.143537238240242, Total Loss 64.80945587158203\n",
      "8: Encoding Loss 4.562554359436035, Transition Loss -0.9868073463439941, Classifier Loss 0.08985478430986404, Total Loss 36.36041259765625\n",
      "8: Encoding Loss 7.810972690582275, Transition Loss -0.3932381272315979, Classifier Loss 0.1936073899269104, Total Loss 66.2264175415039\n",
      "8: Encoding Loss 5.739199161529541, Transition Loss -2.684481620788574, Classifier Loss 0.17738860845565796, Total Loss 52.1729850769043\n",
      "8: Encoding Loss 5.581896781921387, Transition Loss -1.0580564737319946, Classifier Loss 0.1196114793419838, Total Loss 45.45210647583008\n",
      "8: Encoding Loss 5.615294456481934, Transition Loss -1.8609081506729126, Classifier Loss 0.14794695377349854, Total Loss 48.485721588134766\n",
      "8: Encoding Loss 3.261357069015503, Transition Loss -1.3439483642578125, Classifier Loss 0.08057291805744171, Total Loss 27.624897003173828\n",
      "8: Encoding Loss 3.80902361869812, Transition Loss -0.408305287361145, Classifier Loss 0.08508983254432678, Total Loss 31.36296272277832\n",
      "8: Encoding Loss 4.3510026931762695, Transition Loss -0.7842311263084412, Classifier Loss 0.10519194602966309, Total Loss 36.62489700317383\n",
      "8: Encoding Loss 4.328488349914551, Transition Loss -2.7786428928375244, Classifier Loss 0.07828287035226822, Total Loss 33.7981071472168\n",
      "8: Encoding Loss 5.6757707595825195, Transition Loss -1.5636262893676758, Classifier Loss 0.13519304990768433, Total Loss 47.573307037353516\n",
      "8: Encoding Loss 4.921920299530029, Transition Loss -1.4286127090454102, Classifier Loss 0.09552928060293198, Total Loss 39.08387756347656\n",
      "8: Encoding Loss 3.5182292461395264, Transition Loss -0.3474798798561096, Classifier Loss 0.09535612910985947, Total Loss 30.64484977722168\n",
      "8: Encoding Loss 4.279325008392334, Transition Loss -1.6295757293701172, Classifier Loss 0.051856353878974915, Total Loss 30.86093521118164\n",
      "8: Encoding Loss 6.111738681793213, Transition Loss -1.2556899785995483, Classifier Loss 0.12543614208698273, Total Loss 49.21354293823242\n",
      "8: Encoding Loss 4.914291858673096, Transition Loss 0.13634353876113892, Classifier Loss 0.06341258436441422, Total Loss 35.88154983520508\n",
      "8: Encoding Loss 6.751224517822266, Transition Loss 0.09632617235183716, Classifier Loss 0.09680599719285965, Total Loss 50.226478576660156\n",
      "8: Encoding Loss 6.5058698654174805, Transition Loss -1.7332106828689575, Classifier Loss 0.2844139039516449, Total Loss 67.47591400146484\n",
      "8: Encoding Loss 6.07836389541626, Transition Loss -1.7372832298278809, Classifier Loss 0.059173136949539185, Total Loss 42.386802673339844\n",
      "8: Encoding Loss 5.206658363342285, Transition Loss -1.3311622142791748, Classifier Loss 0.10896506160497665, Total Loss 42.135921478271484\n",
      "8: Encoding Loss 4.1580400466918945, Transition Loss -1.2752513885498047, Classifier Loss 0.07579921931028366, Total Loss 32.52764892578125\n",
      "8: Encoding Loss 5.6659650802612305, Transition Loss -2.109957695007324, Classifier Loss 0.20408545434474945, Total Loss 54.40349197387695\n",
      "8: Encoding Loss 3.814640760421753, Transition Loss -1.0123599767684937, Classifier Loss 0.06840824335813522, Total Loss 29.7282657623291\n",
      "8: Encoding Loss 6.8929243087768555, Transition Loss -2.1989829540252686, Classifier Loss 0.1095440611243248, Total Loss 52.311073303222656\n",
      "8: Encoding Loss 5.7748236656188965, Transition Loss -1.4474942684173584, Classifier Loss 0.13848073780536652, Total Loss 48.496437072753906\n",
      "8: Encoding Loss 6.009880065917969, Transition Loss -1.8842662572860718, Classifier Loss 0.05720844864845276, Total Loss 41.77936935424805\n",
      "8: Encoding Loss 6.440463066101074, Transition Loss -0.8508771061897278, Classifier Loss 0.14197354018688202, Total Loss 52.83979415893555\n",
      "8: Encoding Loss 2.434093952178955, Transition Loss -1.9644060134887695, Classifier Loss 0.07266725599765778, Total Loss 21.870502471923828\n",
      "8: Encoding Loss 2.626373052597046, Transition Loss -0.66330885887146, Classifier Loss 0.05486693233251572, Total Loss 21.244667053222656\n",
      "8: Encoding Loss 6.067259788513184, Transition Loss -0.4321485161781311, Classifier Loss 0.09553063660860062, Total Loss 45.956451416015625\n",
      "8: Encoding Loss 4.205775260925293, Transition Loss 0.46838754415512085, Classifier Loss 0.08131895214319229, Total Loss 33.55390167236328\n",
      "8: Encoding Loss 5.807376861572266, Transition Loss -0.9675015211105347, Classifier Loss 0.16649937629699707, Total Loss 51.493812561035156\n",
      "8: Encoding Loss 5.486900806427002, Transition Loss -1.9111872911453247, Classifier Loss 0.06716480106115341, Total Loss 39.637123107910156\n",
      "8: Encoding Loss 3.739837169647217, Transition Loss -2.5498692989349365, Classifier Loss 0.11172628402709961, Total Loss 33.610633850097656\n",
      "8: Encoding Loss 3.1394710540771484, Transition Loss -1.5156382322311401, Classifier Loss 0.07437770068645477, Total Loss 26.273990631103516\n",
      "8: Encoding Loss 7.590213775634766, Transition Loss -1.4386076927185059, Classifier Loss 0.17477622628211975, Total Loss 63.01832962036133\n",
      "8: Encoding Loss 4.266332149505615, Transition Loss -0.14290621876716614, Classifier Loss 0.1730184555053711, Total Loss 42.899784088134766\n",
      "8: Encoding Loss 4.713963985443115, Transition Loss -1.5024008750915527, Classifier Loss 0.05444073677062988, Total Loss 33.727256774902344\n",
      "8: Encoding Loss 5.291604042053223, Transition Loss 0.13939376175403595, Classifier Loss 0.1343177855014801, Total Loss 45.237159729003906\n",
      "8: Encoding Loss 3.140475273132324, Transition Loss 0.6058098077774048, Classifier Loss 0.06065978854894638, Total Loss 25.151155471801758\n",
      "8: Encoding Loss 3.7093663215637207, Transition Loss -1.4887399673461914, Classifier Loss 0.0887833684682846, Total Loss 31.133941650390625\n",
      "8: Encoding Loss 6.279282569885254, Transition Loss -0.9376891255378723, Classifier Loss 0.09794873744249344, Total Loss 47.47019958496094\n",
      "8: Encoding Loss 3.6835880279541016, Transition Loss -0.96453458070755, Classifier Loss 0.11257915943861008, Total Loss 33.35905838012695\n",
      "8: Encoding Loss 6.366076946258545, Transition Loss -0.9859066009521484, Classifier Loss 0.14637768268585205, Total Loss 52.833839416503906\n",
      "8: Encoding Loss 6.803623199462891, Transition Loss -1.3675251007080078, Classifier Loss 0.18633709847927094, Total Loss 59.45490264892578\n",
      "8: Encoding Loss 4.6501898765563965, Transition Loss -1.1193580627441406, Classifier Loss 0.037573546171188354, Total Loss 31.65804672241211\n",
      "8: Encoding Loss 4.6323747634887695, Transition Loss -0.39788690209388733, Classifier Loss 0.16743670403957367, Total Loss 44.537757873535156\n",
      "8: Encoding Loss 5.920982837677002, Transition Loss -3.3485240936279297, Classifier Loss 0.17696048319339752, Total Loss 53.22060775756836\n",
      "8: Encoding Loss 6.673861026763916, Transition Loss -1.6207305192947388, Classifier Loss 0.06077847257256508, Total Loss 46.120365142822266\n",
      "8: Encoding Loss 5.048886299133301, Transition Loss -1.3059428930282593, Classifier Loss 0.07880441844463348, Total Loss 38.17323684692383\n",
      "8: Encoding Loss 6.302971839904785, Transition Loss -2.269601345062256, Classifier Loss 0.14346374571323395, Total Loss 52.163299560546875\n",
      "8: Encoding Loss 5.0886054039001465, Transition Loss -0.460873007774353, Classifier Loss 0.07013219594955444, Total Loss 37.54467010498047\n",
      "8: Encoding Loss 2.899305820465088, Transition Loss -1.5668177604675293, Classifier Loss 0.0825587809085846, Total Loss 25.651086807250977\n",
      "8: Encoding Loss 5.014914035797119, Transition Loss -1.9326690435409546, Classifier Loss 0.12514103949069977, Total Loss 42.602813720703125\n",
      "8: Encoding Loss 9.276496887207031, Transition Loss -3.303375244140625, Classifier Loss 0.17287875711917877, Total Loss 72.94554138183594\n",
      "8: Encoding Loss 5.460970878601074, Transition Loss -1.3923628330230713, Classifier Loss 0.05275090038776398, Total Loss 38.04035949707031\n",
      "8: Encoding Loss 5.311860084533691, Transition Loss -1.0201796293258667, Classifier Loss 0.05206970497965813, Total Loss 37.07772445678711\n",
      "8: Encoding Loss 6.166293621063232, Transition Loss -0.2344086766242981, Classifier Loss 0.16020509600639343, Total Loss 53.0181770324707\n",
      "8: Encoding Loss 5.992631912231445, Transition Loss -0.8941406011581421, Classifier Loss 0.27340322732925415, Total Loss 63.295753479003906\n",
      "8: Encoding Loss 6.163822174072266, Transition Loss -1.6029016971588135, Classifier Loss 0.18367061018943787, Total Loss 55.3493537902832\n",
      "8: Encoding Loss 6.3282036781311035, Transition Loss -2.171968936920166, Classifier Loss 0.1475113481283188, Total Loss 52.71949005126953\n",
      "8: Encoding Loss 5.256973743438721, Transition Loss -1.40813410282135, Classifier Loss 0.11886255443096161, Total Loss 43.42753601074219\n",
      "8: Encoding Loss 5.456761360168457, Transition Loss -0.49619126319885254, Classifier Loss 0.14854882657527924, Total Loss 47.595252990722656\n",
      "8: Encoding Loss 6.861291408538818, Transition Loss -0.7414586544036865, Classifier Loss 0.17177912592887878, Total Loss 58.345367431640625\n",
      "8: Encoding Loss 4.855952262878418, Transition Loss -0.5725140571594238, Classifier Loss 0.06302060186862946, Total Loss 35.43754577636719\n",
      "8: Encoding Loss 7.576533317565918, Transition Loss -2.0885448455810547, Classifier Loss 0.07290972769260406, Total Loss 52.74934005737305\n",
      "8: Encoding Loss 4.795539379119873, Transition Loss -0.9914439916610718, Classifier Loss 0.10555066168308258, Total Loss 39.32790756225586\n",
      "8: Encoding Loss 4.145538330078125, Transition Loss -1.688225507736206, Classifier Loss 0.08441651612520218, Total Loss 33.314208984375\n",
      "8: Encoding Loss 5.622783660888672, Transition Loss -0.4542948603630066, Classifier Loss 0.18111133575439453, Total Loss 51.847652435302734\n",
      "8: Encoding Loss 5.051082611083984, Transition Loss -1.4300339221954346, Classifier Loss 0.07363870739936829, Total Loss 37.669795989990234\n",
      "8: Encoding Loss 4.859153747558594, Transition Loss -1.4919734001159668, Classifier Loss 0.15175816416740417, Total Loss 44.33014678955078\n",
      "8: Encoding Loss 10.883913040161133, Transition Loss -2.2394516468048096, Classifier Loss 0.13125453889369965, Total Loss 78.42804718017578\n",
      "8: Encoding Loss 6.3065080642700195, Transition Loss -0.7263875007629395, Classifier Loss 0.06507901102304459, Total Loss 44.34666061401367\n",
      "8: Encoding Loss 6.764151096343994, Transition Loss 0.7313321828842163, Classifier Loss 0.06768636405467987, Total Loss 47.64607620239258\n",
      "8: Encoding Loss 4.593802452087402, Transition Loss -2.2895209789276123, Classifier Loss 0.10991674661636353, Total Loss 38.55357360839844\n",
      "8: Encoding Loss 5.961735725402832, Transition Loss -1.5768764019012451, Classifier Loss 0.1203770861029625, Total Loss 47.8074951171875\n",
      "8: Encoding Loss 3.743185520172119, Transition Loss -0.5259106159210205, Classifier Loss 0.09809068590402603, Total Loss 32.26797103881836\n",
      "8: Encoding Loss 5.551466941833496, Transition Loss -1.4017603397369385, Classifier Loss 0.2006678432226181, Total Loss 53.37502670288086\n",
      "8: Encoding Loss 9.304234504699707, Transition Loss -1.3275816440582275, Classifier Loss 0.11511099338531494, Total Loss 67.33597564697266\n",
      "8: Encoding Loss 4.85252571105957, Transition Loss -1.7955559492111206, Classifier Loss 0.09918460249900818, Total Loss 39.03289794921875\n",
      "8: Encoding Loss 5.320884704589844, Transition Loss -1.1002607345581055, Classifier Loss 0.07198288291692734, Total Loss 39.12316131591797\n",
      "8: Encoding Loss 5.950928688049316, Transition Loss -2.096513509750366, Classifier Loss 0.04961336404085159, Total Loss 40.666072845458984\n",
      "8: Encoding Loss 5.182264804840088, Transition Loss -2.1917243003845215, Classifier Loss 0.04977836087346077, Total Loss 36.07054901123047\n",
      "8: Encoding Loss 4.527447700500488, Transition Loss -1.0903271436691284, Classifier Loss 0.038460277020931244, Total Loss 31.010276794433594\n",
      "8: Encoding Loss 4.157180309295654, Transition Loss -2.474360704421997, Classifier Loss 0.10672294348478317, Total Loss 35.6143913269043\n",
      "8: Encoding Loss 6.1545891761779785, Transition Loss -0.5384640693664551, Classifier Loss 0.11687235534191132, Total Loss 48.614559173583984\n",
      "8: Encoding Loss 5.6821794509887695, Transition Loss -0.7701115608215332, Classifier Loss 0.07360869646072388, Total Loss 41.45363998413086\n",
      "8: Encoding Loss 5.42266321182251, Transition Loss -3.0824437141418457, Classifier Loss 0.11082718521356583, Total Loss 43.61746597290039\n",
      "8: Encoding Loss 6.923373222351074, Transition Loss -3.0923964977264404, Classifier Loss 0.11713260412216187, Total Loss 53.25226593017578\n",
      "8: Encoding Loss 4.593067169189453, Transition Loss 0.09117195010185242, Classifier Loss 0.06370823085308075, Total Loss 33.9656982421875\n",
      "8: Encoding Loss 4.840723514556885, Transition Loss -1.8835455179214478, Classifier Loss 0.14935821294784546, Total Loss 43.979408264160156\n",
      "8: Encoding Loss 5.037450790405273, Transition Loss -1.3916593790054321, Classifier Loss 0.08278543502092361, Total Loss 38.50269317626953\n",
      "8: Encoding Loss 3.63499116897583, Transition Loss -1.916972041130066, Classifier Loss 0.07378353923559189, Total Loss 29.18753433227539\n",
      "8: Encoding Loss 5.042496681213379, Transition Loss -2.7910971641540527, Classifier Loss 0.13887359201908112, Total Loss 44.1412239074707\n",
      "8: Encoding Loss 6.591879367828369, Transition Loss -2.223379611968994, Classifier Loss 0.09829665720462799, Total Loss 49.38005447387695\n",
      "8: Encoding Loss 6.491299629211426, Transition Loss -2.8011441230773926, Classifier Loss 0.11212120950222015, Total Loss 50.15879821777344\n",
      "8: Encoding Loss 7.073792457580566, Transition Loss -1.0733098983764648, Classifier Loss 0.1538570076227188, Total Loss 57.828025817871094\n",
      "8: Encoding Loss 5.023055553436279, Transition Loss 0.38418352603912354, Classifier Loss 0.10035444051027298, Total Loss 40.32745361328125\n",
      "8: Encoding Loss 4.9677042961120605, Transition Loss -2.6464319229125977, Classifier Loss 0.07988259196281433, Total Loss 37.79343032836914\n",
      "8: Encoding Loss 5.0321831703186035, Transition Loss -1.8973339796066284, Classifier Loss 0.11515214294195175, Total Loss 41.70755386352539\n",
      "8: Encoding Loss 4.30718469619751, Transition Loss -1.7429325580596924, Classifier Loss 0.08149825036525726, Total Loss 33.99223709106445\n",
      "8: Encoding Loss 4.005827903747559, Transition Loss -1.9843196868896484, Classifier Loss 0.0929616391658783, Total Loss 33.33033752441406\n",
      "8: Encoding Loss 3.465949535369873, Transition Loss -2.256744861602783, Classifier Loss 0.05664844438433647, Total Loss 26.459640502929688\n",
      "8: Encoding Loss 3.529128313064575, Transition Loss -0.6657090187072754, Classifier Loss 0.10903654247522354, Total Loss 32.078155517578125\n",
      "8: Encoding Loss 5.099846839904785, Transition Loss -0.9212793707847595, Classifier Loss 0.12241244316101074, Total Loss 42.8399543762207\n",
      "8: Encoding Loss 6.304558753967285, Transition Loss -0.3518635034561157, Classifier Loss 0.10320346802473068, Total Loss 48.147560119628906\n",
      "8: Encoding Loss 4.195209503173828, Transition Loss -0.6627073287963867, Classifier Loss 0.10803709924221039, Total Loss 35.97470474243164\n",
      "8: Encoding Loss 6.914482116699219, Transition Loss -1.2287664413452148, Classifier Loss 0.11709029972553253, Total Loss 53.195430755615234\n",
      "8: Encoding Loss 6.4768829345703125, Transition Loss -0.8456796407699585, Classifier Loss 0.11952737718820572, Total Loss 50.81369400024414\n",
      "8: Encoding Loss 4.760567665100098, Transition Loss -1.7368104457855225, Classifier Loss 0.14961087703704834, Total Loss 43.523799896240234\n",
      "8: Encoding Loss 5.255608558654785, Transition Loss -0.6331525444984436, Classifier Loss 0.11209899932146072, Total Loss 42.74330139160156\n",
      "8: Encoding Loss 7.648622989654541, Transition Loss -2.2535247802734375, Classifier Loss 0.1659606397151947, Total Loss 62.486900329589844\n",
      "8: Encoding Loss 5.072378635406494, Transition Loss -1.7720012664794922, Classifier Loss 0.05316004529595375, Total Loss 35.749568939208984\n",
      "8: Encoding Loss 7.072686672210693, Transition Loss -0.98716801404953, Classifier Loss 0.19112052023410797, Total Loss 61.54777908325195\n",
      "8: Encoding Loss 5.7720417976379395, Transition Loss -0.5313454866409302, Classifier Loss 0.14284224808216095, Total Loss 48.916263580322266\n",
      "8: Encoding Loss 4.9881744384765625, Transition Loss -1.814614176750183, Classifier Loss 0.12333746254444122, Total Loss 42.26206970214844\n",
      "8: Encoding Loss 4.489284992218018, Transition Loss -2.764181613922119, Classifier Loss 0.15295056998729706, Total Loss 42.22966003417969\n",
      "8: Encoding Loss 6.5642595291137695, Transition Loss -2.0939695835113525, Classifier Loss 0.1394316703081131, Total Loss 53.32788848876953\n",
      "8: Encoding Loss 4.455075263977051, Transition Loss -0.8051796555519104, Classifier Loss 0.09435230493545532, Total Loss 36.16535949707031\n",
      "8: Encoding Loss 2.9673383235931396, Transition Loss -1.1937347650527954, Classifier Loss 0.07350192964076996, Total Loss 25.15374755859375\n",
      "8: Encoding Loss 2.5635008811950684, Transition Loss -1.7498443126678467, Classifier Loss 0.08117485046386719, Total Loss 23.497791290283203\n",
      "8: Encoding Loss 3.6481785774230957, Transition Loss -1.45907461643219, Classifier Loss 0.09351503849029541, Total Loss 31.239992141723633\n",
      "8: Encoding Loss 8.983457565307617, Transition Loss 0.6581997871398926, Classifier Loss 0.10545185953378677, Total Loss 64.70922088623047\n",
      "8: Encoding Loss 5.819262504577637, Transition Loss 0.18012720346450806, Classifier Loss 0.14762990176677704, Total Loss 49.75061798095703\n",
      "8: Encoding Loss 4.182537078857422, Transition Loss 0.5910160541534424, Classifier Loss 0.07095272839069366, Total Loss 32.42690658569336\n",
      "8: Encoding Loss 7.607243537902832, Transition Loss -0.46626487374305725, Classifier Loss 0.2558271586894989, Total Loss 71.22599792480469\n",
      "8: Encoding Loss 4.592301368713379, Transition Loss -1.6314233541488647, Classifier Loss 0.05735798180103302, Total Loss 33.28895568847656\n",
      "8: Encoding Loss 4.787661552429199, Transition Loss -2.288454532623291, Classifier Loss 0.12251763790845871, Total Loss 40.9768180847168\n",
      "8: Encoding Loss 5.729559898376465, Transition Loss -1.1968646049499512, Classifier Loss 0.14172212779521942, Total Loss 48.549095153808594\n",
      "8: Encoding Loss 4.944589138031006, Transition Loss -1.4677938222885132, Classifier Loss 0.05637967586517334, Total Loss 35.30491638183594\n",
      "8: Encoding Loss 4.464652061462402, Transition Loss -0.7328775525093079, Classifier Loss 0.06631725281476974, Total Loss 33.41934585571289\n",
      "8: Encoding Loss 5.875905990600586, Transition Loss -0.8334642648696899, Classifier Loss 0.11440707743167877, Total Loss 46.6958122253418\n",
      "8: Encoding Loss 3.8106396198272705, Transition Loss 0.13323412835597992, Classifier Loss 0.0411619171500206, Total Loss 27.033323287963867\n",
      "8: Encoding Loss 6.880967617034912, Transition Loss 0.5335056781768799, Classifier Loss 0.09497381001710892, Total Loss 50.99658966064453\n",
      "8: Encoding Loss 5.356302738189697, Transition Loss -1.9963154792785645, Classifier Loss 0.10763325542211533, Total Loss 42.90034484863281\n",
      "8: Encoding Loss 4.02184534072876, Transition Loss -1.6247472763061523, Classifier Loss 0.14187389612197876, Total Loss 38.317813873291016\n",
      "8: Encoding Loss 6.267545700073242, Transition Loss -1.9739489555358887, Classifier Loss 0.062113068997859955, Total Loss 43.815792083740234\n",
      "8: Encoding Loss 3.703453540802002, Transition Loss -1.0987777709960938, Classifier Loss 0.11705891788005829, Total Loss 33.92617416381836\n",
      "8: Encoding Loss 2.0898396968841553, Transition Loss -1.6348965167999268, Classifier Loss 0.09784455597400665, Total Loss 22.322839736938477\n",
      "8: Encoding Loss 2.372117042541504, Transition Loss -1.131220817565918, Classifier Loss 0.059828925877809525, Total Loss 20.21514320373535\n",
      "8: Encoding Loss 8.681467056274414, Transition Loss -2.129390239715576, Classifier Loss 0.11838290840387344, Total Loss 63.92624282836914\n",
      "8: Encoding Loss 4.6803107261657715, Transition Loss -1.747605800628662, Classifier Loss 0.05408027023077011, Total Loss 33.489192962646484\n",
      "8: Encoding Loss 2.6830177307128906, Transition Loss -1.962892770767212, Classifier Loss 0.0730317011475563, Total Loss 23.400489807128906\n",
      "8: Encoding Loss 6.3917107582092285, Transition Loss -1.3043639659881592, Classifier Loss 0.12406864017248154, Total Loss 50.75660705566406\n",
      "8: Encoding Loss 3.3460533618927, Transition Loss -1.0410945415496826, Classifier Loss 0.07651180028915405, Total Loss 27.72708511352539\n",
      "8: Encoding Loss 5.277928829193115, Transition Loss -0.7619422674179077, Classifier Loss 0.09942401200532913, Total Loss 41.60966873168945\n",
      "8: Encoding Loss 5.56223726272583, Transition Loss -0.5176064968109131, Classifier Loss 0.07529260963201523, Total Loss 40.90248107910156\n",
      "8: Encoding Loss 5.2945966720581055, Transition Loss -2.839034080505371, Classifier Loss 0.13033372163772583, Total Loss 44.7998161315918\n",
      "8: Encoding Loss 5.4402947425842285, Transition Loss -0.49841129779815674, Classifier Loss 0.12510846555233002, Total Loss 45.15241622924805\n",
      "8: Encoding Loss 5.082015037536621, Transition Loss -1.5660295486450195, Classifier Loss 0.1699151247739792, Total Loss 47.48297882080078\n",
      "8: Encoding Loss 4.662929534912109, Transition Loss -2.6011133193969727, Classifier Loss 0.0932508260011673, Total Loss 37.30162048339844\n",
      "8: Encoding Loss 3.6018643379211426, Transition Loss -1.2733315229415894, Classifier Loss 0.04077225551009178, Total Loss 25.687902450561523\n",
      "8: Encoding Loss 7.4654083251953125, Transition Loss -1.1147973537445068, Classifier Loss 0.15987661480903625, Total Loss 60.779666900634766\n",
      "8: Encoding Loss 6.714656829833984, Transition Loss -0.9328198432922363, Classifier Loss 0.15480244159698486, Total Loss 55.7678108215332\n",
      "8: Encoding Loss 5.271119117736816, Transition Loss -1.2058714628219604, Classifier Loss 0.15706734359264374, Total Loss 47.332969665527344\n",
      "8: Encoding Loss 3.6772656440734863, Transition Loss -1.8156222105026245, Classifier Loss 0.06790778785943985, Total Loss 28.853647232055664\n",
      "8: Encoding Loss 10.198099136352539, Transition Loss -1.1837470531463623, Classifier Loss 0.16931240260601044, Total Loss 78.1193618774414\n",
      "8: Encoding Loss 6.111510276794434, Transition Loss -2.600703477859497, Classifier Loss 0.07364559918642044, Total Loss 44.0325813293457\n",
      "8: Encoding Loss 4.892156600952148, Transition Loss -1.985957145690918, Classifier Loss 0.1484505832195282, Total Loss 44.19720458984375\n",
      "8: Encoding Loss 6.511165618896484, Transition Loss -2.1826601028442383, Classifier Loss 0.17725634574890137, Total Loss 56.79175567626953\n",
      "8: Encoding Loss 6.7342023849487305, Transition Loss -1.4555983543395996, Classifier Loss 0.1307297646999359, Total Loss 53.47760772705078\n",
      "8: Encoding Loss 5.467253684997559, Transition Loss -0.5971972942352295, Classifier Loss 0.09568214416503906, Total Loss 42.371498107910156\n",
      "8: Encoding Loss 7.180335998535156, Transition Loss -2.7139577865600586, Classifier Loss 0.08630742132663727, Total Loss 51.711669921875\n",
      "8: Encoding Loss 7.399806022644043, Transition Loss -1.4177762269973755, Classifier Loss 0.2016991823911667, Total Loss 64.56819152832031\n",
      "8: Encoding Loss 7.493227958679199, Transition Loss -2.2461040019989014, Classifier Loss 0.06180434301495552, Total Loss 51.1389045715332\n",
      "8: Encoding Loss 6.587472438812256, Transition Loss -0.5002473592758179, Classifier Loss 0.2089402973651886, Total Loss 60.418670654296875\n",
      "8: Encoding Loss 5.985543251037598, Transition Loss -1.8734735250473022, Classifier Loss 0.061928652226924896, Total Loss 42.105377197265625\n",
      "8: Encoding Loss 5.287335395812988, Transition Loss -1.325959324836731, Classifier Loss 0.06908803433179855, Total Loss 38.632286071777344\n",
      "8: Encoding Loss 4.101805686950684, Transition Loss -2.5330042839050293, Classifier Loss 0.06316538900136948, Total Loss 30.926362991333008\n",
      "8: Encoding Loss 4.187117099761963, Transition Loss -1.0351884365081787, Classifier Loss 0.10074100643396378, Total Loss 35.196388244628906\n",
      "8: Encoding Loss 3.927396774291992, Transition Loss -2.9806065559387207, Classifier Loss 0.07458298653364182, Total Loss 31.021486282348633\n",
      "8: Encoding Loss 7.503180027008057, Transition Loss -0.9109683036804199, Classifier Loss 0.10319806635379791, Total Loss 55.338523864746094\n",
      "8: Encoding Loss 4.46903133392334, Transition Loss -1.8201631307601929, Classifier Loss 0.10581550747156143, Total Loss 37.39501190185547\n",
      "8: Encoding Loss 2.914522171020508, Transition Loss -1.2580509185791016, Classifier Loss 0.04754588380455971, Total Loss 22.24121856689453\n",
      "8: Encoding Loss 7.311704158782959, Transition Loss -2.2698731422424316, Classifier Loss 0.07397996634244919, Total Loss 51.26731491088867\n",
      "8: Encoding Loss 7.908935546875, Transition Loss -1.4445524215698242, Classifier Loss 0.07783524692058563, Total Loss 55.2365608215332\n",
      "8: Encoding Loss 5.821810245513916, Transition Loss -0.4863298535346985, Classifier Loss 0.13014379143714905, Total Loss 47.94504928588867\n",
      "8: Encoding Loss 5.986350059509277, Transition Loss -1.7393395900726318, Classifier Loss 0.04237537831068039, Total Loss 40.154945373535156\n",
      "8: Encoding Loss 4.881433486938477, Transition Loss -1.043669581413269, Classifier Loss 0.11024702340364456, Total Loss 40.312889099121094\n",
      "8: Encoding Loss 5.59282112121582, Transition Loss -1.6237969398498535, Classifier Loss 0.09839262813329697, Total Loss 43.39554214477539\n",
      "8: Encoding Loss 5.639512538909912, Transition Loss -1.164092779159546, Classifier Loss 0.096446692943573, Total Loss 43.48128128051758\n",
      "8: Encoding Loss 4.1016340255737305, Transition Loss -0.9020090103149414, Classifier Loss 0.08585964888334274, Total Loss 33.19540786743164\n",
      "8: Encoding Loss 6.030271053314209, Transition Loss -2.0646493434906006, Classifier Loss 0.15896418690681458, Total Loss 52.07722473144531\n",
      "8: Encoding Loss 3.7713234424591064, Transition Loss -1.1118760108947754, Classifier Loss 0.07846154272556305, Total Loss 30.473649978637695\n",
      "8: Encoding Loss 5.131948947906494, Transition Loss -0.35985901951789856, Classifier Loss 0.04673232510685921, Total Loss 35.46478271484375\n",
      "8: Encoding Loss 6.975565433502197, Transition Loss -1.4221408367156982, Classifier Loss 0.1762232929468155, Total Loss 59.475154876708984\n",
      "8: Encoding Loss 5.145333290100098, Transition Loss -0.9068290591239929, Classifier Loss 0.060327768325805664, Total Loss 36.904415130615234\n",
      "8: Encoding Loss 5.552523612976074, Transition Loss -3.6988487243652344, Classifier Loss 0.07766257226467133, Total Loss 41.07992172241211\n",
      "8: Encoding Loss 4.385584831237793, Transition Loss -1.7293519973754883, Classifier Loss 0.050652824342250824, Total Loss 31.378101348876953\n",
      "8: Encoding Loss 5.372003078460693, Transition Loss -0.2659539580345154, Classifier Loss 0.164510577917099, Total Loss 48.6829719543457\n",
      "8: Encoding Loss 3.749554395675659, Transition Loss -1.3814737796783447, Classifier Loss 0.1013733297586441, Total Loss 32.63410568237305\n",
      "8: Encoding Loss 8.010836601257324, Transition Loss -1.9748985767364502, Classifier Loss 0.16605353355407715, Total Loss 64.66958618164062\n",
      "8: Encoding Loss 5.245847225189209, Transition Loss -0.8117052912712097, Classifier Loss 0.06624270975589752, Total Loss 38.099029541015625\n",
      "8: Encoding Loss 5.006030559539795, Transition Loss -2.6447348594665527, Classifier Loss 0.10659483075141907, Total Loss 40.694610595703125\n",
      "8: Encoding Loss 4.67584228515625, Transition Loss -2.901414155960083, Classifier Loss 0.1313534826040268, Total Loss 41.18924331665039\n",
      "8: Encoding Loss 4.67229700088501, Transition Loss -1.5841290950775146, Classifier Loss 0.13139007985591888, Total Loss 41.172157287597656\n",
      "8: Encoding Loss 5.870774269104004, Transition Loss -1.369379997253418, Classifier Loss 0.09798116981983185, Total Loss 45.022216796875\n",
      "8: Encoding Loss 5.133471965789795, Transition Loss -1.9243061542510986, Classifier Loss 0.17351458966732025, Total Loss 48.151519775390625\n",
      "8: Encoding Loss 6.183199405670166, Transition Loss -1.3777979612350464, Classifier Loss 0.036862026900053024, Total Loss 40.78485107421875\n",
      "8: Encoding Loss 4.767076015472412, Transition Loss 0.39700189232826233, Classifier Loss 0.055614642798900604, Total Loss 34.322723388671875\n",
      "8: Encoding Loss 5.9140143394470215, Transition Loss -1.971116542816162, Classifier Loss 0.05863575264811516, Total Loss 41.34687423706055\n",
      "8: Encoding Loss 4.292927265167236, Transition Loss -1.531764030456543, Classifier Loss 0.07500983029603958, Total Loss 33.2579345703125\n",
      "8: Encoding Loss 6.653820991516113, Transition Loss -0.4073535203933716, Classifier Loss 0.12315844744443893, Total Loss 52.238609313964844\n",
      "8: Encoding Loss 4.8230671882629395, Transition Loss -2.3765358924865723, Classifier Loss 0.04456617683172226, Total Loss 33.394073486328125\n",
      "8: Encoding Loss 7.300005912780762, Transition Loss -1.6369576454162598, Classifier Loss 0.22434593737125397, Total Loss 66.23397827148438\n",
      "8: Encoding Loss 5.49891996383667, Transition Loss -1.138584017753601, Classifier Loss 0.12352731823921204, Total Loss 45.34579849243164\n",
      "8: Encoding Loss 5.306351661682129, Transition Loss -1.8849132061004639, Classifier Loss 0.0635102391242981, Total Loss 38.188377380371094\n",
      "8: Encoding Loss 5.084238529205322, Transition Loss -0.07159388065338135, Classifier Loss 0.07595501095056534, Total Loss 38.10090255737305\n",
      "8: Encoding Loss 5.3332719802856445, Transition Loss -1.1535813808441162, Classifier Loss 0.05980442836880684, Total Loss 37.9796142578125\n",
      "8: Encoding Loss 5.476860046386719, Transition Loss -0.33394181728363037, Classifier Loss 0.12338685244321823, Total Loss 45.199710845947266\n",
      "8: Encoding Loss 7.043219566345215, Transition Loss -0.4045797288417816, Classifier Loss 0.05130482837557793, Total Loss 47.38964080810547\n",
      "8: Encoding Loss 4.450900077819824, Transition Loss -1.7125589847564697, Classifier Loss 0.1816350221633911, Total Loss 44.86821746826172\n",
      "8: Encoding Loss 5.235920429229736, Transition Loss -0.39478227496147156, Classifier Loss 0.0675392895936966, Total Loss 38.16929626464844\n",
      "8: Encoding Loss 4.44155216217041, Transition Loss -0.44679656624794006, Classifier Loss 0.10544905066490173, Total Loss 37.19403839111328\n",
      "8: Encoding Loss 4.766223907470703, Transition Loss -1.753415822982788, Classifier Loss 0.04306182637810707, Total Loss 32.90282440185547\n",
      "8: Encoding Loss 4.8899641036987305, Transition Loss -2.044313669204712, Classifier Loss 0.1781262755393982, Total Loss 47.15159606933594\n",
      "8: Encoding Loss 4.959288597106934, Transition Loss -2.131159782409668, Classifier Loss 0.14677943289279938, Total Loss 44.43282699584961\n",
      "8: Encoding Loss 3.594438076019287, Transition Loss -2.8914589881896973, Classifier Loss 0.07843951880931854, Total Loss 29.409425735473633\n",
      "8: Encoding Loss 4.04878568649292, Transition Loss -0.633962869644165, Classifier Loss 0.12590362131595612, Total Loss 36.88282775878906\n",
      "8: Encoding Loss 5.7200469970703125, Transition Loss -0.5730345249176025, Classifier Loss 0.1262650191783905, Total Loss 46.946556091308594\n",
      "8: Encoding Loss 3.5610804557800293, Transition Loss -2.2599539756774902, Classifier Loss 0.06292174756526947, Total Loss 27.65775489807129\n",
      "8: Encoding Loss 5.190009593963623, Transition Loss 0.20336002111434937, Classifier Loss 0.042394064366817474, Total Loss 35.46080780029297\n",
      "8: Encoding Loss 5.697880744934082, Transition Loss -2.2410728931427, Classifier Loss 0.08786511421203613, Total Loss 42.972900390625\n",
      "8: Encoding Loss 4.222348213195801, Transition Loss -0.8816202878952026, Classifier Loss 0.06496477127075195, Total Loss 31.83021354675293\n",
      "8: Encoding Loss 8.022247314453125, Transition Loss 0.5531114339828491, Classifier Loss 0.24989396333694458, Total Loss 73.34413146972656\n",
      "8: Encoding Loss 5.978459358215332, Transition Loss -1.6169935464859009, Classifier Loss 0.059288229793310165, Total Loss 41.79893112182617\n",
      "8: Encoding Loss 7.1345534324646, Transition Loss 0.4019578695297241, Classifier Loss 0.0872303768992424, Total Loss 51.69114303588867\n",
      "8: Encoding Loss 5.087990760803223, Transition Loss -1.7121624946594238, Classifier Loss 0.15397118031978607, Total Loss 45.92437744140625\n",
      "8: Encoding Loss 3.5551891326904297, Transition Loss -1.1661643981933594, Classifier Loss 0.10338400304317474, Total Loss 31.6690673828125\n",
      "8: Encoding Loss 6.76540994644165, Transition Loss -1.6887102127075195, Classifier Loss 0.12697750329971313, Total Loss 53.28953552246094\n",
      "8: Encoding Loss 8.788297653198242, Transition Loss -2.875971794128418, Classifier Loss 0.18703104555606842, Total Loss 71.4317398071289\n",
      "8: Encoding Loss 7.242448329925537, Transition Loss -1.4874765872955322, Classifier Loss 0.2941572964191437, Total Loss 72.86982727050781\n",
      "8: Encoding Loss 4.2121100425720215, Transition Loss -1.3678514957427979, Classifier Loss 0.11403633654117584, Total Loss 36.675750732421875\n",
      "8: Encoding Loss 7.872443199157715, Transition Loss 0.3506026864051819, Classifier Loss 0.09528876096010208, Total Loss 56.903778076171875\n",
      "8: Encoding Loss 9.155885696411133, Transition Loss -1.5307018756866455, Classifier Loss 0.3225201666355133, Total Loss 87.18672180175781\n",
      "8: Encoding Loss 6.734572410583496, Transition Loss -0.8373653888702393, Classifier Loss 0.12448478490114212, Total Loss 52.8555793762207\n",
      "8: Encoding Loss 7.433629512786865, Transition Loss -0.21088692545890808, Classifier Loss 0.1329687386751175, Total Loss 57.8985710144043\n",
      "8: Encoding Loss 7.307997226715088, Transition Loss -0.5479419827461243, Classifier Loss 0.10760490596294403, Total Loss 54.60825729370117\n",
      "8: Encoding Loss 5.328552722930908, Transition Loss -1.9681522846221924, Classifier Loss 0.09581109881401062, Total Loss 41.55164337158203\n",
      "8: Encoding Loss 4.848641872406006, Transition Loss -1.9316015243530273, Classifier Loss 0.12498494982719421, Total Loss 41.58957290649414\n",
      "8: Encoding Loss 5.948873996734619, Transition Loss -3.412604570388794, Classifier Loss 0.13227584958076477, Total Loss 48.919464111328125\n",
      "8: Encoding Loss 4.796721458435059, Transition Loss -2.2674005031585693, Classifier Loss 0.08176802843809128, Total Loss 36.95622634887695\n",
      "8: Encoding Loss 4.381031513214111, Transition Loss -1.2779449224472046, Classifier Loss 0.06374499201774597, Total Loss 32.660179138183594\n",
      "8: Encoding Loss 5.089005470275879, Transition Loss -1.8278460502624512, Classifier Loss 0.17094287276268005, Total Loss 47.627586364746094\n",
      "8: Encoding Loss 3.1154489517211914, Transition Loss -1.922021746635437, Classifier Loss 0.09359178692102432, Total Loss 28.051103591918945\n",
      "8: Encoding Loss 4.217199802398682, Transition Loss -1.5225858688354492, Classifier Loss 0.11522821336984634, Total Loss 36.825408935546875\n",
      "8: Encoding Loss 6.387788772583008, Transition Loss -1.6522130966186523, Classifier Loss 0.2089567333459854, Total Loss 59.221744537353516\n",
      "8: Encoding Loss 5.391181945800781, Transition Loss -0.4123814105987549, Classifier Loss 0.15574166178703308, Total Loss 47.92109298706055\n",
      "8: Encoding Loss 3.9252567291259766, Transition Loss -1.9053616523742676, Classifier Loss 0.12522399425506592, Total Loss 36.073177337646484\n",
      "8: Encoding Loss 5.551846027374268, Transition Loss -2.2572531700134277, Classifier Loss 0.08275534212589264, Total Loss 41.58570861816406\n",
      "8: Encoding Loss 4.133201599121094, Transition Loss -0.6416848301887512, Classifier Loss 0.061008911579847336, Total Loss 30.899845123291016\n",
      "8: Encoding Loss 5.453089237213135, Transition Loss -1.3862441778182983, Classifier Loss 0.039719995111227036, Total Loss 36.68998336791992\n",
      "8: Encoding Loss 4.427680015563965, Transition Loss -1.4252794981002808, Classifier Loss 0.05140979215502739, Total Loss 31.706491470336914\n",
      "8: Encoding Loss 3.758033037185669, Transition Loss -0.5392692685127258, Classifier Loss 0.1291993260383606, Total Loss 35.46791458129883\n",
      "8: Encoding Loss 5.898711204528809, Transition Loss -0.7524833679199219, Classifier Loss 0.13760258257389069, Total Loss 49.152225494384766\n",
      "8: Encoding Loss 7.019686698913574, Transition Loss -0.15049441158771515, Classifier Loss 0.14033293724060059, Total Loss 56.1513557434082\n",
      "8: Encoding Loss 6.4643354415893555, Transition Loss -0.7115170359611511, Classifier Loss 0.10903264582157135, Total Loss 49.68899154663086\n",
      "8: Encoding Loss 3.055661678314209, Transition Loss -2.2901675701141357, Classifier Loss 0.09718192368745804, Total Loss 28.051246643066406\n",
      "8: Encoding Loss 7.174233913421631, Transition Loss -1.504448652267456, Classifier Loss 0.1555362343788147, Total Loss 58.598426818847656\n",
      "8: Encoding Loss 4.605759143829346, Transition Loss -2.507122755050659, Classifier Loss 0.10262057185173035, Total Loss 37.89561080932617\n",
      "8: Encoding Loss 6.0411295890808105, Transition Loss -0.9762541651725769, Classifier Loss 0.09276525676250458, Total Loss 45.522918701171875\n",
      "8: Encoding Loss 7.10819149017334, Transition Loss -2.067333221435547, Classifier Loss 0.18648874759674072, Total Loss 61.29719924926758\n",
      "8: Encoding Loss 5.558135032653809, Transition Loss -2.0443146228790283, Classifier Loss 0.15250714123249054, Total Loss 48.59870910644531\n",
      "8: Encoding Loss 7.381810665130615, Transition Loss -0.18659017980098724, Classifier Loss 0.14736533164978027, Total Loss 59.02732467651367\n",
      "8: Encoding Loss 4.860694408416748, Transition Loss 0.13984699547290802, Classifier Loss 0.10259657353162766, Total Loss 39.47976303100586\n",
      "8: Encoding Loss 4.8749566078186035, Transition Loss -2.0080726146698, Classifier Loss 0.09389632940292358, Total Loss 38.63856887817383\n",
      "8: Encoding Loss 6.914375305175781, Transition Loss -0.7194972038269043, Classifier Loss 0.1389976143836975, Total Loss 55.38572692871094\n",
      "8: Encoding Loss 6.359139442443848, Transition Loss -1.3311481475830078, Classifier Loss 0.10808595269918442, Total Loss 48.96289825439453\n",
      "8: Encoding Loss 5.970490455627441, Transition Loss -0.806276798248291, Classifier Loss 0.115062415599823, Total Loss 47.328861236572266\n",
      "8: Encoding Loss 4.331839084625244, Transition Loss 0.20752373337745667, Classifier Loss 0.12138350307941437, Total Loss 38.21239471435547\n",
      "8: Encoding Loss 3.3021016120910645, Transition Loss -0.8396773338317871, Classifier Loss 0.0619896799325943, Total Loss 26.011241912841797\n",
      "8: Encoding Loss 5.653347969055176, Transition Loss -1.7759038209915161, Classifier Loss 0.07469256222248077, Total Loss 41.38863754272461\n",
      "8: Encoding Loss 4.465517520904541, Transition Loss -1.3011257648468018, Classifier Loss 0.10057146847248077, Total Loss 36.849735260009766\n",
      "8: Encoding Loss 4.727604866027832, Transition Loss -0.7241702675819397, Classifier Loss 0.20046353340148926, Total Loss 48.41168975830078\n",
      "8: Encoding Loss 4.350152015686035, Transition Loss -2.227932929992676, Classifier Loss 0.04599935933947563, Total Loss 30.6999568939209\n",
      "8: Encoding Loss 4.409049034118652, Transition Loss -2.6578547954559326, Classifier Loss 0.12065708637237549, Total Loss 38.51893997192383\n",
      "8: Encoding Loss 1.9792991876602173, Transition Loss -0.6963632106781006, Classifier Loss 0.06131763756275177, Total Loss 18.007280349731445\n",
      "8: Encoding Loss 4.93727970123291, Transition Loss -0.7925339937210083, Classifier Loss 0.059711869806051254, Total Loss 35.594547271728516\n",
      "8: Encoding Loss 4.603926658630371, Transition Loss -1.5035048723220825, Classifier Loss 0.13916975259780884, Total Loss 41.53993225097656\n",
      "8: Encoding Loss 3.7212002277374268, Transition Loss -0.711682915687561, Classifier Loss 0.06578654795885086, Total Loss 28.90557289123535\n",
      "8: Encoding Loss 4.730987071990967, Transition Loss -0.17533713579177856, Classifier Loss 0.03123203106224537, Total Loss 31.509056091308594\n",
      "8: Encoding Loss 7.7391533851623535, Transition Loss -2.136355400085449, Classifier Loss 0.20341837406158447, Total Loss 66.7759017944336\n",
      "8: Encoding Loss 5.055229663848877, Transition Loss -0.6019856929779053, Classifier Loss 0.19002346694469452, Total Loss 49.3334846496582\n",
      "8: Encoding Loss 5.068495750427246, Transition Loss -1.1397230625152588, Classifier Loss 0.19805164635181427, Total Loss 50.21568298339844\n",
      "8: Encoding Loss 4.602108955383301, Transition Loss -1.8054653406143188, Classifier Loss 0.08885237574577332, Total Loss 36.497169494628906\n",
      "8: Encoding Loss 6.5416579246521, Transition Loss -1.7135462760925293, Classifier Loss 0.15161174535751343, Total Loss 54.410438537597656\n",
      "8: Encoding Loss 5.6819539070129395, Transition Loss -1.0194530487060547, Classifier Loss 0.09926391392946243, Total Loss 44.01770782470703\n",
      "8: Encoding Loss 5.952834129333496, Transition Loss -0.5153934359550476, Classifier Loss 0.07138977944850922, Total Loss 42.855777740478516\n",
      "8: Encoding Loss 4.166123390197754, Transition Loss -1.9515917301177979, Classifier Loss 0.10838164389133453, Total Loss 35.83412170410156\n",
      "8: Encoding Loss 6.256040573120117, Transition Loss -0.07246538996696472, Classifier Loss 0.08834364265203476, Total Loss 46.370574951171875\n",
      "8: Encoding Loss 4.833874702453613, Transition Loss -1.6032898426055908, Classifier Loss 0.09408117085695267, Total Loss 38.41072463989258\n",
      "8: Encoding Loss 5.957371234893799, Transition Loss -1.4053544998168945, Classifier Loss 0.11238546669483185, Total Loss 46.98221206665039\n",
      "8: Encoding Loss 6.230639934539795, Transition Loss -0.6438074111938477, Classifier Loss 0.09055346250534058, Total Loss 46.43893051147461\n",
      "8: Encoding Loss 4.825202465057373, Transition Loss -1.8326414823532104, Classifier Loss 0.05388619005680084, Total Loss 34.33910369873047\n",
      "8: Encoding Loss 3.9963841438293457, Transition Loss -1.2802226543426514, Classifier Loss 0.09195195883512497, Total Loss 33.17298889160156\n",
      "8: Encoding Loss 3.6323482990264893, Transition Loss -1.206174612045288, Classifier Loss 0.08668909966945648, Total Loss 30.4625186920166\n",
      "8: Encoding Loss 7.503933906555176, Transition Loss -2.4824111461639404, Classifier Loss 0.09323889017105103, Total Loss 54.34650421142578\n",
      "8: Encoding Loss 4.042732238769531, Transition Loss -3.4469048976898193, Classifier Loss 0.06495758146047592, Total Loss 30.750774383544922\n",
      "8: Encoding Loss 3.1025469303131104, Transition Loss -1.1814073324203491, Classifier Loss 0.09978410601615906, Total Loss 28.593219757080078\n",
      "8: Encoding Loss 10.124558448791504, Transition Loss 0.36109811067581177, Classifier Loss 0.1806487888097763, Total Loss 78.95667266845703\n",
      "8: Encoding Loss 6.614622592926025, Transition Loss -0.12440337240695953, Classifier Loss 0.1372944563627243, Total Loss 53.41713333129883\n",
      "8: Encoding Loss 4.672514915466309, Transition Loss -2.6522865295410156, Classifier Loss 0.07935599237680435, Total Loss 35.96963119506836\n",
      "8: Encoding Loss 4.100752830505371, Transition Loss -0.68327796459198, Classifier Loss 0.07961582392454147, Total Loss 32.565826416015625\n",
      "8: Encoding Loss 7.209939002990723, Transition Loss -2.5519919395446777, Classifier Loss 0.21266476809978485, Total Loss 64.52509307861328\n",
      "8: Encoding Loss 4.725579738616943, Transition Loss -1.1316237449645996, Classifier Loss 0.07924631983041763, Total Loss 36.27765655517578\n",
      "8: Encoding Loss 4.3646464347839355, Transition Loss -1.1339335441589355, Classifier Loss 0.07613126933574677, Total Loss 33.80055236816406\n",
      "8: Encoding Loss 4.178545951843262, Transition Loss -1.5993075370788574, Classifier Loss 0.13300880789756775, Total Loss 38.371517181396484\n",
      "8: Encoding Loss 6.256112575531006, Transition Loss -0.5890970826148987, Classifier Loss 0.045095354318618774, Total Loss 42.04597854614258\n",
      "8: Encoding Loss 4.730380058288574, Transition Loss -1.4474222660064697, Classifier Loss 0.08143937587738037, Total Loss 36.52564239501953\n",
      "8: Encoding Loss 6.769667148590088, Transition Loss -2.0385701656341553, Classifier Loss 0.09334127604961395, Total Loss 49.951316833496094\n",
      "8: Encoding Loss 5.537578105926514, Transition Loss -0.5891932249069214, Classifier Loss 0.07358914613723755, Total Loss 40.58414840698242\n",
      "8: Encoding Loss 5.651066780090332, Transition Loss -0.26493921875953674, Classifier Loss 0.14085009694099426, Total Loss 47.99130630493164\n",
      "8: Encoding Loss 7.3617706298828125, Transition Loss -1.03114652633667, Classifier Loss 0.05118707939982414, Total Loss 49.28892135620117\n",
      "8: Encoding Loss 5.806197643280029, Transition Loss -1.0960631370544434, Classifier Loss 0.19499213993549347, Total Loss 54.33596420288086\n",
      "8: Encoding Loss 5.207097053527832, Transition Loss -0.5371341705322266, Classifier Loss 0.11100344359874725, Total Loss 42.34271240234375\n",
      "8: Encoding Loss 3.2805163860321045, Transition Loss -0.9571458697319031, Classifier Loss 0.06755930185317993, Total Loss 26.43864631652832\n",
      "8: Encoding Loss 5.034231185913086, Transition Loss -1.5302093029022217, Classifier Loss 0.08479943871498108, Total Loss 38.684722900390625\n",
      "8: Encoding Loss 5.086544036865234, Transition Loss -0.35722100734710693, Classifier Loss 0.09159715473651886, Total Loss 39.67884063720703\n",
      "8: Encoding Loss 4.512124061584473, Transition Loss 0.38578224182128906, Classifier Loss 0.10808378458023071, Total Loss 38.03543472290039\n",
      "8: Encoding Loss 4.277024269104004, Transition Loss -1.7464994192123413, Classifier Loss 0.07832273840904236, Total Loss 33.49372100830078\n",
      "8: Encoding Loss 3.208405017852783, Transition Loss -2.7521395683288574, Classifier Loss 0.0829375833272934, Total Loss 27.543088912963867\n",
      "8: Encoding Loss 5.956662654876709, Transition Loss -2.0180628299713135, Classifier Loss 0.11257863789796829, Total Loss 46.997032165527344\n",
      "8: Encoding Loss 5.145804405212402, Transition Loss -2.224729061126709, Classifier Loss 0.16244792938232422, Total Loss 47.11873245239258\n",
      "8: Encoding Loss 5.359087944030762, Transition Loss -0.007500998675823212, Classifier Loss 0.10250982642173767, Total Loss 42.4055061340332\n",
      "8: Encoding Loss 5.270278453826904, Transition Loss -1.0067003965377808, Classifier Loss 0.14026975631713867, Total Loss 45.64824295043945\n",
      "8: Encoding Loss 6.210144996643066, Transition Loss -0.572866678237915, Classifier Loss 0.17448949813842773, Total Loss 54.7095947265625\n",
      "8: Encoding Loss 5.509582996368408, Transition Loss -0.6302124857902527, Classifier Loss 0.12912818789482117, Total Loss 45.97006607055664\n",
      "8: Encoding Loss 5.059437274932861, Transition Loss -1.1697593927383423, Classifier Loss 0.09801992028951645, Total Loss 40.158145904541016\n",
      "8: Encoding Loss 5.229126453399658, Transition Loss -2.9703121185302734, Classifier Loss 0.07895725965499878, Total Loss 39.2692985534668\n",
      "8: Encoding Loss 4.6210150718688965, Transition Loss -0.8606405854225159, Classifier Loss 0.059696536511182785, Total Loss 33.69540023803711\n",
      "8: Encoding Loss 4.984963417053223, Transition Loss -1.4692336320877075, Classifier Loss 0.11154820024967194, Total Loss 41.06401443481445\n",
      "8: Encoding Loss 5.580875396728516, Transition Loss -2.876800298690796, Classifier Loss 0.05680853873491287, Total Loss 39.164955139160156\n",
      "8: Encoding Loss 6.9067888259887695, Transition Loss -0.8256487846374512, Classifier Loss 0.057119689881801605, Total Loss 47.15237045288086\n",
      "8: Encoding Loss 3.2628657817840576, Transition Loss -0.7721616625785828, Classifier Loss 0.07281269878149033, Total Loss 26.858158111572266\n",
      "8: Encoding Loss 5.579984664916992, Transition Loss -1.465111494064331, Classifier Loss 0.05608685687184334, Total Loss 39.08800506591797\n",
      "8: Encoding Loss 6.068963527679443, Transition Loss -1.2435302734375, Classifier Loss 0.07266465574502945, Total Loss 43.679752349853516\n",
      "8: Encoding Loss 3.7930965423583984, Transition Loss -0.5838757753372192, Classifier Loss 0.036465004086494446, Total Loss 26.40484619140625\n",
      "8: Encoding Loss 5.120165824890137, Transition Loss -2.8035454750061035, Classifier Loss 0.13122864067554474, Total Loss 43.84273910522461\n",
      "8: Encoding Loss 6.853875637054443, Transition Loss -0.2888055443763733, Classifier Loss 0.10268915444612503, Total Loss 51.392059326171875\n",
      "8: Encoding Loss 6.625370979309082, Transition Loss -0.9889719486236572, Classifier Loss 0.1350005567073822, Total Loss 53.25188446044922\n",
      "8: Encoding Loss 5.593345642089844, Transition Loss -1.4614285230636597, Classifier Loss 0.06953296065330505, Total Loss 40.512786865234375\n",
      "8: Encoding Loss 5.407015323638916, Transition Loss -1.5911688804626465, Classifier Loss 0.1003306433558464, Total Loss 42.47452163696289\n",
      "8: Encoding Loss 5.330594539642334, Transition Loss -0.7523225545883179, Classifier Loss 0.1678137332201004, Total Loss 48.76464080810547\n",
      "8: Encoding Loss 6.274389266967773, Transition Loss -1.7063111066818237, Classifier Loss 0.07056006789207458, Total Loss 44.70166015625\n",
      "8: Encoding Loss 3.37908673286438, Transition Loss -1.2544772624969482, Classifier Loss 0.14311733841896057, Total Loss 34.585750579833984\n",
      "8: Encoding Loss 8.433629989624023, Transition Loss 0.3911195397377014, Classifier Loss 0.14078955352306366, Total Loss 64.83718872070312\n",
      "8: Encoding Loss 4.931795597076416, Transition Loss -1.142174243927002, Classifier Loss 0.04843086749315262, Total Loss 34.43340301513672\n",
      "8: Encoding Loss 3.1254448890686035, Transition Loss -2.364396333694458, Classifier Loss 0.06331939250230789, Total Loss 25.083663940429688\n",
      "8: Encoding Loss 6.027392387390137, Transition Loss -0.22724296152591705, Classifier Loss 0.05461324006319046, Total Loss 41.625587463378906\n",
      "8: Encoding Loss 2.6371333599090576, Transition Loss -0.9745328426361084, Classifier Loss 0.044624704867601395, Total Loss 20.284881591796875\n",
      "8: Encoding Loss 6.966296195983887, Transition Loss -1.8027311563491821, Classifier Loss 0.1316477209329605, Total Loss 54.961830139160156\n",
      "8: Encoding Loss 6.441175937652588, Transition Loss -1.0563806295394897, Classifier Loss 0.10832250118255615, Total Loss 49.478885650634766\n",
      "8: Encoding Loss 5.401330947875977, Transition Loss -2.1836981773376465, Classifier Loss 0.0726955235004425, Total Loss 39.67666244506836\n",
      "8: Encoding Loss 3.6152889728546143, Transition Loss -1.0878413915634155, Classifier Loss 0.1329593062400818, Total Loss 34.98722839355469\n",
      "8: Encoding Loss 4.612607479095459, Transition Loss -1.5295751094818115, Classifier Loss 0.09526985883712769, Total Loss 37.202022552490234\n",
      "8: Encoding Loss 5.527395725250244, Transition Loss -1.6454081535339355, Classifier Loss 0.08855792880058289, Total Loss 42.019508361816406\n",
      "8: Encoding Loss 3.4930527210235596, Transition Loss -0.8425402641296387, Classifier Loss 0.08890990167856216, Total Loss 29.848970413208008\n",
      "8: Encoding Loss 6.071887493133545, Transition Loss -2.152773857116699, Classifier Loss 0.18443721532821655, Total Loss 54.87418746948242\n",
      "8: Encoding Loss 4.1498918533325195, Transition Loss -2.009167194366455, Classifier Loss 0.0652453750371933, Total Loss 31.423086166381836\n",
      "8: Encoding Loss 3.478148937225342, Transition Loss -0.7287544012069702, Classifier Loss 0.06671862304210663, Total Loss 27.540464401245117\n",
      "8: Encoding Loss 5.311482906341553, Transition Loss 0.028934329748153687, Classifier Loss 0.14658518135547638, Total Loss 46.53899002075195\n",
      "8: Encoding Loss 4.787775993347168, Transition Loss -1.737971305847168, Classifier Loss 0.08367281407117844, Total Loss 37.09324645996094\n",
      "8: Encoding Loss 7.896670341491699, Transition Loss -0.6153614521026611, Classifier Loss 0.10842575132846832, Total Loss 58.22235107421875\n",
      "8: Encoding Loss 4.062115669250488, Transition Loss -2.5453834533691406, Classifier Loss 0.20165163278579712, Total Loss 44.53683853149414\n",
      "8: Encoding Loss 6.021240234375, Transition Loss -1.1264491081237793, Classifier Loss 0.0899742990732193, Total Loss 45.124420166015625\n",
      "8: Encoding Loss 4.23177433013916, Transition Loss -1.2322206497192383, Classifier Loss 0.04783215373754501, Total Loss 30.173370361328125\n",
      "8: Encoding Loss 8.315670013427734, Transition Loss -0.2292681336402893, Classifier Loss 0.15024009346961975, Total Loss 64.91793823242188\n",
      "8: Encoding Loss 5.350656509399414, Transition Loss -1.2796999216079712, Classifier Loss 0.13276246190071106, Total Loss 45.37967300415039\n",
      "8: Encoding Loss 4.3729119300842285, Transition Loss -1.0900294780731201, Classifier Loss 0.08530186116695404, Total Loss 34.7672233581543\n",
      "8: Encoding Loss 4.875914096832275, Transition Loss -2.813246726989746, Classifier Loss 0.09419780969619751, Total Loss 38.67414093017578\n",
      "8: Encoding Loss 3.440598249435425, Transition Loss 0.6625697612762451, Classifier Loss 0.11369052529335022, Total Loss 32.27766799926758\n",
      "8: Encoding Loss 3.339322566986084, Transition Loss -0.2146027684211731, Classifier Loss 0.05239749327301979, Total Loss 25.27560043334961\n",
      "8: Encoding Loss 4.697405815124512, Transition Loss -2.520195960998535, Classifier Loss 0.14092347025871277, Total Loss 42.27577590942383\n",
      "8: Encoding Loss 5.705811500549316, Transition Loss -1.7840023040771484, Classifier Loss 0.14387787878513336, Total Loss 48.621944427490234\n",
      "8: Encoding Loss 4.236568927764893, Transition Loss -0.6951828002929688, Classifier Loss 0.09246852993965149, Total Loss 34.66598892211914\n",
      "8: Encoding Loss 4.643059730529785, Transition Loss -0.648666262626648, Classifier Loss 0.07510566711425781, Total Loss 35.36866760253906\n",
      "8: Encoding Loss 4.338923931121826, Transition Loss -1.2183400392532349, Classifier Loss 0.04206683859229088, Total Loss 30.2397403717041\n",
      "8: Encoding Loss 6.954780578613281, Transition Loss -0.1743512749671936, Classifier Loss 0.25347188115119934, Total Loss 67.0758056640625\n",
      "8: Encoding Loss 4.599038124084473, Transition Loss -1.2569794654846191, Classifier Loss 0.07292529940605164, Total Loss 34.886253356933594\n",
      "8: Encoding Loss 6.751599311828613, Transition Loss -1.5884404182434082, Classifier Loss 0.08222006261348724, Total Loss 48.7309684753418\n",
      "8: Encoding Loss 4.128651142120361, Transition Loss -0.06796243786811829, Classifier Loss 0.06313876062631607, Total Loss 31.085758209228516\n",
      "8: Encoding Loss 5.5740227699279785, Transition Loss -1.6456772089004517, Classifier Loss 0.03461093455553055, Total Loss 36.904571533203125\n",
      "8: Encoding Loss 4.923560619354248, Transition Loss -0.9024720191955566, Classifier Loss 0.0917523056268692, Total Loss 38.71623229980469\n",
      "8: Encoding Loss 5.0820207595825195, Transition Loss -1.364366888999939, Classifier Loss 0.21487195789813995, Total Loss 51.97877502441406\n",
      "8: Encoding Loss 8.321157455444336, Transition Loss -0.32664307951927185, Classifier Loss 0.2232823520898819, Total Loss 72.25505828857422\n",
      "8: Encoding Loss 6.426045894622803, Transition Loss -1.3627629280090332, Classifier Loss 0.08431459963321686, Total Loss 46.9871940612793\n",
      "8: Encoding Loss 6.585396766662598, Transition Loss -1.3417832851409912, Classifier Loss 0.1673402637243271, Total Loss 56.24586868286133\n",
      "8: Encoding Loss 4.871947288513184, Transition Loss -1.8525331020355225, Classifier Loss 0.04186365008354187, Total Loss 33.41731262207031\n",
      "8: Encoding Loss 5.709052562713623, Transition Loss -1.1948089599609375, Classifier Loss 0.07419826090335846, Total Loss 41.673667907714844\n",
      "8: Encoding Loss 6.6989426612854, Transition Loss -1.0950816869735718, Classifier Loss 0.0954083502292633, Total Loss 49.73405456542969\n",
      "8: Encoding Loss 5.02195405960083, Transition Loss -1.3695083856582642, Classifier Loss 0.118278868496418, Total Loss 41.95906066894531\n",
      "8: Encoding Loss 3.3893191814422607, Transition Loss 0.4123844504356384, Classifier Loss 0.04472283646464348, Total Loss 24.973154067993164\n",
      "8: Encoding Loss 4.031612396240234, Transition Loss -0.6761285066604614, Classifier Loss 0.08401203155517578, Total Loss 32.59061050415039\n",
      "8: Encoding Loss 4.498835563659668, Transition Loss 0.24125249683856964, Classifier Loss 0.07228824496269226, Total Loss 34.31834030151367\n",
      "8: Encoding Loss 5.2974534034729, Transition Loss -1.3649519681930542, Classifier Loss 0.060187455266714096, Total Loss 37.802921295166016\n",
      "8: Encoding Loss 6.868592739105225, Transition Loss -0.32607799768447876, Classifier Loss 0.19554300606250763, Total Loss 60.76573181152344\n",
      "8: Encoding Loss 5.20681095123291, Transition Loss -1.8260550498962402, Classifier Loss 0.08644051849842072, Total Loss 39.88418960571289\n",
      "8: Encoding Loss 8.40218448638916, Transition Loss -2.7884416580200195, Classifier Loss 0.12649749219417572, Total Loss 63.061744689941406\n",
      "8: Encoding Loss 5.91102409362793, Transition Loss -1.6899292469024658, Classifier Loss 0.0804535523056984, Total Loss 43.51082229614258\n",
      "8: Encoding Loss 3.7211997509002686, Transition Loss -1.1674933433532715, Classifier Loss 0.08888888359069824, Total Loss 31.215620040893555\n",
      "8: Encoding Loss 3.964810609817505, Transition Loss -0.41029661893844604, Classifier Loss 0.025327205657958984, Total Loss 26.321420669555664\n",
      "8: Encoding Loss 6.5490312576293945, Transition Loss -3.091681957244873, Classifier Loss 0.2447642982006073, Total Loss 63.76938247680664\n",
      "8: Encoding Loss 5.274094581604004, Transition Loss -1.4386073350906372, Classifier Loss 0.131999671459198, Total Loss 44.84395980834961\n",
      "8: Encoding Loss 3.7256481647491455, Transition Loss 0.8533955216407776, Classifier Loss 0.05123061686754227, Total Loss 27.818309783935547\n",
      "8: Encoding Loss 5.425212860107422, Transition Loss -1.0121686458587646, Classifier Loss 0.23253042995929718, Total Loss 55.803916931152344\n",
      "8: Encoding Loss 4.168979644775391, Transition Loss -1.6369140148162842, Classifier Loss 0.11997110396623611, Total Loss 37.01033401489258\n",
      "8: Encoding Loss 6.412528038024902, Transition Loss -0.8101681470870972, Classifier Loss 0.1784360408782959, Total Loss 56.318450927734375\n",
      "8: Encoding Loss 3.2365784645080566, Transition Loss -0.397344708442688, Classifier Loss 0.06347550451755524, Total Loss 25.766862869262695\n",
      "8: Encoding Loss 3.139545202255249, Transition Loss -0.5560226440429688, Classifier Loss 0.15950928628444672, Total Loss 34.78797912597656\n",
      "8: Encoding Loss 5.465929985046387, Transition Loss -2.27763032913208, Classifier Loss 0.11337698996067047, Total Loss 44.13236999511719\n",
      "8: Encoding Loss 4.936030387878418, Transition Loss -1.6015775203704834, Classifier Loss 0.16848517954349518, Total Loss 46.46406173706055\n",
      "8: Encoding Loss 5.508601665496826, Transition Loss -2.2916982173919678, Classifier Loss 0.12786492705345154, Total Loss 45.837188720703125\n",
      "8: Encoding Loss 5.758269309997559, Transition Loss -1.7218927145004272, Classifier Loss 0.04542939364910126, Total Loss 39.09186553955078\n",
      "8: Encoding Loss 5.587929725646973, Transition Loss -2.787360429763794, Classifier Loss 0.11075083166360855, Total Loss 44.60154724121094\n",
      "8: Encoding Loss 7.263540744781494, Transition Loss -1.1107195615768433, Classifier Loss 0.15809062123298645, Total Loss 59.38986587524414\n",
      "8: Encoding Loss 6.190976142883301, Transition Loss -1.7052911520004272, Classifier Loss 0.15344910323619843, Total Loss 52.49008560180664\n",
      "8: Encoding Loss 6.114264965057373, Transition Loss -0.8296502232551575, Classifier Loss 0.08199933171272278, Total Loss 44.88519287109375\n",
      "8: Encoding Loss 4.080521583557129, Transition Loss -2.2947487831115723, Classifier Loss 0.08168391138315201, Total Loss 32.65060043334961\n",
      "8: Encoding Loss 6.554657936096191, Transition Loss -1.5973002910614014, Classifier Loss 0.10017137974500656, Total Loss 49.34444808959961\n",
      "8: Encoding Loss 4.401973247528076, Transition Loss -2.2596447467803955, Classifier Loss 0.07565806806087494, Total Loss 33.976741790771484\n",
      "8: Encoding Loss 5.781160831451416, Transition Loss -1.883997917175293, Classifier Loss 0.04680025950074196, Total Loss 39.36623764038086\n",
      "8: Encoding Loss 4.7269206047058105, Transition Loss -1.0116554498672485, Classifier Loss 0.06323565542697906, Total Loss 34.68468475341797\n",
      "8: Encoding Loss 5.291463375091553, Transition Loss -1.6076031923294067, Classifier Loss 0.1358434557914734, Total Loss 45.332481384277344\n",
      "8: Encoding Loss 3.7261345386505127, Transition Loss 0.23069798946380615, Classifier Loss 0.05591944605112076, Total Loss 28.041032791137695\n",
      "8: Encoding Loss 8.15580940246582, Transition Loss -1.3103749752044678, Classifier Loss 0.17662851512432098, Total Loss 66.59718322753906\n",
      "8: Encoding Loss 8.028891563415527, Transition Loss -2.193711757659912, Classifier Loss 0.15736810863018036, Total Loss 63.90928649902344\n",
      "8: Encoding Loss 7.416152000427246, Transition Loss -1.048241138458252, Classifier Loss 0.1829475611448288, Total Loss 62.79125213623047\n",
      "8: Encoding Loss 6.030158042907715, Transition Loss -0.2631739675998688, Classifier Loss 0.12002803385257721, Total Loss 48.18364715576172\n",
      "8: Encoding Loss 7.512839317321777, Transition Loss -0.21226978302001953, Classifier Loss 0.24071088433265686, Total Loss 69.14804077148438\n",
      "8: Encoding Loss 4.536719799041748, Transition Loss 0.27591490745544434, Classifier Loss 0.053748540580272675, Total Loss 32.70553970336914\n",
      "8: Encoding Loss 3.6591804027557373, Transition Loss -0.8941491842269897, Classifier Loss 0.10503192991018295, Total Loss 32.457916259765625\n",
      "8: Encoding Loss 2.6703429222106934, Transition Loss 0.45674845576286316, Classifier Loss 0.0944490134716034, Total Loss 25.649660110473633\n",
      "8: Encoding Loss 5.109269618988037, Transition Loss -0.6531568169593811, Classifier Loss 0.13341258466243744, Total Loss 43.99661636352539\n",
      "8: Encoding Loss 6.053420066833496, Transition Loss -1.7540349960327148, Classifier Loss 0.1009686291217804, Total Loss 46.416683197021484\n",
      "8: Encoding Loss 6.808544158935547, Transition Loss -1.7528173923492432, Classifier Loss 0.18549297749996185, Total Loss 59.39986038208008\n",
      "8: Encoding Loss 4.220540523529053, Transition Loss -1.7415642738342285, Classifier Loss 0.07023295760154724, Total Loss 32.34584045410156\n",
      "8: Encoding Loss 6.296723365783691, Transition Loss -2.336794376373291, Classifier Loss 0.17508874833583832, Total Loss 55.28828048706055\n",
      "8: Encoding Loss 4.74686336517334, Transition Loss -1.1632872819900513, Classifier Loss 0.07575320452451706, Total Loss 36.05603790283203\n",
      "8: Encoding Loss 4.726447105407715, Transition Loss -1.5771573781967163, Classifier Loss 0.10251972079277039, Total Loss 38.61002731323242\n",
      "8: Encoding Loss 4.394988059997559, Transition Loss -3.234398126602173, Classifier Loss 0.08259746432304382, Total Loss 34.62838363647461\n",
      "8: Encoding Loss 2.5696377754211426, Transition Loss -1.867618441581726, Classifier Loss 0.06795982271432877, Total Loss 22.213062286376953\n",
      "8: Encoding Loss 5.573226451873779, Transition Loss -1.7286005020141602, Classifier Loss 0.09301087260246277, Total Loss 42.7397575378418\n",
      "8: Encoding Loss 5.297696113586426, Transition Loss -1.958001971244812, Classifier Loss 0.15011315047740936, Total Loss 46.79671096801758\n",
      "8: Encoding Loss 3.1093039512634277, Transition Loss -1.654439091682434, Classifier Loss 0.0748613104224205, Total Loss 26.141294479370117\n",
      "8: Encoding Loss 6.772500514984131, Transition Loss -2.3025314807891846, Classifier Loss 0.08203577250242233, Total Loss 48.83766555786133\n",
      "8: Encoding Loss 4.4454498291015625, Transition Loss -0.9780979156494141, Classifier Loss 0.0760856419801712, Total Loss 34.2808723449707\n",
      "8: Encoding Loss 6.637574672698975, Transition Loss -1.1547178030014038, Classifier Loss 0.11572137475013733, Total Loss 51.39712905883789\n",
      "8: Encoding Loss 6.893208980560303, Transition Loss -1.5143660306930542, Classifier Loss 0.14723330736160278, Total Loss 56.08198165893555\n",
      "8: Encoding Loss 6.326342582702637, Transition Loss -0.8670677542686462, Classifier Loss 0.10240299999713898, Total Loss 48.1980094909668\n",
      "8: Encoding Loss 4.912844657897949, Transition Loss -1.5661442279815674, Classifier Loss 0.11252762377262115, Total Loss 40.72920608520508\n",
      "8: Encoding Loss 5.9872636795043945, Transition Loss -1.8554048538208008, Classifier Loss 0.10851368308067322, Total Loss 46.774208068847656\n",
      "8: Encoding Loss 5.122897148132324, Transition Loss -1.6456395387649536, Classifier Loss 0.07339352369308472, Total Loss 38.07607650756836\n",
      "8: Encoding Loss 2.7060303688049316, Transition Loss -0.12762346863746643, Classifier Loss 0.04053039848804474, Total Loss 20.28917121887207\n",
      "8: Encoding Loss 7.431588172912598, Transition Loss -0.19393640756607056, Classifier Loss 0.08955122530460358, Total Loss 53.544578552246094\n",
      "8: Encoding Loss 6.384471893310547, Transition Loss -1.3223215341567993, Classifier Loss 0.07312414795160294, Total Loss 45.618717193603516\n",
      "8: Encoding Loss 6.620881080627441, Transition Loss -1.0150834321975708, Classifier Loss 0.0410226471722126, Total Loss 43.8271484375\n",
      "8: Encoding Loss 4.160911560058594, Transition Loss -0.09177374839782715, Classifier Loss 0.060178257524967194, Total Loss 30.983261108398438\n",
      "8: Encoding Loss 3.9227490425109863, Transition Loss -2.1676671504974365, Classifier Loss 0.03923938050866127, Total Loss 27.459566116333008\n",
      "8: Encoding Loss 3.860752582550049, Transition Loss -1.733712911605835, Classifier Loss 0.13679324090480804, Total Loss 36.84314727783203\n",
      "8: Encoding Loss 5.525247097015381, Transition Loss -0.4241633415222168, Classifier Loss 0.04421505331993103, Total Loss 37.57282257080078\n",
      "8: Encoding Loss 4.527310848236084, Transition Loss -1.13602614402771, Classifier Loss 0.04775036498904228, Total Loss 31.938447952270508\n",
      "8: Encoding Loss 4.199233055114746, Transition Loss -0.5729419589042664, Classifier Loss 0.04613113030791283, Total Loss 29.808284759521484\n",
      "8: Encoding Loss 3.721541166305542, Transition Loss -0.8762565851211548, Classifier Loss 0.07613268494606018, Total Loss 29.94216537475586\n",
      "8: Encoding Loss 6.1958208084106445, Transition Loss -1.43965482711792, Classifier Loss 0.07598373293876648, Total Loss 44.77272415161133\n",
      "8: Encoding Loss 4.426753997802734, Transition Loss -1.8828582763671875, Classifier Loss 0.07648614794015884, Total Loss 34.20838928222656\n",
      "8: Encoding Loss 3.9104838371276855, Transition Loss -2.533271551132202, Classifier Loss 0.05581872537732124, Total Loss 29.043764114379883\n",
      "8: Encoding Loss 3.6924312114715576, Transition Loss -1.4796996116638184, Classifier Loss 0.11137900501489639, Total Loss 33.29189682006836\n",
      "8: Encoding Loss 6.121148109436035, Transition Loss -0.8033013343811035, Classifier Loss 0.10240007191896439, Total Loss 46.966575622558594\n",
      "8: Encoding Loss 3.5883586406707764, Transition Loss -0.8339676856994629, Classifier Loss 0.05568820238113403, Total Loss 27.0986385345459\n",
      "8: Encoding Loss 4.543726444244385, Transition Loss -1.1502583026885986, Classifier Loss 0.11136859655380249, Total Loss 38.39875793457031\n",
      "8: Encoding Loss 5.118186950683594, Transition Loss -2.878096580505371, Classifier Loss 0.0830163061618805, Total Loss 39.00960159301758\n",
      "8: Encoding Loss 4.167076587677002, Transition Loss -1.090761423110962, Classifier Loss 0.04659489542245865, Total Loss 29.66151237487793\n",
      "8: Encoding Loss 3.3845341205596924, Transition Loss -0.744092583656311, Classifier Loss 0.06151574105024338, Total Loss 26.458480834960938\n",
      "8: Encoding Loss 4.430815696716309, Transition Loss -1.7497215270996094, Classifier Loss 0.15435601770877838, Total Loss 42.01980209350586\n",
      "8: Encoding Loss 3.2892050743103027, Transition Loss -2.3315014839172363, Classifier Loss 0.08409394323825836, Total Loss 28.143693923950195\n",
      "8: Encoding Loss 3.6980299949645996, Transition Loss -0.9174317121505737, Classifier Loss 0.08693335205316544, Total Loss 30.881149291992188\n",
      "8: Encoding Loss 4.6553053855896, Transition Loss -2.408871650695801, Classifier Loss 0.07360529899597168, Total Loss 35.29139709472656\n",
      "8: Encoding Loss 5.6386237144470215, Transition Loss -1.020783543586731, Classifier Loss 0.16316281259059906, Total Loss 50.14761734008789\n",
      "8: Encoding Loss 5.534126281738281, Transition Loss -1.6047532558441162, Classifier Loss 0.10473262518644333, Total Loss 43.6773796081543\n",
      "8: Encoding Loss 3.402651309967041, Transition Loss -2.136997699737549, Classifier Loss 0.0863306075334549, Total Loss 29.048114776611328\n",
      "8: Encoding Loss 6.280602931976318, Transition Loss -1.182366967201233, Classifier Loss 0.18561986088752747, Total Loss 56.24513244628906\n",
      "8: Encoding Loss 7.2211809158325195, Transition Loss -1.9512107372283936, Classifier Loss 0.2346915602684021, Total Loss 66.79546356201172\n",
      "8: Encoding Loss 7.449647426605225, Transition Loss -1.8699485063552856, Classifier Loss 0.13294923305511475, Total Loss 57.9920654296875\n",
      "8: Encoding Loss 7.157756805419922, Transition Loss -2.2898619174957275, Classifier Loss 0.1232704222202301, Total Loss 55.272666931152344\n",
      "8: Encoding Loss 4.372828006744385, Transition Loss -2.9246809482574463, Classifier Loss 0.08167250454425812, Total Loss 34.40304946899414\n",
      "8: Encoding Loss 7.517117500305176, Transition Loss -2.4423112869262695, Classifier Loss 0.08929841220378876, Total Loss 54.03157043457031\n",
      "8: Encoding Loss 7.8809356689453125, Transition Loss -0.17919513583183289, Classifier Loss 0.13345974683761597, Total Loss 60.63151550292969\n",
      "8: Encoding Loss 6.87009334564209, Transition Loss -2.556501626968384, Classifier Loss 0.24227291345596313, Total Loss 65.44683074951172\n",
      "8: Encoding Loss 7.565042972564697, Transition Loss -1.971989631652832, Classifier Loss 0.1657259315252304, Total Loss 61.96206283569336\n",
      "8: Encoding Loss 6.603006839752197, Transition Loss 0.41402679681777954, Classifier Loss 0.0632428377866745, Total Loss 46.10793685913086\n",
      "8: Encoding Loss 5.5112624168396, Transition Loss -1.6300623416900635, Classifier Loss 0.04227465018630028, Total Loss 37.29439163208008\n",
      "8: Encoding Loss 2.9012160301208496, Transition Loss -1.2902333736419678, Classifier Loss 0.09708484262228012, Total Loss 27.115264892578125\n",
      "8: Encoding Loss 4.2232232093811035, Transition Loss -0.7049755454063416, Classifier Loss 0.08934499323368073, Total Loss 34.2735595703125\n",
      "8: Encoding Loss 3.8560657501220703, Transition Loss 0.5983246564865112, Classifier Loss 0.06631714850664139, Total Loss 30.00743865966797\n",
      "8: Encoding Loss 2.2039692401885986, Transition Loss -1.199700117111206, Classifier Loss 0.04147944226861, Total Loss 17.371280670166016\n",
      "8: Encoding Loss 4.3495659828186035, Transition Loss -0.9718630909919739, Classifier Loss 0.1589740812778473, Total Loss 41.994415283203125\n",
      "8: Encoding Loss 3.8369686603546143, Transition Loss -0.8943572640419006, Classifier Loss 0.14758847653865814, Total Loss 37.78030014038086\n",
      "8: Encoding Loss 6.7593793869018555, Transition Loss -0.1476610153913498, Classifier Loss 0.06127960979938507, Total Loss 46.684181213378906\n",
      "8: Encoding Loss 5.379359245300293, Transition Loss -2.3082191944122314, Classifier Loss 0.07993117719888687, Total Loss 40.26835250854492\n",
      "8: Encoding Loss 8.142134666442871, Transition Loss -1.2591121196746826, Classifier Loss 0.29657140374183655, Total Loss 78.50944519042969\n",
      "8: Encoding Loss 5.279623985290527, Transition Loss -2.4760396480560303, Classifier Loss 0.14281779527664185, Total Loss 45.958534240722656\n",
      "8: Encoding Loss 4.699977874755859, Transition Loss -1.4368990659713745, Classifier Loss 0.05464062839746475, Total Loss 33.66335678100586\n",
      "8: Encoding Loss 6.2814621925354, Transition Loss -1.7691309452056885, Classifier Loss 0.14607034623622894, Total Loss 52.29509735107422\n",
      "8: Encoding Loss 6.7807722091674805, Transition Loss -1.7114406824111938, Classifier Loss 0.1105847880244255, Total Loss 51.742431640625\n",
      "8: Encoding Loss 5.145251274108887, Transition Loss 0.15930140018463135, Classifier Loss 0.13466225564479828, Total Loss 44.40145492553711\n",
      "8: Encoding Loss 4.0706353187561035, Transition Loss 0.6930435299873352, Classifier Loss 0.08993813395500183, Total Loss 33.69484329223633\n",
      "8: Encoding Loss 5.081204891204834, Transition Loss -2.13675594329834, Classifier Loss 0.08525419235229492, Total Loss 39.01179504394531\n",
      "8: Encoding Loss 6.996884346008301, Transition Loss -1.5536452531814575, Classifier Loss 0.0962037742137909, Total Loss 51.6010627746582\n",
      "8: Encoding Loss 5.281388282775879, Transition Loss -0.27134236693382263, Classifier Loss 0.13527709245681763, Total Loss 45.2159309387207\n",
      "8: Encoding Loss 3.29699444770813, Transition Loss -0.4154250919818878, Classifier Loss 0.04867051914334297, Total Loss 24.648853302001953\n",
      "8: Encoding Loss 4.6902642250061035, Transition Loss -1.9468740224838257, Classifier Loss 0.13346756994724274, Total Loss 41.48756408691406\n",
      "8: Encoding Loss 3.643291473388672, Transition Loss 0.8253563642501831, Classifier Loss 0.09612235426902771, Total Loss 31.802127838134766\n",
      "8: Encoding Loss 6.443020820617676, Transition Loss -1.8849890232086182, Classifier Loss 0.0934959203004837, Total Loss 48.00696563720703\n",
      "8: Encoding Loss 3.8563361167907715, Transition Loss -2.2610080242156982, Classifier Loss 0.07825585454702377, Total Loss 30.96269989013672\n",
      "8: Encoding Loss 6.210888862609863, Transition Loss -1.276413083076477, Classifier Loss 0.11534839868545532, Total Loss 48.79966354370117\n",
      "8: Encoding Loss 4.424514293670654, Transition Loss -2.1647849082946777, Classifier Loss 0.06902746111154556, Total Loss 33.44896697998047\n",
      "8: Encoding Loss 5.333619117736816, Transition Loss -1.4264593124389648, Classifier Loss 0.11535108089447021, Total Loss 43.5362548828125\n",
      "8: Encoding Loss 5.377799034118652, Transition Loss -2.1214723587036133, Classifier Loss 0.11317361146211624, Total Loss 43.583309173583984\n",
      "8: Encoding Loss 4.00665283203125, Transition Loss 0.8710267543792725, Classifier Loss 0.10079500824213028, Total Loss 34.467830657958984\n",
      "8: Encoding Loss 2.0409843921661377, Transition Loss -0.8024117946624756, Classifier Loss 0.06835188716650009, Total Loss 19.08077621459961\n",
      "8: Encoding Loss 9.731135368347168, Transition Loss -2.0434324741363525, Classifier Loss 0.10973268747329712, Total Loss 69.35926818847656\n",
      "8: Encoding Loss 6.241485595703125, Transition Loss -1.4251854419708252, Classifier Loss 0.09861735999584198, Total Loss 47.310081481933594\n",
      "8: Encoding Loss 5.0704240798950195, Transition Loss -0.9163479208946228, Classifier Loss 0.10537338256835938, Total Loss 40.95951843261719\n",
      "8: Encoding Loss 4.627281665802002, Transition Loss -1.2137705087661743, Classifier Loss 0.059215910732746124, Total Loss 33.68479919433594\n",
      "8: Encoding Loss 3.447734832763672, Transition Loss -1.1104822158813477, Classifier Loss 0.12574553489685059, Total Loss 33.260520935058594\n",
      "8: Encoding Loss 5.123368263244629, Transition Loss -1.7520500421524048, Classifier Loss 0.15355640649795532, Total Loss 46.09514617919922\n",
      "8: Encoding Loss 3.421513080596924, Transition Loss 0.21068869531154633, Classifier Loss 0.05298464745283127, Total Loss 25.911821365356445\n",
      "8: Encoding Loss 6.055816650390625, Transition Loss -2.3432250022888184, Classifier Loss 0.06963326036930084, Total Loss 43.29728698730469\n",
      "8: Encoding Loss 5.721733093261719, Transition Loss -1.105310320854187, Classifier Loss 0.035396624356508255, Total Loss 37.8696174621582\n",
      "8: Encoding Loss 5.341365337371826, Transition Loss -1.9281647205352783, Classifier Loss 0.07776617258787155, Total Loss 39.82404327392578\n",
      "8: Encoding Loss 4.199284553527832, Transition Loss -0.928684413433075, Classifier Loss 0.037095408886671066, Total Loss 28.904876708984375\n",
      "8: Encoding Loss 4.257102012634277, Transition Loss 0.1571439951658249, Classifier Loss 0.14785616099834442, Total Loss 40.391090393066406\n",
      "8: Encoding Loss 5.0873260498046875, Transition Loss -1.104111909866333, Classifier Loss 0.05946619063615799, Total Loss 36.47013473510742\n",
      "8: Encoding Loss 3.614354372024536, Transition Loss -2.633352756500244, Classifier Loss 0.10271710157394409, Total Loss 31.956783294677734\n",
      "8: Encoding Loss 5.150376319885254, Transition Loss -1.6337265968322754, Classifier Loss 0.200121209025383, Total Loss 50.913726806640625\n",
      "8: Encoding Loss 5.115745544433594, Transition Loss -1.2613658905029297, Classifier Loss 0.10310690850019455, Total Loss 41.004661560058594\n",
      "8: Encoding Loss 7.5641584396362305, Transition Loss -1.2670657634735107, Classifier Loss 0.09547489136457443, Total Loss 54.93193435668945\n",
      "8: Encoding Loss 4.789738655090332, Transition Loss -0.4479120671749115, Classifier Loss 0.09333866834640503, Total Loss 38.072120666503906\n",
      "8: Encoding Loss 4.342629909515381, Transition Loss 0.14246834814548492, Classifier Loss 0.10510388761758804, Total Loss 36.6231575012207\n",
      "8: Encoding Loss 6.963246822357178, Transition Loss -1.4678447246551514, Classifier Loss 0.09521234780550003, Total Loss 51.300132751464844\n",
      "8: Encoding Loss 5.341849327087402, Transition Loss -2.311306953430176, Classifier Loss 0.14814262092113495, Total Loss 46.864437103271484\n",
      "8: Encoding Loss 7.316497802734375, Transition Loss -2.108849287033081, Classifier Loss 0.12329693138599396, Total Loss 56.22783660888672\n",
      "8: Encoding Loss 5.234731197357178, Transition Loss -2.196722984313965, Classifier Loss 0.05870485678315163, Total Loss 37.27799606323242\n",
      "8: Encoding Loss 3.848444700241089, Transition Loss -2.291694164276123, Classifier Loss 0.13355624675750732, Total Loss 36.44538116455078\n",
      "8: Encoding Loss 6.517463684082031, Transition Loss -1.100475549697876, Classifier Loss 0.09417441487312317, Total Loss 48.521785736083984\n",
      "8: Encoding Loss 8.697702407836914, Transition Loss -1.392836332321167, Classifier Loss 0.14760802686214447, Total Loss 66.94645690917969\n",
      "8: Encoding Loss 5.733776092529297, Transition Loss -1.4872530698776245, Classifier Loss 0.04938757047057152, Total Loss 39.3408203125\n",
      "8: Encoding Loss 4.274410724639893, Transition Loss -0.2028694450855255, Classifier Loss 0.06132185459136963, Total Loss 31.778568267822266\n",
      "8: Encoding Loss 3.4951610565185547, Transition Loss -1.0525306463241577, Classifier Loss 0.0935736894607544, Total Loss 30.327913284301758\n",
      "8: Encoding Loss 5.660379409790039, Transition Loss -1.507112979888916, Classifier Loss 0.06801721453666687, Total Loss 40.763397216796875\n",
      "8: Encoding Loss 4.137389183044434, Transition Loss -1.0678269863128662, Classifier Loss 0.04080220311880112, Total Loss 28.904129028320312\n",
      "8: Encoding Loss 6.25063419342041, Transition Loss -2.0619239807128906, Classifier Loss 0.049523208290338516, Total Loss 42.45530319213867\n",
      "8: Encoding Loss 3.380781888961792, Transition Loss -1.9827916622161865, Classifier Loss 0.0884915217757225, Total Loss 29.133052825927734\n",
      "8: Encoding Loss 8.184354782104492, Transition Loss 0.14772352576255798, Classifier Loss 0.2620316743850708, Total Loss 75.3683853149414\n",
      "8: Encoding Loss 5.207004070281982, Transition Loss -1.1425827741622925, Classifier Loss 0.1221558228135109, Total Loss 43.457149505615234\n",
      "8: Encoding Loss 6.2998366355896, Transition Loss -1.2758734226226807, Classifier Loss 0.10268629342317581, Total Loss 48.067138671875\n",
      "8: Encoding Loss 5.728058815002441, Transition Loss 0.17221452295780182, Classifier Loss 0.09892230480909348, Total Loss 44.329471588134766\n",
      "8: Encoding Loss 6.619077205657959, Transition Loss -0.5482539534568787, Classifier Loss 0.19385042786598206, Total Loss 59.09929275512695\n",
      "8: Encoding Loss 6.000443458557129, Transition Loss -1.4073867797851562, Classifier Loss 0.17763832211494446, Total Loss 53.76593017578125\n",
      "8: Encoding Loss 4.388488292694092, Transition Loss -0.5683184266090393, Classifier Loss 0.08838231861591339, Total Loss 35.1689338684082\n",
      "8: Encoding Loss 4.7360029220581055, Transition Loss -2.1689565181732178, Classifier Loss 0.12754113972187042, Total Loss 41.16926956176758\n",
      "8: Encoding Loss 4.976755142211914, Transition Loss -0.7231771945953369, Classifier Loss 0.12397574633359909, Total Loss 42.257816314697266\n",
      "8: Encoding Loss 5.882883548736572, Transition Loss -0.2961832582950592, Classifier Loss 0.10921414941549301, Total Loss 46.21860122680664\n",
      "8: Encoding Loss 3.3248419761657715, Transition Loss -0.9228725433349609, Classifier Loss 0.060642898082733154, Total Loss 26.01297378540039\n",
      "8: Encoding Loss 3.9073646068573, Transition Loss -0.8919038772583008, Classifier Loss 0.11360539495944977, Total Loss 34.80437088012695\n",
      "8: Encoding Loss 3.685614824295044, Transition Loss -1.896205186843872, Classifier Loss 0.08660636097192764, Total Loss 30.77356719970703\n",
      "8: Encoding Loss 4.801491737365723, Transition Loss -0.7043280601501465, Classifier Loss 0.04802423715591431, Total Loss 33.61109161376953\n",
      "8: Encoding Loss 5.147770881652832, Transition Loss -1.1037486791610718, Classifier Loss 0.12536095082759857, Total Loss 43.422279357910156\n",
      "8: Encoding Loss 3.9780325889587402, Transition Loss -2.2939701080322266, Classifier Loss 0.09312453866004944, Total Loss 33.17972946166992\n",
      "8: Encoding Loss 4.742245674133301, Transition Loss -1.3736422061920166, Classifier Loss 0.12950904667377472, Total Loss 41.403831481933594\n",
      "8: Encoding Loss 3.808882474899292, Transition Loss -0.17817342281341553, Classifier Loss 0.04373745620250702, Total Loss 27.226970672607422\n",
      "8: Encoding Loss 3.4862186908721924, Transition Loss -0.31956401467323303, Classifier Loss 0.05726994201540947, Total Loss 26.64417839050293\n",
      "8: Encoding Loss 4.811309814453125, Transition Loss -0.5981363654136658, Classifier Loss 0.20421065390110016, Total Loss 49.2886848449707\n",
      "8: Encoding Loss 5.912563323974609, Transition Loss -1.6883180141448975, Classifier Loss 0.20128387212753296, Total Loss 55.603092193603516\n",
      "8: Encoding Loss 4.062870025634766, Transition Loss -0.5546475648880005, Classifier Loss 0.13105107843875885, Total Loss 37.48210906982422\n",
      "8: Encoding Loss 6.243467807769775, Transition Loss -1.7002835273742676, Classifier Loss 0.144644096493721, Total Loss 51.924537658691406\n",
      "8: Encoding Loss 3.822662353515625, Transition Loss -2.019805908203125, Classifier Loss 0.07890035957098007, Total Loss 30.8252010345459\n",
      "8: Encoding Loss 5.662578582763672, Transition Loss -1.6731773614883423, Classifier Loss 0.1277528703212738, Total Loss 46.750091552734375\n",
      "8: Encoding Loss 5.225554466247559, Transition Loss -1.008496880531311, Classifier Loss 0.04377761483192444, Total Loss 35.73068618774414\n",
      "8: Encoding Loss 3.1793007850646973, Transition Loss -1.2545337677001953, Classifier Loss 0.10144253820180893, Total Loss 29.21955680847168\n",
      "8: Encoding Loss 3.71008038520813, Transition Loss -2.2309579849243164, Classifier Loss 0.05836281180381775, Total Loss 28.095870971679688\n",
      "8: Encoding Loss 3.8067727088928223, Transition Loss -0.2465430051088333, Classifier Loss 0.14793679118156433, Total Loss 37.63421630859375\n",
      "8: Encoding Loss 6.847138404846191, Transition Loss -0.7819673418998718, Classifier Loss 0.06073670834302902, Total Loss 47.15618896484375\n",
      "8: Encoding Loss 5.616846561431885, Transition Loss -1.4594722986221313, Classifier Loss 0.22871121764183044, Total Loss 56.57162094116211\n",
      "8: Encoding Loss 6.3838725090026855, Transition Loss -0.2508434057235718, Classifier Loss 0.14566893875598907, Total Loss 52.870033264160156\n",
      "8: Encoding Loss 5.477588653564453, Transition Loss -2.3023838996887207, Classifier Loss 0.14686280488967896, Total Loss 47.5508918762207\n",
      "8: Encoding Loss 4.885457992553711, Transition Loss -0.9075911641120911, Classifier Loss 0.06273011863231659, Total Loss 35.58539962768555\n",
      "8: Encoding Loss 3.038233995437622, Transition Loss -2.384664535522461, Classifier Loss 0.06338382512331009, Total Loss 24.56683349609375\n",
      "8: Encoding Loss 2.812216281890869, Transition Loss -1.291487216949463, Classifier Loss 0.06719642132520676, Total Loss 23.592424392700195\n",
      "8: Encoding Loss 3.8338069915771484, Transition Loss -2.639925956726074, Classifier Loss 0.0636419728398323, Total Loss 29.365982055664062\n",
      "8: Encoding Loss 6.27831506729126, Transition Loss 0.13956308364868164, Classifier Loss 0.046992409974336624, Total Loss 42.424957275390625\n",
      "8: Encoding Loss 5.3979411125183105, Transition Loss -0.3026279807090759, Classifier Loss 0.10576634109020233, Total Loss 42.96416091918945\n",
      "8: Encoding Loss 6.701800346374512, Transition Loss -3.496058464050293, Classifier Loss 0.07278574258089066, Total Loss 47.487979888916016\n",
      "8: Encoding Loss 5.437904357910156, Transition Loss -1.9515013694763184, Classifier Loss 0.09613905102014542, Total Loss 42.24055099487305\n",
      "8: Encoding Loss 4.255374431610107, Transition Loss 0.04797530174255371, Classifier Loss 0.10881954431533813, Total Loss 36.43339157104492\n",
      "8: Encoding Loss 4.4107489585876465, Transition Loss -2.359482526779175, Classifier Loss 0.04409872740507126, Total Loss 30.873422622680664\n",
      "8: Encoding Loss 7.428579807281494, Transition Loss -1.072323203086853, Classifier Loss 0.11463476717472076, Total Loss 56.03453063964844\n",
      "8: Encoding Loss 6.118307590484619, Transition Loss -2.7259933948516846, Classifier Loss 0.1729866862297058, Total Loss 54.007423400878906\n",
      "8: Encoding Loss 5.6792402267456055, Transition Loss -1.6856517791748047, Classifier Loss 0.09954936057329178, Total Loss 44.02970504760742\n",
      "8: Encoding Loss 6.211536407470703, Transition Loss -0.6328356266021729, Classifier Loss 0.16759946942329407, Total Loss 54.02891540527344\n",
      "8: Encoding Loss 6.285076141357422, Transition Loss -1.7372850179672241, Classifier Loss 0.02966335415840149, Total Loss 40.67609786987305\n",
      "8: Encoding Loss 5.113924980163574, Transition Loss -1.6365975141525269, Classifier Loss 0.1812160164117813, Total Loss 48.80449676513672\n",
      "8: Encoding Loss 3.8908424377441406, Transition Loss -0.700491189956665, Classifier Loss 0.07743921875953674, Total Loss 31.088695526123047\n",
      "8: Encoding Loss 6.582190036773682, Transition Loss -2.0107123851776123, Classifier Loss 0.11861210316419601, Total Loss 51.353546142578125\n",
      "8: Encoding Loss 5.535922050476074, Transition Loss -0.6760324239730835, Classifier Loss 0.10383304953575134, Total Loss 43.598567962646484\n",
      "8: Encoding Loss 4.7046217918396, Transition Loss -0.553512692451477, Classifier Loss 0.07906390726566315, Total Loss 36.1338996887207\n",
      "8: Encoding Loss 4.329899787902832, Transition Loss -1.136405110359192, Classifier Loss 0.12118945270776749, Total Loss 38.0978889465332\n",
      "8: Encoding Loss 4.929633140563965, Transition Loss -0.7667531371116638, Classifier Loss 0.14310458302497864, Total Loss 43.88795471191406\n",
      "8: Encoding Loss 5.602977752685547, Transition Loss -0.44096609950065613, Classifier Loss 0.12435036897659302, Total Loss 46.05272674560547\n",
      "8: Encoding Loss 5.1105170249938965, Transition Loss -2.986816167831421, Classifier Loss 0.05656586214900017, Total Loss 36.31849670410156\n",
      "8: Encoding Loss 2.8095576763153076, Transition Loss -1.7375249862670898, Classifier Loss 0.08992922306060791, Total Loss 25.84957504272461\n",
      "8: Encoding Loss 9.339170455932617, Transition Loss -2.223761558532715, Classifier Loss 0.1578148901462555, Total Loss 71.81562042236328\n",
      "8: Encoding Loss 6.246542453765869, Transition Loss -0.16555562615394592, Classifier Loss 0.04170596972107887, Total Loss 41.64978790283203\n",
      "8: Encoding Loss 5.633233070373535, Transition Loss -0.8732473254203796, Classifier Loss 0.1778268665075302, Total Loss 51.58173370361328\n",
      "8: Encoding Loss 4.957090377807617, Transition Loss -1.0767775774002075, Classifier Loss 0.1485781967639923, Total Loss 44.59993362426758\n",
      "8: Encoding Loss 6.289783477783203, Transition Loss -0.9199463129043579, Classifier Loss 0.060263894498348236, Total Loss 43.76472473144531\n",
      "8: Encoding Loss 6.706758975982666, Transition Loss -1.5684260129928589, Classifier Loss 0.2661089599132538, Total Loss 66.85082244873047\n",
      "8: Encoding Loss 5.0646867752075195, Transition Loss -0.29265087842941284, Classifier Loss 0.09037558734416962, Total Loss 39.425559997558594\n",
      "8: Encoding Loss 5.281681537628174, Transition Loss 0.4815496802330017, Classifier Loss 0.12305386364459991, Total Loss 44.18809509277344\n",
      "8: Encoding Loss 5.3484787940979, Transition Loss -0.5483756065368652, Classifier Loss 0.09179556369781494, Total Loss 41.27021026611328\n",
      "8: Encoding Loss 3.9772605895996094, Transition Loss 0.2726377248764038, Classifier Loss 0.02742033451795578, Total Loss 26.714651107788086\n",
      "8: Encoding Loss 6.555971622467041, Transition Loss -1.2773663997650146, Classifier Loss 0.08335521072149277, Total Loss 47.670841217041016\n",
      "8: Encoding Loss 7.31601095199585, Transition Loss -1.7635419368743896, Classifier Loss 0.1958288550376892, Total Loss 63.478248596191406\n",
      "8: Encoding Loss 4.569178581237793, Transition Loss -1.01909601688385, Classifier Loss 0.07749433815479279, Total Loss 35.164100646972656\n",
      "8: Encoding Loss 3.9038777351379395, Transition Loss -1.9410333633422852, Classifier Loss 0.07542740553617477, Total Loss 30.96523094177246\n",
      "8: Encoding Loss 4.103566646575928, Transition Loss -1.2583897113800049, Classifier Loss 0.07496832311153412, Total Loss 32.11772918701172\n",
      "8: Encoding Loss 4.840608596801758, Transition Loss -0.3844524621963501, Classifier Loss 0.04539008438587189, Total Loss 33.5825080871582\n",
      "8: Encoding Loss 5.771347999572754, Transition Loss -0.9976463317871094, Classifier Loss 0.16728967428207397, Total Loss 51.35665512084961\n",
      "8: Encoding Loss 3.212418794631958, Transition Loss -0.3045865297317505, Classifier Loss 0.13353437185287476, Total Loss 32.627830505371094\n",
      "8: Encoding Loss 5.789150238037109, Transition Loss -2.072235345840454, Classifier Loss 0.08997132629156113, Total Loss 43.731204986572266\n",
      "8: Encoding Loss 3.6405892372131348, Transition Loss -0.9696605801582336, Classifier Loss 0.04955701529979706, Total Loss 26.798851013183594\n",
      "8: Encoding Loss 4.626471519470215, Transition Loss -2.7120463848114014, Classifier Loss 0.04321527108550072, Total Loss 32.07927322387695\n",
      "8: Encoding Loss 2.1733415126800537, Transition Loss -1.501027226448059, Classifier Loss 0.03912530466914177, Total Loss 16.95197868347168\n",
      "8: Encoding Loss 6.119464874267578, Transition Loss -1.666093111038208, Classifier Loss 0.13276098668575287, Total Loss 49.99222183227539\n",
      "8: Encoding Loss 4.21798038482666, Transition Loss -1.8136845827102661, Classifier Loss 0.1001998782157898, Total Loss 35.327144622802734\n",
      "8: Encoding Loss 3.8633651733398438, Transition Loss -0.6662688255310059, Classifier Loss 0.06881611794233322, Total Loss 30.061534881591797\n",
      "8: Encoding Loss 5.289278984069824, Transition Loss 0.3778533637523651, Classifier Loss 0.0917859748005867, Total Loss 41.06541442871094\n",
      "8: Encoding Loss 6.219526767730713, Transition Loss -1.7814724445343018, Classifier Loss 0.15406149625778198, Total Loss 52.722599029541016\n",
      "8: Encoding Loss 6.7867889404296875, Transition Loss -3.685344696044922, Classifier Loss 0.1361662894487381, Total Loss 54.33589172363281\n",
      "8: Encoding Loss 4.636900901794434, Transition Loss -1.6372562646865845, Classifier Loss 0.08285188674926758, Total Loss 36.10594177246094\n",
      "8: Encoding Loss 5.46710729598999, Transition Loss -2.262876510620117, Classifier Loss 0.041589461266994476, Total Loss 36.960689544677734\n",
      "8: Encoding Loss 2.709974765777588, Transition Loss -2.7322099208831787, Classifier Loss 0.048475489020347595, Total Loss 21.106306076049805\n",
      "8: Encoding Loss 4.758761405944824, Transition Loss -2.911205768585205, Classifier Loss 0.1810099184513092, Total Loss 46.652400970458984\n",
      "8: Encoding Loss 4.977612495422363, Transition Loss -0.7893024682998657, Classifier Loss 0.18723152577877045, Total Loss 48.5885124206543\n",
      "8: Encoding Loss 7.461796760559082, Transition Loss -0.457430362701416, Classifier Loss 0.09575247019529343, Total Loss 54.345848083496094\n",
      "8: Encoding Loss 4.278154373168945, Transition Loss -1.1578913927078247, Classifier Loss 0.04801340773701668, Total Loss 30.469804763793945\n",
      "8: Encoding Loss 3.7628865242004395, Transition Loss -1.6694824695587158, Classifier Loss 0.06088203191757202, Total Loss 28.66485595703125\n",
      "8: Encoding Loss 5.454016208648682, Transition Loss -1.5562705993652344, Classifier Loss 0.037270329892635345, Total Loss 36.45050811767578\n",
      "8: Encoding Loss 4.474998950958252, Transition Loss -0.8962730169296265, Classifier Loss 0.05863566696643829, Total Loss 32.71320343017578\n",
      "8: Encoding Loss 4.842130661010742, Transition Loss -1.4376620054244995, Classifier Loss 0.06874460726976395, Total Loss 35.92667007446289\n",
      "8: Encoding Loss 5.04816198348999, Transition Loss -1.4281208515167236, Classifier Loss 0.05633603408932686, Total Loss 35.92200469970703\n",
      "8: Encoding Loss 5.2402448654174805, Transition Loss -1.3571910858154297, Classifier Loss 0.04721600562334061, Total Loss 36.16252899169922\n",
      "8: Encoding Loss 4.572264671325684, Transition Loss -1.2240440845489502, Classifier Loss 0.1297127902507782, Total Loss 40.404380798339844\n",
      "8: Encoding Loss 4.1606879234313965, Transition Loss -0.873644232749939, Classifier Loss 0.08331044018268585, Total Loss 33.294822692871094\n",
      "8: Encoding Loss 5.496926784515381, Transition Loss 0.361875981092453, Classifier Loss 0.11374974995851517, Total Loss 44.501285552978516\n",
      "8: Encoding Loss 4.462933540344238, Transition Loss -1.8904814720153809, Classifier Loss 0.06727898865938187, Total Loss 33.50474548339844\n",
      "8: Encoding Loss 4.606114864349365, Transition Loss -2.060575485229492, Classifier Loss 0.06733496487140656, Total Loss 34.369361877441406\n",
      "8: Encoding Loss 4.413936138153076, Transition Loss -0.7183651328086853, Classifier Loss 0.1478513479232788, Total Loss 41.26846694946289\n",
      "8: Encoding Loss 3.9049644470214844, Transition Loss -0.2601161003112793, Classifier Loss 0.04415089264512062, Total Loss 27.844770431518555\n",
      "8: Encoding Loss 5.127954006195068, Transition Loss -2.149358034133911, Classifier Loss 0.06884253025054932, Total Loss 37.651119232177734\n",
      "8: Encoding Loss 4.084855079650879, Transition Loss -0.9170399904251099, Classifier Loss 0.037747498601675034, Total Loss 28.28351402282715\n",
      "8: Encoding Loss 6.222655296325684, Transition Loss 0.07902286946773529, Classifier Loss 0.19789962470531464, Total Loss 57.15750503540039\n",
      "8: Encoding Loss 5.763838768005371, Transition Loss -1.7123124599456787, Classifier Loss 0.08685368299484253, Total Loss 43.26771545410156\n",
      "8: Encoding Loss 6.322510719299316, Transition Loss -0.47321873903274536, Classifier Loss 0.15842483937740326, Total Loss 53.77735900878906\n",
      "8: Encoding Loss 4.723065376281738, Transition Loss -1.1250438690185547, Classifier Loss 0.030249735340476036, Total Loss 31.3629150390625\n",
      "8: Encoding Loss 4.870007514953613, Transition Loss -1.061613917350769, Classifier Loss 0.14601799845695496, Total Loss 43.8214225769043\n",
      "8: Encoding Loss 2.9756100177764893, Transition Loss -1.5428078174591064, Classifier Loss 0.11302109807729721, Total Loss 29.1551513671875\n",
      "8: Encoding Loss 5.618849277496338, Transition Loss -0.3452155590057373, Classifier Loss 0.13402189314365387, Total Loss 47.115150451660156\n",
      "8: Encoding Loss 5.793610572814941, Transition Loss -1.355873703956604, Classifier Loss 0.09347464889287949, Total Loss 44.10858917236328\n",
      "8: Encoding Loss 3.9231438636779785, Transition Loss -0.4763832092285156, Classifier Loss 0.11873146891593933, Total Loss 35.41181945800781\n",
      "8: Encoding Loss 4.724373817443848, Transition Loss -1.374252438545227, Classifier Loss 0.08067009598016739, Total Loss 36.41270446777344\n",
      "8: Encoding Loss 6.060683250427246, Transition Loss -0.46243661642074585, Classifier Loss 0.07324997335672379, Total Loss 43.68891525268555\n",
      "8: Encoding Loss 3.7423362731933594, Transition Loss 0.11169406771659851, Classifier Loss 0.06380646675825119, Total Loss 28.87934112548828\n",
      "8: Encoding Loss 6.3775177001953125, Transition Loss -0.05074629187583923, Classifier Loss 0.17317502200603485, Total Loss 55.58259201049805\n",
      "8: Encoding Loss 5.482229232788086, Transition Loss -1.0810679197311401, Classifier Loss 0.16121387481689453, Total Loss 49.01433181762695\n",
      "8: Encoding Loss 4.589631080627441, Transition Loss -1.3830102682113647, Classifier Loss 0.08316105604171753, Total Loss 35.85334014892578\n",
      "8: Encoding Loss 4.045004844665527, Transition Loss -1.4865233898162842, Classifier Loss 0.1549612283706665, Total Loss 39.76555633544922\n",
      "8: Encoding Loss 6.21176290512085, Transition Loss -0.06939089298248291, Classifier Loss 0.1346176266670227, Total Loss 50.73231506347656\n",
      "8: Encoding Loss 5.406530857086182, Transition Loss -1.5542681217193604, Classifier Loss 0.13568799197673798, Total Loss 46.007362365722656\n",
      "8: Encoding Loss 5.748292446136475, Transition Loss -1.2053793668746948, Classifier Loss 0.1460980325937271, Total Loss 49.09907913208008\n",
      "8: Encoding Loss 4.993437767028809, Transition Loss -1.199129581451416, Classifier Loss 0.08261638879776001, Total Loss 38.22178649902344\n",
      "8: Encoding Loss 5.855418682098389, Transition Loss -1.5831977128982544, Classifier Loss 0.12214686721563339, Total Loss 47.3465690612793\n",
      "8: Encoding Loss 5.534646034240723, Transition Loss -0.8448646664619446, Classifier Loss 0.09449545294046402, Total Loss 42.65708541870117\n",
      "8: Encoding Loss 4.623439788818359, Transition Loss -1.2108867168426514, Classifier Loss 0.09261497855186462, Total Loss 37.001651763916016\n",
      "8: Encoding Loss 4.6681108474731445, Transition Loss 0.020808041095733643, Classifier Loss 0.049649640917778015, Total Loss 32.98195266723633\n",
      "8: Encoding Loss 3.7653472423553467, Transition Loss -0.2516952157020569, Classifier Loss 0.05442800000309944, Total Loss 28.0347843170166\n",
      "8: Encoding Loss 6.8116936683654785, Transition Loss -1.5776509046554565, Classifier Loss 0.16036087274551392, Total Loss 56.90562057495117\n",
      "8: Encoding Loss 5.902540683746338, Transition Loss -1.8144607543945312, Classifier Loss 0.1198062002658844, Total Loss 47.3951416015625\n",
      "8: Encoding Loss 3.3796966075897217, Transition Loss -0.9830548167228699, Classifier Loss 0.0843016505241394, Total Loss 28.70795440673828\n",
      "8: Encoding Loss 7.166481018066406, Transition Loss -1.608524203300476, Classifier Loss 0.09288322925567627, Total Loss 52.286563873291016\n",
      "8: Encoding Loss 4.657543659210205, Transition Loss -0.8878123760223389, Classifier Loss 0.06970782577991486, Total Loss 34.91569137573242\n",
      "8: Encoding Loss 4.471897125244141, Transition Loss -0.9256787300109863, Classifier Loss 0.049671657383441925, Total Loss 31.798179626464844\n",
      "8: Encoding Loss 6.425408840179443, Transition Loss -1.9140722751617432, Classifier Loss 0.1032458022236824, Total Loss 48.87627029418945\n",
      "8: Encoding Loss 5.774263381958008, Transition Loss -1.0304630994796753, Classifier Loss 0.1965465247631073, Total Loss 54.29981994628906\n",
      "8: Encoding Loss 5.501227855682373, Transition Loss -1.3565378189086914, Classifier Loss 0.09129811078310013, Total Loss 42.13663864135742\n",
      "8: Encoding Loss 4.491464138031006, Transition Loss -0.05728060007095337, Classifier Loss 0.07503491640090942, Total Loss 34.45225524902344\n",
      "8: Encoding Loss 5.086033344268799, Transition Loss -2.3664681911468506, Classifier Loss 0.07993873208761215, Total Loss 38.50912857055664\n",
      "8: Encoding Loss 5.104910850524902, Transition Loss -1.8558753728866577, Classifier Loss 0.09151976555585861, Total Loss 39.78070068359375\n",
      "8: Encoding Loss 5.913282871246338, Transition Loss 0.2534787952899933, Classifier Loss 0.12318458408117294, Total Loss 47.8995475769043\n",
      "8: Encoding Loss 7.628481864929199, Transition Loss -1.312215805053711, Classifier Loss 0.1363915205001831, Total Loss 59.40951919555664\n",
      "8: Encoding Loss 5.0398969650268555, Transition Loss -2.237186908721924, Classifier Loss 0.050220802426338196, Total Loss 35.26056671142578\n",
      "8: Encoding Loss 5.996359825134277, Transition Loss -1.010434627532959, Classifier Loss 0.0566021129488945, Total Loss 41.63796615600586\n",
      "8: Encoding Loss 5.354705333709717, Transition Loss -0.7839967012405396, Classifier Loss 0.15226922929286957, Total Loss 47.35484313964844\n",
      "8: Encoding Loss 4.836611747741699, Transition Loss -1.107141375541687, Classifier Loss 0.03921034559607506, Total Loss 32.94026565551758\n",
      "9: Encoding Loss 3.7656643390655518, Transition Loss -1.1652616262435913, Classifier Loss 0.07205922156572342, Total Loss 29.799442291259766\n",
      "9: Encoding Loss 6.300674915313721, Transition Loss -0.1816442608833313, Classifier Loss 0.10588925331830978, Total Loss 48.39290237426758\n",
      "9: Encoding Loss 5.069418430328369, Transition Loss -1.5706894397735596, Classifier Loss 0.08979003876447678, Total Loss 39.3948860168457\n",
      "9: Encoding Loss 5.791103363037109, Transition Loss -1.0814456939697266, Classifier Loss 0.07148243486881256, Total Loss 41.894432067871094\n",
      "9: Encoding Loss 7.576684474945068, Transition Loss 0.43517422676086426, Classifier Loss 0.18675456941127777, Total Loss 64.30963897705078\n",
      "9: Encoding Loss 6.382673740386963, Transition Loss -2.8263509273529053, Classifier Loss 0.059589285403490067, Total Loss 44.253841400146484\n",
      "9: Encoding Loss 4.250845909118652, Transition Loss -2.0697426795959473, Classifier Loss 0.13737082481384277, Total Loss 39.2413330078125\n",
      "9: Encoding Loss 4.325539588928223, Transition Loss -1.655340313911438, Classifier Loss 0.10675356537103653, Total Loss 36.6279296875\n",
      "9: Encoding Loss 5.077108860015869, Transition Loss -1.1201378107070923, Classifier Loss 0.15161441266536713, Total Loss 45.62364959716797\n",
      "9: Encoding Loss 3.2990012168884277, Transition Loss -0.6678648591041565, Classifier Loss 0.07844287902116776, Total Loss 27.638029098510742\n",
      "9: Encoding Loss 6.000541687011719, Transition Loss -1.4064747095108032, Classifier Loss 0.14413270354270935, Total Loss 50.415958404541016\n",
      "9: Encoding Loss 3.8712189197540283, Transition Loss -1.221140742301941, Classifier Loss 0.07571659982204437, Total Loss 30.798484802246094\n",
      "9: Encoding Loss 2.5194859504699707, Transition Loss -0.3742782473564148, Classifier Loss 0.044937651604413986, Total Loss 19.610532760620117\n",
      "9: Encoding Loss 4.5757317543029785, Transition Loss -2.1741881370544434, Classifier Loss 0.08510034531354904, Total Loss 35.96355438232422\n",
      "9: Encoding Loss 3.477046489715576, Transition Loss -2.144664764404297, Classifier Loss 0.08064036816358566, Total Loss 28.925457000732422\n",
      "9: Encoding Loss 4.483974456787109, Transition Loss -1.3893542289733887, Classifier Loss 0.0733751654624939, Total Loss 34.240806579589844\n",
      "9: Encoding Loss 3.8741984367370605, Transition Loss -1.9062329530715942, Classifier Loss 0.06315299868583679, Total Loss 29.559728622436523\n",
      "9: Encoding Loss 2.9556033611297607, Transition Loss 0.4910160005092621, Classifier Loss 0.07590977102518082, Total Loss 25.521005630493164\n",
      "9: Encoding Loss 6.918379783630371, Transition Loss -2.55985689163208, Classifier Loss 0.10027250647544861, Total Loss 51.53650665283203\n",
      "9: Encoding Loss 4.150252342224121, Transition Loss -0.5932615995407104, Classifier Loss 0.20016756653785706, Total Loss 44.91803741455078\n",
      "9: Encoding Loss 6.557395935058594, Transition Loss -2.3920769691467285, Classifier Loss 0.23252879083156586, Total Loss 62.59629440307617\n",
      "9: Encoding Loss 5.55676794052124, Transition Loss -1.2855619192123413, Classifier Loss 0.1241273283958435, Total Loss 45.75282669067383\n",
      "9: Encoding Loss 6.350411891937256, Transition Loss -0.508143961429596, Classifier Loss 0.1099824607372284, Total Loss 49.10051727294922\n",
      "9: Encoding Loss 6.278050899505615, Transition Loss -0.1040768027305603, Classifier Loss 0.10494396090507507, Total Loss 48.162662506103516\n",
      "9: Encoding Loss 5.343443870544434, Transition Loss 0.35418203473091125, Classifier Loss 0.05583826079964638, Total Loss 37.78616714477539\n",
      "9: Encoding Loss 5.101284980773926, Transition Loss -1.9599189758300781, Classifier Loss 0.08447237312793732, Total Loss 39.054161071777344\n",
      "9: Encoding Loss 3.596317768096924, Transition Loss -0.36364561319351196, Classifier Loss 0.0693705677986145, Total Loss 28.514820098876953\n",
      "9: Encoding Loss 7.426091194152832, Transition Loss -1.5509068965911865, Classifier Loss 0.1674814373254776, Total Loss 61.304073333740234\n",
      "9: Encoding Loss 3.6187424659729004, Transition Loss -2.389266014099121, Classifier Loss 0.06835894286632538, Total Loss 28.547393798828125\n",
      "9: Encoding Loss 3.4603352546691895, Transition Loss -1.4075913429260254, Classifier Loss 0.15842470526695251, Total Loss 36.603919982910156\n",
      "9: Encoding Loss 4.836952209472656, Transition Loss -1.8254204988479614, Classifier Loss 0.08820629119873047, Total Loss 37.841617584228516\n",
      "9: Encoding Loss 4.500273704528809, Transition Loss 0.6810014247894287, Classifier Loss 0.057652585208415985, Total Loss 33.039302825927734\n",
      "9: Encoding Loss 6.056374549865723, Transition Loss -1.1522486209869385, Classifier Loss 0.15705224871635437, Total Loss 52.04301071166992\n",
      "9: Encoding Loss 4.073458194732666, Transition Loss -1.5067901611328125, Classifier Loss 0.08489438891410828, Total Loss 32.929588317871094\n",
      "9: Encoding Loss 4.1221418380737305, Transition Loss -1.3109186887741089, Classifier Loss 0.12325573712587357, Total Loss 37.05790328979492\n",
      "9: Encoding Loss 5.140630722045898, Transition Loss -1.4548876285552979, Classifier Loss 0.2340075969696045, Total Loss 54.243961334228516\n",
      "9: Encoding Loss 4.617626190185547, Transition Loss -1.366828203201294, Classifier Loss 0.10389123111963272, Total Loss 38.094337463378906\n",
      "9: Encoding Loss 4.836223125457764, Transition Loss -0.3286741375923157, Classifier Loss 0.06486791372299194, Total Loss 35.50400161743164\n",
      "9: Encoding Loss 8.498291969299316, Transition Loss 0.09075683355331421, Classifier Loss 0.2008036971092224, Total Loss 71.1064224243164\n",
      "9: Encoding Loss 7.522107124328613, Transition Loss -1.6714916229248047, Classifier Loss 0.1163228452205658, Total Loss 56.76426315307617\n",
      "9: Encoding Loss 5.250690460205078, Transition Loss -1.1616582870483398, Classifier Loss 0.12853196263313293, Total Loss 44.356876373291016\n",
      "9: Encoding Loss 6.2398810386657715, Transition Loss -1.2023361921310425, Classifier Loss 0.12731528282165527, Total Loss 50.17033767700195\n",
      "9: Encoding Loss 5.692636013031006, Transition Loss -1.1922450065612793, Classifier Loss 0.13733728229999542, Total Loss 47.88907241821289\n",
      "9: Encoding Loss 8.461092948913574, Transition Loss -2.4124467372894287, Classifier Loss 0.06939436495304108, Total Loss 57.70503234863281\n",
      "9: Encoding Loss 5.075436592102051, Transition Loss -1.8514556884765625, Classifier Loss 0.11345529556274414, Total Loss 41.79740905761719\n",
      "9: Encoding Loss 3.451258897781372, Transition Loss 0.14614444971084595, Classifier Loss 0.05846486985683441, Total Loss 26.612499237060547\n",
      "9: Encoding Loss 4.173445701599121, Transition Loss -1.0699384212493896, Classifier Loss 0.10011613368988037, Total Loss 35.05186462402344\n",
      "9: Encoding Loss 6.955615043640137, Transition Loss 0.04699401557445526, Classifier Loss 0.08737290650606155, Total Loss 50.48978042602539\n",
      "9: Encoding Loss 7.214388847351074, Transition Loss -0.30755701661109924, Classifier Loss 0.21795277297496796, Total Loss 65.08148956298828\n",
      "9: Encoding Loss 4.009337425231934, Transition Loss -0.26596400141716003, Classifier Loss 0.06190856546163559, Total Loss 30.246776580810547\n",
      "9: Encoding Loss 4.798198699951172, Transition Loss -1.6017415523529053, Classifier Loss 0.14168430864810944, Total Loss 42.95698547363281\n",
      "9: Encoding Loss 3.4819626808166504, Transition Loss -1.8867883682250977, Classifier Loss 0.03508685529232025, Total Loss 24.399707794189453\n",
      "9: Encoding Loss 7.580243110656738, Transition Loss -0.9765585660934448, Classifier Loss 0.08648192882537842, Total Loss 54.12926483154297\n",
      "9: Encoding Loss 5.080070495605469, Transition Loss 0.041704773902893066, Classifier Loss 0.159046933054924, Total Loss 46.401798248291016\n",
      "9: Encoding Loss 3.0812582969665527, Transition Loss -1.8503808975219727, Classifier Loss 0.11765304952859879, Total Loss 30.25211524963379\n",
      "9: Encoding Loss 7.270768165588379, Transition Loss -0.25641870498657227, Classifier Loss 0.06036076694726944, Total Loss 49.66058349609375\n",
      "9: Encoding Loss 6.9908952713012695, Transition Loss -0.48252785205841064, Classifier Loss 0.15297695994377136, Total Loss 57.24287414550781\n",
      "9: Encoding Loss 3.855456590652466, Transition Loss -1.0885257720947266, Classifier Loss 0.06011134386062622, Total Loss 29.14344024658203\n",
      "9: Encoding Loss 10.173478126525879, Transition Loss -2.1223864555358887, Classifier Loss 0.19900284707546234, Total Loss 80.9403076171875\n",
      "9: Encoding Loss 6.483478546142578, Transition Loss -1.66865074634552, Classifier Loss 0.041689153760671616, Total Loss 43.06911849975586\n",
      "9: Encoding Loss 5.887102127075195, Transition Loss -0.8498592376708984, Classifier Loss 0.0887153297662735, Total Loss 44.19380569458008\n",
      "9: Encoding Loss 4.168177604675293, Transition Loss -2.3856351375579834, Classifier Loss 0.08264974504709244, Total Loss 33.27308654785156\n",
      "9: Encoding Loss 3.8957698345184326, Transition Loss -2.3695244789123535, Classifier Loss 0.1252068281173706, Total Loss 35.89435577392578\n",
      "9: Encoding Loss 5.126789569854736, Transition Loss -4.278160572052002, Classifier Loss 0.10129743069410324, Total Loss 40.88876724243164\n",
      "9: Encoding Loss 4.690941333770752, Transition Loss -0.8575613498687744, Classifier Loss 0.11781485378742218, Total Loss 39.92679214477539\n",
      "9: Encoding Loss 6.167037487030029, Transition Loss -2.0661420822143555, Classifier Loss 0.21498145163059235, Total Loss 58.49954605102539\n",
      "9: Encoding Loss 3.529468536376953, Transition Loss -1.2413451671600342, Classifier Loss 0.21346424520015717, Total Loss 42.52273941040039\n",
      "9: Encoding Loss 5.845859050750732, Transition Loss -1.7009731531143188, Classifier Loss 0.0808815285563469, Total Loss 43.16263198852539\n",
      "9: Encoding Loss 4.249015808105469, Transition Loss -0.0063233524560928345, Classifier Loss 0.09808135777711868, Total Loss 35.30222702026367\n",
      "9: Encoding Loss 2.8610072135925293, Transition Loss -0.05206820368766785, Classifier Loss 0.09013232588768005, Total Loss 26.17925453186035\n",
      "9: Encoding Loss 4.776792049407959, Transition Loss -2.19162917137146, Classifier Loss 0.13991567492485046, Total Loss 42.65144348144531\n",
      "9: Encoding Loss 2.835446357727051, Transition Loss -1.4861160516738892, Classifier Loss 0.11097393929958344, Total Loss 28.109477996826172\n",
      "9: Encoding Loss 2.3020758628845215, Transition Loss -2.2021658420562744, Classifier Loss 0.09591329097747803, Total Loss 23.402904510498047\n",
      "9: Encoding Loss 7.1559157371521, Transition Loss -0.34746092557907104, Classifier Loss 0.042104724794626236, Total Loss 47.14583206176758\n",
      "9: Encoding Loss 4.318700313568115, Transition Loss -1.5428879261016846, Classifier Loss 0.05765673518180847, Total Loss 31.677257537841797\n",
      "9: Encoding Loss 5.383854866027832, Transition Loss -1.8169273138046265, Classifier Loss 0.15412476658821106, Total Loss 47.71487808227539\n",
      "9: Encoding Loss 3.8789851665496826, Transition Loss -1.9164931774139404, Classifier Loss 0.04171198979020119, Total Loss 27.44434356689453\n",
      "9: Encoding Loss 6.998231410980225, Transition Loss -1.575634479522705, Classifier Loss 0.09072799235582352, Total Loss 51.061561584472656\n",
      "9: Encoding Loss 4.745694160461426, Transition Loss -1.464176893234253, Classifier Loss 0.14307329058647156, Total Loss 42.780906677246094\n",
      "9: Encoding Loss 2.871366500854492, Transition Loss -1.3223903179168701, Classifier Loss 0.06769559532403946, Total Loss 23.997230529785156\n",
      "9: Encoding Loss 6.952359676361084, Transition Loss -0.9812833070755005, Classifier Loss 0.06825682520866394, Total Loss 48.539451599121094\n",
      "9: Encoding Loss 5.327456474304199, Transition Loss -1.0342800617218018, Classifier Loss 0.17799217998981476, Total Loss 49.763545989990234\n",
      "9: Encoding Loss 3.6196341514587402, Transition Loss -0.20112058520317078, Classifier Loss 0.1516938954591751, Total Loss 36.887115478515625\n",
      "9: Encoding Loss 4.700135231018066, Transition Loss -1.093725562095642, Classifier Loss 0.11864674836397171, Total Loss 40.06504821777344\n",
      "9: Encoding Loss 5.423000812530518, Transition Loss -1.194441795349121, Classifier Loss 0.13786806166172028, Total Loss 46.324337005615234\n",
      "9: Encoding Loss 5.004779815673828, Transition Loss -2.5256242752075195, Classifier Loss 0.11203160136938095, Total Loss 41.230831146240234\n",
      "9: Encoding Loss 4.446516036987305, Transition Loss 0.9217897653579712, Classifier Loss 0.07271529734134674, Total Loss 34.31934356689453\n",
      "9: Encoding Loss 5.2613983154296875, Transition Loss -1.6420444250106812, Classifier Loss 0.1663346290588379, Total Loss 48.20119857788086\n",
      "9: Encoding Loss 5.385724067687988, Transition Loss -2.507107973098755, Classifier Loss 0.039491619914770126, Total Loss 36.26250457763672\n",
      "9: Encoding Loss 4.467899799346924, Transition Loss -1.7858525514602661, Classifier Loss 0.13785266876220703, Total Loss 40.59195327758789\n",
      "9: Encoding Loss 3.8683552742004395, Transition Loss 0.9031686782836914, Classifier Loss 0.11892412602901459, Total Loss 35.46381378173828\n",
      "9: Encoding Loss 2.9888501167297363, Transition Loss -1.3301012516021729, Classifier Loss 0.06645730882883072, Total Loss 24.57830047607422\n",
      "9: Encoding Loss 5.782898426055908, Transition Loss -1.8159663677215576, Classifier Loss 0.0741540715098381, Total Loss 42.11207580566406\n",
      "9: Encoding Loss 4.689410209655762, Transition Loss 1.0880988836288452, Classifier Loss 0.07439962774515152, Total Loss 36.01166534423828\n",
      "9: Encoding Loss 3.743739604949951, Transition Loss -1.828016996383667, Classifier Loss 0.05249004065990448, Total Loss 27.710712432861328\n",
      "9: Encoding Loss 2.8395473957061768, Transition Loss -1.3432364463806152, Classifier Loss 0.1097881942987442, Total Loss 28.015567779541016\n",
      "9: Encoding Loss 3.517972946166992, Transition Loss 0.2965542674064636, Classifier Loss 0.056114278733730316, Total Loss 26.837886810302734\n",
      "9: Encoding Loss 10.695635795593262, Transition Loss 0.22245019674301147, Classifier Loss 0.24380221962928772, Total Loss 88.64302825927734\n",
      "9: Encoding Loss 6.80936336517334, Transition Loss -1.19807767868042, Classifier Loss 0.10181860625743866, Total Loss 51.03756332397461\n",
      "9: Encoding Loss 8.30648136138916, Transition Loss -1.0490601062774658, Classifier Loss 0.16347630321979523, Total Loss 66.18610382080078\n",
      "9: Encoding Loss 6.182097434997559, Transition Loss -1.6763027906417847, Classifier Loss 0.057118333876132965, Total Loss 42.803749084472656\n",
      "9: Encoding Loss 5.0882954597473145, Transition Loss -1.8993279933929443, Classifier Loss 0.05439282953739166, Total Loss 35.96829605102539\n",
      "9: Encoding Loss 4.597792148590088, Transition Loss -0.43359091877937317, Classifier Loss 0.13183115422725677, Total Loss 40.76969528198242\n",
      "9: Encoding Loss 4.5613508224487305, Transition Loss -2.1795694828033447, Classifier Loss 0.10140900313854218, Total Loss 37.50813293457031\n",
      "9: Encoding Loss 2.970156192779541, Transition Loss -0.18390852212905884, Classifier Loss 0.13347351551055908, Total Loss 31.168214797973633\n",
      "9: Encoding Loss 2.980299711227417, Transition Loss 0.8072517514228821, Classifier Loss 0.04708337411284447, Total Loss 22.91303825378418\n",
      "9: Encoding Loss 6.583588600158691, Transition Loss 0.08792534470558167, Classifier Loss 0.09163988381624222, Total Loss 48.70069122314453\n",
      "9: Encoding Loss 3.98514986038208, Transition Loss -2.2493112087249756, Classifier Loss 0.039597056806087494, Total Loss 27.869705200195312\n",
      "9: Encoding Loss 9.218000411987305, Transition Loss -2.111257553100586, Classifier Loss 0.13160966336727142, Total Loss 68.46812438964844\n",
      "9: Encoding Loss 7.075169563293457, Transition Loss -1.1132545471191406, Classifier Loss 0.17751498520374298, Total Loss 60.20207214355469\n",
      "9: Encoding Loss 4.242972373962402, Transition Loss -1.6479969024658203, Classifier Loss 0.06559283286333084, Total Loss 32.01646041870117\n",
      "9: Encoding Loss 6.216368675231934, Transition Loss -0.3792106509208679, Classifier Loss 0.10045593231916428, Total Loss 47.34365463256836\n",
      "9: Encoding Loss 6.611880302429199, Transition Loss -0.9539581537246704, Classifier Loss 0.0787372887134552, Total Loss 47.54463195800781\n",
      "9: Encoding Loss 5.673192024230957, Transition Loss -1.0845834016799927, Classifier Loss 0.04072768613696098, Total Loss 38.111488342285156\n",
      "9: Encoding Loss 6.878716468811035, Transition Loss -1.306065320968628, Classifier Loss 0.06886711716651917, Total Loss 48.15848922729492\n",
      "9: Encoding Loss 4.283792495727539, Transition Loss -2.603170156478882, Classifier Loss 0.08797057718038559, Total Loss 34.498775482177734\n",
      "9: Encoding Loss 3.845210552215576, Transition Loss -9.715557098388672e-06, Classifier Loss 0.08306380361318588, Total Loss 31.377643585205078\n",
      "9: Encoding Loss 2.9369616508483887, Transition Loss -0.9283384680747986, Classifier Loss 0.06329680234193802, Total Loss 23.951078414916992\n",
      "9: Encoding Loss 4.108612060546875, Transition Loss -1.253669023513794, Classifier Loss 0.08516835421323776, Total Loss 33.16801071166992\n",
      "9: Encoding Loss 5.777016639709473, Transition Loss -0.19942009449005127, Classifier Loss 0.10443753749132156, Total Loss 45.105777740478516\n",
      "9: Encoding Loss 6.1825385093688965, Transition Loss -0.9057304263114929, Classifier Loss 0.19629094004631042, Total Loss 56.72396469116211\n",
      "9: Encoding Loss 4.786693572998047, Transition Loss -1.8574126958847046, Classifier Loss 0.137749582529068, Total Loss 42.49437713623047\n",
      "9: Encoding Loss 6.949278354644775, Transition Loss -1.5240201950073242, Classifier Loss 0.09684448689222336, Total Loss 51.37950897216797\n",
      "9: Encoding Loss 4.495787143707275, Transition Loss -1.8606666326522827, Classifier Loss 0.06115500628948212, Total Loss 33.089481353759766\n",
      "9: Encoding Loss 2.697183132171631, Transition Loss -1.3570877313613892, Classifier Loss 0.06888537853956223, Total Loss 23.071094512939453\n",
      "9: Encoding Loss 5.8647589683532715, Transition Loss -2.3421759605407715, Classifier Loss 0.06553763896226883, Total Loss 41.74138259887695\n",
      "9: Encoding Loss 6.914783477783203, Transition Loss -2.398643732070923, Classifier Loss 0.19300855696201324, Total Loss 60.788597106933594\n",
      "9: Encoding Loss 5.094416618347168, Transition Loss -2.174826145172119, Classifier Loss 0.042895447462797165, Total Loss 34.85517501831055\n",
      "9: Encoding Loss 3.101217031478882, Transition Loss -1.1316603422164917, Classifier Loss 0.05000615864992142, Total Loss 23.607467651367188\n",
      "9: Encoding Loss 2.4515268802642822, Transition Loss -3.614872694015503, Classifier Loss 0.05837467685341835, Total Loss 20.545183181762695\n",
      "9: Encoding Loss 1.3044815063476562, Transition Loss -1.0114681720733643, Classifier Loss 0.1367540806531906, Total Loss 21.501893997192383\n",
      "9: Encoding Loss 6.053380489349365, Transition Loss -1.2586108446121216, Classifier Loss 0.07627268880605698, Total Loss 43.947052001953125\n",
      "9: Encoding Loss 5.425105094909668, Transition Loss -1.6425641775131226, Classifier Loss 0.10287056863307953, Total Loss 42.837032318115234\n",
      "9: Encoding Loss 4.025062561035156, Transition Loss -1.468908429145813, Classifier Loss 0.056625645607709885, Total Loss 29.812355041503906\n",
      "9: Encoding Loss 6.102368354797363, Transition Loss -1.1867316961288452, Classifier Loss 0.11441976577043533, Total Loss 48.05571746826172\n",
      "9: Encoding Loss 6.085740089416504, Transition Loss -1.095851182937622, Classifier Loss 0.18572339415550232, Total Loss 55.08634567260742\n",
      "9: Encoding Loss 4.684163570404053, Transition Loss 0.13829627633094788, Classifier Loss 0.07057897746562958, Total Loss 35.218196868896484\n",
      "9: Encoding Loss 5.315138816833496, Transition Loss 0.21224284172058105, Classifier Loss 0.10321154445409775, Total Loss 42.2968864440918\n",
      "9: Encoding Loss 3.6511783599853516, Transition Loss -1.7948508262634277, Classifier Loss 0.056816376745700836, Total Loss 27.587989807128906\n",
      "9: Encoding Loss 7.941459655761719, Transition Loss 0.10651353001594543, Classifier Loss 0.05699298903346062, Total Loss 53.390663146972656\n",
      "9: Encoding Loss 5.894815444946289, Transition Loss -1.4911811351776123, Classifier Loss 0.05493852123618126, Total Loss 40.86214828491211\n",
      "9: Encoding Loss 6.306392669677734, Transition Loss -0.5024100542068481, Classifier Loss 0.10389674454927444, Total Loss 48.22782897949219\n",
      "9: Encoding Loss 5.272543907165527, Transition Loss -1.0015833377838135, Classifier Loss 0.04819630831480026, Total Loss 36.45449447631836\n",
      "9: Encoding Loss 5.192765712738037, Transition Loss -0.16453902423381805, Classifier Loss 0.0792514979839325, Total Loss 39.08168029785156\n",
      "9: Encoding Loss 3.5845344066619873, Transition Loss -1.2429546117782593, Classifier Loss 0.0783456563949585, Total Loss 29.341276168823242\n",
      "9: Encoding Loss 5.152958393096924, Transition Loss -0.8673475980758667, Classifier Loss 0.18811285495758057, Total Loss 49.72868728637695\n",
      "9: Encoding Loss 2.858160972595215, Transition Loss -1.630350112915039, Classifier Loss 0.0504935085773468, Total Loss 22.197664260864258\n",
      "9: Encoding Loss 6.589230537414551, Transition Loss -1.210075855255127, Classifier Loss 0.2978294789791107, Total Loss 69.3178482055664\n",
      "9: Encoding Loss 6.055959224700928, Transition Loss -0.6602124571800232, Classifier Loss 0.2052917629480362, Total Loss 56.86467361450195\n",
      "9: Encoding Loss 3.918605327606201, Transition Loss -1.5631963014602661, Classifier Loss 0.13605715334415436, Total Loss 37.116722106933594\n",
      "9: Encoding Loss 5.704063415527344, Transition Loss -2.0038130283355713, Classifier Loss 0.06921502202749252, Total Loss 41.14508056640625\n",
      "9: Encoding Loss 6.903194427490234, Transition Loss -0.3208285868167877, Classifier Loss 0.08668351918458939, Total Loss 50.08738708496094\n",
      "9: Encoding Loss 7.1926422119140625, Transition Loss -0.6619079113006592, Classifier Loss 0.1698814481496811, Total Loss 60.143733978271484\n",
      "9: Encoding Loss 4.84232234954834, Transition Loss -1.9569145441055298, Classifier Loss 0.07558996230363846, Total Loss 36.61214828491211\n",
      "9: Encoding Loss 4.450216770172119, Transition Loss -3.387650966644287, Classifier Loss 0.09373865276575089, Total Loss 36.07381057739258\n",
      "9: Encoding Loss 6.911290168762207, Transition Loss -2.220487356185913, Classifier Loss 0.1636175662279129, Total Loss 57.828609466552734\n",
      "9: Encoding Loss 5.859980583190918, Transition Loss -0.8098940849304199, Classifier Loss 0.07274523377418518, Total Loss 42.434085845947266\n",
      "9: Encoding Loss 3.7962474822998047, Transition Loss -2.3904190063476562, Classifier Loss 0.08830143511295319, Total Loss 31.606672286987305\n",
      "9: Encoding Loss 2.600006580352783, Transition Loss -0.9105240106582642, Classifier Loss 0.08693350106477737, Total Loss 24.29302406311035\n",
      "9: Encoding Loss 7.518153190612793, Transition Loss -2.0874645709991455, Classifier Loss 0.115836963057518, Total Loss 56.6917839050293\n",
      "9: Encoding Loss 4.7869367599487305, Transition Loss 0.01778484880924225, Classifier Loss 0.09245732426643372, Total Loss 37.97446823120117\n",
      "9: Encoding Loss 5.939919948577881, Transition Loss -0.6936647891998291, Classifier Loss 0.06886576116085052, Total Loss 42.525821685791016\n",
      "9: Encoding Loss 3.846342086791992, Transition Loss -0.9738286733627319, Classifier Loss 0.044708915054798126, Total Loss 27.548555374145508\n",
      "9: Encoding Loss 4.039768218994141, Transition Loss -1.4731818437576294, Classifier Loss 0.06695997714996338, Total Loss 30.934019088745117\n",
      "9: Encoding Loss 7.95711612701416, Transition Loss 0.176792174577713, Classifier Loss 0.16328608989715576, Total Loss 64.14202117919922\n",
      "9: Encoding Loss 5.741086483001709, Transition Loss -1.3826515674591064, Classifier Loss 0.17245492339134216, Total Loss 51.691463470458984\n",
      "9: Encoding Loss 5.515936851501465, Transition Loss -3.110435724258423, Classifier Loss 0.15821099281311035, Total Loss 48.91547775268555\n",
      "9: Encoding Loss 3.746626615524292, Transition Loss -2.0153777599334717, Classifier Loss 0.06290031969547272, Total Loss 28.768985748291016\n",
      "9: Encoding Loss 7.43240213394165, Transition Loss -1.8620465993881226, Classifier Loss 0.14070650935173035, Total Loss 58.66432189941406\n",
      "9: Encoding Loss 5.34200382232666, Transition Loss -0.7121034860610962, Classifier Loss 0.05567213147878647, Total Loss 37.618953704833984\n",
      "9: Encoding Loss 4.686158180236816, Transition Loss -1.431483268737793, Classifier Loss 0.069495789706707, Total Loss 35.065956115722656\n",
      "9: Encoding Loss 3.9088661670684814, Transition Loss -1.9716566801071167, Classifier Loss 0.042640261352062225, Total Loss 27.7164363861084\n",
      "9: Encoding Loss 5.256173610687256, Transition Loss -1.3007556200027466, Classifier Loss 0.17473220825195312, Total Loss 49.009742736816406\n",
      "9: Encoding Loss 6.073346138000488, Transition Loss -3.2043612003326416, Classifier Loss 0.11184289306402206, Total Loss 47.623085021972656\n",
      "9: Encoding Loss 4.040406227111816, Transition Loss -0.7755681872367859, Classifier Loss 0.08743105828762054, Total Loss 32.985233306884766\n",
      "9: Encoding Loss 6.696835517883301, Transition Loss -0.10564850270748138, Classifier Loss 0.14871534705162048, Total Loss 55.05250930786133\n",
      "9: Encoding Loss 6.851415157318115, Transition Loss -1.3926692008972168, Classifier Loss 0.1348552405834198, Total Loss 54.59346008300781\n",
      "9: Encoding Loss 5.412416934967041, Transition Loss -2.6220223903656006, Classifier Loss 0.10415371507406235, Total Loss 42.888824462890625\n",
      "9: Encoding Loss 6.455220699310303, Transition Loss -1.1289364099502563, Classifier Loss 0.048249971121549606, Total Loss 43.55587387084961\n",
      "9: Encoding Loss 5.2399492263793945, Transition Loss 0.44968095421791077, Classifier Loss 0.09734120219945908, Total Loss 41.35368728637695\n",
      "9: Encoding Loss 5.457882881164551, Transition Loss -0.6636590957641602, Classifier Loss 0.12933063507080078, Total Loss 45.68009567260742\n",
      "9: Encoding Loss 7.066994667053223, Transition Loss -0.7831199169158936, Classifier Loss 0.05325213074684143, Total Loss 47.726871490478516\n",
      "9: Encoding Loss 5.0823469161987305, Transition Loss -2.2463624477386475, Classifier Loss 0.051317617297172546, Total Loss 35.62494659423828\n",
      "9: Encoding Loss 6.9249677658081055, Transition Loss -1.5532101392745972, Classifier Loss 0.14749515056610107, Total Loss 56.298702239990234\n",
      "9: Encoding Loss 5.195465087890625, Transition Loss -0.9529297351837158, Classifier Loss 0.20099247992038727, Total Loss 51.27165985107422\n",
      "9: Encoding Loss 5.467288017272949, Transition Loss -2.6652748584747314, Classifier Loss 0.11218897998332977, Total Loss 44.02156448364258\n",
      "9: Encoding Loss 3.3233840465545654, Transition Loss 0.465396523475647, Classifier Loss 0.09379228204488754, Total Loss 29.505693435668945\n",
      "9: Encoding Loss 3.335242748260498, Transition Loss -0.6002996563911438, Classifier Loss 0.08085805922746658, Total Loss 28.097023010253906\n",
      "9: Encoding Loss 2.4145331382751465, Transition Loss -1.0799604654312134, Classifier Loss 0.07211516797542572, Total Loss 21.698286056518555\n",
      "9: Encoding Loss 7.7519731521606445, Transition Loss -2.113887071609497, Classifier Loss 0.14731688797473907, Total Loss 61.24268341064453\n",
      "9: Encoding Loss 4.804806709289551, Transition Loss -1.4957451820373535, Classifier Loss 0.10224082320928574, Total Loss 39.05232238769531\n",
      "9: Encoding Loss 6.362512588500977, Transition Loss -1.603647232055664, Classifier Loss 0.0961308479309082, Total Loss 47.78752136230469\n",
      "9: Encoding Loss 4.413185119628906, Transition Loss -1.1203209161758423, Classifier Loss 0.11966562271118164, Total Loss 38.445228576660156\n",
      "9: Encoding Loss 4.461116790771484, Transition Loss -1.8050451278686523, Classifier Loss 0.12236980348825455, Total Loss 39.00296401977539\n",
      "9: Encoding Loss 4.084835052490234, Transition Loss -1.7786678075790405, Classifier Loss 0.14137589931488037, Total Loss 38.64588928222656\n",
      "9: Encoding Loss 4.634049415588379, Transition Loss -1.4148155450820923, Classifier Loss 0.10891162604093552, Total Loss 38.69489288330078\n",
      "9: Encoding Loss 6.5198822021484375, Transition Loss -1.082207202911377, Classifier Loss 0.12374584376811981, Total Loss 51.493446350097656\n",
      "9: Encoding Loss 5.016515731811523, Transition Loss -0.6856576204299927, Classifier Loss 0.24032080173492432, Total Loss 54.13090133666992\n",
      "9: Encoding Loss 6.959800720214844, Transition Loss -1.834707498550415, Classifier Loss 0.09658626466989517, Total Loss 51.41669845581055\n",
      "9: Encoding Loss 4.414346694946289, Transition Loss -1.925323724746704, Classifier Loss 0.07872863113880157, Total Loss 34.35817337036133\n",
      "9: Encoding Loss 6.168108940124512, Transition Loss -1.5805357694625854, Classifier Loss 0.0822613313794136, Total Loss 45.234153747558594\n",
      "9: Encoding Loss 4.411439418792725, Transition Loss -0.9434417486190796, Classifier Loss 0.0807972103357315, Total Loss 34.54798126220703\n",
      "9: Encoding Loss 4.347290992736816, Transition Loss -0.9280696511268616, Classifier Loss 0.055444251745939255, Total Loss 31.627798080444336\n",
      "9: Encoding Loss 3.0932726860046387, Transition Loss -0.4213922619819641, Classifier Loss 0.07136144489049911, Total Loss 25.695613861083984\n",
      "9: Encoding Loss 5.57480525970459, Transition Loss -1.0193032026290894, Classifier Loss 0.07339544594287872, Total Loss 40.78797149658203\n",
      "9: Encoding Loss 5.6331467628479, Transition Loss -0.7484616041183472, Classifier Loss 0.16298259794712067, Total Loss 50.09684371948242\n",
      "9: Encoding Loss 3.58550763130188, Transition Loss -1.531301498413086, Classifier Loss 0.16825337707996368, Total Loss 38.337772369384766\n",
      "9: Encoding Loss 4.0295515060424805, Transition Loss -0.052911996841430664, Classifier Loss 0.10887324810028076, Total Loss 35.064613342285156\n",
      "9: Encoding Loss 7.065517425537109, Transition Loss -1.3259565830230713, Classifier Loss 0.1951998770236969, Total Loss 61.91256332397461\n",
      "9: Encoding Loss 5.966866970062256, Transition Loss -0.3014415204524994, Classifier Loss 0.09425269067287445, Total Loss 45.22635269165039\n",
      "9: Encoding Loss 5.203494071960449, Transition Loss -0.8581051230430603, Classifier Loss 0.043331075459718704, Total Loss 35.55373001098633\n",
      "9: Encoding Loss 6.596170902252197, Transition Loss -0.47465384006500244, Classifier Loss 0.11537010967731476, Total Loss 51.11384582519531\n",
      "9: Encoding Loss 4.678109645843506, Transition Loss -3.109607696533203, Classifier Loss 0.12343779951334, Total Loss 40.41119384765625\n",
      "9: Encoding Loss 3.3886585235595703, Transition Loss 1.1361364126205444, Classifier Loss 0.07664470374584198, Total Loss 28.450876235961914\n",
      "9: Encoding Loss 3.5481553077697754, Transition Loss -2.3925938606262207, Classifier Loss 0.07701859623193741, Total Loss 28.98983383178711\n",
      "9: Encoding Loss 5.838345050811768, Transition Loss -2.137733221054077, Classifier Loss 0.03973248600959778, Total Loss 39.002464294433594\n",
      "9: Encoding Loss 6.568045139312744, Transition Loss -1.687572956085205, Classifier Loss 0.07147282361984253, Total Loss 46.55487823486328\n",
      "9: Encoding Loss 6.043751239776611, Transition Loss -1.1921378374099731, Classifier Loss 0.05835038051009178, Total Loss 42.097068786621094\n",
      "9: Encoding Loss 4.570889949798584, Transition Loss -0.7937720417976379, Classifier Loss 0.11112169921398163, Total Loss 38.537193298339844\n",
      "9: Encoding Loss 4.943312168121338, Transition Loss -0.7418590188026428, Classifier Loss 0.045988380908966064, Total Loss 34.25841522216797\n",
      "9: Encoding Loss 3.0374181270599365, Transition Loss -0.7635841965675354, Classifier Loss 0.08927051723003387, Total Loss 27.151256561279297\n",
      "9: Encoding Loss 6.761775970458984, Transition Loss -2.428649425506592, Classifier Loss 0.07935070246458054, Total Loss 48.50475311279297\n",
      "9: Encoding Loss 4.836244583129883, Transition Loss -1.4441754817962646, Classifier Loss 0.057192590087652206, Total Loss 34.73615264892578\n",
      "9: Encoding Loss 6.260918617248535, Transition Loss -1.649435043334961, Classifier Loss 0.1453631967306137, Total Loss 52.101173400878906\n",
      "9: Encoding Loss 4.38712215423584, Transition Loss -1.4692145586013794, Classifier Loss 0.025752387940883636, Total Loss 28.89738655090332\n",
      "9: Encoding Loss 5.6061882972717285, Transition Loss -0.19458597898483276, Classifier Loss 0.07091169059276581, Total Loss 40.72822189331055\n",
      "9: Encoding Loss 5.051810264587402, Transition Loss -1.3583285808563232, Classifier Loss 0.129075825214386, Total Loss 43.21790313720703\n",
      "9: Encoding Loss 4.522430896759033, Transition Loss -1.4052581787109375, Classifier Loss 0.07743089646100998, Total Loss 34.87711715698242\n",
      "9: Encoding Loss 2.9163413047790527, Transition Loss -2.442795515060425, Classifier Loss 0.06072505936026573, Total Loss 23.569578170776367\n",
      "9: Encoding Loss 5.460287570953369, Transition Loss -3.4479658603668213, Classifier Loss 0.05814898759126663, Total Loss 38.57524490356445\n",
      "9: Encoding Loss 3.5043575763702393, Transition Loss -0.659697949886322, Classifier Loss 0.14013062417507172, Total Loss 35.038944244384766\n",
      "9: Encoding Loss 4.945529937744141, Transition Loss -0.8943630456924438, Classifier Loss 0.15409715473651886, Total Loss 45.08253860473633\n",
      "9: Encoding Loss 5.554233551025391, Transition Loss -0.5553910732269287, Classifier Loss 0.10200968384742737, Total Loss 43.52614974975586\n",
      "9: Encoding Loss 3.9481301307678223, Transition Loss -0.3958737254142761, Classifier Loss 0.04613607004284859, Total Loss 28.302230834960938\n",
      "9: Encoding Loss 8.578924179077148, Transition Loss -0.14379721879959106, Classifier Loss 0.1304498314857483, Total Loss 64.51847076416016\n",
      "9: Encoding Loss 4.501633644104004, Transition Loss -0.990001916885376, Classifier Loss 0.08496404439210892, Total Loss 35.50580978393555\n",
      "9: Encoding Loss 7.833180904388428, Transition Loss -0.37322723865509033, Classifier Loss 0.19532856345176697, Total Loss 66.53179168701172\n",
      "9: Encoding Loss 5.733667850494385, Transition Loss -2.7111220359802246, Classifier Loss 0.17197537422180176, Total Loss 51.59846115112305\n",
      "9: Encoding Loss 5.731889247894287, Transition Loss -1.059065818786621, Classifier Loss 0.11163809150457382, Total Loss 45.55472183227539\n",
      "9: Encoding Loss 5.5716047286987305, Transition Loss -1.8499596118927002, Classifier Loss 0.14969253540039062, Total Loss 48.39814376831055\n",
      "9: Encoding Loss 3.1608800888061523, Transition Loss -1.305598497390747, Classifier Loss 0.08077699691057205, Total Loss 27.042457580566406\n",
      "9: Encoding Loss 3.796471118927002, Transition Loss -0.3893547058105469, Classifier Loss 0.08253588527441025, Total Loss 31.03226089477539\n",
      "9: Encoding Loss 4.39931058883667, Transition Loss -0.7920714616775513, Classifier Loss 0.10446685552597046, Total Loss 36.84223556518555\n",
      "9: Encoding Loss 4.4254655838012695, Transition Loss -2.770233631134033, Classifier Loss 0.07465218752622604, Total Loss 34.01690673828125\n",
      "9: Encoding Loss 5.5514960289001465, Transition Loss -1.5551831722259521, Classifier Loss 0.13168364763259888, Total Loss 46.476722717285156\n",
      "9: Encoding Loss 4.850641250610352, Transition Loss -1.4188368320465088, Classifier Loss 0.0891692265868187, Total Loss 38.02020263671875\n",
      "9: Encoding Loss 3.422884464263916, Transition Loss -0.3500403165817261, Classifier Loss 0.09006837010383606, Total Loss 29.544004440307617\n",
      "9: Encoding Loss 4.088473796844482, Transition Loss -1.6335234642028809, Classifier Loss 0.05051552876830101, Total Loss 29.581743240356445\n",
      "9: Encoding Loss 5.957360744476318, Transition Loss -1.243507981300354, Classifier Loss 0.12712432444095612, Total Loss 48.45610427856445\n",
      "9: Encoding Loss 4.73825740814209, Transition Loss 0.16321837902069092, Classifier Loss 0.061380527913570404, Total Loss 34.63288879394531\n",
      "9: Encoding Loss 7.031259059906006, Transition Loss 0.10919451713562012, Classifier Loss 0.09548016637563705, Total Loss 51.77925109863281\n",
      "9: Encoding Loss 6.314234256744385, Transition Loss -1.7335140705108643, Classifier Loss 0.2763930857181549, Total Loss 65.5240249633789\n",
      "9: Encoding Loss 6.081197261810303, Transition Loss -1.7255120277404785, Classifier Loss 0.05990706384181976, Total Loss 42.477203369140625\n",
      "9: Encoding Loss 5.181645393371582, Transition Loss -1.31178879737854, Classifier Loss 0.11974384635686874, Total Loss 43.06372833251953\n",
      "9: Encoding Loss 4.368075370788574, Transition Loss -1.2671650648117065, Classifier Loss 0.07511227577924728, Total Loss 33.719173431396484\n",
      "9: Encoding Loss 5.750739097595215, Transition Loss -2.107422351837158, Classifier Loss 0.1925177425146103, Total Loss 53.755367279052734\n",
      "9: Encoding Loss 3.761169910430908, Transition Loss -0.9863932728767395, Classifier Loss 0.06316433846950531, Total Loss 28.883058547973633\n",
      "9: Encoding Loss 6.927778720855713, Transition Loss -2.162818670272827, Classifier Loss 0.1200231984257698, Total Loss 53.5681266784668\n",
      "9: Encoding Loss 5.736762523651123, Transition Loss -1.4301948547363281, Classifier Loss 0.1396113634109497, Total Loss 48.381141662597656\n",
      "9: Encoding Loss 5.959840297698975, Transition Loss -1.8378666639328003, Classifier Loss 0.053478434681892395, Total Loss 41.10615158081055\n",
      "9: Encoding Loss 6.515567779541016, Transition Loss -0.8401703834533691, Classifier Loss 0.13855485618114471, Total Loss 52.94855499267578\n",
      "9: Encoding Loss 2.4472620487213135, Transition Loss -1.986369013786316, Classifier Loss 0.0648009330034256, Total Loss 21.162870407104492\n",
      "9: Encoding Loss 2.582703113555908, Transition Loss -0.6261027455329895, Classifier Loss 0.052594032138586044, Total Loss 20.755373001098633\n",
      "9: Encoding Loss 5.910323143005371, Transition Loss -0.4365806579589844, Classifier Loss 0.09647175669670105, Total Loss 45.10894012451172\n",
      "9: Encoding Loss 4.055851936340332, Transition Loss 0.46952712535858154, Classifier Loss 0.07841834425926208, Total Loss 32.36475372314453\n",
      "9: Encoding Loss 5.890982627868652, Transition Loss -0.9381654262542725, Classifier Loss 0.16156208515167236, Total Loss 51.501731872558594\n",
      "9: Encoding Loss 5.655719757080078, Transition Loss -1.89854097366333, Classifier Loss 0.06766818463802338, Total Loss 40.70037841796875\n",
      "9: Encoding Loss 3.778883457183838, Transition Loss -2.5588762760162354, Classifier Loss 0.10662703216075897, Total Loss 33.334983825683594\n",
      "9: Encoding Loss 3.051867961883545, Transition Loss -1.509369969367981, Classifier Loss 0.07115329802036285, Total Loss 25.425933837890625\n",
      "9: Encoding Loss 7.764412879943848, Transition Loss -1.450851559638977, Classifier Loss 0.16906748712062836, Total Loss 63.492645263671875\n",
      "9: Encoding Loss 4.290316104888916, Transition Loss -0.13566261529922485, Classifier Loss 0.16237980127334595, Total Loss 41.97982406616211\n",
      "9: Encoding Loss 4.673417568206787, Transition Loss -1.4779059886932373, Classifier Loss 0.05245298892259598, Total Loss 33.285213470458984\n",
      "9: Encoding Loss 5.391690731048584, Transition Loss 0.15027421712875366, Classifier Loss 0.13692796230316162, Total Loss 46.10305404663086\n",
      "9: Encoding Loss 3.12966251373291, Transition Loss 0.6168026924133301, Classifier Loss 0.059683747589588165, Total Loss 24.993070602416992\n",
      "9: Encoding Loss 3.6949715614318848, Transition Loss -1.491161823272705, Classifier Loss 0.0838373526930809, Total Loss 30.552968978881836\n",
      "9: Encoding Loss 6.1282243728637695, Transition Loss -0.8856428861618042, Classifier Loss 0.09612995386123657, Total Loss 46.381988525390625\n",
      "9: Encoding Loss 3.7019405364990234, Transition Loss -0.9350758194923401, Classifier Loss 0.11594843864440918, Total Loss 33.806114196777344\n",
      "9: Encoding Loss 6.423164367675781, Transition Loss -0.9853318333625793, Classifier Loss 0.14288686215877533, Total Loss 52.82727813720703\n",
      "9: Encoding Loss 6.720771789550781, Transition Loss -1.3638702630996704, Classifier Loss 0.18995536863803864, Total Loss 59.31962203979492\n",
      "9: Encoding Loss 4.593920707702637, Transition Loss -1.1751208305358887, Classifier Loss 0.03512296825647354, Total Loss 31.075353622436523\n",
      "9: Encoding Loss 4.717589378356934, Transition Loss -0.44659659266471863, Classifier Loss 0.1619516909122467, Total Loss 44.50053024291992\n",
      "9: Encoding Loss 5.961588382720947, Transition Loss -3.3900375366210938, Classifier Loss 0.168779194355011, Total Loss 52.64609909057617\n",
      "9: Encoding Loss 6.665139675140381, Transition Loss -1.5970677137374878, Classifier Loss 0.05828436464071274, Total Loss 45.818641662597656\n",
      "9: Encoding Loss 5.059322357177734, Transition Loss -1.2992151975631714, Classifier Loss 0.07102259248495102, Total Loss 37.45767593383789\n",
      "9: Encoding Loss 6.189278602600098, Transition Loss -2.2837166786193848, Classifier Loss 0.14223067462444305, Total Loss 51.35783004760742\n",
      "9: Encoding Loss 5.246800899505615, Transition Loss -0.4263414740562439, Classifier Loss 0.06797998398542404, Total Loss 38.27863311767578\n",
      "9: Encoding Loss 2.928943157196045, Transition Loss -1.5497519969940186, Classifier Loss 0.07710103690624237, Total Loss 25.283143997192383\n",
      "9: Encoding Loss 5.098084449768066, Transition Loss -1.9393622875213623, Classifier Loss 0.11831741034984589, Total Loss 42.41947555541992\n",
      "9: Encoding Loss 9.336939811706543, Transition Loss -3.286679983139038, Classifier Loss 0.16747009754180908, Total Loss 72.767333984375\n",
      "9: Encoding Loss 5.235777378082275, Transition Loss -1.3938549757003784, Classifier Loss 0.05624837428331375, Total Loss 37.038944244384766\n",
      "9: Encoding Loss 5.3184614181518555, Transition Loss -1.0185508728027344, Classifier Loss 0.0477919764816761, Total Loss 36.68955993652344\n",
      "9: Encoding Loss 6.033759593963623, Transition Loss -0.20812638103961945, Classifier Loss 0.15650735795497894, Total Loss 51.85321044921875\n",
      "9: Encoding Loss 6.065716743469238, Transition Loss -0.8558531999588013, Classifier Loss 0.27544349431991577, Total Loss 63.93830871582031\n",
      "9: Encoding Loss 6.213456630706787, Transition Loss -1.6089234352111816, Classifier Loss 0.18303757905960083, Total Loss 55.583858489990234\n",
      "9: Encoding Loss 6.359273433685303, Transition Loss -2.1779861450195312, Classifier Loss 0.14962363243103027, Total Loss 53.11713790893555\n",
      "9: Encoding Loss 5.142623424530029, Transition Loss -1.4062219858169556, Classifier Loss 0.1161612719297409, Total Loss 42.471309661865234\n",
      "9: Encoding Loss 5.444014072418213, Transition Loss -0.5037660598754883, Classifier Loss 0.14151889085769653, Total Loss 46.815773010253906\n",
      "9: Encoding Loss 6.705361843109131, Transition Loss -0.7570163607597351, Classifier Loss 0.165394127368927, Total Loss 56.77128601074219\n",
      "9: Encoding Loss 4.646450996398926, Transition Loss -0.5742789506912231, Classifier Loss 0.058484602719545364, Total Loss 33.72693634033203\n",
      "9: Encoding Loss 7.762322902679443, Transition Loss -2.079831600189209, Classifier Loss 0.07782965153455734, Total Loss 54.356075286865234\n",
      "9: Encoding Loss 4.810516357421875, Transition Loss -0.9992380738258362, Classifier Loss 0.09675208479166031, Total Loss 38.537906646728516\n",
      "9: Encoding Loss 4.1175456047058105, Transition Loss -1.7087340354919434, Classifier Loss 0.08822258561849594, Total Loss 33.526851654052734\n",
      "9: Encoding Loss 5.547465801239014, Transition Loss -0.44691088795661926, Classifier Loss 0.17287836968898773, Total Loss 50.572452545166016\n",
      "9: Encoding Loss 4.8606462478637695, Transition Loss -1.4549555778503418, Classifier Loss 0.07281488180160522, Total Loss 36.44478225708008\n",
      "9: Encoding Loss 5.117066383361816, Transition Loss -1.4830743074417114, Classifier Loss 0.1437983512878418, Total Loss 45.08163833618164\n",
      "9: Encoding Loss 10.75018310546875, Transition Loss -2.208338737487793, Classifier Loss 0.12855973839759827, Total Loss 77.35618591308594\n",
      "9: Encoding Loss 6.025964260101318, Transition Loss -0.7130380868911743, Classifier Loss 0.0658172219991684, Total Loss 42.73722457885742\n",
      "9: Encoding Loss 6.798750877380371, Transition Loss 0.7680469155311584, Classifier Loss 0.06047136336565018, Total Loss 47.14686584472656\n",
      "9: Encoding Loss 4.534359931945801, Transition Loss -2.2844126224517822, Classifier Loss 0.11056854575872421, Total Loss 38.26210021972656\n",
      "9: Encoding Loss 6.052201271057129, Transition Loss -1.5637620687484741, Classifier Loss 0.11534041166305542, Total Loss 47.84662628173828\n",
      "9: Encoding Loss 3.7631354331970215, Transition Loss -0.531374454498291, Classifier Loss 0.0888308733701706, Total Loss 31.461688995361328\n",
      "9: Encoding Loss 5.641983985900879, Transition Loss -1.4063974618911743, Classifier Loss 0.1954120546579361, Total Loss 53.39255142211914\n",
      "9: Encoding Loss 9.143854141235352, Transition Loss -1.3315738439559937, Classifier Loss 0.11328381299972534, Total Loss 66.19097137451172\n",
      "9: Encoding Loss 4.664886474609375, Transition Loss -1.796320915222168, Classifier Loss 0.0926896259188652, Total Loss 37.257568359375\n",
      "9: Encoding Loss 5.35460090637207, Transition Loss -1.0973767042160034, Classifier Loss 0.07004956901073456, Total Loss 39.13212203979492\n",
      "9: Encoding Loss 5.796619892120361, Transition Loss -2.118149518966675, Classifier Loss 0.04797902703285217, Total Loss 39.57677459716797\n",
      "9: Encoding Loss 5.065358638763428, Transition Loss -2.223691940307617, Classifier Loss 0.04723285883665085, Total Loss 35.11455154418945\n",
      "9: Encoding Loss 4.484314918518066, Transition Loss -1.0813591480255127, Classifier Loss 0.03479974716901779, Total Loss 30.38543128967285\n",
      "9: Encoding Loss 4.201253414154053, Transition Loss -2.478689432144165, Classifier Loss 0.10701967775821686, Total Loss 35.90849685668945\n",
      "9: Encoding Loss 6.446928977966309, Transition Loss -0.5272979140281677, Classifier Loss 0.10560674965381622, Total Loss 49.24203872680664\n",
      "9: Encoding Loss 5.511809349060059, Transition Loss -0.7434110045433044, Classifier Loss 0.06768371164798737, Total Loss 39.838932037353516\n",
      "9: Encoding Loss 5.4029951095581055, Transition Loss -3.0932517051696777, Classifier Loss 0.10132405906915665, Total Loss 42.54914093017578\n",
      "9: Encoding Loss 6.879067897796631, Transition Loss -3.1033620834350586, Classifier Loss 0.12341563403606415, Total Loss 53.6147346496582\n",
      "9: Encoding Loss 4.605277061462402, Transition Loss 0.09961584210395813, Classifier Loss 0.06352648884057999, Total Loss 34.0241584777832\n",
      "9: Encoding Loss 4.733263969421387, Transition Loss -1.9106647968292236, Classifier Loss 0.15580369532108307, Total Loss 43.97919464111328\n",
      "9: Encoding Loss 5.031001091003418, Transition Loss -1.3689756393432617, Classifier Loss 0.07933217287063599, Total Loss 38.118675231933594\n",
      "9: Encoding Loss 3.610995054244995, Transition Loss -1.900083303451538, Classifier Loss 0.07373519241809845, Total Loss 29.038732528686523\n",
      "9: Encoding Loss 4.9020490646362305, Transition Loss -2.794606924057007, Classifier Loss 0.14813607931137085, Total Loss 44.224788665771484\n",
      "9: Encoding Loss 6.462865352630615, Transition Loss -2.211838483810425, Classifier Loss 0.09125866740942001, Total Loss 47.90217590332031\n",
      "9: Encoding Loss 6.461002349853516, Transition Loss -2.808502674102783, Classifier Loss 0.10000132769346237, Total Loss 48.7650260925293\n",
      "9: Encoding Loss 7.393246650695801, Transition Loss -1.0484508275985718, Classifier Loss 0.1555882692337036, Total Loss 59.91788864135742\n",
      "9: Encoding Loss 5.013294696807861, Transition Loss 0.39667809009552, Classifier Loss 0.09697267413139343, Total Loss 39.93571090698242\n",
      "9: Encoding Loss 4.82408332824707, Transition Loss -2.6390366554260254, Classifier Loss 0.07835319638252258, Total Loss 36.77876663208008\n",
      "9: Encoding Loss 4.971571922302246, Transition Loss -1.8675403594970703, Classifier Loss 0.11340156942605972, Total Loss 41.16884231567383\n",
      "9: Encoding Loss 4.303776741027832, Transition Loss -1.7039175033569336, Classifier Loss 0.07819380611181259, Total Loss 33.641357421875\n",
      "9: Encoding Loss 4.244172096252441, Transition Loss -1.9732898473739624, Classifier Loss 0.09049209952354431, Total Loss 34.51345443725586\n",
      "9: Encoding Loss 3.4436581134796143, Transition Loss -2.23313045501709, Classifier Loss 0.05525441840291023, Total Loss 26.186498641967773\n",
      "9: Encoding Loss 3.5125670433044434, Transition Loss -0.6945856809616089, Classifier Loss 0.10155463963747025, Total Loss 31.230588912963867\n",
      "9: Encoding Loss 4.905829906463623, Transition Loss -0.9031846523284912, Classifier Loss 0.12003360688686371, Total Loss 41.4379768371582\n",
      "9: Encoding Loss 6.179098129272461, Transition Loss -0.35343289375305176, Classifier Loss 0.10055286437273026, Total Loss 47.12973403930664\n",
      "9: Encoding Loss 4.014446258544922, Transition Loss -0.6299331188201904, Classifier Loss 0.10440737009048462, Total Loss 34.527164459228516\n",
      "9: Encoding Loss 7.0678887367248535, Transition Loss -1.1980702877044678, Classifier Loss 0.11311498284339905, Total Loss 53.718353271484375\n",
      "9: Encoding Loss 6.30782413482666, Transition Loss -0.7989288568496704, Classifier Loss 0.1124735027551651, Total Loss 49.09397888183594\n",
      "9: Encoding Loss 4.662833213806152, Transition Loss -1.7261340618133545, Classifier Loss 0.14184483885765076, Total Loss 42.16079330444336\n",
      "9: Encoding Loss 4.966486930847168, Transition Loss -0.5869338512420654, Classifier Loss 0.10552031546831131, Total Loss 40.3507194519043\n",
      "9: Encoding Loss 7.732630729675293, Transition Loss -2.2286524772644043, Classifier Loss 0.16285675764083862, Total Loss 62.68056869506836\n",
      "9: Encoding Loss 4.891632556915283, Transition Loss -1.754221796989441, Classifier Loss 0.049747806042432785, Total Loss 34.323875427246094\n",
      "9: Encoding Loss 6.994105339050293, Transition Loss -0.9671502709388733, Classifier Loss 0.185395747423172, Total Loss 60.50382614135742\n",
      "9: Encoding Loss 5.7204461097717285, Transition Loss -0.48854780197143555, Classifier Loss 0.1378437727689743, Total Loss 48.10686111450195\n",
      "9: Encoding Loss 4.978902816772461, Transition Loss -1.7955973148345947, Classifier Loss 0.12200519442558289, Total Loss 42.073219299316406\n",
      "9: Encoding Loss 4.442819118499756, Transition Loss -2.7671778202056885, Classifier Loss 0.14307016134262085, Total Loss 40.962825775146484\n",
      "9: Encoding Loss 6.540837287902832, Transition Loss -2.1111788749694824, Classifier Loss 0.13540473580360413, Total Loss 52.7846565246582\n",
      "9: Encoding Loss 4.403983116149902, Transition Loss -0.7993255257606506, Classifier Loss 0.09110493212938309, Total Loss 35.53407287597656\n",
      "9: Encoding Loss 2.9767959117889404, Transition Loss -1.1841611862182617, Classifier Loss 0.07034336030483246, Total Loss 24.89463996887207\n",
      "9: Encoding Loss 2.559948205947876, Transition Loss -1.735801339149475, Classifier Loss 0.07635825872421265, Total Loss 22.994821548461914\n",
      "9: Encoding Loss 3.6335761547088623, Transition Loss -1.446655511856079, Classifier Loss 0.09062431007623672, Total Loss 30.863311767578125\n",
      "9: Encoding Loss 8.986786842346191, Transition Loss 0.6683743000030518, Classifier Loss 0.11210913956165314, Total Loss 65.39898681640625\n",
      "9: Encoding Loss 5.828393459320068, Transition Loss 0.1994645595550537, Classifier Loss 0.1466560810804367, Total Loss 49.715755462646484\n",
      "9: Encoding Loss 4.1966071128845215, Transition Loss 0.6281132698059082, Classifier Loss 0.0668695867061615, Total Loss 32.11784744262695\n",
      "9: Encoding Loss 7.685294151306152, Transition Loss -0.46361881494522095, Classifier Loss 0.2504304051399231, Total Loss 71.15462493896484\n",
      "9: Encoding Loss 4.637209892272949, Transition Loss -1.6175754070281982, Classifier Loss 0.05571989715099335, Total Loss 33.39460372924805\n",
      "9: Encoding Loss 4.686670780181885, Transition Loss -2.283501148223877, Classifier Loss 0.11431198567152023, Total Loss 39.55031204223633\n",
      "9: Encoding Loss 5.677779674530029, Transition Loss -1.2368977069854736, Classifier Loss 0.13794532418251038, Total Loss 47.8607177734375\n",
      "9: Encoding Loss 4.951578140258789, Transition Loss -1.4632537364959717, Classifier Loss 0.05382134020328522, Total Loss 35.09102249145508\n",
      "9: Encoding Loss 4.4677324295043945, Transition Loss -0.7103111147880554, Classifier Loss 0.06425579637289047, Total Loss 33.231693267822266\n",
      "9: Encoding Loss 6.008267402648926, Transition Loss -0.8376350998878479, Classifier Loss 0.10724298655986786, Total Loss 46.77356719970703\n",
      "9: Encoding Loss 3.647879123687744, Transition Loss 0.1136179268360138, Classifier Loss 0.03932543471455574, Total Loss 25.865264892578125\n",
      "9: Encoding Loss 6.965190887451172, Transition Loss 0.5129767656326294, Classifier Loss 0.07842160016298294, Total Loss 49.838497161865234\n",
      "9: Encoding Loss 5.440865516662598, Transition Loss -2.0212888717651367, Classifier Loss 0.1056305319070816, Total Loss 43.20743942260742\n",
      "9: Encoding Loss 4.161755561828613, Transition Loss -1.637259602546692, Classifier Loss 0.1326085776090622, Total Loss 38.230735778808594\n",
      "9: Encoding Loss 6.235537052154541, Transition Loss -1.9851858615875244, Classifier Loss 0.06315594166517258, Total Loss 43.728023529052734\n",
      "9: Encoding Loss 3.569140672683716, Transition Loss -1.0925467014312744, Classifier Loss 0.11306087672710419, Total Loss 32.72049331665039\n",
      "9: Encoding Loss 2.188563108444214, Transition Loss -1.6605416536331177, Classifier Loss 0.09297893941402435, Total Loss 22.428607940673828\n",
      "9: Encoding Loss 2.3051702976226807, Transition Loss -1.139662504196167, Classifier Loss 0.06159689649939537, Total Loss 19.99025535583496\n",
      "9: Encoding Loss 8.63447380065918, Transition Loss -2.133997917175293, Classifier Loss 0.11534343659877777, Total Loss 63.340335845947266\n",
      "9: Encoding Loss 4.635133266448975, Transition Loss -1.7365275621414185, Classifier Loss 0.05401858314871788, Total Loss 33.21196365356445\n",
      "9: Encoding Loss 2.607491970062256, Transition Loss -1.981463074684143, Classifier Loss 0.07127020508050919, Total Loss 22.77117919921875\n",
      "9: Encoding Loss 6.212714672088623, Transition Loss -1.2956652641296387, Classifier Loss 0.12499045580625534, Total Loss 49.774818420410156\n",
      "9: Encoding Loss 3.362940549850464, Transition Loss -1.0192139148712158, Classifier Loss 0.06933871656656265, Total Loss 27.111108779907227\n",
      "9: Encoding Loss 5.369696617126465, Transition Loss -0.7531452178955078, Classifier Loss 0.10020527243614197, Total Loss 42.238407135009766\n",
      "9: Encoding Loss 5.359618663787842, Transition Loss -0.5143449306488037, Classifier Loss 0.07248273491859436, Total Loss 39.40578079223633\n",
      "9: Encoding Loss 5.2623796463012695, Transition Loss -2.8683524131774902, Classifier Loss 0.1283867210149765, Total Loss 44.411800384521484\n",
      "9: Encoding Loss 5.409010410308838, Transition Loss -0.4865744709968567, Classifier Loss 0.11654038727283478, Total Loss 44.107906341552734\n",
      "9: Encoding Loss 5.274980545043945, Transition Loss -1.586022138595581, Classifier Loss 0.16162650287151337, Total Loss 47.81190490722656\n",
      "9: Encoding Loss 4.690213680267334, Transition Loss -2.6289079189300537, Classifier Loss 0.09417178481817245, Total Loss 37.55740737915039\n",
      "9: Encoding Loss 3.627593517303467, Transition Loss -1.2617621421813965, Classifier Loss 0.04372560605406761, Total Loss 26.137617111206055\n",
      "9: Encoding Loss 7.54147481918335, Transition Loss -1.139007568359375, Classifier Loss 0.1531345248222351, Total Loss 60.561851501464844\n",
      "9: Encoding Loss 6.663923740386963, Transition Loss -0.9345203638076782, Classifier Loss 0.15186704695224762, Total Loss 55.16987609863281\n",
      "9: Encoding Loss 5.250957489013672, Transition Loss -1.2078683376312256, Classifier Loss 0.1502578854560852, Total Loss 46.53105163574219\n",
      "9: Encoding Loss 3.695669651031494, Transition Loss -1.8433164358139038, Classifier Loss 0.06801062822341919, Total Loss 28.97434425354004\n",
      "9: Encoding Loss 9.900420188903809, Transition Loss -1.2200238704681396, Classifier Loss 0.17253467440605164, Total Loss 76.65550231933594\n",
      "9: Encoding Loss 5.815661430358887, Transition Loss -2.629577398300171, Classifier Loss 0.06942276656627655, Total Loss 41.8351936340332\n",
      "9: Encoding Loss 4.806329250335693, Transition Loss -2.002363920211792, Classifier Loss 0.14456893503665924, Total Loss 43.2940673828125\n",
      "9: Encoding Loss 6.415684700012207, Transition Loss -2.180880546569824, Classifier Loss 0.1717139035463333, Total Loss 55.66462707519531\n",
      "9: Encoding Loss 6.653511047363281, Transition Loss -1.4829862117767334, Classifier Loss 0.12211652845144272, Total Loss 52.13212585449219\n",
      "9: Encoding Loss 5.417514324188232, Transition Loss -0.6214975714683533, Classifier Loss 0.08997370302677155, Total Loss 41.50221252441406\n",
      "9: Encoding Loss 7.185126304626465, Transition Loss -2.705009698867798, Classifier Loss 0.08648277819156647, Total Loss 51.75795364379883\n",
      "9: Encoding Loss 7.257009983062744, Transition Loss -1.4694011211395264, Classifier Loss 0.18832898139953613, Total Loss 62.37437057495117\n",
      "9: Encoding Loss 7.492585182189941, Transition Loss -2.2915706634521484, Classifier Loss 0.062498319894075394, Total Loss 51.204429626464844\n",
      "9: Encoding Loss 6.653928279876709, Transition Loss -0.5083409547805786, Classifier Loss 0.20431135594844818, Total Loss 60.3545036315918\n",
      "9: Encoding Loss 5.962662696838379, Transition Loss -1.9082520008087158, Classifier Loss 0.0614389106631279, Total Loss 41.919105529785156\n",
      "9: Encoding Loss 5.235478401184082, Transition Loss -1.3809291124343872, Classifier Loss 0.06375030428171158, Total Loss 37.787349700927734\n",
      "9: Encoding Loss 4.038945198059082, Transition Loss -2.5711123943328857, Classifier Loss 0.06877242773771286, Total Loss 31.109886169433594\n",
      "9: Encoding Loss 4.347763538360596, Transition Loss -1.0368233919143677, Classifier Loss 0.10366170853376389, Total Loss 36.452335357666016\n",
      "9: Encoding Loss 3.872729539871216, Transition Loss -2.99039888381958, Classifier Loss 0.0738067626953125, Total Loss 30.61585807800293\n",
      "9: Encoding Loss 7.520524024963379, Transition Loss -0.9036402702331543, Classifier Loss 0.1023331880569458, Total Loss 55.356101989746094\n",
      "9: Encoding Loss 4.3734636306762695, Transition Loss -1.8761159181594849, Classifier Loss 0.106386199593544, Total Loss 36.8786506652832\n",
      "9: Encoding Loss 2.9486567974090576, Transition Loss -1.273888111114502, Classifier Loss 0.04616351053118706, Total Loss 22.307783126831055\n",
      "9: Encoding Loss 7.03350830078125, Transition Loss -2.280245780944824, Classifier Loss 0.07009874284267426, Total Loss 49.21001052856445\n",
      "9: Encoding Loss 7.780608177185059, Transition Loss -1.49479341506958, Classifier Loss 0.08399144560098648, Total Loss 55.08219528198242\n",
      "9: Encoding Loss 5.76357889175415, Transition Loss -0.552436351776123, Classifier Loss 0.12826064229011536, Total Loss 47.407318115234375\n",
      "9: Encoding Loss 6.029398441314697, Transition Loss -1.7687113285064697, Classifier Loss 0.03783618286252022, Total Loss 39.95930480957031\n",
      "9: Encoding Loss 4.809606075286865, Transition Loss -1.073147177696228, Classifier Loss 0.10957290232181549, Total Loss 39.81449508666992\n",
      "9: Encoding Loss 5.516793251037598, Transition Loss -1.6323426961898804, Classifier Loss 0.09327101707458496, Total Loss 42.42721176147461\n",
      "9: Encoding Loss 5.533419609069824, Transition Loss -1.167421817779541, Classifier Loss 0.10119698941707611, Total Loss 43.31975173950195\n",
      "9: Encoding Loss 4.071722030639648, Transition Loss -0.8925466537475586, Classifier Loss 0.08393695205450058, Total Loss 32.82366943359375\n",
      "9: Encoding Loss 6.127546787261963, Transition Loss -2.036919593811035, Classifier Loss 0.15515166521072388, Total Loss 52.279632568359375\n",
      "9: Encoding Loss 3.7558071613311768, Transition Loss -1.1259657144546509, Classifier Loss 0.07992427796125412, Total Loss 30.52682113647461\n",
      "9: Encoding Loss 5.099897861480713, Transition Loss -0.426869660615921, Classifier Loss 0.04544736444950104, Total Loss 35.143951416015625\n",
      "9: Encoding Loss 7.102668762207031, Transition Loss -1.4622247219085693, Classifier Loss 0.1881912797689438, Total Loss 61.4345588684082\n",
      "9: Encoding Loss 5.176965713500977, Transition Loss -0.9453157186508179, Classifier Loss 0.0573590025305748, Total Loss 36.79731750488281\n",
      "9: Encoding Loss 5.807519912719727, Transition Loss -3.7432942390441895, Classifier Loss 0.07934506237506866, Total Loss 42.77812576293945\n",
      "9: Encoding Loss 4.453690052032471, Transition Loss -1.7431418895721436, Classifier Loss 0.051131825894117355, Total Loss 31.834625244140625\n",
      "9: Encoding Loss 5.256166458129883, Transition Loss -0.2791377305984497, Classifier Loss 0.15441583096981049, Total Loss 46.97847366333008\n",
      "9: Encoding Loss 3.7108864784240723, Transition Loss -1.4260106086730957, Classifier Loss 0.09566080570220947, Total Loss 31.830829620361328\n",
      "9: Encoding Loss 7.952632904052734, Transition Loss -1.977459192276001, Classifier Loss 0.15957356989383698, Total Loss 63.672367095947266\n",
      "9: Encoding Loss 5.087750434875488, Transition Loss -0.8096373081207275, Classifier Loss 0.06405720114707947, Total Loss 36.93190002441406\n",
      "9: Encoding Loss 4.947551727294922, Transition Loss -2.661491870880127, Classifier Loss 0.10087746381759644, Total Loss 39.771995544433594\n",
      "9: Encoding Loss 4.723729133605957, Transition Loss -2.9118967056274414, Classifier Loss 0.13128243386745453, Total Loss 41.46945571899414\n",
      "9: Encoding Loss 4.694941520690918, Transition Loss -1.6010314226150513, Classifier Loss 0.12371977418661118, Total Loss 40.54098892211914\n",
      "9: Encoding Loss 5.632699966430664, Transition Loss -1.3603789806365967, Classifier Loss 0.1034461185336113, Total Loss 44.14026641845703\n",
      "9: Encoding Loss 5.128683567047119, Transition Loss -1.9261654615402222, Classifier Loss 0.168317973613739, Total Loss 47.603126525878906\n",
      "9: Encoding Loss 6.084487438201904, Transition Loss -1.3659064769744873, Classifier Loss 0.03755614906549454, Total Loss 40.26199722290039\n",
      "9: Encoding Loss 4.77181339263916, Transition Loss 0.4113115668296814, Classifier Loss 0.05684706196188927, Total Loss 34.48011016845703\n",
      "9: Encoding Loss 5.768768310546875, Transition Loss -1.9676306247711182, Classifier Loss 0.05746423825621605, Total Loss 40.358245849609375\n",
      "9: Encoding Loss 4.189535617828369, Transition Loss -1.541614294052124, Classifier Loss 0.07429333031177521, Total Loss 32.5659294128418\n",
      "9: Encoding Loss 6.676999092102051, Transition Loss -0.4206017255783081, Classifier Loss 0.11401064693927765, Total Loss 51.462894439697266\n",
      "9: Encoding Loss 4.7562713623046875, Transition Loss -2.39030385017395, Classifier Loss 0.042963095009326935, Total Loss 32.83298110961914\n",
      "9: Encoding Loss 7.2239227294921875, Transition Loss -1.6371046304702759, Classifier Loss 0.220793217420578, Total Loss 65.42220306396484\n",
      "9: Encoding Loss 5.499011993408203, Transition Loss -1.1536551713943481, Classifier Loss 0.12337779253721237, Total Loss 45.331390380859375\n",
      "9: Encoding Loss 5.248843669891357, Transition Loss -1.8870117664337158, Classifier Loss 0.0625184029340744, Total Loss 37.74414825439453\n",
      "9: Encoding Loss 5.148728847503662, Transition Loss -0.049175456166267395, Classifier Loss 0.06740120053291321, Total Loss 37.63247299194336\n",
      "9: Encoding Loss 5.312500953674316, Transition Loss -1.169233798980713, Classifier Loss 0.05879240855574608, Total Loss 37.75377655029297\n",
      "9: Encoding Loss 5.480842590332031, Transition Loss -0.3074846863746643, Classifier Loss 0.12054409831762314, Total Loss 44.9393424987793\n",
      "9: Encoding Loss 7.039105415344238, Transition Loss -0.4061460494995117, Classifier Loss 0.052449800074100494, Total Loss 47.47945022583008\n",
      "9: Encoding Loss 4.522403240203857, Transition Loss -1.7277470827102661, Classifier Loss 0.17865905165672302, Total Loss 44.999637603759766\n",
      "9: Encoding Loss 5.286340713500977, Transition Loss -0.37259745597839355, Classifier Loss 0.07150179892778397, Total Loss 38.86807632446289\n",
      "9: Encoding Loss 4.33206844329834, Transition Loss -0.44418662786483765, Classifier Loss 0.10022673010826111, Total Loss 36.0149040222168\n",
      "9: Encoding Loss 4.8007917404174805, Transition Loss -1.7655975818634033, Classifier Loss 0.03976745903491974, Total Loss 32.780792236328125\n",
      "9: Encoding Loss 4.7044572830200195, Transition Loss -2.066474437713623, Classifier Loss 0.1744728684425354, Total Loss 45.67320251464844\n",
      "9: Encoding Loss 4.856858730316162, Transition Loss -2.1360414028167725, Classifier Loss 0.14930382370948792, Total Loss 44.070682525634766\n",
      "9: Encoding Loss 3.574676036834717, Transition Loss -2.901759147644043, Classifier Loss 0.07609180361032486, Total Loss 29.056076049804688\n",
      "9: Encoding Loss 4.044803142547607, Transition Loss -0.6160825490951538, Classifier Loss 0.1279788464307785, Total Loss 37.06645584106445\n",
      "9: Encoding Loss 5.632688522338867, Transition Loss -0.5726572275161743, Classifier Loss 0.12236384302377701, Total Loss 46.03228759765625\n",
      "9: Encoding Loss 3.4557900428771973, Transition Loss -2.2842283248901367, Classifier Loss 0.0614846833050251, Total Loss 26.882295608520508\n",
      "9: Encoding Loss 5.072915554046631, Transition Loss 0.18631130456924438, Classifier Loss 0.03819841146469116, Total Loss 34.33185958862305\n",
      "9: Encoding Loss 5.720793724060059, Transition Loss -2.2540435791015625, Classifier Loss 0.08682327717542648, Total Loss 43.00619125366211\n",
      "9: Encoding Loss 4.180727005004883, Transition Loss -0.9190247654914856, Classifier Loss 0.060351528227329254, Total Loss 31.11914825439453\n",
      "9: Encoding Loss 8.093803405761719, Transition Loss 0.5633998513221741, Classifier Loss 0.24956797063350677, Total Loss 73.74497985839844\n",
      "9: Encoding Loss 5.826465606689453, Transition Loss -1.6391353607177734, Classifier Loss 0.05931355431675911, Total Loss 40.88949203491211\n",
      "9: Encoding Loss 7.057214260101318, Transition Loss 0.4154486656188965, Classifier Loss 0.08150428533554077, Total Loss 50.65989685058594\n",
      "9: Encoding Loss 5.127298831939697, Transition Loss -1.7056070566177368, Classifier Loss 0.16011522710323334, Total Loss 46.77463150024414\n",
      "9: Encoding Loss 3.6660079956054688, Transition Loss -1.1596615314483643, Classifier Loss 0.09820415079593658, Total Loss 31.81599998474121\n",
      "9: Encoding Loss 6.440550327301025, Transition Loss -1.677731990814209, Classifier Loss 0.12094135582447052, Total Loss 50.73676681518555\n",
      "9: Encoding Loss 8.920445442199707, Transition Loss -2.8729259967803955, Classifier Loss 0.18741446733474731, Total Loss 72.26296997070312\n",
      "9: Encoding Loss 7.101718902587891, Transition Loss -1.495795726776123, Classifier Loss 0.2790956199169159, Total Loss 70.51927947998047\n",
      "9: Encoding Loss 4.12246561050415, Transition Loss -1.3718481063842773, Classifier Loss 0.1117832139134407, Total Loss 35.912567138671875\n",
      "9: Encoding Loss 7.964116096496582, Transition Loss 0.32844579219818115, Classifier Loss 0.09407556056976318, Total Loss 57.323631286621094\n",
      "9: Encoding Loss 9.110082626342773, Transition Loss -1.5211272239685059, Classifier Loss 0.3165065348148346, Total Loss 86.310546875\n",
      "9: Encoding Loss 6.7075605392456055, Transition Loss -0.8457407355308533, Classifier Loss 0.12088418751955032, Total Loss 52.33344650268555\n",
      "9: Encoding Loss 7.302641868591309, Transition Loss -0.21310076117515564, Classifier Loss 0.13069739937782288, Total Loss 56.8855094909668\n",
      "9: Encoding Loss 7.288673400878906, Transition Loss -0.5265349745750427, Classifier Loss 0.10322621464729309, Total Loss 54.05445098876953\n",
      "9: Encoding Loss 5.307670593261719, Transition Loss -2.010878086090088, Classifier Loss 0.09533195197582245, Total Loss 41.378414154052734\n",
      "9: Encoding Loss 4.972187519073486, Transition Loss -1.9158504009246826, Classifier Loss 0.12464025616645813, Total Loss 42.296382904052734\n",
      "9: Encoding Loss 6.011748790740967, Transition Loss -3.410306930541992, Classifier Loss 0.13461805880069733, Total Loss 49.53093719482422\n",
      "9: Encoding Loss 4.702300071716309, Transition Loss -2.266359567642212, Classifier Loss 0.08121763914823532, Total Loss 36.334659576416016\n",
      "9: Encoding Loss 4.17586612701416, Transition Loss -1.2779831886291504, Classifier Loss 0.05504794791340828, Total Loss 30.559480667114258\n",
      "9: Encoding Loss 5.207766532897949, Transition Loss -1.8129616975784302, Classifier Loss 0.16767163574695587, Total Loss 48.013038635253906\n",
      "9: Encoding Loss 3.1662356853485107, Transition Loss -1.9380701780319214, Classifier Loss 0.0884704440832138, Total Loss 27.843685150146484\n",
      "9: Encoding Loss 4.2304205894470215, Transition Loss -1.5087952613830566, Classifier Loss 0.11625838279724121, Total Loss 37.00775909423828\n",
      "9: Encoding Loss 6.441706657409668, Transition Loss -1.6535615921020508, Classifier Loss 0.20459330081939697, Total Loss 59.10891342163086\n",
      "9: Encoding Loss 5.332918167114258, Transition Loss -0.3734375536441803, Classifier Loss 0.15499068796634674, Total Loss 47.49643325805664\n",
      "9: Encoding Loss 3.9437942504882812, Transition Loss -1.900081753730774, Classifier Loss 0.12475563585758209, Total Loss 36.137569427490234\n",
      "9: Encoding Loss 5.552456855773926, Transition Loss -2.268345594406128, Classifier Loss 0.07972314953804016, Total Loss 41.28614807128906\n",
      "9: Encoding Loss 4.034030437469482, Transition Loss -0.6610012650489807, Classifier Loss 0.058813415467739105, Total Loss 30.08526039123535\n",
      "9: Encoding Loss 5.473538398742676, Transition Loss -1.398780345916748, Classifier Loss 0.040722738951444626, Total Loss 36.91294479370117\n",
      "9: Encoding Loss 4.4210615158081055, Transition Loss -1.4233936071395874, Classifier Loss 0.04678710177540779, Total Loss 31.204511642456055\n",
      "9: Encoding Loss 3.9494025707244873, Transition Loss -0.5793827772140503, Classifier Loss 0.12669984996318817, Total Loss 36.36616897583008\n",
      "9: Encoding Loss 5.812004089355469, Transition Loss -0.7276178002357483, Classifier Loss 0.13867977261543274, Total Loss 48.73971176147461\n",
      "9: Encoding Loss 6.668582916259766, Transition Loss -0.16148534417152405, Classifier Loss 0.138716921210289, Total Loss 53.88312530517578\n",
      "9: Encoding Loss 6.619351863861084, Transition Loss -0.7210125923156738, Classifier Loss 0.10876581072807312, Total Loss 50.5924072265625\n",
      "9: Encoding Loss 2.987388849258423, Transition Loss -2.2940869331359863, Classifier Loss 0.09300830215215683, Total Loss 27.224245071411133\n",
      "9: Encoding Loss 7.21938943862915, Transition Loss -1.5173401832580566, Classifier Loss 0.15430547297000885, Total Loss 58.746280670166016\n",
      "9: Encoding Loss 4.564706325531006, Transition Loss -2.532600164413452, Classifier Loss 0.10691195726394653, Total Loss 38.07841873168945\n",
      "9: Encoding Loss 5.866266250610352, Transition Loss -1.0020209550857544, Classifier Loss 0.08944964408874512, Total Loss 44.14216232299805\n",
      "9: Encoding Loss 7.137644290924072, Transition Loss -2.057875394821167, Classifier Loss 0.18618841469287872, Total Loss 61.443885803222656\n",
      "9: Encoding Loss 5.542757987976074, Transition Loss -2.03790545463562, Classifier Loss 0.14649216830730438, Total Loss 47.90494918823242\n",
      "9: Encoding Loss 7.408194541931152, Transition Loss -0.1857362687587738, Classifier Loss 0.14248868823051453, Total Loss 58.69796371459961\n",
      "9: Encoding Loss 4.746181488037109, Transition Loss 0.12242500483989716, Classifier Loss 0.09602229297161102, Total Loss 38.12828826904297\n",
      "9: Encoding Loss 5.040373802185059, Transition Loss -2.016296863555908, Classifier Loss 0.08706896752119064, Total Loss 38.94833755493164\n",
      "9: Encoding Loss 6.981942176818848, Transition Loss -0.7259052991867065, Classifier Loss 0.13714346289634705, Total Loss 55.605712890625\n",
      "9: Encoding Loss 6.311247825622559, Transition Loss -1.3558588027954102, Classifier Loss 0.11029061675071716, Total Loss 48.8960075378418\n",
      "9: Encoding Loss 5.841400146484375, Transition Loss -0.7999646663665771, Classifier Loss 0.12048355489969254, Total Loss 47.096435546875\n",
      "9: Encoding Loss 4.189005374908447, Transition Loss 0.2012196183204651, Classifier Loss 0.1192333772778511, Total Loss 37.13785934448242\n",
      "9: Encoding Loss 3.1684584617614746, Transition Loss -0.8460788130760193, Classifier Loss 0.06185096129775047, Total Loss 25.195510864257812\n",
      "9: Encoding Loss 5.680970191955566, Transition Loss -1.763680100440979, Classifier Loss 0.08038640767335892, Total Loss 42.12376022338867\n",
      "9: Encoding Loss 4.737803936004639, Transition Loss -1.278595209121704, Classifier Loss 0.09582172334194183, Total Loss 38.00848388671875\n",
      "9: Encoding Loss 4.686732769012451, Transition Loss -0.7212411165237427, Classifier Loss 0.19420862197875977, Total Loss 47.54096984863281\n",
      "9: Encoding Loss 4.265059471130371, Transition Loss -2.250708818435669, Classifier Loss 0.04726502671837807, Total Loss 30.315961837768555\n",
      "9: Encoding Loss 4.484555244445801, Transition Loss -2.6715497970581055, Classifier Loss 0.12571169435977936, Total Loss 39.47743225097656\n",
      "9: Encoding Loss 1.9100892543792725, Transition Loss -0.6801280975341797, Classifier Loss 0.06233520060777664, Total Loss 17.693782806396484\n",
      "9: Encoding Loss 5.057925701141357, Transition Loss -0.7715557813644409, Classifier Loss 0.059461548924446106, Total Loss 36.293399810791016\n",
      "9: Encoding Loss 4.601844310760498, Transition Loss -1.5057975053787231, Classifier Loss 0.1369929015636444, Total Loss 41.30975341796875\n",
      "9: Encoding Loss 3.8628430366516113, Transition Loss -0.6781749725341797, Classifier Loss 0.062072623521089554, Total Loss 29.384050369262695\n",
      "9: Encoding Loss 4.714583396911621, Transition Loss -0.1556546837091446, Classifier Loss 0.03202779218554497, Total Loss 31.490219116210938\n",
      "9: Encoding Loss 7.678353786468506, Transition Loss -2.167240619659424, Classifier Loss 0.195328950881958, Total Loss 65.6021499633789\n",
      "9: Encoding Loss 5.091391563415527, Transition Loss -0.6057721376419067, Classifier Loss 0.1855236142873764, Total Loss 49.100467681884766\n",
      "9: Encoding Loss 5.080897331237793, Transition Loss -1.1432362794876099, Classifier Loss 0.19309689104557037, Total Loss 49.79461669921875\n",
      "9: Encoding Loss 4.624182224273682, Transition Loss -1.8505855798721313, Classifier Loss 0.08400756865739822, Total Loss 36.145111083984375\n",
      "9: Encoding Loss 6.456662178039551, Transition Loss -1.7389168739318848, Classifier Loss 0.14987266063690186, Total Loss 53.72654724121094\n",
      "9: Encoding Loss 5.625627517700195, Transition Loss -1.0003890991210938, Classifier Loss 0.09512197971343994, Total Loss 43.265560150146484\n",
      "9: Encoding Loss 5.744982719421387, Transition Loss -0.48028796911239624, Classifier Loss 0.06804594397544861, Total Loss 41.2743034362793\n",
      "9: Encoding Loss 4.234791278839111, Transition Loss -1.9598854780197144, Classifier Loss 0.10862424969673157, Total Loss 36.2703857421875\n",
      "9: Encoding Loss 6.459229469299316, Transition Loss -0.09177154302597046, Classifier Loss 0.08264058083295822, Total Loss 47.0193977355957\n",
      "9: Encoding Loss 4.633091926574707, Transition Loss -1.6515698432922363, Classifier Loss 0.08826754987239838, Total Loss 36.6246452331543\n",
      "9: Encoding Loss 5.83583927154541, Transition Loss -1.3888287544250488, Classifier Loss 0.11523651331663132, Total Loss 46.53813171386719\n",
      "9: Encoding Loss 6.387360572814941, Transition Loss -0.6481102705001831, Classifier Loss 0.08217315375804901, Total Loss 46.541221618652344\n",
      "9: Encoding Loss 4.702463626861572, Transition Loss -1.8075151443481445, Classifier Loss 0.05019436776638031, Total Loss 33.23349380493164\n",
      "9: Encoding Loss 3.9436745643615723, Transition Loss -1.283879280090332, Classifier Loss 0.0901692658662796, Total Loss 32.67845916748047\n",
      "9: Encoding Loss 3.6693592071533203, Transition Loss -1.1737637519836426, Classifier Loss 0.08346235752105713, Total Loss 30.361921310424805\n",
      "9: Encoding Loss 7.613595485687256, Transition Loss -2.4761226177215576, Classifier Loss 0.08424469828605652, Total Loss 54.10505294799805\n",
      "9: Encoding Loss 3.9562735557556152, Transition Loss -3.43117356300354, Classifier Loss 0.06195782870054245, Total Loss 29.932052612304688\n",
      "9: Encoding Loss 3.0500693321228027, Transition Loss -1.164329171180725, Classifier Loss 0.09973426163196564, Total Loss 28.27337646484375\n",
      "9: Encoding Loss 9.822607040405273, Transition Loss 0.38265562057495117, Classifier Loss 0.1736217886209488, Total Loss 76.45088195800781\n",
      "9: Encoding Loss 6.587687015533447, Transition Loss -0.13982871174812317, Classifier Loss 0.13332858681678772, Total Loss 52.858924865722656\n",
      "9: Encoding Loss 4.671828269958496, Transition Loss -2.6629536151885986, Classifier Loss 0.07036828249692917, Total Loss 35.066734313964844\n",
      "9: Encoding Loss 4.054508686065674, Transition Loss -0.6745585799217224, Classifier Loss 0.08333319425582886, Total Loss 32.66010284423828\n",
      "9: Encoding Loss 7.078289985656738, Transition Loss -2.5528831481933594, Classifier Loss 0.19936001300811768, Total Loss 62.404720306396484\n",
      "9: Encoding Loss 4.7113213539123535, Transition Loss -1.1293710470199585, Classifier Loss 0.06758706271648407, Total Loss 35.02618408203125\n",
      "9: Encoding Loss 4.409904956817627, Transition Loss -1.143852949142456, Classifier Loss 0.07549378275871277, Total Loss 34.00835037231445\n",
      "9: Encoding Loss 4.064452171325684, Transition Loss -1.5772254467010498, Classifier Loss 0.14638064801692963, Total Loss 39.02415084838867\n",
      "9: Encoding Loss 6.219379425048828, Transition Loss -0.5505192875862122, Classifier Loss 0.04264413192868233, Total Loss 41.580467224121094\n",
      "9: Encoding Loss 4.572558879852295, Transition Loss -1.4110690355300903, Classifier Loss 0.07792317867279053, Total Loss 35.227108001708984\n",
      "9: Encoding Loss 6.7228498458862305, Transition Loss -2.0077013969421387, Classifier Loss 0.10204848647117615, Total Loss 50.54114532470703\n",
      "9: Encoding Loss 5.5604424476623535, Transition Loss -0.578374981880188, Classifier Loss 0.07243513315916061, Total Loss 40.60593795776367\n",
      "9: Encoding Loss 5.552774906158447, Transition Loss -0.2401973307132721, Classifier Loss 0.1411375254392624, Total Loss 47.4303092956543\n",
      "9: Encoding Loss 7.326467990875244, Transition Loss -1.0331780910491943, Classifier Loss 0.047537170350551605, Total Loss 48.71211242675781\n",
      "9: Encoding Loss 5.797632694244385, Transition Loss -1.060985803604126, Classifier Loss 0.1926438808441162, Total Loss 54.04976272583008\n",
      "9: Encoding Loss 5.118128299713135, Transition Loss -0.5234588980674744, Classifier Loss 0.11766410619020462, Total Loss 42.474971771240234\n",
      "9: Encoding Loss 3.290921449661255, Transition Loss -0.9575334787368774, Classifier Loss 0.06608474254608154, Total Loss 26.353620529174805\n",
      "9: Encoding Loss 4.987074851989746, Transition Loss -1.5281710624694824, Classifier Loss 0.07894176244735718, Total Loss 37.816017150878906\n",
      "9: Encoding Loss 4.87740421295166, Transition Loss -0.34991908073425293, Classifier Loss 0.08932076394557953, Total Loss 38.19636154174805\n",
      "9: Encoding Loss 4.407504081726074, Transition Loss 0.4192933440208435, Classifier Loss 0.09876087307929993, Total Loss 36.48883056640625\n",
      "9: Encoding Loss 4.267068386077881, Transition Loss -1.7385716438293457, Classifier Loss 0.07768175005912781, Total Loss 33.36989212036133\n",
      "9: Encoding Loss 3.2461516857147217, Transition Loss -2.756592273712158, Classifier Loss 0.08204145729541779, Total Loss 27.679954528808594\n",
      "9: Encoding Loss 5.95546817779541, Transition Loss -2.0276267528533936, Classifier Loss 0.1129378229379654, Total Loss 47.025779724121094\n",
      "9: Encoding Loss 5.034333229064941, Transition Loss -2.2425267696380615, Classifier Loss 0.15312480926513672, Total Loss 45.517581939697266\n",
      "9: Encoding Loss 5.023817539215088, Transition Loss 0.017300665378570557, Classifier Loss 0.0990750640630722, Total Loss 40.05733108520508\n",
      "9: Encoding Loss 5.202296733856201, Transition Loss -0.9991635084152222, Classifier Loss 0.12845176458358765, Total Loss 44.05855941772461\n",
      "9: Encoding Loss 6.211936950683594, Transition Loss -0.5482054948806763, Classifier Loss 0.15912757813930511, Total Loss 53.18416213989258\n",
      "9: Encoding Loss 5.453029155731201, Transition Loss -0.6126165390014648, Classifier Loss 0.12369918823242188, Total Loss 45.087852478027344\n",
      "9: Encoding Loss 4.981415748596191, Transition Loss -1.1473954916000366, Classifier Loss 0.09906952828168869, Total Loss 39.79499053955078\n",
      "9: Encoding Loss 5.228309631347656, Transition Loss -3.0097413063049316, Classifier Loss 0.08053983747959137, Total Loss 39.422637939453125\n",
      "9: Encoding Loss 4.473906517028809, Transition Loss -0.8432302474975586, Classifier Loss 0.056454114615917206, Total Loss 32.48851776123047\n",
      "9: Encoding Loss 5.078160285949707, Transition Loss -1.4308016300201416, Classifier Loss 0.10129639506340027, Total Loss 40.59803009033203\n",
      "9: Encoding Loss 5.674983501434326, Transition Loss -2.835090398788452, Classifier Loss 0.05424805358052254, Total Loss 39.473575592041016\n",
      "9: Encoding Loss 6.816897392272949, Transition Loss -0.8073158264160156, Classifier Loss 0.054882150143384933, Total Loss 46.389278411865234\n",
      "9: Encoding Loss 3.264984130859375, Transition Loss -0.7611164450645447, Classifier Loss 0.07047917693853378, Total Loss 26.63751792907715\n",
      "9: Encoding Loss 5.540709495544434, Transition Loss -1.4660648107528687, Classifier Loss 0.049506776034832, Total Loss 38.1943473815918\n",
      "9: Encoding Loss 5.981479644775391, Transition Loss -1.2248272895812988, Classifier Loss 0.063882015645504, Total Loss 42.27659225463867\n",
      "9: Encoding Loss 3.755293607711792, Transition Loss -0.5838857889175415, Classifier Loss 0.03547641262412071, Total Loss 26.079172134399414\n",
      "9: Encoding Loss 5.19879674911499, Transition Loss -2.825147867202759, Classifier Loss 0.13345742225646973, Total Loss 44.53739547729492\n",
      "9: Encoding Loss 6.889233589172363, Transition Loss -0.3164500594139099, Classifier Loss 0.09876938909292221, Total Loss 51.212215423583984\n",
      "9: Encoding Loss 6.653597831726074, Transition Loss -1.0013511180877686, Classifier Loss 0.13597528636455536, Total Loss 53.51871871948242\n",
      "9: Encoding Loss 5.680440902709961, Transition Loss -1.4581472873687744, Classifier Loss 0.0640491172671318, Total Loss 40.48697280883789\n",
      "9: Encoding Loss 5.262557506561279, Transition Loss -1.5595111846923828, Classifier Loss 0.08810365945100784, Total Loss 40.38508605957031\n",
      "9: Encoding Loss 5.395052433013916, Transition Loss -0.765080451965332, Classifier Loss 0.15865486860275269, Total Loss 48.235496520996094\n",
      "9: Encoding Loss 6.12060022354126, Transition Loss -1.707297682762146, Classifier Loss 0.06740785390138626, Total Loss 43.46370315551758\n",
      "9: Encoding Loss 3.399714708328247, Transition Loss -1.263474941253662, Classifier Loss 0.1321675032377243, Total Loss 33.61453628540039\n",
      "9: Encoding Loss 8.06350040435791, Transition Loss 0.38521429896354675, Classifier Loss 0.13801659643650055, Total Loss 62.33675003051758\n",
      "9: Encoding Loss 4.79830265045166, Transition Loss -1.128507137298584, Classifier Loss 0.04692107066512108, Total Loss 33.48147201538086\n",
      "9: Encoding Loss 3.0730655193328857, Transition Loss -2.371039390563965, Classifier Loss 0.060911186039447784, Total Loss 24.528566360473633\n",
      "9: Encoding Loss 5.98453426361084, Transition Loss -0.22155047953128815, Classifier Loss 0.05395340174436569, Total Loss 41.302459716796875\n",
      "9: Encoding Loss 2.5973525047302246, Transition Loss -0.9585552215576172, Classifier Loss 0.040822550654411316, Total Loss 19.665987014770508\n",
      "9: Encoding Loss 6.876073360443115, Transition Loss -1.7957128286361694, Classifier Loss 0.12763667106628418, Total Loss 54.01939392089844\n",
      "9: Encoding Loss 6.314090251922607, Transition Loss -1.0498563051223755, Classifier Loss 0.10244548320770264, Total Loss 48.1286735534668\n",
      "9: Encoding Loss 5.475854396820068, Transition Loss -2.1851601600646973, Classifier Loss 0.06406746804714203, Total Loss 39.26100158691406\n",
      "9: Encoding Loss 3.633319139480591, Transition Loss -1.0834494829177856, Classifier Loss 0.11680544912815094, Total Loss 33.48002624511719\n",
      "9: Encoding Loss 4.4838361740112305, Transition Loss -1.5334563255310059, Classifier Loss 0.0915648490190506, Total Loss 36.05888748168945\n",
      "9: Encoding Loss 5.418054580688477, Transition Loss -1.618972659111023, Classifier Loss 0.08698935806751251, Total Loss 41.20661544799805\n",
      "9: Encoding Loss 3.666220188140869, Transition Loss -0.8098412752151489, Classifier Loss 0.08458860218524933, Total Loss 30.45585823059082\n",
      "9: Encoding Loss 5.954180717468262, Transition Loss -2.1452643871307373, Classifier Loss 0.17441056668758392, Total Loss 53.165287017822266\n",
      "9: Encoding Loss 4.025737762451172, Transition Loss -1.9992367029190063, Classifier Loss 0.057672739028930664, Total Loss 29.9209041595459\n",
      "9: Encoding Loss 3.4272994995117188, Transition Loss -0.7271857261657715, Classifier Loss 0.06461916118860245, Total Loss 27.025421142578125\n",
      "9: Encoding Loss 5.431102275848389, Transition Loss 0.055722981691360474, Classifier Loss 0.14374232292175293, Total Loss 46.98313903808594\n",
      "9: Encoding Loss 4.708261966705322, Transition Loss -1.7053321599960327, Classifier Loss 0.08195111155509949, Total Loss 36.444000244140625\n",
      "9: Encoding Loss 7.7216596603393555, Transition Loss -0.620844304561615, Classifier Loss 0.10417674481868744, Total Loss 56.74738693237305\n",
      "9: Encoding Loss 3.9798972606658936, Transition Loss -2.582911729812622, Classifier Loss 0.18843801319599152, Total Loss 42.72215270996094\n",
      "9: Encoding Loss 5.99735164642334, Transition Loss -1.0804905891418457, Classifier Loss 0.08703218400478363, Total Loss 44.6869010925293\n",
      "9: Encoding Loss 4.123025894165039, Transition Loss -1.2414941787719727, Classifier Loss 0.04643959552049637, Total Loss 29.381620407104492\n",
      "9: Encoding Loss 8.45206069946289, Transition Loss -0.22114217281341553, Classifier Loss 0.1469494104385376, Total Loss 65.40721893310547\n",
      "9: Encoding Loss 5.369806289672852, Transition Loss -1.2809041738510132, Classifier Loss 0.1260681003332138, Total Loss 44.82513427734375\n",
      "9: Encoding Loss 4.364360332489014, Transition Loss -1.1037840843200684, Classifier Loss 0.07870437949895859, Total Loss 34.05615997314453\n",
      "9: Encoding Loss 4.84565544128418, Transition Loss -2.817044496536255, Classifier Loss 0.09364166855812073, Total Loss 38.43697738647461\n",
      "9: Encoding Loss 3.3998615741729736, Transition Loss 0.6903707981109619, Classifier Loss 0.11363667994737625, Total Loss 32.03898620605469\n",
      "9: Encoding Loss 3.473001480102539, Transition Loss -0.20745518803596497, Classifier Loss 0.049701038748025894, Total Loss 25.808029174804688\n",
      "9: Encoding Loss 4.730493545532227, Transition Loss -2.5202746391296387, Classifier Loss 0.14019063115119934, Total Loss 42.40102005004883\n",
      "9: Encoding Loss 5.6880083084106445, Transition Loss -1.7739735841751099, Classifier Loss 0.133052796125412, Total Loss 47.432621002197266\n",
      "9: Encoding Loss 4.192234992980957, Transition Loss -0.6877140402793884, Classifier Loss 0.08661548793315887, Total Loss 33.81468200683594\n",
      "9: Encoding Loss 4.456742286682129, Transition Loss -0.5889526605606079, Classifier Loss 0.07199355959892273, Total Loss 33.9395751953125\n",
      "9: Encoding Loss 4.490424156188965, Transition Loss -1.211940050125122, Classifier Loss 0.04108782857656479, Total Loss 31.050846099853516\n",
      "9: Encoding Loss 6.913049221038818, Transition Loss -0.17272448539733887, Classifier Loss 0.2477664202451706, Total Loss 66.25487518310547\n",
      "9: Encoding Loss 4.513623237609863, Transition Loss -1.2551974058151245, Classifier Loss 0.0758160948753357, Total Loss 34.662845611572266\n",
      "9: Encoding Loss 6.790484428405762, Transition Loss -1.5610121488571167, Classifier Loss 0.08651651442050934, Total Loss 49.39393615722656\n",
      "9: Encoding Loss 4.106400489807129, Transition Loss -0.04015088081359863, Classifier Loss 0.05739772319793701, Total Loss 30.37816047668457\n",
      "9: Encoding Loss 5.701120853424072, Transition Loss -1.6323343515396118, Classifier Loss 0.0335649698972702, Total Loss 37.56257247924805\n",
      "9: Encoding Loss 5.157767295837402, Transition Loss -0.90155029296875, Classifier Loss 0.1033710464835167, Total Loss 41.283348083496094\n",
      "9: Encoding Loss 5.004074573516846, Transition Loss -1.346861481666565, Classifier Loss 0.2118745893239975, Total Loss 51.211368560791016\n",
      "9: Encoding Loss 8.337761878967285, Transition Loss -0.32962536811828613, Classifier Loss 0.21901372075080872, Total Loss 71.92781829833984\n",
      "9: Encoding Loss 6.281200885772705, Transition Loss -1.368479609489441, Classifier Loss 0.08259405195713043, Total Loss 45.94606399536133\n",
      "9: Encoding Loss 6.532958030700684, Transition Loss -1.3431419134140015, Classifier Loss 0.1633547693490982, Total Loss 55.53268814086914\n",
      "9: Encoding Loss 4.813033103942871, Transition Loss -1.860740303993225, Classifier Loss 0.03995492309331894, Total Loss 32.872947692871094\n",
      "9: Encoding Loss 5.496480941772461, Transition Loss -1.1802423000335693, Classifier Loss 0.0737290158867836, Total Loss 40.351314544677734\n",
      "9: Encoding Loss 6.658710479736328, Transition Loss -1.097923994064331, Classifier Loss 0.08940840512514114, Total Loss 48.89266586303711\n",
      "9: Encoding Loss 4.975798606872559, Transition Loss -1.3607299327850342, Classifier Loss 0.11320901662111282, Total Loss 41.17515182495117\n",
      "9: Encoding Loss 3.5402634143829346, Transition Loss 0.4337983727455139, Classifier Loss 0.041482433676719666, Total Loss 25.563343048095703\n",
      "9: Encoding Loss 3.9598135948181152, Transition Loss -0.6453537940979004, Classifier Loss 0.08567193895578384, Total Loss 32.3258171081543\n",
      "9: Encoding Loss 4.525054454803467, Transition Loss 0.2694447338581085, Classifier Loss 0.07299281656742096, Total Loss 34.5573844909668\n",
      "9: Encoding Loss 5.020741939544678, Transition Loss -1.4250246286392212, Classifier Loss 0.0586039200425148, Total Loss 35.984275817871094\n",
      "9: Encoding Loss 6.991380214691162, Transition Loss -0.30433130264282227, Classifier Loss 0.18950724601745605, Total Loss 60.898887634277344\n",
      "9: Encoding Loss 5.18364953994751, Transition Loss -1.8100355863571167, Classifier Loss 0.0796445906162262, Total Loss 39.06563186645508\n",
      "9: Encoding Loss 8.296026229858398, Transition Loss -2.831334114074707, Classifier Loss 0.12392806261777878, Total Loss 62.1678352355957\n",
      "9: Encoding Loss 5.821959018707275, Transition Loss -1.6603140830993652, Classifier Loss 0.08034656941890717, Total Loss 42.96574783325195\n",
      "9: Encoding Loss 3.6565520763397217, Transition Loss -1.1664644479751587, Classifier Loss 0.08826988190412521, Total Loss 30.76583480834961\n",
      "9: Encoding Loss 3.8695406913757324, Transition Loss -0.4076751470565796, Classifier Loss 0.02208428457379341, Total Loss 25.425512313842773\n",
      "9: Encoding Loss 6.6279497146606445, Transition Loss -3.0885813236236572, Classifier Loss 0.242319256067276, Total Loss 63.998390197753906\n",
      "9: Encoding Loss 5.221768856048584, Transition Loss -1.4237643480300903, Classifier Loss 0.1338379979133606, Total Loss 44.713844299316406\n",
      "9: Encoding Loss 3.701786518096924, Transition Loss 0.8511217832565308, Classifier Loss 0.051827218383550644, Total Loss 27.733890533447266\n",
      "9: Encoding Loss 5.301924705505371, Transition Loss -0.995954692363739, Classifier Loss 0.2273740917444229, Total Loss 54.548561096191406\n",
      "9: Encoding Loss 4.053736686706543, Transition Loss -1.6148786544799805, Classifier Loss 0.12349894642829895, Total Loss 36.67167282104492\n",
      "9: Encoding Loss 6.232133388519287, Transition Loss -0.8046959042549133, Classifier Loss 0.18030375242233276, Total Loss 55.42285919189453\n",
      "9: Encoding Loss 3.184312105178833, Transition Loss -0.4029650092124939, Classifier Loss 0.05710696056485176, Total Loss 24.816408157348633\n",
      "9: Encoding Loss 3.0990517139434814, Transition Loss -0.5580394268035889, Classifier Loss 0.16397300362586975, Total Loss 34.99138641357422\n",
      "9: Encoding Loss 5.539414882659912, Transition Loss -2.277233123779297, Classifier Loss 0.11759817600250244, Total Loss 44.99539566040039\n",
      "9: Encoding Loss 4.783234596252441, Transition Loss -1.6208913326263428, Classifier Loss 0.15941554307937622, Total Loss 44.64031219482422\n",
      "9: Encoding Loss 5.4045538902282715, Transition Loss -2.2987661361694336, Classifier Loss 0.12352973222732544, Total Loss 44.779380798339844\n",
      "9: Encoding Loss 5.707584857940674, Transition Loss -1.715468406677246, Classifier Loss 0.04853590577840805, Total Loss 39.09841537475586\n",
      "9: Encoding Loss 5.547395706176758, Transition Loss -2.772942543029785, Classifier Loss 0.11230909824371338, Total Loss 44.5141716003418\n",
      "9: Encoding Loss 7.255063056945801, Transition Loss -1.0949159860610962, Classifier Loss 0.14891061186790466, Total Loss 58.42100143432617\n",
      "9: Encoding Loss 6.160333633422852, Transition Loss -1.7111942768096924, Classifier Loss 0.14779724180698395, Total Loss 51.74104309082031\n",
      "9: Encoding Loss 6.077129364013672, Transition Loss -0.7994439601898193, Classifier Loss 0.080691397190094, Total Loss 44.531593322753906\n",
      "9: Encoding Loss 3.9392383098602295, Transition Loss -2.288916826248169, Classifier Loss 0.07811848819255829, Total Loss 31.446365356445312\n",
      "9: Encoding Loss 6.552359104156494, Transition Loss -1.6326203346252441, Classifier Loss 0.10001329332590103, Total Loss 49.3148307800293\n",
      "9: Encoding Loss 4.4846391677856445, Transition Loss -2.283010959625244, Classifier Loss 0.07099442183971405, Total Loss 34.00636672973633\n",
      "9: Encoding Loss 5.802371501922607, Transition Loss -1.8905690908432007, Classifier Loss 0.04709641635417938, Total Loss 39.52311706542969\n",
      "9: Encoding Loss 4.620582580566406, Transition Loss -1.005021095275879, Classifier Loss 0.06912344694137573, Total Loss 34.635440826416016\n",
      "9: Encoding Loss 5.268446922302246, Transition Loss -1.6385438442230225, Classifier Loss 0.11855784058570862, Total Loss 43.46581268310547\n",
      "9: Encoding Loss 3.7213778495788574, Transition Loss 0.21522969007492065, Classifier Loss 0.05619601905345917, Total Loss 28.03396224975586\n",
      "9: Encoding Loss 8.290800094604492, Transition Loss -1.3254096508026123, Classifier Loss 0.18091076612472534, Total Loss 67.8353500366211\n",
      "9: Encoding Loss 7.933363914489746, Transition Loss -2.2223429679870605, Classifier Loss 0.1522398442029953, Total Loss 62.823280334472656\n",
      "9: Encoding Loss 7.395449638366699, Transition Loss -1.0730243921279907, Classifier Loss 0.18833312392234802, Total Loss 63.20558166503906\n",
      "9: Encoding Loss 5.9276933670043945, Transition Loss -0.2755819261074066, Classifier Loss 0.11490442603826523, Total Loss 47.05649185180664\n",
      "9: Encoding Loss 7.570047855377197, Transition Loss -0.2136392593383789, Classifier Loss 0.2317756861448288, Total Loss 68.59777069091797\n",
      "9: Encoding Loss 4.55271053314209, Transition Loss 0.25723209977149963, Classifier Loss 0.05227375775575638, Total Loss 32.64653396606445\n",
      "9: Encoding Loss 3.711503505706787, Transition Loss -0.8752424716949463, Classifier Loss 0.09796362370252609, Total Loss 32.065032958984375\n",
      "9: Encoding Loss 2.676553964614868, Transition Loss 0.4503031373023987, Classifier Loss 0.09065570682287216, Total Loss 25.305015563964844\n",
      "9: Encoding Loss 5.049121856689453, Transition Loss -0.6912858486175537, Classifier Loss 0.1272130161523819, Total Loss 43.01576232910156\n",
      "9: Encoding Loss 5.978867530822754, Transition Loss -1.7774925231933594, Classifier Loss 0.09386865794658661, Total Loss 45.259361267089844\n",
      "9: Encoding Loss 7.129103183746338, Transition Loss -1.7606871128082275, Classifier Loss 0.17990922927856445, Total Loss 60.764835357666016\n",
      "9: Encoding Loss 4.260319709777832, Transition Loss -1.757340431213379, Classifier Loss 0.07288042455911636, Total Loss 32.84925842285156\n",
      "9: Encoding Loss 6.396716594696045, Transition Loss -2.4035677909851074, Classifier Loss 0.18334896862506866, Total Loss 56.714237213134766\n",
      "9: Encoding Loss 4.66942834854126, Transition Loss -1.1840620040893555, Classifier Loss 0.07334629446268082, Total Loss 35.35072708129883\n",
      "9: Encoding Loss 4.527094841003418, Transition Loss -1.5482853651046753, Classifier Loss 0.10731428861618042, Total Loss 37.89338302612305\n",
      "9: Encoding Loss 4.376152038574219, Transition Loss -3.2509348392486572, Classifier Loss 0.08880924433469772, Total Loss 35.136539459228516\n",
      "9: Encoding Loss 2.5827419757843018, Transition Loss -1.8781726360321045, Classifier Loss 0.0664425641298294, Total Loss 22.139957427978516\n",
      "9: Encoding Loss 5.628132343292236, Transition Loss -1.7402961254119873, Classifier Loss 0.09187022596597672, Total Loss 42.95512390136719\n",
      "9: Encoding Loss 5.209632873535156, Transition Loss -1.969206690788269, Classifier Loss 0.14836469292640686, Total Loss 46.093482971191406\n",
      "9: Encoding Loss 3.101994752883911, Transition Loss -1.631890058517456, Classifier Loss 0.07322706282138824, Total Loss 25.934022903442383\n",
      "9: Encoding Loss 6.664350986480713, Transition Loss -2.317365884780884, Classifier Loss 0.07573258876800537, Total Loss 47.55843734741211\n",
      "9: Encoding Loss 4.402834892272949, Transition Loss -0.9835650324821472, Classifier Loss 0.07419861108064651, Total Loss 33.83647918701172\n",
      "9: Encoding Loss 6.515961647033691, Transition Loss -1.1870131492614746, Classifier Loss 0.11757440865039825, Total Loss 50.85273742675781\n",
      "9: Encoding Loss 6.887938022613525, Transition Loss -1.548326849937439, Classifier Loss 0.14933855831623077, Total Loss 56.260868072509766\n",
      "9: Encoding Loss 6.438691139221191, Transition Loss -0.8833414316177368, Classifier Loss 0.1007886454463005, Total Loss 48.71065902709961\n",
      "9: Encoding Loss 4.952307224273682, Transition Loss -1.55635666847229, Classifier Loss 0.11712837219238281, Total Loss 41.42605972290039\n",
      "9: Encoding Loss 5.949368000030518, Transition Loss -1.8563382625579834, Classifier Loss 0.10734112560749054, Total Loss 46.4295768737793\n",
      "9: Encoding Loss 5.151891708374023, Transition Loss -1.673734188079834, Classifier Loss 0.06724470853805542, Total Loss 37.63515090942383\n",
      "9: Encoding Loss 2.755406141281128, Transition Loss -0.1129864752292633, Classifier Loss 0.040456317365169525, Total Loss 20.57802391052246\n",
      "9: Encoding Loss 7.240910530090332, Transition Loss -0.19760334491729736, Classifier Loss 0.08291428536176682, Total Loss 51.736812591552734\n",
      "9: Encoding Loss 6.44320821762085, Transition Loss -1.3183798789978027, Classifier Loss 0.07103821635246277, Total Loss 45.76254653930664\n",
      "9: Encoding Loss 6.589081764221191, Transition Loss -1.0234006643295288, Classifier Loss 0.03673505038022995, Total Loss 43.20758819580078\n",
      "9: Encoding Loss 4.031022071838379, Transition Loss -0.11683923006057739, Classifier Loss 0.0573158860206604, Total Loss 29.917673110961914\n",
      "9: Encoding Loss 4.1329779624938965, Transition Loss -2.160167694091797, Classifier Loss 0.03699478507041931, Total Loss 28.496482849121094\n",
      "9: Encoding Loss 3.8538730144500732, Transition Loss -1.7317585945129395, Classifier Loss 0.13175462186336517, Total Loss 36.29800796508789\n",
      "9: Encoding Loss 5.316032409667969, Transition Loss -0.4171791672706604, Classifier Loss 0.04244378209114075, Total Loss 36.14040756225586\n",
      "9: Encoding Loss 4.640966415405273, Transition Loss -1.1290948390960693, Classifier Loss 0.04506642371416092, Total Loss 32.351993560791016\n",
      "9: Encoding Loss 4.339521884918213, Transition Loss -0.5769792795181274, Classifier Loss 0.042064715176820755, Total Loss 30.243371963500977\n",
      "9: Encoding Loss 3.650181531906128, Transition Loss -0.8784964084625244, Classifier Loss 0.0733969658613205, Total Loss 29.240436553955078\n",
      "9: Encoding Loss 6.019437313079834, Transition Loss -1.4500302076339722, Classifier Loss 0.06979095935821533, Total Loss 43.09514236450195\n",
      "9: Encoding Loss 4.393418788909912, Transition Loss -1.877703070640564, Classifier Loss 0.06874965131282806, Total Loss 33.23472595214844\n",
      "9: Encoding Loss 3.868579864501953, Transition Loss -2.5354251861572266, Classifier Loss 0.04942198097705841, Total Loss 28.15266227722168\n",
      "9: Encoding Loss 3.847066879272461, Transition Loss -1.5018138885498047, Classifier Loss 0.10403294861316681, Total Loss 33.4850959777832\n",
      "9: Encoding Loss 6.130511283874512, Transition Loss -0.7937820553779602, Classifier Loss 0.0995718389749527, Total Loss 46.73993682861328\n",
      "9: Encoding Loss 3.5324180126190186, Transition Loss -0.8090763092041016, Classifier Loss 0.0550275593996048, Total Loss 26.696941375732422\n",
      "9: Encoding Loss 4.367237091064453, Transition Loss -1.1510119438171387, Classifier Loss 0.10301617532968521, Total Loss 36.504581451416016\n",
      "9: Encoding Loss 4.8325090408325195, Transition Loss -2.894627332687378, Classifier Loss 0.08142534643411636, Total Loss 37.13642883300781\n",
      "9: Encoding Loss 3.9754798412323, Transition Loss -1.0911927223205566, Classifier Loss 0.04488356038928032, Total Loss 28.34079933166504\n",
      "9: Encoding Loss 3.4556021690368652, Transition Loss -0.7417688369750977, Classifier Loss 0.05668584629893303, Total Loss 26.401901245117188\n",
      "9: Encoding Loss 4.689014434814453, Transition Loss -1.7329480648040771, Classifier Loss 0.14735224843025208, Total Loss 42.86861801147461\n",
      "9: Encoding Loss 3.3211982250213623, Transition Loss -2.3170435428619385, Classifier Loss 0.08110406994819641, Total Loss 28.036670684814453\n",
      "9: Encoding Loss 3.721478223800659, Transition Loss -0.9246348142623901, Classifier Loss 0.08488216251134872, Total Loss 30.81671714782715\n",
      "9: Encoding Loss 4.583261489868164, Transition Loss -2.4456522464752197, Classifier Loss 0.06703446060419083, Total Loss 34.20204162597656\n",
      "9: Encoding Loss 5.784613132476807, Transition Loss -1.0306745767593384, Classifier Loss 0.1595035195350647, Total Loss 50.65761947631836\n",
      "9: Encoding Loss 5.41250467300415, Transition Loss -1.6219098567962646, Classifier Loss 0.10537436604499817, Total Loss 43.011817932128906\n",
      "9: Encoding Loss 3.3590939044952393, Transition Loss -2.132330894470215, Classifier Loss 0.08408508449792862, Total Loss 28.562219619750977\n",
      "9: Encoding Loss 6.2996978759765625, Transition Loss -1.1865959167480469, Classifier Loss 0.18780416250228882, Total Loss 56.57813262939453\n",
      "9: Encoding Loss 7.13611364364624, Transition Loss -1.973982572555542, Classifier Loss 0.22546258568763733, Total Loss 65.3621597290039\n",
      "9: Encoding Loss 7.562291145324707, Transition Loss -1.8906171321868896, Classifier Loss 0.12907761335372925, Total Loss 58.28075408935547\n",
      "9: Encoding Loss 7.182298183441162, Transition Loss -2.2814483642578125, Classifier Loss 0.13660164177417755, Total Loss 56.75304412841797\n",
      "9: Encoding Loss 4.351464748382568, Transition Loss -2.9569103717803955, Classifier Loss 0.0816655084490776, Total Loss 34.2741584777832\n",
      "9: Encoding Loss 7.29218053817749, Transition Loss -2.433819532394409, Classifier Loss 0.09031104296445847, Total Loss 52.78321838378906\n",
      "9: Encoding Loss 7.950880527496338, Transition Loss -0.21214115619659424, Classifier Loss 0.1317010372877121, Total Loss 60.87530517578125\n",
      "9: Encoding Loss 6.802985191345215, Transition Loss -2.565112352371216, Classifier Loss 0.226824551820755, Total Loss 63.49934005737305\n",
      "9: Encoding Loss 7.589605808258057, Transition Loss -1.9861191511154175, Classifier Loss 0.1678403913974762, Total Loss 62.32088088989258\n",
      "9: Encoding Loss 6.6547441482543945, Transition Loss 0.43234574794769287, Classifier Loss 0.062222130596637726, Total Loss 46.3236198425293\n",
      "9: Encoding Loss 5.668260097503662, Transition Loss -1.6588345766067505, Classifier Loss 0.034522853791713715, Total Loss 37.461185455322266\n",
      "9: Encoding Loss 2.9958717823028564, Transition Loss -1.3070151805877686, Classifier Loss 0.09422627091407776, Total Loss 27.397335052490234\n",
      "9: Encoding Loss 4.101069450378418, Transition Loss -0.7355815172195435, Classifier Loss 0.08654862642288208, Total Loss 33.260986328125\n",
      "9: Encoding Loss 3.8197944164276123, Transition Loss 0.6052886247634888, Classifier Loss 0.05723711475729942, Total Loss 28.884593963623047\n",
      "9: Encoding Loss 2.1850571632385254, Transition Loss -1.2156933546066284, Classifier Loss 0.03851500526070595, Total Loss 16.96135711669922\n",
      "9: Encoding Loss 4.237453460693359, Transition Loss -0.9707902669906616, Classifier Loss 0.16392429172992706, Total Loss 41.81676483154297\n",
      "9: Encoding Loss 3.8416476249694824, Transition Loss -0.9025654792785645, Classifier Loss 0.1513368785381317, Total Loss 38.18321228027344\n",
      "9: Encoding Loss 6.861325263977051, Transition Loss -0.1611417829990387, Classifier Loss 0.057854197919368744, Total Loss 46.95330810546875\n",
      "9: Encoding Loss 5.265198707580566, Transition Loss -2.3085880279541016, Classifier Loss 0.07933904975652695, Total Loss 39.524173736572266\n",
      "9: Encoding Loss 8.186723709106445, Transition Loss -1.2721877098083496, Classifier Loss 0.2907741963863373, Total Loss 78.19725036621094\n",
      "9: Encoding Loss 5.170831203460693, Transition Loss -2.5000007152557373, Classifier Loss 0.1369466483592987, Total Loss 44.71865463256836\n",
      "9: Encoding Loss 4.714848518371582, Transition Loss -1.4764779806137085, Classifier Loss 0.052310291677713394, Total Loss 33.519527435302734\n",
      "9: Encoding Loss 6.0288615226745605, Transition Loss -1.7989100217819214, Classifier Loss 0.14279130101203918, Total Loss 50.45158004760742\n",
      "9: Encoding Loss 6.959902286529541, Transition Loss -1.7620255947113037, Classifier Loss 0.10819090902805328, Total Loss 52.57780075073242\n",
      "9: Encoding Loss 5.220649242401123, Transition Loss 0.1703404188156128, Classifier Loss 0.13513991236686707, Total Loss 44.90602111816406\n",
      "9: Encoding Loss 4.1841864585876465, Transition Loss 0.7030596137046814, Classifier Loss 0.08771833777427673, Total Loss 34.15817642211914\n",
      "9: Encoding Loss 5.046261787414551, Transition Loss -2.1212923526763916, Classifier Loss 0.08407782018184662, Total Loss 38.684505462646484\n",
      "9: Encoding Loss 7.109048366546631, Transition Loss -1.5493199825286865, Classifier Loss 0.10251045227050781, Total Loss 52.904720306396484\n",
      "9: Encoding Loss 5.369175910949707, Transition Loss -0.2547966539859772, Classifier Loss 0.13500577211380005, Total Loss 45.71553039550781\n",
      "9: Encoding Loss 3.389853000640869, Transition Loss -0.43500012159347534, Classifier Loss 0.04484757408499718, Total Loss 24.823701858520508\n",
      "9: Encoding Loss 4.711269855499268, Transition Loss -1.9778449535369873, Classifier Loss 0.13636615872383118, Total Loss 41.903446197509766\n",
      "9: Encoding Loss 3.609621524810791, Transition Loss 0.8202515244483948, Classifier Loss 0.09466176480054855, Total Loss 31.45200538635254\n",
      "9: Encoding Loss 6.511516571044922, Transition Loss -1.9048303365707397, Classifier Loss 0.08978794515132904, Total Loss 48.0471305847168\n",
      "9: Encoding Loss 3.807481288909912, Transition Loss -2.2786998748779297, Classifier Loss 0.07517586648464203, Total Loss 30.36156463623047\n",
      "9: Encoding Loss 6.31619930267334, Transition Loss -1.2917059659957886, Classifier Loss 0.127017080783844, Total Loss 50.598392486572266\n",
      "9: Encoding Loss 4.389113903045654, Transition Loss -2.1553151607513428, Classifier Loss 0.06224428489804268, Total Loss 32.558250427246094\n",
      "9: Encoding Loss 5.302896022796631, Transition Loss -1.4456510543823242, Classifier Loss 0.11456651240587234, Total Loss 43.2734489440918\n",
      "9: Encoding Loss 5.478339195251465, Transition Loss -2.1237051486968994, Classifier Loss 0.10471274703741074, Total Loss 43.34046173095703\n",
      "9: Encoding Loss 4.1177496910095215, Transition Loss 0.8908027410507202, Classifier Loss 0.09663187712430954, Total Loss 34.72600555419922\n",
      "9: Encoding Loss 2.131087303161621, Transition Loss -0.8210875988006592, Classifier Loss 0.0662430003285408, Total Loss 19.41049575805664\n",
      "9: Encoding Loss 9.440436363220215, Transition Loss -2.0610411167144775, Classifier Loss 0.10865604132413864, Total Loss 67.50740051269531\n",
      "9: Encoding Loss 5.842480659484863, Transition Loss -1.4357681274414062, Classifier Loss 0.1010913997888565, Total Loss 45.163448333740234\n",
      "9: Encoding Loss 4.8971147537231445, Transition Loss -0.9433505535125732, Classifier Loss 0.10004829615354538, Total Loss 39.387142181396484\n",
      "9: Encoding Loss 4.590023517608643, Transition Loss -1.2183400392532349, Classifier Loss 0.06677818298339844, Total Loss 34.217472076416016\n",
      "9: Encoding Loss 3.474851131439209, Transition Loss -1.104224443435669, Classifier Loss 0.12430344521999359, Total Loss 33.27901077270508\n",
      "9: Encoding Loss 5.038095951080322, Transition Loss -1.7691600322723389, Classifier Loss 0.15465925633907318, Total Loss 45.69379425048828\n",
      "9: Encoding Loss 3.3913447856903076, Transition Loss 0.24569369852542877, Classifier Loss 0.05297146737575531, Total Loss 25.74349594116211\n",
      "9: Encoding Loss 6.05473518371582, Transition Loss -2.355806827545166, Classifier Loss 0.06936075538396835, Total Loss 43.263545989990234\n",
      "9: Encoding Loss 5.52689266204834, Transition Loss -1.1172306537628174, Classifier Loss 0.03529908508062363, Total Loss 36.690818786621094\n",
      "9: Encoding Loss 5.28966760635376, Transition Loss -1.9527465105056763, Classifier Loss 0.07130585610866547, Total Loss 38.8678092956543\n",
      "9: Encoding Loss 4.154170036315918, Transition Loss -0.9164870977401733, Classifier Loss 0.035589054226875305, Total Loss 28.48356056213379\n",
      "9: Encoding Loss 4.228358268737793, Transition Loss 0.16206344962120056, Classifier Loss 0.13786421716213226, Total Loss 39.22140121459961\n",
      "9: Encoding Loss 5.018346309661865, Transition Loss -1.0954936742782593, Classifier Loss 0.05322650074958801, Total Loss 35.432289123535156\n",
      "9: Encoding Loss 3.594142198562622, Transition Loss -2.6575541496276855, Classifier Loss 0.09777265042066574, Total Loss 31.341054916381836\n",
      "9: Encoding Loss 5.218168258666992, Transition Loss -1.6382023096084595, Classifier Loss 0.20061203837394714, Total Loss 51.36956024169922\n",
      "9: Encoding Loss 5.095241069793701, Transition Loss -1.277877926826477, Classifier Loss 0.10419725626707077, Total Loss 40.99066162109375\n",
      "9: Encoding Loss 7.421137809753418, Transition Loss -1.2682156562805176, Classifier Loss 0.09451459348201752, Total Loss 53.977779388427734\n",
      "9: Encoding Loss 4.711014270782471, Transition Loss -0.46138834953308105, Classifier Loss 0.09132317453622818, Total Loss 37.39822006225586\n",
      "9: Encoding Loss 4.262129783630371, Transition Loss 0.1401204615831375, Classifier Loss 0.11342005431652069, Total Loss 36.9708366394043\n",
      "9: Encoding Loss 7.055571556091309, Transition Loss -1.4470311403274536, Classifier Loss 0.09288369119167328, Total Loss 51.621219635009766\n",
      "9: Encoding Loss 5.384110450744629, Transition Loss -2.2929110527038574, Classifier Loss 0.13647957146167755, Total Loss 45.95170593261719\n",
      "9: Encoding Loss 7.414331912994385, Transition Loss -2.1046369075775146, Classifier Loss 0.12271111458539963, Total Loss 56.75625991821289\n",
      "9: Encoding Loss 5.2721943855285645, Transition Loss -2.208470344543457, Classifier Loss 0.057542018592357635, Total Loss 37.38648223876953\n",
      "9: Encoding Loss 3.9016201496124268, Transition Loss -2.2903685569763184, Classifier Loss 0.12935124337673187, Total Loss 36.343929290771484\n",
      "9: Encoding Loss 6.512281894683838, Transition Loss -1.0751943588256836, Classifier Loss 0.09370498359203339, Total Loss 48.44375991821289\n",
      "9: Encoding Loss 8.74502182006836, Transition Loss -1.389228343963623, Classifier Loss 0.1481107771396637, Total Loss 67.28065490722656\n",
      "9: Encoding Loss 5.676495552062988, Transition Loss -1.4840936660766602, Classifier Loss 0.04722686856985092, Total Loss 38.78106689453125\n",
      "9: Encoding Loss 4.250124931335449, Transition Loss -0.1913868486881256, Classifier Loss 0.06003399193286896, Total Loss 31.504074096679688\n",
      "9: Encoding Loss 3.5397543907165527, Transition Loss -1.0267707109451294, Classifier Loss 0.0897480770945549, Total Loss 30.21292495727539\n",
      "9: Encoding Loss 5.546905040740967, Transition Loss -1.48723566532135, Classifier Loss 0.06304298341274261, Total Loss 39.58513641357422\n",
      "9: Encoding Loss 4.071763515472412, Transition Loss -1.0637280941009521, Classifier Loss 0.03427448123693466, Total Loss 27.85760498046875\n",
      "9: Encoding Loss 6.190108299255371, Transition Loss -2.049388885498047, Classifier Loss 0.048675257712602615, Total Loss 42.00735855102539\n",
      "9: Encoding Loss 3.3357739448547363, Transition Loss -1.9850285053253174, Classifier Loss 0.08862194418907166, Total Loss 28.87604522705078\n",
      "9: Encoding Loss 8.07763671875, Transition Loss 0.1852821707725525, Classifier Loss 0.26309216022491455, Total Loss 74.84915161132812\n",
      "9: Encoding Loss 5.139851093292236, Transition Loss -1.1382720470428467, Classifier Loss 0.12762266397476196, Total Loss 43.60091781616211\n",
      "9: Encoding Loss 6.178684234619141, Transition Loss -1.2554776668548584, Classifier Loss 0.09482335299253464, Total Loss 46.55393600463867\n",
      "9: Encoding Loss 5.763138294219971, Transition Loss 0.17428217828273773, Classifier Loss 0.09428776800632477, Total Loss 44.07732009887695\n",
      "9: Encoding Loss 6.703936576843262, Transition Loss -0.5494124293327332, Classifier Loss 0.19251446425914764, Total Loss 59.47484588623047\n",
      "9: Encoding Loss 5.998673915863037, Transition Loss -1.4176561832427979, Classifier Loss 0.16400131583213806, Total Loss 52.39160919189453\n",
      "9: Encoding Loss 4.37593936920166, Transition Loss -0.5693787932395935, Classifier Loss 0.08865378797054291, Total Loss 35.120784759521484\n",
      "9: Encoding Loss 4.7681803703308105, Transition Loss -2.155062198638916, Classifier Loss 0.12805061042308807, Total Loss 41.41328430175781\n",
      "9: Encoding Loss 4.954558849334717, Transition Loss -0.7453845143318176, Classifier Loss 0.11723936349153519, Total Loss 41.450992584228516\n",
      "9: Encoding Loss 5.715379238128662, Transition Loss -0.3118574321269989, Classifier Loss 0.10536044836044312, Total Loss 44.82819747924805\n",
      "9: Encoding Loss 3.2650465965270996, Transition Loss -0.9141969680786133, Classifier Loss 0.05734161660075188, Total Loss 25.32407569885254\n",
      "9: Encoding Loss 3.751727342605591, Transition Loss -0.8803653120994568, Classifier Loss 0.11274141818284988, Total Loss 33.784156799316406\n",
      "9: Encoding Loss 3.5066566467285156, Transition Loss -1.9006915092468262, Classifier Loss 0.09309274703264236, Total Loss 30.348453521728516\n",
      "9: Encoding Loss 4.814755439758301, Transition Loss -0.6837728023529053, Classifier Loss 0.044353727251291275, Total Loss 33.323631286621094\n",
      "9: Encoding Loss 5.14119291305542, Transition Loss -1.1064857244491577, Classifier Loss 0.1257905513048172, Total Loss 43.4257698059082\n",
      "9: Encoding Loss 3.996342420578003, Transition Loss -2.2835447788238525, Classifier Loss 0.09328199177980423, Total Loss 33.30534362792969\n",
      "9: Encoding Loss 4.733450412750244, Transition Loss -1.3758842945098877, Classifier Loss 0.12503677606582642, Total Loss 40.903831481933594\n",
      "9: Encoding Loss 3.734818935394287, Transition Loss -0.14441156387329102, Classifier Loss 0.0417095385491848, Total Loss 26.579811096191406\n",
      "9: Encoding Loss 3.5074620246887207, Transition Loss -0.33675920963287354, Classifier Loss 0.05401040241122246, Total Loss 26.445676803588867\n",
      "9: Encoding Loss 4.799249172210693, Transition Loss -0.6036524772644043, Classifier Loss 0.1983177214860916, Total Loss 48.62702560424805\n",
      "9: Encoding Loss 5.954048156738281, Transition Loss -1.697073221206665, Classifier Loss 0.19849099218845367, Total Loss 55.57270812988281\n",
      "9: Encoding Loss 4.029397964477539, Transition Loss -0.5472288727760315, Classifier Loss 0.12872286140918732, Total Loss 37.048458099365234\n",
      "9: Encoding Loss 6.446206569671631, Transition Loss -1.6607294082641602, Classifier Loss 0.14908048510551453, Total Loss 53.584625244140625\n",
      "9: Encoding Loss 3.8228259086608887, Transition Loss -2.011291980743408, Classifier Loss 0.07466258853673935, Total Loss 30.40241050720215\n",
      "9: Encoding Loss 5.552915096282959, Transition Loss -1.6455957889556885, Classifier Loss 0.12934382259845734, Total Loss 46.251216888427734\n",
      "9: Encoding Loss 5.120968341827393, Transition Loss -0.9784186482429504, Classifier Loss 0.04314351826906204, Total Loss 35.03976821899414\n",
      "9: Encoding Loss 3.132596969604492, Transition Loss -1.254156470298767, Classifier Loss 0.10243182629346848, Total Loss 29.03826332092285\n",
      "9: Encoding Loss 3.713249683380127, Transition Loss -2.204930305480957, Classifier Loss 0.05787515267729759, Total Loss 28.066133499145508\n",
      "9: Encoding Loss 3.7832188606262207, Transition Loss -0.21725620329380035, Classifier Loss 0.14438197016716003, Total Loss 37.13742446899414\n",
      "9: Encoding Loss 6.862707614898682, Transition Loss -0.7647488713264465, Classifier Loss 0.060031451284885406, Total Loss 47.17908477783203\n",
      "9: Encoding Loss 5.6222333908081055, Transition Loss -1.4821323156356812, Classifier Loss 0.21986958384513855, Total Loss 55.71976852416992\n",
      "9: Encoding Loss 6.339570045471191, Transition Loss -0.25897276401519775, Classifier Loss 0.13695712387561798, Total Loss 51.7330322265625\n",
      "9: Encoding Loss 5.360311985015869, Transition Loss -2.2968578338623047, Classifier Loss 0.1357586830854416, Total Loss 45.736820220947266\n",
      "9: Encoding Loss 4.9229021072387695, Transition Loss -0.9108664989471436, Classifier Loss 0.05961520969867706, Total Loss 35.498565673828125\n",
      "9: Encoding Loss 3.0491786003112793, Transition Loss -2.3871262073516846, Classifier Loss 0.0634903758764267, Total Loss 24.64315414428711\n",
      "9: Encoding Loss 2.8538403511047363, Transition Loss -1.3135035037994385, Classifier Loss 0.057981133460998535, Total Loss 22.920631408691406\n",
      "9: Encoding Loss 3.9660580158233643, Transition Loss -2.6330645084381104, Classifier Loss 0.06264633685350418, Total Loss 30.05992889404297\n",
      "9: Encoding Loss 6.1116485595703125, Transition Loss 0.13929694890975952, Classifier Loss 0.045794110745191574, Total Loss 41.30501937866211\n",
      "9: Encoding Loss 5.287533283233643, Transition Loss -0.2722432613372803, Classifier Loss 0.10831262171268463, Total Loss 42.55635452270508\n",
      "9: Encoding Loss 6.573434352874756, Transition Loss -3.485740900039673, Classifier Loss 0.07035545259714127, Total Loss 46.47475814819336\n",
      "9: Encoding Loss 5.386311054229736, Transition Loss -1.9263054132461548, Classifier Loss 0.0896540954709053, Total Loss 41.28250503540039\n",
      "9: Encoding Loss 4.349445343017578, Transition Loss 0.08025223016738892, Classifier Loss 0.10876403003931046, Total Loss 37.00517654418945\n",
      "9: Encoding Loss 4.368203163146973, Transition Loss -2.351781129837036, Classifier Loss 0.04343164712190628, Total Loss 30.551443099975586\n",
      "9: Encoding Loss 7.33688497543335, Transition Loss -1.063638687133789, Classifier Loss 0.11124483495950699, Total Loss 55.14537048339844\n",
      "9: Encoding Loss 5.996243476867676, Transition Loss -2.714229106903076, Classifier Loss 0.16099625825881958, Total Loss 52.07600021362305\n",
      "9: Encoding Loss 5.630875587463379, Transition Loss -1.6749608516693115, Classifier Loss 0.09899899363517761, Total Loss 43.68448257446289\n",
      "9: Encoding Loss 6.222561836242676, Transition Loss -0.6184759140014648, Classifier Loss 0.16027072072029114, Total Loss 53.36219787597656\n",
      "9: Encoding Loss 6.203215599060059, Transition Loss -1.7297152280807495, Classifier Loss 0.02750275284051895, Total Loss 39.96887969970703\n",
      "9: Encoding Loss 5.197065353393555, Transition Loss -1.625178337097168, Classifier Loss 0.18327219784259796, Total Loss 49.50896453857422\n",
      "9: Encoding Loss 4.007601737976074, Transition Loss -0.7036406993865967, Classifier Loss 0.07107921689748764, Total Loss 31.15325164794922\n",
      "9: Encoding Loss 6.466329097747803, Transition Loss -1.9929509162902832, Classifier Loss 0.11747389286756516, Total Loss 50.5445671081543\n",
      "9: Encoding Loss 5.54911994934082, Transition Loss -0.7050853967666626, Classifier Loss 0.09825362265110016, Total Loss 43.11980056762695\n",
      "9: Encoding Loss 4.598940849304199, Transition Loss -0.5019079446792603, Classifier Loss 0.07280135154724121, Total Loss 34.87358093261719\n",
      "9: Encoding Loss 4.436808109283447, Transition Loss -1.122926115989685, Classifier Loss 0.11388840526342392, Total Loss 38.009239196777344\n",
      "9: Encoding Loss 4.815552234649658, Transition Loss -0.7565114498138428, Classifier Loss 0.14234229922294617, Total Loss 43.12724304199219\n",
      "9: Encoding Loss 5.563236713409424, Transition Loss -0.4598388671875, Classifier Loss 0.12074027955532074, Total Loss 45.45326614379883\n",
      "9: Encoding Loss 5.156750679016113, Transition Loss -2.997185707092285, Classifier Loss 0.0606035441160202, Total Loss 36.99966049194336\n",
      "9: Encoding Loss 2.689898729324341, Transition Loss -1.7601735591888428, Classifier Loss 0.08881159126758575, Total Loss 25.019847869873047\n",
      "9: Encoding Loss 9.404333114624023, Transition Loss -2.230912446975708, Classifier Loss 0.14888054132461548, Total Loss 71.31316375732422\n",
      "9: Encoding Loss 6.217555046081543, Transition Loss -0.18725991249084473, Classifier Loss 0.0349690243601799, Total Loss 40.80215835571289\n",
      "9: Encoding Loss 5.7329182624816895, Transition Loss -0.881145715713501, Classifier Loss 0.169769287109375, Total Loss 51.374088287353516\n",
      "9: Encoding Loss 5.04409122467041, Transition Loss -1.089832067489624, Classifier Loss 0.14700427651405334, Total Loss 44.96453857421875\n",
      "9: Encoding Loss 6.131762504577637, Transition Loss -0.9248023629188538, Classifier Loss 0.061059609055519104, Total Loss 42.89616775512695\n",
      "9: Encoding Loss 6.821479797363281, Transition Loss -1.5609532594680786, Classifier Loss 0.27074912190437317, Total Loss 68.00316619873047\n",
      "9: Encoding Loss 5.013733863830566, Transition Loss -0.28740090131759644, Classifier Loss 0.08782286196947098, Total Loss 38.86457443237305\n",
      "9: Encoding Loss 5.103652000427246, Transition Loss 0.48164641857147217, Classifier Loss 0.11873120814561844, Total Loss 42.68769073486328\n",
      "9: Encoding Loss 5.1991472244262695, Transition Loss -0.5653336048126221, Classifier Loss 0.09211404621601105, Total Loss 40.406063079833984\n",
      "9: Encoding Loss 3.9257073402404785, Transition Loss 0.2585214078426361, Classifier Loss 0.026783784851431847, Total Loss 26.33603286743164\n",
      "9: Encoding Loss 6.616583347320557, Transition Loss -1.3012850284576416, Classifier Loss 0.08102338016033173, Total Loss 47.80131912231445\n",
      "9: Encoding Loss 7.116288661956787, Transition Loss -1.7716376781463623, Classifier Loss 0.20154136419296265, Total Loss 62.85116195678711\n",
      "9: Encoding Loss 4.659670352935791, Transition Loss -1.0222772359848022, Classifier Loss 0.07712403684854507, Total Loss 35.67001724243164\n",
      "9: Encoding Loss 3.870241641998291, Transition Loss -1.951385259628296, Classifier Loss 0.07020343095064163, Total Loss 30.24101448059082\n",
      "9: Encoding Loss 4.23392915725708, Transition Loss -1.270101547241211, Classifier Loss 0.0751364454627037, Total Loss 32.91671371459961\n",
      "9: Encoding Loss 5.018550872802734, Transition Loss -0.3632843494415283, Classifier Loss 0.04268772527575493, Total Loss 34.37993621826172\n",
      "9: Encoding Loss 5.650148391723633, Transition Loss -0.9927067160606384, Classifier Loss 0.1610219031572342, Total Loss 50.002685546875\n",
      "9: Encoding Loss 3.1511662006378174, Transition Loss -0.2907274663448334, Classifier Loss 0.12774746119976044, Total Loss 31.68162727355957\n",
      "9: Encoding Loss 5.8367204666137695, Transition Loss -2.083390235900879, Classifier Loss 0.080595463514328, Total Loss 43.07904052734375\n",
      "9: Encoding Loss 3.7343056201934814, Transition Loss -0.9979320764541626, Classifier Loss 0.046153340488672256, Total Loss 27.020769119262695\n",
      "9: Encoding Loss 4.673943996429443, Transition Loss -2.7304604053497314, Classifier Loss 0.04243858531117439, Total Loss 32.28643035888672\n",
      "9: Encoding Loss 2.1279642581939697, Transition Loss -1.53090500831604, Classifier Loss 0.03656843304634094, Total Loss 16.42401695251465\n",
      "9: Encoding Loss 6.052031993865967, Transition Loss -1.6929914951324463, Classifier Loss 0.12843796610832214, Total Loss 49.155311584472656\n",
      "9: Encoding Loss 4.285524368286133, Transition Loss -1.8493061065673828, Classifier Loss 0.09704374521970749, Total Loss 35.41678237915039\n",
      "9: Encoding Loss 3.871997117996216, Transition Loss -0.6823280453681946, Classifier Loss 0.0714375227689743, Total Loss 30.375463485717773\n",
      "9: Encoding Loss 5.436344146728516, Transition Loss 0.3642003834247589, Classifier Loss 0.08515048772096634, Total Loss 41.27879333496094\n",
      "9: Encoding Loss 6.144907474517822, Transition Loss -1.8079358339309692, Classifier Loss 0.14937086403369904, Total Loss 51.805809020996094\n",
      "9: Encoding Loss 6.6514892578125, Transition Loss -3.741015911102295, Classifier Loss 0.14175130426883698, Total Loss 54.08257293701172\n",
      "9: Encoding Loss 4.66204309463501, Transition Loss -1.6536954641342163, Classifier Loss 0.07873426377773285, Total Loss 35.845027923583984\n",
      "9: Encoding Loss 5.303054332733154, Transition Loss -2.282763957977295, Classifier Loss 0.03994552791118622, Total Loss 35.81196975708008\n",
      "9: Encoding Loss 2.7151429653167725, Transition Loss -2.7643661499023438, Classifier Loss 0.05197572335600853, Total Loss 21.48732566833496\n",
      "9: Encoding Loss 4.754857540130615, Transition Loss -2.932229995727539, Classifier Loss 0.17464618384838104, Total Loss 45.99259567260742\n",
      "9: Encoding Loss 4.962467670440674, Transition Loss -0.8271529078483582, Classifier Loss 0.1820434182882309, Total Loss 47.978816986083984\n",
      "9: Encoding Loss 7.292115211486816, Transition Loss -0.4512544870376587, Classifier Loss 0.09348386526107788, Total Loss 53.10090255737305\n",
      "9: Encoding Loss 4.2048540115356445, Transition Loss -1.115421175956726, Classifier Loss 0.04536549746990204, Total Loss 29.765228271484375\n",
      "9: Encoding Loss 4.021109104156494, Transition Loss -1.657869815826416, Classifier Loss 0.05526234954595566, Total Loss 29.6522274017334\n",
      "9: Encoding Loss 5.441578388214111, Transition Loss -1.5365298986434937, Classifier Loss 0.03313661739230156, Total Loss 35.962520599365234\n",
      "9: Encoding Loss 4.521146297454834, Transition Loss -0.8950961828231812, Classifier Loss 0.057512037456035614, Total Loss 32.877723693847656\n",
      "9: Encoding Loss 4.882635593414307, Transition Loss -1.4253709316253662, Classifier Loss 0.06623750180006027, Total Loss 35.91899490356445\n",
      "9: Encoding Loss 4.968876838684082, Transition Loss -1.4243959188461304, Classifier Loss 0.05982235074043274, Total Loss 35.794925689697266\n",
      "9: Encoding Loss 5.413508415222168, Transition Loss -1.3754664659500122, Classifier Loss 0.05012991651892662, Total Loss 37.49349594116211\n",
      "9: Encoding Loss 4.608642578125, Transition Loss -1.236596703529358, Classifier Loss 0.1227521076798439, Total Loss 39.926570892333984\n",
      "9: Encoding Loss 4.074098110198975, Transition Loss -0.8666846752166748, Classifier Loss 0.0860331580042839, Total Loss 33.04755783081055\n",
      "9: Encoding Loss 5.411643028259277, Transition Loss 0.35314488410949707, Classifier Loss 0.11676500737667084, Total Loss 44.287620544433594\n",
      "9: Encoding Loss 4.586949825286865, Transition Loss -1.8918256759643555, Classifier Loss 0.06483256816864014, Total Loss 34.00419998168945\n",
      "9: Encoding Loss 4.741002082824707, Transition Loss -2.077899694442749, Classifier Loss 0.06329582631587982, Total Loss 34.77476501464844\n",
      "9: Encoding Loss 4.403809547424316, Transition Loss -0.7105151414871216, Classifier Loss 0.14343580603599548, Total Loss 40.766151428222656\n",
      "9: Encoding Loss 3.8983864784240723, Transition Loss -0.2535068094730377, Classifier Loss 0.04242786020040512, Total Loss 27.633005142211914\n",
      "9: Encoding Loss 5.074021816253662, Transition Loss -2.159590005874634, Classifier Loss 0.06521343439817429, Total Loss 36.96461486816406\n",
      "9: Encoding Loss 4.141486167907715, Transition Loss -0.8988676071166992, Classifier Loss 0.035528890788555145, Total Loss 28.401447296142578\n",
      "9: Encoding Loss 6.188220024108887, Transition Loss 0.09736236929893494, Classifier Loss 0.19046838581562042, Total Loss 56.21510696411133\n",
      "9: Encoding Loss 5.772933006286621, Transition Loss -1.6969099044799805, Classifier Loss 0.08405305445194244, Total Loss 43.04222869873047\n",
      "9: Encoding Loss 6.325634002685547, Transition Loss -0.4956490695476532, Classifier Loss 0.15072013437747955, Total Loss 53.02561950683594\n",
      "9: Encoding Loss 4.463482856750488, Transition Loss -1.1170297861099243, Classifier Loss 0.027192145586013794, Total Loss 29.499664306640625\n",
      "9: Encoding Loss 4.803969383239746, Transition Loss -1.051008939743042, Classifier Loss 0.1400623470544815, Total Loss 42.82963562011719\n",
      "9: Encoding Loss 2.8595528602600098, Transition Loss -1.5428171157836914, Classifier Loss 0.10408142954111099, Total Loss 27.564842224121094\n",
      "9: Encoding Loss 5.81374454498291, Transition Loss -0.34882232546806335, Classifier Loss 0.12511850893497467, Total Loss 47.39418029785156\n",
      "9: Encoding Loss 5.623423099517822, Transition Loss -1.35234534740448, Classifier Loss 0.09024478495121002, Total Loss 42.76447677612305\n",
      "9: Encoding Loss 3.8978257179260254, Transition Loss -0.5016658902168274, Classifier Loss 0.11370916664600372, Total Loss 34.75767135620117\n",
      "9: Encoding Loss 4.765786647796631, Transition Loss -1.3772013187408447, Classifier Loss 0.0779290646314621, Total Loss 36.38707733154297\n",
      "9: Encoding Loss 6.105975151062012, Transition Loss -0.4536547064781189, Classifier Loss 0.0704408511519432, Total Loss 43.67975616455078\n",
      "9: Encoding Loss 3.862398862838745, Transition Loss 0.11049909889698029, Classifier Loss 0.06010652706027031, Total Loss 29.229246139526367\n",
      "9: Encoding Loss 6.289915561676025, Transition Loss -0.017114683985710144, Classifier Loss 0.15901623666286469, Total Loss 53.641109466552734\n",
      "9: Encoding Loss 5.386969566345215, Transition Loss -1.0711381435394287, Classifier Loss 0.158120259642601, Total Loss 48.133419036865234\n",
      "9: Encoding Loss 4.602736473083496, Transition Loss -1.374391794204712, Classifier Loss 0.0737704485654831, Total Loss 34.992916107177734\n",
      "9: Encoding Loss 4.0108747482299805, Transition Loss -1.4834975004196167, Classifier Loss 0.1500740349292755, Total Loss 39.072059631347656\n",
      "9: Encoding Loss 6.134603023529053, Transition Loss -0.05122113227844238, Classifier Loss 0.13770952820777893, Total Loss 50.578556060791016\n",
      "9: Encoding Loss 5.364362716674805, Transition Loss -1.5561702251434326, Classifier Loss 0.13696731626987457, Total Loss 45.882286071777344\n",
      "9: Encoding Loss 5.731546401977539, Transition Loss -1.1954551935195923, Classifier Loss 0.1488042026758194, Total Loss 49.269222259521484\n",
      "9: Encoding Loss 5.093955039978027, Transition Loss -1.2061471939086914, Classifier Loss 0.07724770158529282, Total Loss 38.288021087646484\n",
      "9: Encoding Loss 5.879912376403809, Transition Loss -1.6112834215164185, Classifier Loss 0.12262390553951263, Total Loss 47.541221618652344\n",
      "9: Encoding Loss 5.526289463043213, Transition Loss -0.858737587928772, Classifier Loss 0.09619744122028351, Total Loss 42.777137756347656\n",
      "9: Encoding Loss 4.575072765350342, Transition Loss -1.2161177396774292, Classifier Loss 0.08903739601373672, Total Loss 36.35369110107422\n",
      "9: Encoding Loss 4.7186455726623535, Transition Loss 0.03880143165588379, Classifier Loss 0.04500308632850647, Total Loss 32.82770538330078\n",
      "9: Encoding Loss 3.8174946308135986, Transition Loss -0.2712375521659851, Classifier Loss 0.056004155427217484, Total Loss 28.50527572631836\n",
      "9: Encoding Loss 6.7785139083862305, Transition Loss -1.6167563199996948, Classifier Loss 0.15868611633777618, Total Loss 56.53904724121094\n",
      "9: Encoding Loss 5.767989158630371, Transition Loss -1.8376256227493286, Classifier Loss 0.11811001598834991, Total Loss 46.4182014465332\n",
      "9: Encoding Loss 3.201554775238037, Transition Loss -1.0101457834243774, Classifier Loss 0.08214519917964935, Total Loss 27.423444747924805\n",
      "9: Encoding Loss 7.208096981048584, Transition Loss -1.6302785873413086, Classifier Loss 0.08554917573928833, Total Loss 51.80284881591797\n",
      "9: Encoding Loss 4.605191230773926, Transition Loss -0.9085359573364258, Classifier Loss 0.06519700586795807, Total Loss 34.15048599243164\n",
      "9: Encoding Loss 4.499101638793945, Transition Loss -0.9553805589675903, Classifier Loss 0.04344255104660988, Total Loss 31.338485717773438\n",
      "9: Encoding Loss 6.323600769042969, Transition Loss -1.9047986268997192, Classifier Loss 0.10727749764919281, Total Loss 48.6685905456543\n",
      "9: Encoding Loss 5.786866664886475, Transition Loss -1.067649245262146, Classifier Loss 0.19723603129386902, Total Loss 54.44437789916992\n",
      "9: Encoding Loss 5.426434516906738, Transition Loss -1.3136000633239746, Classifier Loss 0.0793360024690628, Total Loss 40.49168395996094\n",
      "9: Encoding Loss 4.499852657318115, Transition Loss -0.023910045623779297, Classifier Loss 0.07472283393144608, Total Loss 34.47138977050781\n",
      "9: Encoding Loss 5.153153419494629, Transition Loss -2.334969997406006, Classifier Loss 0.08033210039138794, Total Loss 38.951194763183594\n",
      "9: Encoding Loss 5.110940933227539, Transition Loss -1.8667055368423462, Classifier Loss 0.08433812111616135, Total Loss 39.09871292114258\n",
      "9: Encoding Loss 6.056717872619629, Transition Loss 0.24139276146888733, Classifier Loss 0.11980514228343964, Total Loss 48.417381286621094\n",
      "9: Encoding Loss 7.5651702880859375, Transition Loss -1.3209829330444336, Classifier Loss 0.1343279629945755, Total Loss 58.82328796386719\n",
      "9: Encoding Loss 4.893162727355957, Transition Loss -2.259963035583496, Classifier Loss 0.05139106512069702, Total Loss 34.4971809387207\n",
      "9: Encoding Loss 6.074986457824707, Transition Loss -1.014465093612671, Classifier Loss 0.05471384525299072, Total Loss 41.920902252197266\n",
      "9: Encoding Loss 5.333817958831787, Transition Loss -0.7655977010726929, Classifier Loss 0.14465871453285217, Total Loss 46.468475341796875\n",
      "9: Encoding Loss 4.918630599975586, Transition Loss -1.0990675687789917, Classifier Loss 0.03715302422642708, Total Loss 33.22665023803711\n",
      "10: Encoding Loss 3.783616542816162, Transition Loss -1.1898341178894043, Classifier Loss 0.07361727952957153, Total Loss 30.062950134277344\n",
      "10: Encoding Loss 6.205037593841553, Transition Loss -0.20596462488174438, Classifier Loss 0.09124569594860077, Total Loss 46.354713439941406\n",
      "10: Encoding Loss 5.029950141906738, Transition Loss -1.5959525108337402, Classifier Loss 0.08162613213062286, Total Loss 38.341678619384766\n",
      "10: Encoding Loss 5.890259742736816, Transition Loss -1.1036150455474854, Classifier Loss 0.06782691925764084, Total Loss 42.123809814453125\n",
      "10: Encoding Loss 7.6044721603393555, Transition Loss 0.42372822761535645, Classifier Loss 0.18050923943519592, Total Loss 63.84724807739258\n",
      "10: Encoding Loss 6.414567470550537, Transition Loss -2.869410753250122, Classifier Loss 0.05686149746179581, Total Loss 44.17240905761719\n",
      "10: Encoding Loss 4.194669246673584, Transition Loss -2.06046986579895, Classifier Loss 0.1372755616903305, Total Loss 38.89474868774414\n",
      "10: Encoding Loss 4.250118732452393, Transition Loss -1.664290428161621, Classifier Loss 0.1038408949971199, Total Loss 35.88413619995117\n",
      "10: Encoding Loss 4.983495712280273, Transition Loss -1.1289714574813843, Classifier Loss 0.1490071564912796, Total Loss 44.80124282836914\n",
      "10: Encoding Loss 3.1837100982666016, Transition Loss -0.676374077796936, Classifier Loss 0.07761739194393158, Total Loss 26.86372947692871\n",
      "10: Encoding Loss 5.937603950500488, Transition Loss -1.4048784971237183, Classifier Loss 0.1415311098098755, Total Loss 49.778175354003906\n",
      "10: Encoding Loss 3.8260955810546875, Transition Loss -1.2088360786437988, Classifier Loss 0.07547467201948166, Total Loss 30.503555297851562\n",
      "10: Encoding Loss 2.5944015979766846, Transition Loss -0.38197430968284607, Classifier Loss 0.04181567206978798, Total Loss 19.747825622558594\n",
      "10: Encoding Loss 4.425960063934326, Transition Loss -2.17069935798645, Classifier Loss 0.07947265356779099, Total Loss 34.50215530395508\n",
      "10: Encoding Loss 3.5013465881347656, Transition Loss -2.1621012687683105, Classifier Loss 0.07383719086647034, Total Loss 28.390933990478516\n",
      "10: Encoding Loss 4.446153163909912, Transition Loss -1.4171669483184814, Classifier Loss 0.07165279239416122, Total Loss 33.84163284301758\n",
      "10: Encoding Loss 3.8570034503936768, Transition Loss -1.9007055759429932, Classifier Loss 0.06466854363679886, Total Loss 29.60811424255371\n",
      "10: Encoding Loss 2.699923515319824, Transition Loss 0.5401394367218018, Classifier Loss 0.07335936278104782, Total Loss 23.75153160095215\n",
      "10: Encoding Loss 6.977541923522949, Transition Loss -2.538874387741089, Classifier Loss 0.09884427487850189, Total Loss 51.74866485595703\n",
      "10: Encoding Loss 4.158390998840332, Transition Loss -0.5812476873397827, Classifier Loss 0.19869881868362427, Total Loss 44.81999588012695\n",
      "10: Encoding Loss 6.594500541687012, Transition Loss -2.396533727645874, Classifier Loss 0.22246219217777252, Total Loss 61.8122673034668\n",
      "10: Encoding Loss 5.64805269241333, Transition Loss -1.2849457263946533, Classifier Loss 0.11641474813222885, Total Loss 45.52927780151367\n",
      "10: Encoding Loss 6.330124855041504, Transition Loss -0.505520224571228, Classifier Loss 0.10996055603027344, Total Loss 48.97660446166992\n",
      "10: Encoding Loss 6.2963643074035645, Transition Loss -0.10160267353057861, Classifier Loss 0.1037069708108902, Total Loss 48.148841857910156\n",
      "10: Encoding Loss 5.426495552062988, Transition Loss 0.359099805355072, Classifier Loss 0.05076556280255318, Total Loss 37.779170989990234\n",
      "10: Encoding Loss 5.117018222808838, Transition Loss -1.9854387044906616, Classifier Loss 0.0885220319032669, Total Loss 39.55352020263672\n",
      "10: Encoding Loss 3.569056749343872, Transition Loss -0.3486396074295044, Classifier Loss 0.06438907235860825, Total Loss 27.85310935974121\n",
      "10: Encoding Loss 7.295725345611572, Transition Loss -1.5685451030731201, Classifier Loss 0.16091080009937286, Total Loss 59.86480712890625\n",
      "10: Encoding Loss 3.510831356048584, Transition Loss -2.4220850467681885, Classifier Loss 0.064719557762146, Total Loss 27.53597640991211\n",
      "10: Encoding Loss 3.4413902759552, Transition Loss -1.391259789466858, Classifier Loss 0.15464933216571808, Total Loss 36.11271667480469\n",
      "10: Encoding Loss 4.840258598327637, Transition Loss -1.8386856317520142, Classifier Loss 0.0891648605465889, Total Loss 37.95730209350586\n",
      "10: Encoding Loss 4.436823844909668, Transition Loss 0.6664740443229675, Classifier Loss 0.05443704500794411, Total Loss 32.331241607666016\n",
      "10: Encoding Loss 6.169679641723633, Transition Loss -1.156844139099121, Classifier Loss 0.15042150020599365, Total Loss 52.05976486206055\n",
      "10: Encoding Loss 4.079780101776123, Transition Loss -1.4912232160568237, Classifier Loss 0.0720164105296135, Total Loss 31.679725646972656\n",
      "10: Encoding Loss 4.093563079833984, Transition Loss -1.3118445873260498, Classifier Loss 0.11856033653020859, Total Loss 36.41688537597656\n",
      "10: Encoding Loss 5.095636367797852, Transition Loss -1.4479215145111084, Classifier Loss 0.2298838347196579, Total Loss 53.561622619628906\n",
      "10: Encoding Loss 4.75537633895874, Transition Loss -1.3559458255767822, Classifier Loss 0.09941162168979645, Total Loss 38.472877502441406\n",
      "10: Encoding Loss 4.819358825683594, Transition Loss -0.32803773880004883, Classifier Loss 0.06180379539728165, Total Loss 35.096405029296875\n",
      "10: Encoding Loss 8.40186595916748, Transition Loss 0.0890321135520935, Classifier Loss 0.19573096930980682, Total Loss 70.01991271972656\n",
      "10: Encoding Loss 7.316684722900391, Transition Loss -1.679703712463379, Classifier Loss 0.11541157215833664, Total Loss 55.44059371948242\n",
      "10: Encoding Loss 5.295034408569336, Transition Loss -1.1622356176376343, Classifier Loss 0.13098891079425812, Total Loss 44.86863327026367\n",
      "10: Encoding Loss 6.157782554626465, Transition Loss -1.1994978189468384, Classifier Loss 0.12588545680046082, Total Loss 49.53476333618164\n",
      "10: Encoding Loss 5.668463706970215, Transition Loss -1.191401720046997, Classifier Loss 0.13647879660129547, Total Loss 47.65818786621094\n",
      "10: Encoding Loss 8.342656135559082, Transition Loss -2.4036784172058105, Classifier Loss 0.06774821132421494, Total Loss 56.82979965209961\n",
      "10: Encoding Loss 5.086116313934326, Transition Loss -1.879775047302246, Classifier Loss 0.11778921633958817, Total Loss 42.29486846923828\n",
      "10: Encoding Loss 3.4347481727600098, Transition Loss 0.11977434158325195, Classifier Loss 0.058174967765808105, Total Loss 26.473896026611328\n",
      "10: Encoding Loss 4.24611234664917, Transition Loss -1.0916638374328613, Classifier Loss 0.0981900617480278, Total Loss 35.29524612426758\n",
      "10: Encoding Loss 6.974206447601318, Transition Loss 0.032696619629859924, Classifier Loss 0.08398321270942688, Total Loss 50.25663757324219\n",
      "10: Encoding Loss 6.978255271911621, Transition Loss -0.32973966002464294, Classifier Loss 0.210442915558815, Total Loss 62.913692474365234\n",
      "10: Encoding Loss 4.0771708488464355, Transition Loss -0.29132336378097534, Classifier Loss 0.060764413326978683, Total Loss 30.539350509643555\n",
      "10: Encoding Loss 4.734255790710449, Transition Loss -1.6099810600280762, Classifier Loss 0.1399136632680893, Total Loss 42.39625930786133\n",
      "10: Encoding Loss 3.4248485565185547, Transition Loss -1.9118770360946655, Classifier Loss 0.03416920825839043, Total Loss 23.965248107910156\n",
      "10: Encoding Loss 7.639615058898926, Transition Loss -0.9915481805801392, Classifier Loss 0.08180400729179382, Total Loss 54.017696380615234\n",
      "10: Encoding Loss 4.942580223083496, Transition Loss 0.017995238304138184, Classifier Loss 0.15797625482082367, Total Loss 45.46030807495117\n",
      "10: Encoding Loss 2.8700740337371826, Transition Loss -1.9222019910812378, Classifier Loss 0.11411713063716888, Total Loss 28.631389617919922\n",
      "10: Encoding Loss 7.276886940002441, Transition Loss -0.2901449203491211, Classifier Loss 0.060180552303791046, Total Loss 49.679264068603516\n",
      "10: Encoding Loss 7.0618391036987305, Transition Loss -0.49632763862609863, Classifier Loss 0.15158282220363617, Total Loss 57.52912139892578\n",
      "10: Encoding Loss 3.818215847015381, Transition Loss -1.1248061656951904, Classifier Loss 0.05760987102985382, Total Loss 28.66983413696289\n",
      "10: Encoding Loss 10.247567176818848, Transition Loss -2.144134759902954, Classifier Loss 0.19012655317783356, Total Loss 80.49720764160156\n",
      "10: Encoding Loss 6.447991371154785, Transition Loss -1.6989169120788574, Classifier Loss 0.03940188139677048, Total Loss 42.62746047973633\n",
      "10: Encoding Loss 5.912721633911133, Transition Loss -0.8704264163970947, Classifier Loss 0.08695544302463531, Total Loss 44.17152786254883\n",
      "10: Encoding Loss 4.167330741882324, Transition Loss -2.396188259124756, Classifier Loss 0.07347571104764938, Total Loss 32.35060119628906\n",
      "10: Encoding Loss 3.9072585105895996, Transition Loss -2.4081218242645264, Classifier Loss 0.12126712501049042, Total Loss 35.56930160522461\n",
      "10: Encoding Loss 5.098452568054199, Transition Loss -4.322176456451416, Classifier Loss 0.09707537293434143, Total Loss 40.29652786254883\n",
      "10: Encoding Loss 4.694666385650635, Transition Loss -0.862313985824585, Classifier Loss 0.11619795858860016, Total Loss 39.787452697753906\n",
      "10: Encoding Loss 6.084179878234863, Transition Loss -2.0849227905273438, Classifier Loss 0.21403566002845764, Total Loss 57.90781021118164\n",
      "10: Encoding Loss 3.5573947429656982, Transition Loss -1.2494263648986816, Classifier Loss 0.21228085458278656, Total Loss 42.571956634521484\n",
      "10: Encoding Loss 5.545408725738525, Transition Loss -1.720383882522583, Classifier Loss 0.08116492629051208, Total Loss 41.38825988769531\n",
      "10: Encoding Loss 4.348913669586182, Transition Loss 0.029550164937973022, Classifier Loss 0.09535710513591766, Total Loss 35.641014099121094\n",
      "10: Encoding Loss 2.7867722511291504, Transition Loss -0.047804802656173706, Classifier Loss 0.08183690160512924, Total Loss 24.90430450439453\n",
      "10: Encoding Loss 4.752469062805176, Transition Loss -2.216705560684204, Classifier Loss 0.142570361495018, Total Loss 42.770965576171875\n",
      "10: Encoding Loss 2.803257465362549, Transition Loss -1.5001857280731201, Classifier Loss 0.11016881465911865, Total Loss 27.835826873779297\n",
      "10: Encoding Loss 2.251739263534546, Transition Loss -2.2077689170837402, Classifier Loss 0.09272628277540207, Total Loss 22.782180786132812\n",
      "10: Encoding Loss 7.050706386566162, Transition Loss -0.3437993824481964, Classifier Loss 0.04059438779950142, Total Loss 46.36354064941406\n",
      "10: Encoding Loss 4.261682033538818, Transition Loss -1.548272967338562, Classifier Loss 0.053678855299949646, Total Loss 30.937358856201172\n",
      "10: Encoding Loss 5.290084362030029, Transition Loss -1.8210605382919312, Classifier Loss 0.1558740735054016, Total Loss 47.327186584472656\n",
      "10: Encoding Loss 3.859609603881836, Transition Loss -1.9199576377868652, Classifier Loss 0.03841535747051239, Total Loss 26.998424530029297\n",
      "10: Encoding Loss 6.956366539001465, Transition Loss -1.5824449062347412, Classifier Loss 0.08686578273773193, Total Loss 50.42414855957031\n",
      "10: Encoding Loss 4.7020182609558105, Transition Loss -1.4586131572723389, Classifier Loss 0.14125925302505493, Total Loss 42.33745193481445\n",
      "10: Encoding Loss 2.788194179534912, Transition Loss -1.3070759773254395, Classifier Loss 0.06729087978601456, Total Loss 23.457731246948242\n",
      "10: Encoding Loss 6.786062717437744, Transition Loss -0.9562422037124634, Classifier Loss 0.06464125216007233, Total Loss 47.18012237548828\n",
      "10: Encoding Loss 5.392928123474121, Transition Loss -1.0534582138061523, Classifier Loss 0.18359847366809845, Total Loss 50.71699905395508\n",
      "10: Encoding Loss 3.5791428089141846, Transition Loss -0.19440412521362305, Classifier Loss 0.1568511724472046, Total Loss 37.15989685058594\n",
      "10: Encoding Loss 4.757441520690918, Transition Loss -1.0885062217712402, Classifier Loss 0.11521155387163162, Total Loss 40.065372467041016\n",
      "10: Encoding Loss 5.479428291320801, Transition Loss -1.2007503509521484, Classifier Loss 0.14264671504497528, Total Loss 47.14076232910156\n",
      "10: Encoding Loss 4.91418981552124, Transition Loss -2.5482120513916016, Classifier Loss 0.11101002246141434, Total Loss 40.58512496948242\n",
      "10: Encoding Loss 4.434859275817871, Transition Loss 0.9123802185058594, Classifier Loss 0.06870810687541962, Total Loss 33.84492111206055\n",
      "10: Encoding Loss 5.325512886047363, Transition Loss -1.6654725074768066, Classifier Loss 0.16782593727111816, Total Loss 48.73500442504883\n",
      "10: Encoding Loss 5.342954158782959, Transition Loss -2.536851406097412, Classifier Loss 0.0391051210463047, Total Loss 35.96722412109375\n",
      "10: Encoding Loss 4.470560550689697, Transition Loss -1.820909857749939, Classifier Loss 0.12782183289527893, Total Loss 39.604820251464844\n",
      "10: Encoding Loss 3.814972400665283, Transition Loss 0.8718396425247192, Classifier Loss 0.11636579781770706, Total Loss 34.87514877319336\n",
      "10: Encoding Loss 2.984405517578125, Transition Loss -1.3616254329681396, Classifier Loss 0.06800112873315811, Total Loss 24.70600128173828\n",
      "10: Encoding Loss 5.51869010925293, Transition Loss -1.866011381149292, Classifier Loss 0.07233083993196487, Total Loss 40.344478607177734\n",
      "10: Encoding Loss 4.50502347946167, Transition Loss 1.0843275785446167, Classifier Loss 0.08159653842449188, Total Loss 35.62352752685547\n",
      "10: Encoding Loss 3.6787314414978027, Transition Loss -1.8598241806030273, Classifier Loss 0.05642036348581314, Total Loss 27.713682174682617\n",
      "10: Encoding Loss 2.7440192699432373, Transition Loss -1.3669967651367188, Classifier Loss 0.10722365230321884, Total Loss 27.18593406677246\n",
      "10: Encoding Loss 3.5322463512420654, Transition Loss 0.2604910135269165, Classifier Loss 0.057729341089725494, Total Loss 27.07061004638672\n",
      "10: Encoding Loss 10.67070198059082, Transition Loss 0.210832417011261, Classifier Loss 0.2447497397661209, Total Loss 88.58352661132812\n",
      "10: Encoding Loss 6.565720558166504, Transition Loss -1.1951438188552856, Classifier Loss 0.09880083054304123, Total Loss 49.273929595947266\n",
      "10: Encoding Loss 7.876265525817871, Transition Loss -1.0656282901763916, Classifier Loss 0.15876874327659607, Total Loss 63.13404083251953\n",
      "10: Encoding Loss 6.224797248840332, Transition Loss -1.6909382343292236, Classifier Loss 0.0561797060072422, Total Loss 42.96607971191406\n",
      "10: Encoding Loss 5.272251129150391, Transition Loss -1.914674997329712, Classifier Loss 0.05452234297990799, Total Loss 37.08497619628906\n",
      "10: Encoding Loss 4.680027008056641, Transition Loss -0.43824130296707153, Classifier Loss 0.13779443502426147, Total Loss 41.859432220458984\n",
      "10: Encoding Loss 4.53421688079834, Transition Loss -2.207916498184204, Classifier Loss 0.09312563389539719, Total Loss 36.51698303222656\n",
      "10: Encoding Loss 2.9394278526306152, Transition Loss -0.17607581615447998, Classifier Loss 0.13285012543201447, Total Loss 30.921510696411133\n",
      "10: Encoding Loss 3.082258701324463, Transition Loss 0.8270900249481201, Classifier Loss 0.04100595414638519, Total Loss 22.924983978271484\n",
      "10: Encoding Loss 6.569733619689941, Transition Loss 0.0857086181640625, Classifier Loss 0.09726542234420776, Total Loss 49.179229736328125\n",
      "10: Encoding Loss 3.9878578186035156, Transition Loss -2.240046977996826, Classifier Loss 0.03717796504497528, Total Loss 27.644046783447266\n",
      "10: Encoding Loss 8.88542652130127, Transition Loss -2.1044387817382812, Classifier Loss 0.125373974442482, Total Loss 65.84912109375\n",
      "10: Encoding Loss 6.905364990234375, Transition Loss -1.1272791624069214, Classifier Loss 0.18449227511882782, Total Loss 59.88096618652344\n",
      "10: Encoding Loss 4.093284606933594, Transition Loss -1.667974591255188, Classifier Loss 0.06863725185394287, Total Loss 31.422767639160156\n",
      "10: Encoding Loss 6.233396053314209, Transition Loss -0.3800969123840332, Classifier Loss 0.09699059277772903, Total Loss 47.09928512573242\n",
      "10: Encoding Loss 6.561457633972168, Transition Loss -0.9681112170219421, Classifier Loss 0.07067596167325974, Total Loss 46.43595504760742\n",
      "10: Encoding Loss 5.746181011199951, Transition Loss -1.107383131980896, Classifier Loss 0.037244439125061035, Total Loss 38.20109176635742\n",
      "10: Encoding Loss 6.920362949371338, Transition Loss -1.3425757884979248, Classifier Loss 0.06510403007268906, Total Loss 48.03204345703125\n",
      "10: Encoding Loss 4.237873554229736, Transition Loss -2.633265733718872, Classifier Loss 0.0834536999464035, Total Loss 33.77156066894531\n",
      "10: Encoding Loss 4.121482849121094, Transition Loss -0.0014760792255401611, Classifier Loss 0.08009263128042221, Total Loss 32.738162994384766\n",
      "10: Encoding Loss 2.982107162475586, Transition Loss -0.9314432144165039, Classifier Loss 0.0562879703938961, Total Loss 23.521068572998047\n",
      "10: Encoding Loss 4.110949516296387, Transition Loss -1.2604832649230957, Classifier Loss 0.08268807828426361, Total Loss 32.93400192260742\n",
      "10: Encoding Loss 5.426505088806152, Transition Loss -0.23598739504814148, Classifier Loss 0.0980532169342041, Total Loss 42.3642578125\n",
      "10: Encoding Loss 6.230684757232666, Transition Loss -0.9047982096672058, Classifier Loss 0.1926453858613968, Total Loss 56.648284912109375\n",
      "10: Encoding Loss 4.74835729598999, Transition Loss -1.8609639406204224, Classifier Loss 0.13358615338802338, Total Loss 41.84801483154297\n",
      "10: Encoding Loss 6.699131965637207, Transition Loss -1.5451453924179077, Classifier Loss 0.09269001334905624, Total Loss 49.46317672729492\n",
      "10: Encoding Loss 4.461615562438965, Transition Loss -1.8845635652542114, Classifier Loss 0.06160192936658859, Total Loss 32.929134368896484\n",
      "10: Encoding Loss 2.715494394302368, Transition Loss -1.3721994161605835, Classifier Loss 0.058143191039562225, Total Loss 22.10673713684082\n",
      "10: Encoding Loss 5.811758995056152, Transition Loss -2.3419504165649414, Classifier Loss 0.06324135512113571, Total Loss 41.19375228881836\n",
      "10: Encoding Loss 6.915457725524902, Transition Loss -2.4301555156707764, Classifier Loss 0.1887369304895401, Total Loss 60.3654670715332\n",
      "10: Encoding Loss 4.95956563949585, Transition Loss -2.159912109375, Classifier Loss 0.04272441193461418, Total Loss 34.02897262573242\n",
      "10: Encoding Loss 3.0120632648468018, Transition Loss -1.146626353263855, Classifier Loss 0.04910479858517647, Total Loss 22.982402801513672\n",
      "10: Encoding Loss 2.4941670894622803, Transition Loss -3.647778034210205, Classifier Loss 0.054801635444164276, Total Loss 20.443708419799805\n",
      "10: Encoding Loss 1.388094186782837, Transition Loss -1.0279059410095215, Classifier Loss 0.13467241823673248, Total Loss 21.795394897460938\n",
      "10: Encoding Loss 5.718801021575928, Transition Loss -1.2588638067245483, Classifier Loss 0.07274115085601807, Total Loss 41.586421966552734\n",
      "10: Encoding Loss 5.300759315490723, Transition Loss -1.683760643005371, Classifier Loss 0.09412393718957901, Total Loss 41.21627426147461\n",
      "10: Encoding Loss 4.0577473640441895, Transition Loss -1.4671379327774048, Classifier Loss 0.05213356018066406, Total Loss 29.559253692626953\n",
      "10: Encoding Loss 6.221576690673828, Transition Loss -1.1880512237548828, Classifier Loss 0.10990094393491745, Total Loss 48.31907653808594\n",
      "10: Encoding Loss 6.119782447814941, Transition Loss -1.1277453899383545, Classifier Loss 0.17904408276081085, Total Loss 54.62265396118164\n",
      "10: Encoding Loss 4.724295616149902, Transition Loss 0.13062524795532227, Classifier Loss 0.06621195375919342, Total Loss 35.019222259521484\n",
      "10: Encoding Loss 5.2630815505981445, Transition Loss 0.21103662252426147, Classifier Loss 0.09822310507297516, Total Loss 41.48521423339844\n",
      "10: Encoding Loss 3.6227874755859375, Transition Loss -1.783184289932251, Classifier Loss 0.05140405148267746, Total Loss 26.87641716003418\n",
      "10: Encoding Loss 7.865476131439209, Transition Loss 0.13579726219177246, Classifier Loss 0.0543772429227829, Total Loss 52.68490219116211\n",
      "10: Encoding Loss 5.795459747314453, Transition Loss -1.5002785921096802, Classifier Loss 0.05334092304110527, Total Loss 40.10625076293945\n",
      "10: Encoding Loss 6.323845386505127, Transition Loss -0.5099591612815857, Classifier Loss 0.10121947526931763, Total Loss 48.0648193359375\n",
      "10: Encoding Loss 5.29539680480957, Transition Loss -1.0110245943069458, Classifier Loss 0.045861389487981796, Total Loss 36.358116149902344\n",
      "10: Encoding Loss 5.188544273376465, Transition Loss -0.1339636743068695, Classifier Loss 0.07413732260465622, Total Loss 38.544944763183594\n",
      "10: Encoding Loss 3.589745283126831, Transition Loss -1.2413331270217896, Classifier Loss 0.07631949335336685, Total Loss 29.169925689697266\n",
      "10: Encoding Loss 5.2292962074279785, Transition Loss -0.8720703125, Classifier Loss 0.18306846916675568, Total Loss 49.68227767944336\n",
      "10: Encoding Loss 2.8287100791931152, Transition Loss -1.6623096466064453, Classifier Loss 0.0541301891207695, Total Loss 22.384614944458008\n",
      "10: Encoding Loss 6.536379814147949, Transition Loss -1.2362260818481445, Classifier Loss 0.2842541038990021, Total Loss 67.64319610595703\n",
      "10: Encoding Loss 6.147857189178467, Transition Loss -0.6630118489265442, Classifier Loss 0.2036561816930771, Total Loss 57.25249481201172\n",
      "10: Encoding Loss 3.8899314403533936, Transition Loss -1.5813027620315552, Classifier Loss 0.1288793683052063, Total Loss 36.22689437866211\n",
      "10: Encoding Loss 5.629611968994141, Transition Loss -2.0137031078338623, Classifier Loss 0.06933862715959549, Total Loss 40.71072769165039\n",
      "10: Encoding Loss 6.643773555755615, Transition Loss -0.29565614461898804, Classifier Loss 0.08639597147703171, Total Loss 48.50212478637695\n",
      "10: Encoding Loss 7.31235408782959, Transition Loss -0.6703650951385498, Classifier Loss 0.16850143671035767, Total Loss 60.724002838134766\n",
      "10: Encoding Loss 4.958782196044922, Transition Loss -1.9349972009658813, Classifier Loss 0.07136408984661102, Total Loss 36.888328552246094\n",
      "10: Encoding Loss 4.529719352722168, Transition Loss -3.3974881172180176, Classifier Loss 0.07837987691164017, Total Loss 35.01494598388672\n",
      "10: Encoding Loss 7.114232063293457, Transition Loss -2.208305835723877, Classifier Loss 0.161643847823143, Total Loss 58.84889221191406\n",
      "10: Encoding Loss 5.702906608581543, Transition Loss -0.8225551247596741, Classifier Loss 0.07015486806631088, Total Loss 41.232601165771484\n",
      "10: Encoding Loss 3.748222827911377, Transition Loss -2.4246089458465576, Classifier Loss 0.08946290612220764, Total Loss 31.434659957885742\n",
      "10: Encoding Loss 2.5233089923858643, Transition Loss -0.9240211248397827, Classifier Loss 0.07939494401216507, Total Loss 23.0789794921875\n",
      "10: Encoding Loss 7.456128120422363, Transition Loss -2.0762672424316406, Classifier Loss 0.12563645839691162, Total Loss 57.299583435058594\n",
      "10: Encoding Loss 4.710484504699707, Transition Loss 0.013727322220802307, Classifier Loss 0.0858488529920578, Total Loss 36.8532829284668\n",
      "10: Encoding Loss 5.87556266784668, Transition Loss -0.6964148879051208, Classifier Loss 0.06764940917491913, Total Loss 42.01803970336914\n",
      "10: Encoding Loss 3.7614026069641113, Transition Loss -0.993475615978241, Classifier Loss 0.043573491275310516, Total Loss 26.925369262695312\n",
      "10: Encoding Loss 4.05439567565918, Transition Loss -1.482857346534729, Classifier Loss 0.07007909566164017, Total Loss 31.33369255065918\n",
      "10: Encoding Loss 7.733091831207275, Transition Loss 0.16429901123046875, Classifier Loss 0.16219007968902588, Total Loss 62.68328094482422\n",
      "10: Encoding Loss 5.646048545837402, Transition Loss -1.41225266456604, Classifier Loss 0.16145451366901398, Total Loss 50.02117919921875\n",
      "10: Encoding Loss 5.4010910987854, Transition Loss -3.1289241313934326, Classifier Loss 0.1505945473909378, Total Loss 47.464752197265625\n",
      "10: Encoding Loss 3.612902879714966, Transition Loss -2.047978162765503, Classifier Loss 0.062186673283576965, Total Loss 27.895267486572266\n",
      "10: Encoding Loss 7.524034023284912, Transition Loss -1.887894630432129, Classifier Loss 0.13557182252407074, Total Loss 58.70063400268555\n",
      "10: Encoding Loss 5.299639701843262, Transition Loss -0.7241053581237793, Classifier Loss 0.05101511627435684, Total Loss 36.89906311035156\n",
      "10: Encoding Loss 4.606559753417969, Transition Loss -1.4688825607299805, Classifier Loss 0.07003147155046463, Total Loss 34.64192199707031\n",
      "10: Encoding Loss 3.9177260398864746, Transition Loss -1.9572234153747559, Classifier Loss 0.0411238893866539, Total Loss 27.617963790893555\n",
      "10: Encoding Loss 5.157322883605957, Transition Loss -1.3066133260726929, Classifier Loss 0.1729608029127121, Total Loss 48.23949432373047\n",
      "10: Encoding Loss 5.98657751083374, Transition Loss -3.2302310466766357, Classifier Loss 0.11255371570587158, Total Loss 47.173545837402344\n",
      "10: Encoding Loss 4.134852409362793, Transition Loss -0.8005837798118591, Classifier Loss 0.09268543869256973, Total Loss 34.07733917236328\n",
      "10: Encoding Loss 6.626513481140137, Transition Loss -0.11319850385189056, Classifier Loss 0.14725488424301147, Total Loss 54.484527587890625\n",
      "10: Encoding Loss 6.826493740081787, Transition Loss -1.4108633995056152, Classifier Loss 0.13591767847537994, Total Loss 54.5501708984375\n",
      "10: Encoding Loss 5.382317066192627, Transition Loss -2.660588026046753, Classifier Loss 0.09309244155883789, Total Loss 41.60208511352539\n",
      "10: Encoding Loss 6.315881252288818, Transition Loss -1.149520754814148, Classifier Loss 0.04562092944979668, Total Loss 42.4569206237793\n",
      "10: Encoding Loss 5.175673007965088, Transition Loss 0.43390923738479614, Classifier Loss 0.0950513705611229, Total Loss 40.73274230957031\n",
      "10: Encoding Loss 5.459481239318848, Transition Loss -0.6887068748474121, Classifier Loss 0.12664736807346344, Total Loss 45.42135238647461\n",
      "10: Encoding Loss 7.1540422439575195, Transition Loss -0.8010467290878296, Classifier Loss 0.050580549985170364, Total Loss 47.981990814208984\n",
      "10: Encoding Loss 5.070399284362793, Transition Loss -2.2639145851135254, Classifier Loss 0.051657192409038544, Total Loss 35.58721160888672\n",
      "10: Encoding Loss 7.041651725769043, Transition Loss -1.5546784400939941, Classifier Loss 0.14834879338741302, Total Loss 57.084171295166016\n",
      "10: Encoding Loss 5.170924186706543, Transition Loss -0.9616625905036926, Classifier Loss 0.18521730601787567, Total Loss 49.54689407348633\n",
      "10: Encoding Loss 5.446164608001709, Transition Loss -2.6869802474975586, Classifier Loss 0.10857807099819183, Total Loss 43.533721923828125\n",
      "10: Encoding Loss 3.4144797325134277, Transition Loss 0.45346903800964355, Classifier Loss 0.09251146763563156, Total Loss 29.91941261291504\n",
      "10: Encoding Loss 3.407982349395752, Transition Loss -0.6112343072891235, Classifier Loss 0.06656405329704285, Total Loss 27.10405731201172\n",
      "10: Encoding Loss 2.456958770751953, Transition Loss -1.1083219051361084, Classifier Loss 0.06992536783218384, Total Loss 21.733848571777344\n",
      "10: Encoding Loss 7.446076393127441, Transition Loss -2.1253015995025635, Classifier Loss 0.1436535269021988, Total Loss 59.04096221923828\n",
      "10: Encoding Loss 4.6131181716918945, Transition Loss -1.5040674209594727, Classifier Loss 0.10045506805181503, Total Loss 37.72361373901367\n",
      "10: Encoding Loss 6.412503242492676, Transition Loss -1.606770634651184, Classifier Loss 0.09643273800611496, Total Loss 48.117652893066406\n",
      "10: Encoding Loss 4.501884460449219, Transition Loss -1.1140003204345703, Classifier Loss 0.1148582249879837, Total Loss 38.49668502807617\n",
      "10: Encoding Loss 4.50050687789917, Transition Loss -1.8223118782043457, Classifier Loss 0.11714711040258408, Total Loss 38.71702575683594\n",
      "10: Encoding Loss 4.145504474639893, Transition Loss -1.795437216758728, Classifier Loss 0.14144843816757202, Total Loss 39.017154693603516\n",
      "10: Encoding Loss 4.407477378845215, Transition Loss -1.420449137687683, Classifier Loss 0.10337062925100327, Total Loss 36.7813606262207\n",
      "10: Encoding Loss 6.565522193908691, Transition Loss -1.099216341972351, Classifier Loss 0.11613814532756805, Total Loss 51.00651168823242\n",
      "10: Encoding Loss 4.976131439208984, Transition Loss -0.7322593927383423, Classifier Loss 0.22226762771606445, Total Loss 52.08325958251953\n",
      "10: Encoding Loss 7.038029670715332, Transition Loss -1.8475356101989746, Classifier Loss 0.09242837876081467, Total Loss 51.47027587890625\n",
      "10: Encoding Loss 4.549687385559082, Transition Loss -1.9197038412094116, Classifier Loss 0.07415486872196198, Total Loss 34.71284484863281\n",
      "10: Encoding Loss 6.08607816696167, Transition Loss -1.5490339994430542, Classifier Loss 0.06992612034082413, Total Loss 43.50846481323242\n",
      "10: Encoding Loss 4.330687046051025, Transition Loss -0.9505337476730347, Classifier Loss 0.07815168052911758, Total Loss 33.79890823364258\n",
      "10: Encoding Loss 4.485142707824707, Transition Loss -0.9218875765800476, Classifier Loss 0.05414583534002304, Total Loss 32.325069427490234\n",
      "10: Encoding Loss 3.134267568588257, Transition Loss -0.4138679504394531, Classifier Loss 0.06946348398923874, Total Loss 25.751789093017578\n",
      "10: Encoding Loss 5.401485443115234, Transition Loss -1.0132259130477905, Classifier Loss 0.07167080044746399, Total Loss 39.57558822631836\n",
      "10: Encoding Loss 5.747369766235352, Transition Loss -0.7326995730400085, Classifier Loss 0.1681077480316162, Total Loss 51.294700622558594\n",
      "10: Encoding Loss 3.500609874725342, Transition Loss -1.5231150388717651, Classifier Loss 0.16290606558322906, Total Loss 37.29365539550781\n",
      "10: Encoding Loss 4.058253288269043, Transition Loss -0.037117183208465576, Classifier Loss 0.10829775780439377, Total Loss 35.179283142089844\n",
      "10: Encoding Loss 6.953457832336426, Transition Loss -1.2917732000350952, Classifier Loss 0.1908474862575531, Total Loss 60.80498123168945\n",
      "10: Encoding Loss 5.98272180557251, Transition Loss -0.3100866377353668, Classifier Loss 0.08899770677089691, Total Loss 44.79597854614258\n",
      "10: Encoding Loss 5.078826427459717, Transition Loss -0.8405144810676575, Classifier Loss 0.045789796859025955, Total Loss 35.051605224609375\n",
      "10: Encoding Loss 6.671940803527832, Transition Loss -0.4610363245010376, Classifier Loss 0.11681229621171951, Total Loss 51.71269226074219\n",
      "10: Encoding Loss 4.6719841957092285, Transition Loss -3.1140851974487305, Classifier Loss 0.11751210689544678, Total Loss 39.7818717956543\n",
      "10: Encoding Loss 3.313512086868286, Transition Loss 1.1187318563461304, Classifier Loss 0.06868062913417816, Total Loss 27.19662857055664\n",
      "10: Encoding Loss 3.523449659347534, Transition Loss -2.3805289268493652, Classifier Loss 0.07309172302484512, Total Loss 28.44891929626465\n",
      "10: Encoding Loss 5.933559417724609, Transition Loss -2.1467747688293457, Classifier Loss 0.03935530409216881, Total Loss 39.53602981567383\n",
      "10: Encoding Loss 6.6566009521484375, Transition Loss -1.6711676120758057, Classifier Loss 0.06551459431648254, Total Loss 46.49039840698242\n",
      "10: Encoding Loss 6.149653434753418, Transition Loss -1.2043240070343018, Classifier Loss 0.05567018687725067, Total Loss 42.46446228027344\n",
      "10: Encoding Loss 4.634243965148926, Transition Loss -0.8076668381690979, Classifier Loss 0.10180632770061493, Total Loss 37.98577117919922\n",
      "10: Encoding Loss 4.914953708648682, Transition Loss -0.7470905780792236, Classifier Loss 0.044353634119033813, Total Loss 33.92478942871094\n",
      "10: Encoding Loss 3.011023998260498, Transition Loss -0.7513474822044373, Classifier Loss 0.0893668383359909, Total Loss 27.002527236938477\n",
      "10: Encoding Loss 6.65817928314209, Transition Loss -2.425313711166382, Classifier Loss 0.0667392835021019, Total Loss 46.62203598022461\n",
      "10: Encoding Loss 4.8367719650268555, Transition Loss -1.453959345817566, Classifier Loss 0.05285818129777908, Total Loss 34.305870056152344\n",
      "10: Encoding Loss 6.181820869445801, Transition Loss -1.6319212913513184, Classifier Loss 0.15653835237026215, Total Loss 52.744110107421875\n",
      "10: Encoding Loss 4.235770225524902, Transition Loss -1.4704958200454712, Classifier Loss 0.023605911061167717, Total Loss 27.774627685546875\n",
      "10: Encoding Loss 5.690141201019287, Transition Loss -0.20744016766548157, Classifier Loss 0.07006430625915527, Total Loss 41.14719772338867\n",
      "10: Encoding Loss 5.034934043884277, Transition Loss -1.3639492988586426, Classifier Loss 0.12712536752223969, Total Loss 42.92159652709961\n",
      "10: Encoding Loss 4.414887428283691, Transition Loss -1.4184529781341553, Classifier Loss 0.07130077481269836, Total Loss 33.618831634521484\n",
      "10: Encoding Loss 2.81192684173584, Transition Loss -2.452500343322754, Classifier Loss 0.06633444130420685, Total Loss 23.504024505615234\n",
      "10: Encoding Loss 5.368198871612549, Transition Loss -3.4563119411468506, Classifier Loss 0.05718926340341568, Total Loss 37.92673873901367\n",
      "10: Encoding Loss 3.3894264698028564, Transition Loss -0.6379095315933228, Classifier Loss 0.13786116242408752, Total Loss 34.12242126464844\n",
      "10: Encoding Loss 4.946686267852783, Transition Loss -0.8827798962593079, Classifier Loss 0.15478789806365967, Total Loss 45.15855407714844\n",
      "10: Encoding Loss 5.526667594909668, Transition Loss -0.5438433289527893, Classifier Loss 0.10250717401504517, Total Loss 43.41050720214844\n",
      "10: Encoding Loss 3.9966061115264893, Transition Loss -0.39206039905548096, Classifier Loss 0.04522363096475601, Total Loss 28.50184440612793\n",
      "10: Encoding Loss 8.40585708618164, Transition Loss -0.13485220074653625, Classifier Loss 0.1266947239637375, Total Loss 63.10456466674805\n",
      "10: Encoding Loss 4.471554756164551, Transition Loss -0.975480318069458, Classifier Loss 0.08332375437021255, Total Loss 35.16131591796875\n",
      "10: Encoding Loss 7.803431510925293, Transition Loss -0.39801207184791565, Classifier Loss 0.1924731284379959, Total Loss 66.06774139404297\n",
      "10: Encoding Loss 5.6958770751953125, Transition Loss -2.730592727661133, Classifier Loss 0.16661131381988525, Total Loss 50.835304260253906\n",
      "10: Encoding Loss 5.795568943023682, Transition Loss -1.0697908401489258, Classifier Loss 0.10780347883701324, Total Loss 45.55333709716797\n",
      "10: Encoding Loss 5.604947566986084, Transition Loss -1.8667277097702026, Classifier Loss 0.14971661567687988, Total Loss 48.60060119628906\n",
      "10: Encoding Loss 3.2971997261047363, Transition Loss -1.3022172451019287, Classifier Loss 0.0762481689453125, Total Loss 27.407495498657227\n",
      "10: Encoding Loss 3.7086150646209717, Transition Loss -0.39781510829925537, Classifier Loss 0.07919653505086899, Total Loss 30.171186447143555\n",
      "10: Encoding Loss 4.363582611083984, Transition Loss -0.7905409932136536, Classifier Loss 0.09100591391324997, Total Loss 35.28177261352539\n",
      "10: Encoding Loss 4.464808464050293, Transition Loss -2.77939510345459, Classifier Loss 0.07282468676567078, Total Loss 34.07020950317383\n",
      "10: Encoding Loss 5.547658920288086, Transition Loss -1.5617868900299072, Classifier Loss 0.123952716588974, Total Loss 45.680599212646484\n",
      "10: Encoding Loss 4.9606194496154785, Transition Loss -1.4078574180603027, Classifier Loss 0.08907277882099152, Total Loss 38.670433044433594\n",
      "10: Encoding Loss 3.5181422233581543, Transition Loss -0.3328050374984741, Classifier Loss 0.09022323787212372, Total Loss 30.131044387817383\n",
      "10: Encoding Loss 4.171116352081299, Transition Loss -1.6279211044311523, Classifier Loss 0.04843367636203766, Total Loss 29.869417190551758\n",
      "10: Encoding Loss 5.865015506744385, Transition Loss -1.2383341789245605, Classifier Loss 0.12645268440246582, Total Loss 47.83486557006836\n",
      "10: Encoding Loss 5.0656867027282715, Transition Loss 0.16890060901641846, Classifier Loss 0.05624166876077652, Total Loss 36.08584976196289\n",
      "10: Encoding Loss 6.676518440246582, Transition Loss 0.12360444664955139, Classifier Loss 0.08896050602197647, Total Loss 49.00460433959961\n",
      "10: Encoding Loss 6.458296775817871, Transition Loss -1.7464630603790283, Classifier Loss 0.2742854952812195, Total Loss 66.17762756347656\n",
      "10: Encoding Loss 6.094947338104248, Transition Loss -1.6989604234695435, Classifier Loss 0.05289960652589798, Total Loss 41.858970642089844\n",
      "10: Encoding Loss 5.135100841522217, Transition Loss -1.3172866106033325, Classifier Loss 0.10646557807922363, Total Loss 41.45663833618164\n",
      "10: Encoding Loss 4.203738212585449, Transition Loss -1.2583990097045898, Classifier Loss 0.08131660521030426, Total Loss 33.35358810424805\n",
      "10: Encoding Loss 5.666382789611816, Transition Loss -2.117403030395508, Classifier Loss 0.18704256415367126, Total Loss 52.70170593261719\n",
      "10: Encoding Loss 3.6306967735290527, Transition Loss -0.9892194271087646, Classifier Loss 0.06136166304349899, Total Loss 27.919952392578125\n",
      "10: Encoding Loss 6.987783908843994, Transition Loss -2.155442714691162, Classifier Loss 0.11180007457733154, Total Loss 53.10585021972656\n",
      "10: Encoding Loss 5.5902485847473145, Transition Loss -1.460723876953125, Classifier Loss 0.1464848667383194, Total Loss 48.189395904541016\n",
      "10: Encoding Loss 5.964442729949951, Transition Loss -1.8619790077209473, Classifier Loss 0.050487592816352844, Total Loss 40.83467483520508\n",
      "10: Encoding Loss 6.797904014587402, Transition Loss -0.8479838967323303, Classifier Loss 0.13700304925441742, Total Loss 54.48739242553711\n",
      "10: Encoding Loss 2.500985622406006, Transition Loss -1.9339203834533691, Classifier Loss 0.0648321732878685, Total Loss 21.488357543945312\n",
      "10: Encoding Loss 2.7124199867248535, Transition Loss -0.6071525812149048, Classifier Loss 0.050596192479133606, Total Loss 21.333898544311523\n",
      "10: Encoding Loss 5.967413425445557, Transition Loss -0.4510115683078766, Classifier Loss 0.09678911417722702, Total Loss 45.483211517333984\n",
      "10: Encoding Loss 3.852644443511963, Transition Loss 0.5091482400894165, Classifier Loss 0.07428032159805298, Total Loss 30.74755859375\n",
      "10: Encoding Loss 5.9104132652282715, Transition Loss -0.944957435131073, Classifier Loss 0.15751506388187408, Total Loss 51.2136116027832\n",
      "10: Encoding Loss 5.526674747467041, Transition Loss -1.897748351097107, Classifier Loss 0.059256747364997864, Total Loss 39.084964752197266\n",
      "10: Encoding Loss 3.7609989643096924, Transition Loss -2.587209939956665, Classifier Loss 0.10931502282619476, Total Loss 33.496463775634766\n",
      "10: Encoding Loss 3.137021780014038, Transition Loss -1.5069146156311035, Classifier Loss 0.06820113956928253, Total Loss 25.641643524169922\n",
      "10: Encoding Loss 7.459916114807129, Transition Loss -1.4230420589447021, Classifier Loss 0.16732238233089447, Total Loss 61.49116897583008\n",
      "10: Encoding Loss 4.151015281677246, Transition Loss -0.11165213584899902, Classifier Loss 0.1628694087266922, Total Loss 41.192989349365234\n",
      "10: Encoding Loss 4.722557544708252, Transition Loss -1.4447799921035767, Classifier Loss 0.05325436219573021, Total Loss 33.66020584106445\n",
      "10: Encoding Loss 5.378201484680176, Transition Loss 0.16632118821144104, Classifier Loss 0.12998619675636292, Total Loss 45.33435821533203\n",
      "10: Encoding Loss 3.0687308311462402, Transition Loss 0.6482914686203003, Classifier Loss 0.05599655956029892, Total Loss 24.271360397338867\n",
      "10: Encoding Loss 3.7312564849853516, Transition Loss -1.4730377197265625, Classifier Loss 0.08132723718881607, Total Loss 30.51967430114746\n",
      "10: Encoding Loss 6.032987117767334, Transition Loss -0.8591381311416626, Classifier Loss 0.0936475321650505, Total Loss 45.56233596801758\n",
      "10: Encoding Loss 3.5882580280303955, Transition Loss -0.9188793897628784, Classifier Loss 0.11214804649353027, Total Loss 32.743988037109375\n",
      "10: Encoding Loss 6.4007062911987305, Transition Loss -0.9725490808486938, Classifier Loss 0.13569530844688416, Total Loss 51.97338104248047\n",
      "10: Encoding Loss 6.538538932800293, Transition Loss -1.3378010988235474, Classifier Loss 0.17894893884658813, Total Loss 57.12559509277344\n",
      "10: Encoding Loss 4.639101505279541, Transition Loss -1.1707972288131714, Classifier Loss 0.03303025662899017, Total Loss 31.13716697692871\n",
      "10: Encoding Loss 4.533617973327637, Transition Loss -0.40500369668006897, Classifier Loss 0.1573215126991272, Total Loss 42.93370056152344\n",
      "10: Encoding Loss 5.788113594055176, Transition Loss -3.388479709625244, Classifier Loss 0.1611447036266327, Total Loss 50.841800689697266\n",
      "10: Encoding Loss 6.737972259521484, Transition Loss -1.6033265590667725, Classifier Loss 0.05114267021417618, Total Loss 45.54145812988281\n",
      "10: Encoding Loss 5.0338921546936035, Transition Loss -1.2786178588867188, Classifier Loss 0.06723689287900925, Total Loss 36.92653274536133\n",
      "10: Encoding Loss 6.141648292541504, Transition Loss -2.2835946083068848, Classifier Loss 0.13686946034431458, Total Loss 50.535926818847656\n",
      "10: Encoding Loss 5.200016498565674, Transition Loss -0.41503793001174927, Classifier Loss 0.061669543385505676, Total Loss 37.366886138916016\n",
      "10: Encoding Loss 2.943885087966919, Transition Loss -1.5463266372680664, Classifier Loss 0.07386185973882675, Total Loss 25.048879623413086\n",
      "10: Encoding Loss 5.273459434509277, Transition Loss -1.922673225402832, Classifier Loss 0.11473610997200012, Total Loss 43.11359786987305\n",
      "10: Encoding Loss 9.186941146850586, Transition Loss -3.2736940383911133, Classifier Loss 0.16381298005580902, Total Loss 71.50163269042969\n",
      "10: Encoding Loss 5.1662468910217285, Transition Loss -1.3815770149230957, Classifier Loss 0.04880629852414131, Total Loss 35.877559661865234\n",
      "10: Encoding Loss 5.3604936599731445, Transition Loss -0.9856570959091187, Classifier Loss 0.044250089675188065, Total Loss 36.587581634521484\n",
      "10: Encoding Loss 6.037306308746338, Transition Loss -0.19205591082572937, Classifier Loss 0.1551273614168167, Total Loss 51.73649978637695\n",
      "10: Encoding Loss 5.800014972686768, Transition Loss -0.8349440097808838, Classifier Loss 0.2696661353111267, Total Loss 61.7663688659668\n",
      "10: Encoding Loss 6.164775848388672, Transition Loss -1.5608296394348145, Classifier Loss 0.1793784201145172, Total Loss 54.925872802734375\n",
      "10: Encoding Loss 6.348633766174316, Transition Loss -2.1496987342834473, Classifier Loss 0.15137161314487457, Total Loss 53.22810745239258\n",
      "10: Encoding Loss 5.172905445098877, Transition Loss -1.4167304039001465, Classifier Loss 0.11288821697235107, Total Loss 42.325687408447266\n",
      "10: Encoding Loss 5.1546430587768555, Transition Loss -0.49585258960723877, Classifier Loss 0.13677528500556946, Total Loss 44.60519027709961\n",
      "10: Encoding Loss 6.876431465148926, Transition Loss -0.7440172433853149, Classifier Loss 0.17322923243045807, Total Loss 58.581214904785156\n",
      "10: Encoding Loss 4.716776371002197, Transition Loss -0.5283514857292175, Classifier Loss 0.05752082169055939, Total Loss 34.05253219604492\n",
      "10: Encoding Loss 7.569207191467285, Transition Loss -2.038079023361206, Classifier Loss 0.0687548890709877, Total Loss 52.2899169921875\n",
      "10: Encoding Loss 4.6845479011535645, Transition Loss -0.9790202975273132, Classifier Loss 0.10666843503713608, Total Loss 38.773738861083984\n",
      "10: Encoding Loss 4.121376991271973, Transition Loss -1.6899158954620361, Classifier Loss 0.09436716139316559, Total Loss 34.164302825927734\n",
      "10: Encoding Loss 5.539092063903809, Transition Loss -0.4110689163208008, Classifier Loss 0.16968652606010437, Total Loss 50.20304489135742\n",
      "10: Encoding Loss 4.915902137756348, Transition Loss -1.4075021743774414, Classifier Loss 0.07689952850341797, Total Loss 37.18479919433594\n",
      "10: Encoding Loss 5.012969970703125, Transition Loss -1.445099949836731, Classifier Loss 0.14357773959636688, Total Loss 44.43501663208008\n",
      "10: Encoding Loss 10.674467086791992, Transition Loss -2.178804636001587, Classifier Loss 0.13076522946357727, Total Loss 77.1224594116211\n",
      "10: Encoding Loss 5.768649101257324, Transition Loss -0.6803325414657593, Classifier Loss 0.062225341796875, Total Loss 40.83415985107422\n",
      "10: Encoding Loss 6.626711845397949, Transition Loss 0.8019982576370239, Classifier Loss 0.06523415446281433, Total Loss 46.604488372802734\n",
      "10: Encoding Loss 4.480356216430664, Transition Loss -2.249051570892334, Classifier Loss 0.10681332647800446, Total Loss 37.56257247924805\n",
      "10: Encoding Loss 6.0015177726745605, Transition Loss -1.5031286478042603, Classifier Loss 0.1140005886554718, Total Loss 47.408565521240234\n",
      "10: Encoding Loss 3.6719489097595215, Transition Loss -0.5145928859710693, Classifier Loss 0.08561376482248306, Total Loss 30.592864990234375\n",
      "10: Encoding Loss 5.529705047607422, Transition Loss -1.3904107809066772, Classifier Loss 0.18537160754203796, Total Loss 51.71483612060547\n",
      "10: Encoding Loss 9.136201858520508, Transition Loss -1.3065953254699707, Classifier Loss 0.11813626438379288, Total Loss 66.63031005859375\n",
      "10: Encoding Loss 4.627335071563721, Transition Loss -1.7790049314498901, Classifier Loss 0.09111788868904114, Total Loss 36.87508773803711\n",
      "10: Encoding Loss 5.481204509735107, Transition Loss -1.0782002210617065, Classifier Loss 0.07293462008237839, Total Loss 40.180259704589844\n",
      "10: Encoding Loss 5.790163993835449, Transition Loss -2.083608627319336, Classifier Loss 0.04635799676179886, Total Loss 39.375953674316406\n",
      "10: Encoding Loss 5.1917314529418945, Transition Loss -2.194186210632324, Classifier Loss 0.049745235592126846, Total Loss 36.1240348815918\n",
      "10: Encoding Loss 4.444439888000488, Transition Loss -1.065987467765808, Classifier Loss 0.031886886805295944, Total Loss 29.854900360107422\n",
      "10: Encoding Loss 4.139458656311035, Transition Loss -2.480494499206543, Classifier Loss 0.1001044362783432, Total Loss 34.8462028503418\n",
      "10: Encoding Loss 6.200109004974365, Transition Loss -0.5730080008506775, Classifier Loss 0.10588118433952332, Total Loss 47.78854751586914\n",
      "10: Encoding Loss 5.508525848388672, Transition Loss -0.7324461936950684, Classifier Loss 0.06061294302344322, Total Loss 39.11215591430664\n",
      "10: Encoding Loss 5.349401950836182, Transition Loss -3.0905117988586426, Classifier Loss 0.10636822134256363, Total Loss 42.731998443603516\n",
      "10: Encoding Loss 6.932867527008057, Transition Loss -3.101341724395752, Classifier Loss 0.11835484951734543, Total Loss 53.43144989013672\n",
      "10: Encoding Loss 4.569730281829834, Transition Loss 0.0940520167350769, Classifier Loss 0.06493576616048813, Total Loss 33.949581146240234\n",
      "10: Encoding Loss 4.669950008392334, Transition Loss -1.9005513191223145, Classifier Loss 0.14159716665744781, Total Loss 42.17865753173828\n",
      "10: Encoding Loss 5.099679470062256, Transition Loss -1.3635010719299316, Classifier Loss 0.0746602937579155, Total Loss 38.063560485839844\n",
      "10: Encoding Loss 3.5844032764434814, Transition Loss -1.8955039978027344, Classifier Loss 0.07115007191896439, Total Loss 28.620668411254883\n",
      "10: Encoding Loss 4.993643283843994, Transition Loss -2.79579758644104, Classifier Loss 0.14332975447177887, Total Loss 44.29372024536133\n",
      "10: Encoding Loss 6.509763240814209, Transition Loss -2.175959348678589, Classifier Loss 0.09259240329265594, Total Loss 48.316951751708984\n",
      "10: Encoding Loss 6.377307891845703, Transition Loss -2.848220109939575, Classifier Loss 0.1034892275929451, Total Loss 48.611629486083984\n",
      "10: Encoding Loss 7.242030143737793, Transition Loss -1.0862481594085693, Classifier Loss 0.158402219414711, Total Loss 59.291969299316406\n",
      "10: Encoding Loss 5.037942886352539, Transition Loss 0.3842736780643463, Classifier Loss 0.0950431078672409, Total Loss 39.88568115234375\n",
      "10: Encoding Loss 4.890920639038086, Transition Loss -2.6350882053375244, Classifier Loss 0.07284730672836304, Total Loss 36.62920379638672\n",
      "10: Encoding Loss 5.035844326019287, Transition Loss -1.8718544244766235, Classifier Loss 0.11299895495176315, Total Loss 41.51421356201172\n",
      "10: Encoding Loss 4.348124980926514, Transition Loss -1.704360008239746, Classifier Loss 0.07654285430908203, Total Loss 33.74235153198242\n",
      "10: Encoding Loss 4.149618148803711, Transition Loss -1.9823625087738037, Classifier Loss 0.08987535536289215, Total Loss 33.88445281982422\n",
      "10: Encoding Loss 3.526052474975586, Transition Loss -2.225834369659424, Classifier Loss 0.050676990300416946, Total Loss 26.22312355041504\n",
      "10: Encoding Loss 3.50978422164917, Transition Loss -0.6858444213867188, Classifier Loss 0.09940890967845917, Total Loss 30.99932289123535\n",
      "10: Encoding Loss 5.1032209396362305, Transition Loss -0.9150308966636658, Classifier Loss 0.11505924165248871, Total Loss 42.12488555908203\n",
      "10: Encoding Loss 6.1838579177856445, Transition Loss -0.3549312353134155, Classifier Loss 0.10501137375831604, Total Loss 47.60414505004883\n",
      "10: Encoding Loss 4.211043834686279, Transition Loss -0.6183800101280212, Classifier Loss 0.10183174163103104, Total Loss 35.44919204711914\n",
      "10: Encoding Loss 6.948395729064941, Transition Loss -1.215039610862732, Classifier Loss 0.11158537864685059, Total Loss 52.84843063354492\n",
      "10: Encoding Loss 6.29562520980835, Transition Loss -0.7942817211151123, Classifier Loss 0.11376550048589706, Total Loss 49.149986267089844\n",
      "10: Encoding Loss 4.667504787445068, Transition Loss -1.7294095754623413, Classifier Loss 0.14039716124534607, Total Loss 42.0440559387207\n",
      "10: Encoding Loss 5.15872049331665, Transition Loss -0.5584905743598938, Classifier Loss 0.11132622510194778, Total Loss 42.084720611572266\n",
      "10: Encoding Loss 7.49188756942749, Transition Loss -2.2425410747528076, Classifier Loss 0.15990786254405975, Total Loss 60.941219329833984\n",
      "10: Encoding Loss 4.841689109802246, Transition Loss -1.7983901500701904, Classifier Loss 0.045565344393253326, Total Loss 33.60594940185547\n",
      "10: Encoding Loss 6.829028129577637, Transition Loss -0.9960067868232727, Classifier Loss 0.18494802713394165, Total Loss 59.46857452392578\n",
      "10: Encoding Loss 5.69994592666626, Transition Loss -0.480294406414032, Classifier Loss 0.126413032412529, Total Loss 46.840789794921875\n",
      "10: Encoding Loss 4.910609722137451, Transition Loss -1.8171888589859009, Classifier Loss 0.11930741369724274, Total Loss 41.393672943115234\n",
      "10: Encoding Loss 4.423245906829834, Transition Loss -2.7603604793548584, Classifier Loss 0.13892945647239685, Total Loss 40.43132019042969\n",
      "10: Encoding Loss 6.49814510345459, Transition Loss -2.11574649810791, Classifier Loss 0.1308881938457489, Total Loss 52.07684326171875\n",
      "10: Encoding Loss 4.340947151184082, Transition Loss -0.8082663416862488, Classifier Loss 0.08722124993801117, Total Loss 34.76748275756836\n",
      "10: Encoding Loss 2.9878718852996826, Transition Loss -1.1950912475585938, Classifier Loss 0.07069259881973267, Total Loss 24.996013641357422\n",
      "10: Encoding Loss 2.5170300006866455, Transition Loss -1.7209372520446777, Classifier Loss 0.07431187480688095, Total Loss 22.532678604125977\n",
      "10: Encoding Loss 3.6244819164276123, Transition Loss -1.4577136039733887, Classifier Loss 0.09012790769338608, Total Loss 30.75909996032715\n",
      "10: Encoding Loss 8.625219345092773, Transition Loss 0.710317075252533, Classifier Loss 0.1120891198515892, Total Loss 63.24435806274414\n",
      "10: Encoding Loss 5.825214862823486, Transition Loss 0.1721537709236145, Classifier Loss 0.13916192948818207, Total Loss 48.93634796142578\n",
      "10: Encoding Loss 4.156128883361816, Transition Loss 0.6154721975326538, Classifier Loss 0.06197617948055267, Total Loss 31.38058090209961\n",
      "10: Encoding Loss 7.546723365783691, Transition Loss -0.46442705392837524, Classifier Loss 0.24665039777755737, Total Loss 69.94519805908203\n",
      "10: Encoding Loss 4.608391761779785, Transition Loss -1.6100538969039917, Classifier Loss 0.0552021861076355, Total Loss 33.169925689697266\n",
      "10: Encoding Loss 4.647469520568848, Transition Loss -2.260139226913452, Classifier Loss 0.11087943613529205, Total Loss 38.971858978271484\n",
      "10: Encoding Loss 5.8011369705200195, Transition Loss -1.2364338636398315, Classifier Loss 0.13497231900691986, Total Loss 48.303558349609375\n",
      "10: Encoding Loss 5.025903701782227, Transition Loss -1.435053825378418, Classifier Loss 0.06312993168830872, Total Loss 36.46784591674805\n",
      "10: Encoding Loss 4.541673183441162, Transition Loss -0.7216598391532898, Classifier Loss 0.05848737061023712, Total Loss 33.098487854003906\n",
      "10: Encoding Loss 5.947822570800781, Transition Loss -0.8250248432159424, Classifier Loss 0.10486072301864624, Total Loss 46.17267608642578\n",
      "10: Encoding Loss 3.6672115325927734, Transition Loss 0.10856752097606659, Classifier Loss 0.03800341114401817, Total Loss 25.847036361694336\n",
      "10: Encoding Loss 6.783490180969238, Transition Loss 0.53056800365448, Classifier Loss 0.0817805677652359, Total Loss 49.091224670410156\n",
      "10: Encoding Loss 5.4381208419799805, Transition Loss -2.0048632621765137, Classifier Loss 0.10630152374505997, Total Loss 43.258079528808594\n",
      "10: Encoding Loss 4.223587512969971, Transition Loss -1.6275005340576172, Classifier Loss 0.1326090395450592, Total Loss 38.60177993774414\n",
      "10: Encoding Loss 6.2162394523620605, Transition Loss -1.9858171939849854, Classifier Loss 0.057200729846954346, Total Loss 43.016719818115234\n",
      "10: Encoding Loss 3.6238644123077393, Transition Loss -1.1027599573135376, Classifier Loss 0.11368414759635925, Total Loss 33.11116027832031\n",
      "10: Encoding Loss 2.2321205139160156, Transition Loss -1.6698898077011108, Classifier Loss 0.09069579094648361, Total Loss 22.46163558959961\n",
      "10: Encoding Loss 2.300729751586914, Transition Loss -1.1359355449676514, Classifier Loss 0.05422572046518326, Total Loss 19.226497650146484\n",
      "10: Encoding Loss 8.58432388305664, Transition Loss -2.1188254356384277, Classifier Loss 0.11796223372220993, Total Loss 63.30132293701172\n",
      "10: Encoding Loss 4.357903480529785, Transition Loss -1.7264593839645386, Classifier Loss 0.05122813582420349, Total Loss 31.26954460144043\n",
      "10: Encoding Loss 2.498457193374634, Transition Loss -1.984084129333496, Classifier Loss 0.0708707943558693, Total Loss 22.077030181884766\n",
      "10: Encoding Loss 6.392577171325684, Transition Loss -1.2989696264266968, Classifier Loss 0.12464584410190582, Total Loss 50.81953048706055\n",
      "10: Encoding Loss 3.193328619003296, Transition Loss -1.0064798593521118, Classifier Loss 0.06178360804915428, Total Loss 25.33793067932129\n",
      "10: Encoding Loss 5.366601467132568, Transition Loss -0.7397762537002563, Classifier Loss 0.10340406000614166, Total Loss 42.53971862792969\n",
      "10: Encoding Loss 5.357949733734131, Transition Loss -0.5092015266418457, Classifier Loss 0.07335330545902252, Total Loss 39.48283004760742\n",
      "10: Encoding Loss 5.430152416229248, Transition Loss -2.8614745140075684, Classifier Loss 0.11425645649433136, Total Loss 44.00541687011719\n",
      "10: Encoding Loss 5.3505706787109375, Transition Loss -0.45252862572669983, Classifier Loss 0.12355569005012512, Total Loss 44.45881271362305\n",
      "10: Encoding Loss 5.2823052406311035, Transition Loss -1.5777602195739746, Classifier Loss 0.16060128808021545, Total Loss 47.75333023071289\n",
      "10: Encoding Loss 4.674213409423828, Transition Loss -2.6199679374694824, Classifier Loss 0.09336504340171814, Total Loss 37.3807373046875\n",
      "10: Encoding Loss 3.596379041671753, Transition Loss -1.229795217514038, Classifier Loss 0.04174000024795532, Total Loss 25.75178337097168\n",
      "10: Encoding Loss 7.438254356384277, Transition Loss -1.1511633396148682, Classifier Loss 0.14952296018600464, Total Loss 59.581363677978516\n",
      "10: Encoding Loss 6.6274824142456055, Transition Loss -0.9195796251296997, Classifier Loss 0.14740394055843353, Total Loss 54.50492477416992\n",
      "10: Encoding Loss 5.274359226226807, Transition Loss -1.2197471857070923, Classifier Loss 0.14877595007419586, Total Loss 46.52326202392578\n",
      "10: Encoding Loss 3.627462863922119, Transition Loss -1.8128026723861694, Classifier Loss 0.06188352778553963, Total Loss 27.95240592956543\n",
      "10: Encoding Loss 9.919509887695312, Transition Loss -1.2071774005889893, Classifier Loss 0.17584337294101715, Total Loss 77.10092163085938\n",
      "10: Encoding Loss 5.634708881378174, Transition Loss -2.634995460510254, Classifier Loss 0.06957568228244781, Total Loss 40.7647705078125\n",
      "10: Encoding Loss 4.7905378341674805, Transition Loss -1.9854481220245361, Classifier Loss 0.1424202024936676, Total Loss 42.98445510864258\n",
      "10: Encoding Loss 6.349940299987793, Transition Loss -2.146357536315918, Classifier Loss 0.16628295183181763, Total Loss 54.727081298828125\n",
      "10: Encoding Loss 6.561087608337402, Transition Loss -1.4703043699264526, Classifier Loss 0.13332557678222656, Total Loss 52.6984977722168\n",
      "10: Encoding Loss 5.376312732696533, Transition Loss -0.6251845955848694, Classifier Loss 0.08832864463329315, Total Loss 41.090492248535156\n",
      "10: Encoding Loss 7.161433219909668, Transition Loss -2.731506109237671, Classifier Loss 0.08740102499723434, Total Loss 51.707611083984375\n",
      "10: Encoding Loss 7.356764793395996, Transition Loss -1.4415370225906372, Classifier Loss 0.1867590695619583, Total Loss 62.815921783447266\n",
      "10: Encoding Loss 7.4826250076293945, Transition Loss -2.277003526687622, Classifier Loss 0.055364787578582764, Total Loss 50.43132019042969\n",
      "10: Encoding Loss 6.44522762298584, Transition Loss -0.5523822903633118, Classifier Loss 0.20374299585819244, Total Loss 59.045448303222656\n",
      "10: Encoding Loss 6.001810550689697, Transition Loss -1.9115450382232666, Classifier Loss 0.05949285253882408, Total Loss 41.95938491821289\n",
      "10: Encoding Loss 5.244378089904785, Transition Loss -1.3571559190750122, Classifier Loss 0.0624275729060173, Total Loss 37.7084846496582\n",
      "10: Encoding Loss 4.0302863121032715, Transition Loss -2.5510687828063965, Classifier Loss 0.06032456085085869, Total Loss 30.213153839111328\n",
      "10: Encoding Loss 4.317805767059326, Transition Loss -1.0285413265228271, Classifier Loss 0.10194388777017593, Total Loss 36.10081100463867\n",
      "10: Encoding Loss 3.849954128265381, Transition Loss -2.980374574661255, Classifier Loss 0.07105626910924911, Total Loss 30.204160690307617\n",
      "10: Encoding Loss 7.495033264160156, Transition Loss -0.882722020149231, Classifier Loss 0.10245846211910248, Total Loss 55.21569061279297\n",
      "10: Encoding Loss 4.242251873016357, Transition Loss -1.8512815237045288, Classifier Loss 0.10525399446487427, Total Loss 35.978172302246094\n",
      "10: Encoding Loss 2.926654577255249, Transition Loss -1.2624248266220093, Classifier Loss 0.04131332412362099, Total Loss 21.69075584411621\n",
      "10: Encoding Loss 7.251538276672363, Transition Loss -2.2687606811523438, Classifier Loss 0.07002198696136475, Total Loss 50.510520935058594\n",
      "10: Encoding Loss 7.697644233703613, Transition Loss -1.4845471382141113, Classifier Loss 0.0818043127655983, Total Loss 54.36570358276367\n",
      "10: Encoding Loss 5.79237174987793, Transition Loss -0.5378209948539734, Classifier Loss 0.12334945797920227, Total Loss 47.08896255493164\n",
      "10: Encoding Loss 5.94991397857666, Transition Loss -1.7499854564666748, Classifier Loss 0.03598742187023163, Total Loss 39.29753112792969\n",
      "10: Encoding Loss 4.889786720275879, Transition Loss -1.059245228767395, Classifier Loss 0.10883736610412598, Total Loss 40.2220344543457\n",
      "10: Encoding Loss 5.45847225189209, Transition Loss -1.6465028524398804, Classifier Loss 0.09173335880041122, Total Loss 41.92351150512695\n",
      "10: Encoding Loss 5.629976749420166, Transition Loss -1.15878427028656, Classifier Loss 0.1047411635518074, Total Loss 44.25351333618164\n",
      "10: Encoding Loss 4.116662979125977, Transition Loss -0.8763240575790405, Classifier Loss 0.08033537864685059, Total Loss 32.7331657409668\n",
      "10: Encoding Loss 5.916686058044434, Transition Loss -2.0398173332214355, Classifier Loss 0.1479707509279251, Total Loss 50.29637908935547\n",
      "10: Encoding Loss 3.826071262359619, Transition Loss -1.1054916381835938, Classifier Loss 0.0720459520816803, Total Loss 30.160581588745117\n",
      "10: Encoding Loss 5.005406856536865, Transition Loss -0.4130849838256836, Classifier Loss 0.04407802224159241, Total Loss 34.44007873535156\n",
      "10: Encoding Loss 7.1203508377075195, Transition Loss -1.4508321285247803, Classifier Loss 0.19111520051956177, Total Loss 61.833045959472656\n",
      "10: Encoding Loss 5.138247013092041, Transition Loss -0.9071864485740662, Classifier Loss 0.05074484273791313, Total Loss 35.90360641479492\n",
      "10: Encoding Loss 5.722282409667969, Transition Loss -3.7374470233917236, Classifier Loss 0.0788988396525383, Total Loss 42.222084045410156\n",
      "10: Encoding Loss 4.495392322540283, Transition Loss -1.7532215118408203, Classifier Loss 0.046978555619716644, Total Loss 31.66950798034668\n",
      "10: Encoding Loss 5.291558265686035, Transition Loss -0.2935467064380646, Classifier Loss 0.17173586785793304, Total Loss 48.92281723022461\n",
      "10: Encoding Loss 3.65873646736145, Transition Loss -1.420695424079895, Classifier Loss 0.09905953705310822, Total Loss 31.857803344726562\n",
      "10: Encoding Loss 7.9390387535095215, Transition Loss -1.9687838554382324, Classifier Loss 0.15552452206611633, Total Loss 63.1859016418457\n",
      "10: Encoding Loss 5.050474166870117, Transition Loss -0.795188844203949, Classifier Loss 0.05863043665885925, Total Loss 36.16557312011719\n",
      "10: Encoding Loss 4.990502834320068, Transition Loss -2.6531496047973633, Classifier Loss 0.09659864753484726, Total Loss 39.60182189941406\n",
      "10: Encoding Loss 4.699191093444824, Transition Loss -2.9563629627227783, Classifier Loss 0.1288205087184906, Total Loss 41.07601547241211\n",
      "10: Encoding Loss 4.7668867111206055, Transition Loss -1.6076226234436035, Classifier Loss 0.12171068787574768, Total Loss 40.77174758911133\n",
      "10: Encoding Loss 5.55108118057251, Transition Loss -1.354486346244812, Classifier Loss 0.09910311549901962, Total Loss 43.21625900268555\n",
      "10: Encoding Loss 5.273639678955078, Transition Loss -1.9507461786270142, Classifier Loss 0.16443322598934174, Total Loss 48.084381103515625\n",
      "10: Encoding Loss 5.855740547180176, Transition Loss -1.352993130683899, Classifier Loss 0.03696915879845619, Total Loss 38.83081817626953\n",
      "10: Encoding Loss 4.803970813751221, Transition Loss 0.41737791895866394, Classifier Loss 0.05710699409246445, Total Loss 34.70147705078125\n",
      "10: Encoding Loss 5.657270431518555, Transition Loss -1.9584596157073975, Classifier Loss 0.05511237308382988, Total Loss 39.454078674316406\n",
      "10: Encoding Loss 4.366199016571045, Transition Loss -1.5610475540161133, Classifier Loss 0.06955381482839584, Total Loss 33.15195083618164\n",
      "10: Encoding Loss 6.462075233459473, Transition Loss -0.42422235012054443, Classifier Loss 0.11054354906082153, Total Loss 49.82664108276367\n",
      "10: Encoding Loss 4.619071006774902, Transition Loss -2.393695831298828, Classifier Loss 0.0410735048353672, Total Loss 31.820819854736328\n",
      "10: Encoding Loss 7.291092872619629, Transition Loss -1.645092248916626, Classifier Loss 0.22428864240646362, Total Loss 66.17476654052734\n",
      "10: Encoding Loss 5.655547618865967, Transition Loss -1.1612671613693237, Classifier Loss 0.12745703756809235, Total Loss 46.67852783203125\n",
      "10: Encoding Loss 5.324747085571289, Transition Loss -1.891722559928894, Classifier Loss 0.06516577303409576, Total Loss 38.46430587768555\n",
      "10: Encoding Loss 5.067011833190918, Transition Loss -0.03368903696537018, Classifier Loss 0.06570959091186523, Total Loss 36.97301483154297\n",
      "10: Encoding Loss 5.286145210266113, Transition Loss -1.1600916385650635, Classifier Loss 0.055744633078575134, Total Loss 37.290870666503906\n",
      "10: Encoding Loss 5.366370677947998, Transition Loss -0.3111269474029541, Classifier Loss 0.13125678896903992, Total Loss 45.32378005981445\n",
      "10: Encoding Loss 7.145620822906494, Transition Loss -0.41951674222946167, Classifier Loss 0.0480644553899765, Total Loss 47.68000411987305\n",
      "10: Encoding Loss 4.521595001220703, Transition Loss -1.760244607925415, Classifier Loss 0.1789608597755432, Total Loss 45.02495193481445\n",
      "10: Encoding Loss 5.173579216003418, Transition Loss -0.37055742740631104, Classifier Loss 0.07232172787189484, Total Loss 38.273502349853516\n",
      "10: Encoding Loss 4.219074726104736, Transition Loss -0.47969046235084534, Classifier Loss 0.10067547112703323, Total Loss 35.381805419921875\n",
      "10: Encoding Loss 4.767413139343262, Transition Loss -1.7809820175170898, Classifier Loss 0.041228439658880234, Total Loss 32.72661209106445\n",
      "10: Encoding Loss 4.950089931488037, Transition Loss -2.0637032985687256, Classifier Loss 0.17207416892051697, Total Loss 46.907135009765625\n",
      "10: Encoding Loss 5.053062438964844, Transition Loss -2.1239185333251953, Classifier Loss 0.14263491332530975, Total Loss 44.581016540527344\n",
      "10: Encoding Loss 3.6934218406677246, Transition Loss -2.913668155670166, Classifier Loss 0.07681230455636978, Total Loss 29.84059715270996\n",
      "10: Encoding Loss 4.073702812194824, Transition Loss -0.598169207572937, Classifier Loss 0.11919005215167999, Total Loss 36.360984802246094\n",
      "10: Encoding Loss 5.898222923278809, Transition Loss -0.5496070384979248, Classifier Loss 0.12538214027881622, Total Loss 47.927330017089844\n",
      "10: Encoding Loss 3.448038101196289, Transition Loss -2.270843982696533, Classifier Loss 0.05485983192920685, Total Loss 26.173303604125977\n",
      "10: Encoding Loss 5.069826602935791, Transition Loss 0.18942278623580933, Classifier Loss 0.03602864220738411, Total Loss 34.097591400146484\n",
      "10: Encoding Loss 5.5648345947265625, Transition Loss -2.2491631507873535, Classifier Loss 0.080654576420784, Total Loss 41.45356369018555\n",
      "10: Encoding Loss 4.21470832824707, Transition Loss -0.9134807586669922, Classifier Loss 0.06407413631677628, Total Loss 31.69529914855957\n",
      "10: Encoding Loss 7.906201362609863, Transition Loss 0.5731225609779358, Classifier Loss 0.2381073534488678, Total Loss 71.4771957397461\n",
      "10: Encoding Loss 5.8033013343811035, Transition Loss -1.6509126424789429, Classifier Loss 0.04921216517686844, Total Loss 39.74036407470703\n",
      "10: Encoding Loss 7.041723728179932, Transition Loss 0.40477123856544495, Classifier Loss 0.07283985614776611, Total Loss 49.69623565673828\n",
      "10: Encoding Loss 5.125542640686035, Transition Loss -1.723558783531189, Classifier Loss 0.1609862893819809, Total Loss 46.851192474365234\n",
      "10: Encoding Loss 3.6303844451904297, Transition Loss -1.1777445077896118, Classifier Loss 0.0922977551817894, Total Loss 31.01161003112793\n",
      "10: Encoding Loss 6.43792200088501, Transition Loss -1.6908365488052368, Classifier Loss 0.13099037110805511, Total Loss 51.725894927978516\n",
      "10: Encoding Loss 8.935602188110352, Transition Loss -2.89290714263916, Classifier Loss 0.1778806746006012, Total Loss 71.40052032470703\n",
      "10: Encoding Loss 7.080375671386719, Transition Loss -1.4841352701187134, Classifier Loss 0.28036096692085266, Total Loss 70.51775360107422\n",
      "10: Encoding Loss 4.205315113067627, Transition Loss -1.394978642463684, Classifier Loss 0.10769849270582199, Total Loss 36.001182556152344\n",
      "10: Encoding Loss 7.650453090667725, Transition Loss 0.31768906116485596, Classifier Loss 0.08792020380496979, Total Loss 54.821815490722656\n",
      "10: Encoding Loss 9.239261627197266, Transition Loss -1.5761306285858154, Classifier Loss 0.30673134326934814, Total Loss 86.10807037353516\n",
      "10: Encoding Loss 6.689962863922119, Transition Loss -0.8790277242660522, Classifier Loss 0.11749762296676636, Total Loss 51.889190673828125\n",
      "10: Encoding Loss 7.0437188148498535, Transition Loss -0.277424156665802, Classifier Loss 0.13010264933109283, Total Loss 55.27246856689453\n",
      "10: Encoding Loss 7.364358425140381, Transition Loss -0.5333497524261475, Classifier Loss 0.09916877001523972, Total Loss 54.10281753540039\n",
      "10: Encoding Loss 5.417418479919434, Transition Loss -2.039811611175537, Classifier Loss 0.09381511807441711, Total Loss 41.88520812988281\n",
      "10: Encoding Loss 4.842286109924316, Transition Loss -1.9118437767028809, Classifier Loss 0.12579230964183807, Total Loss 41.63218688964844\n",
      "10: Encoding Loss 5.925438404083252, Transition Loss -3.399070978164673, Classifier Loss 0.13272784650325775, Total Loss 48.824058532714844\n",
      "10: Encoding Loss 4.873003959655762, Transition Loss -2.273670196533203, Classifier Loss 0.07935811579227448, Total Loss 37.17292785644531\n",
      "10: Encoding Loss 4.381563186645508, Transition Loss -1.3207402229309082, Classifier Loss 0.05642094835639, Total Loss 31.93094825744629\n",
      "10: Encoding Loss 5.069312572479248, Transition Loss -1.814687728881836, Classifier Loss 0.17363609373569489, Total Loss 47.77876281738281\n",
      "10: Encoding Loss 3.17716383934021, Transition Loss -1.9511096477508545, Classifier Loss 0.0836624950170517, Total Loss 27.42845344543457\n",
      "10: Encoding Loss 4.161075115203857, Transition Loss -1.5000948905944824, Classifier Loss 0.1237640306353569, Total Loss 37.342254638671875\n",
      "10: Encoding Loss 6.490935325622559, Transition Loss -1.6691272258758545, Classifier Loss 0.2041919231414795, Total Loss 59.364139556884766\n",
      "10: Encoding Loss 5.165554523468018, Transition Loss -0.4020526111125946, Classifier Loss 0.15493617951869965, Total Loss 46.486785888671875\n",
      "10: Encoding Loss 4.068587303161621, Transition Loss -1.9099233150482178, Classifier Loss 0.122218556702137, Total Loss 36.63261795043945\n",
      "10: Encoding Loss 5.420501232147217, Transition Loss -2.2524354457855225, Classifier Loss 0.07773905992507935, Total Loss 40.296016693115234\n",
      "10: Encoding Loss 3.9369239807128906, Transition Loss -0.6280056238174438, Classifier Loss 0.05389399826526642, Total Loss 29.010692596435547\n",
      "10: Encoding Loss 5.534590721130371, Transition Loss -1.3876259326934814, Classifier Loss 0.04154250770807266, Total Loss 37.36124038696289\n",
      "10: Encoding Loss 4.398800849914551, Transition Loss -1.4079928398132324, Classifier Loss 0.04936233162879944, Total Loss 31.328475952148438\n",
      "10: Encoding Loss 3.8877005577087402, Transition Loss -0.5512861609458923, Classifier Loss 0.12104444205760956, Total Loss 35.43042755126953\n",
      "10: Encoding Loss 5.715134143829346, Transition Loss -0.7315865755081177, Classifier Loss 0.13396665453910828, Total Loss 47.68717575073242\n",
      "10: Encoding Loss 6.836934566497803, Transition Loss -0.1198839396238327, Classifier Loss 0.13377675414085388, Total Loss 54.399234771728516\n",
      "10: Encoding Loss 6.5449090003967285, Transition Loss -0.7060273885726929, Classifier Loss 0.11051476746797562, Total Loss 50.320648193359375\n",
      "10: Encoding Loss 3.109961748123169, Transition Loss -2.289918899536133, Classifier Loss 0.09008634835481644, Total Loss 27.667490005493164\n",
      "10: Encoding Loss 7.1540703773498535, Transition Loss -1.4872028827667236, Classifier Loss 0.1484721601009369, Total Loss 57.77104187011719\n",
      "10: Encoding Loss 4.462503433227539, Transition Loss -2.5344080924987793, Classifier Loss 0.10113663226366043, Total Loss 36.887672424316406\n",
      "10: Encoding Loss 5.891210556030273, Transition Loss -1.0219148397445679, Classifier Loss 0.0881807878613472, Total Loss 44.16493606567383\n",
      "10: Encoding Loss 7.116544246673584, Transition Loss -2.043478012084961, Classifier Loss 0.18885378539562225, Total Loss 61.583831787109375\n",
      "10: Encoding Loss 5.565921783447266, Transition Loss -2.026299238204956, Classifier Loss 0.14200882613658905, Total Loss 47.595603942871094\n",
      "10: Encoding Loss 7.352080345153809, Transition Loss -0.1682775616645813, Classifier Loss 0.14607521891593933, Total Loss 58.71993637084961\n",
      "10: Encoding Loss 4.667168140411377, Transition Loss 0.12081734836101532, Classifier Loss 0.09411919862031937, Total Loss 37.4632568359375\n",
      "10: Encoding Loss 4.954901695251465, Transition Loss -2.0051379203796387, Classifier Loss 0.08794471621513367, Total Loss 38.5230827331543\n",
      "10: Encoding Loss 7.0974836349487305, Transition Loss -0.7512378692626953, Classifier Loss 0.13337671756744385, Total Loss 55.92227554321289\n",
      "10: Encoding Loss 6.362532138824463, Transition Loss -1.3739259243011475, Classifier Loss 0.10792791843414307, Total Loss 48.967437744140625\n",
      "10: Encoding Loss 5.876222610473633, Transition Loss -0.7898433208465576, Classifier Loss 0.1229998767375946, Total Loss 47.5570068359375\n",
      "10: Encoding Loss 4.222546577453613, Transition Loss 0.18858325481414795, Classifier Loss 0.11750607937574387, Total Loss 37.161319732666016\n",
      "10: Encoding Loss 3.2580299377441406, Transition Loss -0.8474181890487671, Classifier Loss 0.05638384073972702, Total Loss 25.18622398376465\n",
      "10: Encoding Loss 5.535353660583496, Transition Loss -1.7556703090667725, Classifier Loss 0.08370455354452133, Total Loss 41.581878662109375\n",
      "10: Encoding Loss 4.591914176940918, Transition Loss -1.2732596397399902, Classifier Loss 0.08504322916269302, Total Loss 36.0552978515625\n",
      "10: Encoding Loss 4.727062702178955, Transition Loss -0.7127740383148193, Classifier Loss 0.19077028334140778, Total Loss 47.43912124633789\n",
      "10: Encoding Loss 4.231108665466309, Transition Loss -2.240586757659912, Classifier Loss 0.05091934651136398, Total Loss 30.477691650390625\n",
      "10: Encoding Loss 4.618943691253662, Transition Loss -2.669856548309326, Classifier Loss 0.12312063574790955, Total Loss 40.024658203125\n",
      "10: Encoding Loss 1.8589681386947632, Transition Loss -0.6611590385437012, Classifier Loss 0.06140606850385666, Total Loss 17.294151306152344\n",
      "10: Encoding Loss 4.904544353485107, Transition Loss -0.7755326628684998, Classifier Loss 0.05699579790234566, Total Loss 35.12653732299805\n",
      "10: Encoding Loss 4.606359958648682, Transition Loss -1.51710844039917, Classifier Loss 0.13489004969596863, Total Loss 41.12656021118164\n",
      "10: Encoding Loss 3.6903624534606934, Transition Loss -0.7014593482017517, Classifier Loss 0.056675106287002563, Total Loss 27.809406280517578\n",
      "10: Encoding Loss 4.723729133605957, Transition Loss -0.17099758982658386, Classifier Loss 0.03135809674859047, Total Loss 31.47811508178711\n",
      "10: Encoding Loss 7.714845180511475, Transition Loss -2.1861166954040527, Classifier Loss 0.1914130300283432, Total Loss 65.42949676513672\n",
      "10: Encoding Loss 4.93902063369751, Transition Loss -0.6133608222007751, Classifier Loss 0.18403685092926025, Total Loss 48.037567138671875\n",
      "10: Encoding Loss 5.001541614532471, Transition Loss -1.144463062286377, Classifier Loss 0.18706540763378143, Total Loss 48.71533203125\n",
      "10: Encoding Loss 4.667350769042969, Transition Loss -1.8536440134048462, Classifier Loss 0.08147415518760681, Total Loss 36.150779724121094\n",
      "10: Encoding Loss 6.367142677307129, Transition Loss -1.7429783344268799, Classifier Loss 0.15238933265209198, Total Loss 53.44109344482422\n",
      "10: Encoding Loss 5.608355522155762, Transition Loss -0.9983384609222412, Classifier Loss 0.09348863363265991, Total Loss 42.998600006103516\n",
      "10: Encoding Loss 5.820517063140869, Transition Loss -0.49732187390327454, Classifier Loss 0.06696154177188873, Total Loss 41.61906051635742\n",
      "10: Encoding Loss 4.288750171661377, Transition Loss -1.9698171615600586, Classifier Loss 0.10631114989519119, Total Loss 36.36282730102539\n",
      "10: Encoding Loss 6.345081806182861, Transition Loss -0.07590442895889282, Classifier Loss 0.08457627892494202, Total Loss 46.52809143066406\n",
      "10: Encoding Loss 4.608384132385254, Transition Loss -1.6491787433624268, Classifier Loss 0.08480921387672424, Total Loss 36.13056564331055\n",
      "10: Encoding Loss 5.7885026931762695, Transition Loss -1.4055858850479126, Classifier Loss 0.10283288359642029, Total Loss 45.01374435424805\n",
      "10: Encoding Loss 6.235010623931885, Transition Loss -0.6482763290405273, Classifier Loss 0.08411957323551178, Total Loss 45.82176208496094\n",
      "10: Encoding Loss 4.728912353515625, Transition Loss -1.8169057369232178, Classifier Loss 0.04879995808005333, Total Loss 33.252742767333984\n",
      "10: Encoding Loss 3.9784045219421387, Transition Loss -1.2957526445388794, Classifier Loss 0.08433191478252411, Total Loss 32.3031005859375\n",
      "10: Encoding Loss 3.66398549079895, Transition Loss -1.169439673423767, Classifier Loss 0.08156516402959824, Total Loss 30.139963150024414\n",
      "10: Encoding Loss 7.481142997741699, Transition Loss -2.4833688735961914, Classifier Loss 0.09912769496440887, Total Loss 54.79863739013672\n",
      "10: Encoding Loss 4.0280985832214355, Transition Loss -3.4595589637756348, Classifier Loss 0.06219727545976639, Total Loss 30.38693618774414\n",
      "10: Encoding Loss 3.117002487182617, Transition Loss -1.1705923080444336, Classifier Loss 0.09603388607501984, Total Loss 28.3049373626709\n",
      "10: Encoding Loss 9.96844482421875, Transition Loss 0.38984447717666626, Classifier Loss 0.17361174523830414, Total Loss 77.3277816772461\n",
      "10: Encoding Loss 6.5982513427734375, Transition Loss -0.10147710889577866, Classifier Loss 0.12243736535310745, Total Loss 51.83320236206055\n",
      "10: Encoding Loss 4.623236656188965, Transition Loss -2.663731336593628, Classifier Loss 0.0740751400589943, Total Loss 35.145870208740234\n",
      "10: Encoding Loss 4.095271110534668, Transition Loss -0.6647037863731384, Classifier Loss 0.07722244411706924, Total Loss 32.29360580444336\n",
      "10: Encoding Loss 6.9540228843688965, Transition Loss -2.5595474243164062, Classifier Loss 0.18762101233005524, Total Loss 60.48522186279297\n",
      "10: Encoding Loss 4.720569133758545, Transition Loss -1.1405690908432007, Classifier Loss 0.06498418748378754, Total Loss 34.82137680053711\n",
      "10: Encoding Loss 4.328301906585693, Transition Loss -1.165799856185913, Classifier Loss 0.07226080447435379, Total Loss 33.19542694091797\n",
      "10: Encoding Loss 4.228201389312744, Transition Loss -1.577394723892212, Classifier Loss 0.1317138969898224, Total Loss 38.53997039794922\n",
      "10: Encoding Loss 6.200443267822266, Transition Loss -0.556100606918335, Classifier Loss 0.04245239496231079, Total Loss 41.44767761230469\n",
      "10: Encoding Loss 4.5324201583862305, Transition Loss -1.424335241317749, Classifier Loss 0.07672945410013199, Total Loss 34.86690139770508\n",
      "10: Encoding Loss 6.6244049072265625, Transition Loss -2.0213146209716797, Classifier Loss 0.0998554453253746, Total Loss 49.731163024902344\n",
      "10: Encoding Loss 5.551239967346191, Transition Loss -0.5883846282958984, Classifier Loss 0.07259833067655563, Total Loss 40.567039489746094\n",
      "10: Encoding Loss 5.595072269439697, Transition Loss -0.24593663215637207, Classifier Loss 0.14029549062252045, Total Loss 47.599884033203125\n",
      "10: Encoding Loss 7.2834343910217285, Transition Loss -1.0365052223205566, Classifier Loss 0.04347975179553032, Total Loss 48.04816818237305\n",
      "10: Encoding Loss 5.796015739440918, Transition Loss -1.0543209314346313, Classifier Loss 0.1975727677345276, Total Loss 54.5329475402832\n",
      "10: Encoding Loss 5.258121013641357, Transition Loss -0.554885983467102, Classifier Loss 0.1053578108549118, Total Loss 42.084285736083984\n",
      "10: Encoding Loss 3.2828993797302246, Transition Loss -0.9731143712997437, Classifier Loss 0.06765353679656982, Total Loss 26.46236228942871\n",
      "10: Encoding Loss 5.036029815673828, Transition Loss -1.5451303720474243, Classifier Loss 0.08053906261920929, Total Loss 38.26947021484375\n",
      "10: Encoding Loss 5.036125659942627, Transition Loss -0.3777927756309509, Classifier Loss 0.08487170934677124, Total Loss 38.703773498535156\n",
      "10: Encoding Loss 4.557700157165527, Transition Loss 0.38425031304359436, Classifier Loss 0.09653939306735992, Total Loss 37.15384292602539\n",
      "10: Encoding Loss 4.278451919555664, Transition Loss -1.7770824432373047, Classifier Loss 0.07664961367845535, Total Loss 33.334964752197266\n",
      "10: Encoding Loss 3.233659267425537, Transition Loss -2.8078155517578125, Classifier Loss 0.06767362356185913, Total Loss 26.168195724487305\n",
      "10: Encoding Loss 5.895874977111816, Transition Loss -2.070000648498535, Classifier Loss 0.10768893361091614, Total Loss 46.14331817626953\n",
      "10: Encoding Loss 5.133201599121094, Transition Loss -2.265192985534668, Classifier Loss 0.1442100703716278, Total Loss 45.21931076049805\n",
      "10: Encoding Loss 5.199448585510254, Transition Loss 0.010381817817687988, Classifier Loss 0.0949631929397583, Total Loss 40.69716262817383\n",
      "10: Encoding Loss 5.20013427734375, Transition Loss -0.9940392374992371, Classifier Loss 0.12753243744373322, Total Loss 43.95365524291992\n",
      "10: Encoding Loss 6.167191505432129, Transition Loss -0.5581437349319458, Classifier Loss 0.16333839297294617, Total Loss 53.33676528930664\n",
      "10: Encoding Loss 5.436598777770996, Transition Loss -0.6293481588363647, Classifier Loss 0.1218440979719162, Total Loss 44.80375289916992\n",
      "10: Encoding Loss 4.906462669372559, Transition Loss -1.1755194664001465, Classifier Loss 0.09187480807304382, Total Loss 38.625789642333984\n",
      "10: Encoding Loss 5.219439506530762, Transition Loss -3.0307767391204834, Classifier Loss 0.07329319417476654, Total Loss 38.644744873046875\n",
      "10: Encoding Loss 4.387289524078369, Transition Loss -0.8378528356552124, Classifier Loss 0.05558980628848076, Total Loss 31.882383346557617\n",
      "10: Encoding Loss 5.138535499572754, Transition Loss -1.4423223733901978, Classifier Loss 0.09890006482601166, Total Loss 40.72064208984375\n",
      "10: Encoding Loss 5.569003105163574, Transition Loss -2.8416025638580322, Classifier Loss 0.05191575735807419, Total Loss 38.60445785522461\n",
      "10: Encoding Loss 6.825005531311035, Transition Loss -0.8267512321472168, Classifier Loss 0.051453154534101486, Total Loss 46.09502029418945\n",
      "10: Encoding Loss 3.1578030586242676, Transition Loss -0.7572078704833984, Classifier Loss 0.06995370239019394, Total Loss 25.94188690185547\n",
      "10: Encoding Loss 5.379586219787598, Transition Loss -1.5180153846740723, Classifier Loss 0.047637104988098145, Total Loss 37.04062271118164\n",
      "10: Encoding Loss 6.089463710784912, Transition Loss -1.218592643737793, Classifier Loss 0.06490767747163773, Total Loss 43.02706527709961\n",
      "10: Encoding Loss 3.7205824851989746, Transition Loss -0.5727410316467285, Classifier Loss 0.03097468614578247, Total Loss 25.42073631286621\n",
      "10: Encoding Loss 5.2092132568359375, Transition Loss -2.8326778411865234, Classifier Loss 0.1312139481306076, Total Loss 44.375545501708984\n",
      "10: Encoding Loss 7.0047101974487305, Transition Loss -0.31265613436698914, Classifier Loss 0.09986318647861481, Total Loss 52.01445388793945\n",
      "10: Encoding Loss 6.75317907333374, Transition Loss -0.9911415576934814, Classifier Loss 0.13582955300807953, Total Loss 54.10163497924805\n",
      "10: Encoding Loss 5.683294773101807, Transition Loss -1.442258596420288, Classifier Loss 0.06194151192903519, Total Loss 40.2933464050293\n",
      "10: Encoding Loss 5.235805988311768, Transition Loss -1.566176176071167, Classifier Loss 0.07829389721155167, Total Loss 39.24359893798828\n",
      "10: Encoding Loss 5.287010669708252, Transition Loss -0.7695311307907104, Classifier Loss 0.1575111597776413, Total Loss 47.47287368774414\n",
      "10: Encoding Loss 6.207262992858887, Transition Loss -1.70912766456604, Classifier Loss 0.06331682205200195, Total Loss 43.574581146240234\n",
      "10: Encoding Loss 3.3236312866210938, Transition Loss -1.2591259479522705, Classifier Loss 0.12910334765911102, Total Loss 32.851619720458984\n",
      "10: Encoding Loss 8.130067825317383, Transition Loss 0.3892688751220703, Classifier Loss 0.13478709757328033, Total Loss 62.414825439453125\n",
      "10: Encoding Loss 4.802430629730225, Transition Loss -1.162156343460083, Classifier Loss 0.04240240901708603, Total Loss 33.054359436035156\n",
      "10: Encoding Loss 3.136279582977295, Transition Loss -2.3799891471862793, Classifier Loss 0.057584308087825775, Total Loss 24.575157165527344\n",
      "10: Encoding Loss 5.988157749176025, Transition Loss -0.20849868655204773, Classifier Loss 0.049462635070085526, Total Loss 40.875125885009766\n",
      "10: Encoding Loss 2.658752918243408, Transition Loss -0.9823900461196899, Classifier Loss 0.038386713713407516, Total Loss 19.790796279907227\n",
      "10: Encoding Loss 6.732630729675293, Transition Loss -1.8171836137771606, Classifier Loss 0.12541036307811737, Total Loss 52.936092376708984\n",
      "10: Encoding Loss 6.1295647621154785, Transition Loss -1.057072401046753, Classifier Loss 0.09945505857467651, Total Loss 46.72247314453125\n",
      "10: Encoding Loss 5.2140913009643555, Transition Loss -2.1949284076690674, Classifier Loss 0.06566837430000305, Total Loss 37.85050964355469\n",
      "10: Encoding Loss 3.479112148284912, Transition Loss -1.1041383743286133, Classifier Loss 0.1220841184258461, Total Loss 33.0826416015625\n",
      "10: Encoding Loss 4.694339752197266, Transition Loss -1.5301318168640137, Classifier Loss 0.09309611469507217, Total Loss 37.475040435791016\n",
      "10: Encoding Loss 5.343001842498779, Transition Loss -1.624574899673462, Classifier Loss 0.0815177857875824, Total Loss 40.209144592285156\n",
      "10: Encoding Loss 3.4428794384002686, Transition Loss -0.8234302401542664, Classifier Loss 0.0839872658252716, Total Loss 29.055673599243164\n",
      "10: Encoding Loss 5.9466423988342285, Transition Loss -2.137377977371216, Classifier Loss 0.18363644182682037, Total Loss 54.04264450073242\n",
      "10: Encoding Loss 3.965815305709839, Transition Loss -2.0118777751922607, Classifier Loss 0.05396299809217453, Total Loss 29.190387725830078\n",
      "10: Encoding Loss 3.372727155685425, Transition Loss -0.7411300539970398, Classifier Loss 0.0635857954621315, Total Loss 26.594648361206055\n",
      "10: Encoding Loss 5.44348669052124, Transition Loss 0.024607479572296143, Classifier Loss 0.14169856905937195, Total Loss 46.84062194824219\n",
      "10: Encoding Loss 4.80501651763916, Transition Loss -1.7126654386520386, Classifier Loss 0.07867585867643356, Total Loss 36.696998596191406\n",
      "10: Encoding Loss 7.791567802429199, Transition Loss -0.6033939123153687, Classifier Loss 0.09650975465774536, Total Loss 56.400142669677734\n",
      "10: Encoding Loss 3.9766509532928467, Transition Loss -2.5951528549194336, Classifier Loss 0.19252018630504608, Total Loss 43.11088562011719\n",
      "10: Encoding Loss 6.01720666885376, Transition Loss -1.1112210750579834, Classifier Loss 0.08450749516487122, Total Loss 44.55354309082031\n",
      "10: Encoding Loss 4.122847557067871, Transition Loss -1.2632064819335938, Classifier Loss 0.0519234761595726, Total Loss 29.928930282592773\n",
      "10: Encoding Loss 8.567336082458496, Transition Loss -0.22712409496307373, Classifier Loss 0.14592257142066956, Total Loss 65.99618530273438\n",
      "10: Encoding Loss 5.300024032592773, Transition Loss -1.2500704526901245, Classifier Loss 0.1267755627632141, Total Loss 44.477203369140625\n",
      "10: Encoding Loss 4.387420177459717, Transition Loss -1.098802089691162, Classifier Loss 0.07866863161325455, Total Loss 34.19094467163086\n",
      "10: Encoding Loss 4.834048271179199, Transition Loss -2.797415256500244, Classifier Loss 0.09129471331834793, Total Loss 38.13264465332031\n",
      "10: Encoding Loss 3.454345226287842, Transition Loss 0.7043144106864929, Classifier Loss 0.1154913455247879, Total Loss 32.55693435668945\n",
      "10: Encoding Loss 3.399026870727539, Transition Loss -0.20143675804138184, Classifier Loss 0.04864140599966049, Total Loss 25.258220672607422\n",
      "10: Encoding Loss 4.700222015380859, Transition Loss -2.5445632934570312, Classifier Loss 0.1316642165184021, Total Loss 41.366737365722656\n",
      "10: Encoding Loss 5.662430763244629, Transition Loss -1.7752900123596191, Classifier Loss 0.12940779328346252, Total Loss 46.91465759277344\n",
      "10: Encoding Loss 4.143484115600586, Transition Loss -0.7079482674598694, Classifier Loss 0.08509640395641327, Total Loss 33.37026596069336\n",
      "10: Encoding Loss 4.43045711517334, Transition Loss -0.5670781135559082, Classifier Loss 0.07008212059736252, Total Loss 33.59073257446289\n",
      "10: Encoding Loss 4.357851505279541, Transition Loss -1.2012546062469482, Classifier Loss 0.0395456962287426, Total Loss 30.101198196411133\n",
      "10: Encoding Loss 7.002805233001709, Transition Loss -0.16339057683944702, Classifier Loss 0.24104252457618713, Total Loss 66.12101745605469\n",
      "10: Encoding Loss 4.568840503692627, Transition Loss -1.241875171661377, Classifier Loss 0.0691741406917572, Total Loss 34.32996368408203\n",
      "10: Encoding Loss 6.742781639099121, Transition Loss -1.5603731870651245, Classifier Loss 0.08021748065948486, Total Loss 48.477813720703125\n",
      "10: Encoding Loss 4.178589820861816, Transition Loss -0.02573305368423462, Classifier Loss 0.057493194937705994, Total Loss 30.82084846496582\n",
      "10: Encoding Loss 5.602047920227051, Transition Loss -1.6292612552642822, Classifier Loss 0.034917186945676804, Total Loss 37.103355407714844\n",
      "10: Encoding Loss 5.004508018493652, Transition Loss -0.8890660405158997, Classifier Loss 0.08999481052160263, Total Loss 39.02617645263672\n",
      "10: Encoding Loss 5.122939109802246, Transition Loss -1.3648966550827026, Classifier Loss 0.20688475668430328, Total Loss 51.425567626953125\n",
      "10: Encoding Loss 8.307110786437988, Transition Loss -0.34277594089508057, Classifier Loss 0.21604202687740326, Total Loss 71.44673156738281\n",
      "10: Encoding Loss 6.271484375, Transition Loss -1.3327467441558838, Classifier Loss 0.07592606544494629, Total Loss 45.220977783203125\n",
      "10: Encoding Loss 6.581973075866699, Transition Loss -1.3263616561889648, Classifier Loss 0.16465391218662262, Total Loss 55.95669937133789\n",
      "10: Encoding Loss 4.823714256286621, Transition Loss -1.84926176071167, Classifier Loss 0.03994492441415787, Total Loss 32.936038970947266\n",
      "10: Encoding Loss 5.547191619873047, Transition Loss -1.1675407886505127, Classifier Loss 0.07100990414619446, Total Loss 40.38367462158203\n",
      "10: Encoding Loss 6.456406593322754, Transition Loss -1.1291989088058472, Classifier Loss 0.08988325297832489, Total Loss 47.726318359375\n",
      "10: Encoding Loss 4.8988261222839355, Transition Loss -1.3632341623306274, Classifier Loss 0.11334607005119324, Total Loss 40.727020263671875\n",
      "10: Encoding Loss 3.4093689918518066, Transition Loss 0.4310457110404968, Classifier Loss 0.03761066496372223, Total Loss 24.389699935913086\n",
      "10: Encoding Loss 3.973188877105713, Transition Loss -0.681554913520813, Classifier Loss 0.07672105729579926, Total Loss 31.510967254638672\n",
      "10: Encoding Loss 4.53432559967041, Transition Loss 0.26746779680252075, Classifier Loss 0.06660299003124237, Total Loss 33.97323989868164\n",
      "10: Encoding Loss 4.956034183502197, Transition Loss -1.425351619720459, Classifier Loss 0.06197420135140419, Total Loss 35.93305587768555\n",
      "10: Encoding Loss 7.102565765380859, Transition Loss -0.30555224418640137, Classifier Loss 0.1945696324110031, Total Loss 62.072235107421875\n",
      "10: Encoding Loss 5.1052961349487305, Transition Loss -1.8252358436584473, Classifier Loss 0.08059481531381607, Total Loss 38.69053268432617\n",
      "10: Encoding Loss 8.401019096374512, Transition Loss -2.8171191215515137, Classifier Loss 0.1250743567943573, Total Loss 62.91242599487305\n",
      "10: Encoding Loss 5.791754245758057, Transition Loss -1.7187631130218506, Classifier Loss 0.07461211085319519, Total Loss 42.21105194091797\n",
      "10: Encoding Loss 3.6212470531463623, Transition Loss -1.1916011571884155, Classifier Loss 0.08307279646396637, Total Loss 30.034286499023438\n",
      "10: Encoding Loss 3.944445848464966, Transition Loss -0.43180668354034424, Classifier Loss 0.019296294078230858, Total Loss 25.596132278442383\n",
      "10: Encoding Loss 6.5542473793029785, Transition Loss -3.088127374649048, Classifier Loss 0.2420763075351715, Total Loss 63.53187942504883\n",
      "10: Encoding Loss 5.189723968505859, Transition Loss -1.4280509948730469, Classifier Loss 0.1303161382675171, Total Loss 44.16938781738281\n",
      "10: Encoding Loss 3.7961106300354004, Transition Loss 0.838625967502594, Classifier Loss 0.04623643308877945, Total Loss 27.735759735107422\n",
      "10: Encoding Loss 5.334469795227051, Transition Loss -1.0286009311676025, Classifier Loss 0.2288045585155487, Total Loss 54.886863708496094\n",
      "10: Encoding Loss 4.070080280303955, Transition Loss -1.6228307485580444, Classifier Loss 0.11280004680156708, Total Loss 35.69983673095703\n",
      "10: Encoding Loss 6.164793968200684, Transition Loss -0.7935384511947632, Classifier Loss 0.17338207364082336, Total Loss 54.326656341552734\n",
      "10: Encoding Loss 3.147259473800659, Transition Loss -0.40182191133499146, Classifier Loss 0.054459694772958755, Total Loss 24.329368591308594\n",
      "10: Encoding Loss 3.129167318344116, Transition Loss -0.5587410926818848, Classifier Loss 0.14597280323505402, Total Loss 33.3720588684082\n",
      "10: Encoding Loss 5.510505199432373, Transition Loss -2.272001266479492, Classifier Loss 0.11303770542144775, Total Loss 44.36589813232422\n",
      "10: Encoding Loss 4.814456939697266, Transition Loss -1.663499116897583, Classifier Loss 0.16624535620212555, Total Loss 45.510616302490234\n",
      "10: Encoding Loss 5.492912769317627, Transition Loss -2.3243026733398438, Classifier Loss 0.11638005077838898, Total Loss 44.59455108642578\n",
      "10: Encoding Loss 5.66016149520874, Transition Loss -1.734802007675171, Classifier Loss 0.042898811399936676, Total Loss 38.250160217285156\n",
      "10: Encoding Loss 5.56417179107666, Transition Loss -2.7901268005371094, Classifier Loss 0.1093636006116867, Total Loss 44.320274353027344\n",
      "10: Encoding Loss 7.311468601226807, Transition Loss -1.1070568561553955, Classifier Loss 0.15281179547309875, Total Loss 59.14955139160156\n",
      "10: Encoding Loss 6.030116081237793, Transition Loss -1.741292953491211, Classifier Loss 0.14201118052005768, Total Loss 50.38111877441406\n",
      "10: Encoding Loss 5.988543510437012, Transition Loss -0.8358637094497681, Classifier Loss 0.07984521239995956, Total Loss 43.91544723510742\n",
      "10: Encoding Loss 3.972670793533325, Transition Loss -2.282606601715088, Classifier Loss 0.07333838939666748, Total Loss 31.1689510345459\n",
      "10: Encoding Loss 6.498808860778809, Transition Loss -1.6349163055419922, Classifier Loss 0.08745108544826508, Total Loss 47.73731231689453\n",
      "10: Encoding Loss 4.410584449768066, Transition Loss -2.301439046859741, Classifier Loss 0.07257020473480225, Total Loss 33.719608306884766\n",
      "10: Encoding Loss 5.886404991149902, Transition Loss -1.90494704246521, Classifier Loss 0.04528085142374039, Total Loss 39.84575271606445\n",
      "10: Encoding Loss 4.622975826263428, Transition Loss -1.028795599937439, Classifier Loss 0.06326008588075638, Total Loss 34.063453674316406\n",
      "10: Encoding Loss 5.273609161376953, Transition Loss -1.6964473724365234, Classifier Loss 0.11765684187412262, Total Loss 43.40666198730469\n",
      "10: Encoding Loss 3.6885910034179688, Transition Loss 0.18846166133880615, Classifier Loss 0.052598804235458374, Total Loss 27.46681022644043\n",
      "10: Encoding Loss 8.152328491210938, Transition Loss -1.3463579416275024, Classifier Loss 0.17408236861228943, Total Loss 66.32167053222656\n",
      "10: Encoding Loss 8.036466598510742, Transition Loss -2.212981939315796, Classifier Loss 0.14891277253627777, Total Loss 63.109195709228516\n",
      "10: Encoding Loss 7.422776699066162, Transition Loss -1.0640816688537598, Classifier Loss 0.18904364109039307, Total Loss 63.44059753417969\n",
      "10: Encoding Loss 6.0424041748046875, Transition Loss -0.27004775404930115, Classifier Loss 0.11127810925245285, Total Loss 47.38212966918945\n",
      "10: Encoding Loss 7.386360168457031, Transition Loss -0.19768500328063965, Classifier Loss 0.22749829292297363, Total Loss 67.06791687011719\n",
      "10: Encoding Loss 4.542625904083252, Transition Loss 0.23932203650474548, Classifier Loss 0.052838508039712906, Total Loss 32.635337829589844\n",
      "10: Encoding Loss 3.8722240924835205, Transition Loss -0.9185921549797058, Classifier Loss 0.09542608261108398, Total Loss 32.77558898925781\n",
      "10: Encoding Loss 2.694303035736084, Transition Loss 0.45901787281036377, Classifier Loss 0.09176816046237946, Total Loss 25.526243209838867\n",
      "10: Encoding Loss 4.963133335113525, Transition Loss -0.6931658983230591, Classifier Loss 0.1233452558517456, Total Loss 42.1130485534668\n",
      "10: Encoding Loss 5.9464287757873535, Transition Loss -1.7721004486083984, Classifier Loss 0.098012775182724, Total Loss 45.47914123535156\n",
      "10: Encoding Loss 7.0046539306640625, Transition Loss -1.7408241033554077, Classifier Loss 0.18350383639335632, Total Loss 60.37760925292969\n",
      "10: Encoding Loss 4.2793402671813965, Transition Loss -1.7932337522506714, Classifier Loss 0.0668608620762825, Total Loss 32.361412048339844\n",
      "10: Encoding Loss 6.279279708862305, Transition Loss -2.405566692352295, Classifier Loss 0.1790359914302826, Total Loss 55.57831573486328\n",
      "10: Encoding Loss 4.646071910858154, Transition Loss -1.2306573390960693, Classifier Loss 0.06558480858802795, Total Loss 34.43442153930664\n",
      "10: Encoding Loss 4.582695484161377, Transition Loss -1.5617740154266357, Classifier Loss 0.10318870842456818, Total Loss 37.81441879272461\n",
      "10: Encoding Loss 4.491700649261475, Transition Loss -3.2653744220733643, Classifier Loss 0.07874147593975067, Total Loss 34.82304763793945\n",
      "10: Encoding Loss 2.726855516433716, Transition Loss -1.8894999027252197, Classifier Loss 0.062364231795072556, Total Loss 22.5968017578125\n",
      "10: Encoding Loss 5.599200248718262, Transition Loss -1.75978684425354, Classifier Loss 0.09194797277450562, Total Loss 42.7892951965332\n",
      "10: Encoding Loss 5.30467414855957, Transition Loss -1.9892849922180176, Classifier Loss 0.14705497026443481, Total Loss 46.532745361328125\n",
      "10: Encoding Loss 3.193178653717041, Transition Loss -1.6355644464492798, Classifier Loss 0.06984735280275345, Total Loss 26.14315414428711\n",
      "10: Encoding Loss 6.535386085510254, Transition Loss -2.3395254611968994, Classifier Loss 0.07673759013414383, Total Loss 46.8851432800293\n",
      "10: Encoding Loss 4.5293755531311035, Transition Loss -0.9483374357223511, Classifier Loss 0.07001157850027084, Total Loss 34.17703628540039\n",
      "10: Encoding Loss 6.501128196716309, Transition Loss -1.2130857706069946, Classifier Loss 0.10139457881450653, Total Loss 49.14574432373047\n",
      "10: Encoding Loss 6.83806848526001, Transition Loss -1.5581309795379639, Classifier Loss 0.14182330667972565, Total Loss 55.210121154785156\n",
      "10: Encoding Loss 6.313399791717529, Transition Loss -0.8705132007598877, Classifier Loss 0.09444216638803482, Total Loss 47.32427215576172\n",
      "10: Encoding Loss 4.716270446777344, Transition Loss -1.5616753101348877, Classifier Loss 0.11088436096906662, Total Loss 39.38543701171875\n",
      "10: Encoding Loss 6.045852184295654, Transition Loss -1.8568376302719116, Classifier Loss 0.1061069592833519, Total Loss 46.885066986083984\n",
      "10: Encoding Loss 5.092129230499268, Transition Loss -1.6732299327850342, Classifier Loss 0.06692909449338913, Total Loss 37.245018005371094\n",
      "10: Encoding Loss 2.799323320388794, Transition Loss -0.12145024538040161, Classifier Loss 0.04056752845644951, Total Loss 20.852645874023438\n",
      "10: Encoding Loss 7.301751136779785, Transition Loss -0.18533191084861755, Classifier Loss 0.08691279590129852, Total Loss 52.50171661376953\n",
      "10: Encoding Loss 6.489575386047363, Transition Loss -1.334108591079712, Classifier Loss 0.06606600433588028, Total Loss 45.543521881103516\n",
      "10: Encoding Loss 6.823534965515137, Transition Loss -1.0353695154190063, Classifier Loss 0.03744575008749962, Total Loss 44.68537139892578\n",
      "10: Encoding Loss 4.105686187744141, Transition Loss -0.13073095679283142, Classifier Loss 0.05458898842334747, Total Loss 30.092966079711914\n",
      "10: Encoding Loss 4.071154594421387, Transition Loss -2.1487088203430176, Classifier Loss 0.03523538261651993, Total Loss 27.949607849121094\n",
      "10: Encoding Loss 3.9297332763671875, Transition Loss -1.7379181385040283, Classifier Loss 0.13505446910858154, Total Loss 37.083152770996094\n",
      "10: Encoding Loss 5.442266941070557, Transition Loss -0.4268164038658142, Classifier Loss 0.04186047241091728, Total Loss 36.8394775390625\n",
      "10: Encoding Loss 4.565054416656494, Transition Loss -1.1628667116165161, Classifier Loss 0.03875695914030075, Total Loss 31.26555824279785\n",
      "10: Encoding Loss 4.277407646179199, Transition Loss -0.5631192326545715, Classifier Loss 0.04583855718374252, Total Loss 30.248077392578125\n",
      "10: Encoding Loss 3.6452250480651855, Transition Loss -0.8746928572654724, Classifier Loss 0.06823932379484177, Total Loss 28.694934844970703\n",
      "10: Encoding Loss 6.089938640594482, Transition Loss -1.4387072324752808, Classifier Loss 0.07249805331230164, Total Loss 43.78886413574219\n",
      "10: Encoding Loss 4.4825944900512695, Transition Loss -1.8981786966323853, Classifier Loss 0.07068830728530884, Total Loss 33.96363830566406\n",
      "10: Encoding Loss 3.945397138595581, Transition Loss -2.5342228412628174, Classifier Loss 0.04681282117962837, Total Loss 28.35265350341797\n",
      "10: Encoding Loss 3.798213481903076, Transition Loss -1.5099472999572754, Classifier Loss 0.10832658410072327, Total Loss 33.621337890625\n",
      "10: Encoding Loss 6.082383632659912, Transition Loss -0.7857455015182495, Classifier Loss 0.09439638257026672, Total Loss 45.933631896972656\n",
      "10: Encoding Loss 3.4982857704162598, Transition Loss -0.8163630962371826, Classifier Loss 0.054072462022304535, Total Loss 26.396635055541992\n",
      "10: Encoding Loss 4.355875015258789, Transition Loss -1.1719127893447876, Classifier Loss 0.10356630384922028, Total Loss 36.49141311645508\n",
      "10: Encoding Loss 4.909790992736816, Transition Loss -2.905441999435425, Classifier Loss 0.0793408751487732, Total Loss 37.39167022705078\n",
      "10: Encoding Loss 4.179903984069824, Transition Loss -1.0963305234909058, Classifier Loss 0.04044884070754051, Total Loss 29.123870849609375\n",
      "10: Encoding Loss 3.290722370147705, Transition Loss -0.7292190790176392, Classifier Loss 0.057572923600673676, Total Loss 25.50133514404297\n",
      "10: Encoding Loss 4.625858306884766, Transition Loss -1.7180229425430298, Classifier Loss 0.14341898262500763, Total Loss 42.09636306762695\n",
      "10: Encoding Loss 3.3871865272521973, Transition Loss -2.293591260910034, Classifier Loss 0.07816851884126663, Total Loss 28.139055252075195\n",
      "10: Encoding Loss 3.7444121837615967, Transition Loss -0.9000695943832397, Classifier Loss 0.08367615938186646, Total Loss 30.833730697631836\n",
      "10: Encoding Loss 4.667790412902832, Transition Loss -2.4556150436401367, Classifier Loss 0.06805390119552612, Total Loss 34.811153411865234\n",
      "10: Encoding Loss 5.768406391143799, Transition Loss -1.0432915687561035, Classifier Loss 0.1576203852891922, Total Loss 50.37206268310547\n",
      "10: Encoding Loss 5.394387245178223, Transition Loss -1.6520698070526123, Classifier Loss 0.09536973387002945, Total Loss 41.90263748168945\n",
      "10: Encoding Loss 3.3219103813171387, Transition Loss -2.1551449298858643, Classifier Loss 0.07875479757785797, Total Loss 27.806079864501953\n",
      "10: Encoding Loss 6.3849992752075195, Transition Loss -1.195406436920166, Classifier Loss 0.1737094223499298, Total Loss 55.68046188354492\n",
      "10: Encoding Loss 7.0164947509765625, Transition Loss -1.979250192642212, Classifier Loss 0.22531618177890778, Total Loss 64.62979125976562\n",
      "10: Encoding Loss 7.538830757141113, Transition Loss -1.921057105064392, Classifier Loss 0.12108150124549866, Total Loss 57.340370178222656\n",
      "10: Encoding Loss 7.124722480773926, Transition Loss -2.2714197635650635, Classifier Loss 0.13251768052577972, Total Loss 55.99919891357422\n",
      "10: Encoding Loss 4.442888259887695, Transition Loss -2.9345779418945312, Classifier Loss 0.07831045985221863, Total Loss 34.48720169067383\n",
      "10: Encoding Loss 7.354753017425537, Transition Loss -2.453585624694824, Classifier Loss 0.0769415870308876, Total Loss 51.82169723510742\n",
      "10: Encoding Loss 8.074508666992188, Transition Loss -0.17871025204658508, Classifier Loss 0.12521442770957947, Total Loss 60.96842575073242\n",
      "10: Encoding Loss 6.8368096351623535, Transition Loss -2.563236713409424, Classifier Loss 0.22490155696868896, Total Loss 63.50999069213867\n",
      "10: Encoding Loss 7.536248207092285, Transition Loss -1.9830496311187744, Classifier Loss 0.16729512810707092, Total Loss 61.94621276855469\n",
      "10: Encoding Loss 6.609095573425293, Transition Loss 0.4503164291381836, Classifier Loss 0.05258134379982948, Total Loss 45.09283447265625\n",
      "10: Encoding Loss 5.629312038421631, Transition Loss -1.6417717933654785, Classifier Loss 0.03309863433241844, Total Loss 37.0850830078125\n",
      "10: Encoding Loss 2.9238877296447754, Transition Loss -1.2938599586486816, Classifier Loss 0.08967767655849457, Total Loss 26.510578155517578\n",
      "10: Encoding Loss 4.107658386230469, Transition Loss -0.7194501161575317, Classifier Loss 0.08083499222993851, Total Loss 32.729164123535156\n",
      "10: Encoding Loss 3.9491512775421143, Transition Loss 0.6289779543876648, Classifier Loss 0.060263972729444504, Total Loss 29.972896575927734\n",
      "10: Encoding Loss 2.2087574005126953, Transition Loss -1.2157236337661743, Classifier Loss 0.0351080447435379, Total Loss 16.762863159179688\n",
      "10: Encoding Loss 4.36309814453125, Transition Loss -0.9621406197547913, Classifier Loss 0.15630215406417847, Total Loss 41.80842208862305\n",
      "10: Encoding Loss 3.782927989959717, Transition Loss -0.892532467842102, Classifier Loss 0.1499134749174118, Total Loss 37.68855667114258\n",
      "10: Encoding Loss 7.116616725921631, Transition Loss -0.12232689559459686, Classifier Loss 0.053814977407455444, Total Loss 48.08115005493164\n",
      "10: Encoding Loss 5.267452716827393, Transition Loss -2.315566062927246, Classifier Loss 0.0736766904592514, Total Loss 38.971458435058594\n",
      "10: Encoding Loss 8.237054824829102, Transition Loss -1.2637519836425781, Classifier Loss 0.2861923277378082, Total Loss 78.04106140136719\n",
      "10: Encoding Loss 5.16126012802124, Transition Loss -2.482973575592041, Classifier Loss 0.13245908915996552, Total Loss 44.21247863769531\n",
      "10: Encoding Loss 4.747025489807129, Transition Loss -1.4613640308380127, Classifier Loss 0.046101056039333344, Total Loss 33.0916748046875\n",
      "10: Encoding Loss 6.243549346923828, Transition Loss -1.8367279767990112, Classifier Loss 0.1519162356853485, Total Loss 52.652183532714844\n",
      "10: Encoding Loss 6.826455593109131, Transition Loss -1.7530884742736816, Classifier Loss 0.10502549260854721, Total Loss 51.4605827331543\n",
      "10: Encoding Loss 5.1214799880981445, Transition Loss 0.20011258125305176, Classifier Loss 0.13532595336437225, Total Loss 44.34151840209961\n",
      "10: Encoding Loss 4.154935359954834, Transition Loss 0.734230637550354, Classifier Loss 0.08541500568389893, Total Loss 33.764808654785156\n",
      "10: Encoding Loss 5.060240745544434, Transition Loss -2.1179254055023193, Classifier Loss 0.07780228555202484, Total Loss 38.14082717895508\n",
      "10: Encoding Loss 7.089524745941162, Transition Loss -1.5261467695236206, Classifier Loss 0.10197708010673523, Total Loss 52.734249114990234\n",
      "10: Encoding Loss 5.28057336807251, Transition Loss -0.2573997378349304, Classifier Loss 0.13542547821998596, Total Loss 45.225887298583984\n",
      "10: Encoding Loss 3.246117353439331, Transition Loss -0.42259615659713745, Classifier Loss 0.0415177121758461, Total Loss 23.628307342529297\n",
      "10: Encoding Loss 4.616347789764404, Transition Loss -1.959139108657837, Classifier Loss 0.13550680875778198, Total Loss 41.24798583984375\n",
      "10: Encoding Loss 3.692155122756958, Transition Loss 0.8279762268066406, Classifier Loss 0.08980243653059006, Total Loss 31.464365005493164\n",
      "10: Encoding Loss 6.46698522567749, Transition Loss -1.8780460357666016, Classifier Loss 0.09573942422866821, Total Loss 48.37510299682617\n",
      "10: Encoding Loss 3.735438108444214, Transition Loss -2.2551684379577637, Classifier Loss 0.07286112010478973, Total Loss 29.697839736938477\n",
      "10: Encoding Loss 6.330949783325195, Transition Loss -1.280261516571045, Classifier Loss 0.12690916657447815, Total Loss 50.67610549926758\n",
      "10: Encoding Loss 4.45435094833374, Transition Loss -2.189131498336792, Classifier Loss 0.05673036724328995, Total Loss 32.39826583862305\n",
      "10: Encoding Loss 5.305771827697754, Transition Loss -1.423911213874817, Classifier Loss 0.11243796348571777, Total Loss 43.077857971191406\n",
      "10: Encoding Loss 5.459799289703369, Transition Loss -2.1093363761901855, Classifier Loss 0.11055632680654526, Total Loss 43.8135871887207\n",
      "10: Encoding Loss 4.065461158752441, Transition Loss 0.9234106540679932, Classifier Loss 0.09123264253139496, Total Loss 33.88539505004883\n",
      "10: Encoding Loss 1.9775253534317017, Transition Loss -0.8268441557884216, Classifier Loss 0.06720133125782013, Total Loss 18.5849552154541\n",
      "10: Encoding Loss 9.503591537475586, Transition Loss -2.084991931915283, Classifier Loss 0.10657349228858948, Total Loss 67.67807006835938\n",
      "10: Encoding Loss 5.703271865844727, Transition Loss -1.424412727355957, Classifier Loss 0.09736795723438263, Total Loss 43.95586013793945\n",
      "10: Encoding Loss 4.887198448181152, Transition Loss -0.9365650415420532, Classifier Loss 0.09519646316766739, Total Loss 38.842464447021484\n",
      "10: Encoding Loss 4.52132511138916, Transition Loss -1.1997196674346924, Classifier Loss 0.055630359798669815, Total Loss 32.69050598144531\n",
      "10: Encoding Loss 3.5476956367492676, Transition Loss -1.0792231559753418, Classifier Loss 0.13044823706150055, Total Loss 34.33056640625\n",
      "10: Encoding Loss 4.771582126617432, Transition Loss -1.7421841621398926, Classifier Loss 0.1492680013179779, Total Loss 43.55559539794922\n",
      "10: Encoding Loss 3.3007545471191406, Transition Loss 0.24798785150051117, Classifier Loss 0.050904467701911926, Total Loss 24.994169235229492\n",
      "10: Encoding Loss 6.284543037414551, Transition Loss -2.3593764305114746, Classifier Loss 0.07084912806749344, Total Loss 44.791229248046875\n",
      "10: Encoding Loss 5.429676055908203, Transition Loss -1.1120882034301758, Classifier Loss 0.03384046256542206, Total Loss 35.96165466308594\n",
      "10: Encoding Loss 5.223162651062012, Transition Loss -1.9469921588897705, Classifier Loss 0.06978365778923035, Total Loss 38.316566467285156\n",
      "10: Encoding Loss 4.246606826782227, Transition Loss -0.8923184275627136, Classifier Loss 0.03228660300374031, Total Loss 28.70794677734375\n",
      "10: Encoding Loss 4.214158058166504, Transition Loss 0.15105114877223969, Classifier Loss 0.13466674089431763, Total Loss 38.812042236328125\n",
      "10: Encoding Loss 5.206912040710449, Transition Loss -1.0831660032272339, Classifier Loss 0.05890978127717972, Total Loss 37.13201904296875\n",
      "10: Encoding Loss 3.591468334197998, Transition Loss -2.6664319038391113, Classifier Loss 0.09734988212585449, Total Loss 31.282732009887695\n",
      "10: Encoding Loss 5.230661869049072, Transition Loss -1.6217470169067383, Classifier Loss 0.2002229541540146, Total Loss 51.405616760253906\n",
      "10: Encoding Loss 5.017251014709473, Transition Loss -1.270655870437622, Classifier Loss 0.0963297039270401, Total Loss 39.73596954345703\n",
      "10: Encoding Loss 7.53390645980835, Transition Loss -1.2696008682250977, Classifier Loss 0.09050992131233215, Total Loss 54.25392532348633\n",
      "10: Encoding Loss 4.715881824493408, Transition Loss -0.46189913153648376, Classifier Loss 0.09242536127567291, Total Loss 37.53764343261719\n",
      "10: Encoding Loss 4.206063747406006, Transition Loss 0.14330871403217316, Classifier Loss 0.1057087630033493, Total Loss 35.86458206176758\n",
      "10: Encoding Loss 6.945425510406494, Transition Loss -1.4288228750228882, Classifier Loss 0.08897268772125244, Total Loss 50.569252014160156\n",
      "10: Encoding Loss 5.26290225982666, Transition Loss -2.308568000793457, Classifier Loss 0.14145421981811523, Total Loss 45.7219123840332\n",
      "10: Encoding Loss 7.478455543518066, Transition Loss -2.104890823364258, Classifier Loss 0.12461971491575241, Total Loss 57.33186340332031\n",
      "10: Encoding Loss 5.232049942016602, Transition Loss -2.2167232036590576, Classifier Loss 0.05704187601804733, Total Loss 37.095603942871094\n",
      "10: Encoding Loss 3.872243881225586, Transition Loss -2.309490203857422, Classifier Loss 0.13959752023220062, Total Loss 37.192291259765625\n",
      "10: Encoding Loss 6.662316799163818, Transition Loss -1.0736310482025146, Classifier Loss 0.09256505221128464, Total Loss 49.229976654052734\n",
      "10: Encoding Loss 8.890756607055664, Transition Loss -1.3909268379211426, Classifier Loss 0.1418496072292328, Total Loss 67.52894592285156\n",
      "10: Encoding Loss 5.659987449645996, Transition Loss -1.5112905502319336, Classifier Loss 0.0448199138045311, Total Loss 38.441314697265625\n",
      "10: Encoding Loss 4.304190158843994, Transition Loss -0.17216326296329498, Classifier Loss 0.05462892726063728, Total Loss 31.287965774536133\n",
      "10: Encoding Loss 3.551090717315674, Transition Loss -1.0245691537857056, Classifier Loss 0.0862693339586258, Total Loss 29.933069229125977\n",
      "10: Encoding Loss 5.580113887786865, Transition Loss -1.4880847930908203, Classifier Loss 0.06161121279001236, Total Loss 39.641212463378906\n",
      "10: Encoding Loss 4.003916263580322, Transition Loss -1.0531164407730103, Classifier Loss 0.034639086574316025, Total Loss 27.48698616027832\n",
      "10: Encoding Loss 6.217717170715332, Transition Loss -2.064427614212036, Classifier Loss 0.049499817192554474, Total Loss 42.255462646484375\n",
      "10: Encoding Loss 3.21867299079895, Transition Loss -1.9800431728363037, Classifier Loss 0.08365800976753235, Total Loss 27.677047729492188\n",
      "10: Encoding Loss 8.2058687210083, Transition Loss 0.20013681054115295, Classifier Loss 0.2577778100967407, Total Loss 75.09304809570312\n",
      "10: Encoding Loss 5.06157922744751, Transition Loss -1.1419132947921753, Classifier Loss 0.1189832091331482, Total Loss 42.26734161376953\n",
      "10: Encoding Loss 6.141097068786621, Transition Loss -1.262142300605774, Classifier Loss 0.09212960302829742, Total Loss 46.05904006958008\n",
      "10: Encoding Loss 5.807229042053223, Transition Loss 0.19409222900867462, Classifier Loss 0.098614402115345, Total Loss 44.78245544433594\n",
      "10: Encoding Loss 6.568182945251465, Transition Loss -0.5264143943786621, Classifier Loss 0.20092672109603882, Total Loss 59.50156021118164\n",
      "10: Encoding Loss 5.831788063049316, Transition Loss -1.4169262647628784, Classifier Loss 0.1651870608329773, Total Loss 51.50886917114258\n",
      "10: Encoding Loss 4.3037943840026855, Transition Loss -0.5541672706604004, Classifier Loss 0.08755762875080109, Total Loss 34.57830810546875\n",
      "10: Encoding Loss 4.752812385559082, Transition Loss -2.1426644325256348, Classifier Loss 0.1268574446439743, Total Loss 41.201759338378906\n",
      "10: Encoding Loss 5.061060905456543, Transition Loss -0.7400853633880615, Classifier Loss 0.11367426812648773, Total Loss 41.733497619628906\n",
      "10: Encoding Loss 5.616054534912109, Transition Loss -0.29922419786453247, Classifier Loss 0.10098977386951447, Total Loss 43.79518508911133\n",
      "10: Encoding Loss 3.232313871383667, Transition Loss -0.8893541097640991, Classifier Loss 0.05621189624071121, Total Loss 25.01471710205078\n",
      "10: Encoding Loss 3.7753143310546875, Transition Loss -0.8651365041732788, Classifier Loss 0.11173419654369354, Total Loss 33.82495880126953\n",
      "10: Encoding Loss 3.6393635272979736, Transition Loss -1.9241864681243896, Classifier Loss 0.09206423908472061, Total Loss 31.04183578491211\n",
      "10: Encoding Loss 4.822222709655762, Transition Loss -0.6794710159301758, Classifier Loss 0.04126354306936264, Total Loss 33.05942153930664\n",
      "10: Encoding Loss 5.095598220825195, Transition Loss -1.1151764392852783, Classifier Loss 0.11733560264110565, Total Loss 42.306705474853516\n",
      "10: Encoding Loss 3.8866920471191406, Transition Loss -2.2936410903930664, Classifier Loss 0.09223553538322449, Total Loss 32.54278564453125\n",
      "10: Encoding Loss 4.7184739112854, Transition Loss -1.3508892059326172, Classifier Loss 0.12154939770698547, Total Loss 40.465240478515625\n",
      "10: Encoding Loss 3.8435275554656982, Transition Loss -0.15085038542747498, Classifier Loss 0.040306150913238525, Total Loss 27.091720581054688\n",
      "10: Encoding Loss 3.430136203765869, Transition Loss -0.2911902964115143, Classifier Loss 0.0585830956697464, Total Loss 26.43901252746582\n",
      "10: Encoding Loss 4.835101127624512, Transition Loss -0.5834424495697021, Classifier Loss 0.1950092315673828, Total Loss 48.51129913330078\n",
      "10: Encoding Loss 6.027985572814941, Transition Loss -1.7051458358764648, Classifier Loss 0.1897730678319931, Total Loss 55.14453887939453\n",
      "10: Encoding Loss 3.998298168182373, Transition Loss -0.5248544216156006, Classifier Loss 0.12458954006433487, Total Loss 36.44853591918945\n",
      "10: Encoding Loss 6.343510627746582, Transition Loss -1.6384520530700684, Classifier Loss 0.15128806233406067, Total Loss 53.18921661376953\n",
      "10: Encoding Loss 3.766801357269287, Transition Loss -2.021916151046753, Classifier Loss 0.07286659628152847, Total Loss 29.886659622192383\n",
      "10: Encoding Loss 5.574792861938477, Transition Loss -1.648754358291626, Classifier Loss 0.1228763535618782, Total Loss 45.73573303222656\n",
      "10: Encoding Loss 5.123286724090576, Transition Loss -0.9655351638793945, Classifier Loss 0.040695130825042725, Total Loss 34.8088493347168\n",
      "10: Encoding Loss 3.064295530319214, Transition Loss -1.245754599571228, Classifier Loss 0.0994546189904213, Total Loss 28.330739974975586\n",
      "10: Encoding Loss 3.6565639972686768, Transition Loss -2.2012696266174316, Classifier Loss 0.053372569382190704, Total Loss 27.275760650634766\n",
      "10: Encoding Loss 3.895233631134033, Transition Loss -0.21677334606647491, Classifier Loss 0.1395549774169922, Total Loss 37.326812744140625\n",
      "10: Encoding Loss 7.111277103424072, Transition Loss -0.7454858422279358, Classifier Loss 0.05850668251514435, Total Loss 48.518035888671875\n",
      "10: Encoding Loss 5.605284690856934, Transition Loss -1.4884034395217896, Classifier Loss 0.21232077479362488, Total Loss 54.863189697265625\n",
      "10: Encoding Loss 6.370099067687988, Transition Loss -0.24536022543907166, Classifier Loss 0.14494416117668152, Total Loss 52.71491241455078\n",
      "10: Encoding Loss 5.470572471618652, Transition Loss -2.3007192611694336, Classifier Loss 0.13643988966941833, Total Loss 46.46650695800781\n",
      "10: Encoding Loss 4.949215888977051, Transition Loss -0.8548457622528076, Classifier Loss 0.057466257363557816, Total Loss 35.44157791137695\n",
      "10: Encoding Loss 3.016174793243408, Transition Loss -2.370359182357788, Classifier Loss 0.05885280668735504, Total Loss 23.981382369995117\n",
      "10: Encoding Loss 2.782891273498535, Transition Loss -1.3132139444351196, Classifier Loss 0.05747099965810776, Total Loss 22.44392204284668\n",
      "10: Encoding Loss 3.919045925140381, Transition Loss -2.6332807540893555, Classifier Loss 0.06207221373915672, Total Loss 29.72044563293457\n",
      "10: Encoding Loss 6.0692338943481445, Transition Loss 0.15607498586177826, Classifier Loss 0.04040541872382164, Total Loss 40.51837921142578\n",
      "10: Encoding Loss 5.285123825073242, Transition Loss -0.2495463639497757, Classifier Loss 0.09800856560468674, Total Loss 41.51150131225586\n",
      "10: Encoding Loss 6.719478607177734, Transition Loss -3.4776153564453125, Classifier Loss 0.06774183362722397, Total Loss 47.089664459228516\n",
      "10: Encoding Loss 5.337875843048096, Transition Loss -1.9297077655792236, Classifier Loss 0.09506982564926147, Total Loss 41.533470153808594\n",
      "10: Encoding Loss 4.405894756317139, Transition Loss 0.0779489278793335, Classifier Loss 0.10349083691835403, Total Loss 36.815635681152344\n",
      "10: Encoding Loss 4.321363925933838, Transition Loss -2.3531453609466553, Classifier Loss 0.03925422579050064, Total Loss 29.8526668548584\n",
      "10: Encoding Loss 7.119370937347412, Transition Loss -1.070490837097168, Classifier Loss 0.10583313554525375, Total Loss 53.29911422729492\n",
      "10: Encoding Loss 5.942923069000244, Transition Loss -2.709007740020752, Classifier Loss 0.1614556908607483, Total Loss 51.802024841308594\n",
      "10: Encoding Loss 5.850727081298828, Transition Loss -1.6554235219955444, Classifier Loss 0.10073963552713394, Total Loss 45.17766189575195\n",
      "10: Encoding Loss 6.3107194900512695, Transition Loss -0.6240983605384827, Classifier Loss 0.15143465995788574, Total Loss 53.007537841796875\n",
      "10: Encoding Loss 6.187377452850342, Transition Loss -1.7276504039764404, Classifier Loss 0.025468450039625168, Total Loss 39.6704216003418\n",
      "10: Encoding Loss 5.099157333374023, Transition Loss -1.628316879272461, Classifier Loss 0.17407944798469543, Total Loss 48.00223922729492\n",
      "10: Encoding Loss 3.887545347213745, Transition Loss -0.6964505910873413, Classifier Loss 0.06563106179237366, Total Loss 29.88810157775879\n",
      "10: Encoding Loss 6.455435276031494, Transition Loss -1.9596960544586182, Classifier Loss 0.11619735509157181, Total Loss 50.351566314697266\n",
      "10: Encoding Loss 5.733125686645508, Transition Loss -0.7064604759216309, Classifier Loss 0.1008579432964325, Total Loss 44.48426818847656\n",
      "10: Encoding Loss 4.598679542541504, Transition Loss -0.5082823634147644, Classifier Loss 0.0792156234383583, Total Loss 35.5134391784668\n",
      "10: Encoding Loss 4.359034538269043, Transition Loss -1.1284739971160889, Classifier Loss 0.11455301940441132, Total Loss 37.60906219482422\n",
      "10: Encoding Loss 4.878538608551025, Transition Loss -0.7387624979019165, Classifier Loss 0.1358284056186676, Total Loss 42.85377883911133\n",
      "10: Encoding Loss 5.579922199249268, Transition Loss -0.4427652955055237, Classifier Loss 0.1171727105975151, Total Loss 45.19662857055664\n",
      "10: Encoding Loss 5.041416168212891, Transition Loss -3.027935028076172, Classifier Loss 0.06252141296863556, Total Loss 36.499427795410156\n",
      "10: Encoding Loss 2.6398329734802246, Transition Loss -1.732300043106079, Classifier Loss 0.08440669625997543, Total Loss 24.278976440429688\n",
      "10: Encoding Loss 9.336908340454102, Transition Loss -2.2399518489837646, Classifier Loss 0.14662107825279236, Total Loss 70.68266296386719\n",
      "10: Encoding Loss 6.204326152801514, Transition Loss -0.19664792716503143, Classifier Loss 0.0336977019906044, Total Loss 40.59564971923828\n",
      "10: Encoding Loss 5.767409324645996, Transition Loss -0.8997700214385986, Classifier Loss 0.1684979498386383, Total Loss 51.453895568847656\n",
      "10: Encoding Loss 4.94321870803833, Transition Loss -1.1077806949615479, Classifier Loss 0.1421523541212082, Total Loss 43.874107360839844\n",
      "10: Encoding Loss 6.183061122894287, Transition Loss -0.9414502382278442, Classifier Loss 0.06374970078468323, Total Loss 43.47296142578125\n",
      "10: Encoding Loss 6.709693908691406, Transition Loss -1.5723360776901245, Classifier Loss 0.2781640291213989, Total Loss 68.07394409179688\n",
      "10: Encoding Loss 5.054892539978027, Transition Loss -0.31552472710609436, Classifier Loss 0.08317182958126068, Total Loss 38.64641189575195\n",
      "10: Encoding Loss 5.243330001831055, Transition Loss 0.4845646023750305, Classifier Loss 0.11350945383310318, Total Loss 43.00475311279297\n",
      "10: Encoding Loss 5.199254512786865, Transition Loss -0.582869827747345, Classifier Loss 0.09408720582723618, Total Loss 40.6040153503418\n",
      "10: Encoding Loss 3.932034730911255, Transition Loss 0.2273320108652115, Classifier Loss 0.022756898775696754, Total Loss 25.958831787109375\n",
      "10: Encoding Loss 6.525633335113525, Transition Loss -1.3517000675201416, Classifier Loss 0.075942263007164, Total Loss 46.74748611450195\n",
      "10: Encoding Loss 7.171090602874756, Transition Loss -1.8159589767456055, Classifier Loss 0.19793061912059784, Total Loss 62.818885803222656\n",
      "10: Encoding Loss 4.515430927276611, Transition Loss -1.0392205715179443, Classifier Loss 0.07306592911481857, Total Loss 34.39876174926758\n",
      "10: Encoding Loss 3.7290759086608887, Transition Loss -1.9542044401168823, Classifier Loss 0.07100328058004379, Total Loss 29.474002838134766\n",
      "10: Encoding Loss 4.2622528076171875, Transition Loss -1.3077266216278076, Classifier Loss 0.07172131538391113, Total Loss 32.7451286315918\n",
      "10: Encoding Loss 4.813111782073975, Transition Loss -0.344219446182251, Classifier Loss 0.04545092582702637, Total Loss 33.42362594604492\n",
      "10: Encoding Loss 5.783139705657959, Transition Loss -1.0147171020507812, Classifier Loss 0.16292764246463776, Total Loss 50.99120330810547\n",
      "10: Encoding Loss 3.1304965019226074, Transition Loss -0.3131195306777954, Classifier Loss 0.1238846480846405, Total Loss 31.17131805419922\n",
      "10: Encoding Loss 5.845174789428711, Transition Loss -2.102269411087036, Classifier Loss 0.0796147808432579, Total Loss 43.03168869018555\n",
      "10: Encoding Loss 3.563345432281494, Transition Loss -1.0325613021850586, Classifier Loss 0.04101907089352608, Total Loss 25.4815673828125\n",
      "10: Encoding Loss 4.609735012054443, Transition Loss -2.7541279792785645, Classifier Loss 0.04077211394906044, Total Loss 31.734519958496094\n",
      "10: Encoding Loss 2.111919403076172, Transition Loss -1.5536843538284302, Classifier Loss 0.036415256559848785, Total Loss 16.312421798706055\n",
      "10: Encoding Loss 6.003420829772949, Transition Loss -1.7071338891983032, Classifier Loss 0.12722986936569214, Total Loss 48.74283218383789\n",
      "10: Encoding Loss 4.3174638748168945, Transition Loss -1.8435078859329224, Classifier Loss 0.09211904555559158, Total Loss 35.11595153808594\n",
      "10: Encoding Loss 3.96818208694458, Transition Loss -0.692723274230957, Classifier Loss 0.06503209471702576, Total Loss 30.312026977539062\n",
      "10: Encoding Loss 5.34026575088501, Transition Loss 0.35919708013534546, Classifier Loss 0.08572100847959518, Total Loss 40.75737762451172\n",
      "10: Encoding Loss 6.1030097007751465, Transition Loss -1.784286379814148, Classifier Loss 0.14788757264614105, Total Loss 51.406105041503906\n",
      "10: Encoding Loss 6.709477424621582, Transition Loss -3.7263333797454834, Classifier Loss 0.13950031995773315, Total Loss 54.205406188964844\n",
      "10: Encoding Loss 4.650628566741943, Transition Loss -1.6461492776870728, Classifier Loss 0.08067400753498077, Total Loss 35.97051239013672\n",
      "10: Encoding Loss 5.237616539001465, Transition Loss -2.2540223598480225, Classifier Loss 0.037232499569654465, Total Loss 35.14805221557617\n",
      "10: Encoding Loss 2.7236011028289795, Transition Loss -2.7422919273376465, Classifier Loss 0.044347744435071945, Total Loss 20.775285720825195\n",
      "10: Encoding Loss 4.810696125030518, Transition Loss -2.941390037536621, Classifier Loss 0.16947172582149506, Total Loss 45.81017303466797\n",
      "10: Encoding Loss 4.938329219818115, Transition Loss -0.7768533825874329, Classifier Loss 0.18923619389533997, Total Loss 48.553287506103516\n",
      "10: Encoding Loss 7.277544975280762, Transition Loss -0.4361529052257538, Classifier Loss 0.08535470068454742, Total Loss 52.200565338134766\n",
      "10: Encoding Loss 4.1826276779174805, Transition Loss -1.178434133529663, Classifier Loss 0.041842177510261536, Total Loss 29.27951431274414\n",
      "10: Encoding Loss 3.963383436203003, Transition Loss -1.6647231578826904, Classifier Loss 0.05991886928677559, Total Loss 29.77152442932129\n",
      "10: Encoding Loss 5.334685325622559, Transition Loss -1.543759822845459, Classifier Loss 0.03073028475046158, Total Loss 35.08052444458008\n",
      "10: Encoding Loss 4.412642002105713, Transition Loss -0.9249790906906128, Classifier Loss 0.05632731318473816, Total Loss 32.10821533203125\n",
      "10: Encoding Loss 4.827217102050781, Transition Loss -1.4405595064163208, Classifier Loss 0.0634477362036705, Total Loss 35.30750274658203\n",
      "10: Encoding Loss 4.965002536773682, Transition Loss -1.415980577468872, Classifier Loss 0.05585452541708946, Total Loss 35.37490463256836\n",
      "10: Encoding Loss 5.173499584197998, Transition Loss -1.3529757261276245, Classifier Loss 0.04677912965416908, Total Loss 35.71836853027344\n",
      "10: Encoding Loss 4.652597427368164, Transition Loss -1.2633239030838013, Classifier Loss 0.12391293793916702, Total Loss 40.30637741088867\n",
      "10: Encoding Loss 4.002597808837891, Transition Loss -0.855067253112793, Classifier Loss 0.08903566747903824, Total Loss 32.9188117980957\n",
      "10: Encoding Loss 5.475429534912109, Transition Loss 0.3571850657463074, Classifier Loss 0.11139225214719772, Total Loss 44.13467788696289\n",
      "10: Encoding Loss 4.478024959564209, Transition Loss -1.8750916719436646, Classifier Loss 0.0618709921836853, Total Loss 33.054500579833984\n",
      "10: Encoding Loss 4.64711856842041, Transition Loss -2.034487009048462, Classifier Loss 0.05775163322687149, Total Loss 33.65706253051758\n",
      "10: Encoding Loss 4.3969597816467285, Transition Loss -0.7148006558418274, Classifier Loss 0.1411142498254776, Total Loss 40.49290084838867\n",
      "10: Encoding Loss 3.796628952026367, Transition Loss -0.2167159765958786, Classifier Loss 0.04412351921200752, Total Loss 27.192039489746094\n",
      "10: Encoding Loss 5.133872032165527, Transition Loss -2.159595012664795, Classifier Loss 0.06304378807544708, Total Loss 37.10675048828125\n",
      "10: Encoding Loss 4.025896072387695, Transition Loss -0.876797616481781, Classifier Loss 0.03331037238240242, Total Loss 27.486064910888672\n",
      "10: Encoding Loss 6.324591636657715, Transition Loss 0.08480314910411835, Classifier Loss 0.18978145718574524, Total Loss 56.959617614746094\n",
      "10: Encoding Loss 5.602566719055176, Transition Loss -1.7110196352005005, Classifier Loss 0.08221949636936188, Total Loss 41.836669921875\n",
      "10: Encoding Loss 6.33542013168335, Transition Loss -0.5076363682746887, Classifier Loss 0.14891070127487183, Total Loss 52.90339279174805\n",
      "10: Encoding Loss 4.5257978439331055, Transition Loss -1.1408360004425049, Classifier Loss 0.03052668645977974, Total Loss 30.207002639770508\n",
      "10: Encoding Loss 4.808813571929932, Transition Loss -1.0805250406265259, Classifier Loss 0.14302371442317963, Total Loss 43.154823303222656\n",
      "10: Encoding Loss 2.940068244934082, Transition Loss -1.5391731262207031, Classifier Loss 0.09945598244667053, Total Loss 27.585391998291016\n",
      "10: Encoding Loss 5.586460113525391, Transition Loss -0.3714291453361511, Classifier Loss 0.12529240548610687, Total Loss 46.0478515625\n",
      "10: Encoding Loss 5.587844371795654, Transition Loss -1.3715800046920776, Classifier Loss 0.08932197839021683, Total Loss 42.458717346191406\n",
      "10: Encoding Loss 3.8717048168182373, Transition Loss -0.5006852149963379, Classifier Loss 0.11324141919612885, Total Loss 34.554168701171875\n",
      "10: Encoding Loss 4.745914459228516, Transition Loss -1.3770066499710083, Classifier Loss 0.07853859663009644, Total Loss 36.328800201416016\n",
      "10: Encoding Loss 5.993971824645996, Transition Loss -0.45086827874183655, Classifier Loss 0.06559442728757858, Total Loss 42.52309799194336\n",
      "10: Encoding Loss 3.8150687217712402, Transition Loss 0.09311453998088837, Classifier Loss 0.0659181997179985, Total Loss 29.519479751586914\n",
      "10: Encoding Loss 6.168374538421631, Transition Loss -0.00547538697719574, Classifier Loss 0.15365031361579895, Total Loss 52.37527847290039\n",
      "10: Encoding Loss 5.538143634796143, Transition Loss -1.0848445892333984, Classifier Loss 0.15621916949748993, Total Loss 48.850345611572266\n",
      "10: Encoding Loss 4.5156097412109375, Transition Loss -1.3910192251205444, Classifier Loss 0.07445445656776428, Total Loss 34.53854751586914\n",
      "10: Encoding Loss 4.029139041900635, Transition Loss -1.4971868991851807, Classifier Loss 0.14587140083312988, Total Loss 38.761375427246094\n",
      "10: Encoding Loss 6.171940326690674, Transition Loss -0.036752849817276, Classifier Loss 0.1340274214744568, Total Loss 50.43437194824219\n",
      "10: Encoding Loss 5.277380466461182, Transition Loss -1.5745528936386108, Classifier Loss 0.13647019863128662, Total Loss 45.310672760009766\n",
      "10: Encoding Loss 5.695104598999023, Transition Loss -1.2187496423721313, Classifier Loss 0.14288601279258728, Total Loss 48.458740234375\n",
      "10: Encoding Loss 4.951396942138672, Transition Loss -1.2126617431640625, Classifier Loss 0.07577647268772125, Total Loss 37.285545349121094\n",
      "10: Encoding Loss 5.827126979827881, Transition Loss -1.6136234998703003, Classifier Loss 0.11928282678127289, Total Loss 46.890403747558594\n",
      "10: Encoding Loss 5.54720401763916, Transition Loss -0.8467258810997009, Classifier Loss 0.0932622179389, Total Loss 42.609107971191406\n",
      "10: Encoding Loss 4.505241394042969, Transition Loss -1.2376867532730103, Classifier Loss 0.09380698204040527, Total Loss 36.411651611328125\n",
      "10: Encoding Loss 4.663966178894043, Transition Loss 0.017660081386566162, Classifier Loss 0.040538933128118515, Total Loss 32.04475784301758\n",
      "10: Encoding Loss 3.7379353046417236, Transition Loss -0.2834589183330536, Classifier Loss 0.054260678589344025, Total Loss 27.853567123413086\n",
      "10: Encoding Loss 6.710777282714844, Transition Loss -1.6489814519882202, Classifier Loss 0.15441808104515076, Total Loss 55.705810546875\n",
      "10: Encoding Loss 5.721546649932861, Transition Loss -1.8337281942367554, Classifier Loss 0.12333953380584717, Total Loss 46.66250228881836\n",
      "10: Encoding Loss 3.048344373703003, Transition Loss -1.0053037405014038, Classifier Loss 0.08304605633020401, Total Loss 26.594270706176758\n",
      "10: Encoding Loss 7.131800174713135, Transition Loss -1.6387763023376465, Classifier Loss 0.082806296646595, Total Loss 51.070777893066406\n",
      "10: Encoding Loss 4.420441150665283, Transition Loss -0.8702443838119507, Classifier Loss 0.06516218930482864, Total Loss 33.03852081298828\n",
      "10: Encoding Loss 4.374070644378662, Transition Loss -0.9763314723968506, Classifier Loss 0.042845468968153, Total Loss 30.528581619262695\n",
      "10: Encoding Loss 6.366198539733887, Transition Loss -1.91000497341156, Classifier Loss 0.10636060684919357, Total Loss 48.832489013671875\n",
      "10: Encoding Loss 5.815833568572998, Transition Loss -1.0733861923217773, Classifier Loss 0.19458343088626862, Total Loss 54.3529167175293\n",
      "10: Encoding Loss 5.44881534576416, Transition Loss -1.376882553100586, Classifier Loss 0.0832105502486229, Total Loss 41.01340103149414\n",
      "10: Encoding Loss 4.598536491394043, Transition Loss -0.034792765974998474, Classifier Loss 0.07772299647331238, Total Loss 35.36350631713867\n",
      "10: Encoding Loss 5.060333251953125, Transition Loss -2.3543357849121094, Classifier Loss 0.07981085032224655, Total Loss 38.34214401245117\n",
      "10: Encoding Loss 5.120735168457031, Transition Loss -1.8827508687973022, Classifier Loss 0.08543453365564346, Total Loss 39.26711654663086\n",
      "10: Encoding Loss 6.12111759185791, Transition Loss 0.2530333995819092, Classifier Loss 0.1104215681552887, Total Loss 47.87007522583008\n",
      "10: Encoding Loss 7.581517219543457, Transition Loss -1.3186957836151123, Classifier Loss 0.1370304822921753, Total Loss 59.191627502441406\n",
      "10: Encoding Loss 4.9150824546813965, Transition Loss -2.2605538368225098, Classifier Loss 0.04832867160439491, Total Loss 34.32246017456055\n",
      "10: Encoding Loss 6.011946201324463, Transition Loss -1.0098296403884888, Classifier Loss 0.055223263800144196, Total Loss 41.59360122680664\n",
      "10: Encoding Loss 5.291000843048096, Transition Loss -0.77447909116745, Classifier Loss 0.14492076635360718, Total Loss 46.23777389526367\n",
      "10: Encoding Loss 4.861243724822998, Transition Loss -1.0920720100402832, Classifier Loss 0.03642229735851288, Total Loss 32.80925369262695\n",
      "11: Encoding Loss 3.908606767654419, Transition Loss -1.1383752822875977, Classifier Loss 0.0718877837061882, Total Loss 30.639963150024414\n",
      "11: Encoding Loss 5.952392101287842, Transition Loss -0.20895937085151672, Classifier Loss 0.1100272536277771, Total Loss 46.71699523925781\n",
      "11: Encoding Loss 4.880451202392578, Transition Loss -1.6054176092147827, Classifier Loss 0.07842210680246353, Total Loss 37.1242790222168\n",
      "11: Encoding Loss 5.803623199462891, Transition Loss -1.1138601303100586, Classifier Loss 0.06614536792039871, Total Loss 41.435829162597656\n",
      "11: Encoding Loss 7.778140068054199, Transition Loss 0.4270175099372864, Classifier Loss 0.18806657195091248, Total Loss 65.64630889892578\n",
      "11: Encoding Loss 6.33750581741333, Transition Loss -2.8734471797943115, Classifier Loss 0.056192394345998764, Total Loss 43.64312744140625\n",
      "11: Encoding Loss 4.2431111335754395, Transition Loss -2.091125965118408, Classifier Loss 0.13760648667812347, Total Loss 39.21847915649414\n",
      "11: Encoding Loss 4.1651740074157715, Transition Loss -1.6725950241088867, Classifier Loss 0.09974415600299835, Total Loss 34.96479415893555\n",
      "11: Encoding Loss 4.8830671310424805, Transition Loss -1.1658793687820435, Classifier Loss 0.15374763309955597, Total Loss 44.67270278930664\n",
      "11: Encoding Loss 3.1733105182647705, Transition Loss -0.6876472234725952, Classifier Loss 0.0802428349852562, Total Loss 27.063873291015625\n",
      "11: Encoding Loss 5.945141792297363, Transition Loss -1.4145939350128174, Classifier Loss 0.13646277785301208, Total Loss 49.316566467285156\n",
      "11: Encoding Loss 3.719712018966675, Transition Loss -1.1920289993286133, Classifier Loss 0.0739804208278656, Total Loss 29.715839385986328\n",
      "11: Encoding Loss 2.5106282234191895, Transition Loss -0.35599830746650696, Classifier Loss 0.04136360436677933, Total Loss 19.199987411499023\n",
      "11: Encoding Loss 4.380240440368652, Transition Loss -2.1891047954559326, Classifier Loss 0.07927823066711426, Total Loss 34.20838928222656\n",
      "11: Encoding Loss 3.5995736122131348, Transition Loss -2.168743133544922, Classifier Loss 0.0706305205821991, Total Loss 28.65962791442871\n",
      "11: Encoding Loss 4.410893440246582, Transition Loss -1.4304143190383911, Classifier Loss 0.07567921280860901, Total Loss 34.032711029052734\n",
      "11: Encoding Loss 3.7209486961364746, Transition Loss -1.9168397188186646, Classifier Loss 0.05633150041103363, Total Loss 27.95807647705078\n",
      "11: Encoding Loss 2.700550079345703, Transition Loss 0.4998350143432617, Classifier Loss 0.07270905375480652, Total Loss 23.67413902282715\n",
      "11: Encoding Loss 7.02145528793335, Transition Loss -2.4946296215057373, Classifier Loss 0.09271860867738724, Total Loss 51.39959716796875\n",
      "11: Encoding Loss 4.119800090789795, Transition Loss -0.5856674313545227, Classifier Loss 0.195466548204422, Total Loss 44.26522445678711\n",
      "11: Encoding Loss 6.570095062255859, Transition Loss -2.392481803894043, Classifier Loss 0.22233349084854126, Total Loss 61.65296173095703\n",
      "11: Encoding Loss 5.58281135559082, Transition Loss -1.3125464916229248, Classifier Loss 0.11755041033029556, Total Loss 45.251380920410156\n",
      "11: Encoding Loss 6.448178291320801, Transition Loss -0.5299659967422485, Classifier Loss 0.09850582480430603, Total Loss 48.5394401550293\n",
      "11: Encoding Loss 6.255433082580566, Transition Loss -0.15220224857330322, Classifier Loss 0.10160622745752335, Total Loss 47.69316101074219\n",
      "11: Encoding Loss 5.33579158782959, Transition Loss 0.3435727655887604, Classifier Loss 0.04737452045083046, Total Loss 36.88963317871094\n",
      "11: Encoding Loss 5.056372165679932, Transition Loss -1.9658535718917847, Classifier Loss 0.07726773619651794, Total Loss 38.0642204284668\n",
      "11: Encoding Loss 3.5076701641082764, Transition Loss -0.3765031695365906, Classifier Loss 0.060687948018312454, Total Loss 27.114665985107422\n",
      "11: Encoding Loss 7.4033098220825195, Transition Loss -1.580012559890747, Classifier Loss 0.17489346861839294, Total Loss 61.90857696533203\n",
      "11: Encoding Loss 3.4297454357147217, Transition Loss -2.4342901706695557, Classifier Loss 0.061589889228343964, Total Loss 26.736488342285156\n",
      "11: Encoding Loss 3.584571361541748, Transition Loss -1.399500846862793, Classifier Loss 0.14709918200969696, Total Loss 36.2167854309082\n",
      "11: Encoding Loss 4.926558971405029, Transition Loss -1.8439977169036865, Classifier Loss 0.08449557423591614, Total Loss 38.008174896240234\n",
      "11: Encoding Loss 4.348680019378662, Transition Loss 0.6523343920707703, Classifier Loss 0.053372062742710114, Total Loss 31.69021987915039\n",
      "11: Encoding Loss 6.184855937957764, Transition Loss -1.137406349182129, Classifier Loss 0.1460908055305481, Total Loss 51.71776580810547\n",
      "11: Encoding Loss 3.959853410720825, Transition Loss -1.5087770223617554, Classifier Loss 0.06718447804450989, Total Loss 30.476966857910156\n",
      "11: Encoding Loss 4.031187534332275, Transition Loss -1.3231321573257446, Classifier Loss 0.11487694829702377, Total Loss 35.67428970336914\n",
      "11: Encoding Loss 5.191218376159668, Transition Loss -1.4579899311065674, Classifier Loss 0.22211124002933502, Total Loss 53.357852935791016\n",
      "11: Encoding Loss 4.647610664367676, Transition Loss -1.3784531354904175, Classifier Loss 0.09489387273788452, Total Loss 37.3745002746582\n",
      "11: Encoding Loss 4.848559379577637, Transition Loss -0.34598013758659363, Classifier Loss 0.05407582223415375, Total Loss 34.498802185058594\n",
      "11: Encoding Loss 8.575933456420898, Transition Loss 0.09677693247795105, Classifier Loss 0.20349101722240448, Total Loss 71.84341430664062\n",
      "11: Encoding Loss 7.301025390625, Transition Loss -1.698598861694336, Classifier Loss 0.11587834358215332, Total Loss 55.393306732177734\n",
      "11: Encoding Loss 5.351217746734619, Transition Loss -1.1662242412567139, Classifier Loss 0.12552574276924133, Total Loss 44.65941619873047\n",
      "11: Encoding Loss 6.287501335144043, Transition Loss -1.1945421695709229, Classifier Loss 0.123562291264534, Total Loss 50.08076095581055\n",
      "11: Encoding Loss 5.6864914894104, Transition Loss -1.1911041736602783, Classifier Loss 0.13439533114433289, Total Loss 47.558006286621094\n",
      "11: Encoding Loss 8.353449821472168, Transition Loss -2.417935371398926, Classifier Loss 0.06418608874082565, Total Loss 56.5383415222168\n",
      "11: Encoding Loss 4.952765464782715, Transition Loss -1.9237401485443115, Classifier Loss 0.11310438811779022, Total Loss 41.02626419067383\n",
      "11: Encoding Loss 3.4438014030456543, Transition Loss 0.10638681054115295, Classifier Loss 0.051136575639247894, Total Loss 25.819021224975586\n",
      "11: Encoding Loss 4.149383544921875, Transition Loss -1.1063861846923828, Classifier Loss 0.09601566940546036, Total Loss 34.49742889404297\n",
      "11: Encoding Loss 6.939635276794434, Transition Loss 0.008862599730491638, Classifier Loss 0.08086834847927094, Total Loss 49.72819137573242\n",
      "11: Encoding Loss 7.195578575134277, Transition Loss -0.34512898325920105, Classifier Loss 0.2098933905363083, Total Loss 64.16267395019531\n",
      "11: Encoding Loss 4.208464622497559, Transition Loss -0.262449711561203, Classifier Loss 0.060459915548563004, Total Loss 31.296676635742188\n",
      "11: Encoding Loss 4.760397911071777, Transition Loss -1.6223695278167725, Classifier Loss 0.14086833596229553, Total Loss 42.64857482910156\n",
      "11: Encoding Loss 3.5374107360839844, Transition Loss -1.9169458150863647, Classifier Loss 0.03011162579059601, Total Loss 24.234859466552734\n",
      "11: Encoding Loss 7.233310699462891, Transition Loss -0.9843481183052063, Classifier Loss 0.07617361098527908, Total Loss 51.0168342590332\n",
      "11: Encoding Loss 4.753559112548828, Transition Loss -0.0031465142965316772, Classifier Loss 0.16072978079319, Total Loss 44.594337463378906\n",
      "11: Encoding Loss 2.7408969402313232, Transition Loss -1.9386100769042969, Classifier Loss 0.11154007911682129, Total Loss 27.598615646362305\n",
      "11: Encoding Loss 7.30081844329834, Transition Loss -0.2863732874393463, Classifier Loss 0.05527640879154205, Total Loss 49.33243942260742\n",
      "11: Encoding Loss 7.075796604156494, Transition Loss -0.48808568716049194, Classifier Loss 0.14914347231388092, Total Loss 57.368934631347656\n",
      "11: Encoding Loss 3.7608542442321777, Transition Loss -1.0995674133300781, Classifier Loss 0.0570417158305645, Total Loss 28.268857955932617\n",
      "11: Encoding Loss 10.07823371887207, Transition Loss -2.1120898723602295, Classifier Loss 0.18720561265945435, Total Loss 79.18911743164062\n",
      "11: Encoding Loss 6.272250175476074, Transition Loss -1.6834317445755005, Classifier Loss 0.03937675058841705, Total Loss 41.57050323486328\n",
      "11: Encoding Loss 5.800361633300781, Transition Loss -0.8552142977714539, Classifier Loss 0.08723562955856323, Total Loss 43.525390625\n",
      "11: Encoding Loss 4.140050888061523, Transition Loss -2.3953261375427246, Classifier Loss 0.08209487795829773, Total Loss 33.0488395690918\n",
      "11: Encoding Loss 3.90277099609375, Transition Loss -2.3867263793945312, Classifier Loss 0.11708376556634903, Total Loss 35.12405014038086\n",
      "11: Encoding Loss 5.18502950668335, Transition Loss -4.310022354125977, Classifier Loss 0.09526731818914413, Total Loss 40.63518524169922\n",
      "11: Encoding Loss 4.6975555419921875, Transition Loss -0.8301856517791748, Classifier Loss 0.1140471026301384, Total Loss 39.58971405029297\n",
      "11: Encoding Loss 5.851332664489746, Transition Loss -2.0988495349884033, Classifier Loss 0.20597334206104279, Total Loss 55.704490661621094\n",
      "11: Encoding Loss 3.462258815765381, Transition Loss -1.240151047706604, Classifier Loss 0.2068900316953659, Total Loss 41.462059020996094\n",
      "11: Encoding Loss 5.720051288604736, Transition Loss -1.7107526063919067, Classifier Loss 0.07545488327741623, Total Loss 41.865116119384766\n",
      "11: Encoding Loss 4.283139228820801, Transition Loss 0.015485048294067383, Classifier Loss 0.08923781663179398, Total Loss 34.62881088256836\n",
      "11: Encoding Loss 2.7504687309265137, Transition Loss -0.07718813419342041, Classifier Loss 0.07414059340953827, Total Loss 23.916841506958008\n",
      "11: Encoding Loss 4.72226619720459, Transition Loss -2.2445669174194336, Classifier Loss 0.13922218978405, Total Loss 42.254920959472656\n",
      "11: Encoding Loss 2.7830777168273926, Transition Loss -1.5348671674728394, Classifier Loss 0.10369755327701569, Total Loss 27.067607879638672\n",
      "11: Encoding Loss 2.273467540740967, Transition Loss -2.2224485874176025, Classifier Loss 0.09389098733663559, Total Loss 23.029016494750977\n",
      "11: Encoding Loss 6.779242038726807, Transition Loss -0.36022302508354187, Classifier Loss 0.036194901913404465, Total Loss 44.2947998046875\n",
      "11: Encoding Loss 4.099868297576904, Transition Loss -1.5422499179840088, Classifier Loss 0.0489547923207283, Total Loss 29.49407386779785\n",
      "11: Encoding Loss 5.361821174621582, Transition Loss -1.8425403833389282, Classifier Loss 0.14907421171665192, Total Loss 47.077613830566406\n",
      "11: Encoding Loss 3.849719285964966, Transition Loss -1.9912132024765015, Classifier Loss 0.03798484057188034, Total Loss 26.89600372314453\n",
      "11: Encoding Loss 6.909698486328125, Transition Loss -1.5985569953918457, Classifier Loss 0.08651051670312881, Total Loss 50.10860061645508\n",
      "11: Encoding Loss 4.805544376373291, Transition Loss -1.450699806213379, Classifier Loss 0.1388656198978424, Total Loss 42.7192497253418\n",
      "11: Encoding Loss 2.848675489425659, Transition Loss -1.2962734699249268, Classifier Loss 0.0629674568772316, Total Loss 23.388280868530273\n",
      "11: Encoding Loss 6.6709699630737305, Transition Loss -0.9534621238708496, Classifier Loss 0.06160645931959152, Total Loss 46.18608474731445\n",
      "11: Encoding Loss 5.1928277015686035, Transition Loss -1.0617451667785645, Classifier Loss 0.17249414324760437, Total Loss 48.40595626831055\n",
      "11: Encoding Loss 3.5750393867492676, Transition Loss -0.2146221250295639, Classifier Loss 0.14224985241889954, Total Loss 35.67513656616211\n",
      "11: Encoding Loss 4.72545051574707, Transition Loss -1.1166305541992188, Classifier Loss 0.11529890447854996, Total Loss 39.88214874267578\n",
      "11: Encoding Loss 5.367644309997559, Transition Loss -1.190452218055725, Classifier Loss 0.14255325496196747, Total Loss 46.460716247558594\n",
      "11: Encoding Loss 5.040587902069092, Transition Loss -2.5227510929107666, Classifier Loss 0.1067124456167221, Total Loss 40.913761138916016\n",
      "11: Encoding Loss 4.469347953796387, Transition Loss 0.9035587310791016, Classifier Loss 0.05994822829961777, Total Loss 33.17233657836914\n",
      "11: Encoding Loss 5.327671051025391, Transition Loss -1.6403957605361938, Classifier Loss 0.1636243462562561, Total Loss 48.32780456542969\n",
      "11: Encoding Loss 5.326003074645996, Transition Loss -2.521315813064575, Classifier Loss 0.03576352447271347, Total Loss 35.53136444091797\n",
      "11: Encoding Loss 4.45848274230957, Transition Loss -1.8074805736541748, Classifier Loss 0.12637542188167572, Total Loss 39.38771438598633\n",
      "11: Encoding Loss 3.804769992828369, Transition Loss 0.8755543231964111, Classifier Loss 0.11418406665325165, Total Loss 34.597251892089844\n",
      "11: Encoding Loss 3.0038771629333496, Transition Loss -1.365189790725708, Classifier Loss 0.06412538141012192, Total Loss 24.435256958007812\n",
      "11: Encoding Loss 5.425342559814453, Transition Loss -1.8334054946899414, Classifier Loss 0.07256370037794113, Total Loss 39.80769348144531\n",
      "11: Encoding Loss 4.408651351928711, Transition Loss 1.1026611328125, Classifier Loss 0.06778370589017868, Total Loss 33.67134475708008\n",
      "11: Encoding Loss 3.6972649097442627, Transition Loss -1.855348825454712, Classifier Loss 0.051467567682266235, Total Loss 27.329605102539062\n",
      "11: Encoding Loss 2.7022054195404053, Transition Loss -1.3535288572311401, Classifier Loss 0.10505638271570206, Total Loss 26.71833038330078\n",
      "11: Encoding Loss 3.4633800983428955, Transition Loss 0.2679280638694763, Classifier Loss 0.05879995971918106, Total Loss 26.76744842529297\n",
      "11: Encoding Loss 10.626690864562988, Transition Loss 0.20914548635482788, Classifier Loss 0.2457074224948883, Total Loss 88.41454315185547\n",
      "11: Encoding Loss 6.514984130859375, Transition Loss -1.1614967584609985, Classifier Loss 0.10045722126960754, Total Loss 49.135162353515625\n",
      "11: Encoding Loss 8.327372550964355, Transition Loss -1.0768132209777832, Classifier Loss 0.14998415112495422, Total Loss 64.96222686767578\n",
      "11: Encoding Loss 6.021432399749756, Transition Loss -1.66757333278656, Classifier Loss 0.05381622537970543, Total Loss 41.509552001953125\n",
      "11: Encoding Loss 5.099826335906982, Transition Loss -1.9067778587341309, Classifier Loss 0.05182536691427231, Total Loss 35.780731201171875\n",
      "11: Encoding Loss 4.631679534912109, Transition Loss -0.4515887498855591, Classifier Loss 0.1300736367702484, Total Loss 40.797264099121094\n",
      "11: Encoding Loss 4.371385097503662, Transition Loss -2.197047710418701, Classifier Loss 0.09463048726320267, Total Loss 35.69048309326172\n",
      "11: Encoding Loss 2.908358097076416, Transition Loss -0.1901695728302002, Classifier Loss 0.13106116652488708, Total Loss 30.556190490722656\n",
      "11: Encoding Loss 3.1056034564971924, Transition Loss 0.821911633014679, Classifier Loss 0.04180152714252472, Total Loss 23.14253807067871\n",
      "11: Encoding Loss 6.546736717224121, Transition Loss 0.09455165266990662, Classifier Loss 0.09046099334955215, Total Loss 48.364341735839844\n",
      "11: Encoding Loss 3.856471538543701, Transition Loss -2.269143581390381, Classifier Loss 0.036848537623882294, Total Loss 26.822776794433594\n",
      "11: Encoding Loss 8.926603317260742, Transition Loss -2.0530710220336914, Classifier Loss 0.12085334211587906, Total Loss 65.64413452148438\n",
      "11: Encoding Loss 6.8853912353515625, Transition Loss -1.1420164108276367, Classifier Loss 0.1747729629278183, Total Loss 58.7891845703125\n",
      "11: Encoding Loss 4.0548200607299805, Transition Loss -1.6618683338165283, Classifier Loss 0.06619048863649368, Total Loss 30.94730567932129\n",
      "11: Encoding Loss 6.232663631439209, Transition Loss -0.36027365922927856, Classifier Loss 0.09685574471950531, Total Loss 47.08141326904297\n",
      "11: Encoding Loss 6.491419792175293, Transition Loss -0.9576902389526367, Classifier Loss 0.07461594045162201, Total Loss 46.409732818603516\n",
      "11: Encoding Loss 5.906471252441406, Transition Loss -1.1144015789031982, Classifier Loss 0.03728093206882477, Total Loss 39.166473388671875\n",
      "11: Encoding Loss 6.845250129699707, Transition Loss -1.3538010120391846, Classifier Loss 0.06356243789196014, Total Loss 47.42720413208008\n",
      "11: Encoding Loss 4.206121921539307, Transition Loss -2.6482603549957275, Classifier Loss 0.07890936732292175, Total Loss 33.126609802246094\n",
      "11: Encoding Loss 3.9032607078552246, Transition Loss 0.0004577040672302246, Classifier Loss 0.07704970985651016, Total Loss 31.124719619750977\n",
      "11: Encoding Loss 2.912722110748291, Transition Loss -0.9443531632423401, Classifier Loss 0.05724504217505455, Total Loss 23.20046043395996\n",
      "11: Encoding Loss 4.100125312805176, Transition Loss -1.2848483324050903, Classifier Loss 0.07923044264316559, Total Loss 32.52328109741211\n",
      "11: Encoding Loss 5.472504615783691, Transition Loss -0.25949281454086304, Classifier Loss 0.09911537915468216, Total Loss 42.746463775634766\n",
      "11: Encoding Loss 6.2528977394104, Transition Loss -0.9000762104988098, Classifier Loss 0.19202551245689392, Total Loss 56.719581604003906\n",
      "11: Encoding Loss 4.817197322845459, Transition Loss -1.857163429260254, Classifier Loss 0.12769049406051636, Total Loss 41.67148971557617\n",
      "11: Encoding Loss 6.643121242523193, Transition Loss -1.5382157564163208, Classifier Loss 0.09646352380514145, Total Loss 49.50446701049805\n",
      "11: Encoding Loss 4.357593536376953, Transition Loss -1.885326862335205, Classifier Loss 0.05766017735004425, Total Loss 31.91082763671875\n",
      "11: Encoding Loss 2.609614849090576, Transition Loss -1.3758971691131592, Classifier Loss 0.060716088861227036, Total Loss 21.728748321533203\n",
      "11: Encoding Loss 5.906047821044922, Transition Loss -2.3448522090911865, Classifier Loss 0.06068403273820877, Total Loss 41.503753662109375\n",
      "11: Encoding Loss 6.990242004394531, Transition Loss -2.421454429626465, Classifier Loss 0.18596963584423065, Total Loss 60.537445068359375\n",
      "11: Encoding Loss 4.9900898933410645, Transition Loss -2.162811279296875, Classifier Loss 0.04269890859723091, Total Loss 34.209564208984375\n",
      "11: Encoding Loss 3.129591941833496, Transition Loss -1.151338815689087, Classifier Loss 0.047988731414079666, Total Loss 23.575963973999023\n",
      "11: Encoding Loss 2.511378765106201, Transition Loss -3.670473098754883, Classifier Loss 0.04923880845308304, Total Loss 19.990686416625977\n",
      "11: Encoding Loss 1.4612003564834595, Transition Loss -1.0290846824645996, Classifier Loss 0.1302536129951477, Total Loss 21.792152404785156\n",
      "11: Encoding Loss 5.896193981170654, Transition Loss -1.2497987747192383, Classifier Loss 0.07349903881549835, Total Loss 42.72657012939453\n",
      "11: Encoding Loss 5.313459873199463, Transition Loss -1.7132059335708618, Classifier Loss 0.09164218604564667, Total Loss 41.04429244995117\n",
      "11: Encoding Loss 3.946023941040039, Transition Loss -1.5037381649017334, Classifier Loss 0.04994877055287361, Total Loss 28.670419692993164\n",
      "11: Encoding Loss 6.110592842102051, Transition Loss -1.196977972984314, Classifier Loss 0.1057196781039238, Total Loss 47.23504638671875\n",
      "11: Encoding Loss 6.069785118103027, Transition Loss -1.1135965585708618, Classifier Loss 0.17788094282150269, Total Loss 54.20635986328125\n",
      "11: Encoding Loss 4.752586841583252, Transition Loss 0.1253902018070221, Classifier Loss 0.0640081912279129, Total Loss 34.966495513916016\n",
      "11: Encoding Loss 5.319584846496582, Transition Loss 0.2003863900899887, Classifier Loss 0.10087841004133224, Total Loss 42.08550262451172\n",
      "11: Encoding Loss 3.5001208782196045, Transition Loss -1.7738256454467773, Classifier Loss 0.04687787964940071, Total Loss 25.68780517578125\n",
      "11: Encoding Loss 7.823261260986328, Transition Loss 0.1147678792476654, Classifier Loss 0.05024689808487892, Total Loss 52.010162353515625\n",
      "11: Encoding Loss 5.624648094177246, Transition Loss -1.5032439231872559, Classifier Loss 0.051562368869781494, Total Loss 38.903526306152344\n",
      "11: Encoding Loss 6.151680946350098, Transition Loss -0.4857719838619232, Classifier Loss 0.10063371807336807, Total Loss 46.973262786865234\n",
      "11: Encoding Loss 5.206511497497559, Transition Loss -1.0185304880142212, Classifier Loss 0.039557892829179764, Total Loss 35.19445037841797\n",
      "11: Encoding Loss 5.137871265411377, Transition Loss -0.15090695023536682, Classifier Loss 0.0719430223107338, Total Loss 38.02146911621094\n",
      "11: Encoding Loss 3.670267105102539, Transition Loss -1.222462773323059, Classifier Loss 0.07478642463684082, Total Loss 29.499755859375\n",
      "11: Encoding Loss 5.3173112869262695, Transition Loss -0.8537247180938721, Classifier Loss 0.18781444430351257, Total Loss 50.684967041015625\n",
      "11: Encoding Loss 2.8191640377044678, Transition Loss -1.6463911533355713, Classifier Loss 0.047821931540966034, Total Loss 21.69651985168457\n",
      "11: Encoding Loss 6.484480381011963, Transition Loss -1.220512866973877, Classifier Loss 0.27316975593566895, Total Loss 66.22337341308594\n",
      "11: Encoding Loss 6.032963752746582, Transition Loss -0.6574447751045227, Classifier Loss 0.21288183331489563, Total Loss 57.4857063293457\n",
      "11: Encoding Loss 3.86568021774292, Transition Loss -1.591489553451538, Classifier Loss 0.1279633343219757, Total Loss 35.98978042602539\n",
      "11: Encoding Loss 5.617260456085205, Transition Loss -2.022216320037842, Classifier Loss 0.0681796669960022, Total Loss 40.520721435546875\n",
      "11: Encoding Loss 6.560671329498291, Transition Loss -0.30807846784591675, Classifier Loss 0.08972343057394028, Total Loss 48.33625030517578\n",
      "11: Encoding Loss 7.168572902679443, Transition Loss -0.6852501630783081, Classifier Loss 0.1708269566297531, Total Loss 60.0938606262207\n",
      "11: Encoding Loss 4.87600040435791, Transition Loss -1.9234721660614014, Classifier Loss 0.06698557734489441, Total Loss 35.95378875732422\n",
      "11: Encoding Loss 4.438216686248779, Transition Loss -3.372032642364502, Classifier Loss 0.0801440179347992, Total Loss 34.64235305786133\n",
      "11: Encoding Loss 6.85468053817749, Transition Loss -2.1858768463134766, Classifier Loss 0.15446966886520386, Total Loss 56.574180603027344\n",
      "11: Encoding Loss 5.886448860168457, Transition Loss -0.8514733910560608, Classifier Loss 0.06794895231723785, Total Loss 42.113250732421875\n",
      "11: Encoding Loss 3.8390426635742188, Transition Loss -2.4415016174316406, Classifier Loss 0.08903393149375916, Total Loss 31.93667221069336\n",
      "11: Encoding Loss 2.5982539653778076, Transition Loss -0.929176926612854, Classifier Loss 0.07981788367033005, Total Loss 23.570940017700195\n",
      "11: Encoding Loss 7.20803165435791, Transition Loss -2.0622830390930176, Classifier Loss 0.11005061864852905, Total Loss 54.2524299621582\n",
      "11: Encoding Loss 4.5149078369140625, Transition Loss -0.039327383041381836, Classifier Loss 0.07864050567150116, Total Loss 34.95348358154297\n",
      "11: Encoding Loss 5.977274417877197, Transition Loss -0.6934645175933838, Classifier Loss 0.06255028396844864, Total Loss 42.1183967590332\n",
      "11: Encoding Loss 3.8083465099334717, Transition Loss -0.9810498356819153, Classifier Loss 0.0413389578461647, Total Loss 26.983583450317383\n",
      "11: Encoding Loss 4.131346702575684, Transition Loss -1.477401614189148, Classifier Loss 0.06722097098827362, Total Loss 31.50958824157715\n",
      "11: Encoding Loss 8.000642776489258, Transition Loss 0.15197807550430298, Classifier Loss 0.16070397198200226, Total Loss 64.13504028320312\n",
      "11: Encoding Loss 5.54934024810791, Transition Loss -1.4250812530517578, Classifier Loss 0.15620113909244537, Total Loss 48.91558837890625\n",
      "11: Encoding Loss 5.450068473815918, Transition Loss -3.1275634765625, Classifier Loss 0.14780719578266144, Total Loss 47.479881286621094\n",
      "11: Encoding Loss 3.574251890182495, Transition Loss -2.0642924308776855, Classifier Loss 0.05596698448061943, Total Loss 27.041385650634766\n",
      "11: Encoding Loss 7.419213771820068, Transition Loss -1.8712953329086304, Classifier Loss 0.13635164499282837, Total Loss 58.14970397949219\n",
      "11: Encoding Loss 5.24078369140625, Transition Loss -0.7079824805259705, Classifier Loss 0.04935130476951599, Total Loss 36.37955093383789\n",
      "11: Encoding Loss 4.675182819366455, Transition Loss -1.4707071781158447, Classifier Loss 0.06502526998519897, Total Loss 34.55303955078125\n",
      "11: Encoding Loss 3.749504804611206, Transition Loss -1.9362738132476807, Classifier Loss 0.03615538403391838, Total Loss 26.111793518066406\n",
      "11: Encoding Loss 5.185235977172852, Transition Loss -1.3061528205871582, Classifier Loss 0.17334842681884766, Total Loss 48.445735931396484\n",
      "11: Encoding Loss 5.947964668273926, Transition Loss -3.215925693511963, Classifier Loss 0.1068912073969841, Total Loss 46.37562561035156\n",
      "11: Encoding Loss 4.150623798370361, Transition Loss -0.7857759594917297, Classifier Loss 0.0875331461429596, Total Loss 33.65674591064453\n",
      "11: Encoding Loss 6.661990165710449, Transition Loss -0.10021539032459259, Classifier Loss 0.14217907190322876, Total Loss 54.1898078918457\n",
      "11: Encoding Loss 6.813294887542725, Transition Loss -1.4176812171936035, Classifier Loss 0.1338430792093277, Total Loss 54.263511657714844\n",
      "11: Encoding Loss 5.4612321853637695, Transition Loss -2.6638357639312744, Classifier Loss 0.08685222268104553, Total Loss 41.45155334472656\n",
      "11: Encoding Loss 6.316145420074463, Transition Loss -1.1431186199188232, Classifier Loss 0.03910554200410843, Total Loss 41.806968688964844\n",
      "11: Encoding Loss 5.195873737335205, Transition Loss 0.4428021311759949, Classifier Loss 0.09602611511945724, Total Loss 40.95497512817383\n",
      "11: Encoding Loss 5.485589981079102, Transition Loss -0.6861009001731873, Classifier Loss 0.12392333149909973, Total Loss 45.305599212646484\n",
      "11: Encoding Loss 7.197725772857666, Transition Loss -0.7953237295150757, Classifier Loss 0.05100901424884796, Total Loss 48.28694152832031\n",
      "11: Encoding Loss 4.90737247467041, Transition Loss -2.2524638175964355, Classifier Loss 0.04779078811407089, Total Loss 34.222412109375\n",
      "11: Encoding Loss 6.8184685707092285, Transition Loss -1.5681308507919312, Classifier Loss 0.14615800976753235, Total Loss 55.52598571777344\n",
      "11: Encoding Loss 5.232987403869629, Transition Loss -0.9734671115875244, Classifier Loss 0.1881474256515503, Total Loss 50.2122802734375\n",
      "11: Encoding Loss 5.507265567779541, Transition Loss -2.6916470527648926, Classifier Loss 0.10811931639909744, Total Loss 43.85445022583008\n",
      "11: Encoding Loss 3.1647472381591797, Transition Loss 0.4383401870727539, Classifier Loss 0.09303276985883713, Total Loss 28.46709632873535\n",
      "11: Encoding Loss 3.2494735717773438, Transition Loss -0.6211357116699219, Classifier Loss 0.058873869478702545, Total Loss 25.38397979736328\n",
      "11: Encoding Loss 2.5300304889678955, Transition Loss -1.1078194379806519, Classifier Loss 0.06853135675191879, Total Loss 22.03287696838379\n",
      "11: Encoding Loss 7.688133239746094, Transition Loss -2.1327455043792725, Classifier Loss 0.13648414611816406, Total Loss 59.77635955810547\n",
      "11: Encoding Loss 4.614229202270508, Transition Loss -1.5023953914642334, Classifier Loss 0.09903662651777267, Total Loss 37.588436126708984\n",
      "11: Encoding Loss 6.231452941894531, Transition Loss -1.5985766649246216, Classifier Loss 0.09636802971363068, Total Loss 47.024879455566406\n",
      "11: Encoding Loss 4.4257988929748535, Transition Loss -1.1306297779083252, Classifier Loss 0.11384662240743637, Total Loss 37.939002990722656\n",
      "11: Encoding Loss 4.334671974182129, Transition Loss -1.8172664642333984, Classifier Loss 0.11540185660123825, Total Loss 37.547489166259766\n",
      "11: Encoding Loss 4.130375862121582, Transition Loss -1.7715941667556763, Classifier Loss 0.13481558859348297, Total Loss 38.26310348510742\n",
      "11: Encoding Loss 4.4509406089782715, Transition Loss -1.4025099277496338, Classifier Loss 0.10440400242805481, Total Loss 37.145484924316406\n",
      "11: Encoding Loss 6.543643951416016, Transition Loss -1.0829206705093384, Classifier Loss 0.11234715580940247, Total Loss 50.49614334106445\n",
      "11: Encoding Loss 4.914337635040283, Transition Loss -0.7274487018585205, Classifier Loss 0.22313159704208374, Total Loss 51.79889678955078\n",
      "11: Encoding Loss 6.796839714050293, Transition Loss -1.827648639678955, Classifier Loss 0.09381072968244553, Total Loss 50.161380767822266\n",
      "11: Encoding Loss 4.394464015960693, Transition Loss -1.9146175384521484, Classifier Loss 0.07314316928386688, Total Loss 33.680335998535156\n",
      "11: Encoding Loss 6.055901050567627, Transition Loss -1.5272859334945679, Classifier Loss 0.077754445374012, Total Loss 44.1102409362793\n",
      "11: Encoding Loss 4.368588447570801, Transition Loss -0.9814022779464722, Classifier Loss 0.07680908590555191, Total Loss 33.89204788208008\n",
      "11: Encoding Loss 4.345166206359863, Transition Loss -0.9361478686332703, Classifier Loss 0.05411586910486221, Total Loss 31.482210159301758\n",
      "11: Encoding Loss 3.1870851516723633, Transition Loss -0.44477933645248413, Classifier Loss 0.06605900079011917, Total Loss 25.728233337402344\n",
      "11: Encoding Loss 5.280749320983887, Transition Loss -1.0413768291473389, Classifier Loss 0.07213246077299118, Total Loss 38.8973274230957\n",
      "11: Encoding Loss 5.658578872680664, Transition Loss -0.7396925687789917, Classifier Loss 0.1662178337574005, Total Loss 50.572959899902344\n",
      "11: Encoding Loss 3.539006471633911, Transition Loss -1.5328477621078491, Classifier Loss 0.15805338323116302, Total Loss 37.038761138916016\n",
      "11: Encoding Loss 4.206048011779785, Transition Loss -0.04630224406719208, Classifier Loss 0.11026587337255478, Total Loss 36.262855529785156\n",
      "11: Encoding Loss 7.033591270446777, Transition Loss -1.249882459640503, Classifier Loss 0.19158048927783966, Total Loss 61.35909652709961\n",
      "11: Encoding Loss 6.032774448394775, Transition Loss -0.3126371204853058, Classifier Loss 0.09264085441827774, Total Loss 45.46060562133789\n",
      "11: Encoding Loss 5.097856521606445, Transition Loss -0.8371156454086304, Classifier Loss 0.0421738401055336, Total Loss 34.8041877746582\n",
      "11: Encoding Loss 6.404972076416016, Transition Loss -0.48826563358306885, Classifier Loss 0.11400122195482254, Total Loss 49.82976150512695\n",
      "11: Encoding Loss 4.526195049285889, Transition Loss -3.1266188621520996, Classifier Loss 0.11922571063041687, Total Loss 39.0784912109375\n",
      "11: Encoding Loss 3.2797775268554688, Transition Loss 1.1235431432724, Classifier Loss 0.06263735890388489, Total Loss 26.39181900024414\n",
      "11: Encoding Loss 3.38940167427063, Transition Loss -2.383317232131958, Classifier Loss 0.07014742493629456, Total Loss 27.35019874572754\n",
      "11: Encoding Loss 5.479981899261475, Transition Loss -2.1556081771850586, Classifier Loss 0.029385505244135857, Total Loss 35.81758117675781\n",
      "11: Encoding Loss 6.371181011199951, Transition Loss -1.6822896003723145, Classifier Loss 0.06977274268865585, Total Loss 45.20369338989258\n",
      "11: Encoding Loss 5.943737030029297, Transition Loss -1.193636178970337, Classifier Loss 0.052914563566446304, Total Loss 40.95340347290039\n",
      "11: Encoding Loss 4.3917236328125, Transition Loss -0.8086974620819092, Classifier Loss 0.10353774577379227, Total Loss 36.703792572021484\n",
      "11: Encoding Loss 4.786262035369873, Transition Loss -0.7888311743736267, Classifier Loss 0.04520934820175171, Total Loss 33.23819351196289\n",
      "11: Encoding Loss 3.0095348358154297, Transition Loss -0.7565500736236572, Classifier Loss 0.08931705355644226, Total Loss 26.988611221313477\n",
      "11: Encoding Loss 6.782334804534912, Transition Loss -2.435375928878784, Classifier Loss 0.06571700423955917, Total Loss 47.264739990234375\n",
      "11: Encoding Loss 4.726609706878662, Transition Loss -1.4562726020812988, Classifier Loss 0.0531064011156559, Total Loss 33.669715881347656\n",
      "11: Encoding Loss 6.288197040557861, Transition Loss -1.6308503150939941, Classifier Loss 0.14082223176956177, Total Loss 51.810752868652344\n",
      "11: Encoding Loss 4.398366928100586, Transition Loss -1.4851930141448975, Classifier Loss 0.02638891525566578, Total Loss 29.028501510620117\n",
      "11: Encoding Loss 5.7005839347839355, Transition Loss -0.19455397129058838, Classifier Loss 0.0668034479022026, Total Loss 40.88377380371094\n",
      "11: Encoding Loss 5.0257134437561035, Transition Loss -1.4107279777526855, Classifier Loss 0.12789040803909302, Total Loss 42.94275665283203\n",
      "11: Encoding Loss 4.488116264343262, Transition Loss -1.4545822143554688, Classifier Loss 0.07121367752552032, Total Loss 34.04948425292969\n",
      "11: Encoding Loss 2.9490630626678467, Transition Loss -2.457083225250244, Classifier Loss 0.052421219646930695, Total Loss 22.93552017211914\n",
      "11: Encoding Loss 5.255006313323975, Transition Loss -3.4482245445251465, Classifier Loss 0.06221096217632294, Total Loss 37.749755859375\n",
      "11: Encoding Loss 3.348067045211792, Transition Loss -0.6316052675247192, Classifier Loss 0.13410398364067078, Total Loss 33.49855041503906\n",
      "11: Encoding Loss 5.02419376373291, Transition Loss -0.8593763113021851, Classifier Loss 0.1507338434457779, Total Loss 45.218204498291016\n",
      "11: Encoding Loss 5.592811107635498, Transition Loss -0.5498517155647278, Classifier Loss 0.10285293310880661, Total Loss 43.841941833496094\n",
      "11: Encoding Loss 3.999455690383911, Transition Loss -0.4205781817436218, Classifier Loss 0.048800837248563766, Total Loss 28.876649856567383\n",
      "11: Encoding Loss 8.269001007080078, Transition Loss -0.1631929874420166, Classifier Loss 0.12409049272537231, Total Loss 62.02299499511719\n",
      "11: Encoding Loss 4.334856986999512, Transition Loss -0.9848167300224304, Classifier Loss 0.08676012605428696, Total Loss 34.68476486206055\n",
      "11: Encoding Loss 7.892097473144531, Transition Loss -0.36852607131004333, Classifier Loss 0.19368818402290344, Total Loss 66.72126007080078\n",
      "11: Encoding Loss 5.766057014465332, Transition Loss -2.7013087272644043, Classifier Loss 0.1615533083677292, Total Loss 50.75059509277344\n",
      "11: Encoding Loss 5.811527729034424, Transition Loss -1.0438122749328613, Classifier Loss 0.1091018095612526, Total Loss 45.778934478759766\n",
      "11: Encoding Loss 5.649863243103027, Transition Loss -1.861578106880188, Classifier Loss 0.14863678812980652, Total Loss 48.762115478515625\n",
      "11: Encoding Loss 3.246171236038208, Transition Loss -1.2948787212371826, Classifier Loss 0.0730772316455841, Total Loss 26.78423309326172\n",
      "11: Encoding Loss 3.684948682785034, Transition Loss -0.41543132066726685, Classifier Loss 0.08151093125343323, Total Loss 30.260622024536133\n",
      "11: Encoding Loss 4.422473907470703, Transition Loss -0.7818316221237183, Classifier Loss 0.09863169491291046, Total Loss 36.397701263427734\n",
      "11: Encoding Loss 4.367822647094727, Transition Loss -2.7746806144714355, Classifier Loss 0.0693877786397934, Total Loss 33.14460372924805\n",
      "11: Encoding Loss 5.411006450653076, Transition Loss -1.5461831092834473, Classifier Loss 0.12076595425605774, Total Loss 44.54201889038086\n",
      "11: Encoding Loss 4.8817644119262695, Transition Loss -1.3770132064819336, Classifier Loss 0.0885293260216713, Total Loss 38.14297103881836\n",
      "11: Encoding Loss 3.5463130474090576, Transition Loss -0.3211716413497925, Classifier Loss 0.08866408467292786, Total Loss 30.1441593170166\n",
      "11: Encoding Loss 4.130682468414307, Transition Loss -1.5915577411651611, Classifier Loss 0.046076156198978424, Total Loss 29.391075134277344\n",
      "11: Encoding Loss 5.921634674072266, Transition Loss -1.2417632341384888, Classifier Loss 0.12063376605510712, Total Loss 47.592689514160156\n",
      "11: Encoding Loss 4.9159698486328125, Transition Loss 0.15092724561691284, Classifier Loss 0.05729814991354942, Total Loss 35.286006927490234\n",
      "11: Encoding Loss 6.8526434898376465, Transition Loss 0.10888785123825073, Classifier Loss 0.08571143448352814, Total Loss 49.73056411743164\n",
      "11: Encoding Loss 6.398433685302734, Transition Loss -1.7375661134719849, Classifier Loss 0.2623467445373535, Total Loss 64.62458038330078\n",
      "11: Encoding Loss 6.0928544998168945, Transition Loss -1.6967945098876953, Classifier Loss 0.05469876527786255, Total Loss 42.02632522583008\n",
      "11: Encoding Loss 5.1189446449279785, Transition Loss -1.3426134586334229, Classifier Loss 0.10506338626146317, Total Loss 41.2194709777832\n",
      "11: Encoding Loss 4.271843910217285, Transition Loss -1.2609450817108154, Classifier Loss 0.06397562474012375, Total Loss 32.02812194824219\n",
      "11: Encoding Loss 5.556548118591309, Transition Loss -2.1349544525146484, Classifier Loss 0.1836443692445755, Total Loss 51.70287322998047\n",
      "11: Encoding Loss 3.7047603130340576, Transition Loss -1.0334819555282593, Classifier Loss 0.06447749584913254, Total Loss 28.675899505615234\n",
      "11: Encoding Loss 6.924239158630371, Transition Loss -2.1476426124572754, Classifier Loss 0.1132781058549881, Total Loss 52.87238693237305\n",
      "11: Encoding Loss 5.733078956604004, Transition Loss -1.4697874784469604, Classifier Loss 0.13384759426116943, Total Loss 47.78264617919922\n",
      "11: Encoding Loss 6.159728527069092, Transition Loss -1.8466999530792236, Classifier Loss 0.05078863352537155, Total Loss 42.0364990234375\n",
      "11: Encoding Loss 6.684211730957031, Transition Loss -0.8448487520217896, Classifier Loss 0.13301658630371094, Total Loss 53.40658950805664\n",
      "11: Encoding Loss 2.516719341278076, Transition Loss -1.9401822090148926, Classifier Loss 0.06036772578954697, Total Loss 21.13631248474121\n",
      "11: Encoding Loss 2.75728702545166, Transition Loss -0.6106318235397339, Classifier Loss 0.04943004995584488, Total Loss 21.486482620239258\n",
      "11: Encoding Loss 5.75819206237793, Transition Loss -0.448148638010025, Classifier Loss 0.09515775740146637, Total Loss 44.06474685668945\n",
      "11: Encoding Loss 3.7989718914031982, Transition Loss 0.48461735248565674, Classifier Loss 0.07172323763370514, Total Loss 30.160003662109375\n",
      "11: Encoding Loss 5.885307788848877, Transition Loss -0.9669013619422913, Classifier Loss 0.15250782668590546, Total Loss 50.5622444152832\n",
      "11: Encoding Loss 5.590665340423584, Transition Loss -1.8964409828186035, Classifier Loss 0.06013091281056404, Total Loss 39.55632781982422\n",
      "11: Encoding Loss 3.7733917236328125, Transition Loss -2.5444459915161133, Classifier Loss 0.10872538387775421, Total Loss 33.511871337890625\n",
      "11: Encoding Loss 3.1701555252075195, Transition Loss -1.5110046863555908, Classifier Loss 0.06412168592214584, Total Loss 25.432497024536133\n",
      "11: Encoding Loss 7.255335807800293, Transition Loss -1.4373583793640137, Classifier Loss 0.17190539836883545, Total Loss 60.721981048583984\n",
      "11: Encoding Loss 4.158153057098389, Transition Loss -0.12869754433631897, Classifier Loss 0.15857993066310883, Total Loss 40.806861877441406\n",
      "11: Encoding Loss 4.604336738586426, Transition Loss -1.4643394947052002, Classifier Loss 0.05380729213356972, Total Loss 33.006160736083984\n",
      "11: Encoding Loss 5.249856948852539, Transition Loss 0.152792826294899, Classifier Loss 0.1313789039850235, Total Loss 44.698150634765625\n",
      "11: Encoding Loss 3.1477222442626953, Transition Loss 0.6093761920928955, Classifier Loss 0.05544035881757736, Total Loss 24.67411994934082\n",
      "11: Encoding Loss 3.7099971771240234, Transition Loss -1.4532816410064697, Classifier Loss 0.07544472068548203, Total Loss 29.80387306213379\n",
      "11: Encoding Loss 6.10337495803833, Transition Loss -0.8735071420669556, Classifier Loss 0.09365515410900116, Total Loss 45.985416412353516\n",
      "11: Encoding Loss 3.6909470558166504, Transition Loss -0.9303615093231201, Classifier Loss 0.12386991083621979, Total Loss 34.53230285644531\n",
      "11: Encoding Loss 6.262280464172363, Transition Loss -1.0119990110397339, Classifier Loss 0.1344158798456192, Total Loss 51.014869689941406\n",
      "11: Encoding Loss 6.575567722320557, Transition Loss -1.2957650423049927, Classifier Loss 0.18533344566822052, Total Loss 57.98623275756836\n",
      "11: Encoding Loss 4.6085991859436035, Transition Loss -1.1746059656143188, Classifier Loss 0.02982902340590954, Total Loss 30.634029388427734\n",
      "11: Encoding Loss 4.749550819396973, Transition Loss -0.4015968441963196, Classifier Loss 0.15758629143238068, Total Loss 44.255775451660156\n",
      "11: Encoding Loss 5.938261032104492, Transition Loss -3.3917124271392822, Classifier Loss 0.16191595792770386, Total Loss 51.819801330566406\n",
      "11: Encoding Loss 6.605014801025391, Transition Loss -1.6075222492218018, Classifier Loss 0.048304516822099686, Total Loss 44.459896087646484\n",
      "11: Encoding Loss 5.059889316558838, Transition Loss -1.2771129608154297, Classifier Loss 0.062208570539951324, Total Loss 36.579681396484375\n",
      "11: Encoding Loss 6.126799583435059, Transition Loss -2.240696430206299, Classifier Loss 0.13802441954612732, Total Loss 50.56234359741211\n",
      "11: Encoding Loss 5.1753411293029785, Transition Loss -0.4437844157218933, Classifier Loss 0.06779792159795761, Total Loss 37.831661224365234\n",
      "11: Encoding Loss 2.944187641143799, Transition Loss -1.5680878162384033, Classifier Loss 0.07601262629032135, Total Loss 25.265762329101562\n",
      "11: Encoding Loss 5.1860551834106445, Transition Loss -1.920189380645752, Classifier Loss 0.11323639750480652, Total Loss 42.439205169677734\n",
      "11: Encoding Loss 9.327963829040527, Transition Loss -3.282518148422241, Classifier Loss 0.16473627090454102, Total Loss 72.44010162353516\n",
      "11: Encoding Loss 5.142982482910156, Transition Loss -1.401595115661621, Classifier Loss 0.053013041615486145, Total Loss 36.15864181518555\n",
      "11: Encoding Loss 5.175050258636475, Transition Loss -0.9746911525726318, Classifier Loss 0.03769771382212639, Total Loss 34.81968307495117\n",
      "11: Encoding Loss 5.954507827758789, Transition Loss -0.21582353115081787, Classifier Loss 0.15478329360485077, Total Loss 51.20528793334961\n",
      "11: Encoding Loss 5.957283020019531, Transition Loss -0.8248658180236816, Classifier Loss 0.25503426790237427, Total Loss 61.246795654296875\n",
      "11: Encoding Loss 6.104299068450928, Transition Loss -1.5735851526260376, Classifier Loss 0.1811409592628479, Total Loss 54.739261627197266\n",
      "11: Encoding Loss 6.462984561920166, Transition Loss -2.1197125911712646, Classifier Loss 0.1474827378988266, Total Loss 53.52533721923828\n",
      "11: Encoding Loss 5.2269744873046875, Transition Loss -1.4150044918060303, Classifier Loss 0.11181779950857162, Total Loss 42.54306411743164\n",
      "11: Encoding Loss 5.183098793029785, Transition Loss -0.5088969469070435, Classifier Loss 0.13132138550281525, Total Loss 44.23052978515625\n",
      "11: Encoding Loss 6.708859920501709, Transition Loss -0.7364640235900879, Classifier Loss 0.16166487336158752, Total Loss 56.41935348510742\n",
      "11: Encoding Loss 4.650673866271973, Transition Loss -0.5452024340629578, Classifier Loss 0.05476531386375427, Total Loss 33.38035583496094\n",
      "11: Encoding Loss 7.542859077453613, Transition Loss -2.0443062782287598, Classifier Loss 0.0710093230009079, Total Loss 52.35727310180664\n",
      "11: Encoding Loss 4.607930660247803, Transition Loss -1.004701852798462, Classifier Loss 0.09621249884366989, Total Loss 37.268436431884766\n",
      "11: Encoding Loss 4.2198166847229, Transition Loss -1.6666641235351562, Classifier Loss 0.08020232617855072, Total Loss 33.33846664428711\n",
      "11: Encoding Loss 5.459293842315674, Transition Loss -0.42920488119125366, Classifier Loss 0.1657424122095108, Total Loss 49.329833984375\n",
      "11: Encoding Loss 4.911981105804443, Transition Loss -1.4293748140335083, Classifier Loss 0.06810975074768066, Total Loss 36.282291412353516\n",
      "11: Encoding Loss 5.344012260437012, Transition Loss -1.4589284658432007, Classifier Loss 0.13446713984012604, Total Loss 45.51020431518555\n",
      "11: Encoding Loss 10.564643859863281, Transition Loss -2.1779282093048096, Classifier Loss 0.12625405192375183, Total Loss 76.01240539550781\n",
      "11: Encoding Loss 5.630133628845215, Transition Loss -0.6903808116912842, Classifier Loss 0.06302580237388611, Total Loss 40.08311080932617\n",
      "11: Encoding Loss 6.594445705413818, Transition Loss 0.8023221492767334, Classifier Loss 0.06268572807312012, Total Loss 46.15618133544922\n",
      "11: Encoding Loss 4.435924053192139, Transition Loss -2.2698609828948975, Classifier Loss 0.10653162002563477, Total Loss 37.267799377441406\n",
      "11: Encoding Loss 6.034518241882324, Transition Loss -1.537329912185669, Classifier Loss 0.11234541237354279, Total Loss 47.441036224365234\n",
      "11: Encoding Loss 3.6547160148620605, Transition Loss -0.5793933868408203, Classifier Loss 0.08339274674654007, Total Loss 30.267337799072266\n",
      "11: Encoding Loss 5.582640647888184, Transition Loss -1.3913437128067017, Classifier Loss 0.18555870652198792, Total Loss 52.0511589050293\n",
      "11: Encoding Loss 9.179365158081055, Transition Loss -1.3181393146514893, Classifier Loss 0.12243136763572693, Total Loss 67.31880187988281\n",
      "11: Encoding Loss 4.570781707763672, Transition Loss -1.7676403522491455, Classifier Loss 0.08287987858057022, Total Loss 35.71197509765625\n",
      "11: Encoding Loss 5.411815166473389, Transition Loss -1.0836795568466187, Classifier Loss 0.06883718073368073, Total Loss 39.35417556762695\n",
      "11: Encoding Loss 5.878950119018555, Transition Loss -2.088402509689331, Classifier Loss 0.04560413211584091, Total Loss 39.83327865600586\n",
      "11: Encoding Loss 5.154365539550781, Transition Loss -2.2331435680389404, Classifier Loss 0.04057987034320831, Total Loss 34.98329162597656\n",
      "11: Encoding Loss 4.380537986755371, Transition Loss -1.0907349586486816, Classifier Loss 0.030647773295640945, Total Loss 29.347570419311523\n",
      "11: Encoding Loss 4.055785655975342, Transition Loss -2.476886034011841, Classifier Loss 0.09750590473413467, Total Loss 34.084312438964844\n",
      "11: Encoding Loss 6.23496675491333, Transition Loss -0.5668683052062988, Classifier Loss 0.10224995762109756, Total Loss 47.63457107543945\n",
      "11: Encoding Loss 5.493117332458496, Transition Loss -0.7535703778266907, Classifier Loss 0.0654517263174057, Total Loss 39.503578186035156\n",
      "11: Encoding Loss 5.3836188316345215, Transition Loss -3.102789878845215, Classifier Loss 0.10184943675994873, Total Loss 42.48542022705078\n",
      "11: Encoding Loss 6.671833038330078, Transition Loss -3.107668399810791, Classifier Loss 0.11659348011016846, Total Loss 51.68910217285156\n",
      "11: Encoding Loss 4.4718780517578125, Transition Loss 0.09086036682128906, Classifier Loss 0.06144317239522934, Total Loss 33.01192855834961\n",
      "11: Encoding Loss 4.74650764465332, Transition Loss -1.9019566774368286, Classifier Loss 0.1428816318511963, Total Loss 42.76645278930664\n",
      "11: Encoding Loss 5.067605495452881, Transition Loss -1.4065154790878296, Classifier Loss 0.06890300661325455, Total Loss 37.29537582397461\n",
      "11: Encoding Loss 3.564020872116089, Transition Loss -1.905318260192871, Classifier Loss 0.06213594600558281, Total Loss 27.59695816040039\n",
      "11: Encoding Loss 5.086864471435547, Transition Loss -2.797414779663086, Classifier Loss 0.14283056557178497, Total Loss 44.80312728881836\n",
      "11: Encoding Loss 6.475087642669678, Transition Loss -2.178027868270874, Classifier Loss 0.09265147894620895, Total Loss 48.11480712890625\n",
      "11: Encoding Loss 6.489772796630859, Transition Loss -2.8370018005371094, Classifier Loss 0.0946020781993866, Total Loss 48.39771270751953\n",
      "11: Encoding Loss 7.287114143371582, Transition Loss -1.1069260835647583, Classifier Loss 0.15833452343940735, Total Loss 59.555694580078125\n",
      "11: Encoding Loss 4.944443702697754, Transition Loss 0.3898759186267853, Classifier Loss 0.09430529922246933, Total Loss 39.25313949584961\n",
      "11: Encoding Loss 4.690771579742432, Transition Loss -2.641615629196167, Classifier Loss 0.07431014627218246, Total Loss 35.574588775634766\n",
      "11: Encoding Loss 5.011326313018799, Transition Loss -1.8255764245986938, Classifier Loss 0.10726738721132278, Total Loss 40.793968200683594\n",
      "11: Encoding Loss 4.503874778747559, Transition Loss -1.6976127624511719, Classifier Loss 0.07669711858034134, Total Loss 34.692283630371094\n",
      "11: Encoding Loss 4.034298896789551, Transition Loss -1.974469542503357, Classifier Loss 0.07857033610343933, Total Loss 32.06203842163086\n",
      "11: Encoding Loss 3.491015672683716, Transition Loss -2.219024419784546, Classifier Loss 0.05223289877176285, Total Loss 26.16849708557129\n",
      "11: Encoding Loss 3.5205166339874268, Transition Loss -0.7167696952819824, Classifier Loss 0.09501267969608307, Total Loss 30.624082565307617\n",
      "11: Encoding Loss 4.969759941101074, Transition Loss -0.8951239585876465, Classifier Loss 0.11712221056222916, Total Loss 41.530426025390625\n",
      "11: Encoding Loss 6.246027946472168, Transition Loss -0.3646244704723358, Classifier Loss 0.09573087096214294, Total Loss 47.049110412597656\n",
      "11: Encoding Loss 4.02581787109375, Transition Loss -0.6232911944389343, Classifier Loss 0.08979552239179611, Total Loss 33.134212493896484\n",
      "11: Encoding Loss 6.95507287979126, Transition Loss -1.2194379568099976, Classifier Loss 0.10880226641893387, Total Loss 52.61017608642578\n",
      "11: Encoding Loss 6.418398857116699, Transition Loss -0.8082603216171265, Classifier Loss 0.118275485932827, Total Loss 50.33761978149414\n",
      "11: Encoding Loss 4.820611000061035, Transition Loss -1.7686386108398438, Classifier Loss 0.1329108029603958, Total Loss 42.21403884887695\n",
      "11: Encoding Loss 5.1127424240112305, Transition Loss -0.5806249380111694, Classifier Loss 0.10819017887115479, Total Loss 41.495243072509766\n",
      "11: Encoding Loss 7.513376712799072, Transition Loss -2.2235312461853027, Classifier Loss 0.15344123542308807, Total Loss 60.42349624633789\n",
      "11: Encoding Loss 4.904402732849121, Transition Loss -1.8133152723312378, Classifier Loss 0.05091148614883423, Total Loss 34.516841888427734\n",
      "11: Encoding Loss 6.932732582092285, Transition Loss -0.979703962802887, Classifier Loss 0.18640390038490295, Total Loss 60.236392974853516\n",
      "11: Encoding Loss 5.670355796813965, Transition Loss -0.47367167472839355, Classifier Loss 0.12537774443626404, Total Loss 46.55971908569336\n",
      "11: Encoding Loss 4.925199508666992, Transition Loss -1.8073883056640625, Classifier Loss 0.11640308797359467, Total Loss 41.19078063964844\n",
      "11: Encoding Loss 4.463591575622559, Transition Loss -2.744518518447876, Classifier Loss 0.1375376284122467, Total Loss 40.534217834472656\n",
      "11: Encoding Loss 6.4253830909729, Transition Loss -2.121445655822754, Classifier Loss 0.12383829057216644, Total Loss 50.935279846191406\n",
      "11: Encoding Loss 4.331755638122559, Transition Loss -0.8281732797622681, Classifier Loss 0.08581602573394775, Total Loss 34.57180404663086\n",
      "11: Encoding Loss 2.9763295650482178, Transition Loss -1.213976502418518, Classifier Loss 0.06766723841428757, Total Loss 24.624216079711914\n",
      "11: Encoding Loss 2.477504014968872, Transition Loss -1.713059663772583, Classifier Loss 0.07112646847963333, Total Loss 21.976987838745117\n",
      "11: Encoding Loss 3.6104462146759033, Transition Loss -1.4323618412017822, Classifier Loss 0.0908394306898117, Total Loss 30.746047973632812\n",
      "11: Encoding Loss 8.72889232635498, Transition Loss 0.7242977023124695, Classifier Loss 0.107759028673172, Total Loss 63.4389762878418\n",
      "11: Encoding Loss 5.709680557250977, Transition Loss 0.19038796424865723, Classifier Loss 0.1441746950149536, Total Loss 48.751708984375\n",
      "11: Encoding Loss 4.118682384490967, Transition Loss 0.6326960325241089, Classifier Loss 0.06586911529302597, Total Loss 31.552085876464844\n",
      "11: Encoding Loss 7.551852226257324, Transition Loss -0.465454638004303, Classifier Loss 0.24238386750221252, Total Loss 69.54931640625\n",
      "11: Encoding Loss 4.6651411056518555, Transition Loss -1.6138778924942017, Classifier Loss 0.05341700091958046, Total Loss 33.331905364990234\n",
      "11: Encoding Loss 4.849581718444824, Transition Loss -2.245893955230713, Classifier Loss 0.10857296735048294, Total Loss 39.95389175415039\n",
      "11: Encoding Loss 5.793031215667725, Transition Loss -1.2230626344680786, Classifier Loss 0.1313030868768692, Total Loss 47.88801193237305\n",
      "11: Encoding Loss 5.09937858581543, Transition Loss -1.4177039861679077, Classifier Loss 0.05854610353708267, Total Loss 36.450313568115234\n",
      "11: Encoding Loss 4.445725440979004, Transition Loss -0.7455306053161621, Classifier Loss 0.05719033628702164, Total Loss 32.393089294433594\n",
      "11: Encoding Loss 5.88187313079834, Transition Loss -0.8046338558197021, Classifier Loss 0.10046012699604034, Total Loss 45.33693313598633\n",
      "11: Encoding Loss 3.6598339080810547, Transition Loss 0.16150681674480438, Classifier Loss 0.0394580252468586, Total Loss 25.96940803527832\n",
      "11: Encoding Loss 6.906996250152588, Transition Loss 0.5602603554725647, Classifier Loss 0.07494374364614487, Total Loss 49.160457611083984\n",
      "11: Encoding Loss 5.5345659255981445, Transition Loss -1.9361625909805298, Classifier Loss 0.10210685431957245, Total Loss 43.41730880737305\n",
      "11: Encoding Loss 4.019702434539795, Transition Loss -1.5753344297409058, Classifier Loss 0.13157951831817627, Total Loss 37.27553939819336\n",
      "11: Encoding Loss 6.205075740814209, Transition Loss -1.9937318563461304, Classifier Loss 0.05431893467903137, Total Loss 42.66155242919922\n",
      "11: Encoding Loss 3.627333164215088, Transition Loss -1.0971155166625977, Classifier Loss 0.10717545449733734, Total Loss 32.48110580444336\n",
      "11: Encoding Loss 2.1354715824127197, Transition Loss -1.6722666025161743, Classifier Loss 0.0888182744383812, Total Loss 21.693986892700195\n",
      "11: Encoding Loss 2.2621850967407227, Transition Loss -1.1270296573638916, Classifier Loss 0.05236266553401947, Total Loss 18.808927536010742\n",
      "11: Encoding Loss 8.547056198120117, Transition Loss -2.1375813484191895, Classifier Loss 0.11252530664205551, Total Loss 62.534019470214844\n",
      "11: Encoding Loss 4.2235517501831055, Transition Loss -1.7518726587295532, Classifier Loss 0.050028957426548004, Total Loss 30.343507766723633\n",
      "11: Encoding Loss 2.5782370567321777, Transition Loss -1.9861441850662231, Classifier Loss 0.0715954601764679, Total Loss 22.628173828125\n",
      "11: Encoding Loss 6.219776630401611, Transition Loss -1.2976574897766113, Classifier Loss 0.12052476406097412, Total Loss 49.37062072753906\n",
      "11: Encoding Loss 3.0307209491729736, Transition Loss -0.9823879599571228, Classifier Loss 0.05889561399817467, Total Loss 24.07349395751953\n",
      "11: Encoding Loss 5.392667293548584, Transition Loss -0.7120904922485352, Classifier Loss 0.09380003064870834, Total Loss 41.735721588134766\n",
      "11: Encoding Loss 5.5959649085998535, Transition Loss -0.5151737928390503, Classifier Loss 0.0675973892211914, Total Loss 40.335323333740234\n",
      "11: Encoding Loss 5.297795295715332, Transition Loss -2.8232345581054688, Classifier Loss 0.1129266545176506, Total Loss 43.07830810546875\n",
      "11: Encoding Loss 5.212080955505371, Transition Loss -0.432010293006897, Classifier Loss 0.11995267122983932, Total Loss 43.267581939697266\n",
      "11: Encoding Loss 4.9861345291137695, Transition Loss -1.5642354488372803, Classifier Loss 0.15384739637374878, Total Loss 45.30092239379883\n",
      "11: Encoding Loss 4.571642875671387, Transition Loss -2.6091582775115967, Classifier Loss 0.09583307057619095, Total Loss 37.012123107910156\n",
      "11: Encoding Loss 3.588404893875122, Transition Loss -1.2203214168548584, Classifier Loss 0.039102569222450256, Total Loss 25.44019889831543\n",
      "11: Encoding Loss 7.4371843338012695, Transition Loss -1.1363580226898193, Classifier Loss 0.14656125009059906, Total Loss 59.278778076171875\n",
      "11: Encoding Loss 6.534882545471191, Transition Loss -0.9079498052597046, Classifier Loss 0.14661535620689392, Total Loss 53.8704719543457\n",
      "11: Encoding Loss 5.185086727142334, Transition Loss -1.1968739032745361, Classifier Loss 0.1448146253824234, Total Loss 45.59150314331055\n",
      "11: Encoding Loss 3.625331401824951, Transition Loss -1.7967329025268555, Classifier Loss 0.06390849500894547, Total Loss 28.142120361328125\n",
      "11: Encoding Loss 9.74507999420166, Transition Loss -1.1547706127166748, Classifier Loss 0.16879597306251526, Total Loss 75.34961700439453\n",
      "11: Encoding Loss 5.451390266418457, Transition Loss -2.5626277923583984, Classifier Loss 0.05906838923692703, Total Loss 38.61415481567383\n",
      "11: Encoding Loss 4.677772521972656, Transition Loss -1.936021327972412, Classifier Loss 0.1416815221309662, Total Loss 42.234012603759766\n",
      "11: Encoding Loss 6.486825466156006, Transition Loss -2.1176583766937256, Classifier Loss 0.16999566555023193, Total Loss 55.919677734375\n",
      "11: Encoding Loss 6.572351932525635, Transition Loss -1.4633086919784546, Classifier Loss 0.12891680002212524, Total Loss 52.3252067565918\n",
      "11: Encoding Loss 5.336331367492676, Transition Loss -0.6283012628555298, Classifier Loss 0.08787355571985245, Total Loss 40.805091857910156\n",
      "11: Encoding Loss 7.083918571472168, Transition Loss -2.7452874183654785, Classifier Loss 0.08522061258554459, Total Loss 51.02447509765625\n",
      "11: Encoding Loss 7.505812168121338, Transition Loss -1.4047882556915283, Classifier Loss 0.18923252820968628, Total Loss 63.95756530761719\n",
      "11: Encoding Loss 7.360466957092285, Transition Loss -2.2305026054382324, Classifier Loss 0.05539635568857193, Total Loss 49.70154571533203\n",
      "11: Encoding Loss 6.476372718811035, Transition Loss -0.5518648624420166, Classifier Loss 0.19903254508972168, Total Loss 58.76127243041992\n",
      "11: Encoding Loss 6.0314621925354, Transition Loss -1.9029991626739502, Classifier Loss 0.05638331547379494, Total Loss 41.82634353637695\n",
      "11: Encoding Loss 5.1614532470703125, Transition Loss -1.3470871448516846, Classifier Loss 0.062018875032663345, Total Loss 37.17007064819336\n",
      "11: Encoding Loss 4.1029462814331055, Transition Loss -2.520451545715332, Classifier Loss 0.059349626302719116, Total Loss 30.551633834838867\n",
      "11: Encoding Loss 4.311928749084473, Transition Loss -0.9990211129188538, Classifier Loss 0.09898383170366287, Total Loss 35.769554138183594\n",
      "11: Encoding Loss 3.9312355518341064, Transition Loss -2.9627676010131836, Classifier Loss 0.07057541608810425, Total Loss 30.643770217895508\n",
      "11: Encoding Loss 7.442498683929443, Transition Loss -0.8734890818595886, Classifier Loss 0.09946724027395248, Total Loss 54.60136795043945\n",
      "11: Encoding Loss 4.209726810455322, Transition Loss -1.8181140422821045, Classifier Loss 0.10831867158412933, Total Loss 36.089500427246094\n",
      "11: Encoding Loss 2.86660099029541, Transition Loss -1.2374444007873535, Classifier Loss 0.04042452573776245, Total Loss 21.241561889648438\n",
      "11: Encoding Loss 6.87762975692749, Transition Loss -2.2271718978881836, Classifier Loss 0.06842000037431717, Total Loss 48.10688781738281\n",
      "11: Encoding Loss 7.6764235496521, Transition Loss -1.4340556859970093, Classifier Loss 0.09054731577634811, Total Loss 55.112701416015625\n",
      "11: Encoding Loss 5.74777364730835, Transition Loss -0.5413410663604736, Classifier Loss 0.12413444370031357, Total Loss 46.899871826171875\n",
      "11: Encoding Loss 5.9602227210998535, Transition Loss -1.7472772598266602, Classifier Loss 0.036993831396102905, Total Loss 39.46002197265625\n",
      "11: Encoding Loss 4.823174476623535, Transition Loss -1.0283586978912354, Classifier Loss 0.11546429991722107, Total Loss 40.48506546020508\n",
      "11: Encoding Loss 5.370780944824219, Transition Loss -1.6424001455307007, Classifier Loss 0.08240976929664612, Total Loss 40.46500778198242\n",
      "11: Encoding Loss 5.762226581573486, Transition Loss -1.1464028358459473, Classifier Loss 0.088582344353199, Total Loss 43.43113708496094\n",
      "11: Encoding Loss 4.113382339477539, Transition Loss -0.872724175453186, Classifier Loss 0.07455535233020782, Total Loss 32.13547897338867\n",
      "11: Encoding Loss 5.980983257293701, Transition Loss -2.0197510719299316, Classifier Loss 0.1452312022447586, Total Loss 50.4082145690918\n",
      "11: Encoding Loss 3.7920913696289062, Transition Loss -1.0959465503692627, Classifier Loss 0.07055673748254776, Total Loss 29.807783126831055\n",
      "11: Encoding Loss 5.0357770919799805, Transition Loss -0.42240872979164124, Classifier Loss 0.041344255208969116, Total Loss 34.34892272949219\n",
      "11: Encoding Loss 6.8644514083862305, Transition Loss -1.436536431312561, Classifier Loss 0.18095549941062927, Total Loss 59.28168487548828\n",
      "11: Encoding Loss 4.993028163909912, Transition Loss -0.9012948870658875, Classifier Loss 0.0445261187851429, Total Loss 34.41041946411133\n",
      "11: Encoding Loss 5.705268859863281, Transition Loss -3.7129273414611816, Classifier Loss 0.06737025082111359, Total Loss 40.96715545654297\n",
      "11: Encoding Loss 4.538567066192627, Transition Loss -1.7392470836639404, Classifier Loss 0.04715150594711304, Total Loss 31.945858001708984\n",
      "11: Encoding Loss 5.368809223175049, Transition Loss -0.29110023379325867, Classifier Loss 0.1594187319278717, Total Loss 48.15461349487305\n",
      "11: Encoding Loss 3.713494062423706, Transition Loss -1.3847529888153076, Classifier Loss 0.09094294905662537, Total Loss 31.374706268310547\n",
      "11: Encoding Loss 7.879312992095947, Transition Loss -1.9475123882293701, Classifier Loss 0.14994913339614868, Total Loss 62.27001190185547\n",
      "11: Encoding Loss 4.921168804168701, Transition Loss -0.7681710720062256, Classifier Loss 0.05393701791763306, Total Loss 34.920406341552734\n",
      "11: Encoding Loss 5.017470359802246, Transition Loss -2.6290087699890137, Classifier Loss 0.09821655601263046, Total Loss 39.9254264831543\n",
      "11: Encoding Loss 4.670083522796631, Transition Loss -2.950169801712036, Classifier Loss 0.12495393306016922, Total Loss 40.51471710205078\n",
      "11: Encoding Loss 4.826254367828369, Transition Loss -1.589099645614624, Classifier Loss 0.11905661225318909, Total Loss 40.862552642822266\n",
      "11: Encoding Loss 5.52668571472168, Transition Loss -1.3132147789001465, Classifier Loss 0.09463614970445633, Total Loss 42.62320327758789\n",
      "11: Encoding Loss 5.181710720062256, Transition Loss -1.944758653640747, Classifier Loss 0.1630467027425766, Total Loss 47.39415740966797\n",
      "11: Encoding Loss 6.018940448760986, Transition Loss -1.3295388221740723, Classifier Loss 0.035770267248153687, Total Loss 39.69013977050781\n",
      "11: Encoding Loss 4.773617744445801, Transition Loss 0.41901248693466187, Classifier Loss 0.05930185317993164, Total Loss 34.739498138427734\n",
      "11: Encoding Loss 5.767216682434082, Transition Loss -1.940485954284668, Classifier Loss 0.05157670006155968, Total Loss 39.760196685791016\n",
      "11: Encoding Loss 4.197876930236816, Transition Loss -1.5565184354782104, Classifier Loss 0.06968125700950623, Total Loss 32.15476608276367\n",
      "11: Encoding Loss 6.470267295837402, Transition Loss -0.4048508405685425, Classifier Loss 0.10612799227237701, Total Loss 49.43424606323242\n",
      "11: Encoding Loss 4.579221725463867, Transition Loss -2.376782178878784, Classifier Loss 0.04053888097405434, Total Loss 31.528270721435547\n",
      "11: Encoding Loss 7.139448642730713, Transition Loss -1.6431708335876465, Classifier Loss 0.21454738080501556, Total Loss 64.290771484375\n",
      "11: Encoding Loss 5.538029193878174, Transition Loss -1.167648434638977, Classifier Loss 0.12166906148195267, Total Loss 45.394615173339844\n",
      "11: Encoding Loss 5.280462741851807, Transition Loss -1.8634071350097656, Classifier Loss 0.057337794452905655, Total Loss 37.41581344604492\n",
      "11: Encoding Loss 4.970510005950928, Transition Loss -0.020179256796836853, Classifier Loss 0.06305884569883347, Total Loss 36.128936767578125\n",
      "11: Encoding Loss 5.32478666305542, Transition Loss -1.1392910480499268, Classifier Loss 0.056036025285720825, Total Loss 37.5518684387207\n",
      "11: Encoding Loss 5.347407341003418, Transition Loss -0.2974802851676941, Classifier Loss 0.12397851794958115, Total Loss 44.482181549072266\n",
      "11: Encoding Loss 7.1640753746032715, Transition Loss -0.4090328812599182, Classifier Loss 0.04332588613033295, Total Loss 47.31687927246094\n",
      "11: Encoding Loss 4.54470157623291, Transition Loss -1.760779619216919, Classifier Loss 0.17621392011642456, Total Loss 44.88889694213867\n",
      "11: Encoding Loss 5.151827812194824, Transition Loss -0.3477727770805359, Classifier Loss 0.06482387334108353, Total Loss 37.393218994140625\n",
      "11: Encoding Loss 4.203829765319824, Transition Loss -0.4705333709716797, Classifier Loss 0.0939682349562645, Total Loss 34.6196174621582\n",
      "11: Encoding Loss 4.734165191650391, Transition Loss -1.783084750175476, Classifier Loss 0.03551764041185379, Total Loss 31.956043243408203\n",
      "11: Encoding Loss 4.781620025634766, Transition Loss -2.0607903003692627, Classifier Loss 0.1738409399986267, Total Loss 46.07299041748047\n",
      "11: Encoding Loss 4.955801486968994, Transition Loss -2.1085758209228516, Classifier Loss 0.13906645774841309, Total Loss 43.6406135559082\n",
      "11: Encoding Loss 3.6560263633728027, Transition Loss -2.873755931854248, Classifier Loss 0.07354449480772018, Total Loss 29.289457321166992\n",
      "11: Encoding Loss 3.968724489212036, Transition Loss -0.6019277572631836, Classifier Loss 0.11552760750055313, Total Loss 35.3648681640625\n",
      "11: Encoding Loss 5.593701362609863, Transition Loss -0.5272674560546875, Classifier Loss 0.11631111800670624, Total Loss 45.193111419677734\n",
      "11: Encoding Loss 3.3455348014831543, Transition Loss -2.245520830154419, Classifier Loss 0.05845433846116066, Total Loss 25.91774559020996\n",
      "11: Encoding Loss 5.127624034881592, Transition Loss 0.21003133058547974, Classifier Loss 0.03582189604640007, Total Loss 34.43194580078125\n",
      "11: Encoding Loss 5.568996429443359, Transition Loss -2.2223188877105713, Classifier Loss 0.08230580389499664, Total Loss 41.64366912841797\n",
      "11: Encoding Loss 4.170196056365967, Transition Loss -0.8988119959831238, Classifier Loss 0.05917161703109741, Total Loss 30.93798065185547\n",
      "11: Encoding Loss 7.919222831726074, Transition Loss 0.5539710521697998, Classifier Loss 0.23310336470603943, Total Loss 71.0472640991211\n",
      "11: Encoding Loss 5.736506462097168, Transition Loss -1.6275449991226196, Classifier Loss 0.05399017781019211, Total Loss 39.817405700683594\n",
      "11: Encoding Loss 6.997816562652588, Transition Loss 0.41500771045684814, Classifier Loss 0.07067420333623886, Total Loss 49.2203254699707\n",
      "11: Encoding Loss 5.064121723175049, Transition Loss -1.7221558094024658, Classifier Loss 0.16155777871608734, Total Loss 46.539817810058594\n",
      "11: Encoding Loss 3.6911888122558594, Transition Loss -1.140285849571228, Classifier Loss 0.09639132022857666, Total Loss 31.785808563232422\n",
      "11: Encoding Loss 6.290988445281982, Transition Loss -1.6911872625350952, Classifier Loss 0.1209840476512909, Total Loss 49.84366226196289\n",
      "11: Encoding Loss 8.718995094299316, Transition Loss -2.8824312686920166, Classifier Loss 0.1825742870569229, Total Loss 70.57025146484375\n",
      "11: Encoding Loss 7.07770299911499, Transition Loss -1.492936611175537, Classifier Loss 0.2787594199180603, Total Loss 70.34156799316406\n",
      "11: Encoding Loss 4.229620456695557, Transition Loss -1.4196507930755615, Classifier Loss 0.1068684458732605, Total Loss 36.06399917602539\n",
      "11: Encoding Loss 7.702253818511963, Transition Loss 0.2778666615486145, Classifier Loss 0.08669343590736389, Total Loss 54.99401092529297\n",
      "11: Encoding Loss 9.320591926574707, Transition Loss -1.61289644241333, Classifier Loss 0.30800172686576843, Total Loss 86.72307586669922\n",
      "11: Encoding Loss 6.661776542663574, Transition Loss -0.8984287977218628, Classifier Loss 0.11783137172460556, Total Loss 51.753440856933594\n",
      "11: Encoding Loss 7.176602363586426, Transition Loss -0.2560330033302307, Classifier Loss 0.12923865020275116, Total Loss 55.98337936401367\n",
      "11: Encoding Loss 7.222151279449463, Transition Loss -0.5321537256240845, Classifier Loss 0.09590242058038712, Total Loss 52.922935485839844\n",
      "11: Encoding Loss 5.41581916809082, Transition Loss -2.0703012943267822, Classifier Loss 0.09640935808420181, Total Loss 42.1350212097168\n",
      "11: Encoding Loss 4.880080223083496, Transition Loss -1.9258686304092407, Classifier Loss 0.11272876709699631, Total Loss 40.552589416503906\n",
      "11: Encoding Loss 6.022133827209473, Transition Loss -3.419218063354492, Classifier Loss 0.13242772221565247, Total Loss 49.37420654296875\n",
      "11: Encoding Loss 4.670610427856445, Transition Loss -2.2726008892059326, Classifier Loss 0.07426157593727112, Total Loss 35.44891357421875\n",
      "11: Encoding Loss 4.289221286773682, Transition Loss -1.3084216117858887, Classifier Loss 0.053936831653118134, Total Loss 31.128488540649414\n",
      "11: Encoding Loss 5.020786285400391, Transition Loss -1.8074156045913696, Classifier Loss 0.17158767580986023, Total Loss 47.28276062011719\n",
      "11: Encoding Loss 3.1651101112365723, Transition Loss -1.8972747325897217, Classifier Loss 0.08255540579557419, Total Loss 27.24544334411621\n",
      "11: Encoding Loss 4.161807060241699, Transition Loss -1.4959615468978882, Classifier Loss 0.11382386833429337, Total Loss 36.35263442993164\n",
      "11: Encoding Loss 6.406004428863525, Transition Loss -1.6654627323150635, Classifier Loss 0.2012317180633545, Total Loss 58.55853271484375\n",
      "11: Encoding Loss 5.196620941162109, Transition Loss -0.42281806468963623, Classifier Loss 0.15237855911254883, Total Loss 46.417415618896484\n",
      "11: Encoding Loss 3.9866366386413574, Transition Loss -1.9381052255630493, Classifier Loss 0.1208590716123581, Total Loss 36.00495529174805\n",
      "11: Encoding Loss 5.521429061889648, Transition Loss -2.243380546569824, Classifier Loss 0.07007194310426712, Total Loss 40.13487243652344\n",
      "11: Encoding Loss 3.8358051776885986, Transition Loss -0.6454692482948303, Classifier Loss 0.0512821227312088, Total Loss 28.142786026000977\n",
      "11: Encoding Loss 5.395778656005859, Transition Loss -1.3783280849456787, Classifier Loss 0.0347190760076046, Total Loss 35.84602737426758\n",
      "11: Encoding Loss 4.387743949890137, Transition Loss -1.4071168899536133, Classifier Loss 0.05343605577945709, Total Loss 31.66950798034668\n",
      "11: Encoding Loss 3.9554004669189453, Transition Loss -0.5469093322753906, Classifier Loss 0.1182592362165451, Total Loss 35.558109283447266\n",
      "11: Encoding Loss 5.679630279541016, Transition Loss -0.726396918296814, Classifier Loss 0.1321336179971695, Total Loss 47.290855407714844\n",
      "11: Encoding Loss 6.956845283508301, Transition Loss -0.1281324177980423, Classifier Loss 0.13735325634479523, Total Loss 55.476348876953125\n",
      "11: Encoding Loss 6.517845153808594, Transition Loss -0.7249442934989929, Classifier Loss 0.096879743039608, Total Loss 48.79475402832031\n",
      "11: Encoding Loss 3.0274107456207275, Transition Loss -2.3040008544921875, Classifier Loss 0.08808038383722305, Total Loss 26.971582412719727\n",
      "11: Encoding Loss 7.019952297210693, Transition Loss -1.4558184146881104, Classifier Loss 0.14960771799087524, Total Loss 57.07990646362305\n",
      "11: Encoding Loss 4.317647457122803, Transition Loss -2.529878616333008, Classifier Loss 0.10489608347415924, Total Loss 36.39448165893555\n",
      "11: Encoding Loss 5.926520347595215, Transition Loss -1.0078567266464233, Classifier Loss 0.0872386246919632, Total Loss 44.2825813293457\n",
      "11: Encoding Loss 7.154850959777832, Transition Loss -2.054874897003174, Classifier Loss 0.18669456243515015, Total Loss 61.59774398803711\n",
      "11: Encoding Loss 5.483745574951172, Transition Loss -2.06059193611145, Classifier Loss 0.1431872844696045, Total Loss 47.22037887573242\n",
      "11: Encoding Loss 7.373487949371338, Transition Loss -0.19293424487113953, Classifier Loss 0.1466037482023239, Total Loss 58.90122604370117\n",
      "11: Encoding Loss 4.675861835479736, Transition Loss 0.1042993813753128, Classifier Loss 0.09265683591365814, Total Loss 37.36257553100586\n",
      "11: Encoding Loss 4.906218528747559, Transition Loss -2.004896640777588, Classifier Loss 0.0866125077009201, Total Loss 38.09776306152344\n",
      "11: Encoding Loss 6.9571661949157715, Transition Loss -0.7342482209205627, Classifier Loss 0.13091447949409485, Total Loss 54.83415603637695\n",
      "11: Encoding Loss 6.194676399230957, Transition Loss -1.3780708312988281, Classifier Loss 0.10699770599603653, Total Loss 47.867279052734375\n",
      "11: Encoding Loss 5.891143798828125, Transition Loss -0.7939615249633789, Classifier Loss 0.12068673968315125, Total Loss 47.415218353271484\n",
      "11: Encoding Loss 4.197262763977051, Transition Loss 0.17620396614074707, Classifier Loss 0.11496396362781525, Total Loss 36.75045394897461\n",
      "11: Encoding Loss 3.2176601886749268, Transition Loss -0.8343456983566284, Classifier Loss 0.050966355949640274, Total Loss 24.402263641357422\n",
      "11: Encoding Loss 5.665739059448242, Transition Loss -1.7314612865447998, Classifier Loss 0.08446474373340607, Total Loss 42.440216064453125\n",
      "11: Encoding Loss 4.691891670227051, Transition Loss -1.2732422351837158, Classifier Loss 0.09697098284959793, Total Loss 37.847938537597656\n",
      "11: Encoding Loss 4.6804094314575195, Transition Loss -0.7060717344284058, Classifier Loss 0.184885174036026, Total Loss 46.57069396972656\n",
      "11: Encoding Loss 4.173936367034912, Transition Loss -2.229299545288086, Classifier Loss 0.05493340268731117, Total Loss 30.536067962646484\n",
      "11: Encoding Loss 4.461968421936035, Transition Loss -2.653529644012451, Classifier Loss 0.11881547421216965, Total Loss 38.65229797363281\n",
      "11: Encoding Loss 1.874474287033081, Transition Loss -0.6745383739471436, Classifier Loss 0.05223023146390915, Total Loss 16.469600677490234\n",
      "11: Encoding Loss 5.00101900100708, Transition Loss -0.7613971829414368, Classifier Loss 0.05469091981649399, Total Loss 35.47490310668945\n",
      "11: Encoding Loss 4.619790554046631, Transition Loss -1.5096027851104736, Classifier Loss 0.12864957749843597, Total Loss 40.583099365234375\n",
      "11: Encoding Loss 3.7866122722625732, Transition Loss -0.6740952134132385, Classifier Loss 0.05674327537417412, Total Loss 28.393733978271484\n",
      "11: Encoding Loss 4.646045684814453, Transition Loss -0.13810622692108154, Classifier Loss 0.032427310943603516, Total Loss 31.11895179748535\n",
      "11: Encoding Loss 7.759664058685303, Transition Loss -2.164365530014038, Classifier Loss 0.18612226843833923, Total Loss 65.16934967041016\n",
      "11: Encoding Loss 4.924002647399902, Transition Loss -0.5908055901527405, Classifier Loss 0.17916226387023926, Total Loss 47.46000671386719\n",
      "11: Encoding Loss 4.954046249389648, Transition Loss -1.1265884637832642, Classifier Loss 0.1855904757976532, Total Loss 48.282875061035156\n",
      "11: Encoding Loss 4.421664237976074, Transition Loss -1.8318042755126953, Classifier Loss 0.07776182889938354, Total Loss 34.30543899536133\n",
      "11: Encoding Loss 6.393226623535156, Transition Loss -1.7050626277923584, Classifier Loss 0.14658868312835693, Total Loss 53.01754379272461\n",
      "11: Encoding Loss 5.527382850646973, Transition Loss -0.9952417612075806, Classifier Loss 0.08770057559013367, Total Loss 41.9339599609375\n",
      "11: Encoding Loss 5.867110252380371, Transition Loss -0.4767013192176819, Classifier Loss 0.059851858764886856, Total Loss 41.187660217285156\n",
      "11: Encoding Loss 4.185666561126709, Transition Loss -1.9537934064865112, Classifier Loss 0.10674378275871277, Total Loss 35.78759765625\n",
      "11: Encoding Loss 6.182307243347168, Transition Loss -0.11079102754592896, Classifier Loss 0.07774803042411804, Total Loss 44.86860275268555\n",
      "11: Encoding Loss 4.663583755493164, Transition Loss -1.63620924949646, Classifier Loss 0.08382602781057358, Total Loss 36.36344909667969\n",
      "11: Encoding Loss 6.010275363922119, Transition Loss -1.3728464841842651, Classifier Loss 0.10555560141801834, Total Loss 46.61666488647461\n",
      "11: Encoding Loss 6.265410423278809, Transition Loss -0.6447191834449768, Classifier Loss 0.0849660113453865, Total Loss 46.08880615234375\n",
      "11: Encoding Loss 4.67294454574585, Transition Loss -1.7803540229797363, Classifier Loss 0.04625129699707031, Total Loss 32.66208267211914\n",
      "11: Encoding Loss 3.9464640617370605, Transition Loss -1.2836095094680786, Classifier Loss 0.08424062281847, Total Loss 32.102333068847656\n",
      "11: Encoding Loss 3.6060147285461426, Transition Loss -1.139548659324646, Classifier Loss 0.08157223463058472, Total Loss 29.792856216430664\n",
      "11: Encoding Loss 7.3731513023376465, Transition Loss -2.4631667137145996, Classifier Loss 0.08682525902986526, Total Loss 52.92045211791992\n",
      "11: Encoding Loss 4.001123428344727, Transition Loss -3.4704818725585938, Classifier Loss 0.05581492558121681, Total Loss 29.58684539794922\n",
      "11: Encoding Loss 2.981764554977417, Transition Loss -1.1566795110702515, Classifier Loss 0.0976877436041832, Total Loss 27.658899307250977\n",
      "11: Encoding Loss 9.937589645385742, Transition Loss 0.3693698048591614, Classifier Loss 0.1671096384525299, Total Loss 76.4842529296875\n",
      "11: Encoding Loss 6.331182479858398, Transition Loss -0.113993339240551, Classifier Loss 0.13210491836071014, Total Loss 51.197540283203125\n",
      "11: Encoding Loss 4.599602699279785, Transition Loss -2.658600091934204, Classifier Loss 0.06940283626317978, Total Loss 34.536834716796875\n",
      "11: Encoding Loss 4.081063747406006, Transition Loss -0.6614854335784912, Classifier Loss 0.0773724913597107, Total Loss 32.22336959838867\n",
      "11: Encoding Loss 6.92218542098999, Transition Loss -2.5490992069244385, Classifier Loss 0.18856389820575714, Total Loss 60.388484954833984\n",
      "11: Encoding Loss 4.694710731506348, Transition Loss -1.1387813091278076, Classifier Loss 0.06522400677204132, Total Loss 34.69021224975586\n",
      "11: Encoding Loss 4.460151195526123, Transition Loss -1.1501777172088623, Classifier Loss 0.07040751725435257, Total Loss 33.80119705200195\n",
      "11: Encoding Loss 4.11143159866333, Transition Loss -1.5288211107254028, Classifier Loss 0.13873828947544098, Total Loss 38.54180908203125\n",
      "11: Encoding Loss 6.066211223602295, Transition Loss -0.5295543074607849, Classifier Loss 0.038729067891836166, Total Loss 40.269962310791016\n",
      "11: Encoding Loss 4.4746294021606445, Transition Loss -1.4003846645355225, Classifier Loss 0.07986927032470703, Total Loss 34.83414077758789\n",
      "11: Encoding Loss 6.563652992248535, Transition Loss -1.9854053258895874, Classifier Loss 0.0939786434173584, Total Loss 48.77899169921875\n",
      "11: Encoding Loss 5.5409345626831055, Transition Loss -0.5906770825386047, Classifier Loss 0.0722736045718193, Total Loss 40.47273254394531\n",
      "11: Encoding Loss 5.637936115264893, Transition Loss -0.2112017720937729, Classifier Loss 0.13879092037677765, Total Loss 47.706626892089844\n",
      "11: Encoding Loss 7.331124782562256, Transition Loss -1.0318188667297363, Classifier Loss 0.04049699753522873, Total Loss 48.036041259765625\n",
      "11: Encoding Loss 5.79935359954834, Transition Loss -1.0345760583877563, Classifier Loss 0.19313521683216095, Total Loss 54.10923385620117\n",
      "11: Encoding Loss 5.185737609863281, Transition Loss -0.5437294244766235, Classifier Loss 0.10450038313865662, Total Loss 41.564247131347656\n",
      "11: Encoding Loss 3.2248635292053223, Transition Loss -0.9472628235816956, Classifier Loss 0.0653727725148201, Total Loss 25.886079788208008\n",
      "11: Encoding Loss 4.92422342300415, Transition Loss -1.5090386867523193, Classifier Loss 0.0737936869263649, Total Loss 36.92410659790039\n",
      "11: Encoding Loss 4.994287967681885, Transition Loss -0.35883331298828125, Classifier Loss 0.08145691454410553, Total Loss 38.11127471923828\n",
      "11: Encoding Loss 4.468380928039551, Transition Loss 0.3962627649307251, Classifier Loss 0.09165119379758835, Total Loss 36.133907318115234\n",
      "11: Encoding Loss 4.341769695281982, Transition Loss -1.7249243259429932, Classifier Loss 0.0779317319393158, Total Loss 33.843101501464844\n",
      "11: Encoding Loss 3.241417407989502, Transition Loss -2.773247718811035, Classifier Loss 0.07788055390119553, Total Loss 27.235450744628906\n",
      "11: Encoding Loss 5.796374320983887, Transition Loss -2.006166934967041, Classifier Loss 0.10605179518461227, Total Loss 45.382625579833984\n",
      "11: Encoding Loss 5.015542507171631, Transition Loss -2.2235336303710938, Classifier Loss 0.14721474051475525, Total Loss 44.813838958740234\n",
      "11: Encoding Loss 5.180941581726074, Transition Loss 0.053117379546165466, Classifier Loss 0.09265632182359695, Total Loss 40.37253189086914\n",
      "11: Encoding Loss 5.299057960510254, Transition Loss -0.9479314088821411, Classifier Loss 0.12998871505260468, Total Loss 44.792842864990234\n",
      "11: Encoding Loss 6.264843940734863, Transition Loss -0.5464726686477661, Classifier Loss 0.15896020829677582, Total Loss 53.484867095947266\n",
      "11: Encoding Loss 5.359747409820557, Transition Loss -0.587669849395752, Classifier Loss 0.1217731460928917, Total Loss 44.33556365966797\n",
      "11: Encoding Loss 4.9268999099731445, Transition Loss -1.1451607942581177, Classifier Loss 0.09092667698860168, Total Loss 38.65361022949219\n",
      "11: Encoding Loss 5.389880180358887, Transition Loss -3.000136613845825, Classifier Loss 0.07550856471061707, Total Loss 39.888938903808594\n",
      "11: Encoding Loss 4.540412425994873, Transition Loss -0.7890792489051819, Classifier Loss 0.05223051831126213, Total Loss 32.4652099609375\n",
      "11: Encoding Loss 5.040106773376465, Transition Loss -1.3979076147079468, Classifier Loss 0.1020161435008049, Total Loss 40.44169616699219\n",
      "11: Encoding Loss 5.6111626625061035, Transition Loss -2.8013789653778076, Classifier Loss 0.053564295172691345, Total Loss 39.02228546142578\n",
      "11: Encoding Loss 6.729008674621582, Transition Loss -0.8060479164123535, Classifier Loss 0.0501096136868, Total Loss 45.38469314575195\n",
      "11: Encoding Loss 3.133439779281616, Transition Loss -0.7425708770751953, Classifier Loss 0.06938621401786804, Total Loss 25.738964080810547\n",
      "11: Encoding Loss 5.223679542541504, Transition Loss -1.5107035636901855, Classifier Loss 0.04370417818427086, Total Loss 35.711891174316406\n",
      "11: Encoding Loss 6.046820640563965, Transition Loss -1.2009919881820679, Classifier Loss 0.06489288806915283, Total Loss 42.76973342895508\n",
      "11: Encoding Loss 3.709016799926758, Transition Loss -0.5725816488265991, Classifier Loss 0.033272188156843185, Total Loss 25.581090927124023\n",
      "11: Encoding Loss 5.075400352478027, Transition Loss -2.7947981357574463, Classifier Loss 0.13408596813678741, Total Loss 43.85988235473633\n",
      "11: Encoding Loss 6.948023796081543, Transition Loss -0.2916452884674072, Classifier Loss 0.10464467853307724, Total Loss 52.15249252319336\n",
      "11: Encoding Loss 6.707023620605469, Transition Loss -0.9372498989105225, Classifier Loss 0.13586226105690002, Total Loss 53.82799530029297\n",
      "11: Encoding Loss 5.750298976898193, Transition Loss -1.4096463918685913, Classifier Loss 0.06132066622376442, Total Loss 40.63330078125\n",
      "11: Encoding Loss 5.314343452453613, Transition Loss -1.5443063974380493, Classifier Loss 0.08026514202356339, Total Loss 39.911956787109375\n",
      "11: Encoding Loss 5.34556245803833, Transition Loss -0.7610337138175964, Classifier Loss 0.1588890105485916, Total Loss 47.961971282958984\n",
      "11: Encoding Loss 6.311191558837891, Transition Loss -1.6921619176864624, Classifier Loss 0.06627777218818665, Total Loss 44.4942512512207\n",
      "11: Encoding Loss 3.39082670211792, Transition Loss -1.2381774187088013, Classifier Loss 0.1286161243915558, Total Loss 33.206077575683594\n",
      "11: Encoding Loss 8.036844253540039, Transition Loss 0.4270353317260742, Classifier Loss 0.13606593012809753, Total Loss 61.99847412109375\n",
      "11: Encoding Loss 4.740323066711426, Transition Loss -1.1225087642669678, Classifier Loss 0.039804838597774506, Total Loss 32.42197036743164\n",
      "11: Encoding Loss 3.024731397628784, Transition Loss -2.341343879699707, Classifier Loss 0.055296167731285095, Total Loss 23.67707061767578\n",
      "11: Encoding Loss 5.968902587890625, Transition Loss -0.19956102967262268, Classifier Loss 0.042188387364149094, Total Loss 40.03217315673828\n",
      "11: Encoding Loss 2.5496878623962402, Transition Loss -0.9994722604751587, Classifier Loss 0.037229474633932114, Total Loss 19.020675659179688\n",
      "11: Encoding Loss 6.789002895355225, Transition Loss -1.806511640548706, Classifier Loss 0.1230410560965538, Total Loss 53.03740310668945\n",
      "11: Encoding Loss 6.036835193634033, Transition Loss -1.0460659265518188, Classifier Loss 0.09617090225219727, Total Loss 45.837684631347656\n",
      "11: Encoding Loss 5.29195499420166, Transition Loss -2.1707749366760254, Classifier Loss 0.06180219724774361, Total Loss 37.93107986450195\n",
      "11: Encoding Loss 3.517040967941284, Transition Loss -1.0939048528671265, Classifier Loss 0.11501995474100113, Total Loss 32.60380172729492\n",
      "11: Encoding Loss 4.640978813171387, Transition Loss -1.495396375656128, Classifier Loss 0.08986504375934601, Total Loss 36.83177947998047\n",
      "11: Encoding Loss 5.326533317565918, Transition Loss -1.6247045993804932, Classifier Loss 0.08252768218517303, Total Loss 40.21132278442383\n",
      "11: Encoding Loss 3.4555230140686035, Transition Loss -0.8086122274398804, Classifier Loss 0.08053039759397507, Total Loss 28.78585433959961\n",
      "11: Encoding Loss 5.8382673263549805, Transition Loss -2.1041948795318604, Classifier Loss 0.1850365847349167, Total Loss 53.53242111206055\n",
      "11: Encoding Loss 3.9583632946014404, Transition Loss -2.0151422023773193, Classifier Loss 0.05050995945930481, Total Loss 28.800371170043945\n",
      "11: Encoding Loss 3.397048234939575, Transition Loss -0.7541663646697998, Classifier Loss 0.06297779083251953, Total Loss 26.679767608642578\n",
      "11: Encoding Loss 5.307626247406006, Transition Loss 0.008715778589248657, Classifier Loss 0.14073532819747925, Total Loss 45.92277908325195\n",
      "11: Encoding Loss 4.79403018951416, Transition Loss -1.7092645168304443, Classifier Loss 0.0864652544260025, Total Loss 37.4100227355957\n",
      "11: Encoding Loss 8.072725296020508, Transition Loss -0.5816270709037781, Classifier Loss 0.09579121321439743, Total Loss 58.01523971557617\n",
      "11: Encoding Loss 4.072185516357422, Transition Loss -2.565260410308838, Classifier Loss 0.18727511167526245, Total Loss 43.15959930419922\n",
      "11: Encoding Loss 5.835427284240723, Transition Loss -1.084766149520874, Classifier Loss 0.08876553177833557, Total Loss 43.8886833190918\n",
      "11: Encoding Loss 4.069306373596191, Transition Loss -1.220634937286377, Classifier Loss 0.04374486580491066, Total Loss 28.789836883544922\n",
      "11: Encoding Loss 8.514185905456543, Transition Loss -0.2163972556591034, Classifier Loss 0.14787600934505463, Total Loss 65.87263488769531\n",
      "11: Encoding Loss 5.29978084564209, Transition Loss -1.2266802787780762, Classifier Loss 0.13167059421539307, Total Loss 44.96525192260742\n",
      "11: Encoding Loss 4.4185357093811035, Transition Loss -1.114161729812622, Classifier Loss 0.07925960421562195, Total Loss 34.436729431152344\n",
      "11: Encoding Loss 4.772848606109619, Transition Loss -2.810330867767334, Classifier Loss 0.09489350765943527, Total Loss 38.12531661987305\n",
      "11: Encoding Loss 3.465909481048584, Transition Loss 0.7128006815910339, Classifier Loss 0.11062951385974884, Total Loss 32.143531799316406\n",
      "11: Encoding Loss 3.54612398147583, Transition Loss -0.1793900430202484, Classifier Loss 0.04662845656275749, Total Loss 25.939517974853516\n",
      "11: Encoding Loss 4.7864990234375, Transition Loss -2.4890084266662598, Classifier Loss 0.13128286600112915, Total Loss 41.84628677368164\n",
      "11: Encoding Loss 5.758923530578613, Transition Loss -1.7262380123138428, Classifier Loss 0.1228436604142189, Total Loss 46.83721923828125\n",
      "11: Encoding Loss 4.118166446685791, Transition Loss -0.7232658267021179, Classifier Loss 0.0853043645620346, Total Loss 33.2391471862793\n",
      "11: Encoding Loss 4.393945693969727, Transition Loss -0.5710354447364807, Classifier Loss 0.060139816254377365, Total Loss 32.3774299621582\n",
      "11: Encoding Loss 4.370113849639893, Transition Loss -1.2156391143798828, Classifier Loss 0.036097269505262375, Total Loss 29.829923629760742\n",
      "11: Encoding Loss 6.950356960296631, Transition Loss -0.18937504291534424, Classifier Loss 0.236821711063385, Total Loss 65.38423919677734\n",
      "11: Encoding Loss 4.532185077667236, Transition Loss -1.2176954746246338, Classifier Loss 0.06591174751520157, Total Loss 33.78379821777344\n",
      "11: Encoding Loss 6.829124450683594, Transition Loss -1.5421580076217651, Classifier Loss 0.07745012640953064, Total Loss 48.71914291381836\n",
      "11: Encoding Loss 4.178980827331543, Transition Loss -0.06147181987762451, Classifier Loss 0.053549785166978836, Total Loss 30.42884063720703\n",
      "11: Encoding Loss 5.628978252410889, Transition Loss -1.628321886062622, Classifier Loss 0.02880169451236725, Total Loss 36.65338897705078\n",
      "11: Encoding Loss 4.942439079284668, Transition Loss -0.8947393298149109, Classifier Loss 0.08521980047225952, Total Loss 38.1762580871582\n",
      "11: Encoding Loss 5.090449333190918, Transition Loss -1.371256709098816, Classifier Loss 0.1969200074672699, Total Loss 50.23414993286133\n",
      "11: Encoding Loss 8.388818740844727, Transition Loss -0.3336453437805176, Classifier Loss 0.21223001182079315, Total Loss 71.5557861328125\n",
      "11: Encoding Loss 6.285349369049072, Transition Loss -1.2936949729919434, Classifier Loss 0.07971923798322678, Total Loss 45.683502197265625\n",
      "11: Encoding Loss 6.645616054534912, Transition Loss -1.2954767942428589, Classifier Loss 0.1622351109981537, Total Loss 56.0966911315918\n",
      "11: Encoding Loss 4.782138824462891, Transition Loss -1.866083025932312, Classifier Loss 0.036404095590114594, Total Loss 32.332496643066406\n",
      "11: Encoding Loss 5.512125015258789, Transition Loss -1.154247760772705, Classifier Loss 0.07048237323760986, Total Loss 40.12052536010742\n",
      "11: Encoding Loss 6.461032867431641, Transition Loss -1.1101211309432983, Classifier Loss 0.08877892047166824, Total Loss 47.643646240234375\n",
      "11: Encoding Loss 4.890964984893799, Transition Loss -1.337241291999817, Classifier Loss 0.11237848550081253, Total Loss 40.583106994628906\n",
      "11: Encoding Loss 3.4180378913879395, Transition Loss 0.44616585969924927, Classifier Loss 0.03406720608472824, Total Loss 24.093416213989258\n",
      "11: Encoding Loss 3.8803858757019043, Transition Loss -0.6303030848503113, Classifier Loss 0.08331289142370224, Total Loss 31.613353729248047\n",
      "11: Encoding Loss 4.405186653137207, Transition Loss 0.3196909427642822, Classifier Loss 0.06529364734888077, Total Loss 33.08835983276367\n",
      "11: Encoding Loss 4.898869514465332, Transition Loss -1.4185469150543213, Classifier Loss 0.060708142817020416, Total Loss 35.463462829589844\n",
      "11: Encoding Loss 7.33957576751709, Transition Loss -0.2853367328643799, Classifier Loss 0.1851462423801422, Total Loss 62.551963806152344\n",
      "11: Encoding Loss 5.245002269744873, Transition Loss -1.8292503356933594, Classifier Loss 0.07262063026428223, Total Loss 38.731346130371094\n",
      "11: Encoding Loss 8.25644302368164, Transition Loss -2.772395372390747, Classifier Loss 0.12060590833425522, Total Loss 61.598140716552734\n",
      "11: Encoding Loss 5.9206953048706055, Transition Loss -1.71751868724823, Classifier Loss 0.06612283736467361, Total Loss 42.135772705078125\n",
      "11: Encoding Loss 3.6883323192596436, Transition Loss -1.1735244989395142, Classifier Loss 0.0815916433930397, Total Loss 30.2886905670166\n",
      "11: Encoding Loss 3.936699390411377, Transition Loss -0.4213573932647705, Classifier Loss 0.018635941669344902, Total Loss 25.483623504638672\n",
      "11: Encoding Loss 6.546303749084473, Transition Loss -3.0762157440185547, Classifier Loss 0.23646217584609985, Total Loss 62.92280960083008\n",
      "11: Encoding Loss 5.1885786056518555, Transition Loss -1.4649620056152344, Classifier Loss 0.11958906799554825, Total Loss 43.08979415893555\n",
      "11: Encoding Loss 3.81756591796875, Transition Loss 0.8396247625350952, Classifier Loss 0.04539991170167923, Total Loss 27.78123664855957\n",
      "11: Encoding Loss 5.21018123626709, Transition Loss -1.008678913116455, Classifier Loss 0.22541865706443787, Total Loss 53.80255126953125\n",
      "11: Encoding Loss 4.007113456726074, Transition Loss -1.6124705076217651, Classifier Loss 0.1081579178571701, Total Loss 34.85783004760742\n",
      "11: Encoding Loss 6.319691181182861, Transition Loss -0.7840856313705444, Classifier Loss 0.1835322380065918, Total Loss 56.27105712890625\n",
      "11: Encoding Loss 3.206824541091919, Transition Loss -0.38156387209892273, Classifier Loss 0.048570167273283005, Total Loss 24.09781265258789\n",
      "11: Encoding Loss 3.211927890777588, Transition Loss -0.538616418838501, Classifier Loss 0.15133213996887207, Total Loss 34.40456771850586\n",
      "11: Encoding Loss 5.454847812652588, Transition Loss -2.2725162506103516, Classifier Loss 0.11106519401073456, Total Loss 43.83470153808594\n",
      "11: Encoding Loss 4.665389060974121, Transition Loss -1.6919182538986206, Classifier Loss 0.17352686822414398, Total Loss 45.3443489074707\n",
      "11: Encoding Loss 5.502989292144775, Transition Loss -2.293837308883667, Classifier Loss 0.11475740373134613, Total Loss 44.49275588989258\n",
      "11: Encoding Loss 5.549293041229248, Transition Loss -1.6916478872299194, Classifier Loss 0.04122128337621689, Total Loss 37.417213439941406\n",
      "11: Encoding Loss 5.492563247680664, Transition Loss -2.7419991493225098, Classifier Loss 0.10714732855558395, Total Loss 43.66901397705078\n",
      "11: Encoding Loss 7.314953804016113, Transition Loss -1.1172411441802979, Classifier Loss 0.14543089270591736, Total Loss 58.432369232177734\n",
      "11: Encoding Loss 6.040709495544434, Transition Loss -1.7354662418365479, Classifier Loss 0.14076906442642212, Total Loss 50.320472717285156\n",
      "11: Encoding Loss 5.938553810119629, Transition Loss -0.8501247763633728, Classifier Loss 0.08533571660518646, Total Loss 44.164554595947266\n",
      "11: Encoding Loss 4.051373481750488, Transition Loss -2.3224804401397705, Classifier Loss 0.07614977657794952, Total Loss 31.92228889465332\n",
      "11: Encoding Loss 6.495797634124756, Transition Loss -1.643366813659668, Classifier Loss 0.09079870581626892, Total Loss 48.05400085449219\n",
      "11: Encoding Loss 4.413424015045166, Transition Loss -2.285634756088257, Classifier Loss 0.06909620761871338, Total Loss 33.389251708984375\n",
      "11: Encoding Loss 5.769217491149902, Transition Loss -1.91451895236969, Classifier Loss 0.04385031387209892, Total Loss 38.99957275390625\n",
      "11: Encoding Loss 4.5920209884643555, Transition Loss -1.0285098552703857, Classifier Loss 0.06091519072651863, Total Loss 33.64323425292969\n",
      "11: Encoding Loss 5.2223076820373535, Transition Loss -1.7061302661895752, Classifier Loss 0.11422692239284515, Total Loss 42.755855560302734\n",
      "11: Encoding Loss 3.6538314819335938, Transition Loss 0.17110061645507812, Classifier Loss 0.05227850005030632, Total Loss 27.21927833557129\n",
      "11: Encoding Loss 8.109506607055664, Transition Loss -1.3723382949829102, Classifier Loss 0.17095619440078735, Total Loss 65.75210571289062\n",
      "11: Encoding Loss 8.040121078491211, Transition Loss -2.2082085609436035, Classifier Loss 0.14848017692565918, Total Loss 63.08786392211914\n",
      "11: Encoding Loss 7.458011627197266, Transition Loss -1.0544648170471191, Classifier Loss 0.18391971290111542, Total Loss 63.139617919921875\n",
      "11: Encoding Loss 5.893656253814697, Transition Loss -0.25588420033454895, Classifier Loss 0.11400048434734344, Total Loss 46.76188278198242\n",
      "11: Encoding Loss 7.540052890777588, Transition Loss -0.22344571352005005, Classifier Loss 0.23068097233772278, Total Loss 68.3083267211914\n",
      "11: Encoding Loss 4.48917293548584, Transition Loss 0.2701756954193115, Classifier Loss 0.046778298914432526, Total Loss 31.72093963623047\n",
      "11: Encoding Loss 3.7322070598602295, Transition Loss -0.9153738021850586, Classifier Loss 0.09314177930355072, Total Loss 31.707054138183594\n",
      "11: Encoding Loss 2.7019360065460205, Transition Loss 0.44935208559036255, Classifier Loss 0.08751268684864044, Total Loss 25.14262580871582\n",
      "11: Encoding Loss 4.947673797607422, Transition Loss -0.6831517219543457, Classifier Loss 0.12255482375621796, Total Loss 41.941253662109375\n",
      "11: Encoding Loss 6.026264190673828, Transition Loss -1.749577283859253, Classifier Loss 0.08990215510129929, Total Loss 45.14710235595703\n",
      "11: Encoding Loss 6.927478790283203, Transition Loss -1.7565486431121826, Classifier Loss 0.17164772748947144, Total Loss 58.72894287109375\n",
      "11: Encoding Loss 4.2797627449035645, Transition Loss -1.7922159433364868, Classifier Loss 0.06016598269343376, Total Loss 31.6944580078125\n",
      "11: Encoding Loss 6.4222869873046875, Transition Loss -2.3798582553863525, Classifier Loss 0.17872068285942078, Total Loss 56.40483856201172\n",
      "11: Encoding Loss 4.594466209411621, Transition Loss -1.1744441986083984, Classifier Loss 0.06284751743078232, Total Loss 33.85108184814453\n",
      "11: Encoding Loss 4.573592185974121, Transition Loss -1.5450925827026367, Classifier Loss 0.09955291450023651, Total Loss 37.3962287902832\n",
      "11: Encoding Loss 4.408699989318848, Transition Loss -3.250307321548462, Classifier Loss 0.07315641641616821, Total Loss 33.76654052734375\n",
      "11: Encoding Loss 2.6017839908599854, Transition Loss -1.8616852760314941, Classifier Loss 0.06067280098795891, Total Loss 21.6772403717041\n",
      "11: Encoding Loss 5.520162105560303, Transition Loss -1.7609577178955078, Classifier Loss 0.0895785316824913, Total Loss 42.078121185302734\n",
      "11: Encoding Loss 5.3453779220581055, Transition Loss -1.9755542278289795, Classifier Loss 0.15073905885219574, Total Loss 47.1453857421875\n",
      "11: Encoding Loss 3.0914254188537598, Transition Loss -1.6049412488937378, Classifier Loss 0.06625453382730484, Total Loss 25.173364639282227\n",
      "11: Encoding Loss 6.454703330993652, Transition Loss -2.324101448059082, Classifier Loss 0.07485281676054001, Total Loss 46.21257400512695\n",
      "11: Encoding Loss 4.404141902923584, Transition Loss -0.961498498916626, Classifier Loss 0.07699732482433319, Total Loss 34.12419891357422\n",
      "11: Encoding Loss 6.482094764709473, Transition Loss -1.1877622604370117, Classifier Loss 0.10608157515525818, Total Loss 49.50025177001953\n",
      "11: Encoding Loss 6.952160835266113, Transition Loss -1.5415058135986328, Classifier Loss 0.1359720528125763, Total Loss 55.30955505371094\n",
      "11: Encoding Loss 6.246755123138428, Transition Loss -0.8826510310173035, Classifier Loss 0.09077183902263641, Total Loss 46.5573616027832\n",
      "11: Encoding Loss 4.725383758544922, Transition Loss -1.5710958242416382, Classifier Loss 0.10970038920640945, Total Loss 39.321712493896484\n",
      "11: Encoding Loss 5.942343235015869, Transition Loss -1.879643440246582, Classifier Loss 0.11433485150337219, Total Loss 47.086795806884766\n",
      "11: Encoding Loss 5.132143020629883, Transition Loss -1.6523956060409546, Classifier Loss 0.06277782469987869, Total Loss 37.06998062133789\n",
      "11: Encoding Loss 2.776251792907715, Transition Loss -0.16447699069976807, Classifier Loss 0.04051743447780609, Total Loss 20.709190368652344\n",
      "11: Encoding Loss 7.215972900390625, Transition Loss -0.20635096728801727, Classifier Loss 0.0844980776309967, Total Loss 51.74555969238281\n",
      "11: Encoding Loss 6.346294403076172, Transition Loss -1.3043664693832397, Classifier Loss 0.06312624365091324, Total Loss 44.389869689941406\n",
      "11: Encoding Loss 6.661614418029785, Transition Loss -1.0322611331939697, Classifier Loss 0.035399965941905975, Total Loss 43.509273529052734\n",
      "11: Encoding Loss 4.022217750549316, Transition Loss -0.16203346848487854, Classifier Loss 0.053764283657073975, Total Loss 29.50967025756836\n",
      "11: Encoding Loss 4.06217098236084, Transition Loss -2.1218292713165283, Classifier Loss 0.034107405692338943, Total Loss 27.78291893005371\n",
      "11: Encoding Loss 3.8386573791503906, Transition Loss -1.7288885116577148, Classifier Loss 0.13512979447841644, Total Loss 36.54423141479492\n",
      "11: Encoding Loss 5.452439785003662, Transition Loss -0.4211410582065582, Classifier Loss 0.03910680115222931, Total Loss 36.625152587890625\n",
      "11: Encoding Loss 4.527677536010742, Transition Loss -1.1297311782836914, Classifier Loss 0.03915080055594444, Total Loss 31.0806941986084\n",
      "11: Encoding Loss 4.212845802307129, Transition Loss -0.5526801347732544, Classifier Loss 0.0434613972902298, Total Loss 29.62299346923828\n",
      "11: Encoding Loss 3.6055076122283936, Transition Loss -0.8691466450691223, Classifier Loss 0.06892018765211105, Total Loss 28.52471923828125\n",
      "11: Encoding Loss 6.056658744812012, Transition Loss -1.4431662559509277, Classifier Loss 0.07090353965759277, Total Loss 43.42973327636719\n",
      "11: Encoding Loss 4.436207294464111, Transition Loss -1.9073132276535034, Classifier Loss 0.07041458040475845, Total Loss 33.65793991088867\n",
      "11: Encoding Loss 4.006455421447754, Transition Loss -2.512176513671875, Classifier Loss 0.05080558359622955, Total Loss 29.1182861328125\n",
      "11: Encoding Loss 3.885906219482422, Transition Loss -1.5184733867645264, Classifier Loss 0.09837318956851959, Total Loss 33.15214920043945\n",
      "11: Encoding Loss 5.864407062530518, Transition Loss -0.7827399969100952, Classifier Loss 0.09561194479465485, Total Loss 44.7473258972168\n",
      "11: Encoding Loss 3.5164153575897217, Transition Loss -0.7855428457260132, Classifier Loss 0.050976477563381195, Total Loss 26.195825576782227\n",
      "11: Encoding Loss 4.346852779388428, Transition Loss -1.1556142568588257, Classifier Loss 0.10145120322704315, Total Loss 36.22577667236328\n",
      "11: Encoding Loss 4.727393627166748, Transition Loss -2.883279800415039, Classifier Loss 0.07410762459039688, Total Loss 35.77397155761719\n",
      "11: Encoding Loss 4.173270225524902, Transition Loss -1.1028639078140259, Classifier Loss 0.03987841680645943, Total Loss 29.02702522277832\n",
      "11: Encoding Loss 3.324352979660034, Transition Loss -0.7187046408653259, Classifier Loss 0.058098748326301575, Total Loss 25.755706787109375\n",
      "11: Encoding Loss 4.661111354827881, Transition Loss -1.7114408016204834, Classifier Loss 0.1457701176404953, Total Loss 42.542999267578125\n",
      "11: Encoding Loss 3.2878756523132324, Transition Loss -2.2657763957977295, Classifier Loss 0.07749276608228683, Total Loss 27.47562599182129\n",
      "11: Encoding Loss 3.8168673515319824, Transition Loss -0.8856544494628906, Classifier Loss 0.08009590208530426, Total Loss 30.91044044494629\n",
      "11: Encoding Loss 4.7138872146606445, Transition Loss -2.442837715148926, Classifier Loss 0.05705827474594116, Total Loss 33.98817443847656\n",
      "11: Encoding Loss 5.755219459533691, Transition Loss -1.042544960975647, Classifier Loss 0.15466833114624023, Total Loss 49.997737884521484\n",
      "11: Encoding Loss 5.358941078186035, Transition Loss -1.640655517578125, Classifier Loss 0.09364773333072662, Total Loss 41.517765045166016\n",
      "11: Encoding Loss 3.3158116340637207, Transition Loss -2.142488479614258, Classifier Loss 0.08353844285011292, Total Loss 28.24785804748535\n",
      "11: Encoding Loss 6.494538307189941, Transition Loss -1.198979377746582, Classifier Loss 0.17299692332744598, Total Loss 56.266441345214844\n",
      "11: Encoding Loss 6.96154260635376, Transition Loss -1.9585357904434204, Classifier Loss 0.2234221249818802, Total Loss 64.11067962646484\n",
      "11: Encoding Loss 7.6686859130859375, Transition Loss -1.925491452217102, Classifier Loss 0.12376266717910767, Total Loss 58.387611389160156\n",
      "11: Encoding Loss 7.244604587554932, Transition Loss -2.2334916591644287, Classifier Loss 0.12792186439037323, Total Loss 56.2589225769043\n",
      "11: Encoding Loss 4.4026360511779785, Transition Loss -2.9100570678710938, Classifier Loss 0.07548833638429642, Total Loss 33.9634895324707\n",
      "11: Encoding Loss 7.1943864822387695, Transition Loss -2.4898431301116943, Classifier Loss 0.0784837082028389, Total Loss 51.013694763183594\n",
      "11: Encoding Loss 8.002115249633789, Transition Loss -0.18965156376361847, Classifier Loss 0.1300991177558899, Total Loss 61.022525787353516\n",
      "11: Encoding Loss 6.7356767654418945, Transition Loss -2.5494296550750732, Classifier Loss 0.2184910774230957, Total Loss 62.262149810791016\n",
      "11: Encoding Loss 7.5120697021484375, Transition Loss -1.9848363399505615, Classifier Loss 0.16616596281528473, Total Loss 61.6882209777832\n",
      "11: Encoding Loss 6.618016242980957, Transition Loss 0.4247828722000122, Classifier Loss 0.05749254301190376, Total Loss 45.62726593017578\n",
      "11: Encoding Loss 5.5894880294799805, Transition Loss -1.6341568231582642, Classifier Loss 0.029613176360726357, Total Loss 36.497596740722656\n",
      "11: Encoding Loss 3.060445547103882, Transition Loss -1.284440279006958, Classifier Loss 0.0911305844783783, Total Loss 27.4752197265625\n",
      "11: Encoding Loss 4.0763115882873535, Transition Loss -0.6800134778022766, Classifier Loss 0.08589936792850494, Total Loss 33.04753494262695\n",
      "11: Encoding Loss 3.8735039234161377, Transition Loss 0.6203501224517822, Classifier Loss 0.05988939106464386, Total Loss 29.478103637695312\n",
      "11: Encoding Loss 2.2204957008361816, Transition Loss -1.242194414138794, Classifier Loss 0.03658483177423477, Total Loss 16.980958938598633\n",
      "11: Encoding Loss 4.389338493347168, Transition Loss -0.9626288414001465, Classifier Loss 0.15077081322669983, Total Loss 41.4127311706543\n",
      "11: Encoding Loss 3.7015299797058105, Transition Loss -0.9029946327209473, Classifier Loss 0.15106593072414398, Total Loss 37.31541061401367\n",
      "11: Encoding Loss 6.989113807678223, Transition Loss -0.13730992376804352, Classifier Loss 0.049559131264686584, Total Loss 46.89054489135742\n",
      "11: Encoding Loss 5.243196964263916, Transition Loss -2.291438579559326, Classifier Loss 0.07251414656639099, Total Loss 38.70968246459961\n",
      "11: Encoding Loss 8.3177490234375, Transition Loss -1.296365737915039, Classifier Loss 0.28294166922569275, Total Loss 78.20014953613281\n",
      "11: Encoding Loss 5.178363800048828, Transition Loss -2.4523072242736816, Classifier Loss 0.12927408516407013, Total Loss 43.996612548828125\n",
      "11: Encoding Loss 4.780975341796875, Transition Loss -1.4461979866027832, Classifier Loss 0.04380340874195099, Total Loss 33.065616607666016\n",
      "11: Encoding Loss 5.8676252365112305, Transition Loss -1.7614778280258179, Classifier Loss 0.14192219078540802, Total Loss 49.39726638793945\n",
      "11: Encoding Loss 6.957543849945068, Transition Loss -1.7444384098052979, Classifier Loss 0.10099460184574127, Total Loss 51.84402847290039\n",
      "11: Encoding Loss 4.994006633758545, Transition Loss 0.19566917419433594, Classifier Loss 0.13179591298103333, Total Loss 43.22189712524414\n",
      "11: Encoding Loss 4.103212356567383, Transition Loss 0.6935216188430786, Classifier Loss 0.08245261758565903, Total Loss 33.14194869995117\n",
      "11: Encoding Loss 5.054990291595459, Transition Loss -2.1141958236694336, Classifier Loss 0.078980453312397, Total Loss 38.227142333984375\n",
      "11: Encoding Loss 7.236888885498047, Transition Loss -1.5123845338821411, Classifier Loss 0.09847468882799149, Total Loss 53.26819610595703\n",
      "11: Encoding Loss 5.2499775886535645, Transition Loss -0.23208296298980713, Classifier Loss 0.13686519861221313, Total Loss 45.18629455566406\n",
      "11: Encoding Loss 3.255082368850708, Transition Loss -0.4332074224948883, Classifier Loss 0.04159914702177048, Total Loss 23.690235137939453\n",
      "11: Encoding Loss 4.4896979331970215, Transition Loss -1.9469627141952515, Classifier Loss 0.1286732405424118, Total Loss 39.80473327636719\n",
      "11: Encoding Loss 3.6768736839294434, Transition Loss 0.8247692584991455, Classifier Loss 0.08910190314054489, Total Loss 31.301342010498047\n",
      "11: Encoding Loss 6.454508304595947, Transition Loss -1.8987269401550293, Classifier Loss 0.08738979697227478, Total Loss 47.46527099609375\n",
      "11: Encoding Loss 3.7301647663116455, Transition Loss -2.2840840816497803, Classifier Loss 0.06743333488702774, Total Loss 29.123409271240234\n",
      "11: Encoding Loss 6.197471618652344, Transition Loss -1.2774651050567627, Classifier Loss 0.10712683945894241, Total Loss 47.897003173828125\n",
      "11: Encoding Loss 4.541601657867432, Transition Loss -2.197624683380127, Classifier Loss 0.05393195152282715, Total Loss 32.641929626464844\n",
      "11: Encoding Loss 5.242043972015381, Transition Loss -1.4446732997894287, Classifier Loss 0.10854856669902802, Total Loss 42.30654525756836\n",
      "11: Encoding Loss 5.409542083740234, Transition Loss -2.127960205078125, Classifier Loss 0.10679155588150024, Total Loss 43.135555267333984\n",
      "11: Encoding Loss 4.072921276092529, Transition Loss 0.8978601694107056, Classifier Loss 0.08874495327472687, Total Loss 33.671165466308594\n",
      "11: Encoding Loss 2.0956664085388184, Transition Loss -0.8339508771896362, Classifier Loss 0.06161971762776375, Total Loss 18.735637664794922\n",
      "11: Encoding Loss 9.230140686035156, Transition Loss -2.0903539657592773, Classifier Loss 0.10440090298652649, Total Loss 65.82009887695312\n",
      "11: Encoding Loss 5.308417320251465, Transition Loss -1.4259918928146362, Classifier Loss 0.09532143175601959, Total Loss 41.382076263427734\n",
      "11: Encoding Loss 4.783604145050049, Transition Loss -0.9017466902732849, Classifier Loss 0.09681820869445801, Total Loss 38.38308334350586\n",
      "11: Encoding Loss 4.656955718994141, Transition Loss -1.1850035190582275, Classifier Loss 0.05413118749856949, Total Loss 33.3543815612793\n",
      "11: Encoding Loss 3.5759856700897217, Transition Loss -1.0805964469909668, Classifier Loss 0.12029176950454712, Total Loss 33.48466110229492\n",
      "11: Encoding Loss 4.77349853515625, Transition Loss -1.720266580581665, Classifier Loss 0.15086548030376434, Total Loss 43.72685241699219\n",
      "11: Encoding Loss 3.333341598510742, Transition Loss 0.2420753836631775, Classifier Loss 0.04747603461146355, Total Loss 24.844484329223633\n",
      "11: Encoding Loss 6.258524417877197, Transition Loss -2.3469841480255127, Classifier Loss 0.06404294073581696, Total Loss 43.95450210571289\n",
      "11: Encoding Loss 5.501039028167725, Transition Loss -1.103853464126587, Classifier Loss 0.030366024002432823, Total Loss 36.042396545410156\n",
      "11: Encoding Loss 5.167632579803467, Transition Loss -1.9574898481369019, Classifier Loss 0.0688919872045517, Total Loss 37.89421463012695\n",
      "11: Encoding Loss 4.074784278869629, Transition Loss -0.9070873260498047, Classifier Loss 0.03231946378946304, Total Loss 27.68029022216797\n",
      "11: Encoding Loss 4.216658115386963, Transition Loss 0.13645701110363007, Classifier Loss 0.1319156438112259, Total Loss 38.54610061645508\n",
      "11: Encoding Loss 5.092740058898926, Transition Loss -1.0759364366531372, Classifier Loss 0.05468049645423889, Total Loss 36.0240592956543\n",
      "11: Encoding Loss 3.616241931915283, Transition Loss -2.6766746044158936, Classifier Loss 0.09885073453187943, Total Loss 31.581457138061523\n",
      "11: Encoding Loss 5.116153240203857, Transition Loss -1.6506202220916748, Classifier Loss 0.18893904983997345, Total Loss 49.59016799926758\n",
      "11: Encoding Loss 4.9987568855285645, Transition Loss -1.3091331720352173, Classifier Loss 0.09740821272134781, Total Loss 39.73284149169922\n",
      "11: Encoding Loss 7.646337985992432, Transition Loss -1.2771508693695068, Classifier Loss 0.09156176447868347, Total Loss 55.033695220947266\n",
      "11: Encoding Loss 4.589776039123535, Transition Loss -0.4683073163032532, Classifier Loss 0.08424336463212967, Total Loss 35.962806701660156\n",
      "11: Encoding Loss 4.354022979736328, Transition Loss 0.14162297546863556, Classifier Loss 0.10011158138513565, Total Loss 36.19194793701172\n",
      "11: Encoding Loss 6.9199066162109375, Transition Loss -1.392691969871521, Classifier Loss 0.08804753422737122, Total Loss 50.32363510131836\n",
      "11: Encoding Loss 5.236274719238281, Transition Loss -2.297819137573242, Classifier Loss 0.13315431773662567, Total Loss 44.73216247558594\n",
      "11: Encoding Loss 7.268071174621582, Transition Loss -2.091649293899536, Classifier Loss 0.12217561900615692, Total Loss 55.82515335083008\n",
      "11: Encoding Loss 5.296810626983643, Transition Loss -2.2094666957855225, Classifier Loss 0.0559551939368248, Total Loss 37.3754997253418\n",
      "11: Encoding Loss 3.9144840240478516, Transition Loss -2.290855884552002, Classifier Loss 0.13062262535095215, Total Loss 36.54825210571289\n",
      "11: Encoding Loss 6.606347560882568, Transition Loss -1.0581226348876953, Classifier Loss 0.08484166115522385, Total Loss 48.121829986572266\n",
      "11: Encoding Loss 8.853262901306152, Transition Loss -1.3976854085922241, Classifier Loss 0.14041410386562347, Total Loss 67.16043090820312\n",
      "11: Encoding Loss 5.4705681800842285, Transition Loss -1.4863922595977783, Classifier Loss 0.042783547192811966, Total Loss 37.10116958618164\n",
      "11: Encoding Loss 4.221088409423828, Transition Loss -0.18073934316635132, Classifier Loss 0.053765084594488144, Total Loss 30.70296859741211\n",
      "11: Encoding Loss 3.522395610809326, Transition Loss -1.0011119842529297, Classifier Loss 0.0799049362540245, Total Loss 29.124467849731445\n",
      "11: Encoding Loss 5.561131000518799, Transition Loss -1.4648213386535645, Classifier Loss 0.056213609874248505, Total Loss 38.9875602722168\n",
      "11: Encoding Loss 4.02464485168457, Transition Loss -1.029586911201477, Classifier Loss 0.03173518553376198, Total Loss 27.32097816467285\n",
      "11: Encoding Loss 6.034428596496582, Transition Loss -2.0352725982666016, Classifier Loss 0.04369308799505234, Total Loss 40.575069427490234\n",
      "11: Encoding Loss 3.173020362854004, Transition Loss -1.976905107498169, Classifier Loss 0.08435627818107605, Total Loss 27.472959518432617\n",
      "11: Encoding Loss 8.106003761291504, Transition Loss 0.22375404834747314, Classifier Loss 0.25336602330207825, Total Loss 74.06212615966797\n",
      "11: Encoding Loss 5.017843723297119, Transition Loss -1.1219590902328491, Classifier Loss 0.12281693518161774, Total Loss 42.3883056640625\n",
      "11: Encoding Loss 6.061749458312988, Transition Loss -1.2384974956512451, Classifier Loss 0.09397168457508087, Total Loss 45.76716995239258\n",
      "11: Encoding Loss 5.787537574768066, Transition Loss 0.212360218167305, Classifier Loss 0.09754811972379684, Total Loss 44.56498718261719\n",
      "11: Encoding Loss 6.665475368499756, Transition Loss -0.5265769958496094, Classifier Loss 0.20483678579330444, Total Loss 60.476322174072266\n",
      "11: Encoding Loss 6.083104610443115, Transition Loss -1.4089924097061157, Classifier Loss 0.1628226935863495, Total Loss 52.78033447265625\n",
      "11: Encoding Loss 4.218193054199219, Transition Loss -0.5313488841056824, Classifier Loss 0.08891120553016663, Total Loss 34.20006561279297\n",
      "11: Encoding Loss 4.78431510925293, Transition Loss -2.068614959716797, Classifier Loss 0.12274803966283798, Total Loss 40.9798698425293\n",
      "11: Encoding Loss 4.876776695251465, Transition Loss -0.7335699796676636, Classifier Loss 0.11154966056346893, Total Loss 40.41533279418945\n",
      "11: Encoding Loss 5.680318832397461, Transition Loss -0.32086217403411865, Classifier Loss 0.09813009947538376, Total Loss 43.89479446411133\n",
      "11: Encoding Loss 3.219390869140625, Transition Loss -0.8679293394088745, Classifier Loss 0.05960283428430557, Total Loss 25.276281356811523\n",
      "11: Encoding Loss 3.797156810760498, Transition Loss -0.847394585609436, Classifier Loss 0.09983066469430923, Total Loss 32.76566696166992\n",
      "11: Encoding Loss 3.573991060256958, Transition Loss -1.918127417564392, Classifier Loss 0.09349043667316437, Total Loss 30.792224884033203\n",
      "11: Encoding Loss 4.873491287231445, Transition Loss -0.6437612771987915, Classifier Loss 0.0348174013197422, Total Loss 32.72243118286133\n",
      "11: Encoding Loss 5.153943061828613, Transition Loss -1.1435128450393677, Classifier Loss 0.12072096019983292, Total Loss 42.995296478271484\n",
      "11: Encoding Loss 3.9443821907043457, Transition Loss -2.290677309036255, Classifier Loss 0.09505534172058105, Total Loss 33.17091369628906\n",
      "11: Encoding Loss 4.676352500915527, Transition Loss -1.3585405349731445, Classifier Loss 0.11999785155057907, Total Loss 40.0573616027832\n",
      "11: Encoding Loss 3.984220027923584, Transition Loss -0.17299914360046387, Classifier Loss 0.03575568273663521, Total Loss 27.480819702148438\n",
      "11: Encoding Loss 3.4565303325653076, Transition Loss -0.2817363739013672, Classifier Loss 0.054028697311878204, Total Loss 26.14194107055664\n",
      "11: Encoding Loss 4.738478660583496, Transition Loss -0.5717798471450806, Classifier Loss 0.19114074110984802, Total Loss 47.54471969604492\n",
      "11: Encoding Loss 5.921651840209961, Transition Loss -1.6718541383743286, Classifier Loss 0.19168692827224731, Total Loss 54.69793701171875\n",
      "11: Encoding Loss 3.9287891387939453, Transition Loss -0.49413353204727173, Classifier Loss 0.12109632790088654, Total Loss 35.682167053222656\n",
      "11: Encoding Loss 6.393673419952393, Transition Loss -1.6101523637771606, Classifier Loss 0.14551517367362976, Total Loss 52.91291427612305\n",
      "11: Encoding Loss 3.774440050125122, Transition Loss -1.9926737546920776, Classifier Loss 0.0731687992811203, Total Loss 29.962722778320312\n",
      "11: Encoding Loss 5.525163650512695, Transition Loss -1.648369312286377, Classifier Loss 0.1290491819381714, Total Loss 46.055240631103516\n",
      "11: Encoding Loss 5.0375871658325195, Transition Loss -0.9677935838699341, Classifier Loss 0.037648119032382965, Total Loss 33.98994827270508\n",
      "11: Encoding Loss 3.0741217136383057, Transition Loss -1.2525365352630615, Classifier Loss 0.09322041273117065, Total Loss 27.766271591186523\n",
      "11: Encoding Loss 3.6999106407165527, Transition Loss -2.1772096157073975, Classifier Loss 0.0558781772851944, Total Loss 27.78641128540039\n",
      "11: Encoding Loss 3.954310178756714, Transition Loss -0.21676093339920044, Classifier Loss 0.13515599071979523, Total Loss 37.24137496948242\n",
      "11: Encoding Loss 6.744816780090332, Transition Loss -0.7253493070602417, Classifier Loss 0.05682767182588577, Total Loss 46.1513786315918\n",
      "11: Encoding Loss 5.434747219085693, Transition Loss -1.5124058723449707, Classifier Loss 0.207565575838089, Total Loss 53.364437103271484\n",
      "11: Encoding Loss 6.314731121063232, Transition Loss -0.23399928212165833, Classifier Loss 0.14430002868175507, Total Loss 52.31829833984375\n",
      "11: Encoding Loss 5.346149444580078, Transition Loss -2.2757761478424072, Classifier Loss 0.1340542882680893, Total Loss 45.481414794921875\n",
      "11: Encoding Loss 4.721375465393066, Transition Loss -0.8588799238204956, Classifier Loss 0.056322284042835236, Total Loss 33.96013641357422\n",
      "11: Encoding Loss 2.9688563346862793, Transition Loss -2.3705427646636963, Classifier Loss 0.0568498820066452, Total Loss 23.49717903137207\n",
      "11: Encoding Loss 2.840575695037842, Transition Loss -1.309025764465332, Classifier Loss 0.056805387139320374, Total Loss 22.723468780517578\n",
      "11: Encoding Loss 3.9985244274139404, Transition Loss -2.6000795364379883, Classifier Loss 0.05472799390554428, Total Loss 29.462907791137695\n",
      "11: Encoding Loss 6.082942008972168, Transition Loss 0.14377722144126892, Classifier Loss 0.036619964987039566, Total Loss 40.217159271240234\n",
      "11: Encoding Loss 5.193506717681885, Transition Loss -0.261091947555542, Classifier Loss 0.09678684175014496, Total Loss 40.839622497558594\n",
      "11: Encoding Loss 6.68429708480835, Transition Loss -3.4411847591400146, Classifier Loss 0.06402266025543213, Total Loss 46.506675720214844\n",
      "11: Encoding Loss 5.415971755981445, Transition Loss -1.8921914100646973, Classifier Loss 0.07941506057977676, Total Loss 40.436580657958984\n",
      "11: Encoding Loss 4.436574935913086, Transition Loss 0.09397578239440918, Classifier Loss 0.10447170585393906, Total Loss 37.104209899902344\n",
      "11: Encoding Loss 4.2802863121032715, Transition Loss -2.3110058307647705, Classifier Loss 0.03656945377588272, Total Loss 29.337739944458008\n",
      "11: Encoding Loss 7.194258213043213, Transition Loss -1.040087342262268, Classifier Loss 0.10373684018850327, Total Loss 53.538818359375\n",
      "11: Encoding Loss 5.906548023223877, Transition Loss -2.675717353820801, Classifier Loss 0.16940391063690186, Total Loss 52.37860870361328\n",
      "11: Encoding Loss 5.582561016082764, Transition Loss -1.5925297737121582, Classifier Loss 0.09846509248018265, Total Loss 43.34123992919922\n",
      "11: Encoding Loss 6.238492012023926, Transition Loss -0.6131883859634399, Classifier Loss 0.14717862010002136, Total Loss 52.1485710144043\n",
      "11: Encoding Loss 6.170670986175537, Transition Loss -1.6880940198898315, Classifier Loss 0.024923687800765038, Total Loss 39.51572036743164\n",
      "11: Encoding Loss 5.1954240798950195, Transition Loss -1.6048297882080078, Classifier Loss 0.17284882068634033, Total Loss 48.456787109375\n",
      "11: Encoding Loss 3.9821784496307373, Transition Loss -0.6923123598098755, Classifier Loss 0.06783723086118698, Total Loss 30.6765193939209\n",
      "11: Encoding Loss 6.344056129455566, Transition Loss -1.9485256671905518, Classifier Loss 0.10930516570806503, Total Loss 48.994075775146484\n",
      "11: Encoding Loss 5.465699195861816, Transition Loss -0.701782763004303, Classifier Loss 0.10406797379255295, Total Loss 43.200714111328125\n",
      "11: Encoding Loss 4.7513203620910645, Transition Loss -0.508759617805481, Classifier Loss 0.07400651276111603, Total Loss 35.90837097167969\n",
      "11: Encoding Loss 4.369542121887207, Transition Loss -1.1272304058074951, Classifier Loss 0.10951545089483261, Total Loss 37.1683464050293\n",
      "11: Encoding Loss 4.857248783111572, Transition Loss -0.7278430461883545, Classifier Loss 0.13827165961265564, Total Loss 42.970367431640625\n",
      "11: Encoding Loss 5.661667823791504, Transition Loss -0.4534303843975067, Classifier Loss 0.11301858723163605, Total Loss 45.27168273925781\n",
      "11: Encoding Loss 5.294455528259277, Transition Loss -2.9988532066345215, Classifier Loss 0.051616374403238297, Total Loss 36.92717361450195\n",
      "11: Encoding Loss 2.570882797241211, Transition Loss -1.7503575086593628, Classifier Loss 0.08066482841968536, Total Loss 23.491079330444336\n",
      "11: Encoding Loss 9.194323539733887, Transition Loss -2.2158725261688232, Classifier Loss 0.14252403378486633, Total Loss 69.41746520996094\n",
      "11: Encoding Loss 6.016741752624512, Transition Loss -0.2327420562505722, Classifier Loss 0.0324753075838089, Total Loss 39.34789276123047\n",
      "11: Encoding Loss 5.544734001159668, Transition Loss -0.9053804874420166, Classifier Loss 0.15992385149002075, Total Loss 49.26042938232422\n",
      "11: Encoding Loss 4.993885040283203, Transition Loss -1.1007905006408691, Classifier Loss 0.1273583471775055, Total Loss 42.698707580566406\n",
      "11: Encoding Loss 6.112033367156982, Transition Loss -0.9272258877754211, Classifier Loss 0.05730334669351578, Total Loss 42.40216827392578\n",
      "11: Encoding Loss 6.781848907470703, Transition Loss -1.5432697534561157, Classifier Loss 0.27071613073349, Total Loss 67.76209259033203\n",
      "11: Encoding Loss 4.927393436431885, Transition Loss -0.30601808428764343, Classifier Loss 0.08277840912342072, Total Loss 37.842079162597656\n",
      "11: Encoding Loss 5.221676349639893, Transition Loss 0.47741949558258057, Classifier Loss 0.11968562752008438, Total Loss 43.48958969116211\n",
      "11: Encoding Loss 5.26052188873291, Transition Loss -0.5798358917236328, Classifier Loss 0.08060307800769806, Total Loss 39.623207092285156\n",
      "11: Encoding Loss 3.8993289470672607, Transition Loss 0.23959144949913025, Classifier Loss 0.021962031722068787, Total Loss 25.68801498413086\n",
      "11: Encoding Loss 6.564763069152832, Transition Loss -1.2961852550506592, Classifier Loss 0.07675889134407043, Total Loss 47.06394958496094\n",
      "11: Encoding Loss 7.260229587554932, Transition Loss -1.821850299835205, Classifier Loss 0.19547227025032043, Total Loss 63.10787582397461\n",
      "11: Encoding Loss 4.598352432250977, Transition Loss -1.0320141315460205, Classifier Loss 0.06984232366085052, Total Loss 34.573936462402344\n",
      "11: Encoding Loss 3.840519428253174, Transition Loss -1.9803340435028076, Classifier Loss 0.06770732253789902, Total Loss 29.813058853149414\n",
      "11: Encoding Loss 4.133795738220215, Transition Loss -1.336557388305664, Classifier Loss 0.07010491192340851, Total Loss 31.812732696533203\n",
      "11: Encoding Loss 4.846961975097656, Transition Loss -0.3492482006549835, Classifier Loss 0.046514999121427536, Total Loss 33.733131408691406\n",
      "11: Encoding Loss 5.756908416748047, Transition Loss -1.0050816535949707, Classifier Loss 0.15145012736320496, Total Loss 49.68606185913086\n",
      "11: Encoding Loss 3.115964412689209, Transition Loss -0.3040863573551178, Classifier Loss 0.12062998116016388, Total Loss 30.758663177490234\n",
      "11: Encoding Loss 5.797617435455322, Transition Loss -2.061802387237549, Classifier Loss 0.07750646770000458, Total Loss 42.53553009033203\n",
      "11: Encoding Loss 3.5588181018829346, Transition Loss -1.0223197937011719, Classifier Loss 0.03958902880549431, Total Loss 25.311403274536133\n",
      "11: Encoding Loss 4.70557165145874, Transition Loss -2.7297775745391846, Classifier Loss 0.038419585675001144, Total Loss 32.07429885864258\n",
      "11: Encoding Loss 2.096600294113159, Transition Loss -1.5261988639831543, Classifier Loss 0.032029274851083755, Total Loss 15.781919479370117\n",
      "11: Encoding Loss 5.967066764831543, Transition Loss -1.7017817497253418, Classifier Loss 0.12855225801467896, Total Loss 48.65694808959961\n",
      "11: Encoding Loss 4.369422435760498, Transition Loss -1.8044729232788086, Classifier Loss 0.0947304368019104, Total Loss 35.68885803222656\n",
      "11: Encoding Loss 3.9064688682556152, Transition Loss -0.6787280440330505, Classifier Loss 0.06822623312473297, Total Loss 30.261165618896484\n",
      "11: Encoding Loss 5.275366306304932, Transition Loss 0.36697065830230713, Classifier Loss 0.08068831264972687, Total Loss 39.867820739746094\n",
      "11: Encoding Loss 6.004435062408447, Transition Loss -1.7563682794570923, Classifier Loss 0.15124933421611786, Total Loss 51.150840759277344\n",
      "11: Encoding Loss 6.777740478515625, Transition Loss -3.6941030025482178, Classifier Loss 0.13687266409397125, Total Loss 54.35223388671875\n",
      "11: Encoding Loss 4.630374908447266, Transition Loss -1.6229230165481567, Classifier Loss 0.07695023715496063, Total Loss 35.476627349853516\n",
      "11: Encoding Loss 5.360135078430176, Transition Loss -2.213508129119873, Classifier Loss 0.036074958741664886, Total Loss 35.76742172241211\n",
      "11: Encoding Loss 2.769986867904663, Transition Loss -2.7101175785064697, Classifier Loss 0.044501449912786484, Total Loss 21.068984985351562\n",
      "11: Encoding Loss 4.730543613433838, Transition Loss -2.919616222381592, Classifier Loss 0.1708436906337738, Total Loss 45.46646499633789\n",
      "11: Encoding Loss 4.971226692199707, Transition Loss -0.8209167122840881, Classifier Loss 0.18383747339248657, Total Loss 48.210777282714844\n",
      "11: Encoding Loss 7.230328559875488, Transition Loss -0.4119144678115845, Classifier Loss 0.08312965929508209, Total Loss 51.69477462768555\n",
      "11: Encoding Loss 4.18283748626709, Transition Loss -1.1616231203079224, Classifier Loss 0.046128034591674805, Total Loss 29.709365844726562\n",
      "11: Encoding Loss 3.967658281326294, Transition Loss -1.6495789289474487, Classifier Loss 0.05696742981672287, Total Loss 29.502033233642578\n",
      "11: Encoding Loss 5.342637538909912, Transition Loss -1.5204204320907593, Classifier Loss 0.031012721359729767, Total Loss 35.156494140625\n",
      "11: Encoding Loss 4.453233242034912, Transition Loss -0.9150980114936829, Classifier Loss 0.052512165158987045, Total Loss 31.970251083374023\n",
      "11: Encoding Loss 4.832772254943848, Transition Loss -1.43026864528656, Classifier Loss 0.06029501557350159, Total Loss 35.02556228637695\n",
      "11: Encoding Loss 5.040506839752197, Transition Loss -1.3788421154022217, Classifier Loss 0.05144479125738144, Total Loss 35.386966705322266\n",
      "11: Encoding Loss 5.0623626708984375, Transition Loss -1.3267382383346558, Classifier Loss 0.045390620827674866, Total Loss 34.9127082824707\n",
      "11: Encoding Loss 4.613341331481934, Transition Loss -1.2652416229248047, Classifier Loss 0.1276853382587433, Total Loss 40.44807815551758\n",
      "11: Encoding Loss 4.061363220214844, Transition Loss -0.8481321334838867, Classifier Loss 0.08628726750612259, Total Loss 32.9965705871582\n",
      "11: Encoding Loss 5.502877712249756, Transition Loss 0.3372015953063965, Classifier Loss 0.09881443530321121, Total Loss 43.033592224121094\n",
      "11: Encoding Loss 4.457995414733887, Transition Loss -1.8726495504379272, Classifier Loss 0.06021008640527725, Total Loss 32.76823425292969\n",
      "11: Encoding Loss 4.525062561035156, Transition Loss -2.040710210800171, Classifier Loss 0.05625894293189049, Total Loss 32.775455474853516\n",
      "11: Encoding Loss 4.356649875640869, Transition Loss -0.7315738201141357, Classifier Loss 0.1366460621356964, Total Loss 39.8042106628418\n",
      "11: Encoding Loss 3.789616346359253, Transition Loss -0.27427953481674194, Classifier Loss 0.04206881672143936, Total Loss 26.944469451904297\n",
      "11: Encoding Loss 4.996151924133301, Transition Loss -2.1178641319274902, Classifier Loss 0.0632404312491417, Total Loss 36.30010986328125\n",
      "11: Encoding Loss 4.059958457946777, Transition Loss -0.8744385838508606, Classifier Loss 0.036598145961761475, Total Loss 28.01921844482422\n",
      "11: Encoding Loss 6.377994060516357, Transition Loss 0.09457118809223175, Classifier Loss 0.1866164654493332, Total Loss 56.96744155883789\n",
      "11: Encoding Loss 5.650746822357178, Transition Loss -1.6867730617523193, Classifier Loss 0.08435767143964767, Total Loss 42.339576721191406\n",
      "11: Encoding Loss 6.461176872253418, Transition Loss -0.49900901317596436, Classifier Loss 0.15033555030822754, Total Loss 53.800418853759766\n",
      "11: Encoding Loss 4.60488748550415, Transition Loss -1.1210700273513794, Classifier Loss 0.023479726165533066, Total Loss 29.976850509643555\n",
      "11: Encoding Loss 4.638533592224121, Transition Loss -1.0942860841751099, Classifier Loss 0.13701596856117249, Total Loss 41.5323600769043\n",
      "11: Encoding Loss 2.997546434402466, Transition Loss -1.5120201110839844, Classifier Loss 0.09928759932518005, Total Loss 27.913434982299805\n",
      "11: Encoding Loss 5.694449424743652, Transition Loss -0.364379346370697, Classifier Loss 0.12004554271697998, Total Loss 46.17110824584961\n",
      "11: Encoding Loss 5.727447509765625, Transition Loss -1.3138071298599243, Classifier Loss 0.08647888153791428, Total Loss 43.012046813964844\n",
      "11: Encoding Loss 3.871530532836914, Transition Loss -0.5006652474403381, Classifier Loss 0.11358822882175446, Total Loss 34.587806701660156\n",
      "11: Encoding Loss 4.692869663238525, Transition Loss -1.3469140529632568, Classifier Loss 0.07600224018096924, Total Loss 35.75690460205078\n",
      "11: Encoding Loss 5.972441673278809, Transition Loss -0.4808477759361267, Classifier Loss 0.06564867496490479, Total Loss 42.399330139160156\n",
      "11: Encoding Loss 3.834956645965576, Transition Loss 0.06699459254741669, Classifier Loss 0.05545812472701073, Total Loss 28.582351684570312\n",
      "11: Encoding Loss 6.405525207519531, Transition Loss -0.004417508840560913, Classifier Loss 0.1662171632051468, Total Loss 55.054866790771484\n",
      "11: Encoding Loss 5.492784023284912, Transition Loss -1.0474437475204468, Classifier Loss 0.15606199204921722, Total Loss 48.56248474121094\n",
      "11: Encoding Loss 4.55711030960083, Transition Loss -1.369741439819336, Classifier Loss 0.06934701651334763, Total Loss 34.27681350708008\n",
      "11: Encoding Loss 4.052781581878662, Transition Loss -1.4556025266647339, Classifier Loss 0.14692322909832, Total Loss 39.00843048095703\n",
      "11: Encoding Loss 6.323032379150391, Transition Loss 0.012382745742797852, Classifier Loss 0.13591405749320984, Total Loss 51.53455352783203\n",
      "11: Encoding Loss 5.233534812927246, Transition Loss -1.5699571371078491, Classifier Loss 0.1338941752910614, Total Loss 44.78999710083008\n",
      "11: Encoding Loss 5.795953273773193, Transition Loss -1.16953706741333, Classifier Loss 0.13736245036125183, Total Loss 48.511497497558594\n",
      "11: Encoding Loss 4.989528179168701, Transition Loss -1.1897661685943604, Classifier Loss 0.07202962040901184, Total Loss 37.13965606689453\n",
      "11: Encoding Loss 5.79764461517334, Transition Loss -1.586592435836792, Classifier Loss 0.12147116661071777, Total Loss 46.93235397338867\n",
      "11: Encoding Loss 5.507763862609863, Transition Loss -0.7994706630706787, Classifier Loss 0.0871143788099289, Total Loss 41.7577018737793\n",
      "11: Encoding Loss 4.5037336349487305, Transition Loss -1.2202495336532593, Classifier Loss 0.09340567886829376, Total Loss 36.362483978271484\n",
      "11: Encoding Loss 4.559230327606201, Transition Loss 0.06060352921485901, Classifier Loss 0.04407046362757683, Total Loss 31.78666877746582\n",
      "11: Encoding Loss 3.7798092365264893, Transition Loss -0.2607237696647644, Classifier Loss 0.0498477965593338, Total Loss 27.663530349731445\n",
      "11: Encoding Loss 6.713123321533203, Transition Loss -1.641342282295227, Classifier Loss 0.1498192846775055, Total Loss 55.260013580322266\n",
      "11: Encoding Loss 5.701739311218262, Transition Loss -1.8014253377914429, Classifier Loss 0.11801444739103317, Total Loss 46.01116180419922\n",
      "11: Encoding Loss 3.1094918251037598, Transition Loss -0.993415117263794, Classifier Loss 0.07560886442661285, Total Loss 26.21744155883789\n",
      "11: Encoding Loss 7.078403472900391, Transition Loss -1.6292544603347778, Classifier Loss 0.08046822249889374, Total Loss 50.5165901184082\n",
      "11: Encoding Loss 4.339786529541016, Transition Loss -0.8176065683364868, Classifier Loss 0.06682085990905762, Total Loss 32.72047805786133\n",
      "11: Encoding Loss 4.367748260498047, Transition Loss -0.9346890449523926, Classifier Loss 0.04581082984805107, Total Loss 30.787200927734375\n",
      "11: Encoding Loss 6.380341529846191, Transition Loss -1.866955280303955, Classifier Loss 0.11760053038597107, Total Loss 50.04135513305664\n",
      "11: Encoding Loss 5.7599968910217285, Transition Loss -1.0555012226104736, Classifier Loss 0.1982974112033844, Total Loss 54.38930130004883\n",
      "11: Encoding Loss 5.520689010620117, Transition Loss -1.291775107383728, Classifier Loss 0.07885542511940002, Total Loss 41.00916290283203\n",
      "11: Encoding Loss 4.58183479309082, Transition Loss -0.012218490242958069, Classifier Loss 0.08010482788085938, Total Loss 35.50149154663086\n",
      "11: Encoding Loss 4.901904106140137, Transition Loss -2.3233699798583984, Classifier Loss 0.07958310842514038, Total Loss 37.368804931640625\n",
      "11: Encoding Loss 5.114242076873779, Transition Loss -1.8574825525283813, Classifier Loss 0.08587243407964706, Total Loss 39.27195358276367\n",
      "11: Encoding Loss 6.018193244934082, Transition Loss 0.2577194273471832, Classifier Loss 0.10971707105636597, Total Loss 47.183956146240234\n",
      "11: Encoding Loss 7.617989540100098, Transition Loss -1.3008263111114502, Classifier Loss 0.1285187005996704, Total Loss 58.559288024902344\n",
      "11: Encoding Loss 4.788013935089111, Transition Loss -2.2725250720977783, Classifier Loss 0.049259625375270844, Total Loss 33.65313720703125\n",
      "11: Encoding Loss 6.14060640335083, Transition Loss -0.9509356021881104, Classifier Loss 0.056956127285957336, Total Loss 42.53887176513672\n",
      "11: Encoding Loss 5.282557964324951, Transition Loss -0.7434438467025757, Classifier Loss 0.14204561710357666, Total Loss 45.89961242675781\n",
      "11: Encoding Loss 4.833992958068848, Transition Loss -1.0628423690795898, Classifier Loss 0.03485337644815445, Total Loss 32.48887252807617\n",
      "12: Encoding Loss 3.790879011154175, Transition Loss -1.1423832178115845, Classifier Loss 0.06726060062646866, Total Loss 29.47087860107422\n",
      "12: Encoding Loss 6.081744194030762, Transition Loss -0.21594177186489105, Classifier Loss 0.10392998903989792, Total Loss 46.88337707519531\n",
      "12: Encoding Loss 4.884416103363037, Transition Loss -1.585357904434204, Classifier Loss 0.0771929919719696, Total Loss 37.02516174316406\n",
      "12: Encoding Loss 5.782700538635254, Transition Loss -1.1080421209335327, Classifier Loss 0.06601066887378693, Total Loss 41.29682922363281\n",
      "12: Encoding Loss 7.7084479331970215, Transition Loss 0.42940324544906616, Classifier Loss 0.17541243135929108, Total Loss 63.96369171142578\n",
      "12: Encoding Loss 6.308155059814453, Transition Loss -2.85440731048584, Classifier Loss 0.055855873972177505, Total Loss 43.43337631225586\n",
      "12: Encoding Loss 4.155355453491211, Transition Loss -2.076826572418213, Classifier Loss 0.13099080324172974, Total Loss 38.0303840637207\n",
      "12: Encoding Loss 4.534904479980469, Transition Loss -1.6479101181030273, Classifier Loss 0.09780120849609375, Total Loss 36.988887786865234\n",
      "12: Encoding Loss 4.914267539978027, Transition Loss -1.1432628631591797, Classifier Loss 0.1460585594177246, Total Loss 44.09100341796875\n",
      "12: Encoding Loss 3.121297836303711, Transition Loss -0.6769988536834717, Classifier Loss 0.07269322127103806, Total Loss 25.996837615966797\n",
      "12: Encoding Loss 5.863171100616455, Transition Loss -1.3983042240142822, Classifier Loss 0.1364656239748001, Total Loss 48.82503128051758\n",
      "12: Encoding Loss 3.875349521636963, Transition Loss -1.1670361757278442, Classifier Loss 0.07271954417228699, Total Loss 30.523584365844727\n",
      "12: Encoding Loss 2.5636887550354004, Transition Loss -0.34587711095809937, Classifier Loss 0.03940296918153763, Total Loss 19.322290420532227\n",
      "12: Encoding Loss 4.457461357116699, Transition Loss -2.1574530601501465, Classifier Loss 0.08140283823013306, Total Loss 34.884193420410156\n",
      "12: Encoding Loss 3.437861680984497, Transition Loss -2.1560170650482178, Classifier Loss 0.0660930946469307, Total Loss 27.235618591308594\n",
      "12: Encoding Loss 4.410598278045654, Transition Loss -1.4245359897613525, Classifier Loss 0.0690491646528244, Total Loss 33.36793899536133\n",
      "12: Encoding Loss 3.741936206817627, Transition Loss -1.89936101436615, Classifier Loss 0.055274490267038345, Total Loss 27.978307723999023\n",
      "12: Encoding Loss 2.5848610401153564, Transition Loss 0.524954080581665, Classifier Loss 0.07107554376125336, Total Loss 22.826704025268555\n",
      "12: Encoding Loss 6.977195739746094, Transition Loss -2.432339668273926, Classifier Loss 0.09120867401361465, Total Loss 50.983070373535156\n",
      "12: Encoding Loss 4.040504455566406, Transition Loss -0.5716870427131653, Classifier Loss 0.19335827231407166, Total Loss 43.57862854003906\n",
      "12: Encoding Loss 6.617000579833984, Transition Loss -2.392246961593628, Classifier Loss 0.21968770027160645, Total Loss 61.66981506347656\n",
      "12: Encoding Loss 5.526646137237549, Transition Loss -1.3059958219528198, Classifier Loss 0.11149386316537857, Total Loss 44.30874252319336\n",
      "12: Encoding Loss 6.4839019775390625, Transition Loss -0.546349048614502, Classifier Loss 0.10311413556337357, Total Loss 49.21460723876953\n",
      "12: Encoding Loss 6.312952041625977, Transition Loss -0.16478049755096436, Classifier Loss 0.10011158138513565, Total Loss 47.8888053894043\n",
      "12: Encoding Loss 5.421564102172852, Transition Loss 0.3312194347381592, Classifier Loss 0.053010884672403336, Total Loss 37.96296310424805\n",
      "12: Encoding Loss 5.0355658531188965, Transition Loss -1.9472594261169434, Classifier Loss 0.07562954723834991, Total Loss 37.77557373046875\n",
      "12: Encoding Loss 3.5034873485565186, Transition Loss -0.35625582933425903, Classifier Loss 0.06446696072816849, Total Loss 27.467477798461914\n",
      "12: Encoding Loss 7.3176093101501465, Transition Loss -1.5616614818572998, Classifier Loss 0.15680743753910065, Total Loss 59.585777282714844\n",
      "12: Encoding Loss 3.3541488647460938, Transition Loss -2.428743362426758, Classifier Loss 0.06185451149940491, Total Loss 26.30937385559082\n",
      "12: Encoding Loss 3.510345458984375, Transition Loss -1.357409119606018, Classifier Loss 0.14777739346027374, Total Loss 35.839271545410156\n",
      "12: Encoding Loss 4.623594760894775, Transition Loss -1.8221313953399658, Classifier Loss 0.08014603704214096, Total Loss 35.75544357299805\n",
      "12: Encoding Loss 4.396515369415283, Transition Loss 0.6749581098556519, Classifier Loss 0.04986002668738365, Total Loss 31.63507843017578\n",
      "12: Encoding Loss 6.187333106994629, Transition Loss -1.1138397455215454, Classifier Loss 0.14005765318870544, Total Loss 51.12931823730469\n",
      "12: Encoding Loss 4.03987455368042, Transition Loss -1.4512860774993896, Classifier Loss 0.06494294852018356, Total Loss 30.73296356201172\n",
      "12: Encoding Loss 4.049534797668457, Transition Loss -1.3237495422363281, Classifier Loss 0.11721285432577133, Total Loss 36.01796340942383\n",
      "12: Encoding Loss 5.045899391174316, Transition Loss -1.4287643432617188, Classifier Loss 0.21657538414001465, Total Loss 51.93236541748047\n",
      "12: Encoding Loss 4.556153297424316, Transition Loss -1.358762502670288, Classifier Loss 0.08228497952222824, Total Loss 35.564876556396484\n",
      "12: Encoding Loss 4.765798091888428, Transition Loss -0.33216744661331177, Classifier Loss 0.05393156781792641, Total Loss 33.98781204223633\n",
      "12: Encoding Loss 8.444488525390625, Transition Loss 0.08815178275108337, Classifier Loss 0.19842368364334106, Total Loss 70.54457092285156\n",
      "12: Encoding Loss 7.256814002990723, Transition Loss -1.6514843702316284, Classifier Loss 0.10982747375965118, Total Loss 54.522972106933594\n",
      "12: Encoding Loss 5.252735137939453, Transition Loss -1.1326713562011719, Classifier Loss 0.13154202699661255, Total Loss 44.670162200927734\n",
      "12: Encoding Loss 6.086629867553711, Transition Loss -1.128716230392456, Classifier Loss 0.12040375173091888, Total Loss 48.5597038269043\n",
      "12: Encoding Loss 5.643352031707764, Transition Loss -1.1589741706848145, Classifier Loss 0.13608409464359283, Total Loss 47.46805953979492\n",
      "12: Encoding Loss 8.224000930786133, Transition Loss -2.3846638202667236, Classifier Loss 0.06122979521751404, Total Loss 55.46603012084961\n",
      "12: Encoding Loss 4.903332710266113, Transition Loss -1.9156851768493652, Classifier Loss 0.11444941908121109, Total Loss 40.86417007446289\n",
      "12: Encoding Loss 3.4204154014587402, Transition Loss 0.11204665899276733, Classifier Loss 0.05887116864323616, Total Loss 26.454429626464844\n",
      "12: Encoding Loss 4.229295253753662, Transition Loss -1.0814462900161743, Classifier Loss 0.09131687134504318, Total Loss 34.50703048706055\n",
      "12: Encoding Loss 6.876032829284668, Transition Loss 0.013928607106208801, Classifier Loss 0.07817739248275757, Total Loss 49.07950973510742\n",
      "12: Encoding Loss 7.117598056793213, Transition Loss -0.34519195556640625, Classifier Loss 0.20512636005878448, Total Loss 63.21808624267578\n",
      "12: Encoding Loss 4.127735614776611, Transition Loss -0.2777836322784424, Classifier Loss 0.05546589195728302, Total Loss 30.31289291381836\n",
      "12: Encoding Loss 4.745822906494141, Transition Loss -1.6042964458465576, Classifier Loss 0.1367345154285431, Total Loss 42.14775085449219\n",
      "12: Encoding Loss 3.6631112098693848, Transition Loss -1.9034125804901123, Classifier Loss 0.028023555874824524, Total Loss 24.780261993408203\n",
      "12: Encoding Loss 7.087927341461182, Transition Loss -0.980482280254364, Classifier Loss 0.07638762891292572, Total Loss 50.16593551635742\n",
      "12: Encoding Loss 4.717085838317871, Transition Loss -0.010576829314231873, Classifier Loss 0.15347528457641602, Total Loss 43.65004348754883\n",
      "12: Encoding Loss 2.645937442779541, Transition Loss -1.9320106506347656, Classifier Loss 0.10890205204486847, Total Loss 26.765058517456055\n",
      "12: Encoding Loss 7.550408363342285, Transition Loss -0.24787965416908264, Classifier Loss 0.05423295497894287, Total Loss 50.72564697265625\n",
      "12: Encoding Loss 7.155493259429932, Transition Loss -0.46380552649497986, Classifier Loss 0.14935502409934998, Total Loss 57.8682746887207\n",
      "12: Encoding Loss 3.7064616680145264, Transition Loss -1.074617862701416, Classifier Loss 0.053389765322208405, Total Loss 27.57731819152832\n",
      "12: Encoding Loss 10.104318618774414, Transition Loss -2.082385301589966, Classifier Loss 0.19213013350963593, Total Loss 79.83809661865234\n",
      "12: Encoding Loss 5.942398548126221, Transition Loss -1.699628472328186, Classifier Loss 0.03875601664185524, Total Loss 39.52931594848633\n",
      "12: Encoding Loss 5.741398811340332, Transition Loss -0.8306453227996826, Classifier Loss 0.0831741914153099, Total Loss 42.76548385620117\n",
      "12: Encoding Loss 4.043997764587402, Transition Loss -2.3686869144439697, Classifier Loss 0.07046066969633102, Total Loss 31.309106826782227\n",
      "12: Encoding Loss 3.8424220085144043, Transition Loss -2.3610429763793945, Classifier Loss 0.11557792872190475, Total Loss 34.61138153076172\n",
      "12: Encoding Loss 5.073388576507568, Transition Loss -4.270008087158203, Classifier Loss 0.09051160514354706, Total Loss 39.489784240722656\n",
      "12: Encoding Loss 4.5831828117370605, Transition Loss -0.7962074875831604, Classifier Loss 0.11157329380512238, Total Loss 38.65610885620117\n",
      "12: Encoding Loss 5.994855880737305, Transition Loss -2.0875601768493652, Classifier Loss 0.20899012684822083, Total Loss 56.867313385009766\n",
      "12: Encoding Loss 3.397599697113037, Transition Loss -1.2216033935546875, Classifier Loss 0.20808196067810059, Total Loss 41.19330596923828\n",
      "12: Encoding Loss 5.783267021179199, Transition Loss -1.695595145225525, Classifier Loss 0.07227563858032227, Total Loss 41.926490783691406\n",
      "12: Encoding Loss 4.329509735107422, Transition Loss 0.027387887239456177, Classifier Loss 0.0828496441245079, Total Loss 34.272979736328125\n",
      "12: Encoding Loss 2.8436872959136963, Transition Loss -0.07968717813491821, Classifier Loss 0.07162346690893173, Total Loss 24.22443962097168\n",
      "12: Encoding Loss 4.7391510009765625, Transition Loss -2.2328827381134033, Classifier Loss 0.13826270401477814, Total Loss 42.260284423828125\n",
      "12: Encoding Loss 2.781855344772339, Transition Loss -1.518544316291809, Classifier Loss 0.10053534060716629, Total Loss 26.744060516357422\n",
      "12: Encoding Loss 2.192396640777588, Transition Loss -2.199695587158203, Classifier Loss 0.0904565304517746, Total Loss 22.199153900146484\n",
      "12: Encoding Loss 6.941706657409668, Transition Loss -0.3683927357196808, Classifier Loss 0.032989758998155594, Total Loss 44.94906997680664\n",
      "12: Encoding Loss 4.030477046966553, Transition Loss -1.5267808437347412, Classifier Loss 0.04845837131142616, Total Loss 29.02808952331543\n",
      "12: Encoding Loss 5.171815872192383, Transition Loss -1.8191866874694824, Classifier Loss 0.16212989389896393, Total Loss 47.243160247802734\n",
      "12: Encoding Loss 3.8593945503234863, Transition Loss -1.949291467666626, Classifier Loss 0.03569509834051132, Total Loss 26.72509765625\n",
      "12: Encoding Loss 6.830565452575684, Transition Loss -1.5753127336502075, Classifier Loss 0.08907255530357361, Total Loss 49.890018463134766\n",
      "12: Encoding Loss 4.737040996551514, Transition Loss -1.4243652820587158, Classifier Loss 0.1367683857679367, Total Loss 42.09851837158203\n",
      "12: Encoding Loss 2.7742207050323486, Transition Loss -1.2765305042266846, Classifier Loss 0.06704940646886826, Total Loss 23.349754333496094\n",
      "12: Encoding Loss 6.532156944274902, Transition Loss -0.9380587339401245, Classifier Loss 0.06297741830348969, Total Loss 45.49031066894531\n",
      "12: Encoding Loss 5.244719505310059, Transition Loss -1.0339261293411255, Classifier Loss 0.17764471471309662, Total Loss 49.23237609863281\n",
      "12: Encoding Loss 3.58209228515625, Transition Loss -0.19748860597610474, Classifier Loss 0.14015723764896393, Total Loss 35.50819778442383\n",
      "12: Encoding Loss 4.72052526473999, Transition Loss -1.0826704502105713, Classifier Loss 0.11129745841026306, Total Loss 39.45246124267578\n",
      "12: Encoding Loss 5.590117454528809, Transition Loss -1.1580731868743896, Classifier Loss 0.1221771240234375, Total Loss 45.757957458496094\n",
      "12: Encoding Loss 4.955412864685059, Transition Loss -2.496838092803955, Classifier Loss 0.10471130907535553, Total Loss 40.20261001586914\n",
      "12: Encoding Loss 4.390814781188965, Transition Loss 0.892546534538269, Classifier Loss 0.06811469048261642, Total Loss 33.51337814331055\n",
      "12: Encoding Loss 5.259937763214111, Transition Loss -1.6263971328735352, Classifier Loss 0.16075880825519562, Total Loss 47.63485336303711\n",
      "12: Encoding Loss 5.287975311279297, Transition Loss -2.5142006874084473, Classifier Loss 0.038290634751319885, Total Loss 35.555908203125\n",
      "12: Encoding Loss 4.572389602661133, Transition Loss -1.8104124069213867, Classifier Loss 0.12484224140644073, Total Loss 39.91783905029297\n",
      "12: Encoding Loss 3.786653995513916, Transition Loss 0.860711395740509, Classifier Loss 0.11655814200639725, Total Loss 34.72002410888672\n",
      "12: Encoding Loss 3.068572759628296, Transition Loss -1.3653781414031982, Classifier Loss 0.05770304799079895, Total Loss 24.181198120117188\n",
      "12: Encoding Loss 5.575783729553223, Transition Loss -1.8242251873016357, Classifier Loss 0.07066280394792557, Total Loss 40.52025604248047\n",
      "12: Encoding Loss 4.431410789489746, Transition Loss 1.0878547430038452, Classifier Loss 0.06347579509019852, Total Loss 33.37118911743164\n",
      "12: Encoding Loss 3.738736391067505, Transition Loss -1.865915298461914, Classifier Loss 0.04865039885044098, Total Loss 27.29671287536621\n",
      "12: Encoding Loss 2.720120429992676, Transition Loss -1.3678146600723267, Classifier Loss 0.10436466336250305, Total Loss 26.756641387939453\n",
      "12: Encoding Loss 3.4864063262939453, Transition Loss 0.23921909928321838, Classifier Loss 0.050994206219911575, Total Loss 26.11354637145996\n",
      "12: Encoding Loss 10.449751853942871, Transition Loss 0.18471327424049377, Classifier Loss 0.23615935444831848, Total Loss 86.38833618164062\n",
      "12: Encoding Loss 6.287167549133301, Transition Loss -1.1305582523345947, Classifier Loss 0.094383604824543, Total Loss 47.16091537475586\n",
      "12: Encoding Loss 8.12723445892334, Transition Loss -1.0762187242507935, Classifier Loss 0.15615352988243103, Total Loss 64.37833404541016\n",
      "12: Encoding Loss 5.912348747253418, Transition Loss -1.6525585651397705, Classifier Loss 0.0506652295589447, Total Loss 40.53995895385742\n",
      "12: Encoding Loss 5.0554728507995605, Transition Loss -1.8806629180908203, Classifier Loss 0.04789420962333679, Total Loss 35.12150955200195\n",
      "12: Encoding Loss 4.637019157409668, Transition Loss -0.4289482831954956, Classifier Loss 0.1200600191950798, Total Loss 39.827945709228516\n",
      "12: Encoding Loss 4.449551105499268, Transition Loss -2.1618123054504395, Classifier Loss 0.09658634662628174, Total Loss 36.35507583618164\n",
      "12: Encoding Loss 2.8615729808807373, Transition Loss -0.17581012845039368, Classifier Loss 0.1284974068403244, Total Loss 30.01910972595215\n",
      "12: Encoding Loss 2.993717670440674, Transition Loss 0.8324398994445801, Classifier Loss 0.03766252100467682, Total Loss 22.061534881591797\n",
      "12: Encoding Loss 6.517783164978027, Transition Loss 0.08557146787643433, Classifier Loss 0.09027112275362015, Total Loss 48.16804122924805\n",
      "12: Encoding Loss 3.834949493408203, Transition Loss -2.236510753631592, Classifier Loss 0.0357176773250103, Total Loss 26.580570220947266\n",
      "12: Encoding Loss 9.154356002807617, Transition Loss -2.0476419925689697, Classifier Loss 0.12976990640163422, Total Loss 67.90231323242188\n",
      "12: Encoding Loss 6.830837249755859, Transition Loss -1.0823547840118408, Classifier Loss 0.1736908107995987, Total Loss 58.35367202758789\n",
      "12: Encoding Loss 4.074347496032715, Transition Loss -1.6602718830108643, Classifier Loss 0.05658784136176109, Total Loss 30.104206085205078\n",
      "12: Encoding Loss 6.1805925369262695, Transition Loss -0.3373605012893677, Classifier Loss 0.0926026776432991, Total Loss 46.343692779541016\n",
      "12: Encoding Loss 6.474442481994629, Transition Loss -0.9335535764694214, Classifier Loss 0.06494144350290298, Total Loss 45.34042739868164\n",
      "12: Encoding Loss 5.870903968811035, Transition Loss -1.1163660287857056, Classifier Loss 0.03785211592912674, Total Loss 39.010189056396484\n",
      "12: Encoding Loss 6.750826835632324, Transition Loss -1.3551398515701294, Classifier Loss 0.062243908643722534, Total Loss 46.72881317138672\n",
      "12: Encoding Loss 4.1764655113220215, Transition Loss -2.627140522003174, Classifier Loss 0.07870873808860779, Total Loss 32.928619384765625\n",
      "12: Encoding Loss 3.926539897918701, Transition Loss 0.008103370666503906, Classifier Loss 0.0761687308549881, Total Loss 31.179353713989258\n",
      "12: Encoding Loss 3.1654791831970215, Transition Loss -0.93083655834198, Classifier Loss 0.056930504739284515, Total Loss 24.68555450439453\n",
      "12: Encoding Loss 4.0332207679748535, Transition Loss -1.274621844291687, Classifier Loss 0.07465098053216934, Total Loss 31.66391372680664\n",
      "12: Encoding Loss 5.27890157699585, Transition Loss -0.2761552929878235, Classifier Loss 0.09425199776887894, Total Loss 41.0984992980957\n",
      "12: Encoding Loss 6.267305374145508, Transition Loss -0.9016885161399841, Classifier Loss 0.18661564588546753, Total Loss 56.26503372192383\n",
      "12: Encoding Loss 4.713269233703613, Transition Loss -1.828465223312378, Classifier Loss 0.1288028061389923, Total Loss 41.15916442871094\n",
      "12: Encoding Loss 6.714911460876465, Transition Loss -1.5197899341583252, Classifier Loss 0.09845958650112152, Total Loss 50.134822845458984\n",
      "12: Encoding Loss 4.385268211364746, Transition Loss -1.8701751232147217, Classifier Loss 0.053249459713697433, Total Loss 31.63580894470215\n",
      "12: Encoding Loss 2.6720519065856934, Transition Loss -1.3658818006515503, Classifier Loss 0.06134285032749176, Total Loss 22.166051864624023\n",
      "12: Encoding Loss 5.67694616317749, Transition Loss -2.323580503463745, Classifier Loss 0.05935551971197128, Total Loss 39.996299743652344\n",
      "12: Encoding Loss 6.862685203552246, Transition Loss -2.4222161769866943, Classifier Loss 0.17876717448234558, Total Loss 59.05186080932617\n",
      "12: Encoding Loss 4.9787163734436035, Transition Loss -2.187878370285034, Classifier Loss 0.04093855991959572, Total Loss 33.96528244018555\n",
      "12: Encoding Loss 3.1301395893096924, Transition Loss -1.1473218202590942, Classifier Loss 0.0456392802298069, Total Loss 23.34430694580078\n",
      "12: Encoding Loss 2.4409401416778564, Transition Loss -3.646178960800171, Classifier Loss 0.05509449541568756, Total Loss 20.15363121032715\n",
      "12: Encoding Loss 1.4722795486450195, Transition Loss -0.9839012622833252, Classifier Loss 0.1314922571182251, Total Loss 21.98250961303711\n",
      "12: Encoding Loss 5.732504367828369, Transition Loss -1.2433847188949585, Classifier Loss 0.0705973207950592, Total Loss 41.454261779785156\n",
      "12: Encoding Loss 5.305076599121094, Transition Loss -1.7095080614089966, Classifier Loss 0.09993231296539307, Total Loss 41.8230094909668\n",
      "12: Encoding Loss 3.8859589099884033, Transition Loss -1.4649070501327515, Classifier Loss 0.04911890625953674, Total Loss 28.22705841064453\n",
      "12: Encoding Loss 6.095994472503662, Transition Loss -1.166232705116272, Classifier Loss 0.10378852486610413, Total Loss 46.9543571472168\n",
      "12: Encoding Loss 6.015230655670166, Transition Loss -1.09971022605896, Classifier Loss 0.1729869395494461, Total Loss 53.38964080810547\n",
      "12: Encoding Loss 4.631211280822754, Transition Loss 0.11963993310928345, Classifier Loss 0.06407073885202408, Total Loss 34.2421989440918\n",
      "12: Encoding Loss 5.299802303314209, Transition Loss 0.2241087257862091, Classifier Loss 0.10094543546438217, Total Loss 41.983001708984375\n",
      "12: Encoding Loss 3.4993185997009277, Transition Loss -1.7205595970153809, Classifier Loss 0.0518302246928215, Total Loss 26.178247451782227\n",
      "12: Encoding Loss 7.753337860107422, Transition Loss 0.13603784143924713, Classifier Loss 0.052089184522628784, Total Loss 51.783363342285156\n",
      "12: Encoding Loss 5.730103969573975, Transition Loss -1.4743942022323608, Classifier Loss 0.04852532595396042, Total Loss 39.232566833496094\n",
      "12: Encoding Loss 6.23836612701416, Transition Loss -0.4925791621208191, Classifier Loss 0.09679912030696869, Total Loss 47.10991287231445\n",
      "12: Encoding Loss 5.19688606262207, Transition Loss -1.0448977947235107, Classifier Loss 0.041320495307445526, Total Loss 35.312950134277344\n",
      "12: Encoding Loss 5.186800003051758, Transition Loss -0.1648387908935547, Classifier Loss 0.06756346672773361, Total Loss 37.87708282470703\n",
      "12: Encoding Loss 3.634096145629883, Transition Loss -1.1962792873382568, Classifier Loss 0.06897485256195068, Total Loss 28.701583862304688\n",
      "12: Encoding Loss 5.2382493019104, Transition Loss -0.8780101537704468, Classifier Loss 0.18481852114200592, Total Loss 49.91099548339844\n",
      "12: Encoding Loss 2.759267807006836, Transition Loss -1.6519355773925781, Classifier Loss 0.04559759423136711, Total Loss 21.11470603942871\n",
      "12: Encoding Loss 6.4132866859436035, Transition Loss -1.2422715425491333, Classifier Loss 0.28420788049697876, Total Loss 66.90001678466797\n",
      "12: Encoding Loss 6.041491508483887, Transition Loss -0.6529266834259033, Classifier Loss 0.20298166573047638, Total Loss 56.54685974121094\n",
      "12: Encoding Loss 3.8061046600341797, Transition Loss -1.5800713300704956, Classifier Loss 0.12745440006256104, Total Loss 35.58143615722656\n",
      "12: Encoding Loss 5.653475761413574, Transition Loss -2.0127508640289307, Classifier Loss 0.06215596944093704, Total Loss 40.13564682006836\n",
      "12: Encoding Loss 6.613193511962891, Transition Loss -0.2977719008922577, Classifier Loss 0.09435578435659409, Total Loss 49.114620208740234\n",
      "12: Encoding Loss 7.295985221862793, Transition Loss -0.681756317615509, Classifier Loss 0.16461023688316345, Total Loss 60.23666763305664\n",
      "12: Encoding Loss 4.956987380981445, Transition Loss -1.9021438360214233, Classifier Loss 0.06527817249298096, Total Loss 36.268985748291016\n",
      "12: Encoding Loss 4.471431255340576, Transition Loss -3.355323076248169, Classifier Loss 0.07735910266637802, Total Loss 34.56315612792969\n",
      "12: Encoding Loss 6.823693752288818, Transition Loss -2.1585936546325684, Classifier Loss 0.16176597774028778, Total Loss 57.11790084838867\n",
      "12: Encoding Loss 5.872673511505127, Transition Loss -0.828392505645752, Classifier Loss 0.07059667259454727, Total Loss 42.29537582397461\n",
      "12: Encoding Loss 3.8245975971221924, Transition Loss -2.4181060791015625, Classifier Loss 0.07735057175159454, Total Loss 30.681676864624023\n",
      "12: Encoding Loss 2.693437099456787, Transition Loss -0.8937503695487976, Classifier Loss 0.08106110244989395, Total Loss 24.26637840270996\n",
      "12: Encoding Loss 7.004075527191162, Transition Loss -2.032561779022217, Classifier Loss 0.10879921913146973, Total Loss 52.903564453125\n",
      "12: Encoding Loss 4.413366317749023, Transition Loss -0.017248600721359253, Classifier Loss 0.07990095019340515, Total Loss 34.47028732299805\n",
      "12: Encoding Loss 5.930481433868408, Transition Loss -0.6736798286437988, Classifier Loss 0.06646694988012314, Total Loss 42.229312896728516\n",
      "12: Encoding Loss 3.760504961013794, Transition Loss -0.994520902633667, Classifier Loss 0.03806089237332344, Total Loss 26.36872100830078\n",
      "12: Encoding Loss 4.129888534545898, Transition Loss -1.4443610906600952, Classifier Loss 0.06639054417610168, Total Loss 31.417810440063477\n",
      "12: Encoding Loss 7.766333103179932, Transition Loss 0.155079185962677, Classifier Loss 0.1627574861049652, Total Loss 62.9357795715332\n",
      "12: Encoding Loss 5.495182037353516, Transition Loss -1.399440884590149, Classifier Loss 0.1554678976535797, Total Loss 48.5173225402832\n",
      "12: Encoding Loss 5.446866035461426, Transition Loss -3.0965421199798584, Classifier Loss 0.14781911671161652, Total Loss 47.46187210083008\n",
      "12: Encoding Loss 3.5775628089904785, Transition Loss -2.0489587783813477, Classifier Loss 0.05577300488948822, Total Loss 27.041858673095703\n",
      "12: Encoding Loss 7.465699672698975, Transition Loss -1.8545527458190918, Classifier Loss 0.13176490366458893, Total Loss 57.96995162963867\n",
      "12: Encoding Loss 5.246414661407471, Transition Loss -0.7133486270904541, Classifier Loss 0.04822201654314995, Total Loss 36.3004035949707\n",
      "12: Encoding Loss 4.693992614746094, Transition Loss -1.4600800275802612, Classifier Loss 0.09402254223823547, Total Loss 37.56562805175781\n",
      "12: Encoding Loss 3.813526153564453, Transition Loss -1.9162932634353638, Classifier Loss 0.03662462905049324, Total Loss 26.5428524017334\n",
      "12: Encoding Loss 5.241867542266846, Transition Loss -1.3028450012207031, Classifier Loss 0.1685219407081604, Total Loss 48.30287551879883\n",
      "12: Encoding Loss 6.009848594665527, Transition Loss -3.1627824306488037, Classifier Loss 0.1056528389453888, Total Loss 46.623111724853516\n",
      "12: Encoding Loss 3.9341583251953125, Transition Loss -0.7494490742683411, Classifier Loss 0.08518856018781662, Total Loss 32.123504638671875\n",
      "12: Encoding Loss 6.698117733001709, Transition Loss -0.08811694383621216, Classifier Loss 0.14046111702919006, Total Loss 54.23478698730469\n",
      "12: Encoding Loss 6.859461784362793, Transition Loss -1.3771283626556396, Classifier Loss 0.13336969912052155, Total Loss 54.493194580078125\n",
      "12: Encoding Loss 5.42263126373291, Transition Loss -2.663886785507202, Classifier Loss 0.0925121083855629, Total Loss 41.78593826293945\n",
      "12: Encoding Loss 6.36392879486084, Transition Loss -1.1440348625183105, Classifier Loss 0.03803308308124542, Total Loss 41.98642349243164\n",
      "12: Encoding Loss 5.146512031555176, Transition Loss 0.4177771508693695, Classifier Loss 0.09334858506917953, Total Loss 40.38104248046875\n",
      "12: Encoding Loss 5.44467830657959, Transition Loss -0.6791092157363892, Classifier Loss 0.12281662970781326, Total Loss 44.949462890625\n",
      "12: Encoding Loss 7.210007667541504, Transition Loss -0.7854062914848328, Classifier Loss 0.047914501279592514, Total Loss 48.051185607910156\n",
      "12: Encoding Loss 4.996382236480713, Transition Loss -2.2263522148132324, Classifier Loss 0.048165351152420044, Total Loss 34.793941497802734\n",
      "12: Encoding Loss 6.749079704284668, Transition Loss -1.544860601425171, Classifier Loss 0.1414514183998108, Total Loss 54.63900375366211\n",
      "12: Encoding Loss 5.16751766204834, Transition Loss -0.9501105546951294, Classifier Loss 0.18858987092971802, Total Loss 49.86371612548828\n",
      "12: Encoding Loss 5.324660778045654, Transition Loss -2.649402141571045, Classifier Loss 0.10222666710615158, Total Loss 42.16957092285156\n",
      "12: Encoding Loss 3.258091449737549, Transition Loss 0.42437857389450073, Classifier Loss 0.08653810620307922, Total Loss 28.372112274169922\n",
      "12: Encoding Loss 3.380028486251831, Transition Loss -0.6109414100646973, Classifier Loss 0.06190504878759384, Total Loss 26.47043228149414\n",
      "12: Encoding Loss 2.4501214027404785, Transition Loss -1.0885131359100342, Classifier Loss 0.06991476565599442, Total Loss 21.691770553588867\n",
      "12: Encoding Loss 7.359728813171387, Transition Loss -2.1390633583068848, Classifier Loss 0.133116215467453, Total Loss 57.46914291381836\n",
      "12: Encoding Loss 4.52205228805542, Transition Loss -1.5192773342132568, Classifier Loss 0.10048016905784607, Total Loss 37.179725646972656\n",
      "12: Encoding Loss 6.344656944274902, Transition Loss -1.586613416671753, Classifier Loss 0.08882084488868713, Total Loss 46.94939422607422\n",
      "12: Encoding Loss 4.442202568054199, Transition Loss -1.0935657024383545, Classifier Loss 0.11226970702409744, Total Loss 37.8797492980957\n",
      "12: Encoding Loss 4.45028018951416, Transition Loss -1.7926561832427979, Classifier Loss 0.11296987533569336, Total Loss 37.99795150756836\n",
      "12: Encoding Loss 4.010348796844482, Transition Loss -1.74812912940979, Classifier Loss 0.13474228978157043, Total Loss 37.53562545776367\n",
      "12: Encoding Loss 4.431063652038574, Transition Loss -1.3860546350479126, Classifier Loss 0.10273144394159317, Total Loss 36.85897445678711\n",
      "12: Encoding Loss 6.511298179626465, Transition Loss -1.086526870727539, Classifier Loss 0.1134939193725586, Total Loss 50.416748046875\n",
      "12: Encoding Loss 5.025707244873047, Transition Loss -0.6934366226196289, Classifier Loss 0.22003625333309174, Total Loss 52.1575927734375\n",
      "12: Encoding Loss 6.8197197914123535, Transition Loss -1.8178647756576538, Classifier Loss 0.09503383934497833, Total Loss 50.42097473144531\n",
      "12: Encoding Loss 4.32627010345459, Transition Loss -1.9056663513183594, Classifier Loss 0.07032786309719086, Total Loss 32.989646911621094\n",
      "12: Encoding Loss 6.2371015548706055, Transition Loss -1.511345386505127, Classifier Loss 0.07149282097816467, Total Loss 44.5712890625\n",
      "12: Encoding Loss 4.388421535491943, Transition Loss -0.9386030435562134, Classifier Loss 0.07481303066015244, Total Loss 33.811458587646484\n",
      "12: Encoding Loss 4.456470966339111, Transition Loss -0.8978033065795898, Classifier Loss 0.05027613043785095, Total Loss 31.766080856323242\n",
      "12: Encoding Loss 3.0901761054992676, Transition Loss -0.42210131883621216, Classifier Loss 0.063070148229599, Total Loss 24.847902297973633\n",
      "12: Encoding Loss 5.323354721069336, Transition Loss -1.0120675563812256, Classifier Loss 0.07139966636896133, Total Loss 39.07969284057617\n",
      "12: Encoding Loss 5.763503551483154, Transition Loss -0.7225204706192017, Classifier Loss 0.163212850689888, Total Loss 50.90201950073242\n",
      "12: Encoding Loss 3.4258103370666504, Transition Loss -1.50813889503479, Classifier Loss 0.15980499982833862, Total Loss 36.534759521484375\n",
      "12: Encoding Loss 4.077536106109619, Transition Loss -0.02270667254924774, Classifier Loss 0.11136762797832489, Total Loss 35.60197448730469\n",
      "12: Encoding Loss 6.982237815856934, Transition Loss -1.2037694454193115, Classifier Loss 0.18686307966709137, Total Loss 60.579254150390625\n",
      "12: Encoding Loss 6.043461322784424, Transition Loss -0.2826489508152008, Classifier Loss 0.09282227605581284, Total Loss 45.54288101196289\n",
      "12: Encoding Loss 5.219510078430176, Transition Loss -0.8407815098762512, Classifier Loss 0.04137131944298744, Total Loss 35.453857421875\n",
      "12: Encoding Loss 6.300227165222168, Transition Loss -0.47542834281921387, Classifier Loss 0.1084139421582222, Total Loss 48.64257049560547\n",
      "12: Encoding Loss 4.62964391708374, Transition Loss -3.1051065921783447, Classifier Loss 0.11529289931058884, Total Loss 39.305912017822266\n",
      "12: Encoding Loss 3.31825590133667, Transition Loss 1.1317647695541382, Classifier Loss 0.06649139523506165, Total Loss 27.011381149291992\n",
      "12: Encoding Loss 3.496396541595459, Transition Loss -2.356538772583008, Classifier Loss 0.0689387321472168, Total Loss 27.87131118774414\n",
      "12: Encoding Loss 5.43369722366333, Transition Loss -2.159245014190674, Classifier Loss 0.028904695063829422, Total Loss 35.491790771484375\n",
      "12: Encoding Loss 6.155134677886963, Transition Loss -1.6592504978179932, Classifier Loss 0.06311310827732086, Total Loss 43.241455078125\n",
      "12: Encoding Loss 5.8371710777282715, Transition Loss -1.1949639320373535, Classifier Loss 0.04899606853723526, Total Loss 39.92216110229492\n",
      "12: Encoding Loss 4.4359283447265625, Transition Loss -0.7968102693557739, Classifier Loss 0.09939704090356827, Total Loss 36.554954528808594\n",
      "12: Encoding Loss 4.844432830810547, Transition Loss -0.742683470249176, Classifier Loss 0.04482196271419525, Total Loss 33.54849624633789\n",
      "12: Encoding Loss 2.986069917678833, Transition Loss -0.746259331703186, Classifier Loss 0.08685941994190216, Total Loss 26.602062225341797\n",
      "12: Encoding Loss 6.6921844482421875, Transition Loss -2.4003446102142334, Classifier Loss 0.060107097029685974, Total Loss 46.16285705566406\n",
      "12: Encoding Loss 4.73602819442749, Transition Loss -1.4615012407302856, Classifier Loss 0.052284616976976395, Total Loss 33.644046783447266\n",
      "12: Encoding Loss 6.359203338623047, Transition Loss -1.6061450242996216, Classifier Loss 0.1445786952972412, Total Loss 52.612449645996094\n",
      "12: Encoding Loss 4.220782279968262, Transition Loss -1.4948323965072632, Classifier Loss 0.02196132205426693, Total Loss 27.520231246948242\n",
      "12: Encoding Loss 5.718911170959473, Transition Loss -0.20082348585128784, Classifier Loss 0.07276143878698349, Total Loss 41.589534759521484\n",
      "12: Encoding Loss 5.03689432144165, Transition Loss -1.4102405309677124, Classifier Loss 0.12957710027694702, Total Loss 43.17851257324219\n",
      "12: Encoding Loss 4.558693885803223, Transition Loss -1.44779634475708, Classifier Loss 0.06988026946783066, Total Loss 34.3396110534668\n",
      "12: Encoding Loss 2.8770751953125, Transition Loss -2.435129165649414, Classifier Loss 0.05498649179935455, Total Loss 22.7601261138916\n",
      "12: Encoding Loss 5.280359745025635, Transition Loss -3.3945648670196533, Classifier Loss 0.04932627081871033, Total Loss 36.61343002319336\n",
      "12: Encoding Loss 3.288918972015381, Transition Loss -0.6200181245803833, Classifier Loss 0.1327168345451355, Total Loss 33.00495147705078\n",
      "12: Encoding Loss 4.984323978424072, Transition Loss -0.8351895809173584, Classifier Loss 0.14180327951908112, Total Loss 44.0859375\n",
      "12: Encoding Loss 5.611618995666504, Transition Loss -0.5469388961791992, Classifier Loss 0.09956881403923035, Total Loss 43.626380920410156\n",
      "12: Encoding Loss 3.943429470062256, Transition Loss -0.4195297360420227, Classifier Loss 0.044031739234924316, Total Loss 28.063583374023438\n",
      "12: Encoding Loss 8.368622779846191, Transition Loss -0.15583261847496033, Classifier Loss 0.13102301955223083, Total Loss 63.31398010253906\n",
      "12: Encoding Loss 4.320007801055908, Transition Loss -0.984761118888855, Classifier Loss 0.08002105355262756, Total Loss 33.92176055908203\n",
      "12: Encoding Loss 7.982194423675537, Transition Loss -0.34871232509613037, Classifier Loss 0.18724490702152252, Total Loss 66.61752319335938\n",
      "12: Encoding Loss 5.740693092346191, Transition Loss -2.684859275817871, Classifier Loss 0.16156965494155884, Total Loss 50.60005187988281\n",
      "12: Encoding Loss 5.779264450073242, Transition Loss -1.0251288414001465, Classifier Loss 0.10571469366550446, Total Loss 45.246646881103516\n",
      "12: Encoding Loss 5.556702613830566, Transition Loss -1.8418623208999634, Classifier Loss 0.14476647973060608, Total Loss 47.81612777709961\n",
      "12: Encoding Loss 3.1812005043029785, Transition Loss -1.3038607835769653, Classifier Loss 0.06994064152240753, Total Loss 26.080747604370117\n",
      "12: Encoding Loss 3.766392469406128, Transition Loss -0.4086320698261261, Classifier Loss 0.08004564046859741, Total Loss 30.60275650024414\n",
      "12: Encoding Loss 4.433947563171387, Transition Loss -0.7716090083122253, Classifier Loss 0.09745655208826065, Total Loss 36.34903335571289\n",
      "12: Encoding Loss 4.415586471557617, Transition Loss -2.738801956176758, Classifier Loss 0.0693122148513794, Total Loss 33.423648834228516\n",
      "12: Encoding Loss 5.509383201599121, Transition Loss -1.5292404890060425, Classifier Loss 0.12186892330646515, Total Loss 45.242584228515625\n",
      "12: Encoding Loss 4.969533443450928, Transition Loss -1.3391082286834717, Classifier Loss 0.08724088221788406, Total Loss 38.54075622558594\n",
      "12: Encoding Loss 3.4786770343780518, Transition Loss -0.2919888198375702, Classifier Loss 0.08680833131074905, Total Loss 29.552778244018555\n",
      "12: Encoding Loss 4.124666690826416, Transition Loss -1.5517535209655762, Classifier Loss 0.04445461183786392, Total Loss 29.192842483520508\n",
      "12: Encoding Loss 5.986545562744141, Transition Loss -1.2001516819000244, Classifier Loss 0.12239992618560791, Total Loss 48.158782958984375\n",
      "12: Encoding Loss 4.967966556549072, Transition Loss 0.15297210216522217, Classifier Loss 0.055287305265665054, Total Loss 35.39772033691406\n",
      "12: Encoding Loss 6.725955009460449, Transition Loss 0.12944304943084717, Classifier Loss 0.09101836383342743, Total Loss 49.50934600830078\n",
      "12: Encoding Loss 6.380059719085693, Transition Loss -1.7265485525131226, Classifier Loss 0.2569771409034729, Total Loss 63.97738265991211\n",
      "12: Encoding Loss 6.115495204925537, Transition Loss -1.6724032163619995, Classifier Loss 0.05497252568602562, Total Loss 42.18955993652344\n",
      "12: Encoding Loss 5.110233306884766, Transition Loss -1.346071481704712, Classifier Loss 0.10930518805980682, Total Loss 41.59138107299805\n",
      "12: Encoding Loss 4.308253288269043, Transition Loss -1.246556282043457, Classifier Loss 0.07297071069478989, Total Loss 33.14609146118164\n",
      "12: Encoding Loss 5.606514930725098, Transition Loss -2.1249499320983887, Classifier Loss 0.17783382534980774, Total Loss 51.42162322998047\n",
      "12: Encoding Loss 3.5443649291992188, Transition Loss -1.035581111907959, Classifier Loss 0.058357685804367065, Total Loss 27.101545333862305\n",
      "12: Encoding Loss 6.87042236328125, Transition Loss -2.1506576538085938, Classifier Loss 0.10335257649421692, Total Loss 51.55693054199219\n",
      "12: Encoding Loss 5.750304222106934, Transition Loss -1.4666674137115479, Classifier Loss 0.12966591119766235, Total Loss 47.467830657958984\n",
      "12: Encoding Loss 6.020948886871338, Transition Loss -1.8475385904312134, Classifier Loss 0.04591885954141617, Total Loss 40.71683883666992\n",
      "12: Encoding Loss 6.557254791259766, Transition Loss -0.8438228368759155, Classifier Loss 0.12994229793548584, Total Loss 52.337425231933594\n",
      "12: Encoding Loss 2.5564255714416504, Transition Loss -1.936553716659546, Classifier Loss 0.0587073378264904, Total Loss 21.208513259887695\n",
      "12: Encoding Loss 2.8254265785217285, Transition Loss -0.5921494960784912, Classifier Loss 0.04776766151189804, Total Loss 21.729089736938477\n",
      "12: Encoding Loss 5.674625396728516, Transition Loss -0.441365122795105, Classifier Loss 0.09147899597883224, Total Loss 43.19547653198242\n",
      "12: Encoding Loss 3.7578330039978027, Transition Loss 0.4762367010116577, Classifier Loss 0.0692426934838295, Total Loss 29.661762237548828\n",
      "12: Encoding Loss 6.054412841796875, Transition Loss -0.9591912031173706, Classifier Loss 0.14675500988960266, Total Loss 51.001590728759766\n",
      "12: Encoding Loss 5.574094772338867, Transition Loss -1.8640838861465454, Classifier Loss 0.057033952325582504, Total Loss 39.147220611572266\n",
      "12: Encoding Loss 3.7833824157714844, Transition Loss -2.510209083557129, Classifier Loss 0.10285912454128265, Total Loss 32.98520278930664\n",
      "12: Encoding Loss 3.0944066047668457, Transition Loss -1.4810011386871338, Classifier Loss 0.06483720988035202, Total Loss 25.04956817626953\n",
      "12: Encoding Loss 7.131923675537109, Transition Loss -1.4332908391952515, Classifier Loss 0.16736802458763123, Total Loss 59.52777099609375\n",
      "12: Encoding Loss 4.026617527008057, Transition Loss -0.13825452327728271, Classifier Loss 0.16539938747882843, Total Loss 40.69959259033203\n",
      "12: Encoding Loss 4.6357808113098145, Transition Loss -1.4604129791259766, Classifier Loss 0.051717061549425125, Total Loss 32.985809326171875\n",
      "12: Encoding Loss 5.2896857261657715, Transition Loss 0.12861938774585724, Classifier Loss 0.1343071013689041, Total Loss 45.220272064208984\n",
      "12: Encoding Loss 3.123487949371338, Transition Loss 0.614536464214325, Classifier Loss 0.05427822098135948, Total Loss 24.414566040039062\n",
      "12: Encoding Loss 3.7529938220977783, Transition Loss -1.4120051860809326, Classifier Loss 0.07627855241298676, Total Loss 30.145254135131836\n",
      "12: Encoding Loss 5.867891788482666, Transition Loss -0.8371137976646423, Classifier Loss 0.08976650983095169, Total Loss 44.18366622924805\n",
      "12: Encoding Loss 3.5992350578308105, Transition Loss -0.9290030002593994, Classifier Loss 0.11314623802900314, Total Loss 32.909664154052734\n",
      "12: Encoding Loss 6.337133407592773, Transition Loss -1.0001188516616821, Classifier Loss 0.1305273175239563, Total Loss 51.075130462646484\n",
      "12: Encoding Loss 6.713804721832275, Transition Loss -1.2514359951019287, Classifier Loss 0.17959344387054443, Total Loss 58.24167251586914\n",
      "12: Encoding Loss 4.6112165451049805, Transition Loss -1.1505537033081055, Classifier Loss 0.02769407257437706, Total Loss 30.436248779296875\n",
      "12: Encoding Loss 4.66404390335083, Transition Loss -0.4010242819786072, Classifier Loss 0.15395858883857727, Total Loss 43.37996292114258\n",
      "12: Encoding Loss 5.77571964263916, Transition Loss -3.349134922027588, Classifier Loss 0.15706735849380493, Total Loss 50.35971450805664\n",
      "12: Encoding Loss 6.6851935386657715, Transition Loss -1.561838150024414, Classifier Loss 0.04291680082678795, Total Loss 44.402217864990234\n",
      "12: Encoding Loss 5.042011737823486, Transition Loss -1.24072265625, Classifier Loss 0.062403857707977295, Total Loss 36.49196243286133\n",
      "12: Encoding Loss 6.161834239959717, Transition Loss -2.2249228954315186, Classifier Loss 0.14039166271686554, Total Loss 51.00928497314453\n",
      "12: Encoding Loss 5.252404689788818, Transition Loss -0.4210352301597595, Classifier Loss 0.056184787303209305, Total Loss 37.13274002075195\n",
      "12: Encoding Loss 2.9945831298828125, Transition Loss -1.5620760917663574, Classifier Loss 0.06949800252914429, Total Loss 24.91667366027832\n",
      "12: Encoding Loss 4.97532844543457, Transition Loss -1.896914005279541, Classifier Loss 0.11038579046726227, Total Loss 40.889793395996094\n",
      "12: Encoding Loss 9.214109420776367, Transition Loss -3.231475353240967, Classifier Loss 0.1555580496788025, Total Loss 70.83917999267578\n",
      "12: Encoding Loss 5.038607120513916, Transition Loss -1.3548705577850342, Classifier Loss 0.045763708651065826, Total Loss 34.807472229003906\n",
      "12: Encoding Loss 5.094001770019531, Transition Loss -0.9614095687866211, Classifier Loss 0.03904111683368683, Total Loss 34.46773910522461\n",
      "12: Encoding Loss 5.922250270843506, Transition Loss -0.20151109993457794, Classifier Loss 0.1511067897081375, Total Loss 50.64410400390625\n",
      "12: Encoding Loss 5.940977096557617, Transition Loss -0.8002246022224426, Classifier Loss 0.26240044832229614, Total Loss 61.885589599609375\n",
      "12: Encoding Loss 6.205636978149414, Transition Loss -1.5442911386489868, Classifier Loss 0.1741391122341156, Total Loss 54.647117614746094\n",
      "12: Encoding Loss 6.296337604522705, Transition Loss -2.1038565635681152, Classifier Loss 0.14478857815265656, Total Loss 52.25604248046875\n",
      "12: Encoding Loss 5.153087615966797, Transition Loss -1.4136710166931152, Classifier Loss 0.10725465416908264, Total Loss 41.643428802490234\n",
      "12: Encoding Loss 5.224720001220703, Transition Loss -0.4978415369987488, Classifier Loss 0.130239799618721, Total Loss 44.37210464477539\n",
      "12: Encoding Loss 6.750492095947266, Transition Loss -0.7452549934387207, Classifier Loss 0.15985363721847534, Total Loss 56.48801803588867\n",
      "12: Encoding Loss 4.626849174499512, Transition Loss -0.5348302125930786, Classifier Loss 0.05436338484287262, Total Loss 33.19721984863281\n",
      "12: Encoding Loss 7.517860412597656, Transition Loss -2.003640651702881, Classifier Loss 0.06521963328123093, Total Loss 51.628326416015625\n",
      "12: Encoding Loss 4.560961723327637, Transition Loss -0.9864667057991028, Classifier Loss 0.09869477152824402, Total Loss 37.23485565185547\n",
      "12: Encoding Loss 4.134365558624268, Transition Loss -1.67966628074646, Classifier Loss 0.08537112921476364, Total Loss 33.34263610839844\n",
      "12: Encoding Loss 5.491344451904297, Transition Loss -0.4043568968772888, Classifier Loss 0.16352108120918274, Total Loss 49.30001449584961\n",
      "12: Encoding Loss 4.7129807472229, Transition Loss -1.391770601272583, Classifier Loss 0.06175478547811508, Total Loss 34.45280838012695\n",
      "12: Encoding Loss 5.054943561553955, Transition Loss -1.425142526626587, Classifier Loss 0.13500076532363892, Total Loss 43.82917022705078\n",
      "12: Encoding Loss 10.806780815124512, Transition Loss -2.141753673553467, Classifier Loss 0.12570257484912872, Total Loss 77.41009521484375\n",
      "12: Encoding Loss 5.597897529602051, Transition Loss -0.6873464584350586, Classifier Loss 0.058938924223184586, Total Loss 39.48100280761719\n",
      "12: Encoding Loss 6.5517168045043945, Transition Loss 0.8000214099884033, Classifier Loss 0.05605239421129227, Total Loss 45.23554992675781\n",
      "12: Encoding Loss 4.511026859283447, Transition Loss -2.232799530029297, Classifier Loss 0.10530388355255127, Total Loss 37.59565734863281\n",
      "12: Encoding Loss 6.089921474456787, Transition Loss -1.5111531019210815, Classifier Loss 0.10876069962978363, Total Loss 47.415000915527344\n",
      "12: Encoding Loss 3.7216906547546387, Transition Loss -0.5694417357444763, Classifier Loss 0.08458064496517181, Total Loss 30.787981033325195\n",
      "12: Encoding Loss 5.446058750152588, Transition Loss -1.3473981618881226, Classifier Loss 0.18772700428962708, Total Loss 51.448516845703125\n",
      "12: Encoding Loss 9.02833366394043, Transition Loss -1.2949163913726807, Classifier Loss 0.10272690653800964, Total Loss 64.44217681884766\n",
      "12: Encoding Loss 4.507179260253906, Transition Loss -1.7336821556091309, Classifier Loss 0.08211369812488556, Total Loss 35.253753662109375\n",
      "12: Encoding Loss 5.436726093292236, Transition Loss -1.055129051208496, Classifier Loss 0.06871651113033295, Total Loss 39.49158477783203\n",
      "12: Encoding Loss 5.641570091247559, Transition Loss -2.043945789337158, Classifier Loss 0.04549828916788101, Total Loss 38.398433685302734\n",
      "12: Encoding Loss 5.22365665435791, Transition Loss -2.1700124740600586, Classifier Loss 0.04068157449364662, Total Loss 35.40922927856445\n",
      "12: Encoding Loss 4.428565979003906, Transition Loss -1.0405830144882202, Classifier Loss 0.02781064435839653, Total Loss 29.352046966552734\n",
      "12: Encoding Loss 4.122457027435303, Transition Loss -2.442619562149048, Classifier Loss 0.10400138050317764, Total Loss 35.13390350341797\n",
      "12: Encoding Loss 6.106639385223389, Transition Loss -0.5789622068405151, Classifier Loss 0.10503354668617249, Total Loss 47.14296340942383\n",
      "12: Encoding Loss 5.467613220214844, Transition Loss -0.7264155149459839, Classifier Loss 0.06292450428009033, Total Loss 39.09783935546875\n",
      "12: Encoding Loss 5.231630802154541, Transition Loss -3.0645456314086914, Classifier Loss 0.09505955874919891, Total Loss 40.89451599121094\n",
      "12: Encoding Loss 6.6937689781188965, Transition Loss -3.071655511856079, Classifier Loss 0.09323979914188385, Total Loss 49.48536682128906\n",
      "12: Encoding Loss 4.3138017654418945, Transition Loss 0.08115121722221375, Classifier Loss 0.06195562705397606, Total Loss 32.11083221435547\n",
      "12: Encoding Loss 4.738376617431641, Transition Loss -1.853036642074585, Classifier Loss 0.14115777611732483, Total Loss 42.5452995300293\n",
      "12: Encoding Loss 5.035107612609863, Transition Loss -1.382899284362793, Classifier Loss 0.07842928916215897, Total Loss 38.05302047729492\n",
      "12: Encoding Loss 3.5519466400146484, Transition Loss -1.859554409980774, Classifier Loss 0.06532561779022217, Total Loss 27.84349822998047\n",
      "12: Encoding Loss 4.897083759307861, Transition Loss -2.7516770362854004, Classifier Loss 0.13929389417171478, Total Loss 43.310791015625\n",
      "12: Encoding Loss 6.340243339538574, Transition Loss -2.130495548248291, Classifier Loss 0.0897090882062912, Total Loss 47.01152038574219\n",
      "12: Encoding Loss 6.53405237197876, Transition Loss -2.8017826080322266, Classifier Loss 0.10253589600324631, Total Loss 49.456783294677734\n",
      "12: Encoding Loss 7.1146955490112305, Transition Loss -1.0950322151184082, Classifier Loss 0.1482655555009842, Total Loss 57.5142936706543\n",
      "12: Encoding Loss 4.9640326499938965, Transition Loss 0.4281421899795532, Classifier Loss 0.09101322293281555, Total Loss 39.05677795410156\n",
      "12: Encoding Loss 4.804131507873535, Transition Loss -2.587756633758545, Classifier Loss 0.0708862692117691, Total Loss 35.91238021850586\n",
      "12: Encoding Loss 5.017934322357178, Transition Loss -1.7595796585083008, Classifier Loss 0.10422419756650925, Total Loss 40.529319763183594\n",
      "12: Encoding Loss 4.305242538452148, Transition Loss -1.6930272579193115, Classifier Loss 0.07431012392044067, Total Loss 33.26179122924805\n",
      "12: Encoding Loss 4.2297043800354, Transition Loss -1.9428619146347046, Classifier Loss 0.08979343622922897, Total Loss 34.35679244995117\n",
      "12: Encoding Loss 3.418613910675049, Transition Loss -2.1918716430664062, Classifier Loss 0.048235174268484116, Total Loss 25.33432388305664\n",
      "12: Encoding Loss 3.4319214820861816, Transition Loss -0.6932686567306519, Classifier Loss 0.09445785731077194, Total Loss 30.037038803100586\n",
      "12: Encoding Loss 5.113902568817139, Transition Loss -0.9100797176361084, Classifier Loss 0.11325541138648987, Total Loss 42.00859451293945\n",
      "12: Encoding Loss 6.199371337890625, Transition Loss -0.3566358685493469, Classifier Loss 0.09798147529363632, Total Loss 46.99423599243164\n",
      "12: Encoding Loss 4.034296989440918, Transition Loss -0.6138929128646851, Classifier Loss 0.1025642529129982, Total Loss 34.46196365356445\n",
      "12: Encoding Loss 6.927041530609131, Transition Loss -1.1948314905166626, Classifier Loss 0.10798699408769608, Total Loss 52.3604736328125\n",
      "12: Encoding Loss 6.384570598602295, Transition Loss -0.7836360335350037, Classifier Loss 0.12353783845901489, Total Loss 50.66089630126953\n",
      "12: Encoding Loss 4.661830425262451, Transition Loss -1.7201744318008423, Classifier Loss 0.13426420092582703, Total Loss 41.3967170715332\n",
      "12: Encoding Loss 5.108098983764648, Transition Loss -0.5697899460792542, Classifier Loss 0.1068616732954979, Total Loss 41.33453369140625\n",
      "12: Encoding Loss 7.554029941558838, Transition Loss -2.1689960956573486, Classifier Loss 0.15471330285072327, Total Loss 60.79464340209961\n",
      "12: Encoding Loss 4.834704399108887, Transition Loss -1.7698628902435303, Classifier Loss 0.04400239139795303, Total Loss 33.40775680541992\n",
      "12: Encoding Loss 6.823558807373047, Transition Loss -0.9137707948684692, Classifier Loss 0.1999606490135193, Total Loss 60.937049865722656\n",
      "12: Encoding Loss 5.667649269104004, Transition Loss -0.44916075468063354, Classifier Loss 0.12723411619663239, Total Loss 46.729129791259766\n",
      "12: Encoding Loss 4.879436492919922, Transition Loss -1.7616143226623535, Classifier Loss 0.11509120464324951, Total Loss 40.785037994384766\n",
      "12: Encoding Loss 4.413681983947754, Transition Loss -2.695753335952759, Classifier Loss 0.13326574862003326, Total Loss 39.807586669921875\n",
      "12: Encoding Loss 6.551532745361328, Transition Loss -2.104302167892456, Classifier Loss 0.12963147461414337, Total Loss 52.27149963378906\n",
      "12: Encoding Loss 4.341376781463623, Transition Loss -0.8027629256248474, Classifier Loss 0.08290995657444, Total Loss 34.33893585205078\n",
      "12: Encoding Loss 2.937316417694092, Transition Loss -1.2021808624267578, Classifier Loss 0.0633261427283287, Total Loss 23.95603370666504\n",
      "12: Encoding Loss 2.512218952178955, Transition Loss -1.6780097484588623, Classifier Loss 0.0661487877368927, Total Loss 21.68752098083496\n",
      "12: Encoding Loss 3.6285781860351562, Transition Loss -1.4283546209335327, Classifier Loss 0.08521172404289246, Total Loss 30.292068481445312\n",
      "12: Encoding Loss 8.733275413513184, Transition Loss 0.7419396638870239, Classifier Loss 0.10724321007728577, Total Loss 63.420753479003906\n",
      "12: Encoding Loss 5.7360029220581055, Transition Loss 0.17910730838775635, Classifier Loss 0.1399310827255249, Total Loss 48.480770111083984\n",
      "12: Encoding Loss 4.091264247894287, Transition Loss 0.6540762186050415, Classifier Loss 0.06508367508649826, Total Loss 31.317584991455078\n",
      "12: Encoding Loss 7.4663987159729, Transition Loss -0.4608267545700073, Classifier Loss 0.24566540122032166, Total Loss 69.36474609375\n",
      "12: Encoding Loss 4.676089286804199, Transition Loss -1.5951722860336304, Classifier Loss 0.04952734336256981, Total Loss 33.008636474609375\n",
      "12: Encoding Loss 4.671127796173096, Transition Loss -2.2017197608947754, Classifier Loss 0.10275494307279587, Total Loss 38.3013801574707\n",
      "12: Encoding Loss 5.840343952178955, Transition Loss -1.2025057077407837, Classifier Loss 0.1284930258989334, Total Loss 47.89088439941406\n",
      "12: Encoding Loss 5.071908473968506, Transition Loss -1.407410740852356, Classifier Loss 0.06259575486183167, Total Loss 36.69046401977539\n",
      "12: Encoding Loss 4.695700645446777, Transition Loss -0.7126113176345825, Classifier Loss 0.0552801676094532, Total Loss 33.701934814453125\n",
      "12: Encoding Loss 5.8743977546691895, Transition Loss -0.7839832901954651, Classifier Loss 0.09498681873083115, Total Loss 44.74475860595703\n",
      "12: Encoding Loss 3.631554126739502, Transition Loss 0.14083516597747803, Classifier Loss 0.030984515324234962, Total Loss 24.944110870361328\n",
      "12: Encoding Loss 6.944490909576416, Transition Loss 0.5560488700866699, Classifier Loss 0.087717704474926, Total Loss 50.661136627197266\n",
      "12: Encoding Loss 5.306868553161621, Transition Loss -1.890841007232666, Classifier Loss 0.1008649691939354, Total Loss 41.92695617675781\n",
      "12: Encoding Loss 4.157918453216553, Transition Loss -1.5482807159423828, Classifier Loss 0.12975403666496277, Total Loss 37.922298431396484\n",
      "12: Encoding Loss 6.378965854644775, Transition Loss -1.9507039785385132, Classifier Loss 0.06400169432163239, Total Loss 44.67318344116211\n",
      "12: Encoding Loss 3.5282318592071533, Transition Loss -1.0669504404067993, Classifier Loss 0.11209510266780853, Total Loss 32.378475189208984\n",
      "12: Encoding Loss 2.19389009475708, Transition Loss -1.6537244319915771, Classifier Loss 0.08504268527030945, Total Loss 21.666948318481445\n",
      "12: Encoding Loss 2.2455523014068604, Transition Loss -1.0862324237823486, Classifier Loss 0.05554088577628136, Total Loss 19.026968002319336\n",
      "12: Encoding Loss 8.572620391845703, Transition Loss -2.0987210273742676, Classifier Loss 0.10532685369253159, Total Loss 61.96757125854492\n",
      "12: Encoding Loss 4.266241550445557, Transition Loss -1.7136452198028564, Classifier Loss 0.04944410175085068, Total Loss 30.541175842285156\n",
      "12: Encoding Loss 2.486112117767334, Transition Loss -1.976119875907898, Classifier Loss 0.06891681253910065, Total Loss 21.807565689086914\n",
      "12: Encoding Loss 6.257588863372803, Transition Loss -1.2967524528503418, Classifier Loss 0.12055721879005432, Total Loss 49.600738525390625\n",
      "12: Encoding Loss 3.0678272247314453, Transition Loss -0.9557154774665833, Classifier Loss 0.05756145715713501, Total Loss 24.16272735595703\n",
      "12: Encoding Loss 5.257088661193848, Transition Loss -0.675385057926178, Classifier Loss 0.09948843717575073, Total Loss 41.49110412597656\n",
      "12: Encoding Loss 5.604405403137207, Transition Loss -0.5256484746932983, Classifier Loss 0.0670216754078865, Total Loss 40.328392028808594\n",
      "12: Encoding Loss 5.246598243713379, Transition Loss -2.82842755317688, Classifier Loss 0.11445282399654388, Total Loss 42.92374038696289\n",
      "12: Encoding Loss 5.237608432769775, Transition Loss -0.42065292596817017, Classifier Loss 0.11995036900043488, Total Loss 43.4205207824707\n",
      "12: Encoding Loss 5.032458305358887, Transition Loss -1.556367039680481, Classifier Loss 0.151006281375885, Total Loss 45.29475784301758\n",
      "12: Encoding Loss 4.655002593994141, Transition Loss -2.5852792263031006, Classifier Loss 0.09094978868961334, Total Loss 37.023963928222656\n",
      "12: Encoding Loss 3.5431079864501953, Transition Loss -1.209794521331787, Classifier Loss 0.03898882493376732, Total Loss 25.157045364379883\n",
      "12: Encoding Loss 7.362252712249756, Transition Loss -1.1822558641433716, Classifier Loss 0.14478187263011932, Total Loss 58.6512336730957\n",
      "12: Encoding Loss 6.546147346496582, Transition Loss -0.924181342124939, Classifier Loss 0.14313901960849762, Total Loss 53.59041976928711\n",
      "12: Encoding Loss 5.241599082946777, Transition Loss -1.2133171558380127, Classifier Loss 0.14144980907440186, Total Loss 45.594093322753906\n",
      "12: Encoding Loss 3.6849632263183594, Transition Loss -1.78079354763031, Classifier Loss 0.06129711866378784, Total Loss 28.238779067993164\n",
      "12: Encoding Loss 9.605772018432617, Transition Loss -1.1484622955322266, Classifier Loss 0.17738865315914154, Total Loss 75.373046875\n",
      "12: Encoding Loss 5.294720649719238, Transition Loss -2.5378851890563965, Classifier Loss 0.056442610919475555, Total Loss 37.41157150268555\n",
      "12: Encoding Loss 4.771546363830566, Transition Loss -1.906268835067749, Classifier Loss 0.13469648361206055, Total Loss 42.09816360473633\n",
      "12: Encoding Loss 6.324273586273193, Transition Loss -2.079906463623047, Classifier Loss 0.17256058752536774, Total Loss 55.20087432861328\n",
      "12: Encoding Loss 6.575533866882324, Transition Loss -1.4507914781570435, Classifier Loss 0.121928371489048, Total Loss 51.64546203613281\n",
      "12: Encoding Loss 5.3565239906311035, Transition Loss -0.6386828422546387, Classifier Loss 0.09116627275943756, Total Loss 41.255516052246094\n",
      "12: Encoding Loss 7.11562442779541, Transition Loss -2.702115297317505, Classifier Loss 0.08043073862791061, Total Loss 50.73574447631836\n",
      "12: Encoding Loss 7.375227451324463, Transition Loss -1.3804359436035156, Classifier Loss 0.18403840065002441, Total Loss 62.6546516418457\n",
      "12: Encoding Loss 7.4828200340271, Transition Loss -2.180202007293701, Classifier Loss 0.05452389642596245, Total Loss 50.34843826293945\n",
      "12: Encoding Loss 6.52395486831665, Transition Loss -0.5603020191192627, Classifier Loss 0.1961302012205124, Total Loss 58.756526947021484\n",
      "12: Encoding Loss 6.045361518859863, Transition Loss -1.8668782711029053, Classifier Loss 0.05808248743414879, Total Loss 42.079673767089844\n",
      "12: Encoding Loss 5.208782196044922, Transition Loss -1.313957929611206, Classifier Loss 0.060669906437397, Total Loss 37.31916046142578\n",
      "12: Encoding Loss 4.046236991882324, Transition Loss -2.4851863384246826, Classifier Loss 0.057612594217061996, Total Loss 30.037689208984375\n",
      "12: Encoding Loss 4.2926506996154785, Transition Loss -0.9926053285598755, Classifier Loss 0.09417981654405594, Total Loss 35.173492431640625\n",
      "12: Encoding Loss 3.9545280933380127, Transition Loss -2.9394514560699463, Classifier Loss 0.0692456066608429, Total Loss 30.650554656982422\n",
      "12: Encoding Loss 7.352654933929443, Transition Loss -0.8639911413192749, Classifier Loss 0.09758395701646805, Total Loss 53.87398147583008\n",
      "12: Encoding Loss 4.215875148773193, Transition Loss -1.7318305969238281, Classifier Loss 0.09982883185148239, Total Loss 35.277442932128906\n",
      "12: Encoding Loss 2.882645606994629, Transition Loss -1.2227394580841064, Classifier Loss 0.037248190492391586, Total Loss 21.020204544067383\n",
      "12: Encoding Loss 6.91393518447876, Transition Loss -2.1777117252349854, Classifier Loss 0.06683401763439178, Total Loss 48.16614532470703\n",
      "12: Encoding Loss 7.713949680328369, Transition Loss -1.4032268524169922, Classifier Loss 0.07874865084886551, Total Loss 54.15800476074219\n",
      "12: Encoding Loss 5.645379066467285, Transition Loss -0.5547181367874146, Classifier Loss 0.12223032116889954, Total Loss 46.09508514404297\n",
      "12: Encoding Loss 5.820059299468994, Transition Loss -1.7471418380737305, Classifier Loss 0.035404618829488754, Total Loss 38.460121154785156\n",
      "12: Encoding Loss 4.845882415771484, Transition Loss -0.987687349319458, Classifier Loss 0.10661894083023071, Total Loss 39.736793518066406\n",
      "12: Encoding Loss 5.453366279602051, Transition Loss -1.6274373531341553, Classifier Loss 0.08418600261211395, Total Loss 41.13814926147461\n",
      "12: Encoding Loss 5.95025110244751, Transition Loss -1.1248925924301147, Classifier Loss 0.08992911875247955, Total Loss 44.6939697265625\n",
      "12: Encoding Loss 4.11794900894165, Transition Loss -0.8195205330848694, Classifier Loss 0.07311680912971497, Total Loss 32.019046783447266\n",
      "12: Encoding Loss 5.8689165115356445, Transition Loss -1.9675483703613281, Classifier Loss 0.1463189423084259, Total Loss 49.844608306884766\n",
      "12: Encoding Loss 3.734563112258911, Transition Loss -1.0565533638000488, Classifier Loss 0.06807214021682739, Total Loss 29.214168548583984\n",
      "12: Encoding Loss 5.1009416580200195, Transition Loss -0.38052451610565186, Classifier Loss 0.04047660529613495, Total Loss 34.65315628051758\n",
      "12: Encoding Loss 6.866231441497803, Transition Loss -1.4077364206314087, Classifier Loss 0.1955166459083557, Total Loss 60.74848937988281\n",
      "12: Encoding Loss 5.07241153717041, Transition Loss -0.8618050813674927, Classifier Loss 0.050191495567560196, Total Loss 35.453277587890625\n",
      "12: Encoding Loss 5.747061729431152, Transition Loss -3.6703391075134277, Classifier Loss 0.06909238547086716, Total Loss 41.390140533447266\n",
      "12: Encoding Loss 4.448330879211426, Transition Loss -1.69528329372406, Classifier Loss 0.046053189784288406, Total Loss 31.29462432861328\n",
      "12: Encoding Loss 5.351810455322266, Transition Loss -0.26994842290878296, Classifier Loss 0.16346475481987, Total Loss 48.45722961425781\n",
      "12: Encoding Loss 3.7594757080078125, Transition Loss -1.3452212810516357, Classifier Loss 0.08944731950759888, Total Loss 31.501049041748047\n",
      "12: Encoding Loss 7.7106757164001465, Transition Loss -1.925433874130249, Classifier Loss 0.1462193727493286, Total Loss 60.885223388671875\n",
      "12: Encoding Loss 4.833147048950195, Transition Loss -0.7307597994804382, Classifier Loss 0.05048682540655136, Total Loss 34.047271728515625\n",
      "12: Encoding Loss 5.01748514175415, Transition Loss -2.5873777866363525, Classifier Loss 0.09327490627765656, Total Loss 39.43136978149414\n",
      "12: Encoding Loss 4.680968761444092, Transition Loss -2.9158027172088623, Classifier Loss 0.12645617127418518, Total Loss 40.730262756347656\n",
      "12: Encoding Loss 4.727863311767578, Transition Loss -1.5522338151931763, Classifier Loss 0.11003929376602173, Total Loss 39.370487213134766\n",
      "12: Encoding Loss 5.5691118240356445, Transition Loss -1.2939066886901855, Classifier Loss 0.10044942796230316, Total Loss 43.45909881591797\n",
      "12: Encoding Loss 5.248885631561279, Transition Loss -1.9126393795013428, Classifier Loss 0.15935419499874115, Total Loss 47.4279670715332\n",
      "12: Encoding Loss 5.980740070343018, Transition Loss -1.3101458549499512, Classifier Loss 0.03311507776379585, Total Loss 39.19542694091797\n",
      "12: Encoding Loss 4.744894981384277, Transition Loss 0.4291539788246155, Classifier Loss 0.05677913874387741, Total Loss 34.318946838378906\n",
      "12: Encoding Loss 5.628302574157715, Transition Loss -1.8980416059494019, Classifier Loss 0.04897009953856468, Total Loss 38.66606903076172\n",
      "12: Encoding Loss 4.232494831085205, Transition Loss -1.5490367412567139, Classifier Loss 0.06585178524255753, Total Loss 31.979528427124023\n",
      "12: Encoding Loss 6.52982234954834, Transition Loss -0.42292189598083496, Classifier Loss 0.10220473259687424, Total Loss 49.39924240112305\n",
      "12: Encoding Loss 4.530258655548096, Transition Loss -2.3767917156219482, Classifier Loss 0.03887792304158211, Total Loss 31.068395614624023\n",
      "12: Encoding Loss 7.213100433349609, Transition Loss -1.6271417140960693, Classifier Loss 0.2204296737909317, Total Loss 65.3209228515625\n",
      "12: Encoding Loss 5.578595161437988, Transition Loss -1.1581565141677856, Classifier Loss 0.12406792491674423, Total Loss 45.87790298461914\n",
      "12: Encoding Loss 5.218573570251465, Transition Loss -1.8394602537155151, Classifier Loss 0.057190731167793274, Total Loss 37.029781341552734\n",
      "12: Encoding Loss 5.035378456115723, Transition Loss 0.021231621503829956, Classifier Loss 0.06566809862852097, Total Loss 36.78757095336914\n",
      "12: Encoding Loss 5.385210037231445, Transition Loss -1.150250792503357, Classifier Loss 0.05318916216492653, Total Loss 37.62971496582031\n",
      "12: Encoding Loss 5.4853973388671875, Transition Loss -0.2688142955303192, Classifier Loss 0.11632494628429413, Total Loss 44.54477310180664\n",
      "12: Encoding Loss 7.007588863372803, Transition Loss -0.3815711736679077, Classifier Loss 0.041897356510162354, Total Loss 46.2351188659668\n",
      "12: Encoding Loss 4.498108863830566, Transition Loss -1.752670407295227, Classifier Loss 0.16840732097625732, Total Loss 43.82868194580078\n",
      "12: Encoding Loss 5.102696895599365, Transition Loss -0.32796037197113037, Classifier Loss 0.06619787961244583, Total Loss 37.23583984375\n",
      "12: Encoding Loss 4.160489082336426, Transition Loss -0.45835036039352417, Classifier Loss 0.09746857732534409, Total Loss 34.70960998535156\n",
      "12: Encoding Loss 4.84658145904541, Transition Loss -1.7817444801330566, Classifier Loss 0.035046856850385666, Total Loss 32.58346176147461\n",
      "12: Encoding Loss 4.696206092834473, Transition Loss -2.0435709953308105, Classifier Loss 0.16858124732971191, Total Loss 45.0345458984375\n",
      "12: Encoding Loss 4.921450614929199, Transition Loss -2.1019363403320312, Classifier Loss 0.14093086123466492, Total Loss 43.62095260620117\n",
      "12: Encoding Loss 3.659790515899658, Transition Loss -2.852100372314453, Classifier Loss 0.06777021288871765, Total Loss 28.7346248626709\n",
      "12: Encoding Loss 3.997941493988037, Transition Loss -0.5837675333023071, Classifier Loss 0.11342117190361023, Total Loss 35.329532623291016\n",
      "12: Encoding Loss 5.595062255859375, Transition Loss -0.5126161575317383, Classifier Loss 0.12113755196332932, Total Loss 45.683921813964844\n",
      "12: Encoding Loss 3.356599807739258, Transition Loss -2.2282111644744873, Classifier Loss 0.055088065564632416, Total Loss 25.64751434326172\n",
      "12: Encoding Loss 5.260628700256348, Transition Loss 0.19505557417869568, Classifier Loss 0.03532100096344948, Total Loss 35.173892974853516\n",
      "12: Encoding Loss 5.641162872314453, Transition Loss -2.1926116943359375, Classifier Loss 0.0801488384604454, Total Loss 41.860984802246094\n",
      "12: Encoding Loss 4.2005839347839355, Transition Loss -0.8705527186393738, Classifier Loss 0.057505398988723755, Total Loss 30.95369529724121\n",
      "12: Encoding Loss 8.142424583435059, Transition Loss 0.5494896173477173, Classifier Loss 0.240912526845932, Total Loss 73.16559600830078\n",
      "12: Encoding Loss 5.700692653656006, Transition Loss -1.6388893127441406, Classifier Loss 0.053034957498311996, Total Loss 39.50699996948242\n",
      "12: Encoding Loss 7.042672634124756, Transition Loss 0.402729868888855, Classifier Loss 0.06950613856315613, Total Loss 49.36774444580078\n",
      "12: Encoding Loss 5.077566623687744, Transition Loss -1.7094968557357788, Classifier Loss 0.16005843877792358, Total Loss 46.47056198120117\n",
      "12: Encoding Loss 3.7158398628234863, Transition Loss -1.1425131559371948, Classifier Loss 0.09085141122341156, Total Loss 31.379722595214844\n",
      "12: Encoding Loss 6.5969743728637695, Transition Loss -1.6677178144454956, Classifier Loss 0.1255347579717636, Total Loss 52.1346549987793\n",
      "12: Encoding Loss 8.726851463317871, Transition Loss -2.839228868484497, Classifier Loss 0.17608824372291565, Total Loss 69.96879577636719\n",
      "12: Encoding Loss 6.9767560958862305, Transition Loss -1.451839566230774, Classifier Loss 0.27628159523010254, Total Loss 69.48811340332031\n",
      "12: Encoding Loss 4.157559871673584, Transition Loss -1.4129265546798706, Classifier Loss 0.10219129920005798, Total Loss 35.16392517089844\n",
      "12: Encoding Loss 7.5673627853393555, Transition Loss 0.28535890579223633, Classifier Loss 0.08355224132537842, Total Loss 53.8735466003418\n",
      "12: Encoding Loss 9.410295486450195, Transition Loss -1.6119306087493896, Classifier Loss 0.30684199929237366, Total Loss 87.14532470703125\n",
      "12: Encoding Loss 6.6181817054748535, Transition Loss -0.8862853646278381, Classifier Loss 0.11340048164129257, Total Loss 51.04878616333008\n",
      "12: Encoding Loss 7.2967658042907715, Transition Loss -0.26045408844947815, Classifier Loss 0.12799805402755737, Total Loss 56.580299377441406\n",
      "12: Encoding Loss 7.209194183349609, Transition Loss -0.4894346296787262, Classifier Loss 0.09641459584236145, Total Loss 52.89643096923828\n",
      "12: Encoding Loss 5.354361057281494, Transition Loss -2.0599427223205566, Classifier Loss 0.09663589298725128, Total Loss 41.78893280029297\n",
      "12: Encoding Loss 4.92114782333374, Transition Loss -1.8751859664916992, Classifier Loss 0.11710168421268463, Total Loss 41.236305236816406\n",
      "12: Encoding Loss 5.9893364906311035, Transition Loss -3.3853893280029297, Classifier Loss 0.1274896115064621, Total Loss 48.68362808227539\n",
      "12: Encoding Loss 4.6905412673950195, Transition Loss -2.242611885070801, Classifier Loss 0.0764271691441536, Total Loss 35.78506851196289\n",
      "12: Encoding Loss 4.413724899291992, Transition Loss -1.284482479095459, Classifier Loss 0.04864691197872162, Total Loss 31.346529006958008\n",
      "12: Encoding Loss 5.02398157119751, Transition Loss -1.778017282485962, Classifier Loss 0.1636960208415985, Total Loss 46.51278305053711\n",
      "12: Encoding Loss 3.075260639190674, Transition Loss -1.892014980316162, Classifier Loss 0.08468411862850189, Total Loss 26.919218063354492\n",
      "12: Encoding Loss 4.1018218994140625, Transition Loss -1.4675941467285156, Classifier Loss 0.11184098571538925, Total Loss 35.7944450378418\n",
      "12: Encoding Loss 6.457917213439941, Transition Loss -1.6170480251312256, Classifier Loss 0.1965446174144745, Total Loss 58.40131759643555\n",
      "12: Encoding Loss 5.168829917907715, Transition Loss -0.41114774346351624, Classifier Loss 0.15543115139007568, Total Loss 46.55593490600586\n",
      "12: Encoding Loss 3.9183971881866455, Transition Loss -1.9189839363098145, Classifier Loss 0.11068954318761826, Total Loss 34.57857131958008\n",
      "12: Encoding Loss 5.660767555236816, Transition Loss -2.190218210220337, Classifier Loss 0.07083356380462646, Total Loss 41.04708480834961\n",
      "12: Encoding Loss 3.8400321006774902, Transition Loss -0.6235219240188599, Classifier Loss 0.04780459403991699, Total Loss 27.820402145385742\n",
      "12: Encoding Loss 5.439635753631592, Transition Loss -1.338393211364746, Classifier Loss 0.0338251031935215, Total Loss 36.01979446411133\n",
      "12: Encoding Loss 4.363279819488525, Transition Loss -1.378089189529419, Classifier Loss 0.0464947447180748, Total Loss 30.828603744506836\n",
      "12: Encoding Loss 3.877162218093872, Transition Loss -0.5220171809196472, Classifier Loss 0.12021929025650024, Total Loss 35.28469467163086\n",
      "12: Encoding Loss 5.76903772354126, Transition Loss -0.7197971940040588, Classifier Loss 0.13407428562641144, Total Loss 48.02136993408203\n",
      "12: Encoding Loss 6.656463623046875, Transition Loss -0.12130257487297058, Classifier Loss 0.13214977085590363, Total Loss 53.153709411621094\n",
      "12: Encoding Loss 6.469459533691406, Transition Loss -0.7253996133804321, Classifier Loss 0.10598896443843842, Total Loss 49.41536331176758\n",
      "12: Encoding Loss 3.0304744243621826, Transition Loss -2.287353754043579, Classifier Loss 0.08523184806108475, Total Loss 26.705116271972656\n",
      "12: Encoding Loss 7.072918891906738, Transition Loss -1.4239006042480469, Classifier Loss 0.14616897702217102, Total Loss 57.0538444519043\n",
      "12: Encoding Loss 4.268586158752441, Transition Loss -2.505197286605835, Classifier Loss 0.09942542016506195, Total Loss 35.55305480957031\n",
      "12: Encoding Loss 5.776220798492432, Transition Loss -1.0022114515304565, Classifier Loss 0.08580734580755234, Total Loss 43.2376594543457\n",
      "12: Encoding Loss 7.1490583419799805, Transition Loss -1.9877617359161377, Classifier Loss 0.18720926344394684, Total Loss 61.61448669433594\n",
      "12: Encoding Loss 5.529549598693848, Transition Loss -2.0092833042144775, Classifier Loss 0.13762427866458893, Total Loss 46.93892288208008\n",
      "12: Encoding Loss 7.5240797996521, Transition Loss -0.17117486894130707, Classifier Loss 0.14445194602012634, Total Loss 59.58960723876953\n",
      "12: Encoding Loss 4.644434452056885, Transition Loss 0.12582285702228546, Classifier Loss 0.09422186017036438, Total Loss 37.3391227722168\n",
      "12: Encoding Loss 5.015573978424072, Transition Loss -1.9398977756500244, Classifier Loss 0.0838281586766243, Total Loss 38.475486755371094\n",
      "12: Encoding Loss 6.933314323425293, Transition Loss -0.7172396779060364, Classifier Loss 0.12792859971523285, Total Loss 54.392459869384766\n",
      "12: Encoding Loss 6.224066257476807, Transition Loss -1.3786365985870361, Classifier Loss 0.11209549009799957, Total Loss 48.55339431762695\n",
      "12: Encoding Loss 5.859628677368164, Transition Loss -0.7774498462677002, Classifier Loss 0.10834980010986328, Total Loss 45.99243927001953\n",
      "12: Encoding Loss 4.1295905113220215, Transition Loss 0.1908433735370636, Classifier Loss 0.11011358350515366, Total Loss 35.865238189697266\n",
      "12: Encoding Loss 3.216325283050537, Transition Loss -0.791801393032074, Classifier Loss 0.04970528930425644, Total Loss 24.268165588378906\n",
      "12: Encoding Loss 5.641671657562256, Transition Loss -1.7021069526672363, Classifier Loss 0.07899023592472076, Total Loss 41.74837875366211\n",
      "12: Encoding Loss 4.75205135345459, Transition Loss -1.2058722972869873, Classifier Loss 0.09687985479831696, Total Loss 38.19981384277344\n",
      "12: Encoding Loss 4.613691329956055, Transition Loss -0.6741116046905518, Classifier Loss 0.18199855089187622, Total Loss 45.88173294067383\n",
      "12: Encoding Loss 4.0577569007873535, Transition Loss -2.1788291931152344, Classifier Loss 0.04836913198232651, Total Loss 29.182584762573242\n",
      "12: Encoding Loss 4.333739280700684, Transition Loss -2.62137770652771, Classifier Loss 0.1244286298751831, Total Loss 38.444252014160156\n",
      "12: Encoding Loss 1.779407262802124, Transition Loss -0.6161220669746399, Classifier Loss 0.05370619148015976, Total Loss 16.046817779541016\n",
      "12: Encoding Loss 4.973349094390869, Transition Loss -0.7329491376876831, Classifier Loss 0.05393989756703377, Total Loss 35.23379135131836\n",
      "12: Encoding Loss 4.678297519683838, Transition Loss -1.4960424900054932, Classifier Loss 0.12810783088207245, Total Loss 40.87997055053711\n",
      "12: Encoding Loss 3.8314738273620605, Transition Loss -0.6491916179656982, Classifier Loss 0.054792679846286774, Total Loss 28.467853546142578\n",
      "12: Encoding Loss 4.481936454772949, Transition Loss -0.13425955176353455, Classifier Loss 0.03397664427757263, Total Loss 30.28923225402832\n",
      "12: Encoding Loss 7.665040493011475, Transition Loss -2.1479604244232178, Classifier Loss 0.18654316663742065, Total Loss 64.6436996459961\n",
      "12: Encoding Loss 4.869585990905762, Transition Loss -0.5723356008529663, Classifier Loss 0.1757417768239975, Total Loss 46.791465759277344\n",
      "12: Encoding Loss 4.957553386688232, Transition Loss -1.0933865308761597, Classifier Loss 0.18638980388641357, Total Loss 48.38386154174805\n",
      "12: Encoding Loss 4.567083835601807, Transition Loss -1.808514952659607, Classifier Loss 0.07832489162683487, Total Loss 35.23426818847656\n",
      "12: Encoding Loss 6.427264213562012, Transition Loss -1.6604410409927368, Classifier Loss 0.1476159542798996, Total Loss 53.324520111083984\n",
      "12: Encoding Loss 5.428701877593994, Transition Loss -0.9735003709793091, Classifier Loss 0.08953560143709183, Total Loss 41.52538299560547\n",
      "12: Encoding Loss 5.836886405944824, Transition Loss -0.4390546679496765, Classifier Loss 0.06378859281539917, Total Loss 41.40000534057617\n",
      "12: Encoding Loss 4.188068866729736, Transition Loss -1.9208440780639648, Classifier Loss 0.10296029597520828, Total Loss 35.42367935180664\n",
      "12: Encoding Loss 6.42407751083374, Transition Loss -0.09579277038574219, Classifier Loss 0.07486488670110703, Total Loss 46.03091812133789\n",
      "12: Encoding Loss 4.677070617675781, Transition Loss -1.5956840515136719, Classifier Loss 0.08035280555486679, Total Loss 36.097068786621094\n",
      "12: Encoding Loss 5.969062805175781, Transition Loss -1.3657639026641846, Classifier Loss 0.10612954199314117, Total Loss 46.42678451538086\n",
      "12: Encoding Loss 6.275860786437988, Transition Loss -0.6220096349716187, Classifier Loss 0.08404853940010071, Total Loss 46.05977249145508\n",
      "12: Encoding Loss 4.676403045654297, Transition Loss -1.7491745948791504, Classifier Loss 0.045040152966976166, Total Loss 32.561737060546875\n",
      "12: Encoding Loss 3.9845874309539795, Transition Loss -1.2585210800170898, Classifier Loss 0.08090049773454666, Total Loss 31.997072219848633\n",
      "12: Encoding Loss 3.5967910289764404, Transition Loss -1.0932284593582153, Classifier Loss 0.0767378956079483, Total Loss 29.254100799560547\n",
      "12: Encoding Loss 7.453937530517578, Transition Loss -2.3993518352508545, Classifier Loss 0.09144873917102814, Total Loss 53.86753845214844\n",
      "12: Encoding Loss 4.051033973693848, Transition Loss -3.4100699424743652, Classifier Loss 0.05289394408464432, Total Loss 29.594234466552734\n",
      "12: Encoding Loss 2.9503228664398193, Transition Loss -1.1565850973129272, Classifier Loss 0.0949348509311676, Total Loss 27.19495964050293\n",
      "12: Encoding Loss 9.743136405944824, Transition Loss 0.3821653127670288, Classifier Loss 0.1681809425354004, Total Loss 75.42977905273438\n",
      "12: Encoding Loss 6.27058219909668, Transition Loss -0.10947543382644653, Classifier Loss 0.12592925131320953, Total Loss 50.21637725830078\n",
      "12: Encoding Loss 4.535955429077148, Transition Loss -2.6456685066223145, Classifier Loss 0.06596308201551437, Total Loss 33.81098556518555\n",
      "12: Encoding Loss 4.017003536224365, Transition Loss -0.6398731470108032, Classifier Loss 0.07115442305803299, Total Loss 31.217208862304688\n",
      "12: Encoding Loss 7.0198822021484375, Transition Loss -2.5552327632904053, Classifier Loss 0.18691696226596832, Total Loss 60.809967041015625\n",
      "12: Encoding Loss 4.689521789550781, Transition Loss -1.1443712711334229, Classifier Loss 0.06897653639316559, Total Loss 35.03432846069336\n",
      "12: Encoding Loss 4.547130107879639, Transition Loss -1.1544387340545654, Classifier Loss 0.07002495974302292, Total Loss 34.28481674194336\n",
      "12: Encoding Loss 4.1048173904418945, Transition Loss -1.5494695901870728, Classifier Loss 0.13698828220367432, Total Loss 38.32711410522461\n",
      "12: Encoding Loss 6.081463813781738, Transition Loss -0.5102355480194092, Classifier Loss 0.03796917945146561, Total Loss 40.28549575805664\n",
      "12: Encoding Loss 4.582108974456787, Transition Loss -1.3897725343704224, Classifier Loss 0.06725971400737762, Total Loss 34.21807098388672\n",
      "12: Encoding Loss 6.5333051681518555, Transition Loss -1.970592975616455, Classifier Loss 0.09800193458795547, Total Loss 48.999237060546875\n",
      "12: Encoding Loss 5.439698219299316, Transition Loss -0.593970775604248, Classifier Loss 0.06633567065000534, Total Loss 39.271522521972656\n",
      "12: Encoding Loss 5.567072868347168, Transition Loss -0.20638595521450043, Classifier Loss 0.13554775714874268, Total Loss 46.957130432128906\n",
      "12: Encoding Loss 7.340409278869629, Transition Loss -1.0333209037780762, Classifier Loss 0.042440224438905716, Total Loss 48.286067962646484\n",
      "12: Encoding Loss 5.684727668762207, Transition Loss -1.0240848064422607, Classifier Loss 0.18865133821964264, Total Loss 52.97309494018555\n",
      "12: Encoding Loss 5.154396057128906, Transition Loss -0.5409722328186035, Classifier Loss 0.10213040560483932, Total Loss 41.13920211791992\n",
      "12: Encoding Loss 3.249807357788086, Transition Loss -0.9400370121002197, Classifier Loss 0.06549486517906189, Total Loss 26.047954559326172\n",
      "12: Encoding Loss 5.085091590881348, Transition Loss -1.5135935544967651, Classifier Loss 0.0751001313328743, Total Loss 38.019954681396484\n",
      "12: Encoding Loss 5.032992362976074, Transition Loss -0.3645974397659302, Classifier Loss 0.08156051486730576, Total Loss 38.35386276245117\n",
      "12: Encoding Loss 4.511658668518066, Transition Loss 0.38795483112335205, Classifier Loss 0.09172813594341278, Total Loss 36.39794921875\n",
      "12: Encoding Loss 4.237598896026611, Transition Loss -1.7371350526809692, Classifier Loss 0.07308007031679153, Total Loss 32.732906341552734\n",
      "12: Encoding Loss 3.226811170578003, Transition Loss -2.7663865089416504, Classifier Loss 0.06969032436609268, Total Loss 26.328794479370117\n",
      "12: Encoding Loss 5.798626899719238, Transition Loss -1.998798131942749, Classifier Loss 0.10504276305437088, Total Loss 45.29523849487305\n",
      "12: Encoding Loss 5.100464344024658, Transition Loss -2.196615695953369, Classifier Loss 0.13957534730434418, Total Loss 44.559444427490234\n",
      "12: Encoding Loss 5.4229512214660645, Transition Loss 0.06046667695045471, Classifier Loss 0.09348007291555405, Total Loss 41.9099006652832\n",
      "12: Encoding Loss 5.253403186798096, Transition Loss -0.9438297748565674, Classifier Loss 0.12943395972251892, Total Loss 44.463436126708984\n",
      "12: Encoding Loss 6.2010602951049805, Transition Loss -0.5139194130897522, Classifier Loss 0.1506052315235138, Total Loss 52.26668167114258\n",
      "12: Encoding Loss 5.480598449707031, Transition Loss -0.5586680173873901, Classifier Loss 0.1209498792886734, Total Loss 44.97835159301758\n",
      "12: Encoding Loss 4.950479507446289, Transition Loss -1.1420378684997559, Classifier Loss 0.09696894139051437, Total Loss 39.399314880371094\n",
      "12: Encoding Loss 5.342998504638672, Transition Loss -2.9926013946533203, Classifier Loss 0.06945846229791641, Total Loss 39.00263977050781\n",
      "12: Encoding Loss 4.529783725738525, Transition Loss -0.7659972906112671, Classifier Loss 0.049655377864837646, Total Loss 32.14393615722656\n",
      "12: Encoding Loss 5.1079511642456055, Transition Loss -1.3720980882644653, Classifier Loss 0.09691699594259262, Total Loss 40.33885955810547\n",
      "12: Encoding Loss 5.551027297973633, Transition Loss -2.79907488822937, Classifier Loss 0.05032656341791153, Total Loss 38.33769989013672\n",
      "12: Encoding Loss 6.7466535568237305, Transition Loss -0.7982637882232666, Classifier Loss 0.04762699455022812, Total Loss 45.24230194091797\n",
      "12: Encoding Loss 3.0534539222717285, Transition Loss -0.7255269885063171, Classifier Loss 0.06835106015205383, Total Loss 25.155540466308594\n",
      "12: Encoding Loss 5.312583923339844, Transition Loss -1.4806292057037354, Classifier Loss 0.04897022992372513, Total Loss 36.77193832397461\n",
      "12: Encoding Loss 6.017333030700684, Transition Loss -1.1753191947937012, Classifier Loss 0.0712846890091896, Total Loss 43.231998443603516\n",
      "12: Encoding Loss 3.637874126434326, Transition Loss -0.5625271797180176, Classifier Loss 0.02931373007595539, Total Loss 24.758394241333008\n",
      "12: Encoding Loss 5.118100166320801, Transition Loss -2.779385566711426, Classifier Loss 0.1350989043712616, Total Loss 44.21738052368164\n",
      "12: Encoding Loss 6.92202091217041, Transition Loss -0.27506548166275024, Classifier Loss 0.10290168970823288, Total Loss 51.82218551635742\n",
      "12: Encoding Loss 6.814788818359375, Transition Loss -0.9064905047416687, Classifier Loss 0.13639616966247559, Total Loss 54.52798843383789\n",
      "12: Encoding Loss 5.750358581542969, Transition Loss -1.3990509510040283, Classifier Loss 0.05544852465391159, Total Loss 40.046443939208984\n",
      "12: Encoding Loss 5.372607231140137, Transition Loss -1.5324937105178833, Classifier Loss 0.08253905922174454, Total Loss 40.48893737792969\n",
      "12: Encoding Loss 5.311727046966553, Transition Loss -0.7606658935546875, Classifier Loss 0.14735925197601318, Total Loss 46.60598373413086\n",
      "12: Encoding Loss 6.117419242858887, Transition Loss -1.6768699884414673, Classifier Loss 0.06362701952457428, Total Loss 43.06654739379883\n",
      "12: Encoding Loss 3.3195347785949707, Transition Loss -1.2298977375030518, Classifier Loss 0.1296112984418869, Total Loss 32.87784957885742\n",
      "12: Encoding Loss 8.047643661499023, Transition Loss 0.42930305004119873, Classifier Loss 0.12334436178207397, Total Loss 60.792022705078125\n",
      "12: Encoding Loss 4.6932373046875, Transition Loss -1.1300829648971558, Classifier Loss 0.039174843579530716, Total Loss 32.07646179199219\n",
      "12: Encoding Loss 3.0599255561828613, Transition Loss -2.3257784843444824, Classifier Loss 0.05579409748315811, Total Loss 23.938034057617188\n",
      "12: Encoding Loss 6.1043853759765625, Transition Loss -0.19660666584968567, Classifier Loss 0.04667822644114494, Total Loss 41.2940559387207\n",
      "12: Encoding Loss 2.5772159099578857, Transition Loss -0.9924631714820862, Classifier Loss 0.03323237970471382, Total Loss 18.786136627197266\n",
      "12: Encoding Loss 6.664236545562744, Transition Loss -1.7793691158294678, Classifier Loss 0.12066090106964111, Total Loss 52.05079650878906\n",
      "12: Encoding Loss 5.974917888641357, Transition Loss -1.0279746055603027, Classifier Loss 0.09077956527471542, Total Loss 44.92705535888672\n",
      "12: Encoding Loss 5.101867198944092, Transition Loss -2.150405168533325, Classifier Loss 0.06277871131896973, Total Loss 36.88821792602539\n",
      "12: Encoding Loss 3.3952584266662598, Transition Loss -1.080092191696167, Classifier Loss 0.11667883396148682, Total Loss 32.039005279541016\n",
      "12: Encoding Loss 4.726505756378174, Transition Loss -1.4557732343673706, Classifier Loss 0.0921303778886795, Total Loss 37.57149124145508\n",
      "12: Encoding Loss 5.250075340270996, Transition Loss -1.5742521286010742, Classifier Loss 0.07903522998094559, Total Loss 39.40334701538086\n",
      "12: Encoding Loss 3.4665446281433105, Transition Loss -0.7846808433532715, Classifier Loss 0.08064103126525879, Total Loss 28.86305809020996\n",
      "12: Encoding Loss 5.920576572418213, Transition Loss -2.0994904041290283, Classifier Loss 0.17804010212421417, Total Loss 53.326629638671875\n",
      "12: Encoding Loss 3.993776798248291, Transition Loss -1.9910774230957031, Classifier Loss 0.04940321296453476, Total Loss 28.902185440063477\n",
      "12: Encoding Loss 3.4107272624969482, Transition Loss -0.7422221302986145, Classifier Loss 0.059993818402290344, Total Loss 26.463449478149414\n",
      "12: Encoding Loss 5.352656364440918, Transition Loss 0.005510270595550537, Classifier Loss 0.1417769491672516, Total Loss 46.295841217041016\n",
      "12: Encoding Loss 4.605146884918213, Transition Loss -1.6717678308486938, Classifier Loss 0.07238337397575378, Total Loss 34.868553161621094\n",
      "12: Encoding Loss 7.776068687438965, Transition Loss -0.5790845155715942, Classifier Loss 0.09888023883104324, Total Loss 56.54420471191406\n",
      "12: Encoding Loss 4.0742058753967285, Transition Loss -2.5337300300598145, Classifier Loss 0.1856548935174942, Total Loss 43.00971221923828\n",
      "12: Encoding Loss 5.91926908493042, Transition Loss -1.0765857696533203, Classifier Loss 0.08098912239074707, Total Loss 43.614097595214844\n",
      "12: Encoding Loss 4.046084880828857, Transition Loss -1.195871353149414, Classifier Loss 0.039380788803100586, Total Loss 28.214109420776367\n",
      "12: Encoding Loss 8.543978691101074, Transition Loss -0.1901313215494156, Classifier Loss 0.13427585363388062, Total Loss 64.6913833618164\n",
      "12: Encoding Loss 5.291296482086182, Transition Loss -1.197160243988037, Classifier Loss 0.12449941784143448, Total Loss 44.197242736816406\n",
      "12: Encoding Loss 4.460399627685547, Transition Loss -1.0925674438476562, Classifier Loss 0.07522216439247131, Total Loss 34.284175872802734\n",
      "12: Encoding Loss 4.696761131286621, Transition Loss -2.7808966636657715, Classifier Loss 0.0929236114025116, Total Loss 37.47181701660156\n",
      "12: Encoding Loss 3.4429924488067627, Transition Loss 0.7183237075805664, Classifier Loss 0.10592897236347198, Total Loss 31.53818130493164\n",
      "12: Encoding Loss 3.458202838897705, Transition Loss -0.15021319687366486, Classifier Loss 0.04426833614706993, Total Loss 25.17599105834961\n",
      "12: Encoding Loss 4.874987602233887, Transition Loss -2.480412483215332, Classifier Loss 0.1293744146823883, Total Loss 42.186378479003906\n",
      "12: Encoding Loss 5.651425361633301, Transition Loss -1.7120147943496704, Classifier Loss 0.1203530952334404, Total Loss 45.94317626953125\n",
      "12: Encoding Loss 4.100690841674805, Transition Loss -0.7022152543067932, Classifier Loss 0.081509068608284, Total Loss 32.7547721862793\n",
      "12: Encoding Loss 4.372161865234375, Transition Loss -0.5659160017967224, Classifier Loss 0.06119927018880844, Total Loss 32.35267639160156\n",
      "12: Encoding Loss 4.400720596313477, Transition Loss -1.1690826416015625, Classifier Loss 0.03246559575200081, Total Loss 29.65041732788086\n",
      "12: Encoding Loss 6.9244489669799805, Transition Loss -0.18584460020065308, Classifier Loss 0.23698526620864868, Total Loss 65.24514770507812\n",
      "12: Encoding Loss 4.436282157897949, Transition Loss -1.1855196952819824, Classifier Loss 0.07217773795127869, Total Loss 33.83499526977539\n",
      "12: Encoding Loss 6.5618133544921875, Transition Loss -1.5265223979949951, Classifier Loss 0.07164859771728516, Total Loss 46.53512954711914\n",
      "12: Encoding Loss 4.020109176635742, Transition Loss -0.03579798340797424, Classifier Loss 0.05191576108336449, Total Loss 29.312217712402344\n",
      "12: Encoding Loss 5.698321342468262, Transition Loss -1.5824090242385864, Classifier Loss 0.030876779928803444, Total Loss 37.276973724365234\n",
      "12: Encoding Loss 4.978725433349609, Transition Loss -0.875113844871521, Classifier Loss 0.08537991344928741, Total Loss 38.409996032714844\n",
      "12: Encoding Loss 5.076625347137451, Transition Loss -1.355944275856018, Classifier Loss 0.20903775095939636, Total Loss 51.36298370361328\n",
      "12: Encoding Loss 8.326498031616211, Transition Loss -0.3216972351074219, Classifier Loss 0.21226392686367035, Total Loss 71.18525695800781\n",
      "12: Encoding Loss 6.209121227264404, Transition Loss -1.264365553855896, Classifier Loss 0.06766561418771744, Total Loss 44.02078628540039\n",
      "12: Encoding Loss 6.629486083984375, Transition Loss -1.2717654705047607, Classifier Loss 0.15894395112991333, Total Loss 55.67080307006836\n",
      "12: Encoding Loss 4.763520240783691, Transition Loss -1.8521640300750732, Classifier Loss 0.03585921600461006, Total Loss 32.16630172729492\n",
      "12: Encoding Loss 5.621819496154785, Transition Loss -1.1272530555725098, Classifier Loss 0.06666644662618637, Total Loss 40.39711380004883\n",
      "12: Encoding Loss 6.342629432678223, Transition Loss -1.1033225059509277, Classifier Loss 0.08789362013339996, Total Loss 46.844696044921875\n",
      "12: Encoding Loss 4.894723415374756, Transition Loss -1.3201783895492554, Classifier Loss 0.111335389316082, Total Loss 40.5013542175293\n",
      "12: Encoding Loss 3.4243102073669434, Transition Loss 0.4605598449707031, Classifier Loss 0.03662879765033722, Total Loss 24.39296531677246\n",
      "12: Encoding Loss 3.8276584148406982, Transition Loss -0.6272170543670654, Classifier Loss 0.08013725280761719, Total Loss 30.97942543029785\n",
      "12: Encoding Loss 4.464702129364014, Transition Loss 0.3156721591949463, Classifier Loss 0.06214091181755066, Total Loss 33.12857437133789\n",
      "12: Encoding Loss 5.107539176940918, Transition Loss -1.343355417251587, Classifier Loss 0.05228365585207939, Total Loss 35.87306594848633\n",
      "12: Encoding Loss 7.331520080566406, Transition Loss -0.3106822967529297, Classifier Loss 0.18616856634616852, Total Loss 62.60585403442383\n",
      "12: Encoding Loss 5.080267906188965, Transition Loss -1.8052185773849487, Classifier Loss 0.07840840518474579, Total Loss 38.32172775268555\n",
      "12: Encoding Loss 8.246994018554688, Transition Loss -2.7102150917053223, Classifier Loss 0.12007976323366165, Total Loss 61.488861083984375\n",
      "12: Encoding Loss 5.919765472412109, Transition Loss -1.6883900165557861, Classifier Loss 0.0712222307920456, Total Loss 42.640140533447266\n",
      "12: Encoding Loss 3.669281005859375, Transition Loss -1.1463062763214111, Classifier Loss 0.08051680028438568, Total Loss 30.06690788269043\n",
      "12: Encoding Loss 3.99485445022583, Transition Loss -0.3801424503326416, Classifier Loss 0.018236389383673668, Total Loss 25.792613983154297\n",
      "12: Encoding Loss 6.531030178070068, Transition Loss -3.0328450202941895, Classifier Loss 0.2364194244146347, Total Loss 62.82691192626953\n",
      "12: Encoding Loss 5.118032932281494, Transition Loss -1.4333765506744385, Classifier Loss 0.12209758162498474, Total Loss 42.91738510131836\n",
      "12: Encoding Loss 3.903582811355591, Transition Loss 0.825283944606781, Classifier Loss 0.04595188796520233, Total Loss 28.346799850463867\n",
      "12: Encoding Loss 5.207233428955078, Transition Loss -1.0041842460632324, Classifier Loss 0.22467873990535736, Total Loss 53.71087646484375\n",
      "12: Encoding Loss 4.005670547485352, Transition Loss -1.576533317565918, Classifier Loss 0.1067824512720108, Total Loss 34.71164321899414\n",
      "12: Encoding Loss 6.112032890319824, Transition Loss -0.7632914781570435, Classifier Loss 0.17886532843112946, Total Loss 54.55842590332031\n",
      "12: Encoding Loss 3.097490072250366, Transition Loss -0.37711140513420105, Classifier Loss 0.046517834067344666, Total Loss 23.236574172973633\n",
      "12: Encoding Loss 3.2393527030944824, Transition Loss -0.5261247158050537, Classifier Loss 0.15004895627498627, Total Loss 34.44080352783203\n",
      "12: Encoding Loss 5.557377338409424, Transition Loss -2.245746612548828, Classifier Loss 0.11256783455610275, Total Loss 44.60015106201172\n",
      "12: Encoding Loss 4.604241847991943, Transition Loss -1.681237816810608, Classifier Loss 0.16548499464988708, Total Loss 44.17327880859375\n",
      "12: Encoding Loss 5.629827976226807, Transition Loss -2.2527217864990234, Classifier Loss 0.11385181546211243, Total Loss 45.16324996948242\n",
      "12: Encoding Loss 5.399958610534668, Transition Loss -1.6472499370574951, Classifier Loss 0.041317470371723175, Total Loss 36.53084182739258\n",
      "12: Encoding Loss 5.512759208679199, Transition Loss -2.729055881500244, Classifier Loss 0.10662685334682465, Total Loss 43.73815155029297\n",
      "12: Encoding Loss 7.498703956604004, Transition Loss -1.124021291732788, Classifier Loss 0.14033323526382446, Total Loss 59.02510070800781\n",
      "12: Encoding Loss 6.019107341766357, Transition Loss -1.728579044342041, Classifier Loss 0.1395431011915207, Total Loss 50.068267822265625\n",
      "12: Encoding Loss 5.870683670043945, Transition Loss -0.8402734398841858, Classifier Loss 0.08209066838026047, Total Loss 43.432830810546875\n",
      "12: Encoding Loss 4.063670635223389, Transition Loss -2.2645888328552246, Classifier Loss 0.07229642570018768, Total Loss 31.610761642456055\n",
      "12: Encoding Loss 6.516386985778809, Transition Loss -1.6339268684387207, Classifier Loss 0.08078068494796753, Total Loss 47.17573928833008\n",
      "12: Encoding Loss 4.446139335632324, Transition Loss -2.2511935234069824, Classifier Loss 0.06638910621404648, Total Loss 33.314849853515625\n",
      "12: Encoding Loss 5.766202449798584, Transition Loss -1.885404348373413, Classifier Loss 0.042478762567043304, Total Loss 38.844337463378906\n",
      "12: Encoding Loss 4.654179573059082, Transition Loss -1.0143811702728271, Classifier Loss 0.060481298714876175, Total Loss 33.972801208496094\n",
      "12: Encoding Loss 5.321561336517334, Transition Loss -1.6585136651992798, Classifier Loss 0.11498163640499115, Total Loss 43.4268684387207\n",
      "12: Encoding Loss 3.7371482849121094, Transition Loss 0.18222206830978394, Classifier Loss 0.05109792947769165, Total Loss 27.605571746826172\n",
      "12: Encoding Loss 7.843087196350098, Transition Loss -1.3501564264297485, Classifier Loss 0.1706794798374176, Total Loss 64.12593078613281\n",
      "12: Encoding Loss 8.118330001831055, Transition Loss -2.1754069328308105, Classifier Loss 0.1474674642086029, Total Loss 63.45586013793945\n",
      "12: Encoding Loss 7.431328773498535, Transition Loss -1.0285112857818604, Classifier Loss 0.1786232888698578, Total Loss 62.44989013671875\n",
      "12: Encoding Loss 5.9940876960754395, Transition Loss -0.23680922389030457, Classifier Loss 0.10205131024122238, Total Loss 46.16956329345703\n",
      "12: Encoding Loss 7.522950649261475, Transition Loss -0.2056446075439453, Classifier Loss 0.23048003017902374, Total Loss 68.18562316894531\n",
      "12: Encoding Loss 4.473123073577881, Transition Loss 0.26234549283981323, Classifier Loss 0.04667148366570473, Total Loss 31.61082649230957\n",
      "12: Encoding Loss 3.8056530952453613, Transition Loss -0.8859354257583618, Classifier Loss 0.09032616019248962, Total Loss 31.866180419921875\n",
      "12: Encoding Loss 2.8052496910095215, Transition Loss 0.466536283493042, Classifier Loss 0.0773080438375473, Total Loss 24.748918533325195\n",
      "12: Encoding Loss 4.908501625061035, Transition Loss -0.6612054109573364, Classifier Loss 0.12044131755828857, Total Loss 41.494876861572266\n",
      "12: Encoding Loss 6.070123672485352, Transition Loss -1.7111177444458008, Classifier Loss 0.09362579882144928, Total Loss 45.78263854980469\n",
      "12: Encoding Loss 7.0021820068359375, Transition Loss -1.744214415550232, Classifier Loss 0.15961040556430817, Total Loss 57.97343444824219\n",
      "12: Encoding Loss 4.312743663787842, Transition Loss -1.7812976837158203, Classifier Loss 0.06269894540309906, Total Loss 32.14564514160156\n",
      "12: Encoding Loss 6.263813495635986, Transition Loss -2.35866117477417, Classifier Loss 0.1772538274526596, Total Loss 55.30732345581055\n",
      "12: Encoding Loss 4.787201881408691, Transition Loss -1.1811144351959229, Classifier Loss 0.0605970099568367, Total Loss 34.782440185546875\n",
      "12: Encoding Loss 4.530676364898682, Transition Loss -1.5055887699127197, Classifier Loss 0.10447541624307632, Total Loss 37.63099670410156\n",
      "12: Encoding Loss 4.437258243560791, Transition Loss -3.217852830886841, Classifier Loss 0.0814053863286972, Total Loss 34.76280212402344\n",
      "12: Encoding Loss 2.6273176670074463, Transition Loss -1.84597647190094, Classifier Loss 0.05582632124423981, Total Loss 21.345800399780273\n",
      "12: Encoding Loss 5.517610549926758, Transition Loss -1.740675687789917, Classifier Loss 0.08542259782552719, Total Loss 41.64722442626953\n",
      "12: Encoding Loss 5.348164081573486, Transition Loss -1.9525121450424194, Classifier Loss 0.13839322328567505, Total Loss 45.92752456665039\n",
      "12: Encoding Loss 3.086794853210449, Transition Loss -1.5579569339752197, Classifier Loss 0.06387604027986526, Total Loss 24.90774917602539\n",
      "12: Encoding Loss 6.500672817230225, Transition Loss -2.302656888961792, Classifier Loss 0.07172903418540955, Total Loss 46.176025390625\n",
      "12: Encoding Loss 4.408848762512207, Transition Loss -0.9305217266082764, Classifier Loss 0.07159740477800369, Total Loss 33.612457275390625\n",
      "12: Encoding Loss 6.201375961303711, Transition Loss -1.1506857872009277, Classifier Loss 0.09839984774589539, Total Loss 47.04777908325195\n",
      "12: Encoding Loss 6.855844020843506, Transition Loss -1.5160610675811768, Classifier Loss 0.13470840454101562, Total Loss 54.60530090332031\n",
      "12: Encoding Loss 6.274476528167725, Transition Loss -0.8427971601486206, Classifier Loss 0.09092634916305542, Total Loss 46.73916244506836\n",
      "12: Encoding Loss 4.7825727462768555, Transition Loss -1.525447964668274, Classifier Loss 0.10612332820892334, Total Loss 39.307159423828125\n",
      "12: Encoding Loss 5.9515061378479, Transition Loss -1.8518266677856445, Classifier Loss 0.11200254410505295, Total Loss 46.90855407714844\n",
      "12: Encoding Loss 5.121132850646973, Transition Loss -1.626013159751892, Classifier Loss 0.060756146907806396, Total Loss 36.80176544189453\n",
      "12: Encoding Loss 2.794187307357788, Transition Loss -0.14498370885849, Classifier Loss 0.039224617183208466, Total Loss 20.687530517578125\n",
      "12: Encoding Loss 7.064453125, Transition Loss -0.19561325013637543, Classifier Loss 0.08455591648817062, Total Loss 50.842227935791016\n",
      "12: Encoding Loss 6.580954074859619, Transition Loss -1.2875151634216309, Classifier Loss 0.0610741451382637, Total Loss 45.59262466430664\n",
      "12: Encoding Loss 6.869769096374512, Transition Loss -0.999780535697937, Classifier Loss 0.033433280885219574, Total Loss 44.56154251098633\n",
      "12: Encoding Loss 3.960745334625244, Transition Loss -0.14923124015331268, Classifier Loss 0.05288941413164139, Total Loss 29.053354263305664\n",
      "12: Encoding Loss 3.985511302947998, Transition Loss -2.1033689975738525, Classifier Loss 0.03485414758324623, Total Loss 27.397642135620117\n",
      "12: Encoding Loss 3.87229585647583, Transition Loss -1.7346131801605225, Classifier Loss 0.13767042756080627, Total Loss 37.000125885009766\n",
      "12: Encoding Loss 5.393906593322754, Transition Loss -0.4447614550590515, Classifier Loss 0.03697192668914795, Total Loss 36.060455322265625\n",
      "12: Encoding Loss 4.479835510253906, Transition Loss -1.113444209098816, Classifier Loss 0.0343235582113266, Total Loss 30.310924530029297\n",
      "12: Encoding Loss 4.133114814758301, Transition Loss -0.5378777980804443, Classifier Loss 0.04126099497079849, Total Loss 28.92457389831543\n",
      "12: Encoding Loss 3.505242109298706, Transition Loss -0.8333244323730469, Classifier Loss 0.06599131971597672, Total Loss 27.630252838134766\n",
      "12: Encoding Loss 6.18459939956665, Transition Loss -1.417978286743164, Classifier Loss 0.07012699544429779, Total Loss 44.119728088378906\n",
      "12: Encoding Loss 4.40622615814209, Transition Loss -1.8955235481262207, Classifier Loss 0.0686640813946724, Total Loss 33.303009033203125\n",
      "12: Encoding Loss 4.059405326843262, Transition Loss -2.477268695831299, Classifier Loss 0.04450744017958641, Total Loss 28.80618667602539\n",
      "12: Encoding Loss 3.929091215133667, Transition Loss -1.5064082145690918, Classifier Loss 0.09466840326786041, Total Loss 33.04078674316406\n",
      "12: Encoding Loss 5.78277063369751, Transition Loss -0.7603950500488281, Classifier Loss 0.09180259704589844, Total Loss 43.87657928466797\n",
      "12: Encoding Loss 3.3997962474823, Transition Loss -0.7639051079750061, Classifier Loss 0.04761258885264397, Total Loss 25.159732818603516\n",
      "12: Encoding Loss 4.421496391296387, Transition Loss -1.1378034353256226, Classifier Loss 0.0962119922041893, Total Loss 36.14972686767578\n",
      "12: Encoding Loss 4.871214866638184, Transition Loss -2.854037284851074, Classifier Loss 0.07437749952077866, Total Loss 36.66389846801758\n",
      "12: Encoding Loss 4.067471504211426, Transition Loss -1.0938447713851929, Classifier Loss 0.03743584826588631, Total Loss 28.147977828979492\n",
      "12: Encoding Loss 3.3087711334228516, Transition Loss -0.7126874923706055, Classifier Loss 0.05322510749101639, Total Loss 25.17485237121582\n",
      "12: Encoding Loss 4.601147174835205, Transition Loss -1.7091822624206543, Classifier Loss 0.13890880346298218, Total Loss 41.4970817565918\n",
      "12: Encoding Loss 3.429976224899292, Transition Loss -2.2485480308532715, Classifier Loss 0.07355212420225143, Total Loss 27.93416976928711\n",
      "12: Encoding Loss 3.7728753089904785, Transition Loss -0.8759627342224121, Classifier Loss 0.08652093261480331, Total Loss 31.28899383544922\n",
      "12: Encoding Loss 4.647304058074951, Transition Loss -2.4428203105926514, Classifier Loss 0.058942537754774094, Total Loss 33.777103424072266\n",
      "12: Encoding Loss 5.782375335693359, Transition Loss -1.0464282035827637, Classifier Loss 0.14883612096309662, Total Loss 49.57744598388672\n",
      "12: Encoding Loss 5.233129024505615, Transition Loss -1.6565996408462524, Classifier Loss 0.09048131108283997, Total Loss 40.44624328613281\n",
      "12: Encoding Loss 3.2547709941864014, Transition Loss -2.1201066970825195, Classifier Loss 0.08142130076885223, Total Loss 27.66990852355957\n",
      "12: Encoding Loss 6.507383823394775, Transition Loss -1.196204662322998, Classifier Loss 0.17777709662914276, Total Loss 56.821537017822266\n",
      "12: Encoding Loss 6.982325077056885, Transition Loss -1.934327483177185, Classifier Loss 0.2288886159658432, Total Loss 64.78204345703125\n",
      "12: Encoding Loss 7.692131042480469, Transition Loss -1.9118714332580566, Classifier Loss 0.1200176551938057, Total Loss 58.15378952026367\n",
      "12: Encoding Loss 7.157015800476074, Transition Loss -2.22137451171875, Classifier Loss 0.12498334050178528, Total Loss 55.43954086303711\n",
      "12: Encoding Loss 4.337630271911621, Transition Loss -2.895543098449707, Classifier Loss 0.07499443739652634, Total Loss 33.52406692504883\n",
      "12: Encoding Loss 7.149966239929199, Transition Loss -2.4992189407348633, Classifier Loss 0.07706709951162338, Total Loss 50.60551071166992\n",
      "12: Encoding Loss 8.016304016113281, Transition Loss -0.18174448609352112, Classifier Loss 0.13623619079589844, Total Loss 61.72137451171875\n",
      "12: Encoding Loss 6.780743598937988, Transition Loss -2.5284245014190674, Classifier Loss 0.22390617430210114, Total Loss 63.07406997680664\n",
      "12: Encoding Loss 7.553720951080322, Transition Loss -1.954736590385437, Classifier Loss 0.170662060379982, Total Loss 62.387752532958984\n",
      "12: Encoding Loss 6.6584296226501465, Transition Loss 0.4356563985347748, Classifier Loss 0.05203365162014961, Total Loss 45.328208923339844\n",
      "12: Encoding Loss 5.616462230682373, Transition Loss -1.6088989973068237, Classifier Loss 0.027734069153666496, Total Loss 36.47153854370117\n",
      "12: Encoding Loss 3.0545077323913574, Transition Loss -1.259518027305603, Classifier Loss 0.07699004560709, Total Loss 26.025548934936523\n",
      "12: Encoding Loss 4.074001312255859, Transition Loss -0.6884157061576843, Classifier Loss 0.07736141979694366, Total Loss 32.17987823486328\n",
      "12: Encoding Loss 3.86242938041687, Transition Loss 0.6481744647026062, Classifier Loss 0.055596038699150085, Total Loss 28.993450164794922\n",
      "12: Encoding Loss 2.2395710945129395, Transition Loss -1.206477165222168, Classifier Loss 0.031978197395801544, Total Loss 16.634763717651367\n",
      "12: Encoding Loss 4.299332618713379, Transition Loss -0.9447284936904907, Classifier Loss 0.15312178432941437, Total Loss 41.10779571533203\n",
      "12: Encoding Loss 3.672053575515747, Transition Loss -0.893473744392395, Classifier Loss 0.15286120772361755, Total Loss 37.318084716796875\n",
      "12: Encoding Loss 6.930474281311035, Transition Loss -0.11614641547203064, Classifier Loss 0.050764624029397964, Total Loss 46.659263610839844\n",
      "12: Encoding Loss 5.249091625213623, Transition Loss -2.29811429977417, Classifier Loss 0.06622257828712463, Total Loss 38.11589050292969\n",
      "12: Encoding Loss 8.293910026550293, Transition Loss -1.2982103824615479, Classifier Loss 0.2814987599849701, Total Loss 77.9128189086914\n",
      "12: Encoding Loss 5.143974781036377, Transition Loss -2.436770439147949, Classifier Loss 0.13115127384662628, Total Loss 43.97800064086914\n",
      "12: Encoding Loss 4.731393337249756, Transition Loss -1.4498400688171387, Classifier Loss 0.043971240520477295, Total Loss 32.78490447998047\n",
      "12: Encoding Loss 6.214939594268799, Transition Loss -1.8039178848266602, Classifier Loss 0.15085873007774353, Total Loss 52.37479019165039\n",
      "12: Encoding Loss 6.921722412109375, Transition Loss -1.7466928958892822, Classifier Loss 0.10086528956890106, Total Loss 51.61616516113281\n",
      "12: Encoding Loss 5.092312812805176, Transition Loss 0.19397369027137756, Classifier Loss 0.1349448710680008, Total Loss 44.125953674316406\n",
      "12: Encoding Loss 4.058784484863281, Transition Loss 0.7271328568458557, Classifier Loss 0.0812513530254364, Total Loss 32.76869583129883\n",
      "12: Encoding Loss 5.0520830154418945, Transition Loss -2.081637382507324, Classifier Loss 0.07850848138332367, Total Loss 38.162513732910156\n",
      "12: Encoding Loss 7.261765480041504, Transition Loss -1.4926707744598389, Classifier Loss 0.10273997485637665, Total Loss 53.843994140625\n",
      "12: Encoding Loss 5.212723731994629, Transition Loss -0.22067713737487793, Classifier Loss 0.1388961672782898, Total Loss 45.165870666503906\n",
      "12: Encoding Loss 3.342545986175537, Transition Loss -0.4418904781341553, Classifier Loss 0.0394207201898098, Total Loss 23.99717140197754\n",
      "12: Encoding Loss 4.536462306976318, Transition Loss -1.935178518295288, Classifier Loss 0.1323620229959488, Total Loss 40.45420455932617\n",
      "12: Encoding Loss 3.827845335006714, Transition Loss 0.8042440414428711, Classifier Loss 0.08220838755369186, Total Loss 31.50960922241211\n",
      "12: Encoding Loss 6.4528374671936035, Transition Loss -1.8625833988189697, Classifier Loss 0.07916668057441711, Total Loss 46.63294982910156\n",
      "12: Encoding Loss 3.7298500537872314, Transition Loss -2.2443490028381348, Classifier Loss 0.06684929132461548, Total Loss 29.06313133239746\n",
      "12: Encoding Loss 6.441767692565918, Transition Loss -1.2573041915893555, Classifier Loss 0.12466749548912048, Total Loss 51.116851806640625\n",
      "12: Encoding Loss 4.480669975280762, Transition Loss -2.1950135231018066, Classifier Loss 0.055438872426748276, Total Loss 32.427032470703125\n",
      "12: Encoding Loss 5.323167324066162, Transition Loss -1.4243810176849365, Classifier Loss 0.10804248601198196, Total Loss 42.7426872253418\n",
      "12: Encoding Loss 5.407796382904053, Transition Loss -2.1104657649993896, Classifier Loss 0.10351857542991638, Total Loss 42.797794342041016\n",
      "12: Encoding Loss 4.018941402435303, Transition Loss 0.8978991508483887, Classifier Loss 0.08485924452543259, Total Loss 32.958736419677734\n",
      "12: Encoding Loss 2.033792495727539, Transition Loss -0.8414670825004578, Classifier Loss 0.061364296823740005, Total Loss 18.338850021362305\n",
      "12: Encoding Loss 9.223464012145996, Transition Loss -2.04840087890625, Classifier Loss 0.10470900684595108, Total Loss 65.81086730957031\n",
      "12: Encoding Loss 5.265087127685547, Transition Loss -1.4157904386520386, Classifier Loss 0.0969754010438919, Total Loss 41.287498474121094\n",
      "12: Encoding Loss 4.681301593780518, Transition Loss -0.8663630485534668, Classifier Loss 0.08939001709222794, Total Loss 37.026466369628906\n",
      "12: Encoding Loss 4.506593704223633, Transition Loss -1.1519513130187988, Classifier Loss 0.05422425642609596, Total Loss 32.46152877807617\n",
      "12: Encoding Loss 3.5110726356506348, Transition Loss -1.0543930530548096, Classifier Loss 0.12180957198143005, Total Loss 33.246971130371094\n",
      "12: Encoding Loss 4.83364725112915, Transition Loss -1.690409541130066, Classifier Loss 0.16037426888942719, Total Loss 45.03863525390625\n",
      "12: Encoding Loss 3.3506100177764893, Transition Loss 0.26527154445648193, Classifier Loss 0.04822101816534996, Total Loss 25.031869888305664\n",
      "12: Encoding Loss 6.196981906890869, Transition Loss -2.3343825340270996, Classifier Loss 0.07310420274734497, Total Loss 44.49137878417969\n",
      "12: Encoding Loss 5.461052417755127, Transition Loss -1.089392900466919, Classifier Loss 0.030909549444913864, Total Loss 35.856834411621094\n",
      "12: Encoding Loss 5.388125419616699, Transition Loss -1.9531484842300415, Classifier Loss 0.06666828691959381, Total Loss 38.99480056762695\n",
      "12: Encoding Loss 4.1051435470581055, Transition Loss -0.8843699097633362, Classifier Loss 0.029859833419322968, Total Loss 27.616493225097656\n",
      "12: Encoding Loss 4.21466588973999, Transition Loss 0.15177683532238007, Classifier Loss 0.13302956521511078, Total Loss 38.65166473388672\n",
      "12: Encoding Loss 5.076478004455566, Transition Loss -1.0634945631027222, Classifier Loss 0.048244718462228775, Total Loss 35.28291320800781\n",
      "12: Encoding Loss 3.5571084022521973, Transition Loss -2.650613784790039, Classifier Loss 0.09203816205263138, Total Loss 30.545406341552734\n",
      "12: Encoding Loss 5.070446014404297, Transition Loss -1.6297223567962646, Classifier Loss 0.1897684931755066, Total Loss 49.39887619018555\n",
      "12: Encoding Loss 5.008058071136475, Transition Loss -1.2843782901763916, Classifier Loss 0.09361923485994339, Total Loss 39.40975570678711\n",
      "12: Encoding Loss 7.653411865234375, Transition Loss -1.26277756690979, Classifier Loss 0.09144368022680283, Total Loss 55.064334869384766\n",
      "12: Encoding Loss 4.577552795410156, Transition Loss -0.4928395748138428, Classifier Loss 0.08200006186962128, Total Loss 35.66512680053711\n",
      "12: Encoding Loss 4.215093612670898, Transition Loss 0.12853386998176575, Classifier Loss 0.09621267765760422, Total Loss 34.9632453918457\n",
      "12: Encoding Loss 6.883957862854004, Transition Loss -1.35831618309021, Classifier Loss 0.0878283828496933, Total Loss 50.08604431152344\n",
      "12: Encoding Loss 5.230099201202393, Transition Loss -2.2813754081726074, Classifier Loss 0.13106867671012878, Total Loss 44.48655319213867\n",
      "12: Encoding Loss 7.51077938079834, Transition Loss -2.0563395023345947, Classifier Loss 0.11725258827209473, Total Loss 56.78911209106445\n",
      "12: Encoding Loss 5.362782955169678, Transition Loss -2.2053802013397217, Classifier Loss 0.056913141161203384, Total Loss 37.86713409423828\n",
      "12: Encoding Loss 3.9499669075012207, Transition Loss -2.2954580783843994, Classifier Loss 0.12683960795402527, Total Loss 36.382843017578125\n",
      "12: Encoding Loss 6.662927150726318, Transition Loss -1.0508277416229248, Classifier Loss 0.08651779592037201, Total Loss 48.62892532348633\n",
      "12: Encoding Loss 8.772538185119629, Transition Loss -1.3913629055023193, Classifier Loss 0.13742035627365112, Total Loss 66.376708984375\n",
      "12: Encoding Loss 5.394190788269043, Transition Loss -1.482906460762024, Classifier Loss 0.04485806077718735, Total Loss 36.85036087036133\n",
      "12: Encoding Loss 4.216338157653809, Transition Loss -0.17513157427310944, Classifier Loss 0.057442713528871536, Total Loss 31.0422306060791\n",
      "12: Encoding Loss 3.576333522796631, Transition Loss -0.9865387082099915, Classifier Loss 0.08279863744974136, Total Loss 29.737470626831055\n",
      "12: Encoding Loss 5.453831672668457, Transition Loss -1.4376870393753052, Classifier Loss 0.06498618423938751, Total Loss 39.22103500366211\n",
      "12: Encoding Loss 4.031199932098389, Transition Loss -1.020557165145874, Classifier Loss 0.032964035868644714, Total Loss 27.483196258544922\n",
      "12: Encoding Loss 6.085375785827637, Transition Loss -2.0229573249816895, Classifier Loss 0.044742971658706665, Total Loss 40.98574447631836\n",
      "12: Encoding Loss 3.160775661468506, Transition Loss -1.95628023147583, Classifier Loss 0.08503678441047668, Total Loss 27.467552185058594\n",
      "12: Encoding Loss 8.075520515441895, Transition Loss 0.24370771646499634, Classifier Loss 0.25857675075531006, Total Loss 74.40827941894531\n",
      "12: Encoding Loss 4.89473295211792, Transition Loss -1.1079546213150024, Classifier Loss 0.12647050619125366, Total Loss 42.01500701904297\n",
      "12: Encoding Loss 6.2743377685546875, Transition Loss -1.2133055925369263, Classifier Loss 0.09111906588077545, Total Loss 46.757450103759766\n",
      "12: Encoding Loss 5.785207748413086, Transition Loss 0.23562942445278168, Classifier Loss 0.09139382839202881, Total Loss 43.94488525390625\n",
      "12: Encoding Loss 6.795782566070557, Transition Loss -0.5079571008682251, Classifier Loss 0.18709802627563477, Total Loss 59.48429489135742\n",
      "12: Encoding Loss 5.958766460418701, Transition Loss -1.383376121520996, Classifier Loss 0.15811948478221893, Total Loss 51.56399917602539\n",
      "12: Encoding Loss 4.434347152709961, Transition Loss -0.5027168989181519, Classifier Loss 0.08851693570613861, Total Loss 35.457576751708984\n",
      "12: Encoding Loss 4.782660007476807, Transition Loss -2.08079195022583, Classifier Loss 0.1154046580195427, Total Loss 40.235595703125\n",
      "12: Encoding Loss 4.927206039428711, Transition Loss -0.7252687811851501, Classifier Loss 0.10544869303703308, Total Loss 40.107818603515625\n",
      "12: Encoding Loss 5.663426876068115, Transition Loss -0.314056396484375, Classifier Loss 0.09850742667913437, Total Loss 43.831180572509766\n",
      "12: Encoding Loss 3.1872434616088867, Transition Loss -0.8155113458633423, Classifier Loss 0.051643095910549164, Total Loss 24.287443161010742\n",
      "12: Encoding Loss 3.8815600872039795, Transition Loss -0.8209627866744995, Classifier Loss 0.09997761249542236, Total Loss 33.28679656982422\n",
      "12: Encoding Loss 3.7362399101257324, Transition Loss -1.9106594324111938, Classifier Loss 0.07722688466310501, Total Loss 30.13936424255371\n",
      "12: Encoding Loss 4.817095756530762, Transition Loss -0.6289260387420654, Classifier Loss 0.03823694586753845, Total Loss 32.72602081298828\n",
      "12: Encoding Loss 5.044360160827637, Transition Loss -1.126664161682129, Classifier Loss 0.12095072120428085, Total Loss 42.36078643798828\n",
      "12: Encoding Loss 3.8853561878204346, Transition Loss -2.2745039463043213, Classifier Loss 0.08779560029506683, Total Loss 32.090789794921875\n",
      "12: Encoding Loss 4.746319770812988, Transition Loss -1.333250641822815, Classifier Loss 0.1176256388425827, Total Loss 40.23994827270508\n",
      "12: Encoding Loss 3.935105800628662, Transition Loss -0.13884057104587555, Classifier Loss 0.03555068001151085, Total Loss 27.165647506713867\n",
      "12: Encoding Loss 3.5304782390594482, Transition Loss -0.26152148842811584, Classifier Loss 0.0495261549949646, Total Loss 26.1353816986084\n",
      "12: Encoding Loss 4.7098798751831055, Transition Loss -0.56330806016922, Classifier Loss 0.18556691706180573, Total Loss 46.81574630737305\n",
      "12: Encoding Loss 6.010788917541504, Transition Loss -1.6468135118484497, Classifier Loss 0.18967300653457642, Total Loss 55.031375885009766\n",
      "12: Encoding Loss 3.7502009868621826, Transition Loss -0.4682115316390991, Classifier Loss 0.12076941877603531, Total Loss 34.57796096801758\n",
      "12: Encoding Loss 6.298233985900879, Transition Loss -1.563928246498108, Classifier Loss 0.14513447880744934, Total Loss 52.30222702026367\n",
      "12: Encoding Loss 3.7077784538269043, Transition Loss -1.9936538934707642, Classifier Loss 0.07196890562772751, Total Loss 29.442764282226562\n",
      "12: Encoding Loss 5.5652666091918945, Transition Loss -1.6214854717254639, Classifier Loss 0.12099940329790115, Total Loss 45.49089431762695\n",
      "12: Encoding Loss 5.110054969787598, Transition Loss -0.9410914182662964, Classifier Loss 0.03569766879081726, Total Loss 34.22971725463867\n",
      "12: Encoding Loss 3.037768840789795, Transition Loss -1.228719711303711, Classifier Loss 0.09359484910964966, Total Loss 27.58560562133789\n",
      "12: Encoding Loss 3.7125484943389893, Transition Loss -2.140625476837158, Classifier Loss 0.052626244723796844, Total Loss 27.537059783935547\n",
      "12: Encoding Loss 3.9844841957092285, Transition Loss -0.19695153832435608, Classifier Loss 0.13427558541297913, Total Loss 37.33438491821289\n",
      "12: Encoding Loss 6.597334384918213, Transition Loss -0.7041220664978027, Classifier Loss 0.05523227900266647, Total Loss 45.10695266723633\n",
      "12: Encoding Loss 5.643646717071533, Transition Loss -1.4683647155761719, Classifier Loss 0.20625564455986023, Total Loss 54.48685836791992\n",
      "12: Encoding Loss 6.32827091217041, Transition Loss -0.22371415793895721, Classifier Loss 0.13921630382537842, Total Loss 51.891170501708984\n",
      "12: Encoding Loss 5.5771260261535645, Transition Loss -2.2458739280700684, Classifier Loss 0.13262659311294556, Total Loss 46.72452163696289\n",
      "12: Encoding Loss 4.874022960662842, Transition Loss -0.8112726211547852, Classifier Loss 0.05382119491696358, Total Loss 34.62593460083008\n",
      "12: Encoding Loss 2.8842642307281494, Transition Loss -2.3358757495880127, Classifier Loss 0.05586208775639534, Total Loss 22.890859603881836\n",
      "12: Encoding Loss 2.858975410461426, Transition Loss -1.2917051315307617, Classifier Loss 0.0528789684176445, Total Loss 22.441232681274414\n",
      "12: Encoding Loss 3.9839024543762207, Transition Loss -2.558347702026367, Classifier Loss 0.053172703832387924, Total Loss 29.219661712646484\n",
      "12: Encoding Loss 5.90887975692749, Transition Loss 0.13353541493415833, Classifier Loss 0.039526112377643585, Total Loss 39.45930480957031\n",
      "12: Encoding Loss 5.274832725524902, Transition Loss -0.24517369270324707, Classifier Loss 0.09914322197437286, Total Loss 41.5632209777832\n",
      "12: Encoding Loss 6.6921844482421875, Transition Loss -3.3819241523742676, Classifier Loss 0.06283428519964218, Total Loss 46.4351806640625\n",
      "12: Encoding Loss 5.326557159423828, Transition Loss -1.8477351665496826, Classifier Loss 0.08869555592536926, Total Loss 40.82815933227539\n",
      "12: Encoding Loss 4.318027973175049, Transition Loss 0.08276957273483276, Classifier Loss 0.0983496606349945, Total Loss 35.776241302490234\n",
      "12: Encoding Loss 4.376639366149902, Transition Loss -2.290433168411255, Classifier Loss 0.03812763839960098, Total Loss 30.071685791015625\n",
      "12: Encoding Loss 7.277362823486328, Transition Loss -1.0260381698608398, Classifier Loss 0.10318916290998459, Total Loss 53.98268127441406\n",
      "12: Encoding Loss 5.9733099937438965, Transition Loss -2.634735107421875, Classifier Loss 0.16412152349948883, Total Loss 52.25096130371094\n",
      "12: Encoding Loss 5.731421947479248, Transition Loss -1.5699623823165894, Classifier Loss 0.09663956612348557, Total Loss 44.05186080932617\n",
      "12: Encoding Loss 6.243425369262695, Transition Loss -0.5776450037956238, Classifier Loss 0.14738181233406067, Total Loss 52.19850158691406\n",
      "12: Encoding Loss 6.26661491394043, Transition Loss -1.6525592803955078, Classifier Loss 0.023415634408593178, Total Loss 39.94059371948242\n",
      "12: Encoding Loss 5.081194877624512, Transition Loss -1.5766291618347168, Classifier Loss 0.16919271647930145, Total Loss 47.40581130981445\n",
      "12: Encoding Loss 3.9969305992126465, Transition Loss -0.6878952383995056, Classifier Loss 0.06417715549468994, Total Loss 30.399024963378906\n",
      "12: Encoding Loss 6.308616638183594, Transition Loss -1.9072736501693726, Classifier Loss 0.10631604492664337, Total Loss 48.482540130615234\n",
      "12: Encoding Loss 5.492085933685303, Transition Loss -0.6993781924247742, Classifier Loss 0.09619905799627304, Total Loss 42.572147369384766\n",
      "12: Encoding Loss 4.699830532073975, Transition Loss -0.4839058518409729, Classifier Loss 0.06961958110332489, Total Loss 35.16074752807617\n",
      "12: Encoding Loss 4.3453545570373535, Transition Loss -1.1119154691696167, Classifier Loss 0.10927446186542511, Total Loss 36.99912643432617\n",
      "12: Encoding Loss 4.874717712402344, Transition Loss -0.6908445358276367, Classifier Loss 0.135294571518898, Total Loss 42.777488708496094\n",
      "12: Encoding Loss 5.485764503479004, Transition Loss -0.43544328212738037, Classifier Loss 0.11311068385839462, Total Loss 44.22548294067383\n",
      "12: Encoding Loss 5.182895183563232, Transition Loss -2.966421604156494, Classifier Loss 0.058165132999420166, Total Loss 36.91270065307617\n",
      "12: Encoding Loss 2.5567479133605957, Transition Loss -1.739344835281372, Classifier Loss 0.07795258611440659, Total Loss 23.13504981994629\n",
      "12: Encoding Loss 9.42526912689209, Transition Loss -2.2066400051116943, Classifier Loss 0.1424025595188141, Total Loss 70.79098510742188\n",
      "12: Encoding Loss 6.00860071182251, Transition Loss -0.23921339213848114, Classifier Loss 0.03147771582007408, Total Loss 39.19927978515625\n",
      "12: Encoding Loss 5.728283882141113, Transition Loss -0.9026710987091064, Classifier Loss 0.15964478254318237, Total Loss 50.33382034301758\n",
      "12: Encoding Loss 4.9813032150268555, Transition Loss -1.0937260389328003, Classifier Loss 0.13441941142082214, Total Loss 43.329322814941406\n",
      "12: Encoding Loss 6.327242851257324, Transition Loss -0.9054282903671265, Classifier Loss 0.05572691187262535, Total Loss 43.53578567504883\n",
      "12: Encoding Loss 6.790702819824219, Transition Loss -1.5188663005828857, Classifier Loss 0.273389607667923, Total Loss 68.08256530761719\n",
      "12: Encoding Loss 5.007523536682129, Transition Loss -0.3050357699394226, Classifier Loss 0.07906250655651093, Total Loss 37.951271057128906\n",
      "12: Encoding Loss 5.43648624420166, Transition Loss 0.47267353534698486, Classifier Loss 0.113146111369133, Total Loss 44.12260055541992\n",
      "12: Encoding Loss 5.302952289581299, Transition Loss -0.5638580322265625, Classifier Loss 0.08685048669576645, Total Loss 40.50253677368164\n",
      "12: Encoding Loss 3.985016345977783, Transition Loss 0.22749018669128418, Classifier Loss 0.022329386323690414, Total Loss 26.234033584594727\n",
      "12: Encoding Loss 6.446961879730225, Transition Loss -1.300255537033081, Classifier Loss 0.07728035748004913, Total Loss 46.4092903137207\n",
      "12: Encoding Loss 7.174821376800537, Transition Loss -1.8218083381652832, Classifier Loss 0.19540199637413025, Total Loss 62.588401794433594\n",
      "12: Encoding Loss 4.490235328674316, Transition Loss -0.9893243312835693, Classifier Loss 0.07139919698238373, Total Loss 34.080936431884766\n",
      "12: Encoding Loss 3.8071823120117188, Transition Loss -1.9447747468948364, Classifier Loss 0.06951791048049927, Total Loss 29.794105529785156\n",
      "12: Encoding Loss 4.061013698577881, Transition Loss -1.322369933128357, Classifier Loss 0.06643606722354889, Total Loss 31.0091609954834\n",
      "12: Encoding Loss 5.031566619873047, Transition Loss -0.32828718423843384, Classifier Loss 0.04040062427520752, Total Loss 34.22933578491211\n",
      "12: Encoding Loss 5.744758605957031, Transition Loss -0.9591526985168457, Classifier Loss 0.16357488930225372, Total Loss 50.82565689086914\n",
      "12: Encoding Loss 3.0547733306884766, Transition Loss -0.299934983253479, Classifier Loss 0.11790752410888672, Total Loss 30.119272232055664\n",
      "12: Encoding Loss 5.871074676513672, Transition Loss -2.0280368328094482, Classifier Loss 0.07594332098960876, Total Loss 42.819969177246094\n",
      "12: Encoding Loss 3.49017333984375, Transition Loss -1.033668041229248, Classifier Loss 0.040985357016325, Total Loss 25.039161682128906\n",
      "12: Encoding Loss 4.451738357543945, Transition Loss -2.698246717453003, Classifier Loss 0.0376153402030468, Total Loss 30.47088623046875\n",
      "12: Encoding Loss 1.9918439388275146, Transition Loss -1.5019149780273438, Classifier Loss 0.03540398180484772, Total Loss 15.490861892700195\n",
      "12: Encoding Loss 6.0814595222473145, Transition Loss -1.6977014541625977, Classifier Loss 0.1251789927482605, Total Loss 49.005977630615234\n",
      "12: Encoding Loss 4.329336166381836, Transition Loss -1.769170880317688, Classifier Loss 0.09320393949747086, Total Loss 35.29570388793945\n",
      "12: Encoding Loss 3.8993022441864014, Transition Loss -0.6896514296531677, Classifier Loss 0.06221179664134979, Total Loss 29.616718292236328\n",
      "12: Encoding Loss 5.1781697273254395, Transition Loss 0.3785805106163025, Classifier Loss 0.07668464630842209, Total Loss 38.888916015625\n",
      "12: Encoding Loss 5.981997013092041, Transition Loss -1.7342487573623657, Classifier Loss 0.14898115396499634, Total Loss 50.789405822753906\n",
      "12: Encoding Loss 6.711568355560303, Transition Loss -3.649846076965332, Classifier Loss 0.14443454146385193, Total Loss 54.71140670776367\n",
      "12: Encoding Loss 4.63028621673584, Transition Loss -1.6054037809371948, Classifier Loss 0.07809030264616013, Total Loss 35.59010696411133\n",
      "12: Encoding Loss 5.324781894683838, Transition Loss -2.189124584197998, Classifier Loss 0.03349921852350235, Total Loss 35.29773712158203\n",
      "12: Encoding Loss 2.8024911880493164, Transition Loss -2.695840358734131, Classifier Loss 0.042566776275634766, Total Loss 21.070547103881836\n",
      "12: Encoding Loss 4.719846248626709, Transition Loss -2.8986785411834717, Classifier Loss 0.16645725071430206, Total Loss 44.963645935058594\n",
      "12: Encoding Loss 5.036089897155762, Transition Loss -0.8144730925559998, Classifier Loss 0.18425244092941284, Total Loss 48.64146041870117\n",
      "12: Encoding Loss 7.140328407287598, Transition Loss -0.39683252573013306, Classifier Loss 0.08462661504745483, Total Loss 51.304473876953125\n",
      "12: Encoding Loss 4.142230033874512, Transition Loss -1.1613848209381104, Classifier Loss 0.04258811101317406, Total Loss 29.11172866821289\n",
      "12: Encoding Loss 3.9691238403320312, Transition Loss -1.6155339479446411, Classifier Loss 0.052574340254068375, Total Loss 29.071531295776367\n",
      "12: Encoding Loss 5.3187642097473145, Transition Loss -1.4954594373703003, Classifier Loss 0.030616896227002144, Total Loss 34.97367858886719\n",
      "12: Encoding Loss 4.535388946533203, Transition Loss -0.9159865379333496, Classifier Loss 0.051594629883766174, Total Loss 32.37143325805664\n",
      "12: Encoding Loss 4.858666896820068, Transition Loss -1.4284741878509521, Classifier Loss 0.057572200894355774, Total Loss 34.90864944458008\n",
      "12: Encoding Loss 5.031687259674072, Transition Loss -1.3483481407165527, Classifier Loss 0.053398437798023224, Total Loss 35.5294303894043\n",
      "12: Encoding Loss 5.118687629699707, Transition Loss -1.3108720779418945, Classifier Loss 0.04269632324576378, Total Loss 34.98123550415039\n",
      "12: Encoding Loss 4.592738151550293, Transition Loss -1.2495828866958618, Classifier Loss 0.11785939335823059, Total Loss 39.34186935424805\n",
      "12: Encoding Loss 4.013449668884277, Transition Loss -0.815765380859375, Classifier Loss 0.08222123980522156, Total Loss 32.30249786376953\n",
      "12: Encoding Loss 5.417847633361816, Transition Loss 0.34762758016586304, Classifier Loss 0.10846494138240814, Total Loss 43.49263000488281\n",
      "12: Encoding Loss 4.452559471130371, Transition Loss -1.8552910089492798, Classifier Loss 0.057483259588479996, Total Loss 32.46294021606445\n",
      "12: Encoding Loss 4.778338432312012, Transition Loss -2.0513298511505127, Classifier Loss 0.05531059205532074, Total Loss 34.20027160644531\n",
      "12: Encoding Loss 4.41471529006958, Transition Loss -0.725163459777832, Classifier Loss 0.1336524486541748, Total Loss 39.853248596191406\n",
      "12: Encoding Loss 3.783252239227295, Transition Loss -0.24327726662158966, Classifier Loss 0.037034668028354645, Total Loss 26.402883529663086\n",
      "12: Encoding Loss 5.234323978424072, Transition Loss -2.112762928009033, Classifier Loss 0.06253526359796524, Total Loss 37.65862274169922\n",
      "12: Encoding Loss 4.095862865447998, Transition Loss -0.8567253947257996, Classifier Loss 0.029582999646663666, Total Loss 27.53313446044922\n",
      "12: Encoding Loss 6.389840602874756, Transition Loss 0.08538269996643066, Classifier Loss 0.1844129115343094, Total Loss 56.814491271972656\n",
      "12: Encoding Loss 5.562555313110352, Transition Loss -1.679524302482605, Classifier Loss 0.08018539845943451, Total Loss 41.3931999206543\n",
      "12: Encoding Loss 6.339177131652832, Transition Loss -0.4723435044288635, Classifier Loss 0.14931567013263702, Total Loss 52.9664421081543\n",
      "12: Encoding Loss 4.558251857757568, Transition Loss -1.1091629266738892, Classifier Loss 0.022824982181191444, Total Loss 29.63156509399414\n",
      "12: Encoding Loss 4.61649227142334, Transition Loss -1.0602469444274902, Classifier Loss 0.13790687918663025, Total Loss 41.489219665527344\n",
      "12: Encoding Loss 2.8685731887817383, Transition Loss -1.497564435005188, Classifier Loss 0.10876818001270294, Total Loss 28.087657928466797\n",
      "12: Encoding Loss 5.582502841949463, Transition Loss -0.34908342361450195, Classifier Loss 0.1237369030714035, Total Loss 45.868568420410156\n",
      "12: Encoding Loss 5.837552070617676, Transition Loss -1.2358888387680054, Classifier Loss 0.08608970046043396, Total Loss 43.6337890625\n",
      "12: Encoding Loss 3.7898693084716797, Transition Loss -0.49413001537323, Classifier Loss 0.11467951536178589, Total Loss 34.20697021484375\n",
      "12: Encoding Loss 4.773052215576172, Transition Loss -1.3275518417358398, Classifier Loss 0.0715191438794136, Total Loss 35.78969955444336\n",
      "12: Encoding Loss 5.96065616607666, Transition Loss -0.47207891941070557, Classifier Loss 0.06535784155130386, Total Loss 42.29953384399414\n",
      "12: Encoding Loss 3.8660056591033936, Transition Loss 0.05090826749801636, Classifier Loss 0.051805928349494934, Total Loss 28.396991729736328\n",
      "12: Encoding Loss 6.3387017250061035, Transition Loss 0.008928760886192322, Classifier Loss 0.16205164790153503, Total Loss 54.240943908691406\n",
      "12: Encoding Loss 5.550769805908203, Transition Loss -1.0425965785980225, Classifier Loss 0.16012834012508392, Total Loss 49.31703567504883\n",
      "12: Encoding Loss 4.540701389312744, Transition Loss -1.378190517425537, Classifier Loss 0.06925816088914871, Total Loss 34.169471740722656\n",
      "12: Encoding Loss 4.036280632019043, Transition Loss -1.4748640060424805, Classifier Loss 0.1488630622625351, Total Loss 39.10340118408203\n",
      "12: Encoding Loss 6.168387413024902, Transition Loss 0.01961362361907959, Classifier Loss 0.1355760097503662, Total Loss 50.575775146484375\n",
      "12: Encoding Loss 5.374481201171875, Transition Loss -1.587719202041626, Classifier Loss 0.13050012290477753, Total Loss 45.2962646484375\n",
      "12: Encoding Loss 5.740696907043457, Transition Loss -1.162721872329712, Classifier Loss 0.13509061932563782, Total Loss 47.952781677246094\n",
      "12: Encoding Loss 4.921263694763184, Transition Loss -1.1854997873306274, Classifier Loss 0.07340878993272781, Total Loss 36.86798858642578\n",
      "12: Encoding Loss 5.732883453369141, Transition Loss -1.5919077396392822, Classifier Loss 0.12348757684230804, Total Loss 46.74542236328125\n",
      "12: Encoding Loss 5.4466142654418945, Transition Loss -0.7980849742889404, Classifier Loss 0.08717714250087738, Total Loss 41.39707946777344\n",
      "12: Encoding Loss 4.479081630706787, Transition Loss -1.2140846252441406, Classifier Loss 0.09053251892328262, Total Loss 35.9272575378418\n",
      "12: Encoding Loss 4.766359329223633, Transition Loss 0.06921201944351196, Classifier Loss 0.03875872120261192, Total Loss 32.501712799072266\n",
      "12: Encoding Loss 3.806191921234131, Transition Loss -0.2650817632675171, Classifier Loss 0.04781090468168259, Total Loss 27.618135452270508\n",
      "12: Encoding Loss 6.65396785736084, Transition Loss -1.6377812623977661, Classifier Loss 0.14958596229553223, Total Loss 54.88174819946289\n",
      "12: Encoding Loss 5.5935845375061035, Transition Loss -1.7663202285766602, Classifier Loss 0.11532925069332123, Total Loss 45.093727111816406\n",
      "12: Encoding Loss 3.0490710735321045, Transition Loss -0.9933142066001892, Classifier Loss 0.0759793072938919, Total Loss 25.8919620513916\n",
      "12: Encoding Loss 7.114381313323975, Transition Loss -1.5922532081604004, Classifier Loss 0.08122815936803818, Total Loss 50.808467864990234\n",
      "12: Encoding Loss 4.375246524810791, Transition Loss -0.8286310434341431, Classifier Loss 0.06232156604528427, Total Loss 32.48330307006836\n",
      "12: Encoding Loss 4.342738151550293, Transition Loss -0.959478497505188, Classifier Loss 0.043259114027023315, Total Loss 30.381959915161133\n",
      "12: Encoding Loss 6.318240642547607, Transition Loss -1.8497653007507324, Classifier Loss 0.10529046505689621, Total Loss 48.43775177001953\n",
      "12: Encoding Loss 5.7298970222473145, Transition Loss -1.057124137878418, Classifier Loss 0.19311466813087463, Total Loss 53.690425872802734\n",
      "12: Encoding Loss 5.405438423156738, Transition Loss -1.2417700290679932, Classifier Loss 0.074143186211586, Total Loss 39.84645462036133\n",
      "12: Encoding Loss 4.524717807769775, Transition Loss -0.007912963628768921, Classifier Loss 0.072760671377182, Total Loss 34.42436981201172\n",
      "12: Encoding Loss 4.927365303039551, Transition Loss -2.2934958934783936, Classifier Loss 0.07761403918266296, Total Loss 37.32468032836914\n",
      "12: Encoding Loss 5.174874305725098, Transition Loss -1.8468183279037476, Classifier Loss 0.07884150743484497, Total Loss 38.932655334472656\n",
      "12: Encoding Loss 5.968865394592285, Transition Loss 0.2597687244415283, Classifier Loss 0.10152324289083481, Total Loss 46.069427490234375\n",
      "12: Encoding Loss 7.530470371246338, Transition Loss -1.2904987335205078, Classifier Loss 0.12682612240314484, Total Loss 57.86492156982422\n",
      "12: Encoding Loss 4.853663921356201, Transition Loss -2.26707124710083, Classifier Loss 0.041730042546987534, Total Loss 33.29408264160156\n",
      "12: Encoding Loss 6.072971343994141, Transition Loss -0.9476972818374634, Classifier Loss 0.04542919248342514, Total Loss 40.980369567871094\n",
      "12: Encoding Loss 5.311638832092285, Transition Loss -0.7298391461372375, Classifier Loss 0.1395566463470459, Total Loss 45.82520294189453\n",
      "12: Encoding Loss 4.864990711212158, Transition Loss -1.0598937273025513, Classifier Loss 0.03393755480647087, Total Loss 32.58327865600586\n",
      "13: Encoding Loss 3.9174695014953613, Transition Loss -1.1321160793304443, Classifier Loss 0.06808758527040482, Total Loss 30.31312370300293\n",
      "13: Encoding Loss 6.057171821594238, Transition Loss -0.21060319244861603, Classifier Loss 0.10600945353507996, Total Loss 46.94389343261719\n",
      "13: Encoding Loss 4.859774589538574, Transition Loss -1.575700044631958, Classifier Loss 0.07257884740829468, Total Loss 36.4159049987793\n",
      "13: Encoding Loss 5.770309925079346, Transition Loss -1.1191449165344238, Classifier Loss 0.05929858237504959, Total Loss 40.551273345947266\n",
      "13: Encoding Loss 7.795042514801025, Transition Loss 0.4468911588191986, Classifier Loss 0.17653068900108337, Total Loss 64.60208129882812\n",
      "13: Encoding Loss 6.434657096862793, Transition Loss -2.8197362422943115, Classifier Loss 0.05231498181819916, Total Loss 43.838314056396484\n",
      "13: Encoding Loss 4.16330623626709, Transition Loss -2.0667357444763184, Classifier Loss 0.13417398929595947, Total Loss 38.39641189575195\n",
      "13: Encoding Loss 4.366690635681152, Transition Loss -1.6287672519683838, Classifier Loss 0.09617282450199127, Total Loss 35.816776275634766\n",
      "13: Encoding Loss 4.871438503265381, Transition Loss -1.1366751194000244, Classifier Loss 0.1418817639350891, Total Loss 43.41635513305664\n",
      "13: Encoding Loss 3.1743264198303223, Transition Loss -0.6886815428733826, Classifier Loss 0.07153788954019547, Total Loss 26.199474334716797\n",
      "13: Encoding Loss 5.93748140335083, Transition Loss -1.3812342882156372, Classifier Loss 0.13390392065048218, Total Loss 49.01472854614258\n",
      "13: Encoding Loss 3.7765891551971436, Transition Loss -1.1469876766204834, Classifier Loss 0.07223992794752121, Total Loss 29.88306999206543\n",
      "13: Encoding Loss 2.5627224445343018, Transition Loss -0.3596533238887787, Classifier Loss 0.0421009361743927, Total Loss 19.586286544799805\n",
      "13: Encoding Loss 4.411436557769775, Transition Loss -2.120630979537964, Classifier Loss 0.08412140607833862, Total Loss 34.879913330078125\n",
      "13: Encoding Loss 3.3619537353515625, Transition Loss -2.1380982398986816, Classifier Loss 0.07063153386116028, Total Loss 27.234020233154297\n",
      "13: Encoding Loss 4.373541355133057, Transition Loss -1.4205659627914429, Classifier Loss 0.07699029892683029, Total Loss 33.9397087097168\n",
      "13: Encoding Loss 3.7768378257751465, Transition Loss -1.890871286392212, Classifier Loss 0.062024638056755066, Total Loss 28.862733840942383\n",
      "13: Encoding Loss 2.6247923374176025, Transition Loss 0.5164691805839539, Classifier Loss 0.0711599588394165, Total Loss 23.07133674621582\n",
      "13: Encoding Loss 7.009432792663574, Transition Loss -2.369321346282959, Classifier Loss 0.08871927857398987, Total Loss 50.927581787109375\n",
      "13: Encoding Loss 4.016918659210205, Transition Loss -0.5769259333610535, Classifier Loss 0.19080182909965515, Total Loss 43.18146514892578\n",
      "13: Encoding Loss 6.599679470062256, Transition Loss -2.3531723022460938, Classifier Loss 0.21020051836967468, Total Loss 60.617191314697266\n",
      "13: Encoding Loss 5.623766899108887, Transition Loss -1.302545189857483, Classifier Loss 0.10849130153656006, Total Loss 44.59121322631836\n",
      "13: Encoding Loss 6.280201435089111, Transition Loss -0.5469395518302917, Classifier Loss 0.10424835234880447, Total Loss 48.10582733154297\n",
      "13: Encoding Loss 6.276158332824707, Transition Loss -0.15630602836608887, Classifier Loss 0.10096842050552368, Total Loss 47.75373077392578\n",
      "13: Encoding Loss 5.284904479980469, Transition Loss 0.34181076288223267, Classifier Loss 0.04678702354431152, Total Loss 36.52485275268555\n",
      "13: Encoding Loss 5.04818058013916, Transition Loss -1.9217174053192139, Classifier Loss 0.08338028192520142, Total Loss 38.6263427734375\n",
      "13: Encoding Loss 3.5256686210632324, Transition Loss -0.34000957012176514, Classifier Loss 0.05815589427947998, Total Loss 26.969467163085938\n",
      "13: Encoding Loss 7.147796630859375, Transition Loss -1.5414025783538818, Classifier Loss 0.1548166424036026, Total Loss 58.367828369140625\n",
      "13: Encoding Loss 3.2109103202819824, Transition Loss -2.379801034927368, Classifier Loss 0.06018972024321556, Total Loss 25.283483505249023\n",
      "13: Encoding Loss 3.573012113571167, Transition Loss -1.3528814315795898, Classifier Loss 0.14704053103923798, Total Loss 36.14158630371094\n",
      "13: Encoding Loss 4.493956089019775, Transition Loss -1.8171123266220093, Classifier Loss 0.08066409826278687, Total Loss 35.0294189453125\n",
      "13: Encoding Loss 4.417079448699951, Transition Loss 0.6771562695503235, Classifier Loss 0.04841068759560585, Total Loss 31.614408493041992\n",
      "13: Encoding Loss 6.21457052230835, Transition Loss -1.0816162824630737, Classifier Loss 0.1398238241672516, Total Loss 51.269378662109375\n",
      "13: Encoding Loss 3.9654381275177, Transition Loss -1.4192389249801636, Classifier Loss 0.06644491106271744, Total Loss 30.436552047729492\n",
      "13: Encoding Loss 3.997614622116089, Transition Loss -1.2826405763626099, Classifier Loss 0.11112546920776367, Total Loss 35.09772491455078\n",
      "13: Encoding Loss 5.177538871765137, Transition Loss -1.4056506156921387, Classifier Loss 0.2099880874156952, Total Loss 52.063480377197266\n",
      "13: Encoding Loss 4.624666213989258, Transition Loss -1.3448690176010132, Classifier Loss 0.08763708919286728, Total Loss 36.51116943359375\n",
      "13: Encoding Loss 4.726958274841309, Transition Loss -0.3428121507167816, Classifier Loss 0.057867925614118576, Total Loss 34.148406982421875\n",
      "13: Encoding Loss 8.52850341796875, Transition Loss 0.09403714537620544, Classifier Loss 0.19759854674339294, Total Loss 70.96849060058594\n",
      "13: Encoding Loss 7.2422380447387695, Transition Loss -1.6368545293807983, Classifier Loss 0.11214686930179596, Total Loss 54.66746139526367\n",
      "13: Encoding Loss 5.2563886642456055, Transition Loss -1.1156115531921387, Classifier Loss 0.12741370499134064, Total Loss 44.279258728027344\n",
      "13: Encoding Loss 6.180646896362305, Transition Loss -1.1188828945159912, Classifier Loss 0.12346113473176956, Total Loss 49.42954635620117\n",
      "13: Encoding Loss 5.700406074523926, Transition Loss -1.1533368825912476, Classifier Loss 0.13235501945018768, Total Loss 47.43748092651367\n",
      "13: Encoding Loss 8.172256469726562, Transition Loss -2.3784894943237305, Classifier Loss 0.061096277087926865, Total Loss 55.14221954345703\n",
      "13: Encoding Loss 4.879128456115723, Transition Loss -1.9016319513320923, Classifier Loss 0.10771112143993378, Total Loss 40.04512405395508\n",
      "13: Encoding Loss 3.474179744720459, Transition Loss 0.13734883069992065, Classifier Loss 0.05316068232059479, Total Loss 26.216087341308594\n",
      "13: Encoding Loss 4.332622528076172, Transition Loss -1.0774673223495483, Classifier Loss 0.08895888179540634, Total Loss 34.89119338989258\n",
      "13: Encoding Loss 6.7081618309021, Transition Loss 0.009657666087150574, Classifier Loss 0.07654648274183273, Total Loss 47.90748596191406\n",
      "13: Encoding Loss 7.070793151855469, Transition Loss -0.3407590985298157, Classifier Loss 0.19251297414302826, Total Loss 61.67591857910156\n",
      "13: Encoding Loss 3.9475626945495605, Transition Loss -0.22137483954429626, Classifier Loss 0.05074800178408623, Total Loss 28.760089874267578\n",
      "13: Encoding Loss 4.729474067687988, Transition Loss -1.5488851070404053, Classifier Loss 0.13412213325500488, Total Loss 41.7884407043457\n",
      "13: Encoding Loss 3.514852523803711, Transition Loss -1.8798331022262573, Classifier Loss 0.02632378414273262, Total Loss 23.720741271972656\n",
      "13: Encoding Loss 7.144148826599121, Transition Loss -0.9614806175231934, Classifier Loss 0.0775158628821373, Total Loss 50.61609649658203\n",
      "13: Encoding Loss 4.8335113525390625, Transition Loss -0.03327026963233948, Classifier Loss 0.15335820615291595, Total Loss 44.33687973022461\n",
      "13: Encoding Loss 2.8123512268066406, Transition Loss -1.980339527130127, Classifier Loss 0.10548931360244751, Total Loss 27.4222469329834\n",
      "13: Encoding Loss 7.29132080078125, Transition Loss -0.2609677314758301, Classifier Loss 0.05450941622257233, Total Loss 49.19876480102539\n",
      "13: Encoding Loss 7.119084358215332, Transition Loss -0.46511220932006836, Classifier Loss 0.14239078760147095, Total Loss 56.953399658203125\n",
      "13: Encoding Loss 3.6396305561065674, Transition Loss -1.0820363759994507, Classifier Loss 0.05109548568725586, Total Loss 26.9468994140625\n",
      "13: Encoding Loss 9.985844612121582, Transition Loss -2.0889601707458496, Classifier Loss 0.1894656866788864, Total Loss 78.86080169677734\n",
      "13: Encoding Loss 5.943416118621826, Transition Loss -1.6847864389419556, Classifier Loss 0.034244608134031296, Total Loss 39.084285736083984\n",
      "13: Encoding Loss 5.697326183319092, Transition Loss -0.8490917086601257, Classifier Loss 0.08250397443771362, Total Loss 42.434017181396484\n",
      "13: Encoding Loss 4.046736717224121, Transition Loss -2.374276638031006, Classifier Loss 0.07187634706497192, Total Loss 31.467105865478516\n",
      "13: Encoding Loss 3.823361873626709, Transition Loss -2.3697450160980225, Classifier Loss 0.11628317832946777, Total Loss 34.567543029785156\n",
      "13: Encoding Loss 5.103095054626465, Transition Loss -4.227357387542725, Classifier Loss 0.09337561577558517, Total Loss 39.954444885253906\n",
      "13: Encoding Loss 4.714378833770752, Transition Loss -0.7873578071594238, Classifier Loss 0.11356443166732788, Total Loss 39.642398834228516\n",
      "13: Encoding Loss 5.901750564575195, Transition Loss -2.1369071006774902, Classifier Loss 0.2076321542263031, Total Loss 56.17286682128906\n",
      "13: Encoding Loss 3.403055191040039, Transition Loss -1.2314516305923462, Classifier Loss 0.19896361231803894, Total Loss 40.3141975402832\n",
      "13: Encoding Loss 5.6944098472595215, Transition Loss -1.6853495836257935, Classifier Loss 0.07026074081659317, Total Loss 41.19186019897461\n",
      "13: Encoding Loss 4.31542444229126, Transition Loss 0.030110925436019897, Classifier Loss 0.08686042577028275, Total Loss 34.590633392333984\n",
      "13: Encoding Loss 2.773212432861328, Transition Loss -0.05858969688415527, Classifier Loss 0.06849256157875061, Total Loss 23.488508224487305\n",
      "13: Encoding Loss 4.7143754959106445, Transition Loss -2.234133005142212, Classifier Loss 0.13690230250358582, Total Loss 41.975589752197266\n",
      "13: Encoding Loss 2.782252550125122, Transition Loss -1.5235003232955933, Classifier Loss 0.10078282654285431, Total Loss 26.77118682861328\n",
      "13: Encoding Loss 2.3067705631256104, Transition Loss -2.1816816329956055, Classifier Loss 0.09077711403369904, Total Loss 22.917461395263672\n",
      "13: Encoding Loss 6.8289618492126465, Transition Loss -0.37339597940444946, Classifier Loss 0.03558105230331421, Total Loss 44.53173065185547\n",
      "13: Encoding Loss 3.99025297164917, Transition Loss -1.5154244899749756, Classifier Loss 0.0455690398812294, Total Loss 28.49781608581543\n",
      "13: Encoding Loss 5.42036771774292, Transition Loss -1.7850011587142944, Classifier Loss 0.14252915978431702, Total Loss 46.774410247802734\n",
      "13: Encoding Loss 3.817591428756714, Transition Loss -1.9615591764450073, Classifier Loss 0.03388632461428642, Total Loss 26.293397903442383\n",
      "13: Encoding Loss 6.878855228424072, Transition Loss -1.5221617221832275, Classifier Loss 0.08659995347261429, Total Loss 49.932518005371094\n",
      "13: Encoding Loss 4.736833572387695, Transition Loss -1.3757438659667969, Classifier Loss 0.13930608332157135, Total Loss 42.3510627746582\n",
      "13: Encoding Loss 2.808370351791382, Transition Loss -1.2851877212524414, Classifier Loss 0.0652240440249443, Total Loss 23.372112274169922\n",
      "13: Encoding Loss 6.411862373352051, Transition Loss -0.903721034526825, Classifier Loss 0.06058157980442047, Total Loss 44.52897262573242\n",
      "13: Encoding Loss 5.14605188369751, Transition Loss -1.0449395179748535, Classifier Loss 0.1824018806219101, Total Loss 49.11608123779297\n",
      "13: Encoding Loss 3.5389468669891357, Transition Loss -0.20652826130390167, Classifier Loss 0.14230316877365112, Total Loss 35.46391677856445\n",
      "13: Encoding Loss 4.6764302253723145, Transition Loss -1.0699056386947632, Classifier Loss 0.11370400339365005, Total Loss 39.42855453491211\n",
      "13: Encoding Loss 5.517228603363037, Transition Loss -1.139013648033142, Classifier Loss 0.13256153464317322, Total Loss 46.359073638916016\n",
      "13: Encoding Loss 4.839949607849121, Transition Loss -2.4966073036193848, Classifier Loss 0.10347291827201843, Total Loss 39.38599395751953\n",
      "13: Encoding Loss 4.483222007751465, Transition Loss 0.8647704124450684, Classifier Loss 0.060902830213308334, Total Loss 33.33552551269531\n",
      "13: Encoding Loss 5.339625358581543, Transition Loss -1.5797686576843262, Classifier Loss 0.16876161098480225, Total Loss 48.91328430175781\n",
      "13: Encoding Loss 5.306418418884277, Transition Loss -2.485264301300049, Classifier Loss 0.033872392028570175, Total Loss 35.224754333496094\n",
      "13: Encoding Loss 4.556149959564209, Transition Loss -1.795081615447998, Classifier Loss 0.11881008744239807, Total Loss 39.217193603515625\n",
      "13: Encoding Loss 3.7965404987335205, Transition Loss 0.8429962396621704, Classifier Loss 0.11431854218244553, Total Loss 34.54829788208008\n",
      "13: Encoding Loss 3.0495657920837402, Transition Loss -1.368444561958313, Classifier Loss 0.061732809990644455, Total Loss 24.470129013061523\n",
      "13: Encoding Loss 5.704930305480957, Transition Loss -1.81920325756073, Classifier Loss 0.06927809864282608, Total Loss 41.15666580200195\n",
      "13: Encoding Loss 4.44706916809082, Transition Loss 1.075893521308899, Classifier Loss 0.06338562816381454, Total Loss 33.45133972167969\n",
      "13: Encoding Loss 3.7130465507507324, Transition Loss -1.8554561138153076, Classifier Loss 0.047102026641368866, Total Loss 26.987741470336914\n",
      "13: Encoding Loss 2.771721363067627, Transition Loss -1.3794854879379272, Classifier Loss 0.10142078995704651, Total Loss 26.7718563079834\n",
      "13: Encoding Loss 3.512753963470459, Transition Loss 0.22120141983032227, Classifier Loss 0.04767068475484848, Total Loss 25.93207359313965\n",
      "13: Encoding Loss 10.570719718933105, Transition Loss 0.17106640338897705, Classifier Loss 0.235311359167099, Total Loss 87.02388763427734\n",
      "13: Encoding Loss 6.191665172576904, Transition Loss -1.1140673160552979, Classifier Loss 0.10181660950183868, Total Loss 47.331207275390625\n",
      "13: Encoding Loss 7.7685747146606445, Transition Loss -1.0688741207122803, Classifier Loss 0.15533657371997833, Total Loss 62.14468002319336\n",
      "13: Encoding Loss 5.92394495010376, Transition Loss -1.646013617515564, Classifier Loss 0.048172108829021454, Total Loss 40.36022186279297\n",
      "13: Encoding Loss 5.156883239746094, Transition Loss -1.8702666759490967, Classifier Loss 0.04895049333572388, Total Loss 35.835601806640625\n",
      "13: Encoding Loss 4.674118995666504, Transition Loss -0.44359153509140015, Classifier Loss 0.11792681366205215, Total Loss 39.837215423583984\n",
      "13: Encoding Loss 4.408764839172363, Transition Loss -2.1604714393615723, Classifier Loss 0.08813004195690155, Total Loss 35.26472854614258\n",
      "13: Encoding Loss 2.8665876388549805, Transition Loss -0.1908162236213684, Classifier Loss 0.12440606206655502, Total Loss 29.640056610107422\n",
      "13: Encoding Loss 3.049696922302246, Transition Loss 0.817340612411499, Classifier Loss 0.036567322909832, Total Loss 22.281850814819336\n",
      "13: Encoding Loss 6.490166664123535, Transition Loss 0.09578067064285278, Classifier Loss 0.09887070953845978, Total Loss 48.86638259887695\n",
      "13: Encoding Loss 3.8705813884735107, Transition Loss -2.2079262733459473, Classifier Loss 0.032919783145189285, Total Loss 26.514585494995117\n",
      "13: Encoding Loss 9.018109321594238, Transition Loss -2.0115954875946045, Classifier Loss 0.13680429756641388, Total Loss 67.78828430175781\n",
      "13: Encoding Loss 6.901576042175293, Transition Loss -1.0523395538330078, Classifier Loss 0.1681589037179947, Total Loss 58.22492980957031\n",
      "13: Encoding Loss 3.998537540435791, Transition Loss -1.6331866979599, Classifier Loss 0.06375251710414886, Total Loss 30.36582374572754\n",
      "13: Encoding Loss 6.187146186828613, Transition Loss -0.31834590435028076, Classifier Loss 0.09272114932537079, Total Loss 46.394866943359375\n",
      "13: Encoding Loss 6.0464186668396, Transition Loss -0.921606183052063, Classifier Loss 0.07043956965208054, Total Loss 43.32210159301758\n",
      "13: Encoding Loss 5.9067912101745605, Transition Loss -1.0922316312789917, Classifier Loss 0.03678464889526367, Total Loss 39.118778228759766\n",
      "13: Encoding Loss 6.859352111816406, Transition Loss -1.3318703174591064, Classifier Loss 0.06358323246240616, Total Loss 47.51390075683594\n",
      "13: Encoding Loss 4.189626216888428, Transition Loss -2.606706142425537, Classifier Loss 0.08032596111297607, Total Loss 33.169315338134766\n",
      "13: Encoding Loss 3.9108810424804688, Transition Loss 0.029952406883239746, Classifier Loss 0.070697121322155, Total Loss 30.546977996826172\n",
      "13: Encoding Loss 3.0179009437561035, Transition Loss -0.9286319017410278, Classifier Loss 0.050832927227020264, Total Loss 23.19032859802246\n",
      "13: Encoding Loss 3.996401309967041, Transition Loss -1.2727534770965576, Classifier Loss 0.07442814856767654, Total Loss 31.420713424682617\n",
      "13: Encoding Loss 5.317368030548096, Transition Loss -0.28455349802970886, Classifier Loss 0.09474717080593109, Total Loss 41.37881088256836\n",
      "13: Encoding Loss 6.247898578643799, Transition Loss -0.9043139219284058, Classifier Loss 0.18564186990261078, Total Loss 56.05121612548828\n",
      "13: Encoding Loss 4.7363739013671875, Transition Loss -1.8302568197250366, Classifier Loss 0.12495055794715881, Total Loss 40.912567138671875\n",
      "13: Encoding Loss 6.610072135925293, Transition Loss -1.512599229812622, Classifier Loss 0.08990040421485901, Total Loss 48.64986801147461\n",
      "13: Encoding Loss 4.3124895095825195, Transition Loss -1.8310545682907104, Classifier Loss 0.051643580198287964, Total Loss 31.038562774658203\n",
      "13: Encoding Loss 2.697782039642334, Transition Loss -1.3402167558670044, Classifier Loss 0.060402657836675644, Total Loss 22.226423263549805\n",
      "13: Encoding Loss 5.791760444641113, Transition Loss -2.271559715270996, Classifier Loss 0.05486438050866127, Total Loss 40.2360954284668\n",
      "13: Encoding Loss 6.742918491363525, Transition Loss -2.419754981994629, Classifier Loss 0.17938359081745148, Total Loss 58.394901275634766\n",
      "13: Encoding Loss 4.790596008300781, Transition Loss -2.1116714477539062, Classifier Loss 0.037531301379203796, Total Loss 32.49586486816406\n",
      "13: Encoding Loss 3.142714023590088, Transition Loss -1.1349146366119385, Classifier Loss 0.041774600744247437, Total Loss 23.03329086303711\n",
      "13: Encoding Loss 2.4020791053771973, Transition Loss -3.6379833221435547, Classifier Loss 0.05333352088928223, Total Loss 19.74437141418457\n",
      "13: Encoding Loss 1.436394453048706, Transition Loss -0.9726157784461975, Classifier Loss 0.1287447214126587, Total Loss 21.492450714111328\n",
      "13: Encoding Loss 5.740866661071777, Transition Loss -1.2290412187576294, Classifier Loss 0.06528089940547943, Total Loss 40.972801208496094\n",
      "13: Encoding Loss 5.089958190917969, Transition Loss -1.717392086982727, Classifier Loss 0.08887682110071182, Total Loss 39.4267463684082\n",
      "13: Encoding Loss 3.9324684143066406, Transition Loss -1.4501701593399048, Classifier Loss 0.046980906277894974, Total Loss 28.292320251464844\n",
      "13: Encoding Loss 6.210362911224365, Transition Loss -1.149002194404602, Classifier Loss 0.10382474213838577, Total Loss 47.644195556640625\n",
      "13: Encoding Loss 6.10520076751709, Transition Loss -1.0822798013687134, Classifier Loss 0.1776248961687088, Total Loss 54.39326477050781\n",
      "13: Encoding Loss 4.701506614685059, Transition Loss 0.12183386087417603, Classifier Loss 0.06262756884098053, Total Loss 34.520530700683594\n",
      "13: Encoding Loss 5.203401565551758, Transition Loss 0.23839950561523438, Classifier Loss 0.10341009497642517, Total Loss 41.65678024291992\n",
      "13: Encoding Loss 3.6099040508270264, Transition Loss -1.6816118955612183, Classifier Loss 0.0456814169883728, Total Loss 26.22689437866211\n",
      "13: Encoding Loss 7.688366889953613, Transition Loss 0.14641587436199188, Classifier Loss 0.04894990473985672, Total Loss 51.08375930786133\n",
      "13: Encoding Loss 5.692800998687744, Transition Loss -1.4764513969421387, Classifier Loss 0.04753096029162407, Total Loss 38.9093132019043\n",
      "13: Encoding Loss 6.182279586791992, Transition Loss -0.49584370851516724, Classifier Loss 0.09032409638166428, Total Loss 46.12588882446289\n",
      "13: Encoding Loss 5.1443281173706055, Transition Loss -1.042164921760559, Classifier Loss 0.03865448385477066, Total Loss 34.73100280761719\n",
      "13: Encoding Loss 5.119564533233643, Transition Loss -0.19772930443286896, Classifier Loss 0.06598825007677078, Total Loss 37.316131591796875\n",
      "13: Encoding Loss 3.672351837158203, Transition Loss -1.173120141029358, Classifier Loss 0.07373490929603577, Total Loss 29.407133102416992\n",
      "13: Encoding Loss 5.22017765045166, Transition Loss -0.8671913146972656, Classifier Loss 0.18295210599899292, Total Loss 49.615928649902344\n",
      "13: Encoding Loss 2.8291754722595215, Transition Loss -1.654658317565918, Classifier Loss 0.044511616230010986, Total Loss 21.425554275512695\n",
      "13: Encoding Loss 6.377457618713379, Transition Loss -1.2264066934585571, Classifier Loss 0.28061485290527344, Total Loss 66.32574462890625\n",
      "13: Encoding Loss 6.275606632232666, Transition Loss -0.6479891538619995, Classifier Loss 0.20311322808265686, Total Loss 57.96470642089844\n",
      "13: Encoding Loss 3.806797742843628, Transition Loss -1.61573326587677, Classifier Loss 0.12063213437795639, Total Loss 34.90335464477539\n",
      "13: Encoding Loss 5.708405494689941, Transition Loss -1.991154432296753, Classifier Loss 0.06439817696809769, Total Loss 40.689456939697266\n",
      "13: Encoding Loss 6.873064994812012, Transition Loss -0.25168371200561523, Classifier Loss 0.08650293946266174, Total Loss 49.888587951660156\n",
      "13: Encoding Loss 7.2661895751953125, Transition Loss -0.6464371681213379, Classifier Loss 0.1736413687467575, Total Loss 60.96101379394531\n",
      "13: Encoding Loss 4.880124568939209, Transition Loss -1.8965224027633667, Classifier Loss 0.06201182305812836, Total Loss 35.481170654296875\n",
      "13: Encoding Loss 4.574637413024902, Transition Loss -3.285341739654541, Classifier Loss 0.07598049938678741, Total Loss 35.04456329345703\n",
      "13: Encoding Loss 6.907890319824219, Transition Loss -2.107422351837158, Classifier Loss 0.1570342630147934, Total Loss 57.149925231933594\n",
      "13: Encoding Loss 5.711759090423584, Transition Loss -0.8284391164779663, Classifier Loss 0.06877288222312927, Total Loss 41.14751434326172\n",
      "13: Encoding Loss 3.687134265899658, Transition Loss -2.4178824424743652, Classifier Loss 0.07383361458778381, Total Loss 29.50520133972168\n",
      "13: Encoding Loss 2.6457056999206543, Transition Loss -0.909255862236023, Classifier Loss 0.07134635001420975, Total Loss 23.00850486755371\n",
      "13: Encoding Loss 7.204586029052734, Transition Loss -2.071640729904175, Classifier Loss 0.11192154884338379, Total Loss 54.41884231567383\n",
      "13: Encoding Loss 4.562029838562012, Transition Loss -0.03857612609863281, Classifier Loss 0.08247518539428711, Total Loss 35.61968231201172\n",
      "13: Encoding Loss 5.943421840667725, Transition Loss -0.6754708886146545, Classifier Loss 0.07086542993783951, Total Loss 42.74680709838867\n",
      "13: Encoding Loss 3.7329468727111816, Transition Loss -0.9830004572868347, Classifier Loss 0.04045567661523819, Total Loss 26.442855834960938\n",
      "13: Encoding Loss 4.055678844451904, Transition Loss -1.4066481590270996, Classifier Loss 0.06569123268127441, Total Loss 30.90263557434082\n",
      "13: Encoding Loss 7.684088230133057, Transition Loss 0.15601933002471924, Classifier Loss 0.16253778338432312, Total Loss 62.42071533203125\n",
      "13: Encoding Loss 5.472240447998047, Transition Loss -1.3728907108306885, Classifier Loss 0.15152332186698914, Total Loss 47.985225677490234\n",
      "13: Encoding Loss 5.446382999420166, Transition Loss -3.0334033966064453, Classifier Loss 0.1430434286594391, Total Loss 46.98143005371094\n",
      "13: Encoding Loss 3.5371220111846924, Transition Loss -2.0377771854400635, Classifier Loss 0.05344156175851822, Total Loss 26.56607437133789\n",
      "13: Encoding Loss 7.430014133453369, Transition Loss -1.8348182439804077, Classifier Loss 0.1317378282546997, Total Loss 57.753135681152344\n",
      "13: Encoding Loss 5.1872100830078125, Transition Loss -0.7016111016273499, Classifier Loss 0.04389606788754463, Total Loss 35.51258850097656\n",
      "13: Encoding Loss 4.6035966873168945, Transition Loss -1.4783148765563965, Classifier Loss 0.0626630038022995, Total Loss 33.88728713989258\n",
      "13: Encoding Loss 3.7333176136016846, Transition Loss -1.8788957595825195, Classifier Loss 0.03101363219320774, Total Loss 25.500518798828125\n",
      "13: Encoding Loss 5.255323886871338, Transition Loss -1.3025293350219727, Classifier Loss 0.16601845622062683, Total Loss 48.13326644897461\n",
      "13: Encoding Loss 5.84542989730835, Transition Loss -3.1243767738342285, Classifier Loss 0.10590947419404984, Total Loss 45.66227722167969\n",
      "13: Encoding Loss 3.8977737426757812, Transition Loss -0.7245209217071533, Classifier Loss 0.08706673979759216, Total Loss 32.09302520751953\n",
      "13: Encoding Loss 6.605302333831787, Transition Loss -0.07133037596940994, Classifier Loss 0.13613830506801605, Total Loss 53.24562072753906\n",
      "13: Encoding Loss 6.842512607574463, Transition Loss -1.3356928825378418, Classifier Loss 0.133286714553833, Total Loss 54.38321304321289\n",
      "13: Encoding Loss 5.499494552612305, Transition Loss -2.6317334175109863, Classifier Loss 0.08766482770442963, Total Loss 41.76239776611328\n",
      "13: Encoding Loss 6.239971160888672, Transition Loss -1.1098575592041016, Classifier Loss 0.03849157691001892, Total Loss 41.288543701171875\n",
      "13: Encoding Loss 5.156073093414307, Transition Loss 0.4169785678386688, Classifier Loss 0.09495745599269867, Total Loss 40.598976135253906\n",
      "13: Encoding Loss 5.4416399002075195, Transition Loss -0.6572892069816589, Classifier Loss 0.12028490006923676, Total Loss 44.67806625366211\n",
      "13: Encoding Loss 7.185997009277344, Transition Loss -0.7708927392959595, Classifier Loss 0.047679781913757324, Total Loss 47.88365173339844\n",
      "13: Encoding Loss 5.055180549621582, Transition Loss -2.18747615814209, Classifier Loss 0.04957686737179756, Total Loss 35.28789520263672\n",
      "13: Encoding Loss 6.842409610748291, Transition Loss -1.5263798236846924, Classifier Loss 0.13969434797763824, Total Loss 55.023284912109375\n",
      "13: Encoding Loss 5.135551929473877, Transition Loss -0.9644255638122559, Classifier Loss 0.1772668957710266, Total Loss 48.539615631103516\n",
      "13: Encoding Loss 5.41237735748291, Transition Loss -2.614542007446289, Classifier Loss 0.0988367423415184, Total Loss 42.356895446777344\n",
      "13: Encoding Loss 3.275787353515625, Transition Loss 0.42411699891090393, Classifier Loss 0.08727004379034042, Total Loss 28.551376342773438\n",
      "13: Encoding Loss 3.4837565422058105, Transition Loss -0.5942882299423218, Classifier Loss 0.06517922878265381, Total Loss 27.420225143432617\n",
      "13: Encoding Loss 2.54560923576355, Transition Loss -1.0751607418060303, Classifier Loss 0.06844264268875122, Total Loss 22.117490768432617\n",
      "13: Encoding Loss 7.363482475280762, Transition Loss -2.1312458515167236, Classifier Loss 0.13042259216308594, Total Loss 57.22230529785156\n",
      "13: Encoding Loss 4.465968132019043, Transition Loss -1.510307788848877, Classifier Loss 0.09819840639829636, Total Loss 36.615047454833984\n",
      "13: Encoding Loss 6.520423889160156, Transition Loss -1.5818672180175781, Classifier Loss 0.09335882216691971, Total Loss 48.457794189453125\n",
      "13: Encoding Loss 4.422929286956787, Transition Loss -1.1081209182739258, Classifier Loss 0.10951267182826996, Total Loss 37.4884033203125\n",
      "13: Encoding Loss 4.358412742614746, Transition Loss -1.786182165145874, Classifier Loss 0.11129173636436462, Total Loss 37.27893829345703\n",
      "13: Encoding Loss 3.998364210128784, Transition Loss -1.7220287322998047, Classifier Loss 0.13352759182453156, Total Loss 37.342254638671875\n",
      "13: Encoding Loss 4.433773994445801, Transition Loss -1.3846092224121094, Classifier Loss 0.09800723195075989, Total Loss 36.40281295776367\n",
      "13: Encoding Loss 6.478245735168457, Transition Loss -1.0624135732650757, Classifier Loss 0.11183827370405197, Total Loss 50.052879333496094\n",
      "13: Encoding Loss 4.930398464202881, Transition Loss -0.6603429317474365, Classifier Loss 0.21625560522079468, Total Loss 51.20769119262695\n",
      "13: Encoding Loss 7.028238773345947, Transition Loss -1.777522325515747, Classifier Loss 0.08894605934619904, Total Loss 51.063331604003906\n",
      "13: Encoding Loss 4.319005012512207, Transition Loss -1.8591605424880981, Classifier Loss 0.0697476863861084, Total Loss 32.88805389404297\n",
      "13: Encoding Loss 6.083104133605957, Transition Loss -1.4595388174057007, Classifier Loss 0.07880876958370209, Total Loss 44.37892150878906\n",
      "13: Encoding Loss 4.499183654785156, Transition Loss -0.9278585314750671, Classifier Loss 0.07049163430929184, Total Loss 34.04389572143555\n",
      "13: Encoding Loss 4.361748695373535, Transition Loss -0.8806266188621521, Classifier Loss 0.04937982186675072, Total Loss 31.108121871948242\n",
      "13: Encoding Loss 3.1328048706054688, Transition Loss -0.411507248878479, Classifier Loss 0.059345729649066925, Total Loss 24.731239318847656\n",
      "13: Encoding Loss 5.389991760253906, Transition Loss -1.0067356824874878, Classifier Loss 0.07116671651601791, Total Loss 39.45621871948242\n",
      "13: Encoding Loss 5.75239372253418, Transition Loss -0.7033707499504089, Classifier Loss 0.16623596847057343, Total Loss 51.13767623901367\n",
      "13: Encoding Loss 3.5065975189208984, Transition Loss -1.4951257705688477, Classifier Loss 0.15144230425357819, Total Loss 36.1832160949707\n",
      "13: Encoding Loss 4.219109058380127, Transition Loss -0.010777994990348816, Classifier Loss 0.11630327999591827, Total Loss 36.94498062133789\n",
      "13: Encoding Loss 7.242568016052246, Transition Loss -1.1294102668762207, Classifier Loss 0.18705002963542938, Total Loss 62.15996551513672\n",
      "13: Encoding Loss 5.985917568206787, Transition Loss -0.24235260486602783, Classifier Loss 0.08796125650405884, Total Loss 44.7115364074707\n",
      "13: Encoding Loss 5.1399054527282715, Transition Loss -0.817358136177063, Classifier Loss 0.03993924707174301, Total Loss 34.833030700683594\n",
      "13: Encoding Loss 6.331851005554199, Transition Loss -0.4403069019317627, Classifier Loss 0.1104806438088417, Total Loss 49.038997650146484\n",
      "13: Encoding Loss 4.542299747467041, Transition Loss -3.0922813415527344, Classifier Loss 0.11733655631542206, Total Loss 38.98622131347656\n",
      "13: Encoding Loss 3.2889912128448486, Transition Loss 1.1433802843093872, Classifier Loss 0.061063725501298904, Total Loss 26.297672271728516\n",
      "13: Encoding Loss 3.4958910942077637, Transition Loss -2.321066379547119, Classifier Loss 0.0649813562631607, Total Loss 27.47255516052246\n",
      "13: Encoding Loss 5.0999369621276855, Transition Loss -2.1323890686035156, Classifier Loss 0.03248738497495651, Total Loss 33.84750747680664\n",
      "13: Encoding Loss 6.066015243530273, Transition Loss -1.616328239440918, Classifier Loss 0.06194550171494484, Total Loss 42.589996337890625\n",
      "13: Encoding Loss 5.867233753204346, Transition Loss -1.1644339561462402, Classifier Loss 0.05087205395102501, Total Loss 40.29014205932617\n",
      "13: Encoding Loss 4.4385809898376465, Transition Loss -0.7636892795562744, Classifier Loss 0.09700034558773041, Total Loss 36.331214904785156\n",
      "13: Encoding Loss 4.70827054977417, Transition Loss -0.7130153775215149, Classifier Loss 0.04411039501428604, Total Loss 32.660377502441406\n",
      "13: Encoding Loss 2.902726650238037, Transition Loss -0.7128753662109375, Classifier Loss 0.09066138416528702, Total Loss 26.48221206665039\n",
      "13: Encoding Loss 6.654696464538574, Transition Loss -2.353452444076538, Classifier Loss 0.05245303735136986, Total Loss 45.172542572021484\n",
      "13: Encoding Loss 4.6123456954956055, Transition Loss -1.448101282119751, Classifier Loss 0.05176889896392822, Total Loss 32.85038757324219\n",
      "13: Encoding Loss 6.257370471954346, Transition Loss -1.586082100868225, Classifier Loss 0.15221916139125824, Total Loss 52.765506744384766\n",
      "13: Encoding Loss 4.400223731994629, Transition Loss -1.4789260625839233, Classifier Loss 0.01970416121184826, Total Loss 28.371166229248047\n",
      "13: Encoding Loss 5.6840410232543945, Transition Loss -0.19340254366397858, Classifier Loss 0.06621504575014114, Total Loss 40.725677490234375\n",
      "13: Encoding Loss 4.990874767303467, Transition Loss -1.4240636825561523, Classifier Loss 0.12146693468093872, Total Loss 42.091373443603516\n",
      "13: Encoding Loss 4.592634677886963, Transition Loss -1.4323140382766724, Classifier Loss 0.07116018235683441, Total Loss 34.67125701904297\n",
      "13: Encoding Loss 2.886441469192505, Transition Loss -2.408001661300659, Classifier Loss 0.054878007620573044, Total Loss 22.80548667907715\n",
      "13: Encoding Loss 5.2256083488464355, Transition Loss -3.360445976257324, Classifier Loss 0.05617979168891907, Total Loss 36.97028732299805\n",
      "13: Encoding Loss 3.3875021934509277, Transition Loss -0.6040481328964233, Classifier Loss 0.13154076039791107, Total Loss 33.47884750366211\n",
      "13: Encoding Loss 4.973378658294678, Transition Loss -0.8090402483940125, Classifier Loss 0.14180874824523926, Total Loss 44.02082443237305\n",
      "13: Encoding Loss 5.562318325042725, Transition Loss -0.538716197013855, Classifier Loss 0.10344985127449036, Total Loss 43.718685150146484\n",
      "13: Encoding Loss 4.0218281745910645, Transition Loss -0.42809081077575684, Classifier Loss 0.03788983076810837, Total Loss 27.919780731201172\n",
      "13: Encoding Loss 8.287919044494629, Transition Loss -0.16578441858291626, Classifier Loss 0.11850202828645706, Total Loss 61.57765579223633\n",
      "13: Encoding Loss 4.354237079620361, Transition Loss -0.9789329767227173, Classifier Loss 0.07701913267374039, Total Loss 33.826942443847656\n",
      "13: Encoding Loss 8.025832176208496, Transition Loss -0.32734376192092896, Classifier Loss 0.19598063826560974, Total Loss 67.7529296875\n",
      "13: Encoding Loss 5.809054374694824, Transition Loss -2.6371359825134277, Classifier Loss 0.160988450050354, Total Loss 50.95211410522461\n",
      "13: Encoding Loss 5.551918983459473, Transition Loss -1.0124776363372803, Classifier Loss 0.11059293150901794, Total Loss 44.37040328979492\n",
      "13: Encoding Loss 5.570915222167969, Transition Loss -1.804329514503479, Classifier Loss 0.13654795289039612, Total Loss 47.079566955566406\n",
      "13: Encoding Loss 3.264660596847534, Transition Loss -1.2693511247634888, Classifier Loss 0.06886406987905502, Total Loss 26.473865509033203\n",
      "13: Encoding Loss 3.740835189819336, Transition Loss -0.3969966769218445, Classifier Loss 0.07566527277231216, Total Loss 30.011381149291992\n",
      "13: Encoding Loss 4.455988883972168, Transition Loss -0.741731584072113, Classifier Loss 0.08241745084524155, Total Loss 34.97738265991211\n",
      "13: Encoding Loss 4.381126403808594, Transition Loss -2.711883068084717, Classifier Loss 0.06822893023490906, Total Loss 33.10857009887695\n",
      "13: Encoding Loss 5.490442276000977, Transition Loss -1.4724254608154297, Classifier Loss 0.11854435503482819, Total Loss 44.79650115966797\n",
      "13: Encoding Loss 4.91465425491333, Transition Loss -1.3048588037490845, Classifier Loss 0.08492004871368408, Total Loss 37.979408264160156\n",
      "13: Encoding Loss 3.48126482963562, Transition Loss -0.26632994413375854, Classifier Loss 0.08211888372898102, Total Loss 29.09937286376953\n",
      "13: Encoding Loss 4.047492027282715, Transition Loss -1.5309364795684814, Classifier Loss 0.04463648423552513, Total Loss 28.747989654541016\n",
      "13: Encoding Loss 5.83250617980957, Transition Loss -1.1961278915405273, Classifier Loss 0.12091971188783646, Total Loss 47.08653259277344\n",
      "13: Encoding Loss 4.918841361999512, Transition Loss 0.16482174396514893, Classifier Loss 0.054063230752944946, Total Loss 34.98530197143555\n",
      "13: Encoding Loss 6.740023612976074, Transition Loss 0.11709898710250854, Classifier Loss 0.0888681560754776, Total Loss 49.37379837036133\n",
      "13: Encoding Loss 6.413706302642822, Transition Loss -1.7134747505187988, Classifier Loss 0.25515827536582947, Total Loss 63.997379302978516\n",
      "13: Encoding Loss 6.097777843475342, Transition Loss -1.6178789138793945, Classifier Loss 0.05085773766040802, Total Loss 41.67179489135742\n",
      "13: Encoding Loss 5.134585857391357, Transition Loss -1.3432421684265137, Classifier Loss 0.11272318661212921, Total Loss 42.07929611206055\n",
      "13: Encoding Loss 4.361532688140869, Transition Loss -1.2343456745147705, Classifier Loss 0.0720047727227211, Total Loss 33.36918258666992\n",
      "13: Encoding Loss 5.457427024841309, Transition Loss -2.122285842895508, Classifier Loss 0.17399859428405762, Total Loss 50.14357376098633\n",
      "13: Encoding Loss 3.5128531455993652, Transition Loss -1.0251113176345825, Classifier Loss 0.05308263376355171, Total Loss 26.384973526000977\n",
      "13: Encoding Loss 6.800447940826416, Transition Loss -2.1421797275543213, Classifier Loss 0.10627324134111404, Total Loss 51.42915344238281\n",
      "13: Encoding Loss 5.675196170806885, Transition Loss -1.4719576835632324, Classifier Loss 0.14203104376792908, Total Loss 48.253692626953125\n",
      "13: Encoding Loss 5.996945381164551, Transition Loss -1.8186012506484985, Classifier Loss 0.04612865671515465, Total Loss 40.59381103515625\n",
      "13: Encoding Loss 6.859923362731934, Transition Loss -0.8393852710723877, Classifier Loss 0.1283685266971588, Total Loss 53.99605941772461\n",
      "13: Encoding Loss 2.4760522842407227, Transition Loss -1.94550359249115, Classifier Loss 0.057133499532938004, Total Loss 20.568885803222656\n",
      "13: Encoding Loss 2.8021559715270996, Transition Loss -0.6006064414978027, Classifier Loss 0.0441238209605217, Total Loss 21.225078582763672\n",
      "13: Encoding Loss 5.526064872741699, Transition Loss -0.44505131244659424, Classifier Loss 0.09071533381938934, Total Loss 42.227745056152344\n",
      "13: Encoding Loss 3.668138027191162, Transition Loss 0.4696335196495056, Classifier Loss 0.07092220336198807, Total Loss 29.288902282714844\n",
      "13: Encoding Loss 5.9536519050598145, Transition Loss -0.9615495204925537, Classifier Loss 0.14403726160526276, Total Loss 50.1252555847168\n",
      "13: Encoding Loss 5.666179656982422, Transition Loss -1.8394142389297485, Classifier Loss 0.05471733212471008, Total Loss 39.468074798583984\n",
      "13: Encoding Loss 3.811995029449463, Transition Loss -2.4879496097564697, Classifier Loss 0.1021358072757721, Total Loss 33.084556579589844\n",
      "13: Encoding Loss 3.0075438022613525, Transition Loss -1.4846296310424805, Classifier Loss 0.06437135487794876, Total Loss 24.4818058013916\n",
      "13: Encoding Loss 7.344018459320068, Transition Loss -1.4251441955566406, Classifier Loss 0.16461825370788574, Total Loss 60.52537155151367\n",
      "13: Encoding Loss 4.1030354499816895, Transition Loss -0.14924483001232147, Classifier Loss 0.16038118302822113, Total Loss 40.656272888183594\n",
      "13: Encoding Loss 4.617423057556152, Transition Loss -1.4570356607437134, Classifier Loss 0.05409800261259079, Total Loss 33.1137580871582\n",
      "13: Encoding Loss 5.261030673980713, Transition Loss 0.13969829678535461, Classifier Loss 0.1354721635580063, Total Loss 45.169281005859375\n",
      "13: Encoding Loss 3.2816646099090576, Transition Loss 0.5510147213935852, Classifier Loss 0.05306541174650192, Total Loss 25.216936111450195\n",
      "13: Encoding Loss 3.7115776538848877, Transition Loss -1.416307806968689, Classifier Loss 0.07274407148361206, Total Loss 29.543306350708008\n",
      "13: Encoding Loss 6.009552955627441, Transition Loss -0.8243850469589233, Classifier Loss 0.089894138276577, Total Loss 45.04640579223633\n",
      "13: Encoding Loss 3.606508731842041, Transition Loss -0.9172881841659546, Classifier Loss 0.11666549742221832, Total Loss 33.30523681640625\n",
      "13: Encoding Loss 6.233050346374512, Transition Loss -1.0148471593856812, Classifier Loss 0.1273205727338791, Total Loss 50.12995910644531\n",
      "13: Encoding Loss 6.757174491882324, Transition Loss -1.2143948078155518, Classifier Loss 0.18373192846775055, Total Loss 58.9157600402832\n",
      "13: Encoding Loss 4.685765266418457, Transition Loss -1.1765879392623901, Classifier Loss 0.026058359071612358, Total Loss 30.71995735168457\n",
      "13: Encoding Loss 4.710360527038574, Transition Loss -0.38833925127983093, Classifier Loss 0.15482865273952484, Total Loss 43.744873046875\n",
      "13: Encoding Loss 5.680804252624512, Transition Loss -3.3427488803863525, Classifier Loss 0.15635603666305542, Total Loss 49.719093322753906\n",
      "13: Encoding Loss 6.731464385986328, Transition Loss -1.5452574491500854, Classifier Loss 0.047904349863529205, Total Loss 45.17860412597656\n",
      "13: Encoding Loss 5.029415130615234, Transition Loss -1.2381535768508911, Classifier Loss 0.058004528284072876, Total Loss 35.97644805908203\n",
      "13: Encoding Loss 6.108975410461426, Transition Loss -2.193709373474121, Classifier Loss 0.13709884881973267, Total Loss 50.36286163330078\n",
      "13: Encoding Loss 5.156249046325684, Transition Loss -0.4048507511615753, Classifier Loss 0.05822756141424179, Total Loss 36.760093688964844\n",
      "13: Encoding Loss 2.96297025680542, Transition Loss -1.534010648727417, Classifier Loss 0.06963279843330383, Total Loss 24.740488052368164\n",
      "13: Encoding Loss 5.029891014099121, Transition Loss -1.858976125717163, Classifier Loss 0.10628281533718109, Total Loss 40.806884765625\n",
      "13: Encoding Loss 9.218250274658203, Transition Loss -3.1776461601257324, Classifier Loss 0.15525981783866882, Total Loss 70.83421325683594\n",
      "13: Encoding Loss 5.002591133117676, Transition Loss -1.3475085496902466, Classifier Loss 0.04583244025707245, Total Loss 34.59825134277344\n",
      "13: Encoding Loss 5.245123863220215, Transition Loss -0.9295632839202881, Classifier Loss 0.04055625572800636, Total Loss 35.5260009765625\n",
      "13: Encoding Loss 5.946835041046143, Transition Loss -0.19772431254386902, Classifier Loss 0.15357953310012817, Total Loss 51.038883209228516\n",
      "13: Encoding Loss 6.019967555999756, Transition Loss -0.7734584808349609, Classifier Loss 0.2615501880645752, Total Loss 62.27451705932617\n",
      "13: Encoding Loss 6.183986186981201, Transition Loss -1.5447956323623657, Classifier Loss 0.17468728125095367, Total Loss 54.57202911376953\n",
      "13: Encoding Loss 6.427232265472412, Transition Loss -2.0676932334899902, Classifier Loss 0.15011075139045715, Total Loss 53.57364273071289\n",
      "13: Encoding Loss 5.302063941955566, Transition Loss -1.3878893852233887, Classifier Loss 0.10636892914772034, Total Loss 42.4487190246582\n",
      "13: Encoding Loss 5.297791481018066, Transition Loss -0.4954930543899536, Classifier Loss 0.1307794451713562, Total Loss 44.86449432373047\n",
      "13: Encoding Loss 6.741860389709473, Transition Loss -0.7355843782424927, Classifier Loss 0.16351646184921265, Total Loss 56.80251693725586\n",
      "13: Encoding Loss 4.693483829498291, Transition Loss -0.504758358001709, Classifier Loss 0.05192170292139053, Total Loss 33.35287094116211\n",
      "13: Encoding Loss 7.5298614501953125, Transition Loss -1.9639701843261719, Classifier Loss 0.06746330112218857, Total Loss 51.924713134765625\n",
      "13: Encoding Loss 4.521919250488281, Transition Loss -0.963905930519104, Classifier Loss 0.09723891317844391, Total Loss 36.85502243041992\n",
      "13: Encoding Loss 4.168524742126465, Transition Loss -1.6407864093780518, Classifier Loss 0.0811329111456871, Total Loss 33.12378692626953\n",
      "13: Encoding Loss 5.398507118225098, Transition Loss -0.3894088864326477, Classifier Loss 0.1628444343805313, Total Loss 48.675331115722656\n",
      "13: Encoding Loss 4.9138288497924805, Transition Loss -1.3900147676467896, Classifier Loss 0.061974942684173584, Total Loss 35.67991256713867\n",
      "13: Encoding Loss 4.983283996582031, Transition Loss -1.4166935682296753, Classifier Loss 0.12757106125354767, Total Loss 42.65624237060547\n",
      "13: Encoding Loss 10.580683708190918, Transition Loss -2.0980606079101562, Classifier Loss 0.12363076955080032, Total Loss 75.84634399414062\n",
      "13: Encoding Loss 5.4803361892700195, Transition Loss -0.6939579844474792, Classifier Loss 0.057800114154815674, Total Loss 38.66175079345703\n",
      "13: Encoding Loss 6.600512504577637, Transition Loss 0.791982114315033, Classifier Loss 0.06473236531019211, Total Loss 46.393104553222656\n",
      "13: Encoding Loss 4.3575334548950195, Transition Loss -2.202164888381958, Classifier Loss 0.10179102420806885, Total Loss 36.323421478271484\n",
      "13: Encoding Loss 6.011258602142334, Transition Loss -1.478147029876709, Classifier Loss 0.10746128112077713, Total Loss 46.81309127807617\n",
      "13: Encoding Loss 3.6292881965637207, Transition Loss -0.5761361718177795, Classifier Loss 0.0800706148147583, Total Loss 29.782560348510742\n",
      "13: Encoding Loss 5.547558307647705, Transition Loss -1.3194795846939087, Classifier Loss 0.18257851898670197, Total Loss 51.54267883300781\n",
      "13: Encoding Loss 9.120397567749023, Transition Loss -1.2705111503601074, Classifier Loss 0.11426067352294922, Total Loss 66.14794158935547\n",
      "13: Encoding Loss 4.513805389404297, Transition Loss -1.700891137123108, Classifier Loss 0.07960298657417297, Total Loss 35.04245376586914\n",
      "13: Encoding Loss 5.475144386291504, Transition Loss -1.047499418258667, Classifier Loss 0.06793909519910812, Total Loss 39.64435958862305\n",
      "13: Encoding Loss 5.910357475280762, Transition Loss -2.0359108448028564, Classifier Loss 0.0430540069937706, Total Loss 39.7667350769043\n",
      "13: Encoding Loss 5.122314453125, Transition Loss -2.157327890396118, Classifier Loss 0.04172956198453903, Total Loss 34.905982971191406\n",
      "13: Encoding Loss 4.498987674713135, Transition Loss -1.0349433422088623, Classifier Loss 0.028005074709653854, Total Loss 29.79401969909668\n",
      "13: Encoding Loss 4.0953779220581055, Transition Loss -2.4272866249084473, Classifier Loss 0.0938987210392952, Total Loss 33.9611701965332\n",
      "13: Encoding Loss 6.188850402832031, Transition Loss -0.5458096265792847, Classifier Loss 0.10125168412923813, Total Loss 47.258052825927734\n",
      "13: Encoding Loss 5.425595283508301, Transition Loss -0.7241950035095215, Classifier Loss 0.06190810352563858, Total Loss 38.74409484863281\n",
      "13: Encoding Loss 5.401723861694336, Transition Loss -3.041102647781372, Classifier Loss 0.09808725118637085, Total Loss 42.21785354614258\n",
      "13: Encoding Loss 6.641119480133057, Transition Loss -3.045332431793213, Classifier Loss 0.10809410363435745, Total Loss 50.654911041259766\n",
      "13: Encoding Loss 4.41894006729126, Transition Loss 0.07048484683036804, Classifier Loss 0.05783558636903763, Total Loss 32.32539367675781\n",
      "13: Encoding Loss 4.73779296875, Transition Loss -1.873049259185791, Classifier Loss 0.1341906636953354, Total Loss 41.84507751464844\n",
      "13: Encoding Loss 4.984213829040527, Transition Loss -1.4011473655700684, Classifier Loss 0.06739865988492966, Total Loss 36.644588470458984\n",
      "13: Encoding Loss 3.627058744430542, Transition Loss -1.854756474494934, Classifier Loss 0.06030457466840744, Total Loss 27.792070388793945\n",
      "13: Encoding Loss 4.941123008728027, Transition Loss -2.728717088699341, Classifier Loss 0.1306195706129074, Total Loss 42.70760726928711\n",
      "13: Encoding Loss 6.350179672241211, Transition Loss -2.1001803874969482, Classifier Loss 0.08834031224250793, Total Loss 46.934268951416016\n",
      "13: Encoding Loss 6.531731605529785, Transition Loss -2.773864984512329, Classifier Loss 0.10306133329868317, Total Loss 49.49541473388672\n",
      "13: Encoding Loss 7.152112007141113, Transition Loss -1.1112275123596191, Classifier Loss 0.15058135986328125, Total Loss 57.97036361694336\n",
      "13: Encoding Loss 5.092127799987793, Transition Loss 0.43569111824035645, Classifier Loss 0.09024646878242493, Total Loss 39.75169372558594\n",
      "13: Encoding Loss 4.78183650970459, Transition Loss -2.582007646560669, Classifier Loss 0.0694555938243866, Total Loss 35.63554763793945\n",
      "13: Encoding Loss 5.008431434631348, Transition Loss -1.7345021963119507, Classifier Loss 0.09814325720071793, Total Loss 39.864219665527344\n",
      "13: Encoding Loss 4.340554714202881, Transition Loss -1.6806036233901978, Classifier Loss 0.0745023638010025, Total Loss 33.49289321899414\n",
      "13: Encoding Loss 4.073836326599121, Transition Loss -1.9103363752365112, Classifier Loss 0.09129144251346588, Total Loss 33.57140350341797\n",
      "13: Encoding Loss 3.4631075859069824, Transition Loss -2.1621246337890625, Classifier Loss 0.04645829647779465, Total Loss 25.423612594604492\n",
      "13: Encoding Loss 3.498114824295044, Transition Loss -0.6950281858444214, Classifier Loss 0.09605055302381516, Total Loss 30.59346580505371\n",
      "13: Encoding Loss 5.036622524261475, Transition Loss -0.8780438303947449, Classifier Loss 0.11132137477397919, Total Loss 41.351524353027344\n",
      "13: Encoding Loss 6.156267166137695, Transition Loss -0.33408352732658386, Classifier Loss 0.09691508859395981, Total Loss 46.62897872924805\n",
      "13: Encoding Loss 4.0703020095825195, Transition Loss -0.5871870517730713, Classifier Loss 0.09561342000961304, Total Loss 33.98291778564453\n",
      "13: Encoding Loss 6.907690525054932, Transition Loss -1.163552165031433, Classifier Loss 0.10699829459190369, Total Loss 52.1455078125\n",
      "13: Encoding Loss 6.436626434326172, Transition Loss -0.7555074095726013, Classifier Loss 0.11315762251615524, Total Loss 49.935218811035156\n",
      "13: Encoding Loss 4.637115955352783, Transition Loss -1.6720683574676514, Classifier Loss 0.12857499718666077, Total Loss 40.67953109741211\n",
      "13: Encoding Loss 5.067068576812744, Transition Loss -0.5624061822891235, Classifier Loss 0.10529810935258865, Total Loss 40.93199920654297\n",
      "13: Encoding Loss 7.528575897216797, Transition Loss -2.132808208465576, Classifier Loss 0.15034490823745728, Total Loss 60.20509338378906\n",
      "13: Encoding Loss 4.8356547355651855, Transition Loss -1.7593793869018555, Classifier Loss 0.0433611199259758, Total Loss 33.34933853149414\n",
      "13: Encoding Loss 6.750560760498047, Transition Loss -0.9017921686172485, Classifier Loss 0.17908897995948792, Total Loss 58.41189956665039\n",
      "13: Encoding Loss 5.662699222564697, Transition Loss -0.4376918077468872, Classifier Loss 0.12290950119495392, Total Loss 46.266971588134766\n",
      "13: Encoding Loss 4.925868988037109, Transition Loss -1.7436602115631104, Classifier Loss 0.10867083817720413, Total Loss 40.421600341796875\n",
      "13: Encoding Loss 4.261384010314941, Transition Loss -2.6501731872558594, Classifier Loss 0.12873674929141998, Total Loss 38.44091796875\n",
      "13: Encoding Loss 6.492049217224121, Transition Loss -2.0953283309936523, Classifier Loss 0.12317456305027008, Total Loss 51.26891326904297\n",
      "13: Encoding Loss 4.290471076965332, Transition Loss -0.7773061990737915, Classifier Loss 0.07937444746494293, Total Loss 33.67995834350586\n",
      "13: Encoding Loss 2.9746198654174805, Transition Loss -1.1850662231445312, Classifier Loss 0.06324824690818787, Total Loss 24.172069549560547\n",
      "13: Encoding Loss 2.441762924194336, Transition Loss -1.6527587175369263, Classifier Loss 0.06731396168470383, Total Loss 21.38131332397461\n",
      "13: Encoding Loss 3.6003754138946533, Transition Loss -1.380493402481079, Classifier Loss 0.08420928567647934, Total Loss 30.022628784179688\n",
      "13: Encoding Loss 9.098767280578613, Transition Loss 0.7420935034751892, Classifier Loss 0.11094466596841812, Total Loss 65.9839096069336\n",
      "13: Encoding Loss 5.716721534729004, Transition Loss 0.19398635625839233, Classifier Loss 0.14318417012691498, Total Loss 48.69634246826172\n",
      "13: Encoding Loss 4.139725685119629, Transition Loss 0.6619511842727661, Classifier Loss 0.06099819764494896, Total Loss 31.202953338623047\n",
      "13: Encoding Loss 7.577797889709473, Transition Loss -0.4526543617248535, Classifier Loss 0.23774565756320953, Total Loss 69.24117279052734\n",
      "13: Encoding Loss 4.4775285720825195, Transition Loss -1.5858988761901855, Classifier Loss 0.05188325047492981, Total Loss 32.05286407470703\n",
      "13: Encoding Loss 4.713925361633301, Transition Loss -2.1752583980560303, Classifier Loss 0.10250089317560196, Total Loss 38.532772064208984\n",
      "13: Encoding Loss 5.660518169403076, Transition Loss -1.1783969402313232, Classifier Loss 0.12840011715888977, Total Loss 46.802650451660156\n",
      "13: Encoding Loss 5.019911766052246, Transition Loss -1.3822740316390991, Classifier Loss 0.061744995415210724, Total Loss 36.293418884277344\n",
      "13: Encoding Loss 4.55306339263916, Transition Loss -0.7100116014480591, Classifier Loss 0.05444575846195221, Total Loss 32.762672424316406\n",
      "13: Encoding Loss 5.929068565368652, Transition Loss -0.7720816135406494, Classifier Loss 0.0943523570895195, Total Loss 45.009342193603516\n",
      "13: Encoding Loss 3.4143424034118652, Transition Loss 0.17923495173454285, Classifier Loss 0.04169905185699463, Total Loss 24.72765350341797\n",
      "13: Encoding Loss 7.0041327476501465, Transition Loss 0.5821424722671509, Classifier Loss 0.07931078970432281, Total Loss 50.18873596191406\n",
      "13: Encoding Loss 5.317486763000488, Transition Loss -1.8319816589355469, Classifier Loss 0.1000666692852974, Total Loss 41.91085433959961\n",
      "13: Encoding Loss 3.9721922874450684, Transition Loss -1.5454661846160889, Classifier Loss 0.12702465057373047, Total Loss 36.535003662109375\n",
      "13: Encoding Loss 6.431204319000244, Transition Loss -1.9387024641036987, Classifier Loss 0.05178625136613846, Total Loss 43.765079498291016\n",
      "13: Encoding Loss 3.564352035522461, Transition Loss -1.0547966957092285, Classifier Loss 0.1088932603597641, Total Loss 32.2750129699707\n",
      "13: Encoding Loss 2.2243614196777344, Transition Loss -1.649690866470337, Classifier Loss 0.0814121663570404, Total Loss 21.486724853515625\n",
      "13: Encoding Loss 2.2546589374542236, Transition Loss -1.0728991031646729, Classifier Loss 0.049109142273664474, Total Loss 18.438440322875977\n",
      "13: Encoding Loss 8.40560531616211, Transition Loss -2.0888471603393555, Classifier Loss 0.10906394571065903, Total Loss 61.339195251464844\n",
      "13: Encoding Loss 4.123410701751709, Transition Loss -1.7014434337615967, Classifier Loss 0.04962526261806488, Total Loss 29.70231056213379\n",
      "13: Encoding Loss 2.4340288639068604, Transition Loss -1.941719889640808, Classifier Loss 0.06855776160955429, Total Loss 21.45917320251465\n",
      "13: Encoding Loss 6.061643123626709, Transition Loss -1.2692209482192993, Classifier Loss 0.12663091719150543, Total Loss 49.03244400024414\n",
      "13: Encoding Loss 3.014432907104492, Transition Loss -0.9007415175437927, Classifier Loss 0.05346446856856346, Total Loss 23.43268394470215\n",
      "13: Encoding Loss 5.487244606018066, Transition Loss -0.6540255546569824, Classifier Loss 0.09138273447751999, Total Loss 42.06148147583008\n",
      "13: Encoding Loss 5.5422258377075195, Transition Loss -0.48065805435180664, Classifier Loss 0.06632857769727707, Total Loss 39.886024475097656\n",
      "13: Encoding Loss 5.223690986633301, Transition Loss -2.763089656829834, Classifier Loss 0.1135539636015892, Total Loss 42.696434020996094\n",
      "13: Encoding Loss 5.247136116027832, Transition Loss -0.41415172815322876, Classifier Loss 0.11323336511850357, Total Loss 42.80598831176758\n",
      "13: Encoding Loss 5.292347431182861, Transition Loss -1.5600831508636475, Classifier Loss 0.15217410027980804, Total Loss 46.97087097167969\n",
      "13: Encoding Loss 4.556467056274414, Transition Loss -2.5788772106170654, Classifier Loss 0.07337646186351776, Total Loss 34.675418853759766\n",
      "13: Encoding Loss 3.520777463912964, Transition Loss -1.195820927619934, Classifier Loss 0.03768119215965271, Total Loss 24.89230728149414\n",
      "13: Encoding Loss 7.4580888748168945, Transition Loss -1.141269564628601, Classifier Loss 0.13710954785346985, Total Loss 58.45903015136719\n",
      "13: Encoding Loss 6.526731967926025, Transition Loss -0.9098135232925415, Classifier Loss 0.14323702454566956, Total Loss 53.483734130859375\n",
      "13: Encoding Loss 5.2457098960876465, Transition Loss -1.2057312726974487, Classifier Loss 0.13314805924892426, Total Loss 44.7885856628418\n",
      "13: Encoding Loss 3.6965160369873047, Transition Loss -1.7691415548324585, Classifier Loss 0.05846019089221954, Total Loss 28.0244083404541\n",
      "13: Encoding Loss 9.4889554977417, Transition Loss -1.1218910217285156, Classifier Loss 0.17211148142814636, Total Loss 74.1444320678711\n",
      "13: Encoding Loss 5.240411758422852, Transition Loss -2.5245344638824463, Classifier Loss 0.05204402655363083, Total Loss 36.6458625793457\n",
      "13: Encoding Loss 4.727950096130371, Transition Loss -1.877171516418457, Classifier Loss 0.13454082608222961, Total Loss 41.8210334777832\n",
      "13: Encoding Loss 6.516484260559082, Transition Loss -2.0319206714630127, Classifier Loss 0.16273316740989685, Total Loss 55.37141036987305\n",
      "13: Encoding Loss 6.5174760818481445, Transition Loss -1.451468586921692, Classifier Loss 0.121255062520504, Total Loss 51.22978591918945\n",
      "13: Encoding Loss 5.375175952911377, Transition Loss -0.6497771739959717, Classifier Loss 0.07947003841400146, Total Loss 40.19779968261719\n",
      "13: Encoding Loss 7.223030090332031, Transition Loss -2.659044027328491, Classifier Loss 0.07827358692884445, Total Loss 51.16447448730469\n",
      "13: Encoding Loss 7.428544521331787, Transition Loss -1.3493151664733887, Classifier Loss 0.1786256730556488, Total Loss 62.43330001831055\n",
      "13: Encoding Loss 7.558640956878662, Transition Loss -2.1782517433166504, Classifier Loss 0.05792238190770149, Total Loss 51.143218994140625\n",
      "13: Encoding Loss 6.5364556312561035, Transition Loss -0.5634441375732422, Classifier Loss 0.19830229878425598, Total Loss 59.04874038696289\n",
      "13: Encoding Loss 6.0408549308776855, Transition Loss -1.8516148328781128, Classifier Loss 0.0577709898352623, Total Loss 42.02149200439453\n",
      "13: Encoding Loss 5.185343265533447, Transition Loss -1.2995399236679077, Classifier Loss 0.060284573584795, Total Loss 37.13999938964844\n",
      "13: Encoding Loss 4.112883567810059, Transition Loss -2.45689058303833, Classifier Loss 0.05578475445508957, Total Loss 30.254796981811523\n",
      "13: Encoding Loss 4.1942009925842285, Transition Loss -0.9725473523139954, Classifier Loss 0.09352993965148926, Total Loss 34.5178108215332\n",
      "13: Encoding Loss 4.013272285461426, Transition Loss -2.9013772010803223, Classifier Loss 0.06595846265554428, Total Loss 30.674320220947266\n",
      "13: Encoding Loss 7.385401725769043, Transition Loss -0.8461385369300842, Classifier Loss 0.09725259244441986, Total Loss 54.037330627441406\n",
      "13: Encoding Loss 4.272706508636475, Transition Loss -1.717418909072876, Classifier Loss 0.09442531317472458, Total Loss 35.078086853027344\n",
      "13: Encoding Loss 2.901571035385132, Transition Loss -1.2160909175872803, Classifier Loss 0.03542296960949898, Total Loss 20.95123863220215\n",
      "13: Encoding Loss 6.665346622467041, Transition Loss -2.140723705291748, Classifier Loss 0.06785213202238083, Total Loss 46.77643966674805\n",
      "13: Encoding Loss 7.672570705413818, Transition Loss -1.3651893138885498, Classifier Loss 0.07514792680740356, Total Loss 53.54967498779297\n",
      "13: Encoding Loss 5.639936447143555, Transition Loss -0.5346100330352783, Classifier Loss 0.12150532007217407, Total Loss 45.98993682861328\n",
      "13: Encoding Loss 5.890951633453369, Transition Loss -1.6834486722946167, Classifier Loss 0.034610550850629807, Total Loss 38.80609130859375\n",
      "13: Encoding Loss 4.768655300140381, Transition Loss -0.9524916410446167, Classifier Loss 0.09821800142526627, Total Loss 38.433349609375\n",
      "13: Encoding Loss 5.451547145843506, Transition Loss -1.5965547561645508, Classifier Loss 0.08043352514505386, Total Loss 40.75200271606445\n",
      "13: Encoding Loss 5.653787612915039, Transition Loss -1.1154687404632568, Classifier Loss 0.08401546627283096, Total Loss 42.32382583618164\n",
      "13: Encoding Loss 4.127382278442383, Transition Loss -0.8229943513870239, Classifier Loss 0.07314086705446243, Total Loss 32.07805633544922\n",
      "13: Encoding Loss 5.9921464920043945, Transition Loss -1.945023775100708, Classifier Loss 0.14686495065689087, Total Loss 50.63859558105469\n",
      "13: Encoding Loss 3.8128597736358643, Transition Loss -1.0185811519622803, Classifier Loss 0.06592198461294174, Total Loss 29.468948364257812\n",
      "13: Encoding Loss 4.992179870605469, Transition Loss -0.37766045331954956, Classifier Loss 0.03811931610107422, Total Loss 33.764862060546875\n",
      "13: Encoding Loss 7.117877006530762, Transition Loss -1.338976263999939, Classifier Loss 0.1807655692100525, Total Loss 60.783287048339844\n",
      "13: Encoding Loss 4.878664970397949, Transition Loss -0.8468717336654663, Classifier Loss 0.046385060995817184, Total Loss 33.910160064697266\n",
      "13: Encoding Loss 5.641575813293457, Transition Loss -3.624124050140381, Classifier Loss 0.0645671933889389, Total Loss 40.304725646972656\n",
      "13: Encoding Loss 4.519785404205322, Transition Loss -1.6656075716018677, Classifier Loss 0.04484693333506584, Total Loss 31.602741241455078\n",
      "13: Encoding Loss 5.321723937988281, Transition Loss -0.28003615140914917, Classifier Loss 0.1523054838180542, Total Loss 47.16078567504883\n",
      "13: Encoding Loss 3.7096030712127686, Transition Loss -1.3058445453643799, Classifier Loss 0.08681675046682358, Total Loss 30.938772201538086\n",
      "13: Encoding Loss 7.742392539978027, Transition Loss -1.8935188055038452, Classifier Loss 0.14904749393463135, Total Loss 61.35834884643555\n",
      "13: Encoding Loss 4.757014751434326, Transition Loss -0.7071840763092041, Classifier Loss 0.050058409571647644, Total Loss 33.54764938354492\n",
      "13: Encoding Loss 5.041332244873047, Transition Loss -2.548186779022217, Classifier Loss 0.0924931988120079, Total Loss 39.49629592895508\n",
      "13: Encoding Loss 4.642515182495117, Transition Loss -2.915658712387085, Classifier Loss 0.12528552114963531, Total Loss 40.382476806640625\n",
      "13: Encoding Loss 4.810842037200928, Transition Loss -1.5422114133834839, Classifier Loss 0.11590155959129333, Total Loss 40.45458984375\n",
      "13: Encoding Loss 5.759801864624023, Transition Loss -1.261099100112915, Classifier Loss 0.08422697335481644, Total Loss 42.98100280761719\n",
      "13: Encoding Loss 5.1352925300598145, Transition Loss -1.9040236473083496, Classifier Loss 0.15678560733795166, Total Loss 46.48955535888672\n",
      "13: Encoding Loss 6.03592586517334, Transition Loss -1.289302110671997, Classifier Loss 0.03083846904337406, Total Loss 39.29888916015625\n",
      "13: Encoding Loss 4.755026340484619, Transition Loss 0.4370966851711273, Classifier Loss 0.05318804830312729, Total Loss 34.0238037109375\n",
      "13: Encoding Loss 5.60632848739624, Transition Loss -1.8722666501998901, Classifier Loss 0.04740654677152634, Total Loss 38.37788009643555\n",
      "13: Encoding Loss 4.227292537689209, Transition Loss -1.5484156608581543, Classifier Loss 0.0631788894534111, Total Loss 31.6810245513916\n",
      "13: Encoding Loss 6.2432026863098145, Transition Loss -0.42131197452545166, Classifier Loss 0.10216310620307922, Total Loss 47.67536163330078\n",
      "13: Encoding Loss 4.444832801818848, Transition Loss -2.3181891441345215, Classifier Loss 0.035995882004499435, Total Loss 30.267658233642578\n",
      "13: Encoding Loss 7.222127437591553, Transition Loss -1.595874309539795, Classifier Loss 0.20761188864707947, Total Loss 64.09331512451172\n",
      "13: Encoding Loss 5.5888872146606445, Transition Loss -1.1594815254211426, Classifier Loss 0.12058894336223602, Total Loss 45.59175491333008\n",
      "13: Encoding Loss 5.203571319580078, Transition Loss -1.8285846710205078, Classifier Loss 0.055327218025922775, Total Loss 36.75341796875\n",
      "13: Encoding Loss 4.993813514709473, Transition Loss 0.014587432146072388, Classifier Loss 0.0631064847111702, Total Loss 36.27936553955078\n",
      "13: Encoding Loss 5.313127040863037, Transition Loss -1.133821725845337, Classifier Loss 0.05585077777504921, Total Loss 37.46338653564453\n",
      "13: Encoding Loss 5.359371185302734, Transition Loss -0.30301058292388916, Classifier Loss 0.11771228164434433, Total Loss 43.92733383178711\n",
      "13: Encoding Loss 7.170223236083984, Transition Loss -0.38961702585220337, Classifier Loss 0.04340270906686783, Total Loss 47.361454010009766\n",
      "13: Encoding Loss 4.556065082550049, Transition Loss -1.7493995428085327, Classifier Loss 0.1715327352285385, Total Loss 44.48896408081055\n",
      "13: Encoding Loss 5.190593242645264, Transition Loss -0.33361518383026123, Classifier Loss 0.07188780605792999, Total Loss 38.33220672607422\n",
      "13: Encoding Loss 4.21173095703125, Transition Loss -0.4642826020717621, Classifier Loss 0.09401968121528625, Total Loss 34.67216873168945\n",
      "13: Encoding Loss 4.6590986251831055, Transition Loss -1.7750810384750366, Classifier Loss 0.03678262233734131, Total Loss 31.63214683532715\n",
      "13: Encoding Loss 4.863882541656494, Transition Loss -2.0269594192504883, Classifier Loss 0.1695726364850998, Total Loss 46.139747619628906\n",
      "13: Encoding Loss 4.8845930099487305, Transition Loss -2.0807979106903076, Classifier Loss 0.13602621853351593, Total Loss 42.90935134887695\n",
      "13: Encoding Loss 3.6906332969665527, Transition Loss -2.8170812129974365, Classifier Loss 0.06902564316987991, Total Loss 29.045238494873047\n",
      "13: Encoding Loss 4.009871482849121, Transition Loss -0.572053074836731, Classifier Loss 0.12352152913808823, Total Loss 36.411155700683594\n",
      "13: Encoding Loss 5.654273986816406, Transition Loss -0.5121915340423584, Classifier Loss 0.12125451117753983, Total Loss 46.05088806152344\n",
      "13: Encoding Loss 3.4045321941375732, Transition Loss -2.210139036178589, Classifier Loss 0.054440997540950775, Total Loss 25.870410919189453\n",
      "13: Encoding Loss 5.2449846267700195, Transition Loss 0.19286543130874634, Classifier Loss 0.030111044645309448, Total Loss 34.55815505981445\n",
      "13: Encoding Loss 5.604608058929443, Transition Loss -2.1676840782165527, Classifier Loss 0.08056081086397171, Total Loss 41.682865142822266\n",
      "13: Encoding Loss 4.180515289306641, Transition Loss -0.8606815934181213, Classifier Loss 0.06201041117310524, Total Loss 31.283790588378906\n",
      "13: Encoding Loss 7.908873081207275, Transition Loss 0.5510114431381226, Classifier Loss 0.22998371720314026, Total Loss 70.6720199584961\n",
      "13: Encoding Loss 5.650537014007568, Transition Loss -1.5930567979812622, Classifier Loss 0.05601457878947258, Total Loss 39.50404739379883\n",
      "13: Encoding Loss 7.074527740478516, Transition Loss 0.4050631523132324, Classifier Loss 0.06871743500232697, Total Loss 49.480934143066406\n",
      "13: Encoding Loss 5.097487449645996, Transition Loss -1.7030225992202759, Classifier Loss 0.1598093956708908, Total Loss 46.565181732177734\n",
      "13: Encoding Loss 3.73472261428833, Transition Loss -1.1601011753082275, Classifier Loss 0.08213873952627182, Total Loss 30.621747970581055\n",
      "13: Encoding Loss 6.437746524810791, Transition Loss -1.6575366258621216, Classifier Loss 0.11250224709510803, Total Loss 49.876041412353516\n",
      "13: Encoding Loss 8.663744926452637, Transition Loss -2.815164089202881, Classifier Loss 0.17190907895565033, Total Loss 69.17224884033203\n",
      "13: Encoding Loss 6.986863136291504, Transition Loss -1.4467021226882935, Classifier Loss 0.27206316590309143, Total Loss 69.12691497802734\n",
      "13: Encoding Loss 4.188023567199707, Transition Loss -1.4162604808807373, Classifier Loss 0.10588722676038742, Total Loss 35.7162971496582\n",
      "13: Encoding Loss 7.536484718322754, Transition Loss 0.2626722753047943, Classifier Loss 0.08350865542888641, Total Loss 53.674842834472656\n",
      "13: Encoding Loss 9.399003982543945, Transition Loss -1.6145434379577637, Classifier Loss 0.3049236536026001, Total Loss 86.8857421875\n",
      "13: Encoding Loss 6.503978729248047, Transition Loss -0.895724892616272, Classifier Loss 0.11225613951683044, Total Loss 50.24912643432617\n",
      "13: Encoding Loss 7.236799716949463, Transition Loss -0.2581730782985687, Classifier Loss 0.12430054694414139, Total Loss 55.85074996948242\n",
      "13: Encoding Loss 7.216897487640381, Transition Loss -0.49966299533843994, Classifier Loss 0.09204498678445816, Total Loss 52.50568771362305\n",
      "13: Encoding Loss 5.339568614959717, Transition Loss -2.052712917327881, Classifier Loss 0.09347762167453766, Total Loss 41.38435745239258\n",
      "13: Encoding Loss 5.010565280914307, Transition Loss -1.854815125465393, Classifier Loss 0.11862288415431976, Total Loss 41.92494201660156\n",
      "13: Encoding Loss 5.999863624572754, Transition Loss -3.340768337249756, Classifier Loss 0.13192351162433624, Total Loss 49.19020080566406\n",
      "13: Encoding Loss 4.744104385375977, Transition Loss -2.2151923179626465, Classifier Loss 0.06925192475318909, Total Loss 35.38893508911133\n",
      "13: Encoding Loss 4.390069007873535, Transition Loss -1.2749050855636597, Classifier Loss 0.05060046538710594, Total Loss 31.399951934814453\n",
      "13: Encoding Loss 5.056460380554199, Transition Loss -1.7574502229690552, Classifier Loss 0.16387087106704712, Total Loss 46.72515106201172\n",
      "13: Encoding Loss 3.13036847114563, Transition Loss -1.8745148181915283, Classifier Loss 0.08246548473834991, Total Loss 27.02800941467285\n",
      "13: Encoding Loss 4.11161470413208, Transition Loss -1.4412834644317627, Classifier Loss 0.11193589121103287, Total Loss 35.862701416015625\n",
      "13: Encoding Loss 6.3813958168029785, Transition Loss -1.6022313833236694, Classifier Loss 0.19211408495903015, Total Loss 57.4991455078125\n",
      "13: Encoding Loss 5.102541923522949, Transition Loss -0.395369291305542, Classifier Loss 0.14907103776931763, Total Loss 45.52220153808594\n",
      "13: Encoding Loss 4.015445709228516, Transition Loss -1.8943214416503906, Classifier Loss 0.11501830071210861, Total Loss 35.593746185302734\n",
      "13: Encoding Loss 5.357412815093994, Transition Loss -2.1803054809570312, Classifier Loss 0.06996648013591766, Total Loss 39.14025115966797\n",
      "13: Encoding Loss 3.749401807785034, Transition Loss -0.6069310903549194, Classifier Loss 0.0473567396402359, Total Loss 27.231843948364258\n",
      "13: Encoding Loss 5.607985496520996, Transition Loss -1.302410364151001, Classifier Loss 0.03462865203619003, Total Loss 37.11025619506836\n",
      "13: Encoding Loss 4.327457904815674, Transition Loss -1.3547184467315674, Classifier Loss 0.04453352466225624, Total Loss 30.417558670043945\n",
      "13: Encoding Loss 3.811051845550537, Transition Loss -0.5088576078414917, Classifier Loss 0.11559466272592545, Total Loss 34.425575256347656\n",
      "13: Encoding Loss 5.723125457763672, Transition Loss -0.7025510668754578, Classifier Loss 0.13150060176849365, Total Loss 47.48853302001953\n",
      "13: Encoding Loss 6.8541669845581055, Transition Loss -0.09963423013687134, Classifier Loss 0.12936189770698547, Total Loss 54.0611572265625\n",
      "13: Encoding Loss 6.594305992126465, Transition Loss -0.72447669506073, Classifier Loss 0.10029585659503937, Total Loss 49.59513473510742\n",
      "13: Encoding Loss 2.989927053451538, Transition Loss -2.261298179626465, Classifier Loss 0.08344779908657074, Total Loss 26.28343963623047\n",
      "13: Encoding Loss 7.056589603424072, Transition Loss -1.3776849508285522, Classifier Loss 0.14982402324676514, Total Loss 57.32139205932617\n",
      "13: Encoding Loss 4.356074333190918, Transition Loss -2.4889535903930664, Classifier Loss 0.09718570858240128, Total Loss 35.85402297973633\n",
      "13: Encoding Loss 5.776134490966797, Transition Loss -0.988815188407898, Classifier Loss 0.0835755467414856, Total Loss 43.01396560668945\n",
      "13: Encoding Loss 7.048683166503906, Transition Loss -1.9747662544250488, Classifier Loss 0.19021502137184143, Total Loss 61.312808990478516\n",
      "13: Encoding Loss 5.49593448638916, Transition Loss -1.9884345531463623, Classifier Loss 0.13927772641181946, Total Loss 46.902584075927734\n",
      "13: Encoding Loss 7.453316688537598, Transition Loss -0.1752559244632721, Classifier Loss 0.13863909244537354, Total Loss 58.583740234375\n",
      "13: Encoding Loss 4.642646312713623, Transition Loss 0.12241953611373901, Classifier Loss 0.0877208411693573, Total Loss 36.67693328857422\n",
      "13: Encoding Loss 4.957457542419434, Transition Loss -1.9078727960586548, Classifier Loss 0.08337640762329102, Total Loss 38.081626892089844\n",
      "13: Encoding Loss 7.026086330413818, Transition Loss -0.7206199169158936, Classifier Loss 0.1267435997724533, Total Loss 54.830589294433594\n",
      "13: Encoding Loss 6.12158727645874, Transition Loss -1.3833013772964478, Classifier Loss 0.1067766547203064, Total Loss 47.406639099121094\n",
      "13: Encoding Loss 5.813906192779541, Transition Loss -0.7760652303695679, Classifier Loss 0.12018458545207977, Total Loss 46.901588439941406\n",
      "13: Encoding Loss 4.129281044006348, Transition Loss 0.15431541204452515, Classifier Loss 0.11625228077173233, Total Loss 36.46263885498047\n",
      "13: Encoding Loss 3.309722423553467, Transition Loss -0.7906903624534607, Classifier Loss 0.04823537915945053, Total Loss 24.681556701660156\n",
      "13: Encoding Loss 5.653687477111816, Transition Loss -1.6956928968429565, Classifier Loss 0.07437112182378769, Total Loss 41.358558654785156\n",
      "13: Encoding Loss 4.657048225402832, Transition Loss -1.2420189380645752, Classifier Loss 0.09045683592557907, Total Loss 36.98747634887695\n",
      "13: Encoding Loss 4.597044467926025, Transition Loss -0.6805515289306641, Classifier Loss 0.18472570180892944, Total Loss 46.054569244384766\n",
      "13: Encoding Loss 4.190738201141357, Transition Loss -2.1622869968414307, Classifier Loss 0.04717971384525299, Total Loss 29.86153793334961\n",
      "13: Encoding Loss 4.49283504486084, Transition Loss -2.6254098415374756, Classifier Loss 0.1194397583603859, Total Loss 38.89993667602539\n",
      "13: Encoding Loss 1.8962371349334717, Transition Loss -0.633098840713501, Classifier Loss 0.050071801990270615, Total Loss 16.384349822998047\n",
      "13: Encoding Loss 4.933064937591553, Transition Loss -0.7268580198287964, Classifier Loss 0.051366087049245834, Total Loss 34.734710693359375\n",
      "13: Encoding Loss 4.546306133270264, Transition Loss -1.498429775238037, Classifier Loss 0.12600156664848328, Total Loss 39.87739562988281\n",
      "13: Encoding Loss 3.849761486053467, Transition Loss -0.667737603187561, Classifier Loss 0.05710337683558464, Total Loss 28.808639526367188\n",
      "13: Encoding Loss 4.6259355545043945, Transition Loss -0.12969480454921722, Classifier Loss 0.029867297038435936, Total Loss 30.742292404174805\n",
      "13: Encoding Loss 7.668618202209473, Transition Loss -2.159231424331665, Classifier Loss 0.1840488314628601, Total Loss 64.41573333740234\n",
      "13: Encoding Loss 4.897782802581787, Transition Loss -0.5688863396644592, Classifier Loss 0.1758720576763153, Total Loss 46.97367477416992\n",
      "13: Encoding Loss 4.974282264709473, Transition Loss -1.1062713861465454, Classifier Loss 0.18734851479530334, Total Loss 48.580101013183594\n",
      "13: Encoding Loss 4.642430305480957, Transition Loss -1.7806164026260376, Classifier Loss 0.07886144518852234, Total Loss 35.740013122558594\n",
      "13: Encoding Loss 6.327435493469238, Transition Loss -1.6459121704101562, Classifier Loss 0.15008145570755005, Total Loss 52.97209930419922\n",
      "13: Encoding Loss 5.461130142211914, Transition Loss -0.9758416414260864, Classifier Loss 0.08460581302642822, Total Loss 41.22697448730469\n",
      "13: Encoding Loss 5.855127811431885, Transition Loss -0.45281270146369934, Classifier Loss 0.057952336966991425, Total Loss 40.92582321166992\n",
      "13: Encoding Loss 4.181884765625, Transition Loss -1.9306691884994507, Classifier Loss 0.09705507010221481, Total Loss 34.79604721069336\n",
      "13: Encoding Loss 6.191140651702881, Transition Loss -0.09264224767684937, Classifier Loss 0.07365766912698746, Total Loss 44.512577056884766\n",
      "13: Encoding Loss 4.688961505889893, Transition Loss -1.585160493850708, Classifier Loss 0.07571243494749069, Total Loss 35.70438003540039\n",
      "13: Encoding Loss 6.003466606140137, Transition Loss -1.3608312606811523, Classifier Loss 0.10281091928482056, Total Loss 46.30134963989258\n",
      "13: Encoding Loss 6.408772945404053, Transition Loss -0.6143472194671631, Classifier Loss 0.07546885311603546, Total Loss 45.99928283691406\n",
      "13: Encoding Loss 4.674827575683594, Transition Loss -1.7363715171813965, Classifier Loss 0.04224550724029541, Total Loss 32.272823333740234\n",
      "13: Encoding Loss 3.9821925163269043, Transition Loss -1.242010235786438, Classifier Loss 0.07558923959732056, Total Loss 31.451583862304688\n",
      "13: Encoding Loss 3.592273235321045, Transition Loss -1.066107153892517, Classifier Loss 0.07568402588367462, Total Loss 29.12161636352539\n",
      "13: Encoding Loss 7.586665153503418, Transition Loss -2.367138624191284, Classifier Loss 0.08194174617528915, Total Loss 53.71322250366211\n",
      "13: Encoding Loss 3.98048734664917, Transition Loss -3.409905433654785, Classifier Loss 0.05923329293727875, Total Loss 29.80489158630371\n",
      "13: Encoding Loss 3.1494696140289307, Transition Loss -1.1550878286361694, Classifier Loss 0.09697458893060684, Total Loss 28.593814849853516\n",
      "13: Encoding Loss 9.663432121276855, Transition Loss 0.38831275701522827, Classifier Loss 0.1654459536075592, Total Loss 74.6805191040039\n",
      "13: Encoding Loss 6.278934955596924, Transition Loss -0.10106360912322998, Classifier Loss 0.12581034004688263, Total Loss 50.25460433959961\n",
      "13: Encoding Loss 4.576076507568359, Transition Loss -2.636836051940918, Classifier Loss 0.057583317160606384, Total Loss 33.213741302490234\n",
      "13: Encoding Loss 4.080531597137451, Transition Loss -0.6441256999969482, Classifier Loss 0.07031815499067307, Total Loss 31.51474952697754\n",
      "13: Encoding Loss 7.058077335357666, Transition Loss -2.551058769226074, Classifier Loss 0.18077656626701355, Total Loss 60.42510223388672\n",
      "13: Encoding Loss 4.673211097717285, Transition Loss -1.1490932703018188, Classifier Loss 0.06332606822252274, Total Loss 34.37141418457031\n",
      "13: Encoding Loss 4.41539192199707, Transition Loss -1.1522108316421509, Classifier Loss 0.06778959184885025, Total Loss 33.270851135253906\n",
      "13: Encoding Loss 4.0880537033081055, Transition Loss -1.5274741649627686, Classifier Loss 0.13123221695423126, Total Loss 37.650936126708984\n",
      "13: Encoding Loss 6.113744735717773, Transition Loss -0.5311708450317383, Classifier Loss 0.03518081083893776, Total Loss 40.20033645629883\n",
      "13: Encoding Loss 4.50493049621582, Transition Loss -1.388486623764038, Classifier Loss 0.06955751031637192, Total Loss 33.984779357910156\n",
      "13: Encoding Loss 6.500174522399902, Transition Loss -1.9541624784469604, Classifier Loss 0.0932142361998558, Total Loss 48.32168960571289\n",
      "13: Encoding Loss 5.360270023345947, Transition Loss -0.5740489363670349, Classifier Loss 0.0641205906867981, Total Loss 38.57345199584961\n",
      "13: Encoding Loss 5.571254730224609, Transition Loss -0.2008817493915558, Classifier Loss 0.129257932305336, Total Loss 46.353240966796875\n",
      "13: Encoding Loss 7.469424247741699, Transition Loss -1.0404154062271118, Classifier Loss 0.04004460573196411, Total Loss 48.82059097290039\n",
      "13: Encoding Loss 5.784203052520752, Transition Loss -1.0344207286834717, Classifier Loss 0.18953406810760498, Total Loss 53.6582145690918\n",
      "13: Encoding Loss 5.1639323234558105, Transition Loss -0.5408684611320496, Classifier Loss 0.09832753241062164, Total Loss 40.816131591796875\n",
      "13: Encoding Loss 3.272838592529297, Transition Loss -0.9393903017044067, Classifier Loss 0.06998959183692932, Total Loss 26.6356143951416\n",
      "13: Encoding Loss 5.115847587585449, Transition Loss -1.4995152950286865, Classifier Loss 0.07939821481704712, Total Loss 38.63431167602539\n",
      "13: Encoding Loss 4.949070930480957, Transition Loss -0.3930774927139282, Classifier Loss 0.0781584307551384, Total Loss 37.51011276245117\n",
      "13: Encoding Loss 4.517365455627441, Transition Loss 0.3419143557548523, Classifier Loss 0.09226588904857635, Total Loss 36.46754455566406\n",
      "13: Encoding Loss 4.226308345794678, Transition Loss -1.754706621170044, Classifier Loss 0.07103468477725983, Total Loss 32.46061706542969\n",
      "13: Encoding Loss 3.240360736846924, Transition Loss -2.7625417709350586, Classifier Loss 0.07674019038677216, Total Loss 27.115079879760742\n",
      "13: Encoding Loss 5.748971939086914, Transition Loss -2.0130937099456787, Classifier Loss 0.10635930299758911, Total Loss 45.12895584106445\n",
      "13: Encoding Loss 5.063200950622559, Transition Loss -2.191725730895996, Classifier Loss 0.137271910905838, Total Loss 44.10552215576172\n",
      "13: Encoding Loss 5.246249675750732, Transition Loss 0.057499781250953674, Classifier Loss 0.09094031155109406, Total Loss 40.59452819824219\n",
      "13: Encoding Loss 5.214024543762207, Transition Loss -0.9484215974807739, Classifier Loss 0.13186629116535187, Total Loss 44.47039794921875\n",
      "13: Encoding Loss 6.206834316253662, Transition Loss -0.5548518300056458, Classifier Loss 0.15527494251728058, Total Loss 52.768280029296875\n",
      "13: Encoding Loss 5.469244003295898, Transition Loss -0.5609663128852844, Classifier Loss 0.11966371536254883, Total Loss 44.781612396240234\n",
      "13: Encoding Loss 5.005437850952148, Transition Loss -1.1646562814712524, Classifier Loss 0.09252841770648956, Total Loss 39.285003662109375\n",
      "13: Encoding Loss 5.332676887512207, Transition Loss -2.9834885597229004, Classifier Loss 0.07118229568004608, Total Loss 39.11309814453125\n",
      "13: Encoding Loss 4.501712322235107, Transition Loss -0.7664680480957031, Classifier Loss 0.05233054608106613, Total Loss 32.24302291870117\n",
      "13: Encoding Loss 4.980266571044922, Transition Loss -1.372664213180542, Classifier Loss 0.09606406092643738, Total Loss 39.487457275390625\n",
      "13: Encoding Loss 5.585526466369629, Transition Loss -2.769761085510254, Classifier Loss 0.04822106659412384, Total Loss 38.33415985107422\n",
      "13: Encoding Loss 6.6922502517700195, Transition Loss -0.8232876062393188, Classifier Loss 0.05030118674039841, Total Loss 45.183292388916016\n",
      "13: Encoding Loss 3.044229030609131, Transition Loss -0.7308510541915894, Classifier Loss 0.06589239090681076, Total Loss 24.85432243347168\n",
      "13: Encoding Loss 5.241095066070557, Transition Loss -1.5124506950378418, Classifier Loss 0.03723859414458275, Total Loss 35.169822692871094\n",
      "13: Encoding Loss 6.129106521606445, Transition Loss -1.1731789112091064, Classifier Loss 0.060607124119997025, Total Loss 42.83488082885742\n",
      "13: Encoding Loss 3.662586212158203, Transition Loss -0.5710022449493408, Classifier Loss 0.028247810900211334, Total Loss 24.80006980895996\n",
      "13: Encoding Loss 5.085072994232178, Transition Loss -2.7557284832000732, Classifier Loss 0.1292969286441803, Total Loss 43.439029693603516\n",
      "13: Encoding Loss 6.90725040435791, Transition Loss -0.2898520231246948, Classifier Loss 0.1015409454703331, Total Loss 51.59748458862305\n",
      "13: Encoding Loss 6.772276878356934, Transition Loss -0.8966933488845825, Classifier Loss 0.13799084722995758, Total Loss 54.43238830566406\n",
      "13: Encoding Loss 5.706953048706055, Transition Loss -1.4142314195632935, Classifier Loss 0.053760889917612076, Total Loss 39.61724090576172\n",
      "13: Encoding Loss 5.422545433044434, Transition Loss -1.5541530847549438, Classifier Loss 0.07774082571268082, Total Loss 40.30873489379883\n",
      "13: Encoding Loss 5.272220611572266, Transition Loss -0.7498376369476318, Classifier Loss 0.14432567358016968, Total Loss 46.06559371948242\n",
      "13: Encoding Loss 6.136843204498291, Transition Loss -1.651893138885498, Classifier Loss 0.061289116740226746, Total Loss 42.949310302734375\n",
      "13: Encoding Loss 3.3386425971984863, Transition Loss -1.2162259817123413, Classifier Loss 0.12981407344341278, Total Loss 33.01277542114258\n",
      "13: Encoding Loss 7.939630031585693, Transition Loss 0.44441932439804077, Classifier Loss 0.12089210748672485, Total Loss 59.904762268066406\n",
      "13: Encoding Loss 4.674667835235596, Transition Loss -1.1140403747558594, Classifier Loss 0.037718892097473145, Total Loss 31.81945037841797\n",
      "13: Encoding Loss 3.054211378097534, Transition Loss -2.290440320968628, Classifier Loss 0.0569167397916317, Total Loss 24.016027450561523\n",
      "13: Encoding Loss 5.96005916595459, Transition Loss -0.1735084503889084, Classifier Loss 0.04500223696231842, Total Loss 40.26051330566406\n",
      "13: Encoding Loss 2.541456699371338, Transition Loss -0.9901090860366821, Classifier Loss 0.03534441441297531, Total Loss 18.782785415649414\n",
      "13: Encoding Loss 6.596444129943848, Transition Loss -1.7635951042175293, Classifier Loss 0.11888833343982697, Total Loss 51.466793060302734\n",
      "13: Encoding Loss 5.956703186035156, Transition Loss -1.0226701498031616, Classifier Loss 0.09200556576251984, Total Loss 44.94036865234375\n",
      "13: Encoding Loss 5.038478374481201, Transition Loss -2.118966579437256, Classifier Loss 0.06123990938067436, Total Loss 36.3540153503418\n",
      "13: Encoding Loss 3.311169385910034, Transition Loss -1.0747324228286743, Classifier Loss 0.11742168664932251, Total Loss 31.60875701904297\n",
      "13: Encoding Loss 4.706532955169678, Transition Loss -1.431542158126831, Classifier Loss 0.08599309623241425, Total Loss 36.83793640136719\n",
      "13: Encoding Loss 5.303467750549316, Transition Loss -1.559610366821289, Classifier Loss 0.0791940912604332, Total Loss 39.73958969116211\n",
      "13: Encoding Loss 3.3403713703155518, Transition Loss -0.7729617357254028, Classifier Loss 0.0772562325000763, Total Loss 27.76754379272461\n",
      "13: Encoding Loss 5.754438877105713, Transition Loss -2.068343162536621, Classifier Loss 0.17478002607822418, Total Loss 52.00381088256836\n",
      "13: Encoding Loss 3.982694149017334, Transition Loss -1.9882404804229736, Classifier Loss 0.04984857514500618, Total Loss 28.88022804260254\n",
      "13: Encoding Loss 3.5190510749816895, Transition Loss -0.7442371249198914, Classifier Loss 0.057829905301332474, Total Loss 26.89699935913086\n",
      "13: Encoding Loss 5.232315540313721, Transition Loss 0.018537819385528564, Classifier Loss 0.13517996668815613, Total Loss 44.919307708740234\n",
      "13: Encoding Loss 4.8310956954956055, Transition Loss -1.6814546585083008, Classifier Loss 0.07388786226511002, Total Loss 36.374691009521484\n",
      "13: Encoding Loss 7.749454498291016, Transition Loss -0.5701285600662231, Classifier Loss 0.09310970455408096, Total Loss 55.80746841430664\n",
      "13: Encoding Loss 4.033768653869629, Transition Loss -2.5331192016601562, Classifier Loss 0.1852935254573822, Total Loss 42.73094940185547\n",
      "13: Encoding Loss 5.855631351470947, Transition Loss -1.080601692199707, Classifier Loss 0.08438070118427277, Total Loss 43.57143020629883\n",
      "13: Encoding Loss 4.025534629821777, Transition Loss -1.1788616180419922, Classifier Loss 0.03766266256570816, Total Loss 27.919004440307617\n",
      "13: Encoding Loss 8.53190803527832, Transition Loss -0.19752202928066254, Classifier Loss 0.1400785595178604, Total Loss 65.19922637939453\n",
      "13: Encoding Loss 5.319270133972168, Transition Loss -1.1699376106262207, Classifier Loss 0.12249412387609482, Total Loss 44.16456604003906\n",
      "13: Encoding Loss 4.348954200744629, Transition Loss -1.0774999856948853, Classifier Loss 0.07375190407037735, Total Loss 33.468482971191406\n",
      "13: Encoding Loss 4.705519676208496, Transition Loss -2.760469436645508, Classifier Loss 0.09331973642110825, Total Loss 37.56399154663086\n",
      "13: Encoding Loss 3.44958233833313, Transition Loss 0.7098479270935059, Classifier Loss 0.10418163985013962, Total Loss 31.39959716796875\n",
      "13: Encoding Loss 3.561779022216797, Transition Loss -0.12918609380722046, Classifier Loss 0.04531913623213768, Total Loss 25.902536392211914\n",
      "13: Encoding Loss 4.815604209899902, Transition Loss -2.4370596408843994, Classifier Loss 0.13025660812854767, Total Loss 41.918312072753906\n",
      "13: Encoding Loss 5.6529645919799805, Transition Loss -1.6584752798080444, Classifier Loss 0.11683612316846848, Total Loss 45.600738525390625\n",
      "13: Encoding Loss 4.092536926269531, Transition Loss -0.6906878352165222, Classifier Loss 0.07962338626384735, Total Loss 32.51728820800781\n",
      "13: Encoding Loss 4.377413749694824, Transition Loss -0.5772082805633545, Classifier Loss 0.06596644222736359, Total Loss 32.860897064208984\n",
      "13: Encoding Loss 4.429727554321289, Transition Loss -1.1828596591949463, Classifier Loss 0.032728176563978195, Total Loss 29.850711822509766\n",
      "13: Encoding Loss 6.918954372406006, Transition Loss -0.1898377537727356, Classifier Loss 0.2357671707868576, Total Loss 65.09037017822266\n",
      "13: Encoding Loss 4.282119274139404, Transition Loss -1.188927412033081, Classifier Loss 0.06243978813290596, Total Loss 31.936220169067383\n",
      "13: Encoding Loss 6.690732002258301, Transition Loss -1.5222562551498413, Classifier Loss 0.08018321543931961, Total Loss 48.162105560302734\n",
      "13: Encoding Loss 4.070157051086426, Transition Loss -0.0550805926322937, Classifier Loss 0.05317706614732742, Total Loss 29.73862648010254\n",
      "13: Encoding Loss 5.6183671951293945, Transition Loss -1.5798945426940918, Classifier Loss 0.032556310296058655, Total Loss 36.96520233154297\n",
      "13: Encoding Loss 4.9532294273376465, Transition Loss -0.8802763223648071, Classifier Loss 0.08512863516807556, Total Loss 38.23188781738281\n",
      "13: Encoding Loss 5.058582305908203, Transition Loss -1.3498108386993408, Classifier Loss 0.1975245326757431, Total Loss 50.10340881347656\n",
      "13: Encoding Loss 8.469430923461914, Transition Loss -0.3097723126411438, Classifier Loss 0.20592349767684937, Total Loss 71.4088134765625\n",
      "13: Encoding Loss 6.191781044006348, Transition Loss -1.224940299987793, Classifier Loss 0.06762781739234924, Total Loss 43.91298294067383\n",
      "13: Encoding Loss 6.641732692718506, Transition Loss -1.2413499355316162, Classifier Loss 0.15571066737174988, Total Loss 55.42097091674805\n",
      "13: Encoding Loss 4.834868431091309, Transition Loss -1.8186520338058472, Classifier Loss 0.035352665930986404, Total Loss 32.54375076293945\n",
      "13: Encoding Loss 5.641599655151367, Transition Loss -1.1164608001708984, Classifier Loss 0.06695102155208588, Total Loss 40.544254302978516\n",
      "13: Encoding Loss 6.354729652404785, Transition Loss -1.108655333518982, Classifier Loss 0.08221589773893356, Total Loss 46.349525451660156\n",
      "13: Encoding Loss 4.827445030212402, Transition Loss -1.3007638454437256, Classifier Loss 0.1141328439116478, Total Loss 40.377437591552734\n",
      "13: Encoding Loss 3.3630523681640625, Transition Loss 0.4556393623352051, Classifier Loss 0.0341191291809082, Total Loss 23.772483825683594\n",
      "13: Encoding Loss 3.943493366241455, Transition Loss -0.6212517023086548, Classifier Loss 0.07590599358081818, Total Loss 31.251312255859375\n",
      "13: Encoding Loss 4.407797336578369, Transition Loss 0.3106445074081421, Classifier Loss 0.05989048629999161, Total Loss 32.560089111328125\n",
      "13: Encoding Loss 4.933986663818359, Transition Loss -1.377375602722168, Classifier Loss 0.049912627786397934, Total Loss 34.594635009765625\n",
      "13: Encoding Loss 7.284359931945801, Transition Loss -0.29347020387649536, Classifier Loss 0.18526940047740936, Total Loss 62.23298263549805\n",
      "13: Encoding Loss 5.194387435913086, Transition Loss -1.815577507019043, Classifier Loss 0.07169191539287567, Total Loss 38.33479309082031\n",
      "13: Encoding Loss 8.236796379089355, Transition Loss -2.6900813579559326, Classifier Loss 0.11627693474292755, Total Loss 61.04739761352539\n",
      "13: Encoding Loss 5.7424726486206055, Transition Loss -1.7044768333435059, Classifier Loss 0.06792538613080978, Total Loss 41.2466926574707\n",
      "13: Encoding Loss 3.664454698562622, Transition Loss -1.1468675136566162, Classifier Loss 0.07546041905879974, Total Loss 29.532310485839844\n",
      "13: Encoding Loss 3.901231288909912, Transition Loss -0.4047820568084717, Classifier Loss 0.016397710889577866, Total Loss 25.0469970703125\n",
      "13: Encoding Loss 6.673973083496094, Transition Loss -3.0067272186279297, Classifier Loss 0.22904261946678162, Total Loss 62.9468994140625\n",
      "13: Encoding Loss 5.132904052734375, Transition Loss -1.426684021949768, Classifier Loss 0.12047188729047775, Total Loss 42.84404373168945\n",
      "13: Encoding Loss 3.8550102710723877, Transition Loss 0.8169186115264893, Classifier Loss 0.04299704357981682, Total Loss 27.756534576416016\n",
      "13: Encoding Loss 5.144446849822998, Transition Loss -0.9891509413719177, Classifier Loss 0.22534789144992828, Total Loss 53.40107727050781\n",
      "13: Encoding Loss 3.961674451828003, Transition Loss -1.5572395324707031, Classifier Loss 0.10385116934776306, Total Loss 34.154544830322266\n",
      "13: Encoding Loss 6.287761211395264, Transition Loss -0.7732350826263428, Classifier Loss 0.16379614174365997, Total Loss 54.10587692260742\n",
      "13: Encoding Loss 3.1163687705993652, Transition Loss -0.3658181130886078, Classifier Loss 0.04337393119931221, Total Loss 23.035459518432617\n",
      "13: Encoding Loss 3.1260201930999756, Transition Loss -0.5257134437561035, Classifier Loss 0.15169505774974823, Total Loss 33.925418853759766\n",
      "13: Encoding Loss 5.361562728881836, Transition Loss -2.225935220718384, Classifier Loss 0.10981671512126923, Total Loss 43.1501579284668\n",
      "13: Encoding Loss 4.497544765472412, Transition Loss -1.681705117225647, Classifier Loss 0.16684190928936005, Total Loss 43.66878890991211\n",
      "13: Encoding Loss 5.42882776260376, Transition Loss -2.238741159439087, Classifier Loss 0.10821235179901123, Total Loss 43.393306732177734\n",
      "13: Encoding Loss 5.523275375366211, Transition Loss -1.6329315900802612, Classifier Loss 0.039492376148700714, Total Loss 37.08823776245117\n",
      "13: Encoding Loss 5.528611183166504, Transition Loss -2.667663812637329, Classifier Loss 0.10326080024242401, Total Loss 43.496681213378906\n",
      "13: Encoding Loss 7.369996547698975, Transition Loss -1.109795093536377, Classifier Loss 0.1429978907108307, Total Loss 58.51932907104492\n",
      "13: Encoding Loss 6.00222635269165, Transition Loss -1.7198364734649658, Classifier Loss 0.1374092400074005, Total Loss 49.753597259521484\n",
      "13: Encoding Loss 5.966296195983887, Transition Loss -0.837199330329895, Classifier Loss 0.0783858522772789, Total Loss 43.63602828979492\n",
      "13: Encoding Loss 4.041056156158447, Transition Loss -2.2611284255981445, Classifier Loss 0.07197755575180054, Total Loss 31.44318962097168\n",
      "13: Encoding Loss 6.318924903869629, Transition Loss -1.6298094987869263, Classifier Loss 0.08491694927215576, Total Loss 46.40459442138672\n",
      "13: Encoding Loss 4.4207868576049805, Transition Loss -2.2199807167053223, Classifier Loss 0.06804797053337097, Total Loss 33.32863235473633\n",
      "13: Encoding Loss 5.842829704284668, Transition Loss -1.873437523841858, Classifier Loss 0.041644882410764694, Total Loss 39.22072219848633\n",
      "13: Encoding Loss 4.613073825836182, Transition Loss -1.0052744150161743, Classifier Loss 0.06944053620100021, Total Loss 34.62209701538086\n",
      "13: Encoding Loss 5.3424530029296875, Transition Loss -1.6770002841949463, Classifier Loss 0.1157984659075737, Total Loss 43.63389205932617\n",
      "13: Encoding Loss 3.7196755409240723, Transition Loss 0.17199984192848206, Classifier Loss 0.05148111283779144, Total Loss 27.53496551513672\n",
      "13: Encoding Loss 7.8146796226501465, Transition Loss -1.358328104019165, Classifier Loss 0.17088155448436737, Total Loss 63.97569274902344\n",
      "13: Encoding Loss 8.060362815856934, Transition Loss -2.12937593460083, Classifier Loss 0.14187392592430115, Total Loss 62.54872131347656\n",
      "13: Encoding Loss 7.392672538757324, Transition Loss -1.0443638563156128, Classifier Loss 0.1825544536113739, Total Loss 62.611061096191406\n",
      "13: Encoding Loss 6.018425941467285, Transition Loss -0.2294752597808838, Classifier Loss 0.10461464524269104, Total Loss 46.571929931640625\n",
      "13: Encoding Loss 7.576804161071777, Transition Loss -0.2268354892730713, Classifier Loss 0.22446653246879578, Total Loss 67.90738677978516\n",
      "13: Encoding Loss 4.519370079040527, Transition Loss 0.24991004168987274, Classifier Loss 0.04383370280265808, Total Loss 31.59955596923828\n",
      "13: Encoding Loss 3.7823398113250732, Transition Loss -0.8984271287918091, Classifier Loss 0.08718360215425491, Total Loss 31.41204071044922\n",
      "13: Encoding Loss 2.761086940765381, Transition Loss 0.4399034380912781, Classifier Loss 0.0810304656624794, Total Loss 24.845529556274414\n",
      "13: Encoding Loss 4.897093772888184, Transition Loss -0.6647585034370422, Classifier Loss 0.12117304652929306, Total Loss 41.499603271484375\n",
      "13: Encoding Loss 6.002773284912109, Transition Loss -1.692503571510315, Classifier Loss 0.0916125476360321, Total Loss 45.17721939086914\n",
      "13: Encoding Loss 6.8904290199279785, Transition Loss -1.7000757455825806, Classifier Loss 0.15660695731639862, Total Loss 57.002593994140625\n",
      "13: Encoding Loss 4.211308479309082, Transition Loss -1.773269534111023, Classifier Loss 0.06405378878116608, Total Loss 31.67251968383789\n",
      "13: Encoding Loss 6.208715438842773, Transition Loss -2.3509914875030518, Classifier Loss 0.1796022206544876, Total Loss 55.21157455444336\n",
      "13: Encoding Loss 4.682085037231445, Transition Loss -1.1806318759918213, Classifier Loss 0.061400867998600006, Total Loss 34.23212432861328\n",
      "13: Encoding Loss 4.503568649291992, Transition Loss -1.490427017211914, Classifier Loss 0.10153760015964508, Total Loss 37.17457962036133\n",
      "13: Encoding Loss 4.367016315460205, Transition Loss -3.1824870109558105, Classifier Loss 0.08537966012954712, Total Loss 34.738792419433594\n",
      "13: Encoding Loss 2.6194679737091064, Transition Loss -1.8045525550842285, Classifier Loss 0.057170480489730835, Total Loss 21.433135986328125\n",
      "13: Encoding Loss 5.385392189025879, Transition Loss -1.7174971103668213, Classifier Loss 0.08459850400686264, Total Loss 40.77151870727539\n",
      "13: Encoding Loss 5.374934196472168, Transition Loss -1.9342403411865234, Classifier Loss 0.14450909197330475, Total Loss 46.69974136352539\n",
      "13: Encoding Loss 3.0227644443511963, Transition Loss -1.531550645828247, Classifier Loss 0.06463383883237839, Total Loss 24.5993595123291\n",
      "13: Encoding Loss 6.427052974700928, Transition Loss -2.276278495788574, Classifier Loss 0.0638146623969078, Total Loss 44.942874908447266\n",
      "13: Encoding Loss 4.4145894050598145, Transition Loss -0.8955806493759155, Classifier Loss 0.06826542317867279, Total Loss 33.313720703125\n",
      "13: Encoding Loss 6.198611259460449, Transition Loss -1.1890472173690796, Classifier Loss 0.10160370171070099, Total Loss 47.3515625\n",
      "13: Encoding Loss 6.839191436767578, Transition Loss -1.4906607866287231, Classifier Loss 0.13801489770412445, Total Loss 54.83604431152344\n",
      "13: Encoding Loss 6.312563896179199, Transition Loss -0.8390637636184692, Classifier Loss 0.0903356522321701, Total Loss 46.90861511230469\n",
      "13: Encoding Loss 4.744259834289551, Transition Loss -1.5296452045440674, Classifier Loss 0.11138283461332321, Total Loss 39.603233337402344\n",
      "13: Encoding Loss 5.923832416534424, Transition Loss -1.8483939170837402, Classifier Loss 0.10907018929719925, Total Loss 46.44927215576172\n",
      "13: Encoding Loss 5.180753231048584, Transition Loss -1.5923774242401123, Classifier Loss 0.05690046772360802, Total Loss 36.773929595947266\n",
      "13: Encoding Loss 2.762920379638672, Transition Loss -0.1452559530735016, Classifier Loss 0.03800128772854805, Total Loss 20.377593994140625\n",
      "13: Encoding Loss 7.197778224945068, Transition Loss -0.18304364383220673, Classifier Loss 0.07721323519945145, Total Loss 50.90792465209961\n",
      "13: Encoding Loss 6.5337700843811035, Transition Loss -1.2781846523284912, Classifier Loss 0.05933162569999695, Total Loss 45.13527297973633\n",
      "13: Encoding Loss 6.770267486572266, Transition Loss -1.0133767127990723, Classifier Loss 0.03418527916073799, Total Loss 44.039730072021484\n",
      "13: Encoding Loss 3.9369843006134033, Transition Loss -0.17774157226085663, Classifier Loss 0.052517786622047424, Total Loss 28.873613357543945\n",
      "13: Encoding Loss 4.135888576507568, Transition Loss -2.093147039413452, Classifier Loss 0.031193237751722336, Total Loss 27.933818817138672\n",
      "13: Encoding Loss 3.839406967163086, Transition Loss -1.719928503036499, Classifier Loss 0.13365934789180756, Total Loss 36.40168762207031\n",
      "13: Encoding Loss 5.466324329376221, Transition Loss -0.4334651827812195, Classifier Loss 0.03581947088241577, Total Loss 36.379722595214844\n",
      "13: Encoding Loss 4.571350574493408, Transition Loss -1.102725625038147, Classifier Loss 0.03482149913907051, Total Loss 30.909812927246094\n",
      "13: Encoding Loss 4.2866010665893555, Transition Loss -0.5433804988861084, Classifier Loss 0.04143204540014267, Total Loss 29.862594604492188\n",
      "13: Encoding Loss 3.618670701980591, Transition Loss -0.8532619476318359, Classifier Loss 0.06447417289018631, Total Loss 28.159101486206055\n",
      "13: Encoding Loss 6.05018949508667, Transition Loss -1.4169321060180664, Classifier Loss 0.06629317253828049, Total Loss 42.92988967895508\n",
      "13: Encoding Loss 4.436034202575684, Transition Loss -1.881392002105713, Classifier Loss 0.07082226127386093, Total Loss 33.69768142700195\n",
      "13: Encoding Loss 4.045683860778809, Transition Loss -2.448183059692383, Classifier Loss 0.04576345905661583, Total Loss 28.849472045898438\n",
      "13: Encoding Loss 3.895843982696533, Transition Loss -1.4948735237121582, Classifier Loss 0.08984322845935822, Total Loss 32.358787536621094\n",
      "13: Encoding Loss 5.827518463134766, Transition Loss -0.7590202689170837, Classifier Loss 0.09056809544563293, Total Loss 44.02161407470703\n",
      "13: Encoding Loss 3.4466254711151123, Transition Loss -0.7720545530319214, Classifier Loss 0.048125237226486206, Total Loss 25.49197006225586\n",
      "13: Encoding Loss 4.444167613983154, Transition Loss -1.1338670253753662, Classifier Loss 0.09406694769859314, Total Loss 36.07124710083008\n",
      "13: Encoding Loss 4.652192115783691, Transition Loss -2.822918653488159, Classifier Loss 0.0722401961684227, Total Loss 35.136043548583984\n",
      "13: Encoding Loss 4.05499267578125, Transition Loss -1.0561320781707764, Classifier Loss 0.04233935847878456, Total Loss 28.563472747802734\n",
      "13: Encoding Loss 3.3098113536834717, Transition Loss -0.6702064275741577, Classifier Loss 0.0576033815741539, Total Loss 25.618938446044922\n",
      "13: Encoding Loss 4.596948623657227, Transition Loss -1.6647684574127197, Classifier Loss 0.14183810353279114, Total Loss 41.764835357666016\n",
      "13: Encoding Loss 3.4135193824768066, Transition Loss -2.207204580307007, Classifier Loss 0.07352995872497559, Total Loss 27.83323097229004\n",
      "13: Encoding Loss 3.6756882667541504, Transition Loss -0.8773669600486755, Classifier Loss 0.08565432578325272, Total Loss 30.619213104248047\n",
      "13: Encoding Loss 4.58078670501709, Transition Loss -2.3912179470062256, Classifier Loss 0.05303044989705086, Total Loss 32.786808013916016\n",
      "13: Encoding Loss 5.824883937835693, Transition Loss -1.0232856273651123, Classifier Loss 0.1483955681324005, Total Loss 49.788455963134766\n",
      "13: Encoding Loss 5.347151756286621, Transition Loss -1.5999928712844849, Classifier Loss 0.09214168041944504, Total Loss 41.29644012451172\n",
      "13: Encoding Loss 3.291116714477539, Transition Loss -2.097623825073242, Classifier Loss 0.07803896814584732, Total Loss 27.549758911132812\n",
      "13: Encoding Loss 6.39436149597168, Transition Loss -1.1855686902999878, Classifier Loss 0.17944686114788055, Total Loss 56.31038284301758\n",
      "13: Encoding Loss 6.976470947265625, Transition Loss -1.888474464416504, Classifier Loss 0.22102947533130646, Total Loss 63.96101760864258\n",
      "13: Encoding Loss 7.752169132232666, Transition Loss -1.9055109024047852, Classifier Loss 0.11669855564832687, Total Loss 58.182106018066406\n",
      "13: Encoding Loss 7.197728633880615, Transition Loss -2.2467222213745117, Classifier Loss 0.1207675188779831, Total Loss 55.26222610473633\n",
      "13: Encoding Loss 4.3578338623046875, Transition Loss -2.8558707237243652, Classifier Loss 0.07185124605894089, Total Loss 33.330989837646484\n",
      "13: Encoding Loss 7.1020307540893555, Transition Loss -2.472109317779541, Classifier Loss 0.07536137849092484, Total Loss 50.147335052490234\n",
      "13: Encoding Loss 8.024627685546875, Transition Loss -0.1800410896539688, Classifier Loss 0.12842907011508942, Total Loss 60.990604400634766\n",
      "13: Encoding Loss 6.715392112731934, Transition Loss -2.4841465950012207, Classifier Loss 0.20959430932998657, Total Loss 61.25079345703125\n",
      "13: Encoding Loss 7.613276481628418, Transition Loss -1.9512441158294678, Classifier Loss 0.16170862317085266, Total Loss 61.8497428894043\n",
      "13: Encoding Loss 6.796982765197754, Transition Loss 0.44651228189468384, Classifier Loss 0.051127299666404724, Total Loss 46.07323455810547\n",
      "13: Encoding Loss 5.718017578125, Transition Loss -1.6268235445022583, Classifier Loss 0.028303448110818863, Total Loss 37.13779830932617\n",
      "13: Encoding Loss 3.1146233081817627, Transition Loss -1.2537662982940674, Classifier Loss 0.075806625187397, Total Loss 26.267900466918945\n",
      "13: Encoding Loss 4.106921195983887, Transition Loss -0.6563933491706848, Classifier Loss 0.08033876121044159, Total Loss 32.67514419555664\n",
      "13: Encoding Loss 3.885782241821289, Transition Loss 0.6856024265289307, Classifier Loss 0.0509047731757164, Total Loss 28.679410934448242\n",
      "13: Encoding Loss 2.1940903663635254, Transition Loss -1.2113641500473022, Classifier Loss 0.028989097103476524, Total Loss 16.06296730041504\n",
      "13: Encoding Loss 4.314922332763672, Transition Loss -0.9166085124015808, Classifier Loss 0.15404486656188965, Total Loss 41.29365539550781\n",
      "13: Encoding Loss 3.749019145965576, Transition Loss -0.8774882555007935, Classifier Loss 0.1469302922487259, Total Loss 37.18679428100586\n",
      "13: Encoding Loss 6.869791507720947, Transition Loss -0.0846075564622879, Classifier Loss 0.04870622605085373, Total Loss 46.08933639526367\n",
      "13: Encoding Loss 5.230274677276611, Transition Loss -2.234301805496216, Classifier Loss 0.06734544038772583, Total Loss 38.115299224853516\n",
      "13: Encoding Loss 8.290597915649414, Transition Loss -1.3125683069229126, Classifier Loss 0.27464962005615234, Total Loss 77.20802307128906\n",
      "13: Encoding Loss 5.182501316070557, Transition Loss -2.3835299015045166, Classifier Loss 0.12297111749649048, Total Loss 43.39116668701172\n",
      "13: Encoding Loss 4.742715835571289, Transition Loss -1.4399456977844238, Classifier Loss 0.04592299088835716, Total Loss 33.04801940917969\n",
      "13: Encoding Loss 6.088428020477295, Transition Loss -1.791584849357605, Classifier Loss 0.14904460310935974, Total Loss 51.4343147277832\n",
      "13: Encoding Loss 6.9463791847229, Transition Loss -1.704657793045044, Classifier Loss 0.09659912437200546, Total Loss 51.33750534057617\n",
      "13: Encoding Loss 5.064542770385742, Transition Loss 0.18481042981147766, Classifier Loss 0.12871487438678741, Total Loss 43.332672119140625\n",
      "13: Encoding Loss 4.100774765014648, Transition Loss 0.7092858552932739, Classifier Loss 0.08134728670120239, Total Loss 33.023094177246094\n",
      "13: Encoding Loss 5.033298492431641, Transition Loss -2.064284324645996, Classifier Loss 0.07369894534349442, Total Loss 37.56886291503906\n",
      "13: Encoding Loss 7.056324005126953, Transition Loss -1.4524794816970825, Classifier Loss 0.09810672700405121, Total Loss 52.14803695678711\n",
      "13: Encoding Loss 5.278128147125244, Transition Loss -0.18856434524059296, Classifier Loss 0.13872087001800537, Total Loss 45.54077911376953\n",
      "13: Encoding Loss 3.2707934379577637, Transition Loss -0.4313541650772095, Classifier Loss 0.03972300514578819, Total Loss 23.59688949584961\n",
      "13: Encoding Loss 4.597006797790527, Transition Loss -1.9000558853149414, Classifier Loss 0.13026019930839539, Total Loss 40.607303619384766\n",
      "13: Encoding Loss 3.7961573600769043, Transition Loss 0.8264322280883789, Classifier Loss 0.0792163535952568, Total Loss 31.029151916503906\n",
      "13: Encoding Loss 6.510318756103516, Transition Loss -1.857602596282959, Classifier Loss 0.08776231855154037, Total Loss 47.837398529052734\n",
      "13: Encoding Loss 3.6791884899139404, Transition Loss -2.2173752784729004, Classifier Loss 0.06275065243244171, Total Loss 28.34930992126465\n",
      "13: Encoding Loss 6.405669212341309, Transition Loss -1.2393567562103271, Classifier Loss 0.09631684422492981, Total Loss 48.06520462036133\n",
      "13: Encoding Loss 4.2804341316223145, Transition Loss -2.158435821533203, Classifier Loss 0.05199023336172104, Total Loss 30.880765914916992\n",
      "13: Encoding Loss 5.399305820465088, Transition Loss -1.4228734970092773, Classifier Loss 0.11009715497493744, Total Loss 43.40498352050781\n",
      "13: Encoding Loss 5.14192008972168, Transition Loss -2.0912227630615234, Classifier Loss 0.09752463549375534, Total Loss 40.6031494140625\n",
      "13: Encoding Loss 4.056343078613281, Transition Loss 0.8817669749259949, Classifier Loss 0.08397658914327621, Total Loss 33.08842468261719\n",
      "13: Encoding Loss 2.0697169303894043, Transition Loss -0.8375666737556458, Classifier Loss 0.059066273272037506, Total Loss 18.324594497680664\n",
      "13: Encoding Loss 9.01689624786377, Transition Loss -2.0325703620910645, Classifier Loss 0.10557287186384201, Total Loss 64.65785217285156\n",
      "13: Encoding Loss 5.151593208312988, Transition Loss -1.4029314517974854, Classifier Loss 0.09519529342651367, Total Loss 40.42852783203125\n",
      "13: Encoding Loss 4.647834300994873, Transition Loss -0.8423140645027161, Classifier Loss 0.09229773283004761, Total Loss 37.1164436340332\n",
      "13: Encoding Loss 4.43782901763916, Transition Loss -1.1279451847076416, Classifier Loss 0.047865673899650574, Total Loss 31.413089752197266\n",
      "13: Encoding Loss 3.508808135986328, Transition Loss -1.0450234413146973, Classifier Loss 0.1198972761631012, Total Loss 33.04215621948242\n",
      "13: Encoding Loss 4.90101432800293, Transition Loss -1.6601063013076782, Classifier Loss 0.1403311938047409, Total Loss 43.43854522705078\n",
      "13: Encoding Loss 3.3640353679656982, Transition Loss 0.24935826659202576, Classifier Loss 0.04633861407637596, Total Loss 24.917818069458008\n",
      "13: Encoding Loss 6.260766983032227, Transition Loss -2.338860034942627, Classifier Loss 0.06941606104373932, Total Loss 44.505271911621094\n",
      "13: Encoding Loss 5.543901443481445, Transition Loss -1.1033873558044434, Classifier Loss 0.028608020395040512, Total Loss 36.1237678527832\n",
      "13: Encoding Loss 5.205567836761475, Transition Loss -1.9401781558990479, Classifier Loss 0.06828524172306061, Total Loss 38.0611572265625\n",
      "13: Encoding Loss 4.1215596199035645, Transition Loss -0.8883808255195618, Classifier Loss 0.027599554508924484, Total Loss 27.48896026611328\n",
      "13: Encoding Loss 4.21929407119751, Transition Loss 0.14584152400493622, Classifier Loss 0.13314907252788544, Total Loss 38.68901062011719\n",
      "13: Encoding Loss 5.1548943519592285, Transition Loss -1.0513185262680054, Classifier Loss 0.05372926592826843, Total Loss 36.30187225341797\n",
      "13: Encoding Loss 3.585712432861328, Transition Loss -2.631633996963501, Classifier Loss 0.09191986173391342, Total Loss 30.70520782470703\n",
      "13: Encoding Loss 5.101675987243652, Transition Loss -1.616288185119629, Classifier Loss 0.1884983479976654, Total Loss 49.45924758911133\n",
      "13: Encoding Loss 4.966144561767578, Transition Loss -1.2784371376037598, Classifier Loss 0.09997067600488663, Total Loss 39.793426513671875\n",
      "13: Encoding Loss 7.446223258972168, Transition Loss -1.2512011528015137, Classifier Loss 0.09364630281925201, Total Loss 54.041473388671875\n",
      "13: Encoding Loss 4.677862644195557, Transition Loss -0.49697256088256836, Classifier Loss 0.08252444118261337, Total Loss 36.31942367553711\n",
      "13: Encoding Loss 4.2654829025268555, Transition Loss 0.1119181290268898, Classifier Loss 0.09656885266304016, Total Loss 35.294551849365234\n",
      "13: Encoding Loss 6.77156925201416, Transition Loss -1.3118000030517578, Classifier Loss 0.0853009894490242, Total Loss 49.15898895263672\n",
      "13: Encoding Loss 5.287522792816162, Transition Loss -2.27262544631958, Classifier Loss 0.1288747936487198, Total Loss 44.61170959472656\n",
      "13: Encoding Loss 7.426009178161621, Transition Loss -2.037991762161255, Classifier Loss 0.11948986351490021, Total Loss 56.50422668457031\n",
      "13: Encoding Loss 5.349869251251221, Transition Loss -2.1811888217926025, Classifier Loss 0.05358433723449707, Total Loss 37.4567756652832\n",
      "13: Encoding Loss 3.9038329124450684, Transition Loss -2.2681474685668945, Classifier Loss 0.12478849291801453, Total Loss 35.90093994140625\n",
      "13: Encoding Loss 6.576796054840088, Transition Loss -1.0191150903701782, Classifier Loss 0.08168148249387741, Total Loss 47.628517150878906\n",
      "13: Encoding Loss 8.81326675415039, Transition Loss -1.3700308799743652, Classifier Loss 0.1362302452325821, Total Loss 66.50208282470703\n",
      "13: Encoding Loss 5.310935974121094, Transition Loss -1.4706430435180664, Classifier Loss 0.03972786292433739, Total Loss 35.83781814575195\n",
      "13: Encoding Loss 4.280794620513916, Transition Loss -0.1749940812587738, Classifier Loss 0.05196106433868408, Total Loss 30.88080406188965\n",
      "13: Encoding Loss 3.6307969093322754, Transition Loss -0.9543531537055969, Classifier Loss 0.07813902199268341, Total Loss 29.598302841186523\n",
      "13: Encoding Loss 5.543242931365967, Transition Loss -1.4184969663619995, Classifier Loss 0.06144304573535919, Total Loss 39.4031982421875\n",
      "13: Encoding Loss 3.9612343311309814, Transition Loss -0.998979926109314, Classifier Loss 0.031619515269994736, Total Loss 26.928956985473633\n",
      "13: Encoding Loss 5.966391086578369, Transition Loss -1.9904519319534302, Classifier Loss 0.04176947474479675, Total Loss 39.9744987487793\n",
      "13: Encoding Loss 3.140486240386963, Transition Loss -1.9401922225952148, Classifier Loss 0.08187830448150635, Total Loss 27.029972076416016\n",
      "13: Encoding Loss 8.064449310302734, Transition Loss 0.24407824873924255, Classifier Loss 0.256527304649353, Total Loss 74.1370620727539\n",
      "13: Encoding Loss 4.954775810241699, Transition Loss -1.0868067741394043, Classifier Loss 0.11480144411325455, Total Loss 41.20836639404297\n",
      "13: Encoding Loss 6.113609313964844, Transition Loss -1.1951963901519775, Classifier Loss 0.08880484849214554, Total Loss 45.56166458129883\n",
      "13: Encoding Loss 5.801671981811523, Transition Loss 0.26002931594848633, Classifier Loss 0.08116120100021362, Total Loss 43.0301628112793\n",
      "13: Encoding Loss 6.628186225891113, Transition Loss -0.5249443054199219, Classifier Loss 0.1849067509174347, Total Loss 58.259586334228516\n",
      "13: Encoding Loss 5.978452682495117, Transition Loss -1.3905839920043945, Classifier Loss 0.16119790077209473, Total Loss 51.98994827270508\n",
      "13: Encoding Loss 4.398374557495117, Transition Loss -0.5122334361076355, Classifier Loss 0.08976387232542038, Total Loss 35.366432189941406\n",
      "13: Encoding Loss 4.818163871765137, Transition Loss -2.071042060852051, Classifier Loss 0.11080287396907806, Total Loss 39.98844528198242\n",
      "13: Encoding Loss 4.905356407165527, Transition Loss -0.7335339784622192, Classifier Loss 0.10274885594844818, Total Loss 39.706729888916016\n",
      "13: Encoding Loss 5.697449207305908, Transition Loss -0.34385091066360474, Classifier Loss 0.09447985887527466, Total Loss 43.632545471191406\n",
      "13: Encoding Loss 3.2097480297088623, Transition Loss -0.805708110332489, Classifier Loss 0.052552223205566406, Total Loss 24.513389587402344\n",
      "13: Encoding Loss 3.878528594970703, Transition Loss -0.8216085433959961, Classifier Loss 0.10257872939109802, Total Loss 33.528717041015625\n",
      "13: Encoding Loss 3.706357479095459, Transition Loss -1.892613410949707, Classifier Loss 0.08491266518831253, Total Loss 30.728654861450195\n",
      "13: Encoding Loss 4.816908836364746, Transition Loss -0.6190824508666992, Classifier Loss 0.03651488572359085, Total Loss 32.552696228027344\n",
      "13: Encoding Loss 5.218620777130127, Transition Loss -1.1215429306030273, Classifier Loss 0.1173323318362236, Total Loss 43.04450988769531\n",
      "13: Encoding Loss 3.9354465007781982, Transition Loss -2.257011890411377, Classifier Loss 0.09253907203674316, Total Loss 32.865684509277344\n",
      "13: Encoding Loss 4.75308084487915, Transition Loss -1.3207452297210693, Classifier Loss 0.11745600402355194, Total Loss 40.2635612487793\n",
      "13: Encoding Loss 3.8148300647735596, Transition Loss -0.13462509214878082, Classifier Loss 0.03398846462368965, Total Loss 26.28777313232422\n",
      "13: Encoding Loss 3.482548952102661, Transition Loss -0.26474183797836304, Classifier Loss 0.048285264521837234, Total Loss 25.723712921142578\n",
      "13: Encoding Loss 4.678607940673828, Transition Loss -0.5494489073753357, Classifier Loss 0.18149954080581665, Total Loss 46.22138214111328\n",
      "13: Encoding Loss 5.9951395988464355, Transition Loss -1.652090072631836, Classifier Loss 0.19140301644802094, Total Loss 55.11048126220703\n",
      "13: Encoding Loss 3.8345603942871094, Transition Loss -0.4592410922050476, Classifier Loss 0.11714841425418854, Total Loss 34.722023010253906\n",
      "13: Encoding Loss 6.253537178039551, Transition Loss -1.5457539558410645, Classifier Loss 0.1429905742406845, Total Loss 51.819664001464844\n",
      "13: Encoding Loss 3.6886184215545654, Transition Loss -1.9674047231674194, Classifier Loss 0.06849630922079086, Total Loss 28.980554580688477\n",
      "13: Encoding Loss 5.5399298667907715, Transition Loss -1.6103990077972412, Classifier Loss 0.12336927652359009, Total Loss 45.575862884521484\n",
      "13: Encoding Loss 5.021745204925537, Transition Loss -0.9169639945030212, Classifier Loss 0.034908417612314224, Total Loss 33.620948791503906\n",
      "13: Encoding Loss 3.0951945781707764, Transition Loss -1.2342544794082642, Classifier Loss 0.09199157357215881, Total Loss 27.769832611083984\n",
      "13: Encoding Loss 3.690868854522705, Transition Loss -2.115501880645752, Classifier Loss 0.053497303277254105, Total Loss 27.494098663330078\n",
      "13: Encoding Loss 3.8277697563171387, Transition Loss -0.19967344403266907, Classifier Loss 0.12285542488098145, Total Loss 35.25208282470703\n",
      "13: Encoding Loss 6.767262935638428, Transition Loss -0.6836878657341003, Classifier Loss 0.055617935955524445, Total Loss 46.16510009765625\n",
      "13: Encoding Loss 5.674525737762451, Transition Loss -1.458318829536438, Classifier Loss 0.20202183723449707, Total Loss 54.248756408691406\n",
      "13: Encoding Loss 6.307415008544922, Transition Loss -0.20180033147335052, Classifier Loss 0.13557069003582, Total Loss 51.4014778137207\n",
      "13: Encoding Loss 5.473206520080566, Transition Loss -2.205385684967041, Classifier Loss 0.12987497448921204, Total Loss 45.82585525512695\n",
      "13: Encoding Loss 4.8254923820495605, Transition Loss -0.7894700169563293, Classifier Loss 0.05290118604898453, Total Loss 34.24275588989258\n",
      "13: Encoding Loss 2.926647186279297, Transition Loss -2.3285903930664062, Classifier Loss 0.05247684195637703, Total Loss 22.806636810302734\n",
      "13: Encoding Loss 2.810123920440674, Transition Loss -1.2638864517211914, Classifier Loss 0.056446075439453125, Total Loss 22.504846572875977\n",
      "13: Encoding Loss 3.9775843620300293, Transition Loss -2.5340888500213623, Classifier Loss 0.056002240628004074, Total Loss 29.464717864990234\n",
      "13: Encoding Loss 5.996820449829102, Transition Loss 0.15464338660240173, Classifier Loss 0.0347745344042778, Total Loss 39.52023696899414\n",
      "13: Encoding Loss 5.118667125701904, Transition Loss -0.2265530526638031, Classifier Loss 0.09077826142311096, Total Loss 39.789737701416016\n",
      "13: Encoding Loss 6.685656547546387, Transition Loss -3.352973222732544, Classifier Loss 0.06094341725111008, Total Loss 46.206939697265625\n",
      "13: Encoding Loss 5.332368850708008, Transition Loss -1.808229684829712, Classifier Loss 0.07487093657255173, Total Loss 39.48058319091797\n",
      "13: Encoding Loss 4.23257303237915, Transition Loss 0.09796756505966187, Classifier Loss 0.09632246941328049, Total Loss 35.06687545776367\n",
      "13: Encoding Loss 4.399757385253906, Transition Loss -2.248405694961548, Classifier Loss 0.034664664417505264, Total Loss 29.864112854003906\n",
      "13: Encoding Loss 7.125344276428223, Transition Loss -1.0151708126068115, Classifier Loss 0.10275796800851822, Total Loss 53.02745819091797\n",
      "13: Encoding Loss 5.9027299880981445, Transition Loss -2.58382248878479, Classifier Loss 0.1598091423511505, Total Loss 51.396263122558594\n",
      "13: Encoding Loss 5.659043312072754, Transition Loss -1.5169110298156738, Classifier Loss 0.09545598924160004, Total Loss 43.4992561340332\n",
      "13: Encoding Loss 6.279829025268555, Transition Loss -0.5516901612281799, Classifier Loss 0.15154816210269928, Total Loss 52.83356857299805\n",
      "13: Encoding Loss 6.239850044250488, Transition Loss -1.604008436203003, Classifier Loss 0.022421248257160187, Total Loss 39.68058776855469\n",
      "13: Encoding Loss 5.059638023376465, Transition Loss -1.5495274066925049, Classifier Loss 0.17302776873111725, Total Loss 47.65998840332031\n",
      "13: Encoding Loss 3.944305419921875, Transition Loss -0.6877629160881042, Classifier Loss 0.06094968318939209, Total Loss 29.760526657104492\n",
      "13: Encoding Loss 6.259000301361084, Transition Loss -1.8817875385284424, Classifier Loss 0.12128155678510666, Total Loss 49.6814079284668\n",
      "13: Encoding Loss 5.544211387634277, Transition Loss -0.7262073159217834, Classifier Loss 0.09325775504112244, Total Loss 42.590755462646484\n",
      "13: Encoding Loss 4.674023628234863, Transition Loss -0.4741368293762207, Classifier Loss 0.07213366031646729, Total Loss 35.25731658935547\n",
      "13: Encoding Loss 4.3895182609558105, Transition Loss -1.1027723550796509, Classifier Loss 0.102504663169384, Total Loss 36.587135314941406\n",
      "13: Encoding Loss 4.856274127960205, Transition Loss -0.7060184478759766, Classifier Loss 0.13814802467823029, Total Loss 42.95216369628906\n",
      "13: Encoding Loss 5.426287651062012, Transition Loss -0.44206035137176514, Classifier Loss 0.10824098438024521, Total Loss 43.38165283203125\n",
      "13: Encoding Loss 5.206368446350098, Transition Loss -2.9521079063415527, Classifier Loss 0.05109982192516327, Total Loss 36.34701156616211\n",
      "13: Encoding Loss 2.7336044311523438, Transition Loss -1.7034496068954468, Classifier Loss 0.07722887396812439, Total Loss 24.12383270263672\n",
      "13: Encoding Loss 8.872350692749023, Transition Loss -2.1725449562072754, Classifier Loss 0.1407371610403061, Total Loss 67.30695343017578\n",
      "13: Encoding Loss 5.8651652336120605, Transition Loss -0.2887151837348938, Classifier Loss 0.03307652845978737, Total Loss 38.498531341552734\n",
      "13: Encoding Loss 5.551759719848633, Transition Loss -0.9160519242286682, Classifier Loss 0.1576460301876068, Total Loss 49.07479476928711\n",
      "13: Encoding Loss 4.9807233810424805, Transition Loss -1.0857930183410645, Classifier Loss 0.13146257400512695, Total Loss 43.03016662597656\n",
      "13: Encoding Loss 6.100524425506592, Transition Loss -0.9149007797241211, Classifier Loss 0.0549810416996479, Total Loss 42.100887298583984\n",
      "13: Encoding Loss 6.705777645111084, Transition Loss -1.505321741104126, Classifier Loss 0.26844343543052673, Total Loss 67.07840728759766\n",
      "13: Encoding Loss 4.984248638153076, Transition Loss -0.30448028445243835, Classifier Loss 0.07760690152645111, Total Loss 37.66606140136719\n",
      "13: Encoding Loss 5.437970161437988, Transition Loss 0.4718139171600342, Classifier Loss 0.11807971447706223, Total Loss 44.62451934814453\n",
      "13: Encoding Loss 5.098287582397461, Transition Loss -0.556202232837677, Classifier Loss 0.0874256119132042, Total Loss 39.332069396972656\n",
      "13: Encoding Loss 3.8885109424591064, Transition Loss 0.20368903875350952, Classifier Loss 0.022107992321252823, Total Loss 25.623342514038086\n",
      "13: Encoding Loss 6.630851745605469, Transition Loss -1.3107889890670776, Classifier Loss 0.0719122439622879, Total Loss 46.97581100463867\n",
      "13: Encoding Loss 7.22135066986084, Transition Loss -1.8074958324432373, Classifier Loss 0.19723694026470184, Total Loss 63.05107498168945\n",
      "13: Encoding Loss 4.4598588943481445, Transition Loss -0.975997805595398, Classifier Loss 0.06394375115633011, Total Loss 33.153141021728516\n",
      "13: Encoding Loss 3.803715229034424, Transition Loss -1.94292414188385, Classifier Loss 0.06917675584554672, Total Loss 29.73919105529785\n",
      "13: Encoding Loss 4.20516300201416, Transition Loss -1.315374732017517, Classifier Loss 0.0618913471698761, Total Loss 31.419586181640625\n",
      "13: Encoding Loss 4.860813140869141, Transition Loss -0.30900096893310547, Classifier Loss 0.04260459169745445, Total Loss 33.42521667480469\n",
      "13: Encoding Loss 5.714947700500488, Transition Loss -0.9676545858383179, Classifier Loss 0.15245646238327026, Total Loss 49.534950256347656\n",
      "13: Encoding Loss 3.060412883758545, Transition Loss -0.2957453727722168, Classifier Loss 0.11786879599094391, Total Loss 30.14923858642578\n",
      "13: Encoding Loss 5.831940174102783, Transition Loss -2.0150880813598633, Classifier Loss 0.0771607756614685, Total Loss 42.706912994384766\n",
      "13: Encoding Loss 3.4869728088378906, Transition Loss -1.0560156106948853, Classifier Loss 0.040367186069488525, Total Loss 24.958133697509766\n",
      "13: Encoding Loss 4.6395769119262695, Transition Loss -2.6736233234405518, Classifier Loss 0.037376031279563904, Total Loss 31.57399559020996\n",
      "13: Encoding Loss 1.9617462158203125, Transition Loss -1.5035814046859741, Classifier Loss 0.03127383068203926, Total Loss 14.897258758544922\n",
      "13: Encoding Loss 5.974099636077881, Transition Loss -1.6829140186309814, Classifier Loss 0.1226760670542717, Total Loss 48.111534118652344\n",
      "13: Encoding Loss 4.337037086486816, Transition Loss -1.7613701820373535, Classifier Loss 0.09124206006526947, Total Loss 35.145721435546875\n",
      "13: Encoding Loss 3.9463067054748535, Transition Loss -0.6988592743873596, Classifier Loss 0.06031154468655586, Total Loss 29.708715438842773\n",
      "13: Encoding Loss 5.268074035644531, Transition Loss 0.36961257457733154, Classifier Loss 0.07035095244646072, Total Loss 38.791385650634766\n",
      "13: Encoding Loss 5.97266149520874, Transition Loss -1.7174129486083984, Classifier Loss 0.14703325927257538, Total Loss 50.538612365722656\n",
      "13: Encoding Loss 6.871591091156006, Transition Loss -3.6111016273498535, Classifier Loss 0.13890860974788666, Total Loss 55.11896514892578\n",
      "13: Encoding Loss 4.591755390167236, Transition Loss -1.5930776596069336, Classifier Loss 0.0769554078578949, Total Loss 35.24543762207031\n",
      "13: Encoding Loss 5.269277572631836, Transition Loss -2.155081272125244, Classifier Loss 0.033711280673742294, Total Loss 34.985931396484375\n",
      "13: Encoding Loss 2.746882200241089, Transition Loss -2.6637096405029297, Classifier Loss 0.045624032616615295, Total Loss 21.042631149291992\n",
      "13: Encoding Loss 4.752907752990723, Transition Loss -2.876274585723877, Classifier Loss 0.1628735363483429, Total Loss 44.80364990234375\n",
      "13: Encoding Loss 5.0148396492004395, Transition Loss -0.8097556233406067, Classifier Loss 0.1790323257446289, Total Loss 47.991947174072266\n",
      "13: Encoding Loss 7.3257646560668945, Transition Loss -0.4036540389060974, Classifier Loss 0.08431375026702881, Total Loss 52.38580322265625\n",
      "13: Encoding Loss 4.164886474609375, Transition Loss -1.1615009307861328, Classifier Loss 0.03617454692721367, Total Loss 28.60630989074707\n",
      "13: Encoding Loss 3.94670033454895, Transition Loss -1.6196423768997192, Classifier Loss 0.050270888954401016, Total Loss 28.706642150878906\n",
      "13: Encoding Loss 5.314803123474121, Transition Loss -1.4870378971099854, Classifier Loss 0.029504692181944847, Total Loss 34.83869552612305\n",
      "13: Encoding Loss 4.460317134857178, Transition Loss -0.90785813331604, Classifier Loss 0.05010996386408806, Total Loss 31.772537231445312\n",
      "13: Encoding Loss 4.785836219787598, Transition Loss -1.4201273918151855, Classifier Loss 0.05622328072786331, Total Loss 34.33677673339844\n",
      "13: Encoding Loss 5.0702338218688965, Transition Loss -1.3206090927124023, Classifier Loss 0.051151201128959656, Total Loss 35.53599548339844\n",
      "13: Encoding Loss 4.973725318908691, Transition Loss -1.3141329288482666, Classifier Loss 0.04004243388772011, Total Loss 33.8460693359375\n",
      "13: Encoding Loss 4.543783664703369, Transition Loss -1.2620946168899536, Classifier Loss 0.11727004498243332, Total Loss 38.98920440673828\n",
      "13: Encoding Loss 3.9333622455596924, Transition Loss -0.8357067108154297, Classifier Loss 0.08409395813941956, Total Loss 32.00923538208008\n",
      "13: Encoding Loss 5.511459827423096, Transition Loss 0.3382176458835602, Classifier Loss 0.10802795737981796, Total Loss 44.00684356689453\n",
      "13: Encoding Loss 4.498717784881592, Transition Loss -1.8137074708938599, Classifier Loss 0.05517491698265076, Total Loss 32.50907516479492\n",
      "13: Encoding Loss 4.789461612701416, Transition Loss -2.039229154586792, Classifier Loss 0.052284590899944305, Total Loss 33.964412689208984\n",
      "13: Encoding Loss 4.440155982971191, Transition Loss -0.7154768705368042, Classifier Loss 0.13574735820293427, Total Loss 40.21538543701172\n",
      "13: Encoding Loss 3.7763543128967285, Transition Loss -0.2838688790798187, Classifier Loss 0.03703968971967697, Total Loss 26.361980438232422\n",
      "13: Encoding Loss 5.105463981628418, Transition Loss -2.0909252166748047, Classifier Loss 0.0602899007499218, Total Loss 36.66094207763672\n",
      "13: Encoding Loss 4.068330764770508, Transition Loss -0.8425996899604797, Classifier Loss 0.029212528839707375, Total Loss 27.330902099609375\n",
      "13: Encoding Loss 6.3350677490234375, Transition Loss 0.08235174417495728, Classifier Loss 0.18393054604530334, Total Loss 56.4364013671875\n",
      "13: Encoding Loss 5.483599662780762, Transition Loss -1.65272057056427, Classifier Loss 0.08088381588459015, Total Loss 40.989322662353516\n",
      "13: Encoding Loss 6.4004411697387695, Transition Loss -0.4673216640949249, Classifier Loss 0.15058641135692596, Total Loss 53.46110153198242\n",
      "13: Encoding Loss 4.445441246032715, Transition Loss -1.0880193710327148, Classifier Loss 0.02290031500160694, Total Loss 28.96224594116211\n",
      "13: Encoding Loss 4.935536861419678, Transition Loss -1.07413911819458, Classifier Loss 0.1341090202331543, Total Loss 43.0236930847168\n",
      "13: Encoding Loss 2.9028005599975586, Transition Loss -1.4779536724090576, Classifier Loss 0.09501825273036957, Total Loss 26.91803741455078\n",
      "13: Encoding Loss 5.513712406158447, Transition Loss -0.3524251878261566, Classifier Loss 0.1162184327840805, Total Loss 44.703975677490234\n",
      "13: Encoding Loss 5.845059394836426, Transition Loss -1.2486904859542847, Classifier Loss 0.08229947090148926, Total Loss 43.2998046875\n",
      "13: Encoding Loss 3.8774638175964355, Transition Loss -0.49493077397346497, Classifier Loss 0.11471743136644363, Total Loss 34.736328125\n",
      "13: Encoding Loss 4.863056182861328, Transition Loss -1.3005621433258057, Classifier Loss 0.06815074384212494, Total Loss 35.99289321899414\n",
      "13: Encoding Loss 5.9449334144592285, Transition Loss -0.46390974521636963, Classifier Loss 0.06208533048629761, Total Loss 41.87794876098633\n",
      "13: Encoding Loss 3.775831699371338, Transition Loss 0.03653627634048462, Classifier Loss 0.053328145295381546, Total Loss 28.00242042541504\n",
      "13: Encoding Loss 6.344705104827881, Transition Loss 0.02739730477333069, Classifier Loss 0.17128734290599823, Total Loss 55.20792770385742\n",
      "13: Encoding Loss 5.487729072570801, Transition Loss -1.033180832862854, Classifier Loss 0.1616981327533722, Total Loss 49.09577941894531\n",
      "13: Encoding Loss 4.64847469329834, Transition Loss -1.366279125213623, Classifier Loss 0.06746013462543488, Total Loss 34.63631820678711\n",
      "13: Encoding Loss 4.057386875152588, Transition Loss -1.4551067352294922, Classifier Loss 0.1471146047115326, Total Loss 39.055198669433594\n",
      "13: Encoding Loss 6.267995834350586, Transition Loss 0.03150111436843872, Classifier Loss 0.1362646520137787, Total Loss 51.247039794921875\n",
      "13: Encoding Loss 5.377682685852051, Transition Loss -1.577256679534912, Classifier Loss 0.1348087638616562, Total Loss 45.74634552001953\n",
      "13: Encoding Loss 5.693299293518066, Transition Loss -1.1865234375, Classifier Loss 0.134092777967453, Total Loss 47.568603515625\n",
      "13: Encoding Loss 4.891982555389404, Transition Loss -1.2055537700653076, Classifier Loss 0.07136407494544983, Total Loss 36.487823486328125\n",
      "13: Encoding Loss 5.7899956703186035, Transition Loss -1.601611852645874, Classifier Loss 0.11730709671974182, Total Loss 46.47004318237305\n",
      "13: Encoding Loss 5.48325252532959, Transition Loss -0.7787121534347534, Classifier Loss 0.08414825052022934, Total Loss 41.314029693603516\n",
      "13: Encoding Loss 4.4420061111450195, Transition Loss -1.2147657871246338, Classifier Loss 0.08280053734779358, Total Loss 34.93160629272461\n",
      "13: Encoding Loss 4.660984039306641, Transition Loss 0.07328921556472778, Classifier Loss 0.035271257162094116, Total Loss 31.522348403930664\n",
      "13: Encoding Loss 3.866544246673584, Transition Loss -0.26129525899887085, Classifier Loss 0.050919611006975174, Total Loss 28.291122436523438\n",
      "13: Encoding Loss 6.575881004333496, Transition Loss -1.6514250040054321, Classifier Loss 0.14965787529945374, Total Loss 54.420413970947266\n",
      "13: Encoding Loss 5.573635101318359, Transition Loss -1.7547029256820679, Classifier Loss 0.11531413346529007, Total Loss 44.9725227355957\n",
      "13: Encoding Loss 2.9537367820739746, Transition Loss -1.0160787105560303, Classifier Loss 0.07121875137090683, Total Loss 24.843891143798828\n",
      "13: Encoding Loss 7.167102813720703, Transition Loss -1.6189886331558228, Classifier Loss 0.07794792205095291, Total Loss 50.79676055908203\n",
      "13: Encoding Loss 4.303675651550293, Transition Loss -0.8112777471542358, Classifier Loss 0.05692688003182411, Total Loss 31.514419555664062\n",
      "13: Encoding Loss 4.328128814697266, Transition Loss -0.9296104907989502, Classifier Loss 0.039720382541418076, Total Loss 29.940441131591797\n",
      "13: Encoding Loss 6.324998378753662, Transition Loss -1.8451895713806152, Classifier Loss 0.11848661303520203, Total Loss 49.797916412353516\n",
      "13: Encoding Loss 5.729028701782227, Transition Loss -1.0468553304672241, Classifier Loss 0.19808216392993927, Total Loss 54.181968688964844\n",
      "13: Encoding Loss 5.594317436218262, Transition Loss -1.223351001739502, Classifier Loss 0.07414031028747559, Total Loss 40.97945022583008\n",
      "13: Encoding Loss 4.440720081329346, Transition Loss -0.007885411381721497, Classifier Loss 0.06860668957233429, Total Loss 33.50498580932617\n",
      "13: Encoding Loss 4.772972106933594, Transition Loss -2.2707934379577637, Classifier Loss 0.07340100407600403, Total Loss 35.977027893066406\n",
      "13: Encoding Loss 5.166940689086914, Transition Loss -1.840032935142517, Classifier Loss 0.07532183080911636, Total Loss 38.5330924987793\n",
      "13: Encoding Loss 5.9550909996032715, Transition Loss 0.2641906142234802, Classifier Loss 0.11368938535451889, Total Loss 47.205162048339844\n",
      "13: Encoding Loss 7.615667819976807, Transition Loss -1.2696810960769653, Classifier Loss 0.12963828444480896, Total Loss 58.65732955932617\n",
      "13: Encoding Loss 4.80288553237915, Transition Loss -2.2638936042785645, Classifier Loss 0.04030916094779968, Total Loss 32.84732437133789\n",
      "13: Encoding Loss 5.927117347717285, Transition Loss -0.9418275952339172, Classifier Loss 0.04450015723705292, Total Loss 40.01234436035156\n",
      "13: Encoding Loss 5.297904968261719, Transition Loss -0.7107923030853271, Classifier Loss 0.13685908913612366, Total Loss 45.47305679321289\n",
      "13: Encoding Loss 4.906500816345215, Transition Loss -1.0577303171157837, Classifier Loss 0.035442665219306946, Total Loss 32.98284912109375\n",
      "14: Encoding Loss 3.8784403800964355, Transition Loss -1.154571294784546, Classifier Loss 0.0660879835486412, Total Loss 29.87898063659668\n",
      "14: Encoding Loss 5.857490539550781, Transition Loss -0.23578178882598877, Classifier Loss 0.09123510867357254, Total Loss 44.26836013793945\n",
      "14: Encoding Loss 4.814291000366211, Transition Loss -1.5526760816574097, Classifier Loss 0.0686832144856453, Total Loss 35.753448486328125\n",
      "14: Encoding Loss 5.598293304443359, Transition Loss -1.0959070920944214, Classifier Loss 0.06116913631558418, Total Loss 39.706233978271484\n",
      "14: Encoding Loss 7.7656097412109375, Transition Loss 0.4410848915576935, Classifier Loss 0.1709897518157959, Total Loss 63.86906814575195\n",
      "14: Encoding Loss 6.345444679260254, Transition Loss -2.8061001300811768, Classifier Loss 0.05442610755562782, Total Loss 43.51416015625\n",
      "14: Encoding Loss 4.236494541168213, Transition Loss -2.0672507286071777, Classifier Loss 0.1305614411830902, Total Loss 38.47428512573242\n",
      "14: Encoding Loss 4.518211841583252, Transition Loss -1.6138427257537842, Classifier Loss 0.09728968143463135, Total Loss 36.83759689331055\n",
      "14: Encoding Loss 4.79154634475708, Transition Loss -1.1409342288970947, Classifier Loss 0.1462991088628769, Total Loss 43.37873077392578\n",
      "14: Encoding Loss 3.1222236156463623, Transition Loss -0.6778590083122253, Classifier Loss 0.06877374649047852, Total Loss 25.61044692993164\n",
      "14: Encoding Loss 5.849329948425293, Transition Loss -1.3667188882827759, Classifier Loss 0.1255214363336563, Total Loss 47.647579193115234\n",
      "14: Encoding Loss 3.6927032470703125, Transition Loss -1.132844090461731, Classifier Loss 0.06749842315912247, Total Loss 28.905607223510742\n",
      "14: Encoding Loss 2.5487143993377686, Transition Loss -0.3405778408050537, Classifier Loss 0.03538510203361511, Total Loss 18.83066177368164\n",
      "14: Encoding Loss 4.488255500793457, Transition Loss -2.10656476020813, Classifier Loss 0.07782892882823944, Total Loss 34.71158218383789\n",
      "14: Encoding Loss 3.3772568702697754, Transition Loss -2.143139362335205, Classifier Loss 0.06914354115724564, Total Loss 27.177040100097656\n",
      "14: Encoding Loss 4.3821120262146, Transition Loss -1.407342553138733, Classifier Loss 0.06875055283308029, Total Loss 33.16716384887695\n",
      "14: Encoding Loss 3.737685441970825, Transition Loss -1.8656829595565796, Classifier Loss 0.0729345753788948, Total Loss 29.71882438659668\n",
      "14: Encoding Loss 2.5117526054382324, Transition Loss 0.5182367563247681, Classifier Loss 0.068830206990242, Total Loss 22.160831451416016\n",
      "14: Encoding Loss 6.9174346923828125, Transition Loss -2.304675579071045, Classifier Loss 0.09012623131275177, Total Loss 50.51630783081055\n",
      "14: Encoding Loss 3.9756088256835938, Transition Loss -0.5629692673683167, Classifier Loss 0.1896287500858307, Total Loss 42.81630325317383\n",
      "14: Encoding Loss 6.666118621826172, Transition Loss -2.314897060394287, Classifier Loss 0.21623891592025757, Total Loss 61.61967849731445\n",
      "14: Encoding Loss 5.5438618659973145, Transition Loss -1.3039450645446777, Classifier Loss 0.11591290682554245, Total Loss 44.853939056396484\n",
      "14: Encoding Loss 6.329967975616455, Transition Loss -0.558465838432312, Classifier Loss 0.09812695533037186, Total Loss 47.79227828979492\n",
      "14: Encoding Loss 6.224377632141113, Transition Loss -0.1563515067100525, Classifier Loss 0.09714113920927048, Total Loss 47.06032180786133\n",
      "14: Encoding Loss 5.303569793701172, Transition Loss 0.33013081550598145, Classifier Loss 0.04603362828493118, Total Loss 36.55683517456055\n",
      "14: Encoding Loss 5.1089887619018555, Transition Loss -1.886646032333374, Classifier Loss 0.07854753732681274, Total Loss 38.5079345703125\n",
      "14: Encoding Loss 3.4658801555633545, Transition Loss -0.3407737910747528, Classifier Loss 0.06443148851394653, Total Loss 27.23829460144043\n",
      "14: Encoding Loss 7.208661079406738, Transition Loss -1.5277998447418213, Classifier Loss 0.150123730301857, Total Loss 58.26373291015625\n",
      "14: Encoding Loss 3.320793867111206, Transition Loss -2.4128530025482178, Classifier Loss 0.05958229675889015, Total Loss 25.882028579711914\n",
      "14: Encoding Loss 3.4824042320251465, Transition Loss -1.333667516708374, Classifier Loss 0.14534761011600494, Total Loss 35.428653717041016\n",
      "14: Encoding Loss 4.701220989227295, Transition Loss -1.8064996004104614, Classifier Loss 0.08318081498146057, Total Loss 36.524688720703125\n",
      "14: Encoding Loss 4.33836555480957, Transition Loss 0.6263514757156372, Classifier Loss 0.04562801122665405, Total Loss 30.843536376953125\n",
      "14: Encoding Loss 6.247257709503174, Transition Loss -1.1026744842529297, Classifier Loss 0.13995791971683502, Total Loss 51.47889709472656\n",
      "14: Encoding Loss 3.9471054077148438, Transition Loss -1.436448335647583, Classifier Loss 0.0702374279499054, Total Loss 30.705801010131836\n",
      "14: Encoding Loss 3.933011293411255, Transition Loss -1.2839131355285645, Classifier Loss 0.11428667604923248, Total Loss 35.026222229003906\n",
      "14: Encoding Loss 5.156516075134277, Transition Loss -1.396992802619934, Classifier Loss 0.20974647998809814, Total Loss 51.91318893432617\n",
      "14: Encoding Loss 4.726421356201172, Transition Loss -1.3482779264450073, Classifier Loss 0.0926283523440361, Total Loss 37.620826721191406\n",
      "14: Encoding Loss 4.823130130767822, Transition Loss -0.3773035705089569, Classifier Loss 0.05385714769363403, Total Loss 34.324344635009766\n",
      "14: Encoding Loss 8.473533630371094, Transition Loss 0.09145519137382507, Classifier Loss 0.19555950164794922, Total Loss 70.4337387084961\n",
      "14: Encoding Loss 7.272863864898682, Transition Loss -1.6383843421936035, Classifier Loss 0.10992499440908432, Total Loss 54.6290283203125\n",
      "14: Encoding Loss 5.299760818481445, Transition Loss -1.111366868019104, Classifier Loss 0.12485076487064362, Total Loss 44.283199310302734\n",
      "14: Encoding Loss 6.202134609222412, Transition Loss -1.1122105121612549, Classifier Loss 0.11462809890508652, Total Loss 48.675174713134766\n",
      "14: Encoding Loss 5.722775459289551, Transition Loss -1.14956796169281, Classifier Loss 0.13100706040859222, Total Loss 47.4369010925293\n",
      "14: Encoding Loss 8.120012283325195, Transition Loss -2.353325366973877, Classifier Loss 0.05929546058177948, Total Loss 54.648677825927734\n",
      "14: Encoding Loss 4.840645790100098, Transition Loss -1.8992305994033813, Classifier Loss 0.1010197103023529, Total Loss 39.145084381103516\n",
      "14: Encoding Loss 3.373180866241455, Transition Loss 0.11881083250045776, Classifier Loss 0.046966999769210815, Total Loss 24.98331069946289\n",
      "14: Encoding Loss 4.163317680358887, Transition Loss -1.072959303855896, Classifier Loss 0.08558890223503113, Total Loss 33.538368225097656\n",
      "14: Encoding Loss 6.7640557289123535, Transition Loss -0.016611546277999878, Classifier Loss 0.07559658586978912, Total Loss 48.143985748291016\n",
      "14: Encoding Loss 7.312154293060303, Transition Loss -0.35407036542892456, Classifier Loss 0.20712770521640778, Total Loss 64.58555603027344\n",
      "14: Encoding Loss 4.091057777404785, Transition Loss -0.23798984289169312, Classifier Loss 0.05067017674446106, Total Loss 29.613269805908203\n",
      "14: Encoding Loss 4.800607681274414, Transition Loss -1.5422134399414062, Classifier Loss 0.13179318606853485, Total Loss 41.98234939575195\n",
      "14: Encoding Loss 3.5125668048858643, Transition Loss -1.8812031745910645, Classifier Loss 0.025982873514294624, Total Loss 23.672935485839844\n",
      "14: Encoding Loss 7.0070295333862305, Transition Loss -0.9723637700080872, Classifier Loss 0.07225458323955536, Total Loss 49.267250061035156\n",
      "14: Encoding Loss 4.706657409667969, Transition Loss -0.04966983199119568, Classifier Loss 0.15208539366722107, Total Loss 43.44846725463867\n",
      "14: Encoding Loss 2.7331929206848145, Transition Loss -1.9698673486709595, Classifier Loss 0.10721288621425629, Total Loss 27.119659423828125\n",
      "14: Encoding Loss 7.416699409484863, Transition Loss -0.24469302594661713, Classifier Loss 0.05242599919438362, Total Loss 49.742698669433594\n",
      "14: Encoding Loss 7.094766139984131, Transition Loss -0.45511913299560547, Classifier Loss 0.13606074452400208, Total Loss 56.17449188232422\n",
      "14: Encoding Loss 3.6305835247039795, Transition Loss -1.0662024021148682, Classifier Loss 0.053006842732429504, Total Loss 27.083759307861328\n",
      "14: Encoding Loss 9.898137092590332, Transition Loss -2.0540964603424072, Classifier Loss 0.18687649071216583, Total Loss 78.07565307617188\n",
      "14: Encoding Loss 5.7674241065979, Transition Loss -1.6699938774108887, Classifier Loss 0.03496904298663139, Total Loss 38.10078048706055\n",
      "14: Encoding Loss 5.680227279663086, Transition Loss -0.8311358690261841, Classifier Loss 0.08097971975803375, Total Loss 42.17900466918945\n",
      "14: Encoding Loss 4.038675785064697, Transition Loss -2.343388557434082, Classifier Loss 0.07017058879137039, Total Loss 31.248178482055664\n",
      "14: Encoding Loss 3.861590623855591, Transition Loss -2.338684558868408, Classifier Loss 0.11419706046581268, Total Loss 34.588314056396484\n",
      "14: Encoding Loss 5.154223442077637, Transition Loss -4.1981635093688965, Classifier Loss 0.0865945965051651, Total Loss 39.58312225341797\n",
      "14: Encoding Loss 4.692715167999268, Transition Loss -0.7691837549209595, Classifier Loss 0.1139010488986969, Total Loss 39.54608917236328\n",
      "14: Encoding Loss 6.027069568634033, Transition Loss -2.0718185901641846, Classifier Loss 0.20895349979400635, Total Loss 57.056941986083984\n",
      "14: Encoding Loss 3.449671506881714, Transition Loss -1.2220163345336914, Classifier Loss 0.20497460663318634, Total Loss 41.195003509521484\n",
      "14: Encoding Loss 5.699229717254639, Transition Loss -1.6846868991851807, Classifier Loss 0.0696052834391594, Total Loss 41.155235290527344\n",
      "14: Encoding Loss 4.347043037414551, Transition Loss 0.028549015522003174, Classifier Loss 0.0810507982969284, Total Loss 34.198760986328125\n",
      "14: Encoding Loss 2.7911767959594727, Transition Loss -0.08392822742462158, Classifier Loss 0.06332655996084213, Total Loss 23.079681396484375\n",
      "14: Encoding Loss 4.683724880218506, Transition Loss -2.231045722961426, Classifier Loss 0.14555604755878448, Total Loss 42.65706253051758\n",
      "14: Encoding Loss 2.7792677879333496, Transition Loss -1.510947823524475, Classifier Loss 0.09662755578756332, Total Loss 26.337759017944336\n",
      "14: Encoding Loss 2.30682635307312, Transition Loss -2.149376392364502, Classifier Loss 0.08578304201364517, Total Loss 22.41840171813965\n",
      "14: Encoding Loss 6.696444511413574, Transition Loss -0.39062467217445374, Classifier Loss 0.03269840031862259, Total Loss 43.4483528137207\n",
      "14: Encoding Loss 4.011166572570801, Transition Loss -1.5062735080718994, Classifier Loss 0.04591965302824974, Total Loss 28.658361434936523\n",
      "14: Encoding Loss 5.374001502990723, Transition Loss -1.758225679397583, Classifier Loss 0.14575928449630737, Total Loss 46.819236755371094\n",
      "14: Encoding Loss 3.72412109375, Transition Loss -1.9505064487457275, Classifier Loss 0.03300101310014725, Total Loss 25.6440486907959\n",
      "14: Encoding Loss 6.765045166015625, Transition Loss -1.5305302143096924, Classifier Loss 0.08403348922729492, Total Loss 48.993011474609375\n",
      "14: Encoding Loss 4.748861312866211, Transition Loss -1.355647325515747, Classifier Loss 0.1367880403995514, Total Loss 42.17143249511719\n",
      "14: Encoding Loss 2.7911744117736816, Transition Loss -1.2677441835403442, Classifier Loss 0.06110042706131935, Total Loss 22.856582641601562\n",
      "14: Encoding Loss 6.357591152191162, Transition Loss -0.8903487920761108, Classifier Loss 0.062119968235492706, Total Loss 44.35719299316406\n",
      "14: Encoding Loss 5.202933311462402, Transition Loss -1.0353070497512817, Classifier Loss 0.17379924654960632, Total Loss 48.597110748291016\n",
      "14: Encoding Loss 3.495845317840576, Transition Loss -0.2095661759376526, Classifier Loss 0.13736073672771454, Total Loss 34.711063385009766\n",
      "14: Encoding Loss 4.607532978057861, Transition Loss -1.0818181037902832, Classifier Loss 0.11052313446998596, Total Loss 38.69708251953125\n",
      "14: Encoding Loss 5.498548984527588, Transition Loss -1.1216151714324951, Classifier Loss 0.13631649315357208, Total Loss 46.622493743896484\n",
      "14: Encoding Loss 5.064200401306152, Transition Loss -2.4570045471191406, Classifier Loss 0.10121577233076096, Total Loss 40.50579833984375\n",
      "14: Encoding Loss 4.5699968338012695, Transition Loss 0.844538688659668, Classifier Loss 0.06036657094955444, Total Loss 33.79445266723633\n",
      "14: Encoding Loss 5.32387113571167, Transition Loss -1.5579708814620972, Classifier Loss 0.1647377908229828, Total Loss 48.416385650634766\n",
      "14: Encoding Loss 5.227609157562256, Transition Loss -2.446526527404785, Classifier Loss 0.03635810688138008, Total Loss 35.000484466552734\n",
      "14: Encoding Loss 4.55197811126709, Transition Loss -1.7904995679855347, Classifier Loss 0.12042252719402313, Total Loss 39.3534049987793\n",
      "14: Encoding Loss 3.813063383102417, Transition Loss 0.81593918800354, Classifier Loss 0.1110481545329094, Total Loss 34.3095703125\n",
      "14: Encoding Loss 3.1022191047668457, Transition Loss -1.3820468187332153, Classifier Loss 0.0577428974211216, Total Loss 24.387052536010742\n",
      "14: Encoding Loss 5.5090718269348145, Transition Loss -1.7920986413955688, Classifier Loss 0.07254990190267563, Total Loss 40.3087043762207\n",
      "14: Encoding Loss 4.343132495880127, Transition Loss 1.058853268623352, Classifier Loss 0.06258469820022583, Total Loss 32.740806579589844\n",
      "14: Encoding Loss 3.698326587677002, Transition Loss -1.8523051738739014, Classifier Loss 0.048531126230955124, Total Loss 27.042333602905273\n",
      "14: Encoding Loss 2.7334494590759277, Transition Loss -1.385948896408081, Classifier Loss 0.10000679641962051, Total Loss 26.40082359313965\n",
      "14: Encoding Loss 3.5587143898010254, Transition Loss 0.19326192140579224, Classifier Loss 0.04957818239927292, Total Loss 26.387409210205078\n",
      "14: Encoding Loss 10.441475868225098, Transition Loss 0.15605074167251587, Classifier Loss 0.23227617144584656, Total Loss 85.93889617919922\n",
      "14: Encoding Loss 6.044922828674316, Transition Loss -1.0880484580993652, Classifier Loss 0.09891721606254578, Total Loss 46.16082763671875\n",
      "14: Encoding Loss 7.956545829772949, Transition Loss -1.077048897743225, Classifier Loss 0.15144939720630646, Total Loss 62.883785247802734\n",
      "14: Encoding Loss 5.829606533050537, Transition Loss -1.622380018234253, Classifier Loss 0.046439047902822495, Total Loss 39.62089920043945\n",
      "14: Encoding Loss 4.995330810546875, Transition Loss -1.8506004810333252, Classifier Loss 0.04713433235883713, Total Loss 34.6846809387207\n",
      "14: Encoding Loss 4.684920310974121, Transition Loss -0.4555666446685791, Classifier Loss 0.12653544545173645, Total Loss 40.76288604736328\n",
      "14: Encoding Loss 4.479398727416992, Transition Loss -2.1576552391052246, Classifier Loss 0.08723617345094681, Total Loss 35.599151611328125\n",
      "14: Encoding Loss 2.963043689727783, Transition Loss -0.21824917197227478, Classifier Loss 0.12666410207748413, Total Loss 30.4445858001709\n",
      "14: Encoding Loss 3.0489394664764404, Transition Loss 0.7980246543884277, Classifier Loss 0.03472774475812912, Total Loss 22.085622787475586\n",
      "14: Encoding Loss 6.570622444152832, Transition Loss 0.06984925270080566, Classifier Loss 0.09400159120559692, Total Loss 48.85183334350586\n",
      "14: Encoding Loss 3.730726957321167, Transition Loss -2.2022347450256348, Classifier Loss 0.03585710749030113, Total Loss 25.969192504882812\n",
      "14: Encoding Loss 9.119979858398438, Transition Loss -1.9837098121643066, Classifier Loss 0.13023726642131805, Total Loss 67.74281311035156\n",
      "14: Encoding Loss 6.836667060852051, Transition Loss -1.0751925706863403, Classifier Loss 0.17765496671199799, Total Loss 58.78506851196289\n",
      "14: Encoding Loss 3.9705421924591064, Transition Loss -1.6563009023666382, Classifier Loss 0.059225667268037796, Total Loss 29.745159149169922\n",
      "14: Encoding Loss 6.112720489501953, Transition Loss -0.32955098152160645, Classifier Loss 0.0918816477060318, Total Loss 45.86435317993164\n",
      "14: Encoding Loss 6.204041957855225, Transition Loss -0.9340476393699646, Classifier Loss 0.06783926486968994, Total Loss 44.007808685302734\n",
      "14: Encoding Loss 5.903261184692383, Transition Loss -1.1364346742630005, Classifier Loss 0.0370335578918457, Total Loss 39.12247085571289\n",
      "14: Encoding Loss 6.742793083190918, Transition Loss -1.3517965078353882, Classifier Loss 0.061817068606615067, Total Loss 46.63792419433594\n",
      "14: Encoding Loss 4.037322521209717, Transition Loss -2.604098081588745, Classifier Loss 0.07438775897026062, Total Loss 31.661670684814453\n",
      "14: Encoding Loss 3.8177056312561035, Transition Loss 0.0060994625091552734, Classifier Loss 0.07438407838344574, Total Loss 30.347082138061523\n",
      "14: Encoding Loss 2.903681993484497, Transition Loss -0.9209225177764893, Classifier Loss 0.05369136482477188, Total Loss 22.790861129760742\n",
      "14: Encoding Loss 4.0637664794921875, Transition Loss -1.2481887340545654, Classifier Loss 0.07791488617658615, Total Loss 32.173587799072266\n",
      "14: Encoding Loss 5.258183002471924, Transition Loss -0.3051128089427948, Classifier Loss 0.09341486543416977, Total Loss 40.890464782714844\n",
      "14: Encoding Loss 6.285095691680908, Transition Loss -0.8917803764343262, Classifier Loss 0.18132315576076508, Total Loss 55.842529296875\n",
      "14: Encoding Loss 4.625295162200928, Transition Loss -1.825089454650879, Classifier Loss 0.12377855181694031, Total Loss 40.12889862060547\n",
      "14: Encoding Loss 6.7393341064453125, Transition Loss -1.5037893056869507, Classifier Loss 0.09440548717975616, Total Loss 49.87594985961914\n",
      "14: Encoding Loss 4.339727878570557, Transition Loss -1.8439692258834839, Classifier Loss 0.051740169525146484, Total Loss 31.211647033691406\n",
      "14: Encoding Loss 2.6942286491394043, Transition Loss -1.3626205921173096, Classifier Loss 0.06058366224169731, Total Loss 22.223194122314453\n",
      "14: Encoding Loss 5.617059707641602, Transition Loss -2.2633283138275146, Classifier Loss 0.0533621571958065, Total Loss 39.03767013549805\n",
      "14: Encoding Loss 6.8887200355529785, Transition Loss -2.3837246894836426, Classifier Loss 0.17546741664409637, Total Loss 58.878108978271484\n",
      "14: Encoding Loss 4.858797550201416, Transition Loss -2.1196529865264893, Classifier Loss 0.037097010761499405, Total Loss 32.86164093017578\n",
      "14: Encoding Loss 3.1176598072052, Transition Loss -1.1394277811050415, Classifier Loss 0.043505918234586716, Total Loss 23.056095123291016\n",
      "14: Encoding Loss 2.3606948852539062, Transition Loss -3.6178131103515625, Classifier Loss 0.058335885405540466, Total Loss 19.99631118774414\n",
      "14: Encoding Loss 1.5354655981063843, Transition Loss -0.9869820475578308, Classifier Loss 0.12607136368751526, Total Loss 21.819536209106445\n",
      "14: Encoding Loss 5.783687591552734, Transition Loss -1.2175395488739014, Classifier Loss 0.06848754733800888, Total Loss 41.550392150878906\n",
      "14: Encoding Loss 5.256258964538574, Transition Loss -1.7420680522918701, Classifier Loss 0.08729633688926697, Total Loss 40.2664909362793\n",
      "14: Encoding Loss 3.8492724895477295, Transition Loss -1.4475088119506836, Classifier Loss 0.04504214599728584, Total Loss 27.599271774291992\n",
      "14: Encoding Loss 6.152678966522217, Transition Loss -1.1409471035003662, Classifier Loss 0.100581593811512, Total Loss 46.973777770996094\n",
      "14: Encoding Loss 5.9574432373046875, Transition Loss -1.091341495513916, Classifier Loss 0.17194809019565582, Total Loss 52.93903350830078\n",
      "14: Encoding Loss 4.661732196807861, Transition Loss 0.08783850073814392, Classifier Loss 0.05747721716761589, Total Loss 33.75325393676758\n",
      "14: Encoding Loss 5.328104496002197, Transition Loss 0.237323597073555, Classifier Loss 0.1001981869339943, Total Loss 42.0833740234375\n",
      "14: Encoding Loss 3.5012359619140625, Transition Loss -1.6557550430297852, Classifier Loss 0.040116988122463226, Total Loss 25.01845359802246\n",
      "14: Encoding Loss 7.840115547180176, Transition Loss 0.13312189280986786, Classifier Loss 0.04847128689289093, Total Loss 51.94107437133789\n",
      "14: Encoding Loss 5.498127460479736, Transition Loss -1.4733858108520508, Classifier Loss 0.047910068184137344, Total Loss 37.7791862487793\n",
      "14: Encoding Loss 6.145475387573242, Transition Loss -0.4851433038711548, Classifier Loss 0.09415701031684875, Total Loss 46.28835678100586\n",
      "14: Encoding Loss 5.128628730773926, Transition Loss -1.0584585666656494, Classifier Loss 0.04292697459459305, Total Loss 35.06404495239258\n",
      "14: Encoding Loss 5.1481194496154785, Transition Loss -0.1719551384449005, Classifier Loss 0.06138182803988457, Total Loss 37.026832580566406\n",
      "14: Encoding Loss 3.6548869609832764, Transition Loss -1.153391718864441, Classifier Loss 0.06820885092020035, Total Loss 28.749746322631836\n",
      "14: Encoding Loss 5.289034843444824, Transition Loss -0.8424089550971985, Classifier Loss 0.18297046422958374, Total Loss 50.030921936035156\n",
      "14: Encoding Loss 2.855257749557495, Transition Loss -1.632131814956665, Classifier Loss 0.04409404098987579, Total Loss 21.540300369262695\n",
      "14: Encoding Loss 6.327927112579346, Transition Loss -1.2056232690811157, Classifier Loss 0.27140459418296814, Total Loss 65.1075439453125\n",
      "14: Encoding Loss 5.930718421936035, Transition Loss -0.6343458890914917, Classifier Loss 0.2088290899991989, Total Loss 56.46696853637695\n",
      "14: Encoding Loss 3.7683048248291016, Transition Loss -1.6050457954406738, Classifier Loss 0.11839146912097931, Total Loss 34.448333740234375\n",
      "14: Encoding Loss 5.71160888671875, Transition Loss -1.978525161743164, Classifier Loss 0.06385109573602676, Total Loss 40.65397262573242\n",
      "14: Encoding Loss 6.7213592529296875, Transition Loss -0.26733142137527466, Classifier Loss 0.08231586217880249, Total Loss 48.559635162353516\n",
      "14: Encoding Loss 7.288609981536865, Transition Loss -0.6263872981071472, Classifier Loss 0.16664624214172363, Total Loss 60.396034240722656\n",
      "14: Encoding Loss 4.89285945892334, Transition Loss -1.892168402671814, Classifier Loss 0.060878634452819824, Total Loss 35.44426727294922\n",
      "14: Encoding Loss 4.516663551330566, Transition Loss -3.23591947555542, Classifier Loss 0.06920339912176132, Total Loss 34.01902770996094\n",
      "14: Encoding Loss 7.068983554840088, Transition Loss -2.095761775970459, Classifier Loss 0.15423165261745453, Total Loss 57.83622741699219\n",
      "14: Encoding Loss 5.8875532150268555, Transition Loss -0.8466116189956665, Classifier Loss 0.06489253044128418, Total Loss 41.81423568725586\n",
      "14: Encoding Loss 3.8041343688964844, Transition Loss -2.4089808464050293, Classifier Loss 0.07473406940698624, Total Loss 30.297250747680664\n",
      "14: Encoding Loss 2.6723649501800537, Transition Loss -0.8920959234237671, Classifier Loss 0.07166258245706558, Total Loss 23.200092315673828\n",
      "14: Encoding Loss 7.044378757476807, Transition Loss -2.0336709022521973, Classifier Loss 0.1059601753950119, Total Loss 52.86147689819336\n",
      "14: Encoding Loss 4.410684585571289, Transition Loss -0.04371866583824158, Classifier Loss 0.0783793032169342, Total Loss 34.30202102661133\n",
      "14: Encoding Loss 5.935721397399902, Transition Loss -0.6580008268356323, Classifier Loss 0.06698306649923325, Total Loss 42.312374114990234\n",
      "14: Encoding Loss 3.790686845779419, Transition Loss -0.9577729105949402, Classifier Loss 0.03635745495557785, Total Loss 26.379484176635742\n",
      "14: Encoding Loss 4.166924476623535, Transition Loss -1.3954250812530518, Classifier Loss 0.06640532612800598, Total Loss 31.64151954650879\n",
      "14: Encoding Loss 7.779025077819824, Transition Loss 0.13526177406311035, Classifier Loss 0.16109885275363922, Total Loss 62.83814239501953\n",
      "14: Encoding Loss 5.413836479187012, Transition Loss -1.3776912689208984, Classifier Loss 0.14883336424827576, Total Loss 47.365806579589844\n",
      "14: Encoding Loss 5.457942962646484, Transition Loss -3.0069873332977295, Classifier Loss 0.13642936944961548, Total Loss 46.3893928527832\n",
      "14: Encoding Loss 3.538898468017578, Transition Loss -2.0237021446228027, Classifier Loss 0.05230870097875595, Total Loss 26.463451385498047\n",
      "14: Encoding Loss 7.44534158706665, Transition Loss -1.8129953145980835, Classifier Loss 0.12636053562164307, Total Loss 57.30738067626953\n",
      "14: Encoding Loss 5.118597984313965, Transition Loss -0.6751041412353516, Classifier Loss 0.04583058878779411, Total Loss 35.294376373291016\n",
      "14: Encoding Loss 4.649332523345947, Transition Loss -1.4305939674377441, Classifier Loss 0.06175759807229042, Total Loss 34.07118225097656\n",
      "14: Encoding Loss 3.6822314262390137, Transition Loss -1.8316186666488647, Classifier Loss 0.03189007192850113, Total Loss 25.28166389465332\n",
      "14: Encoding Loss 5.207204341888428, Transition Loss -1.3077137470245361, Classifier Loss 0.16316725313663483, Total Loss 47.55942916870117\n",
      "14: Encoding Loss 5.874722003936768, Transition Loss -3.0813708305358887, Classifier Loss 0.10376927256584167, Total Loss 45.624027252197266\n",
      "14: Encoding Loss 3.812159299850464, Transition Loss -0.7059014439582825, Classifier Loss 0.08254780620336533, Total Loss 31.127456665039062\n",
      "14: Encoding Loss 6.71517276763916, Transition Loss -0.08597166836261749, Classifier Loss 0.13514533638954163, Total Loss 53.805538177490234\n",
      "14: Encoding Loss 6.83285665512085, Transition Loss -1.3415565490722656, Classifier Loss 0.13121679425239563, Total Loss 54.118282318115234\n",
      "14: Encoding Loss 5.3685479164123535, Transition Loss -2.625485897064209, Classifier Loss 0.08534284681081772, Total Loss 40.74452590942383\n",
      "14: Encoding Loss 6.217305660247803, Transition Loss -1.1062462329864502, Classifier Loss 0.03503502905368805, Total Loss 40.8068962097168\n",
      "14: Encoding Loss 5.209885120391846, Transition Loss 0.40846240520477295, Classifier Loss 0.09222538769245148, Total Loss 40.645233154296875\n",
      "14: Encoding Loss 5.58552885055542, Transition Loss -0.6710971593856812, Classifier Loss 0.11859172582626343, Total Loss 45.3720817565918\n",
      "14: Encoding Loss 7.1673994064331055, Transition Loss -0.7833482027053833, Classifier Loss 0.04493078216910362, Total Loss 47.497161865234375\n",
      "14: Encoding Loss 5.076960563659668, Transition Loss -2.1720476150512695, Classifier Loss 0.04549606516957283, Total Loss 35.010501861572266\n",
      "14: Encoding Loss 6.707888126373291, Transition Loss -1.5100880861282349, Classifier Loss 0.13728074729442596, Total Loss 53.97480010986328\n",
      "14: Encoding Loss 5.19449520111084, Transition Loss -0.9662672281265259, Classifier Loss 0.18949764966964722, Total Loss 50.11635208129883\n",
      "14: Encoding Loss 5.423708438873291, Transition Loss -2.616960048675537, Classifier Loss 0.09620319306850433, Total Loss 42.16152572631836\n",
      "14: Encoding Loss 3.4622256755828857, Transition Loss 0.41585415601730347, Classifier Loss 0.09071991592645645, Total Loss 30.011690139770508\n",
      "14: Encoding Loss 3.3578178882598877, Transition Loss -0.6104834079742432, Classifier Loss 0.056955140084028244, Total Loss 25.842178344726562\n",
      "14: Encoding Loss 2.6225833892822266, Transition Loss -1.0923802852630615, Classifier Loss 0.07168414443731308, Total Loss 22.903478622436523\n",
      "14: Encoding Loss 7.291330814361572, Transition Loss -2.1143479347229004, Classifier Loss 0.1265551745891571, Total Loss 56.40265655517578\n",
      "14: Encoding Loss 4.401040077209473, Transition Loss -1.4825472831726074, Classifier Loss 0.09302135556936264, Total Loss 35.70778274536133\n",
      "14: Encoding Loss 6.569222927093506, Transition Loss -1.571079969406128, Classifier Loss 0.09201313555240631, Total Loss 48.616024017333984\n",
      "14: Encoding Loss 4.5551438331604, Transition Loss -1.0805237293243408, Classifier Loss 0.10993435978889465, Total Loss 38.32386779785156\n",
      "14: Encoding Loss 4.38516092300415, Transition Loss -1.7599444389343262, Classifier Loss 0.11043824255466461, Total Loss 37.35408401489258\n",
      "14: Encoding Loss 4.098215103149414, Transition Loss -1.6952201128005981, Classifier Loss 0.13065287470817566, Total Loss 37.653900146484375\n",
      "14: Encoding Loss 4.5481343269348145, Transition Loss -1.379962682723999, Classifier Loss 0.10048186033964157, Total Loss 37.33644104003906\n",
      "14: Encoding Loss 6.551605224609375, Transition Loss -1.0907115936279297, Classifier Loss 0.10841251909732819, Total Loss 50.150447845458984\n",
      "14: Encoding Loss 4.956175327301025, Transition Loss -0.659055769443512, Classifier Loss 0.2086082398891449, Total Loss 50.59761428833008\n",
      "14: Encoding Loss 6.823507785797119, Transition Loss -1.776572585105896, Classifier Loss 0.08222148567438126, Total Loss 49.1624870300293\n",
      "14: Encoding Loss 4.43079137802124, Transition Loss -1.856579303741455, Classifier Loss 0.0716022327542305, Total Loss 33.74422836303711\n",
      "14: Encoding Loss 6.027928829193115, Transition Loss -1.427744746208191, Classifier Loss 0.07446559518575668, Total Loss 43.613563537597656\n",
      "14: Encoding Loss 4.441767692565918, Transition Loss -0.9201266765594482, Classifier Loss 0.07030407339334488, Total Loss 33.68064880371094\n",
      "14: Encoding Loss 4.454161167144775, Transition Loss -0.8669047355651855, Classifier Loss 0.04793946444988251, Total Loss 31.518566131591797\n",
      "14: Encoding Loss 3.1589388847351074, Transition Loss -0.40571677684783936, Classifier Loss 0.05864378809928894, Total Loss 24.81785011291504\n",
      "14: Encoding Loss 5.204684257507324, Transition Loss -1.0118037462234497, Classifier Loss 0.06771989166736603, Total Loss 37.999691009521484\n",
      "14: Encoding Loss 5.688830375671387, Transition Loss -0.7045497298240662, Classifier Loss 0.1753104031085968, Total Loss 51.66374206542969\n",
      "14: Encoding Loss 3.4378790855407715, Transition Loss -1.5010759830474854, Classifier Loss 0.14994904398918152, Total Loss 35.62158203125\n",
      "14: Encoding Loss 3.885275363922119, Transition Loss -0.02831350266933441, Classifier Loss 0.1095781922340393, Total Loss 34.26945877075195\n",
      "14: Encoding Loss 7.187680721282959, Transition Loss -1.1196837425231934, Classifier Loss 0.18440131843090057, Total Loss 61.565773010253906\n",
      "14: Encoding Loss 6.002643585205078, Transition Loss -0.25270041823387146, Classifier Loss 0.09085255861282349, Total Loss 45.101016998291016\n",
      "14: Encoding Loss 5.086479663848877, Transition Loss -0.811251163482666, Classifier Loss 0.03905560076236725, Total Loss 34.42411422729492\n",
      "14: Encoding Loss 6.390593528747559, Transition Loss -0.431576132774353, Classifier Loss 0.10644611716270447, Total Loss 48.98800277709961\n",
      "14: Encoding Loss 4.537458419799805, Transition Loss -3.0522308349609375, Classifier Loss 0.10966014117002487, Total Loss 38.189544677734375\n",
      "14: Encoding Loss 3.323721408843994, Transition Loss 1.1423873901367188, Classifier Loss 0.061708249151706696, Total Loss 26.570110321044922\n",
      "14: Encoding Loss 3.4845261573791504, Transition Loss -2.304352283477783, Classifier Loss 0.0636543333530426, Total Loss 27.271669387817383\n",
      "14: Encoding Loss 5.149919509887695, Transition Loss -2.091020345687866, Classifier Loss 0.029996562749147415, Total Loss 33.898338317871094\n",
      "14: Encoding Loss 5.866951942443848, Transition Loss -1.588243842124939, Classifier Loss 0.06142343953251839, Total Loss 41.343421936035156\n",
      "14: Encoding Loss 5.823344707489014, Transition Loss -1.1684863567352295, Classifier Loss 0.04961579293012619, Total Loss 39.901180267333984\n",
      "14: Encoding Loss 4.467580795288086, Transition Loss -0.7552723288536072, Classifier Loss 0.09589153528213501, Total Loss 36.39433670043945\n",
      "14: Encoding Loss 4.580628395080566, Transition Loss -0.7196748852729797, Classifier Loss 0.038017548620700836, Total Loss 31.28523826599121\n",
      "14: Encoding Loss 2.92657732963562, Transition Loss -0.7009302377700806, Classifier Loss 0.08609578758478165, Total Loss 26.168764114379883\n",
      "14: Encoding Loss 6.594511985778809, Transition Loss -2.3321757316589355, Classifier Loss 0.054797686636447906, Total Loss 45.0459098815918\n",
      "14: Encoding Loss 4.666577339172363, Transition Loss -1.4188518524169922, Classifier Loss 0.05244233459234238, Total Loss 33.24312973022461\n",
      "14: Encoding Loss 6.23334264755249, Transition Loss -1.5385019779205322, Classifier Loss 0.14704474806785583, Total Loss 52.103919982910156\n",
      "14: Encoding Loss 4.3521952629089355, Transition Loss -1.4591301679611206, Classifier Loss 0.020777780562639236, Total Loss 28.190366744995117\n",
      "14: Encoding Loss 5.682487487792969, Transition Loss -0.186778262257576, Classifier Loss 0.06279665231704712, Total Loss 40.37451171875\n",
      "14: Encoding Loss 5.077188014984131, Transition Loss -1.407304048538208, Classifier Loss 0.12459708750247955, Total Loss 42.922271728515625\n",
      "14: Encoding Loss 4.551239967346191, Transition Loss -1.4505786895751953, Classifier Loss 0.06826040148735046, Total Loss 34.13290023803711\n",
      "14: Encoding Loss 2.9635653495788574, Transition Loss -2.3834633827209473, Classifier Loss 0.05555808171629906, Total Loss 23.336246490478516\n",
      "14: Encoding Loss 5.113006114959717, Transition Loss -3.3088111877441406, Classifier Loss 0.046453457325696945, Total Loss 35.322059631347656\n",
      "14: Encoding Loss 3.276831865310669, Transition Loss -0.5808461904525757, Classifier Loss 0.13049954175949097, Total Loss 32.71071243286133\n",
      "14: Encoding Loss 5.0218305587768555, Transition Loss -0.7762583494186401, Classifier Loss 0.14510026574134827, Total Loss 44.64070510864258\n",
      "14: Encoding Loss 5.648476600646973, Transition Loss -0.5049455761909485, Classifier Loss 0.1027352437376976, Total Loss 44.1641845703125\n",
      "14: Encoding Loss 4.008371353149414, Transition Loss -0.4386940598487854, Classifier Loss 0.03930598869919777, Total Loss 27.980653762817383\n",
      "14: Encoding Loss 8.226956367492676, Transition Loss -0.17533361911773682, Classifier Loss 0.1268012970685959, Total Loss 62.04180145263672\n",
      "14: Encoding Loss 4.278536796569824, Transition Loss -0.956566333770752, Classifier Loss 0.07927237451076508, Total Loss 33.598079681396484\n",
      "14: Encoding Loss 7.983975410461426, Transition Loss -0.30503493547439575, Classifier Loss 0.18236498534679413, Total Loss 66.14022827148438\n",
      "14: Encoding Loss 5.711798667907715, Transition Loss -2.6205737590789795, Classifier Loss 0.158394455909729, Total Loss 50.109188079833984\n",
      "14: Encoding Loss 5.589406490325928, Transition Loss -0.9830520749092102, Classifier Loss 0.10370095074176788, Total Loss 43.90614318847656\n",
      "14: Encoding Loss 5.5908098220825195, Transition Loss -1.790636420249939, Classifier Loss 0.14540733397006989, Total Loss 48.084877014160156\n",
      "14: Encoding Loss 3.224858045578003, Transition Loss -1.2687764167785645, Classifier Loss 0.0658750906586647, Total Loss 25.9361515045166\n",
      "14: Encoding Loss 3.8297126293182373, Transition Loss -0.41140443086624146, Classifier Loss 0.07728468626737595, Total Loss 30.706581115722656\n",
      "14: Encoding Loss 4.521786689758301, Transition Loss -0.726384162902832, Classifier Loss 0.08923933655023575, Total Loss 36.05436325073242\n",
      "14: Encoding Loss 4.46778678894043, Transition Loss -2.6938328742980957, Classifier Loss 0.06368811428546906, Total Loss 33.17445755004883\n",
      "14: Encoding Loss 5.492618560791016, Transition Loss -1.4760119915008545, Classifier Loss 0.12020862102508545, Total Loss 44.975982666015625\n",
      "14: Encoding Loss 4.9298505783081055, Transition Loss -1.2942360639572144, Classifier Loss 0.08726876974105835, Total Loss 38.30546569824219\n",
      "14: Encoding Loss 3.5054550170898438, Transition Loss -0.2778906524181366, Classifier Loss 0.08272738754749298, Total Loss 29.30535888671875\n",
      "14: Encoding Loss 4.013986587524414, Transition Loss -1.5217163562774658, Classifier Loss 0.04376111179590225, Total Loss 28.45942497253418\n",
      "14: Encoding Loss 5.898859977722168, Transition Loss -1.172406792640686, Classifier Loss 0.1213497668504715, Total Loss 47.52766799926758\n",
      "14: Encoding Loss 4.968748092651367, Transition Loss 0.13875621557235718, Classifier Loss 0.04632594808936119, Total Loss 34.500587463378906\n",
      "14: Encoding Loss 7.0689544677734375, Transition Loss 0.12266743183135986, Classifier Loss 0.07982050627470016, Total Loss 50.444847106933594\n",
      "14: Encoding Loss 6.447827339172363, Transition Loss -1.7082312107086182, Classifier Loss 0.2512814998626709, Total Loss 63.81443405151367\n",
      "14: Encoding Loss 6.099179267883301, Transition Loss -1.6265361309051514, Classifier Loss 0.05160187557339668, Total Loss 41.75461196899414\n",
      "14: Encoding Loss 5.0844526290893555, Transition Loss -1.3577536344528198, Classifier Loss 0.10475224256515503, Total Loss 40.98139953613281\n",
      "14: Encoding Loss 4.241484642028809, Transition Loss -1.2198797464370728, Classifier Loss 0.06819246709346771, Total Loss 32.267669677734375\n",
      "14: Encoding Loss 5.533987522125244, Transition Loss -2.128344774246216, Classifier Loss 0.17536890506744385, Total Loss 50.73996353149414\n",
      "14: Encoding Loss 3.602027177810669, Transition Loss -1.0361193418502808, Classifier Loss 0.05152004957199097, Total Loss 26.76375389099121\n",
      "14: Encoding Loss 6.986042022705078, Transition Loss -2.1233766078948975, Classifier Loss 0.10711364448070526, Total Loss 52.626766204833984\n",
      "14: Encoding Loss 5.625543117523193, Transition Loss -1.4539196491241455, Classifier Loss 0.13751384615898132, Total Loss 47.504066467285156\n",
      "14: Encoding Loss 6.181971549987793, Transition Loss -1.7993336915969849, Classifier Loss 0.04644957184791565, Total Loss 41.73606872558594\n",
      "14: Encoding Loss 6.5580244064331055, Transition Loss -0.8239471316337585, Classifier Loss 0.12425172328948975, Total Loss 51.77299499511719\n",
      "14: Encoding Loss 2.456657886505127, Transition Loss -1.9226305484771729, Classifier Loss 0.06113967299461365, Total Loss 20.853145599365234\n",
      "14: Encoding Loss 2.7952561378479004, Transition Loss -0.5824620723724365, Classifier Loss 0.042546067386865616, Total Loss 21.025911331176758\n",
      "14: Encoding Loss 5.579555034637451, Transition Loss -0.4347834885120392, Classifier Loss 0.09172042459249496, Total Loss 42.649200439453125\n",
      "14: Encoding Loss 3.6686203479766846, Transition Loss 0.4792175889015198, Classifier Loss 0.06692826747894287, Total Loss 28.896236419677734\n",
      "14: Encoding Loss 5.970528602600098, Transition Loss -0.9473992586135864, Classifier Loss 0.14163048565387726, Total Loss 49.985843658447266\n",
      "14: Encoding Loss 5.661942005157471, Transition Loss -1.8220139741897583, Classifier Loss 0.05113140121102333, Total Loss 39.08406448364258\n",
      "14: Encoding Loss 3.8321776390075684, Transition Loss -2.449822425842285, Classifier Loss 0.10170109570026398, Total Loss 33.16219711303711\n",
      "14: Encoding Loss 3.1839053630828857, Transition Loss -1.4821845293045044, Classifier Loss 0.05875542759895325, Total Loss 24.978384017944336\n",
      "14: Encoding Loss 6.982302665710449, Transition Loss -1.4119436740875244, Classifier Loss 0.16302065551280975, Total Loss 58.19532012939453\n",
      "14: Encoding Loss 4.070492267608643, Transition Loss -0.16801851987838745, Classifier Loss 0.15691062808036804, Total Loss 40.113948822021484\n",
      "14: Encoding Loss 4.578214645385742, Transition Loss -1.468786597251892, Classifier Loss 0.05301471799612045, Total Loss 32.770172119140625\n",
      "14: Encoding Loss 5.1040754318237305, Transition Loss 0.12982431054115295, Classifier Loss 0.1356595754623413, Total Loss 44.24234390258789\n",
      "14: Encoding Loss 3.1116862297058105, Transition Loss 0.5842319130897522, Classifier Loss 0.050573594868183136, Total Loss 23.961170196533203\n",
      "14: Encoding Loss 3.77165150642395, Transition Loss -1.3734827041625977, Classifier Loss 0.07362711429595947, Total Loss 29.9920711517334\n",
      "14: Encoding Loss 5.796188831329346, Transition Loss -0.8050056099891663, Classifier Loss 0.09091000258922577, Total Loss 43.86781311035156\n",
      "14: Encoding Loss 3.525831699371338, Transition Loss -0.921676754951477, Classifier Loss 0.11504455655813217, Total Loss 32.65907669067383\n",
      "14: Encoding Loss 6.265681743621826, Transition Loss -1.0032219886779785, Classifier Loss 0.12665154039859772, Total Loss 50.258846282958984\n",
      "14: Encoding Loss 6.6461286544799805, Transition Loss -1.172322392463684, Classifier Loss 0.1765856295824051, Total Loss 57.53487014770508\n",
      "14: Encoding Loss 4.53391695022583, Transition Loss -1.1495161056518555, Classifier Loss 0.026493359357118607, Total Loss 29.852378845214844\n",
      "14: Encoding Loss 4.663520812988281, Transition Loss -0.36183592677116394, Classifier Loss 0.15078499913215637, Total Loss 43.05948257446289\n",
      "14: Encoding Loss 5.728643894195557, Transition Loss -3.285783052444458, Classifier Loss 0.14929330348968506, Total Loss 49.29987716674805\n",
      "14: Encoding Loss 6.637319087982178, Transition Loss -1.5377130508422852, Classifier Loss 0.04356281831860542, Total Loss 44.17958450317383\n",
      "14: Encoding Loss 5.035721778869629, Transition Loss -1.20139741897583, Classifier Loss 0.05782599002122879, Total Loss 35.9964485168457\n",
      "14: Encoding Loss 6.0283331871032715, Transition Loss -2.1621861457824707, Classifier Loss 0.1393992304801941, Total Loss 50.10905838012695\n",
      "14: Encoding Loss 5.27359676361084, Transition Loss -0.3958325982093811, Classifier Loss 0.05711844936013222, Total Loss 37.353267669677734\n",
      "14: Encoding Loss 2.9916012287139893, Transition Loss -1.5274319648742676, Classifier Loss 0.07034019380807877, Total Loss 24.983016967773438\n",
      "14: Encoding Loss 4.9113359451293945, Transition Loss -1.841733694076538, Classifier Loss 0.10305538773536682, Total Loss 39.77281951904297\n",
      "14: Encoding Loss 9.240086555480957, Transition Loss -3.1372809410095215, Classifier Loss 0.1589149832725525, Total Loss 71.33076477050781\n",
      "14: Encoding Loss 4.972851276397705, Transition Loss -1.308535099029541, Classifier Loss 0.04629620537161827, Total Loss 34.46620559692383\n",
      "14: Encoding Loss 5.263332843780518, Transition Loss -0.9302761554718018, Classifier Loss 0.03832131624221802, Total Loss 35.4117546081543\n",
      "14: Encoding Loss 5.889264106750488, Transition Loss -0.19955532252788544, Classifier Loss 0.14865149557590485, Total Loss 50.20065689086914\n",
      "14: Encoding Loss 6.050368309020996, Transition Loss -0.751882016658783, Classifier Loss 0.25358715653419495, Total Loss 61.66062545776367\n",
      "14: Encoding Loss 6.260870456695557, Transition Loss -1.530704379081726, Classifier Loss 0.17500559985637665, Total Loss 55.06517028808594\n",
      "14: Encoding Loss 6.345477104187012, Transition Loss -2.056387424468994, Classifier Loss 0.14403489232063293, Total Loss 52.475528717041016\n",
      "14: Encoding Loss 5.218616008758545, Transition Loss -1.4016764163970947, Classifier Loss 0.10660858452320099, Total Loss 41.97199630737305\n",
      "14: Encoding Loss 5.26234769821167, Transition Loss -0.5158333778381348, Classifier Loss 0.13007599115371704, Total Loss 44.58148193359375\n",
      "14: Encoding Loss 6.813625335693359, Transition Loss -0.7386017441749573, Classifier Loss 0.1567721664905548, Total Loss 56.55867385864258\n",
      "14: Encoding Loss 4.7218918800354, Transition Loss -0.4980579912662506, Classifier Loss 0.05285939574241638, Total Loss 33.61709213256836\n",
      "14: Encoding Loss 7.529347896575928, Transition Loss -1.9374349117279053, Classifier Loss 0.06546426564455032, Total Loss 51.721744537353516\n",
      "14: Encoding Loss 4.523780822753906, Transition Loss -0.9689714908599854, Classifier Loss 0.0969293937087059, Total Loss 36.835235595703125\n",
      "14: Encoding Loss 4.156080722808838, Transition Loss -1.6171209812164307, Classifier Loss 0.07952053099870682, Total Loss 32.88788986206055\n",
      "14: Encoding Loss 5.445261001586914, Transition Loss -0.3831942677497864, Classifier Loss 0.16084226965904236, Total Loss 48.755638122558594\n",
      "14: Encoding Loss 4.7132487297058105, Transition Loss -1.3731173276901245, Classifier Loss 0.06264856457710266, Total Loss 34.543800354003906\n",
      "14: Encoding Loss 4.944786548614502, Transition Loss -1.3862303495407104, Classifier Loss 0.13323843479156494, Total Loss 42.992008209228516\n",
      "14: Encoding Loss 10.827061653137207, Transition Loss -2.067730188369751, Classifier Loss 0.11724347621202469, Total Loss 76.68589782714844\n",
      "14: Encoding Loss 5.400421142578125, Transition Loss -0.6705210208892822, Classifier Loss 0.0575273260474205, Total Loss 38.154991149902344\n",
      "14: Encoding Loss 6.714923858642578, Transition Loss 0.780148983001709, Classifier Loss 0.0658750981092453, Total Loss 47.18911361694336\n",
      "14: Encoding Loss 4.451411247253418, Transition Loss -2.1839189529418945, Classifier Loss 0.10739193111658096, Total Loss 37.4467887878418\n",
      "14: Encoding Loss 5.907374382019043, Transition Loss -1.473222255706787, Classifier Loss 0.1056903675198555, Total Loss 46.0126953125\n",
      "14: Encoding Loss 3.520331859588623, Transition Loss -0.5599628686904907, Classifier Loss 0.0801142007112503, Total Loss 29.133188247680664\n",
      "14: Encoding Loss 5.467859268188477, Transition Loss -1.3038872480392456, Classifier Loss 0.18633683025836945, Total Loss 51.44031524658203\n",
      "14: Encoding Loss 9.03474235534668, Transition Loss -1.2650045156478882, Classifier Loss 0.10637684166431427, Total Loss 64.84564208984375\n",
      "14: Encoding Loss 4.4766693115234375, Transition Loss -1.698342204093933, Classifier Loss 0.07876214385032654, Total Loss 34.73555374145508\n",
      "14: Encoding Loss 5.392247200012207, Transition Loss -1.0545655488967896, Classifier Loss 0.06565386801958084, Total Loss 38.91844940185547\n",
      "14: Encoding Loss 5.707210063934326, Transition Loss -2.019449234008789, Classifier Loss 0.04077032953500748, Total Loss 38.319488525390625\n",
      "14: Encoding Loss 5.209734916687012, Transition Loss -2.1142678260803223, Classifier Loss 0.03510039299726486, Total Loss 34.76760482788086\n",
      "14: Encoding Loss 4.395716667175293, Transition Loss -1.026401162147522, Classifier Loss 0.026802457869052887, Total Loss 29.05413818359375\n",
      "14: Encoding Loss 4.1393866539001465, Transition Loss -2.3944718837738037, Classifier Loss 0.09338563680648804, Total Loss 34.173927307128906\n",
      "14: Encoding Loss 6.140264987945557, Transition Loss -0.5636623501777649, Classifier Loss 0.1007905900478363, Total Loss 46.92042541503906\n",
      "14: Encoding Loss 5.471087455749512, Transition Loss -0.7135000228881836, Classifier Loss 0.058179356157779694, Total Loss 38.6441764831543\n",
      "14: Encoding Loss 5.443220615386963, Transition Loss -3.0112040042877197, Classifier Loss 0.08916457742452621, Total Loss 41.57457733154297\n",
      "14: Encoding Loss 6.619584083557129, Transition Loss -2.9988417625427246, Classifier Loss 0.10973452031612396, Total Loss 50.68975830078125\n",
      "14: Encoding Loss 4.344749450683594, Transition Loss 0.07307934761047363, Classifier Loss 0.06325849145650864, Total Loss 32.423580169677734\n",
      "14: Encoding Loss 4.748766899108887, Transition Loss -1.837907314300537, Classifier Loss 0.13443730771541595, Total Loss 41.93559646606445\n",
      "14: Encoding Loss 5.072349548339844, Transition Loss -1.3945426940917969, Classifier Loss 0.06814973056316376, Total Loss 37.24851608276367\n",
      "14: Encoding Loss 3.6033473014831543, Transition Loss -1.8278796672821045, Classifier Loss 0.06127425655722618, Total Loss 27.746780395507812\n",
      "14: Encoding Loss 5.124273777008057, Transition Loss -2.7015459537506104, Classifier Loss 0.12865711748600006, Total Loss 43.61027526855469\n",
      "14: Encoding Loss 6.290200710296631, Transition Loss -2.055919885635376, Classifier Loss 0.08479901403188705, Total Loss 46.22028350830078\n",
      "14: Encoding Loss 6.4656476974487305, Transition Loss -2.7614264488220215, Classifier Loss 0.10203437507152557, Total Loss 48.996219635009766\n",
      "14: Encoding Loss 7.235381126403809, Transition Loss -1.092984914779663, Classifier Loss 0.14776992797851562, Total Loss 58.1888427734375\n",
      "14: Encoding Loss 4.979165077209473, Transition Loss 0.4048665165901184, Classifier Loss 0.08905136585235596, Total Loss 38.942073822021484\n",
      "14: Encoding Loss 4.765881538391113, Transition Loss -2.5688235759735107, Classifier Loss 0.06848989427089691, Total Loss 35.44325256347656\n",
      "14: Encoding Loss 5.056954860687256, Transition Loss -1.7128299474716187, Classifier Loss 0.09260901808738708, Total Loss 39.60194396972656\n",
      "14: Encoding Loss 4.411774635314941, Transition Loss -1.6458005905151367, Classifier Loss 0.0725724995136261, Total Loss 33.727237701416016\n",
      "14: Encoding Loss 4.2927446365356445, Transition Loss -1.9005615711212158, Classifier Loss 0.08272629976272583, Total Loss 34.02833938598633\n",
      "14: Encoding Loss 3.3800036907196045, Transition Loss -2.1484460830688477, Classifier Loss 0.046727053821086884, Total Loss 24.951868057250977\n",
      "14: Encoding Loss 3.554716110229492, Transition Loss -0.6960470676422119, Classifier Loss 0.09173641353845596, Total Loss 30.501659393310547\n",
      "14: Encoding Loss 5.0201616287231445, Transition Loss -0.8957479000091553, Classifier Loss 0.110139861702919, Total Loss 41.13459777832031\n",
      "14: Encoding Loss 6.111567974090576, Transition Loss -0.34749072790145874, Classifier Loss 0.09108695387840271, Total Loss 45.77796936035156\n",
      "14: Encoding Loss 3.8626620769500732, Transition Loss -0.5752666592597961, Classifier Loss 0.09934200346469879, Total Loss 33.109947204589844\n",
      "14: Encoding Loss 6.968235969543457, Transition Loss -1.1654242277145386, Classifier Loss 0.10865119099617004, Total Loss 52.674072265625\n",
      "14: Encoding Loss 6.343285083770752, Transition Loss -0.7551643252372742, Classifier Loss 0.10983066260814667, Total Loss 49.042476654052734\n",
      "14: Encoding Loss 4.687883377075195, Transition Loss -1.662785291671753, Classifier Loss 0.12820732593536377, Total Loss 40.94737243652344\n",
      "14: Encoding Loss 5.0925092697143555, Transition Loss -0.539235532283783, Classifier Loss 0.10784702003002167, Total Loss 41.339542388916016\n",
      "14: Encoding Loss 7.5327558517456055, Transition Loss -2.1225123405456543, Classifier Loss 0.1525106132030487, Total Loss 60.446746826171875\n",
      "14: Encoding Loss 4.81402587890625, Transition Loss -1.7698123455047607, Classifier Loss 0.04331182688474655, Total Loss 33.214630126953125\n",
      "14: Encoding Loss 6.75472354888916, Transition Loss -0.9061511158943176, Classifier Loss 0.18148651719093323, Total Loss 58.676631927490234\n",
      "14: Encoding Loss 5.772190093994141, Transition Loss -0.4317625164985657, Classifier Loss 0.12960262596607208, Total Loss 47.593231201171875\n",
      "14: Encoding Loss 4.97802209854126, Transition Loss -1.7387619018554688, Classifier Loss 0.10938852280378342, Total Loss 40.80628967285156\n",
      "14: Encoding Loss 4.502296447753906, Transition Loss -2.626612901687622, Classifier Loss 0.12609370052814484, Total Loss 39.622100830078125\n",
      "14: Encoding Loss 6.340746879577637, Transition Loss -2.0869081020355225, Classifier Loss 0.12205875664949417, Total Loss 50.2495231628418\n",
      "14: Encoding Loss 4.304232597351074, Transition Loss -0.7904782295227051, Classifier Loss 0.08010361343622208, Total Loss 33.83544158935547\n",
      "14: Encoding Loss 3.0337958335876465, Transition Loss -1.2011327743530273, Classifier Loss 0.0618659183382988, Total Loss 24.388887405395508\n",
      "14: Encoding Loss 2.5237205028533936, Transition Loss -1.6477584838867188, Classifier Loss 0.06247100234031677, Total Loss 21.388763427734375\n",
      "14: Encoding Loss 3.5375797748565674, Transition Loss -1.3688985109329224, Classifier Loss 0.08367114514112473, Total Loss 29.5920467376709\n",
      "14: Encoding Loss 8.8300142288208, Transition Loss 0.7362227439880371, Classifier Loss 0.09949861466884613, Total Loss 63.22443771362305\n",
      "14: Encoding Loss 5.827678680419922, Transition Loss 0.2052052617073059, Classifier Loss 0.13836324214935303, Total Loss 48.88447952270508\n",
      "14: Encoding Loss 4.037664413452148, Transition Loss 0.6576821208000183, Classifier Loss 0.05946201831102371, Total Loss 30.43526268005371\n",
      "14: Encoding Loss 7.4864501953125, Transition Loss -0.4565693736076355, Classifier Loss 0.2378309667110443, Total Loss 68.70161437988281\n",
      "14: Encoding Loss 4.420271873474121, Transition Loss -1.5703922510147095, Classifier Loss 0.049071960151195526, Total Loss 31.42820167541504\n",
      "14: Encoding Loss 4.492804050445557, Transition Loss -2.128779411315918, Classifier Loss 0.09852241724729538, Total Loss 36.8082160949707\n",
      "14: Encoding Loss 5.748208522796631, Transition Loss -1.1875663995742798, Classifier Loss 0.12558670341968536, Total Loss 47.047447204589844\n",
      "14: Encoding Loss 4.950411319732666, Transition Loss -1.3757400512695312, Classifier Loss 0.06215705722570419, Total Loss 35.917625427246094\n",
      "14: Encoding Loss 4.6439690589904785, Transition Loss -0.7182886004447937, Classifier Loss 0.053545787930488586, Total Loss 33.218109130859375\n",
      "14: Encoding Loss 5.961403846740723, Transition Loss -0.7761818170547485, Classifier Loss 0.09045823663473129, Total Loss 44.81393814086914\n",
      "14: Encoding Loss 3.6241135597229004, Transition Loss 0.1686726212501526, Classifier Loss 0.041208233684301376, Total Loss 25.932973861694336\n",
      "14: Encoding Loss 7.0064377784729, Transition Loss 0.5744321346282959, Classifier Loss 0.0811147466301918, Total Loss 50.37987518310547\n",
      "14: Encoding Loss 5.363378524780273, Transition Loss -1.7907346487045288, Classifier Loss 0.10175547748804092, Total Loss 42.3551025390625\n",
      "14: Encoding Loss 4.109875202178955, Transition Loss -1.538218379020691, Classifier Loss 0.12683513760566711, Total Loss 37.3421516418457\n",
      "14: Encoding Loss 6.416028022766113, Transition Loss -1.933225393295288, Classifier Loss 0.051696717739105225, Total Loss 43.66506576538086\n",
      "14: Encoding Loss 3.535343647003174, Transition Loss -1.0519180297851562, Classifier Loss 0.10971687734127045, Total Loss 32.18333053588867\n",
      "14: Encoding Loss 2.173590898513794, Transition Loss -1.6607398986816406, Classifier Loss 0.07784377038478851, Total Loss 20.825260162353516\n",
      "14: Encoding Loss 2.2338929176330566, Transition Loss -1.071474313735962, Classifier Loss 0.04549696296453476, Total Loss 17.952625274658203\n",
      "14: Encoding Loss 8.458724021911621, Transition Loss -2.060609817504883, Classifier Loss 0.1128346249461174, Total Loss 62.03498458862305\n",
      "14: Encoding Loss 4.0738139152526855, Transition Loss -1.7024171352386475, Classifier Loss 0.04614710807800293, Total Loss 29.056913375854492\n",
      "14: Encoding Loss 2.420689105987549, Transition Loss -1.927823543548584, Classifier Loss 0.06947936117649078, Total Loss 21.47130012512207\n",
      "14: Encoding Loss 6.256478309631348, Transition Loss -1.2599732875823975, Classifier Loss 0.1140194907784462, Total Loss 48.94031524658203\n",
      "14: Encoding Loss 3.110807418823242, Transition Loss -0.9187728762626648, Classifier Loss 0.05620719492435455, Total Loss 24.28519630432129\n",
      "14: Encoding Loss 5.3411545753479, Transition Loss -0.6438720226287842, Classifier Loss 0.09206721186637878, Total Loss 41.25339126586914\n",
      "14: Encoding Loss 5.531018257141113, Transition Loss -0.5036883354187012, Classifier Loss 0.06129151210188866, Total Loss 39.315059661865234\n",
      "14: Encoding Loss 5.227668762207031, Transition Loss -2.7610888481140137, Classifier Loss 0.10798028111457825, Total Loss 42.16293716430664\n",
      "14: Encoding Loss 5.2746100425720215, Transition Loss -0.40544384717941284, Classifier Loss 0.12129426002502441, Total Loss 43.77692413330078\n",
      "14: Encoding Loss 5.069823741912842, Transition Loss -1.5463355779647827, Classifier Loss 0.1501927673816681, Total Loss 45.43760299682617\n",
      "14: Encoding Loss 4.50376558303833, Transition Loss -2.5466556549072266, Classifier Loss 0.08751583844423294, Total Loss 35.77315902709961\n",
      "14: Encoding Loss 3.4964847564697266, Transition Loss -1.187144160270691, Classifier Loss 0.037730492651462555, Total Loss 24.751482009887695\n",
      "14: Encoding Loss 7.30289363861084, Transition Loss -1.1735919713974, Classifier Loss 0.14146284759044647, Total Loss 57.96318054199219\n",
      "14: Encoding Loss 6.470433235168457, Transition Loss -0.9243588447570801, Classifier Loss 0.13897928595542908, Total Loss 52.720157623291016\n",
      "14: Encoding Loss 5.169232368469238, Transition Loss -1.2088615894317627, Classifier Loss 0.1346191167831421, Total Loss 44.47682189941406\n",
      "14: Encoding Loss 3.5931243896484375, Transition Loss -1.7572407722473145, Classifier Loss 0.0547325499355793, Total Loss 27.03129768371582\n",
      "14: Encoding Loss 9.614442825317383, Transition Loss -1.0984456539154053, Classifier Loss 0.17179463803768158, Total Loss 74.86567687988281\n",
      "14: Encoding Loss 5.232512950897217, Transition Loss -2.4758107662200928, Classifier Loss 0.05252184718847275, Total Loss 36.646270751953125\n",
      "14: Encoding Loss 4.71293830871582, Transition Loss -1.8423892259597778, Classifier Loss 0.13269831240177155, Total Loss 41.54672622680664\n",
      "14: Encoding Loss 6.365647315979004, Transition Loss -1.9929358959197998, Classifier Loss 0.1712844967842102, Total Loss 55.321537017822266\n",
      "14: Encoding Loss 6.423555850982666, Transition Loss -1.4546180963516235, Classifier Loss 0.1233256459236145, Total Loss 50.87331771850586\n",
      "14: Encoding Loss 5.347340106964111, Transition Loss -0.6732611656188965, Classifier Loss 0.08010362833738327, Total Loss 40.09413528442383\n",
      "14: Encoding Loss 7.1486101150512695, Transition Loss -2.6213793754577637, Classifier Loss 0.07864612340927124, Total Loss 50.755226135253906\n",
      "14: Encoding Loss 7.282088279724121, Transition Loss -1.3221195936203003, Classifier Loss 0.17576387524604797, Total Loss 61.26839065551758\n",
      "14: Encoding Loss 7.448876857757568, Transition Loss -2.1039388179779053, Classifier Loss 0.0546148344874382, Total Loss 50.15390396118164\n",
      "14: Encoding Loss 6.476907253265381, Transition Loss -0.5739253163337708, Classifier Loss 0.19435852766036987, Total Loss 58.29706954956055\n",
      "14: Encoding Loss 5.9966816902160645, Transition Loss -1.829131007194519, Classifier Loss 0.05503999814391136, Total Loss 41.483360290527344\n",
      "14: Encoding Loss 5.225259780883789, Transition Loss -1.2754985094070435, Classifier Loss 0.05543987452983856, Total Loss 36.89503479003906\n",
      "14: Encoding Loss 4.168877124786377, Transition Loss -2.4258370399475098, Classifier Loss 0.056595154106616974, Total Loss 30.67180824279785\n",
      "14: Encoding Loss 4.205521106719971, Transition Loss -0.9621263146400452, Classifier Loss 0.09477667510509491, Total Loss 34.710411071777344\n",
      "14: Encoding Loss 3.9230666160583496, Transition Loss -2.8654191493988037, Classifier Loss 0.06835643947124481, Total Loss 30.37289810180664\n",
      "14: Encoding Loss 7.23061466217041, Transition Loss -0.8201061487197876, Classifier Loss 0.0932883620262146, Total Loss 52.712196350097656\n",
      "14: Encoding Loss 4.101942539215088, Transition Loss -1.6912859678268433, Classifier Loss 0.09875115752220154, Total Loss 34.4860954284668\n",
      "14: Encoding Loss 2.8825929164886475, Transition Loss -1.1976497173309326, Classifier Loss 0.034653063863515854, Total Loss 20.760385513305664\n",
      "14: Encoding Loss 6.726005554199219, Transition Loss -2.0957911014556885, Classifier Loss 0.06229819357395172, Total Loss 46.58501434326172\n",
      "14: Encoding Loss 7.764688968658447, Transition Loss -1.3339200019836426, Classifier Loss 0.07411396503448486, Total Loss 53.99899673461914\n",
      "14: Encoding Loss 5.540679931640625, Transition Loss -0.5337380766868591, Classifier Loss 0.11883347481489182, Total Loss 45.12721252441406\n",
      "14: Encoding Loss 5.878470420837402, Transition Loss -1.6792011260986328, Classifier Loss 0.03429410606622696, Total Loss 38.699562072753906\n",
      "14: Encoding Loss 4.881087303161621, Transition Loss -0.9232385754585266, Classifier Loss 0.10629846900701523, Total Loss 39.9160041809082\n",
      "14: Encoding Loss 5.436431407928467, Transition Loss -1.5848267078399658, Classifier Loss 0.07835841178894043, Total Loss 40.453800201416016\n",
      "14: Encoding Loss 5.73524284362793, Transition Loss -1.0942113399505615, Classifier Loss 0.09103892743587494, Total Loss 43.51491165161133\n",
      "14: Encoding Loss 4.093496322631836, Transition Loss -0.8083962202072144, Classifier Loss 0.07406263053417206, Total Loss 31.9669189453125\n",
      "14: Encoding Loss 6.225398540496826, Transition Loss -1.9009122848510742, Classifier Loss 0.1402348428964615, Total Loss 51.375118255615234\n",
      "14: Encoding Loss 3.827639102935791, Transition Loss -1.000015139579773, Classifier Loss 0.06115121766924858, Total Loss 29.080556869506836\n",
      "14: Encoding Loss 4.995819091796875, Transition Loss -0.3609827160835266, Classifier Loss 0.03788009285926819, Total Loss 33.762779235839844\n",
      "14: Encoding Loss 7.111575126647949, Transition Loss -1.3034688234329224, Classifier Loss 0.18881024420261383, Total Loss 61.54995346069336\n",
      "14: Encoding Loss 5.089471340179443, Transition Loss -0.8232808113098145, Classifier Loss 0.04422803968191147, Total Loss 34.95930480957031\n",
      "14: Encoding Loss 5.614308834075928, Transition Loss -3.6076011657714844, Classifier Loss 0.07420770078897476, Total Loss 41.10518264770508\n",
      "14: Encoding Loss 4.460190773010254, Transition Loss -1.6299784183502197, Classifier Loss 0.04231508821249008, Total Loss 30.992000579833984\n",
      "14: Encoding Loss 5.354678153991699, Transition Loss -0.2856380343437195, Classifier Loss 0.14906981587409973, Total Loss 47.03493881225586\n",
      "14: Encoding Loss 3.697219133377075, Transition Loss -1.3013843297958374, Classifier Loss 0.08370840549468994, Total Loss 30.553634643554688\n",
      "14: Encoding Loss 7.71630334854126, Transition Loss -1.8719208240509033, Classifier Loss 0.14453983306884766, Total Loss 60.75105667114258\n",
      "14: Encoding Loss 4.79069185256958, Transition Loss -0.6972559690475464, Classifier Loss 0.053125619888305664, Total Loss 34.056434631347656\n",
      "14: Encoding Loss 4.963444709777832, Transition Loss -2.530010938644409, Classifier Loss 0.09186158329248428, Total Loss 38.965816497802734\n",
      "14: Encoding Loss 4.745180606842041, Transition Loss -2.883084774017334, Classifier Loss 0.12080927193164825, Total Loss 40.55085754394531\n",
      "14: Encoding Loss 4.708779811859131, Transition Loss -1.5360394716262817, Classifier Loss 0.111677385866642, Total Loss 39.419803619384766\n",
      "14: Encoding Loss 5.632805824279785, Transition Loss -1.2486984729766846, Classifier Loss 0.08760615438222885, Total Loss 42.55695343017578\n",
      "14: Encoding Loss 5.15070915222168, Transition Loss -1.867234230041504, Classifier Loss 0.15500302612781525, Total Loss 46.403812408447266\n",
      "14: Encoding Loss 6.015410423278809, Transition Loss -1.251164436340332, Classifier Loss 0.03193289786577225, Total Loss 39.285255432128906\n",
      "14: Encoding Loss 4.822439670562744, Transition Loss 0.4229680001735687, Classifier Loss 0.05333906039595604, Total Loss 34.4377326965332\n",
      "14: Encoding Loss 5.805110931396484, Transition Loss -1.8631647825241089, Classifier Loss 0.05013597384095192, Total Loss 39.8435173034668\n",
      "14: Encoding Loss 4.243436336517334, Transition Loss -1.52522611618042, Classifier Loss 0.0628335177898407, Total Loss 31.74336051940918\n",
      "14: Encoding Loss 6.397906303405762, Transition Loss -0.4138563871383667, Classifier Loss 0.09913913160562515, Total Loss 48.30118942260742\n",
      "14: Encoding Loss 4.45496940612793, Transition Loss -2.2874770164489746, Classifier Loss 0.03719676285982132, Total Loss 30.448579788208008\n",
      "14: Encoding Loss 7.204203128814697, Transition Loss -1.598501443862915, Classifier Loss 0.21284404397010803, Total Loss 64.50898742675781\n",
      "14: Encoding Loss 5.469484806060791, Transition Loss -1.1642296314239502, Classifier Loss 0.12060599774122238, Total Loss 44.877044677734375\n",
      "14: Encoding Loss 5.222466468811035, Transition Loss -1.7931745052337646, Classifier Loss 0.05272332951426506, Total Loss 36.606414794921875\n",
      "14: Encoding Loss 5.060568809509277, Transition Loss 0.03559607267379761, Classifier Loss 0.05934305489063263, Total Loss 36.31196212768555\n",
      "14: Encoding Loss 5.314786434173584, Transition Loss -1.1149523258209229, Classifier Loss 0.056288376450538635, Total Loss 37.51710891723633\n",
      "14: Encoding Loss 5.262709617614746, Transition Loss -0.27652013301849365, Classifier Loss 0.1146845817565918, Total Loss 43.04460525512695\n",
      "14: Encoding Loss 7.25253963470459, Transition Loss -0.3825327157974243, Classifier Loss 0.045762501657009125, Total Loss 48.091339111328125\n",
      "14: Encoding Loss 4.475201606750488, Transition Loss -1.7457218170166016, Classifier Loss 0.17163270711898804, Total Loss 44.0137825012207\n",
      "14: Encoding Loss 5.117842674255371, Transition Loss -0.31260696053504944, Classifier Loss 0.0624966099858284, Total Loss 36.95659255981445\n",
      "14: Encoding Loss 4.051280498504639, Transition Loss -0.434836208820343, Classifier Loss 0.09176797419786453, Total Loss 33.48430633544922\n",
      "14: Encoding Loss 4.793214797973633, Transition Loss -1.738759994506836, Classifier Loss 0.03409513831138611, Total Loss 32.16810989379883\n",
      "14: Encoding Loss 4.618471145629883, Transition Loss -2.054534912109375, Classifier Loss 0.1648823320865631, Total Loss 44.1982421875\n",
      "14: Encoding Loss 4.872785568237305, Transition Loss -2.055598735809326, Classifier Loss 0.14435243606567383, Total Loss 43.67113494873047\n",
      "14: Encoding Loss 3.6467018127441406, Transition Loss -2.776455879211426, Classifier Loss 0.06550700962543488, Total Loss 28.42980194091797\n",
      "14: Encoding Loss 3.8381049633026123, Transition Loss -0.545483410358429, Classifier Loss 0.10956710577011108, Total Loss 33.98512649536133\n",
      "14: Encoding Loss 5.543985366821289, Transition Loss -0.49331074953079224, Classifier Loss 0.11747433990240097, Total Loss 45.011146545410156\n",
      "14: Encoding Loss 3.275285243988037, Transition Loss -2.1632847785949707, Classifier Loss 0.05199538171291351, Total Loss 24.850383758544922\n",
      "14: Encoding Loss 5.229005813598633, Transition Loss 0.1826545000076294, Classifier Loss 0.032939717173576355, Total Loss 34.74106979370117\n",
      "14: Encoding Loss 5.463955879211426, Transition Loss -2.147578001022339, Classifier Loss 0.0799601599574089, Total Loss 40.77889633178711\n",
      "14: Encoding Loss 4.198040962219238, Transition Loss -0.8628139495849609, Classifier Loss 0.05382272228598595, Total Loss 30.570173263549805\n",
      "14: Encoding Loss 7.938690185546875, Transition Loss 0.5403292179107666, Classifier Loss 0.23469306528568268, Total Loss 71.31758117675781\n",
      "14: Encoding Loss 5.677432060241699, Transition Loss -1.6059095859527588, Classifier Loss 0.050939664244651794, Total Loss 39.157920837402344\n",
      "14: Encoding Loss 6.886927127838135, Transition Loss 0.4097048044204712, Classifier Loss 0.06912370026111603, Total Loss 48.3978157043457\n",
      "14: Encoding Loss 5.072569847106934, Transition Loss -1.6794095039367676, Classifier Loss 0.1582261323928833, Total Loss 46.257362365722656\n",
      "14: Encoding Loss 3.86664080619812, Transition Loss -1.1533808708190918, Classifier Loss 0.08525505661964417, Total Loss 31.724891662597656\n",
      "14: Encoding Loss 6.42641544342041, Transition Loss -1.640153408050537, Classifier Loss 0.11828046292066574, Total Loss 50.38588333129883\n",
      "14: Encoding Loss 8.529553413391113, Transition Loss -2.7687854766845703, Classifier Loss 0.17003387212753296, Total Loss 68.17960357666016\n",
      "14: Encoding Loss 6.970728874206543, Transition Loss -1.4188623428344727, Classifier Loss 0.2770991325378418, Total Loss 69.53372192382812\n",
      "14: Encoding Loss 4.14216947555542, Transition Loss -1.4205052852630615, Classifier Loss 0.09957696497440338, Total Loss 34.81014633178711\n",
      "14: Encoding Loss 7.663392066955566, Transition Loss 0.2449968308210373, Classifier Loss 0.07965780049562454, Total Loss 54.04413604736328\n",
      "14: Encoding Loss 9.4312105178833, Transition Loss -1.6319265365600586, Classifier Loss 0.3044027090072632, Total Loss 87.0268783569336\n",
      "14: Encoding Loss 6.571993827819824, Transition Loss -0.8861942291259766, Classifier Loss 0.11146380752325058, Total Loss 50.5779914855957\n",
      "14: Encoding Loss 7.087725639343262, Transition Loss -0.27170392870903015, Classifier Loss 0.1197725236415863, Total Loss 54.503501892089844\n",
      "14: Encoding Loss 7.251741886138916, Transition Loss -0.499186635017395, Classifier Loss 0.09049732983112335, Total Loss 52.55998611450195\n",
      "14: Encoding Loss 5.340427875518799, Transition Loss -2.0365591049194336, Classifier Loss 0.08981166779994965, Total Loss 41.022918701171875\n",
      "14: Encoding Loss 4.904032230377197, Transition Loss -1.8299453258514404, Classifier Loss 0.1157100647687912, Total Loss 40.994468688964844\n",
      "14: Encoding Loss 5.997389793395996, Transition Loss -3.2965691089630127, Classifier Loss 0.1265573650598526, Total Loss 48.638755798339844\n",
      "14: Encoding Loss 4.746392250061035, Transition Loss -2.1938915252685547, Classifier Loss 0.07010933756828308, Total Loss 35.48841094970703\n",
      "14: Encoding Loss 4.289680480957031, Transition Loss -1.2472379207611084, Classifier Loss 0.045511770993471146, Total Loss 30.288761138916016\n",
      "14: Encoding Loss 5.000959396362305, Transition Loss -1.7549203634262085, Classifier Loss 0.16163837909698486, Total Loss 46.16889190673828\n",
      "14: Encoding Loss 3.109541654586792, Transition Loss -1.8772180080413818, Classifier Loss 0.07753343135118484, Total Loss 26.40984344482422\n",
      "14: Encoding Loss 4.07888650894165, Transition Loss -1.4385249614715576, Classifier Loss 0.11461149901151657, Total Loss 35.933895111083984\n",
      "14: Encoding Loss 6.436288356781006, Transition Loss -1.5933798551559448, Classifier Loss 0.19218474626541138, Total Loss 57.8355712890625\n",
      "14: Encoding Loss 4.9921746253967285, Transition Loss -0.426317423582077, Classifier Loss 0.14744432270526886, Total Loss 44.69730758666992\n",
      "14: Encoding Loss 3.9055233001708984, Transition Loss -1.9097659587860107, Classifier Loss 0.11165106296539307, Total Loss 34.59748077392578\n",
      "14: Encoding Loss 5.531448841094971, Transition Loss -2.137629270553589, Classifier Loss 0.06355851143598557, Total Loss 39.5436897277832\n",
      "14: Encoding Loss 3.796910524368286, Transition Loss -0.6275188326835632, Classifier Loss 0.04529488831758499, Total Loss 27.310701370239258\n",
      "14: Encoding Loss 5.506396293640137, Transition Loss -1.2898612022399902, Classifier Loss 0.03415647894144058, Total Loss 36.453514099121094\n",
      "14: Encoding Loss 4.338505744934082, Transition Loss -1.3423068523406982, Classifier Loss 0.043443888425827026, Total Loss 30.37488555908203\n",
      "14: Encoding Loss 3.885218858718872, Transition Loss -0.5102747082710266, Classifier Loss 0.11143280565738678, Total Loss 34.45438766479492\n",
      "14: Encoding Loss 5.621789932250977, Transition Loss -0.7168315649032593, Classifier Loss 0.1261516660451889, Total Loss 46.345619201660156\n",
      "14: Encoding Loss 6.734112739562988, Transition Loss -0.10006564855575562, Classifier Loss 0.12835080921649933, Total Loss 53.239723205566406\n",
      "14: Encoding Loss 6.775725364685059, Transition Loss -0.7187124490737915, Classifier Loss 0.0943623036146164, Total Loss 50.09029769897461\n",
      "14: Encoding Loss 3.115236759185791, Transition Loss -2.2540955543518066, Classifier Loss 0.08206761628389359, Total Loss 26.897279739379883\n",
      "14: Encoding Loss 6.981077671051025, Transition Loss -1.3543347120285034, Classifier Loss 0.14534597098827362, Total Loss 56.42052459716797\n",
      "14: Encoding Loss 4.394667625427246, Transition Loss -2.472569465637207, Classifier Loss 0.09498845040798187, Total Loss 35.86586380004883\n",
      "14: Encoding Loss 5.747100353240967, Transition Loss -0.9938763976097107, Classifier Loss 0.08038992434740067, Total Loss 42.521202087402344\n",
      "14: Encoding Loss 7.245579719543457, Transition Loss -1.9559497833251953, Classifier Loss 0.18422581255435944, Total Loss 61.89527893066406\n",
      "14: Encoding Loss 5.506892204284668, Transition Loss -1.9926806688308716, Classifier Loss 0.13362251222133636, Total Loss 46.402809143066406\n",
      "14: Encoding Loss 7.384706974029541, Transition Loss -0.1905207484960556, Classifier Loss 0.13819734752178192, Total Loss 58.127899169921875\n",
      "14: Encoding Loss 4.697509288787842, Transition Loss 0.10746574401855469, Classifier Loss 0.08676955103874207, Total Loss 36.904998779296875\n",
      "14: Encoding Loss 4.930956840515137, Transition Loss -1.9104576110839844, Classifier Loss 0.0797886773943901, Total Loss 37.563846588134766\n",
      "14: Encoding Loss 7.056663513183594, Transition Loss -0.7294988632202148, Classifier Loss 0.12409280240535736, Total Loss 54.74897003173828\n",
      "14: Encoding Loss 6.096863269805908, Transition Loss -1.3823562860488892, Classifier Loss 0.10193607211112976, Total Loss 46.774234771728516\n",
      "14: Encoding Loss 5.817008018493652, Transition Loss -0.7854722142219543, Classifier Loss 0.10978788137435913, Total Loss 45.880523681640625\n",
      "14: Encoding Loss 4.027929782867432, Transition Loss 0.13767585158348083, Classifier Loss 0.10833930224180222, Total Loss 35.05657958984375\n",
      "14: Encoding Loss 3.2967426776885986, Transition Loss -0.781766414642334, Classifier Loss 0.04678257182240486, Total Loss 24.45840072631836\n",
      "14: Encoding Loss 5.77451229095459, Transition Loss -1.6899420022964478, Classifier Loss 0.07133614271879196, Total Loss 41.78001403808594\n",
      "14: Encoding Loss 4.8830976486206055, Transition Loss -1.214493751525879, Classifier Loss 0.09208501130342484, Total Loss 38.5066032409668\n",
      "14: Encoding Loss 4.600285530090332, Transition Loss -0.6786956191062927, Classifier Loss 0.17942674458026886, Total Loss 45.54411697387695\n",
      "14: Encoding Loss 4.040762901306152, Transition Loss -2.156876802444458, Classifier Loss 0.05151832103729248, Total Loss 29.395549774169922\n",
      "14: Encoding Loss 4.518878936767578, Transition Loss -2.616180896759033, Classifier Loss 0.11997196823358536, Total Loss 39.10942840576172\n",
      "14: Encoding Loss 1.8225886821746826, Transition Loss -0.6312475204467773, Classifier Loss 0.057823143899440765, Total Loss 16.717594146728516\n",
      "14: Encoding Loss 5.019167900085449, Transition Loss -0.7251936197280884, Classifier Loss 0.04758523032069206, Total Loss 34.87324142456055\n",
      "14: Encoding Loss 4.575109958648682, Transition Loss -1.5061372518539429, Classifier Loss 0.12488506734371185, Total Loss 39.93856430053711\n",
      "14: Encoding Loss 3.8712615966796875, Transition Loss -0.6635735034942627, Classifier Loss 0.05485205724835396, Total Loss 28.71251106262207\n",
      "14: Encoding Loss 4.4632368087768555, Transition Loss -0.13725478947162628, Classifier Loss 0.028453325852751732, Total Loss 29.62470054626465\n",
      "14: Encoding Loss 7.628716468811035, Transition Loss -2.1476008892059326, Classifier Loss 0.18851250410079956, Total Loss 64.62268829345703\n",
      "14: Encoding Loss 4.835859775543213, Transition Loss -0.5678718686103821, Classifier Loss 0.1762506067752838, Total Loss 46.639991760253906\n",
      "14: Encoding Loss 4.943692207336426, Transition Loss -1.1037051677703857, Classifier Loss 0.18208366632461548, Total Loss 47.870079040527344\n",
      "14: Encoding Loss 4.571404457092285, Transition Loss -1.7895067930221558, Classifier Loss 0.07413681596517563, Total Loss 34.841392517089844\n",
      "14: Encoding Loss 6.358605861663818, Transition Loss -1.6302038431167603, Classifier Loss 0.14969423413276672, Total Loss 53.12041091918945\n",
      "14: Encoding Loss 5.59475040435791, Transition Loss -0.9633966684341431, Classifier Loss 0.08118381351232529, Total Loss 41.686500549316406\n",
      "14: Encoding Loss 5.792550086975098, Transition Loss -0.4519697427749634, Classifier Loss 0.05765637010335922, Total Loss 40.52075958251953\n",
      "14: Encoding Loss 4.2008280754089355, Transition Loss -1.9291918277740479, Classifier Loss 0.10342613607645035, Total Loss 35.54681396484375\n",
      "14: Encoding Loss 6.204902172088623, Transition Loss -0.11500018835067749, Classifier Loss 0.07076972723007202, Total Loss 44.30634307861328\n",
      "14: Encoding Loss 4.623242378234863, Transition Loss -1.5650097131729126, Classifier Loss 0.07464592903852463, Total Loss 35.20342254638672\n",
      "14: Encoding Loss 5.9788994789123535, Transition Loss -1.357739806175232, Classifier Loss 0.09658873826265335, Total Loss 45.53173065185547\n",
      "14: Encoding Loss 6.233701705932617, Transition Loss -0.6097278594970703, Classifier Loss 0.07691499590873718, Total Loss 45.093467712402344\n",
      "14: Encoding Loss 4.700226783752441, Transition Loss -1.7223737239837646, Classifier Loss 0.042286839336156845, Total Loss 32.42935562133789\n",
      "14: Encoding Loss 3.980191707611084, Transition Loss -1.2466776371002197, Classifier Loss 0.07297161221504211, Total Loss 31.177814483642578\n",
      "14: Encoding Loss 3.570014238357544, Transition Loss -1.0559179782867432, Classifier Loss 0.07593551278114319, Total Loss 29.013216018676758\n",
      "14: Encoding Loss 7.2921462059021, Transition Loss -2.3442282676696777, Classifier Loss 0.08971096575260162, Total Loss 52.72303771972656\n",
      "14: Encoding Loss 3.9188380241394043, Transition Loss -3.3735551834106445, Classifier Loss 0.056085873395204544, Total Loss 29.120267868041992\n",
      "14: Encoding Loss 3.1346888542175293, Transition Loss -1.1615593433380127, Classifier Loss 0.09082170575857162, Total Loss 27.88983917236328\n",
      "14: Encoding Loss 9.989030838012695, Transition Loss 0.37621402740478516, Classifier Loss 0.16515898704528809, Total Loss 76.60057067871094\n",
      "14: Encoding Loss 6.268801689147949, Transition Loss -0.1013161838054657, Classifier Loss 0.12937448918819427, Total Loss 50.55022048950195\n",
      "14: Encoding Loss 4.600634574890137, Transition Loss -2.6199681758880615, Classifier Loss 0.05754127353429794, Total Loss 33.35688781738281\n",
      "14: Encoding Loss 4.029378890991211, Transition Loss -0.6566939353942871, Classifier Loss 0.06864597648382187, Total Loss 31.04060935974121\n",
      "14: Encoding Loss 7.008615970611572, Transition Loss -2.5585217475891113, Classifier Loss 0.17962737381458282, Total Loss 60.01341247558594\n",
      "14: Encoding Loss 4.593482971191406, Transition Loss -1.1571900844573975, Classifier Loss 0.060063108801841736, Total Loss 33.566749572753906\n",
      "14: Encoding Loss 4.510674476623535, Transition Loss -1.1760492324829102, Classifier Loss 0.065383680164814, Total Loss 33.60194778442383\n",
      "14: Encoding Loss 4.047463417053223, Transition Loss -1.5344979763031006, Classifier Loss 0.12809397280216217, Total Loss 37.093563079833984\n",
      "14: Encoding Loss 6.056148529052734, Transition Loss -0.5265427827835083, Classifier Loss 0.03362578898668289, Total Loss 39.69926071166992\n",
      "14: Encoding Loss 4.5186262130737305, Transition Loss -1.3788005113601685, Classifier Loss 0.06779313832521439, Total Loss 33.89052200317383\n",
      "14: Encoding Loss 6.467739105224609, Transition Loss -1.937093734741211, Classifier Loss 0.09374264627695084, Total Loss 48.17992401123047\n",
      "14: Encoding Loss 5.333215236663818, Transition Loss -0.5660982728004456, Classifier Loss 0.061442986130714417, Total Loss 38.143367767333984\n",
      "14: Encoding Loss 5.492467403411865, Transition Loss -0.2182147204875946, Classifier Loss 0.13469775021076202, Total Loss 46.424495697021484\n",
      "14: Encoding Loss 7.3112473487854, Transition Loss -1.0455427169799805, Classifier Loss 0.038232386112213135, Total Loss 47.690303802490234\n",
      "14: Encoding Loss 5.6701130867004395, Transition Loss -1.0010324716567993, Classifier Loss 0.1867886185646057, Total Loss 52.69913864135742\n",
      "14: Encoding Loss 5.103024959564209, Transition Loss -0.5245121717453003, Classifier Loss 0.1013776957988739, Total Loss 40.75571060180664\n",
      "14: Encoding Loss 3.2757439613342285, Transition Loss -0.9067133665084839, Classifier Loss 0.05823637917637825, Total Loss 25.477741241455078\n",
      "14: Encoding Loss 5.114193916320801, Transition Loss -1.5008131265640259, Classifier Loss 0.07714477926492691, Total Loss 38.399044036865234\n",
      "14: Encoding Loss 5.021036624908447, Transition Loss -0.39325374364852905, Classifier Loss 0.07654277235269547, Total Loss 37.78034210205078\n",
      "14: Encoding Loss 4.547276496887207, Transition Loss 0.33921870589256287, Classifier Loss 0.09134391695261002, Total Loss 36.553741455078125\n",
      "14: Encoding Loss 4.254229545593262, Transition Loss -1.7401570081710815, Classifier Loss 0.07496381551027298, Total Loss 33.02106475830078\n",
      "14: Encoding Loss 3.3093924522399902, Transition Loss -2.7272422313690186, Classifier Loss 0.0682675838470459, Total Loss 26.682022094726562\n",
      "14: Encoding Loss 5.708120346069336, Transition Loss -1.961915135383606, Classifier Loss 0.09908023476600647, Total Loss 44.15596008300781\n",
      "14: Encoding Loss 5.034736156463623, Transition Loss -2.186290740966797, Classifier Loss 0.13514681160449982, Total Loss 43.722225189208984\n",
      "14: Encoding Loss 5.144680976867676, Transition Loss 0.05714932084083557, Classifier Loss 0.08771739900112152, Total Loss 39.66268539428711\n",
      "14: Encoding Loss 5.2576212882995605, Transition Loss -0.9188977479934692, Classifier Loss 0.12939824163913727, Total Loss 44.48518753051758\n",
      "14: Encoding Loss 6.203824520111084, Transition Loss -0.5203646421432495, Classifier Loss 0.1545032560825348, Total Loss 52.673065185546875\n",
      "14: Encoding Loss 5.38459587097168, Transition Loss -0.5383580327033997, Classifier Loss 0.11646081507205963, Total Loss 43.95344543457031\n",
      "14: Encoding Loss 4.98668909072876, Transition Loss -1.155313491821289, Classifier Loss 0.09335675090551376, Total Loss 39.255348205566406\n",
      "14: Encoding Loss 5.378358364105225, Transition Loss -2.9677891731262207, Classifier Loss 0.0631636455655098, Total Loss 38.585330963134766\n",
      "14: Encoding Loss 4.519538879394531, Transition Loss -0.7271823883056641, Classifier Loss 0.04865472391247749, Total Loss 31.9824161529541\n",
      "14: Encoding Loss 5.044964790344238, Transition Loss -1.3630945682525635, Classifier Loss 0.08917959779500961, Total Loss 39.18720245361328\n",
      "14: Encoding Loss 5.502216339111328, Transition Loss -2.7356882095336914, Classifier Loss 0.04844357445836067, Total Loss 37.85655975341797\n",
      "14: Encoding Loss 6.7288618087768555, Transition Loss -0.8076705932617188, Classifier Loss 0.047644730657339096, Total Loss 45.13732147216797\n",
      "14: Encoding Loss 2.979579210281372, Transition Loss -0.7114289999008179, Classifier Loss 0.06907710433006287, Total Loss 24.784902572631836\n",
      "14: Encoding Loss 5.181124210357666, Transition Loss -1.526263952255249, Classifier Loss 0.0418831929564476, Total Loss 35.27445602416992\n",
      "14: Encoding Loss 6.09371280670166, Transition Loss -1.1615662574768066, Classifier Loss 0.06723758578300476, Total Loss 43.28557205200195\n",
      "14: Encoding Loss 3.5822086334228516, Transition Loss -0.5516325235366821, Classifier Loss 0.03063020668923855, Total Loss 24.55605125427246\n",
      "14: Encoding Loss 5.081250190734863, Transition Loss -2.706688165664673, Classifier Loss 0.13322561979293823, Total Loss 43.80897903442383\n",
      "14: Encoding Loss 7.115359306335449, Transition Loss -0.2754456400871277, Classifier Loss 0.10370401293039322, Total Loss 53.06245040893555\n",
      "14: Encoding Loss 6.665892124176025, Transition Loss -0.8797701001167297, Classifier Loss 0.1331615149974823, Total Loss 53.311153411865234\n",
      "14: Encoding Loss 5.742307186126709, Transition Loss -1.3640810251235962, Classifier Loss 0.05525531619787216, Total Loss 39.97883224487305\n",
      "14: Encoding Loss 5.415868759155273, Transition Loss -1.5234373807907104, Classifier Loss 0.0810045376420021, Total Loss 40.595054626464844\n",
      "14: Encoding Loss 5.305091857910156, Transition Loss -0.7434278726577759, Classifier Loss 0.1464468538761139, Total Loss 46.47494125366211\n",
      "14: Encoding Loss 6.245591163635254, Transition Loss -1.650415062904358, Classifier Loss 0.060573406517505646, Total Loss 43.53023147583008\n",
      "14: Encoding Loss 3.41961407661438, Transition Loss -1.204323172569275, Classifier Loss 0.12247384339570999, Total Loss 32.76458740234375\n",
      "14: Encoding Loss 8.01161003112793, Transition Loss 0.46041804552078247, Classifier Loss 0.13579244911670685, Total Loss 61.83307647705078\n",
      "14: Encoding Loss 4.631280422210693, Transition Loss -1.0951534509658813, Classifier Loss 0.03922354057431221, Total Loss 31.709598541259766\n",
      "14: Encoding Loss 3.001769781112671, Transition Loss -2.273503303527832, Classifier Loss 0.05806932970881462, Total Loss 23.81664276123047\n",
      "14: Encoding Loss 5.984012126922607, Transition Loss -0.18120752274990082, Classifier Loss 0.045482337474823, Total Loss 40.45223617553711\n",
      "14: Encoding Loss 2.523171901702881, Transition Loss -1.0087406635284424, Classifier Loss 0.03526932746171951, Total Loss 18.66556167602539\n",
      "14: Encoding Loss 6.506937503814697, Transition Loss -1.750523328781128, Classifier Loss 0.11794399470090866, Total Loss 50.835323333740234\n",
      "14: Encoding Loss 5.731884002685547, Transition Loss -1.0109999179840088, Classifier Loss 0.08917898684740067, Total Loss 43.308799743652344\n",
      "14: Encoding Loss 5.016368865966797, Transition Loss -2.0968761444091797, Classifier Loss 0.05551339313387871, Total Loss 35.64871597290039\n",
      "14: Encoding Loss 3.339578628540039, Transition Loss -1.0590450763702393, Classifier Loss 0.11361704766750336, Total Loss 31.398754119873047\n",
      "14: Encoding Loss 4.699080467224121, Transition Loss -1.40886652469635, Classifier Loss 0.08683721721172333, Total Loss 36.87763977050781\n",
      "14: Encoding Loss 5.311497688293457, Transition Loss -1.543170690536499, Classifier Loss 0.08024276047945023, Total Loss 39.892642974853516\n",
      "14: Encoding Loss 3.312386989593506, Transition Loss -0.7432577610015869, Classifier Loss 0.08318872004747391, Total Loss 28.19289779663086\n",
      "14: Encoding Loss 5.847908973693848, Transition Loss -2.036440372467041, Classifier Loss 0.17856118083000183, Total Loss 52.94275665283203\n",
      "14: Encoding Loss 3.9347290992736816, Transition Loss -1.9759784936904907, Classifier Loss 0.04937251657247543, Total Loss 28.544837951660156\n",
      "14: Encoding Loss 3.4343388080596924, Transition Loss -0.7406150102615356, Classifier Loss 0.05859491601586342, Total Loss 26.465229034423828\n",
      "14: Encoding Loss 5.246512413024902, Transition Loss -0.007702916860580444, Classifier Loss 0.13332515954971313, Total Loss 44.811588287353516\n",
      "14: Encoding Loss 4.700911521911621, Transition Loss -1.6568502187728882, Classifier Loss 0.08124977350234985, Total Loss 36.32978439331055\n",
      "14: Encoding Loss 7.83428955078125, Transition Loss -0.5721802115440369, Classifier Loss 0.08608806878328323, Total Loss 55.614315032958984\n",
      "14: Encoding Loss 3.9975380897521973, Transition Loss -2.5039193630218506, Classifier Loss 0.187415212392807, Total Loss 42.72574996948242\n",
      "14: Encoding Loss 5.775461673736572, Transition Loss -1.0544893741607666, Classifier Loss 0.08010364323854446, Total Loss 42.66271209716797\n",
      "14: Encoding Loss 4.053508758544922, Transition Loss -1.1735262870788574, Classifier Loss 0.03755008429288864, Total Loss 28.075593948364258\n",
      "14: Encoding Loss 8.394914627075195, Transition Loss -0.18609772622585297, Classifier Loss 0.14028246700763702, Total Loss 64.39765930175781\n",
      "14: Encoding Loss 5.280763149261475, Transition Loss -1.1708532571792603, Classifier Loss 0.12750031054019928, Total Loss 44.43414306640625\n",
      "14: Encoding Loss 4.318434715270996, Transition Loss -1.1009337902069092, Classifier Loss 0.07046594470739365, Total Loss 32.95676803588867\n",
      "14: Encoding Loss 4.7050981521606445, Transition Loss -2.761981964111328, Classifier Loss 0.09295147657394409, Total Loss 37.52463150024414\n",
      "14: Encoding Loss 3.517491102218628, Transition Loss 0.7146109342575073, Classifier Loss 0.1012999415397644, Total Loss 31.520788192749023\n",
      "14: Encoding Loss 3.5810179710388184, Transition Loss -0.13969214260578156, Classifier Loss 0.044057123363018036, Total Loss 25.891765594482422\n",
      "14: Encoding Loss 4.717822074890137, Transition Loss -2.405341625213623, Classifier Loss 0.1268552988767624, Total Loss 40.99150085449219\n",
      "14: Encoding Loss 5.7740092277526855, Transition Loss -1.639328122138977, Classifier Loss 0.1212364137172699, Total Loss 46.76704406738281\n",
      "14: Encoding Loss 4.1464457511901855, Transition Loss -0.7016538977622986, Classifier Loss 0.07830440253019333, Total Loss 32.708831787109375\n",
      "14: Encoding Loss 4.424919128417969, Transition Loss -0.5939885973930359, Classifier Loss 0.06399792432785034, Total Loss 32.949073791503906\n",
      "14: Encoding Loss 4.4330949783325195, Transition Loss -1.1714293956756592, Classifier Loss 0.03377167508006096, Total Loss 29.97526741027832\n",
      "14: Encoding Loss 6.983123779296875, Transition Loss -0.23336094617843628, Classifier Loss 0.23029878735542297, Total Loss 64.92852783203125\n",
      "14: Encoding Loss 4.465551376342773, Transition Loss -1.2024421691894531, Classifier Loss 0.0609818734228611, Total Loss 32.89101791381836\n",
      "14: Encoding Loss 6.6166582107543945, Transition Loss -1.5220792293548584, Classifier Loss 0.07804692536592484, Total Loss 47.504032135009766\n",
      "14: Encoding Loss 4.063940525054932, Transition Loss -0.06705981492996216, Classifier Loss 0.050714440643787384, Total Loss 29.455060958862305\n",
      "14: Encoding Loss 5.7677812576293945, Transition Loss -1.5527143478393555, Classifier Loss 0.027768537402153015, Total Loss 37.3829231262207\n",
      "14: Encoding Loss 4.909802436828613, Transition Loss -0.8592467308044434, Classifier Loss 0.08925317227840424, Total Loss 38.3837890625\n",
      "14: Encoding Loss 5.094964027404785, Transition Loss -1.3228365182876587, Classifier Loss 0.19297000765800476, Total Loss 49.86625289916992\n",
      "14: Encoding Loss 8.294646263122559, Transition Loss -0.30311262607574463, Classifier Loss 0.2054671347141266, Total Loss 70.31446838378906\n",
      "14: Encoding Loss 6.140824794769287, Transition Loss -1.202467679977417, Classifier Loss 0.06409801542758942, Total Loss 43.2542724609375\n",
      "14: Encoding Loss 6.608799934387207, Transition Loss -1.2184065580368042, Classifier Loss 0.1502409130334854, Total Loss 54.67640686035156\n",
      "14: Encoding Loss 4.696795463562012, Transition Loss -1.8315627574920654, Classifier Loss 0.03259061649441719, Total Loss 31.439104080200195\n",
      "14: Encoding Loss 5.700718879699707, Transition Loss -1.1038169860839844, Classifier Loss 0.06597502529621124, Total Loss 40.80137634277344\n",
      "14: Encoding Loss 6.318587779998779, Transition Loss -1.1077210903167725, Classifier Loss 0.0821918323636055, Total Loss 46.130271911621094\n",
      "14: Encoding Loss 4.807497501373291, Transition Loss -1.2810369729995728, Classifier Loss 0.10374298691749573, Total Loss 39.218772888183594\n",
      "14: Encoding Loss 3.419191837310791, Transition Loss 0.4644116163253784, Classifier Loss 0.03336174413561821, Total Loss 24.037090301513672\n",
      "14: Encoding Loss 3.943211317062378, Transition Loss -0.5891656279563904, Classifier Loss 0.06737495958805084, Total Loss 30.396528244018555\n",
      "14: Encoding Loss 4.317464351654053, Transition Loss 0.34394919872283936, Classifier Loss 0.059648752212524414, Total Loss 32.00724411010742\n",
      "14: Encoding Loss 4.64943265914917, Transition Loss -1.3482623100280762, Classifier Loss 0.05544443801045418, Total Loss 33.44050216674805\n",
      "14: Encoding Loss 7.544840335845947, Transition Loss -0.2638155221939087, Classifier Loss 0.18410107493400574, Total Loss 63.67904281616211\n",
      "14: Encoding Loss 5.14825439453125, Transition Loss -1.808923363685608, Classifier Loss 0.07830585539340973, Total Loss 38.719390869140625\n",
      "14: Encoding Loss 8.214548110961914, Transition Loss -2.610471487045288, Classifier Loss 0.11809419095516205, Total Loss 61.09566116333008\n",
      "14: Encoding Loss 5.756187438964844, Transition Loss -1.676325798034668, Classifier Loss 0.06752081960439682, Total Loss 41.288536071777344\n",
      "14: Encoding Loss 3.6757171154022217, Transition Loss -1.1229228973388672, Classifier Loss 0.0754648968577385, Total Loss 29.600345611572266\n",
      "14: Encoding Loss 3.945093870162964, Transition Loss -0.39192280173301697, Classifier Loss 0.020242413505911827, Total Loss 25.69464874267578\n",
      "14: Encoding Loss 6.6191816329956055, Transition Loss -2.9709713459014893, Classifier Loss 0.2299361228942871, Total Loss 62.70751190185547\n",
      "14: Encoding Loss 5.147124767303467, Transition Loss -1.4414931535720825, Classifier Loss 0.12052052468061447, Total Loss 42.934226989746094\n",
      "14: Encoding Loss 3.808232069015503, Transition Loss 0.8013437986373901, Classifier Loss 0.0391504243016243, Total Loss 27.08497428894043\n",
      "14: Encoding Loss 5.10451078414917, Transition Loss -0.9880985021591187, Classifier Loss 0.22370943427085876, Total Loss 52.99761199951172\n",
      "14: Encoding Loss 3.9549362659454346, Transition Loss -1.5129218101501465, Classifier Loss 0.1014598086476326, Total Loss 33.87499237060547\n",
      "14: Encoding Loss 6.257565975189209, Transition Loss -0.7729070782661438, Classifier Loss 0.17389197647571564, Total Loss 54.934288024902344\n",
      "14: Encoding Loss 3.102647066116333, Transition Loss -0.3423379957675934, Classifier Loss 0.04303578659892082, Total Loss 22.91932487487793\n",
      "14: Encoding Loss 3.210871934890747, Transition Loss -0.5064778327941895, Classifier Loss 0.14697681367397308, Total Loss 33.962711334228516\n",
      "14: Encoding Loss 5.3528642654418945, Transition Loss -2.19382905960083, Classifier Loss 0.11018066108226776, Total Loss 43.134376525878906\n",
      "14: Encoding Loss 4.453604698181152, Transition Loss -1.687030553817749, Classifier Loss 0.1632070392370224, Total Loss 43.04166030883789\n",
      "14: Encoding Loss 5.447930335998535, Transition Loss -2.189817190170288, Classifier Loss 0.11562605947256088, Total Loss 44.24931335449219\n",
      "14: Encoding Loss 5.399953365325928, Transition Loss -1.5985543727874756, Classifier Loss 0.03972567990422249, Total Loss 36.37165069580078\n",
      "14: Encoding Loss 5.4895453453063965, Transition Loss -2.673326253890991, Classifier Loss 0.10564684867858887, Total Loss 43.500892639160156\n",
      "14: Encoding Loss 7.385922908782959, Transition Loss -1.0888612270355225, Classifier Loss 0.14158906042575836, Total Loss 58.4740104675293\n",
      "14: Encoding Loss 5.924476623535156, Transition Loss -1.6855958700180054, Classifier Loss 0.13493864238262177, Total Loss 49.0400505065918\n",
      "14: Encoding Loss 5.9250569343566895, Transition Loss -0.8062247037887573, Classifier Loss 0.07597717642784119, Total Loss 43.147735595703125\n",
      "14: Encoding Loss 4.006131649017334, Transition Loss -2.2039246559143066, Classifier Loss 0.07098604738712311, Total Loss 31.13451385498047\n",
      "14: Encoding Loss 6.294897556304932, Transition Loss -1.5961260795593262, Classifier Loss 0.08097484707832336, Total Loss 45.866233825683594\n",
      "14: Encoding Loss 4.373556613922119, Transition Loss -2.1977455615997314, Classifier Loss 0.06549739837646484, Total Loss 32.79020309448242\n",
      "14: Encoding Loss 5.799918174743652, Transition Loss -1.8459738492965698, Classifier Loss 0.042427051812410355, Total Loss 39.04147720336914\n",
      "14: Encoding Loss 4.822756767272949, Transition Loss -0.9909982085227966, Classifier Loss 0.06308327615261078, Total Loss 35.24447250366211\n",
      "14: Encoding Loss 5.214261054992676, Transition Loss -1.6415817737579346, Classifier Loss 0.1090463250875473, Total Loss 42.189544677734375\n",
      "14: Encoding Loss 3.7430901527404785, Transition Loss 0.17490851879119873, Classifier Loss 0.04779185727238655, Total Loss 27.30769157409668\n",
      "14: Encoding Loss 7.8047590255737305, Transition Loss -1.3640313148498535, Classifier Loss 0.17487286031246185, Total Loss 64.31529235839844\n",
      "14: Encoding Loss 8.174555778503418, Transition Loss -2.108985424041748, Classifier Loss 0.14307968318462372, Total Loss 63.354461669921875\n",
      "14: Encoding Loss 7.365969657897949, Transition Loss -1.0151464939117432, Classifier Loss 0.18383216857910156, Total Loss 62.57863235473633\n",
      "14: Encoding Loss 5.955672264099121, Transition Loss -0.20955295860767365, Classifier Loss 0.09624724090099335, Total Loss 45.35867691040039\n",
      "14: Encoding Loss 7.663991451263428, Transition Loss -0.2132943868637085, Classifier Loss 0.22211581468582153, Total Loss 68.19544982910156\n",
      "14: Encoding Loss 4.486605167388916, Transition Loss 0.25622156262397766, Classifier Loss 0.043029312044382095, Total Loss 31.32505226135254\n",
      "14: Encoding Loss 3.7972800731658936, Transition Loss -0.8517090082168579, Classifier Loss 0.08647741377353668, Total Loss 31.431081771850586\n",
      "14: Encoding Loss 2.749429702758789, Transition Loss 0.450321763753891, Classifier Loss 0.08135607838630676, Total Loss 24.812314987182617\n",
      "14: Encoding Loss 4.936105251312256, Transition Loss -0.6489128470420837, Classifier Loss 0.1177557110786438, Total Loss 41.391944885253906\n",
      "14: Encoding Loss 6.045871734619141, Transition Loss -1.63081693649292, Classifier Loss 0.08723434805870056, Total Loss 44.99801254272461\n",
      "14: Encoding Loss 6.954495429992676, Transition Loss -1.6577702760696411, Classifier Loss 0.15304745733737946, Total Loss 57.03105545043945\n",
      "14: Encoding Loss 4.041594505310059, Transition Loss -1.7545703649520874, Classifier Loss 0.061293892562389374, Total Loss 30.37825584411621\n",
      "14: Encoding Loss 6.360960960388184, Transition Loss -2.304616928100586, Classifier Loss 0.18195010721683502, Total Loss 56.35985565185547\n",
      "14: Encoding Loss 4.64092493057251, Transition Loss -1.1369787454605103, Classifier Loss 0.06168511509895325, Total Loss 34.013607025146484\n",
      "14: Encoding Loss 4.464211940765381, Transition Loss -1.4550384283065796, Classifier Loss 0.10346870869398117, Total Loss 37.13155746459961\n",
      "14: Encoding Loss 4.387073040008545, Transition Loss -3.1435599327087402, Classifier Loss 0.07725533843040466, Total Loss 34.046714782714844\n",
      "14: Encoding Loss 2.5450572967529297, Transition Loss -1.7989821434020996, Classifier Loss 0.053364675492048264, Total Loss 20.60609245300293\n",
      "14: Encoding Loss 5.5160417556762695, Transition Loss -1.698549747467041, Classifier Loss 0.08980352431535721, Total Loss 42.075927734375\n",
      "14: Encoding Loss 5.36820125579834, Transition Loss -1.9294975996017456, Classifier Loss 0.1438882052898407, Total Loss 46.597259521484375\n",
      "14: Encoding Loss 3.0966827869415283, Transition Loss -1.52142333984375, Classifier Loss 0.06306158006191254, Total Loss 24.88564682006836\n",
      "14: Encoding Loss 6.352407932281494, Transition Loss -2.248671531677246, Classifier Loss 0.0672658234834671, Total Loss 44.84012985229492\n",
      "14: Encoding Loss 4.218723297119141, Transition Loss -0.930022120475769, Classifier Loss 0.06953896582126617, Total Loss 32.265865325927734\n",
      "14: Encoding Loss 6.189443111419678, Transition Loss -1.1520785093307495, Classifier Loss 0.09551901370286942, Total Loss 46.68810272216797\n",
      "14: Encoding Loss 6.852492332458496, Transition Loss -1.4779016971588135, Classifier Loss 0.13258148729801178, Total Loss 54.37251281738281\n",
      "14: Encoding Loss 6.279818058013916, Transition Loss -0.8273943066596985, Classifier Loss 0.09065578877925873, Total Loss 46.74415588378906\n",
      "14: Encoding Loss 4.642511367797852, Transition Loss -1.5336041450500488, Classifier Loss 0.11077176779508591, Total Loss 38.93163299560547\n",
      "14: Encoding Loss 5.924826622009277, Transition Loss -1.8237063884735107, Classifier Loss 0.119971863925457, Total Loss 47.5454216003418\n",
      "14: Encoding Loss 5.180068492889404, Transition Loss -1.5551433563232422, Classifier Loss 0.05522127076983452, Total Loss 36.6019172668457\n",
      "14: Encoding Loss 2.8028922080993652, Transition Loss -0.1593090295791626, Classifier Loss 0.03826100006699562, Total Loss 20.643390655517578\n",
      "14: Encoding Loss 7.151219844818115, Transition Loss -0.19211338460445404, Classifier Loss 0.07167121767997742, Total Loss 50.07436752319336\n",
      "14: Encoding Loss 6.6311235427856445, Transition Loss -1.2609667778015137, Classifier Loss 0.05633403733372688, Total Loss 45.41964340209961\n",
      "14: Encoding Loss 6.830173492431641, Transition Loss -1.0170934200286865, Classifier Loss 0.03198828548192978, Total Loss 44.17946243286133\n",
      "14: Encoding Loss 3.969036817550659, Transition Loss -0.16551154851913452, Classifier Loss 0.052224427461624146, Total Loss 29.036598205566406\n",
      "14: Encoding Loss 4.146148681640625, Transition Loss -2.059305191040039, Classifier Loss 0.029695602133870125, Total Loss 27.845630645751953\n",
      "14: Encoding Loss 3.9171385765075684, Transition Loss -1.7074978351593018, Classifier Loss 0.1270970106124878, Total Loss 36.211849212646484\n",
      "14: Encoding Loss 5.3736572265625, Transition Loss -0.4013567864894867, Classifier Loss 0.03331945464015007, Total Loss 35.57373046875\n",
      "14: Encoding Loss 4.516823768615723, Transition Loss -1.08534836769104, Classifier Loss 0.029757516458630562, Total Loss 30.07625961303711\n",
      "14: Encoding Loss 4.2850022315979, Transition Loss -0.4574640393257141, Classifier Loss 0.03673272207379341, Total Loss 29.383102416992188\n",
      "14: Encoding Loss 3.5733847618103027, Transition Loss -0.8190051913261414, Classifier Loss 0.0638989806175232, Total Loss 27.829879760742188\n",
      "14: Encoding Loss 6.143863677978516, Transition Loss -1.3869593143463135, Classifier Loss 0.06448853015899658, Total Loss 43.31148147583008\n",
      "14: Encoding Loss 4.4136738777160645, Transition Loss -1.875692367553711, Classifier Loss 0.06689821183681488, Total Loss 33.17111587524414\n",
      "14: Encoding Loss 4.048127174377441, Transition Loss -2.4032816886901855, Classifier Loss 0.04363872855901718, Total Loss 28.651674270629883\n",
      "14: Encoding Loss 3.9314370155334473, Transition Loss -1.5032615661621094, Classifier Loss 0.08395226299762726, Total Loss 31.983247756958008\n",
      "14: Encoding Loss 5.6934309005737305, Transition Loss -0.7317203283309937, Classifier Loss 0.08764803409576416, Total Loss 42.92509841918945\n",
      "14: Encoding Loss 3.392589569091797, Transition Loss -0.7635020017623901, Classifier Loss 0.04408806934952736, Total Loss 24.7640380859375\n",
      "14: Encoding Loss 4.363366603851318, Transition Loss -1.1135234832763672, Classifier Loss 0.09575950354337692, Total Loss 35.75570297241211\n",
      "14: Encoding Loss 4.826653480529785, Transition Loss -2.791168451309204, Classifier Loss 0.0708434134721756, Total Loss 36.04314422607422\n",
      "14: Encoding Loss 4.0107550621032715, Transition Loss -1.0472170114517212, Classifier Loss 0.0376269705593586, Total Loss 27.82680892944336\n",
      "14: Encoding Loss 3.3106136322021484, Transition Loss -0.6526596546173096, Classifier Loss 0.05448175221681595, Total Loss 25.311595916748047\n",
      "14: Encoding Loss 4.552861213684082, Transition Loss -1.6412543058395386, Classifier Loss 0.13946709036827087, Total Loss 41.263221740722656\n",
      "14: Encoding Loss 3.3770644664764404, Transition Loss -2.1552340984344482, Classifier Loss 0.07015027105808258, Total Loss 27.276554107666016\n",
      "14: Encoding Loss 3.6601028442382812, Transition Loss -0.8559215664863586, Classifier Loss 0.08156223595142365, Total Loss 30.116498947143555\n",
      "14: Encoding Loss 4.74979305267334, Transition Loss -2.338764190673828, Classifier Loss 0.05535510927438736, Total Loss 34.0333366394043\n",
      "14: Encoding Loss 5.771707057952881, Transition Loss -1.0046610832214355, Classifier Loss 0.14935943484306335, Total Loss 49.56578826904297\n",
      "14: Encoding Loss 5.1263108253479, Transition Loss -1.5747125148773193, Classifier Loss 0.08954322338104248, Total Loss 39.7115592956543\n",
      "14: Encoding Loss 3.2937705516815186, Transition Loss -2.0666494369506836, Classifier Loss 0.07818077504634857, Total Loss 27.579875946044922\n",
      "14: Encoding Loss 6.3228044509887695, Transition Loss -1.1782513856887817, Classifier Loss 0.17714917659759521, Total Loss 55.651275634765625\n",
      "14: Encoding Loss 6.941202640533447, Transition Loss -1.8322099447250366, Classifier Loss 0.22332832217216492, Total Loss 63.97931671142578\n",
      "14: Encoding Loss 7.831153869628906, Transition Loss -1.879699945449829, Classifier Loss 0.11307548731565475, Total Loss 58.29372024536133\n",
      "14: Encoding Loss 7.063076972961426, Transition Loss -2.1959755420684814, Classifier Loss 0.1209842711687088, Total Loss 54.47601318359375\n",
      "14: Encoding Loss 4.373204231262207, Transition Loss -2.8184962272644043, Classifier Loss 0.07118375599384308, Total Loss 33.35647201538086\n",
      "14: Encoding Loss 6.989925384521484, Transition Loss -2.4513003826141357, Classifier Loss 0.07482481002807617, Total Loss 49.42105484008789\n",
      "14: Encoding Loss 8.073888778686523, Transition Loss -0.16696518659591675, Classifier Loss 0.12320674955844879, Total Loss 60.76394271850586\n",
      "14: Encoding Loss 6.762454986572266, Transition Loss -2.4421660900115967, Classifier Loss 0.20885857939720154, Total Loss 61.45960998535156\n",
      "14: Encoding Loss 7.557186126708984, Transition Loss -1.9161655902862549, Classifier Loss 0.16303583979606628, Total Loss 61.645931243896484\n",
      "14: Encoding Loss 6.7336578369140625, Transition Loss 0.45562154054641724, Classifier Loss 0.052155837416648865, Total Loss 45.79977798461914\n",
      "14: Encoding Loss 5.676290035247803, Transition Loss -1.6343977451324463, Classifier Loss 0.027760187163949013, Total Loss 36.83311080932617\n",
      "14: Encoding Loss 3.1577651500701904, Transition Loss -1.239959716796875, Classifier Loss 0.07721208781003952, Total Loss 26.66730499267578\n",
      "14: Encoding Loss 4.052252769470215, Transition Loss -0.6737073063850403, Classifier Loss 0.07258043438196182, Total Loss 31.571292877197266\n",
      "14: Encoding Loss 3.8368916511535645, Transition Loss 0.7002511024475098, Classifier Loss 0.05678653344511986, Total Loss 28.980104446411133\n",
      "14: Encoding Loss 2.204558849334717, Transition Loss -1.1982680559158325, Classifier Loss 0.029820391908288002, Total Loss 16.208913803100586\n",
      "14: Encoding Loss 4.369622230529785, Transition Loss -0.9043413996696472, Classifier Loss 0.15081043541431427, Total Loss 41.29841232299805\n",
      "14: Encoding Loss 3.690563678741455, Transition Loss -0.8992176055908203, Classifier Loss 0.14778345823287964, Total Loss 36.92137145996094\n",
      "14: Encoding Loss 6.815093040466309, Transition Loss -0.09633529186248779, Classifier Loss 0.049230076372623444, Total Loss 45.81352996826172\n",
      "14: Encoding Loss 5.296383857727051, Transition Loss -2.246962785720825, Classifier Loss 0.0665460079908371, Total Loss 38.432003021240234\n",
      "14: Encoding Loss 8.300634384155273, Transition Loss -1.3284075260162354, Classifier Loss 0.27907058596611023, Total Loss 77.71033477783203\n",
      "14: Encoding Loss 5.0474467277526855, Transition Loss -2.355175018310547, Classifier Loss 0.12511377036571503, Total Loss 42.79511642456055\n",
      "14: Encoding Loss 4.774796485900879, Transition Loss -1.442159652709961, Classifier Loss 0.04493445158004761, Total Loss 33.14164733886719\n",
      "14: Encoding Loss 6.085644245147705, Transition Loss -1.7730669975280762, Classifier Loss 0.14481514692306519, Total Loss 50.99467086791992\n",
      "14: Encoding Loss 6.819875240325928, Transition Loss -1.6831293106079102, Classifier Loss 0.09822750091552734, Total Loss 50.7413330078125\n",
      "14: Encoding Loss 4.9822845458984375, Transition Loss 0.17295610904693604, Classifier Loss 0.1300438642501831, Total Loss 42.96727752685547\n",
      "14: Encoding Loss 4.109747409820557, Transition Loss 0.6883560419082642, Classifier Loss 0.08061440289020538, Total Loss 32.99526596069336\n",
      "14: Encoding Loss 4.9863786697387695, Transition Loss -2.0464518070220947, Classifier Loss 0.07448378205299377, Total Loss 37.36582946777344\n",
      "14: Encoding Loss 7.161994934082031, Transition Loss -1.4559298753738403, Classifier Loss 0.10083454847335815, Total Loss 53.054840087890625\n",
      "14: Encoding Loss 5.215181827545166, Transition Loss -0.20451036095619202, Classifier Loss 0.13484284281730652, Total Loss 44.77529525756836\n",
      "14: Encoding Loss 3.2653110027313232, Transition Loss -0.44768238067626953, Classifier Loss 0.03634880110621452, Total Loss 23.2265682220459\n",
      "14: Encoding Loss 4.639743804931641, Transition Loss -1.9043430089950562, Classifier Loss 0.12512297928333282, Total Loss 40.349998474121094\n",
      "14: Encoding Loss 3.841447114944458, Transition Loss 0.7994890213012695, Classifier Loss 0.08033844083547592, Total Loss 31.40232276916504\n",
      "14: Encoding Loss 6.472921848297119, Transition Loss -1.8516383171081543, Classifier Loss 0.08269404619932175, Total Loss 47.106197357177734\n",
      "14: Encoding Loss 3.654689073562622, Transition Loss -2.2242610454559326, Classifier Loss 0.06085684150457382, Total Loss 28.012929916381836\n",
      "14: Encoding Loss 6.249156951904297, Transition Loss -1.2257384061813354, Classifier Loss 0.1228470429778099, Total Loss 49.77915573120117\n",
      "14: Encoding Loss 4.225621700286865, Transition Loss -2.1122000217437744, Classifier Loss 0.053320299834012985, Total Loss 30.68491554260254\n",
      "14: Encoding Loss 5.292984962463379, Transition Loss -1.3907830715179443, Classifier Loss 0.104798324406147, Total Loss 42.237186431884766\n",
      "14: Encoding Loss 5.2806901931762695, Transition Loss -2.0759761333465576, Classifier Loss 0.0994991809129715, Total Loss 41.63322830200195\n",
      "14: Encoding Loss 4.05372953414917, Transition Loss 0.8849585056304932, Classifier Loss 0.0843208059668541, Total Loss 33.10844421386719\n",
      "14: Encoding Loss 2.128462553024292, Transition Loss -0.8440568447113037, Classifier Loss 0.0558914840221405, Total Loss 18.359586715698242\n",
      "14: Encoding Loss 8.812417030334473, Transition Loss -2.027440309524536, Classifier Loss 0.1004052460193634, Total Loss 62.914215087890625\n",
      "14: Encoding Loss 4.981939792633057, Transition Loss -1.3947397470474243, Classifier Loss 0.09038479626178741, Total Loss 38.929561614990234\n",
      "14: Encoding Loss 4.660665035247803, Transition Loss -0.8067880272865295, Classifier Loss 0.08836471289396286, Total Loss 36.80013656616211\n",
      "14: Encoding Loss 4.502935886383057, Transition Loss -1.110249400138855, Classifier Loss 0.04877927526831627, Total Loss 31.895099639892578\n",
      "14: Encoding Loss 3.411188840866089, Transition Loss -1.0377922058105469, Classifier Loss 0.12330552190542221, Total Loss 32.797271728515625\n",
      "14: Encoding Loss 4.931361675262451, Transition Loss -1.5926798582077026, Classifier Loss 0.145429328083992, Total Loss 44.13046646118164\n",
      "14: Encoding Loss 3.472257137298584, Transition Loss 0.25293946266174316, Classifier Loss 0.041718922555446625, Total Loss 25.106611251831055\n",
      "14: Encoding Loss 6.332712173461914, Transition Loss -2.3238821029663086, Classifier Loss 0.06752719730138779, Total Loss 44.74806213378906\n",
      "14: Encoding Loss 5.375593662261963, Transition Loss -1.088413119316101, Classifier Loss 0.027651486918330193, Total Loss 35.01827621459961\n",
      "14: Encoding Loss 5.237126350402832, Transition Loss -1.9358400106430054, Classifier Loss 0.06386446207761765, Total Loss 37.80842971801758\n",
      "14: Encoding Loss 4.10784912109375, Transition Loss -0.87907874584198, Classifier Loss 0.028314394876360893, Total Loss 27.478185653686523\n",
      "14: Encoding Loss 4.206350326538086, Transition Loss 0.13483108580112457, Classifier Loss 0.12698695063591003, Total Loss 37.99073028564453\n",
      "14: Encoding Loss 5.095561981201172, Transition Loss -1.0418802499771118, Classifier Loss 0.045444466173648834, Total Loss 35.11740493774414\n",
      "14: Encoding Loss 3.606452226638794, Transition Loss -2.625572681427002, Classifier Loss 0.08758702129125595, Total Loss 30.396364212036133\n",
      "14: Encoding Loss 5.098781108856201, Transition Loss -1.6171625852584839, Classifier Loss 0.1862545609474182, Total Loss 49.21749496459961\n",
      "14: Encoding Loss 5.044712543487549, Transition Loss -1.26789391040802, Classifier Loss 0.09437340497970581, Total Loss 39.705108642578125\n",
      "14: Encoding Loss 7.6646409034729, Transition Loss -1.2519171237945557, Classifier Loss 0.08892454952001572, Total Loss 54.87980270385742\n",
      "14: Encoding Loss 4.602908611297607, Transition Loss -0.476065456867218, Classifier Loss 0.07830417901277542, Total Loss 35.44768142700195\n",
      "14: Encoding Loss 4.269701957702637, Transition Loss 0.11888150870800018, Classifier Loss 0.10173007845878601, Total Loss 35.838775634765625\n",
      "14: Encoding Loss 6.9546613693237305, Transition Loss -1.279029130935669, Classifier Loss 0.08480976521968842, Total Loss 50.20843505859375\n",
      "14: Encoding Loss 5.122828006744385, Transition Loss -2.229801654815674, Classifier Loss 0.12730100750923157, Total Loss 43.46617889404297\n",
      "14: Encoding Loss 7.439846992492676, Transition Loss -2.018176317214966, Classifier Loss 0.12380322068929672, Total Loss 57.01859664916992\n",
      "14: Encoding Loss 5.2321343421936035, Transition Loss -2.177867889404297, Classifier Loss 0.05174144357442856, Total Loss 36.56608200073242\n",
      "14: Encoding Loss 3.977067232131958, Transition Loss -2.25703763961792, Classifier Loss 0.1222764253616333, Total Loss 36.089141845703125\n",
      "14: Encoding Loss 6.547664642333984, Transition Loss -1.0140259265899658, Classifier Loss 0.0827425867319107, Total Loss 47.55984115600586\n",
      "14: Encoding Loss 8.86099624633789, Transition Loss -1.3588275909423828, Classifier Loss 0.13249188661575317, Total Loss 66.41462707519531\n",
      "14: Encoding Loss 5.345553874969482, Transition Loss -1.44016432762146, Classifier Loss 0.04005176201462746, Total Loss 36.07792663574219\n",
      "14: Encoding Loss 4.187198638916016, Transition Loss -0.17091232538223267, Classifier Loss 0.05150711163878441, Total Loss 30.273836135864258\n",
      "14: Encoding Loss 3.646981716156006, Transition Loss -0.9479535818099976, Classifier Loss 0.07799898833036423, Total Loss 29.68140983581543\n",
      "14: Encoding Loss 5.517910003662109, Transition Loss -1.411858320236206, Classifier Loss 0.060632314532995224, Total Loss 39.170127868652344\n",
      "14: Encoding Loss 4.007676124572754, Transition Loss -1.0048216581344604, Classifier Loss 0.03250075504183769, Total Loss 27.295730590820312\n",
      "14: Encoding Loss 5.979711532592773, Transition Loss -1.9951839447021484, Classifier Loss 0.04220754653215408, Total Loss 40.09822463989258\n",
      "14: Encoding Loss 3.105062484741211, Transition Loss -1.9329612255096436, Classifier Loss 0.08294717222452164, Total Loss 26.924318313598633\n",
      "14: Encoding Loss 8.117006301879883, Transition Loss 0.2498377561569214, Classifier Loss 0.25712984800338745, Total Loss 74.51496124267578\n",
      "14: Encoding Loss 4.922632217407227, Transition Loss -1.0774681568145752, Classifier Loss 0.11550791561603546, Total Loss 41.08615493774414\n",
      "14: Encoding Loss 6.091370582580566, Transition Loss -1.1927012205123901, Classifier Loss 0.09255873411893845, Total Loss 45.80362319946289\n",
      "14: Encoding Loss 5.791954040527344, Transition Loss 0.24384844303131104, Classifier Loss 0.08341751247644424, Total Loss 43.19101333618164\n",
      "14: Encoding Loss 6.749275207519531, Transition Loss -0.5405406355857849, Classifier Loss 0.19296351075172424, Total Loss 59.79178237915039\n",
      "14: Encoding Loss 6.038914680480957, Transition Loss -1.3846298456192017, Classifier Loss 0.1629362851381302, Total Loss 52.52656555175781\n",
      "14: Encoding Loss 4.326050758361816, Transition Loss -0.5151731371879578, Classifier Loss 0.08984585851430893, Total Loss 34.9406852722168\n",
      "14: Encoding Loss 4.728213310241699, Transition Loss -2.037687301635742, Classifier Loss 0.11112259328365326, Total Loss 39.4807243347168\n",
      "14: Encoding Loss 4.873898506164551, Transition Loss -0.7437518835067749, Classifier Loss 0.10363295674324036, Total Loss 39.606388092041016\n",
      "14: Encoding Loss 5.7279510498046875, Transition Loss -0.35847145318984985, Classifier Loss 0.09197132289409637, Total Loss 43.564693450927734\n",
      "14: Encoding Loss 3.212540864944458, Transition Loss -0.7963078022003174, Classifier Loss 0.05465800687670708, Total Loss 24.7407283782959\n",
      "14: Encoding Loss 3.833282470703125, Transition Loss -0.8274577856063843, Classifier Loss 0.0958753302693367, Total Loss 32.58689498901367\n",
      "14: Encoding Loss 3.644254684448242, Transition Loss -1.9137144088745117, Classifier Loss 0.0799139216542244, Total Loss 29.856155395507812\n",
      "14: Encoding Loss 4.927369594573975, Transition Loss -0.6227554082870483, Classifier Loss 0.034897010773420334, Total Loss 33.053672790527344\n",
      "14: Encoding Loss 5.06846809387207, Transition Loss -1.1384936571121216, Classifier Loss 0.11596813797950745, Total Loss 42.007171630859375\n",
      "14: Encoding Loss 3.753164291381836, Transition Loss -2.2497305870056152, Classifier Loss 0.0905238538980484, Total Loss 31.570470809936523\n",
      "14: Encoding Loss 4.732125282287598, Transition Loss -1.3208867311477661, Classifier Loss 0.11433044075965881, Total Loss 39.82526779174805\n",
      "14: Encoding Loss 3.9043986797332764, Transition Loss -0.17511296272277832, Classifier Loss 0.032116346061229706, Total Loss 26.637956619262695\n",
      "14: Encoding Loss 3.563115358352661, Transition Loss -0.26138198375701904, Classifier Loss 0.04644251614809036, Total Loss 26.02284049987793\n",
      "14: Encoding Loss 4.739340782165527, Transition Loss -0.5515003800392151, Classifier Loss 0.1820465326309204, Total Loss 46.640480041503906\n",
      "14: Encoding Loss 5.950263977050781, Transition Loss -1.6381146907806396, Classifier Loss 0.18474537134170532, Total Loss 54.17546463012695\n",
      "14: Encoding Loss 3.863664150238037, Transition Loss -0.4501795172691345, Classifier Loss 0.11533162742853165, Total Loss 34.714969635009766\n",
      "14: Encoding Loss 6.329375267028809, Transition Loss -1.519112229347229, Classifier Loss 0.14013531804084778, Total Loss 51.98917770385742\n",
      "14: Encoding Loss 3.713111400604248, Transition Loss -1.9609694480895996, Classifier Loss 0.06648778915405273, Total Loss 28.926664352416992\n",
      "14: Encoding Loss 5.471689701080322, Transition Loss -1.590113639831543, Classifier Loss 0.1219244971871376, Total Loss 45.02195358276367\n",
      "14: Encoding Loss 4.904383182525635, Transition Loss -0.9163124561309814, Classifier Loss 0.033947136253118515, Total Loss 32.820648193359375\n",
      "14: Encoding Loss 3.0161941051483154, Transition Loss -1.2349754571914673, Classifier Loss 0.0888134315609932, Total Loss 26.97801399230957\n",
      "14: Encoding Loss 3.652435302734375, Transition Loss -2.094264030456543, Classifier Loss 0.055112503468990326, Total Loss 27.425024032592773\n",
      "14: Encoding Loss 3.931248903274536, Transition Loss -0.21007263660430908, Classifier Loss 0.12498997896909714, Total Loss 36.08640670776367\n",
      "14: Encoding Loss 6.634191989898682, Transition Loss -0.6873446106910706, Classifier Loss 0.05477166920900345, Total Loss 45.28204345703125\n",
      "14: Encoding Loss 5.584000587463379, Transition Loss -1.4614499807357788, Classifier Loss 0.19809696078300476, Total Loss 53.31311798095703\n",
      "14: Encoding Loss 6.279868125915527, Transition Loss -0.21515290439128876, Classifier Loss 0.12958475947380066, Total Loss 50.63759994506836\n",
      "14: Encoding Loss 5.42659854888916, Transition Loss -2.188991069793701, Classifier Loss 0.13049322366714478, Total Loss 45.60803985595703\n",
      "14: Encoding Loss 4.7241926193237305, Transition Loss -0.7569980025291443, Classifier Loss 0.05259392783045769, Total Loss 33.604248046875\n",
      "14: Encoding Loss 2.906710147857666, Transition Loss -2.3038406372070312, Classifier Loss 0.05180364474654198, Total Loss 22.619705200195312\n",
      "14: Encoding Loss 2.8164420127868652, Transition Loss -1.2777137756347656, Classifier Loss 0.0513942576944828, Total Loss 22.037567138671875\n",
      "14: Encoding Loss 3.852412462234497, Transition Loss -2.4965734481811523, Classifier Loss 0.05088922381401062, Total Loss 28.2023983001709\n",
      "14: Encoding Loss 5.923497200012207, Transition Loss 0.13530121743679047, Classifier Loss 0.032983146607875824, Total Loss 38.89341735839844\n",
      "14: Encoding Loss 5.155128002166748, Transition Loss -0.22405175864696503, Classifier Loss 0.09273803979158401, Total Loss 40.20448684692383\n",
      "14: Encoding Loss 6.6838884353637695, Transition Loss -3.3017494678497314, Classifier Loss 0.06082102656364441, Total Loss 46.18411636352539\n",
      "14: Encoding Loss 5.292395114898682, Transition Loss -1.7717550992965698, Classifier Loss 0.08380605280399323, Total Loss 40.13426971435547\n",
      "14: Encoding Loss 4.23593807220459, Transition Loss 0.08953201770782471, Classifier Loss 0.09463280439376831, Total Loss 34.91472244262695\n",
      "14: Encoding Loss 4.359302043914795, Transition Loss -2.205875873565674, Classifier Loss 0.03375166654586792, Total Loss 29.53009605407715\n",
      "14: Encoding Loss 7.21002197265625, Transition Loss -0.9892405271530151, Classifier Loss 0.10046245902776718, Total Loss 53.30598068237305\n",
      "14: Encoding Loss 5.9967169761657715, Transition Loss -2.5641376972198486, Classifier Loss 0.15947225689888, Total Loss 51.9265022277832\n",
      "14: Encoding Loss 5.651524543762207, Transition Loss -1.4555926322937012, Classifier Loss 0.09142430126667023, Total Loss 43.050994873046875\n",
      "14: Encoding Loss 6.362725257873535, Transition Loss -0.5560310482978821, Classifier Loss 0.13865813612937927, Total Loss 52.04194641113281\n",
      "14: Encoding Loss 6.292169570922852, Transition Loss -1.592690110206604, Classifier Loss 0.02055012248456478, Total Loss 39.80739212036133\n",
      "14: Encoding Loss 5.170902252197266, Transition Loss -1.5427166223526, Classifier Loss 0.16919957101345062, Total Loss 47.94475555419922\n",
      "14: Encoding Loss 3.9582531452178955, Transition Loss -0.6928224563598633, Classifier Loss 0.06111173704266548, Total Loss 29.860416412353516\n",
      "14: Encoding Loss 6.236120223999023, Transition Loss -1.8845293521881104, Classifier Loss 0.10468806326389313, Total Loss 47.88477325439453\n",
      "14: Encoding Loss 5.587240695953369, Transition Loss -0.7342696785926819, Classifier Loss 0.08610714972019196, Total Loss 42.13386535644531\n",
      "14: Encoding Loss 4.689510345458984, Transition Loss -0.4873012900352478, Classifier Loss 0.07197707891464233, Total Loss 35.33457565307617\n",
      "14: Encoding Loss 4.347634792327881, Transition Loss -1.1218822002410889, Classifier Loss 0.10260296612977982, Total Loss 36.34565734863281\n",
      "14: Encoding Loss 4.897140026092529, Transition Loss -0.6881303787231445, Classifier Loss 0.13403023779392242, Total Loss 42.78559112548828\n",
      "14: Encoding Loss 5.586717128753662, Transition Loss -0.4389503598213196, Classifier Loss 0.10985894501209259, Total Loss 44.50602340698242\n",
      "14: Encoding Loss 5.165329456329346, Transition Loss -2.930450677871704, Classifier Loss 0.058320410549640656, Total Loss 36.82284927368164\n",
      "14: Encoding Loss 2.6057281494140625, Transition Loss -1.6544922590255737, Classifier Loss 0.07526975870132446, Total Loss 23.16068458557129\n",
      "14: Encoding Loss 9.048733711242676, Transition Loss -2.1720941066741943, Classifier Loss 0.13762131333351135, Total Loss 68.05366516113281\n",
      "14: Encoding Loss 5.677068710327148, Transition Loss -0.2825263738632202, Classifier Loss 0.028706172481179237, Total Loss 36.93291473388672\n",
      "14: Encoding Loss 5.4275312423706055, Transition Loss -0.9223556518554688, Classifier Loss 0.1549411565065384, Total Loss 48.05893325805664\n",
      "14: Encoding Loss 4.972861289978027, Transition Loss -1.0839033126831055, Classifier Loss 0.12461815774440765, Total Loss 42.29854965209961\n",
      "14: Encoding Loss 6.292393684387207, Transition Loss -0.91715407371521, Classifier Loss 0.052652597427368164, Total Loss 43.019256591796875\n",
      "14: Encoding Loss 6.7819085121154785, Transition Loss -1.4899622201919556, Classifier Loss 0.26869794726371765, Total Loss 67.56065368652344\n",
      "14: Encoding Loss 5.088542938232422, Transition Loss -0.31926900148391724, Classifier Loss 0.0751752108335495, Total Loss 38.04865646362305\n",
      "14: Encoding Loss 5.13486385345459, Transition Loss 0.46343451738357544, Classifier Loss 0.1104644164443016, Total Loss 42.0410041809082\n",
      "14: Encoding Loss 5.261469841003418, Transition Loss -0.5754892826080322, Classifier Loss 0.08277630060911179, Total Loss 39.846221923828125\n",
      "14: Encoding Loss 3.9360299110412598, Transition Loss 0.1807236522436142, Classifier Loss 0.020420540124177933, Total Loss 25.73052215576172\n",
      "14: Encoding Loss 6.507584571838379, Transition Loss -1.3228142261505127, Classifier Loss 0.06963228434324265, Total Loss 46.00820541381836\n",
      "14: Encoding Loss 7.208377838134766, Transition Loss -1.8103246688842773, Classifier Loss 0.19567206501960754, Total Loss 62.816749572753906\n",
      "14: Encoding Loss 4.529803276062012, Transition Loss -0.9629868865013123, Classifier Loss 0.06672821193933487, Total Loss 33.85125732421875\n",
      "14: Encoding Loss 3.8686840534210205, Transition Loss -1.9363634586334229, Classifier Loss 0.061201464384794235, Total Loss 29.33147621154785\n",
      "14: Encoding Loss 4.257299900054932, Transition Loss -1.3059945106506348, Classifier Loss 0.06400348991155624, Total Loss 31.943626403808594\n",
      "14: Encoding Loss 4.835339546203613, Transition Loss -0.3041495084762573, Classifier Loss 0.04320943355560303, Total Loss 33.33285903930664\n",
      "14: Encoding Loss 5.76173210144043, Transition Loss -0.9457322359085083, Classifier Loss 0.14845535159111023, Total Loss 49.415550231933594\n",
      "14: Encoding Loss 3.044646739959717, Transition Loss -0.29107385873794556, Classifier Loss 0.1131599023938179, Total Loss 29.583755493164062\n",
      "14: Encoding Loss 5.927509784698486, Transition Loss -2.0101354122161865, Classifier Loss 0.07833040505647659, Total Loss 43.39729690551758\n",
      "14: Encoding Loss 3.5535800457000732, Transition Loss -1.0444248914718628, Classifier Loss 0.041636765003204346, Total Loss 25.484739303588867\n",
      "14: Encoding Loss 4.496295928955078, Transition Loss -2.6538820266723633, Classifier Loss 0.03664156794548035, Total Loss 30.640871047973633\n",
      "14: Encoding Loss 1.8862097263336182, Transition Loss -1.5075818300247192, Classifier Loss 0.031180987134575844, Total Loss 14.434755325317383\n",
      "14: Encoding Loss 6.114346504211426, Transition Loss -1.67322838306427, Classifier Loss 0.1167941465973854, Total Loss 48.36482620239258\n",
      "14: Encoding Loss 4.348505020141602, Transition Loss -1.733701467514038, Classifier Loss 0.09187140315771103, Total Loss 35.2774772644043\n",
      "14: Encoding Loss 3.9114043712615967, Transition Loss -0.6868244409561157, Classifier Loss 0.06551460176706314, Total Loss 30.01961326599121\n",
      "14: Encoding Loss 5.302731513977051, Transition Loss 0.3655770421028137, Classifier Loss 0.06766262650489807, Total Loss 38.728885650634766\n",
      "14: Encoding Loss 5.930646896362305, Transition Loss -1.6930829286575317, Classifier Loss 0.14833317697048187, Total Loss 50.41651916503906\n",
      "14: Encoding Loss 6.729783535003662, Transition Loss -3.572899580001831, Classifier Loss 0.13204441964626312, Total Loss 53.58171463012695\n",
      "14: Encoding Loss 4.642323970794678, Transition Loss -1.5893621444702148, Classifier Loss 0.07617975026369095, Total Loss 35.471282958984375\n",
      "14: Encoding Loss 5.2343010902404785, Transition Loss -2.1348929405212402, Classifier Loss 0.03293909132480621, Total Loss 34.69886016845703\n",
      "14: Encoding Loss 2.7481143474578857, Transition Loss -2.6469905376434326, Classifier Loss 0.04191172495484352, Total Loss 20.678800582885742\n",
      "14: Encoding Loss 4.60786771774292, Transition Loss -2.864990472793579, Classifier Loss 0.15949980914592743, Total Loss 43.59604263305664\n",
      "14: Encoding Loss 4.999684810638428, Transition Loss -0.8159779906272888, Classifier Loss 0.178695946931839, Total Loss 47.86737823486328\n",
      "14: Encoding Loss 7.274062156677246, Transition Loss -0.4058693051338196, Classifier Loss 0.08535823225975037, Total Loss 52.18003463745117\n",
      "14: Encoding Loss 4.134377479553223, Transition Loss -1.1640130281448364, Classifier Loss 0.03995019942522049, Total Loss 28.800819396972656\n",
      "14: Encoding Loss 3.979783535003662, Transition Loss -1.5839062929153442, Classifier Loss 0.05165312439203262, Total Loss 29.043380737304688\n",
      "14: Encoding Loss 5.295154571533203, Transition Loss -1.4894614219665527, Classifier Loss 0.02869560196995735, Total Loss 34.639892578125\n",
      "14: Encoding Loss 4.452013969421387, Transition Loss -0.9106102585792542, Classifier Loss 0.04891311749815941, Total Loss 31.6030330657959\n",
      "14: Encoding Loss 4.71419095993042, Transition Loss -1.4006134271621704, Classifier Loss 0.05481908097863197, Total Loss 33.76649475097656\n",
      "14: Encoding Loss 4.974902153015137, Transition Loss -1.295135736465454, Classifier Loss 0.04824353754520416, Total Loss 34.673248291015625\n",
      "14: Encoding Loss 4.947772026062012, Transition Loss -1.2909554243087769, Classifier Loss 0.039435695856809616, Total Loss 33.62968826293945\n",
      "14: Encoding Loss 4.494606018066406, Transition Loss -1.2519909143447876, Classifier Loss 0.11696896702051163, Total Loss 38.66403579711914\n",
      "14: Encoding Loss 3.958777904510498, Transition Loss -0.8065853118896484, Classifier Loss 0.07927422225475311, Total Loss 31.679767608642578\n",
      "14: Encoding Loss 5.431436061859131, Transition Loss 0.32616567611694336, Classifier Loss 0.09858547896146774, Total Loss 42.577632904052734\n",
      "14: Encoding Loss 4.53680419921875, Transition Loss -1.7884100675582886, Classifier Loss 0.05601004883646965, Total Loss 32.82111358642578\n",
      "14: Encoding Loss 4.759378910064697, Transition Loss -2.0099852085113525, Classifier Loss 0.051368940621614456, Total Loss 33.69236373901367\n",
      "14: Encoding Loss 4.367188930511475, Transition Loss -0.7144286632537842, Classifier Loss 0.1338692158460617, Total Loss 39.58977127075195\n",
      "14: Encoding Loss 3.828543186187744, Transition Loss -0.2694225311279297, Classifier Loss 0.03924396634101868, Total Loss 26.89554786682129\n",
      "14: Encoding Loss 5.138180732727051, Transition Loss -2.067659854888916, Classifier Loss 0.058401428163051605, Total Loss 36.668399810791016\n",
      "14: Encoding Loss 4.024238586425781, Transition Loss -0.8101160526275635, Classifier Loss 0.02996142953634262, Total Loss 27.141252517700195\n",
      "14: Encoding Loss 6.320305824279785, Transition Loss 0.07814735174179077, Classifier Loss 0.18190893530845642, Total Loss 56.14398956298828\n",
      "14: Encoding Loss 5.437509059906006, Transition Loss -1.641304850578308, Classifier Loss 0.07684657722711563, Total Loss 40.309059143066406\n",
      "14: Encoding Loss 6.351594924926758, Transition Loss -0.4641587436199188, Classifier Loss 0.1493261605501175, Total Loss 53.04199981689453\n",
      "14: Encoding Loss 4.687224388122559, Transition Loss -1.0775442123413086, Classifier Loss 0.021853262558579445, Total Loss 30.308242797851562\n",
      "14: Encoding Loss 4.750069618225098, Transition Loss -1.0751643180847168, Classifier Loss 0.13464319705963135, Total Loss 41.96430587768555\n",
      "14: Encoding Loss 2.891087055206299, Transition Loss -1.4577322006225586, Classifier Loss 0.09100684523582458, Total Loss 26.446624755859375\n",
      "14: Encoding Loss 5.595839023590088, Transition Loss -0.3323201537132263, Classifier Loss 0.11802849173545837, Total Loss 45.377750396728516\n",
      "14: Encoding Loss 5.675134658813477, Transition Loss -1.2388051748275757, Classifier Loss 0.08195708692073822, Total Loss 42.24602127075195\n",
      "14: Encoding Loss 3.8217263221740723, Transition Loss -0.5047791600227356, Classifier Loss 0.11505106091499329, Total Loss 34.43526077270508\n",
      "14: Encoding Loss 4.75955867767334, Transition Loss -1.2920193672180176, Classifier Loss 0.06642214208841324, Total Loss 35.19905471801758\n",
      "14: Encoding Loss 5.994057655334473, Transition Loss -0.4554515779018402, Classifier Loss 0.05949724465608597, Total Loss 41.91389083862305\n",
      "14: Encoding Loss 3.7977523803710938, Transition Loss 0.02524438500404358, Classifier Loss 0.051106858998537064, Total Loss 27.907297134399414\n",
      "14: Encoding Loss 6.214021682739258, Transition Loss 0.017238855361938477, Classifier Loss 0.17321813106536865, Total Loss 54.61283874511719\n",
      "14: Encoding Loss 5.426301002502441, Transition Loss -0.9946097135543823, Classifier Loss 0.15644674003124237, Total Loss 48.20208740234375\n",
      "14: Encoding Loss 4.316810607910156, Transition Loss -1.3392386436462402, Classifier Loss 0.07283688336610794, Total Loss 33.18402099609375\n",
      "14: Encoding Loss 4.023448944091797, Transition Loss -1.428579568862915, Classifier Loss 0.14340601861476898, Total Loss 38.4807243347168\n",
      "14: Encoding Loss 6.275653839111328, Transition Loss 0.057306528091430664, Classifier Loss 0.13787779211997986, Total Loss 51.46462631225586\n",
      "14: Encoding Loss 5.388883590698242, Transition Loss -1.552227258682251, Classifier Loss 0.13383837044239044, Total Loss 45.71651840209961\n",
      "14: Encoding Loss 5.781480312347412, Transition Loss -1.1432104110717773, Classifier Loss 0.13321375846862793, Total Loss 48.009803771972656\n",
      "14: Encoding Loss 4.822561740875244, Transition Loss -1.1927320957183838, Classifier Loss 0.0703582614660263, Total Loss 35.97072219848633\n",
      "14: Encoding Loss 5.786020755767822, Transition Loss -1.593677043914795, Classifier Loss 0.11996155232191086, Total Loss 46.71164321899414\n",
      "14: Encoding Loss 5.4534196853637695, Transition Loss -0.8065910339355469, Classifier Loss 0.08436049520969391, Total Loss 41.156246185302734\n",
      "14: Encoding Loss 4.3789286613464355, Transition Loss -1.2208956480026245, Classifier Loss 0.08379155397415161, Total Loss 34.65224075317383\n",
      "14: Encoding Loss 4.81834602355957, Transition Loss 0.07639428973197937, Classifier Loss 0.04985083267092705, Total Loss 33.92572021484375\n",
      "14: Encoding Loss 3.914698839187622, Transition Loss -0.2993420660495758, Classifier Loss 0.04943661764264107, Total Loss 28.431734085083008\n",
      "14: Encoding Loss 6.494349479675293, Transition Loss -1.6508898735046387, Classifier Loss 0.1439087837934494, Total Loss 53.356319427490234\n",
      "14: Encoding Loss 5.457620143890381, Transition Loss -1.7389366626739502, Classifier Loss 0.1158471629023552, Total Loss 44.32974624633789\n",
      "14: Encoding Loss 2.9282922744750977, Transition Loss -1.0049065351486206, Classifier Loss 0.06957106292247772, Total Loss 24.526456832885742\n",
      "14: Encoding Loss 7.219649314880371, Transition Loss -1.6050689220428467, Classifier Loss 0.07724401354789734, Total Loss 51.041656494140625\n",
      "14: Encoding Loss 4.172242641448975, Transition Loss -0.8035709857940674, Classifier Loss 0.05845789983868599, Total Loss 30.878923416137695\n",
      "14: Encoding Loss 4.343902587890625, Transition Loss -0.9835923910140991, Classifier Loss 0.04230537638068199, Total Loss 30.293561935424805\n",
      "14: Encoding Loss 6.294848442077637, Transition Loss -1.8124542236328125, Classifier Loss 0.09783429652452469, Total Loss 47.551795959472656\n",
      "14: Encoding Loss 5.707277774810791, Transition Loss -1.0624624490737915, Classifier Loss 0.19415618479251862, Total Loss 53.65886306762695\n",
      "14: Encoding Loss 5.508029460906982, Transition Loss -1.1986738443374634, Classifier Loss 0.07831156998872757, Total Loss 40.87885665893555\n",
      "14: Encoding Loss 4.534855842590332, Transition Loss -0.006137639284133911, Classifier Loss 0.07228288799524307, Total Loss 34.43741989135742\n",
      "14: Encoding Loss 4.895272731781006, Transition Loss -2.235053777694702, Classifier Loss 0.07472062110900879, Total Loss 36.84280776977539\n",
      "14: Encoding Loss 5.057600975036621, Transition Loss -1.83299720287323, Classifier Loss 0.08792337775230408, Total Loss 39.13721466064453\n",
      "14: Encoding Loss 5.993647575378418, Transition Loss 0.2659619152545929, Classifier Loss 0.09297795593738556, Total Loss 45.366065979003906\n",
      "14: Encoding Loss 7.6395158767700195, Transition Loss -1.2410613298416138, Classifier Loss 0.12876422703266144, Total Loss 58.7130241394043\n",
      "14: Encoding Loss 4.734003067016602, Transition Loss -2.2514166831970215, Classifier Loss 0.04042849689722061, Total Loss 32.44596862792969\n",
      "14: Encoding Loss 6.033776760101318, Transition Loss -0.9093001484870911, Classifier Loss 0.04629156365990639, Total Loss 40.831459045410156\n",
      "14: Encoding Loss 5.3498005867004395, Transition Loss -0.6596423983573914, Classifier Loss 0.13874895870685577, Total Loss 45.97343826293945\n",
      "14: Encoding Loss 4.829270839691162, Transition Loss -1.0128268003463745, Classifier Loss 0.03320752829313278, Total Loss 32.29597473144531\n"
     ]
    }
   ],
   "source": [
    "# Train Forward Loss\n",
    "training_loop(in_data, out_data, initiations_s, initiations_s_prime,  n_epochs, optimizer_forward, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate\n",
    "def plot_z_space(data):\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_actions):\n",
    "            s = data[i][0][...,:obs_dim]\n",
    "            a = data[i][0][...,obs_dim:]\n",
    "            s_prime = data[i][1][..., :obs_dim]\n",
    "            z = encoder(s)\n",
    "            z_prime_encoded = encoder(s_prime)\n",
    "            z_prime = transition_model(torch.cat((z[:, 0:-1], a), dim=-1))\n",
    "            s_prime = grounding_model(z[:, 0:-1])\n",
    "            # plt.scatter(z[:, 0], z[:, 1], label=f\"encoded action {i}\")\n",
    "            \n",
    "            plt.scatter(z_prime_encoded[:, 0], z_prime_encoded[:, 1], label=f\"encoded s prime {i}\")\n",
    "            plt.scatter(z_prime[:, 0], z_prime[:, 1], label=f\"prediction action {i}\")\n",
    "            # plt.scatter(s_prime[:, 0], s_prime[:, 1], label=f\"action {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_z_space(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = data[0][0][..., :obs_dim]\n",
    "    a = data[0][0][...,obs_dim:]\n",
    "    z = encoder(x)\n",
    "    logits = initiation_classifier(z[:, :latent_dim])\n",
    "    z_prime = transition_model(torch.cat((z[:, 0:-1], a), dim=-1))[..., :latent_dim]\n",
    "    logits_z_prime = initiation_classifier(z_prime)\n",
    "    init_masks = logits\n",
    "    for i in range(n_actions):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.scatter(s[init_masks[...,i] > 0.7, 0], s[init_masks[...,i] > 0.7, 1], label=f\"initiation_set {i}\")\n",
    "        # plt.scatter(s[data[0][2][...,i] > 0.7, 0], s[data[0][2][...,i] > 0.7, 1], label=f\"initiation_set (truth) {i}\")\n",
    "        # plt.scatter(s_prime[0][I_s_prime[i][..., i] == 1, 0], s_prime[0][I_s_prime[i][...,i] == 1, 1])\n",
    "        plt.scatter(s_prime[0][logits_z_prime[..., i] > 0.7, 0], s_prime[0][logits_z_prime[...,i] > 0.7, 1])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(n_actions):\n",
    "        \n",
    "        x = data[i][0][..., :obs_dim]\n",
    "        a = data[i][0][...,obs_dim:]\n",
    "        z = encoder(x)\n",
    "        epsilon = torch.normal(0, 1, (x.size(0), latent_dim))\n",
    "        z_prime_prediction = transition_model(torch.cat((z[:, :latent_dim], a), dim=-1))\n",
    "        # z_prime_prediction = epsilon * z_prime_prediction[..., latent_dim:] + z_prime_prediction[..., :latent_dim]\n",
    "        s_prime = grounding_model(z_prime_prediction[..., :latent_dim])\n",
    "\n",
    "        epsilon = torch.normal(0, 1, (s_prime.size(0), obs_dim))\n",
    "        s_prime_sample = torch.exp(s_prime[..., obs_dim:]) * epsilon + s_prime[..., :obs_dim]\n",
    "        # s_prime_sample = s_prime[..., :obs_dim]\n",
    "        z_prime_effect = encoder(s_prime_sample)\n",
    "\n",
    "        z_prime_truth = encoder(data[i][1][..., :obs_dim])\n",
    "\n",
    "\n",
    "        logits = initiation_classifier(z[:, :latent_dim])\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(z_prime_truth[..., 0], z_prime_truth[..., 1], label=f\"S_prime Truth action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.scatter(z_prime_effect[..., 0], z_prime_effect[..., 1], label=f\"S_prime Effect prediction action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(z_prime_prediction[..., 0], z_prime_prediction[..., 1])\n",
    "        # plt.legend()\n",
    "        \n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
