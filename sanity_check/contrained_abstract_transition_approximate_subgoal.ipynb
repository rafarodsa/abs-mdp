{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "X_MIN, X_MAX, Y_MIN, Y_MAX = 0, 10, 0, 10\n",
    "obs_dim = 10\n",
    "state_dim = 2\n",
    "T = np.random.randn(obs_dim, state_dim)\n",
    "\n",
    "# Centroids\n",
    "centroids = []\n",
    "for x in (X_MAX//4, X_MAX * 3//4):\n",
    "    for y in (Y_MAX//4, Y_MAX * 3//4):\n",
    "        centroids.append(np.array((x, y)))\n",
    "\n",
    "rooms = []\n",
    "for x in (X_MAX//2, X_MAX):\n",
    "    for y in (Y_MAX//2, Y_MAX):\n",
    "        rooms.append((x-X_MAX//2, y-Y_MAX//2, x, y))\n",
    "def initiation_set(room_limits):\n",
    "    def _init(state):\n",
    "        low_limit = room_limits[0:2]\n",
    "        high_limit = room_limits[2:]\n",
    "        return (state[0] > low_limit[0] and state[0] < high_limit[0] and state[1] > low_limit[1] and state[1] < high_limit[1])\n",
    "    return _init\n",
    "\n",
    "\n",
    "def effect_sample(centroid_init, centroid_end):\n",
    "    def _effect(state):\n",
    "         \n",
    "        translation = state - centroid_init\n",
    "        distance = np.sqrt(((translation)**2).sum())\n",
    "        direction = translation/distance\n",
    "        direction[1] = np.abs(direction[1]) \n",
    "        final_pos = centroid_end + direction * np.array([2, 1])\n",
    "        return np.random.multivariate_normal(mean=np.zeros(state_dim), cov=np.eye(state_dim)/2**6, size=1) + final_pos\n",
    "    return _effect\n",
    "\n",
    "\n",
    "# fix room order to be clockwise\n",
    "for l in (rooms, centroids):\n",
    "    _r = l[2]\n",
    "    l[2] = l[3]\n",
    "    l[3] = _r\n",
    "\n",
    "# Synthetic option definition\n",
    "effect_dists_params = []\n",
    "initiation_sets = []\n",
    "std_dev = 1\n",
    "for i, room in enumerate(rooms):\n",
    "    initiation_sets.append(initiation_set(room))\n",
    "    effect_dists_params.append(effect_sample(centroids[i], centroids[(i+1)%4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "\n",
    "N = 5000\n",
    "\n",
    "x = np.random.uniform(0, X_MAX, N)\n",
    "y = np.random.uniform(0, Y_MAX, N)\n",
    "\n",
    "s = np.array((x,y)).T\n",
    "\n",
    "masks = []\n",
    "for i, init_set in enumerate(initiation_sets):\n",
    "    mask = np.zeros((N,1))\n",
    "    for j in range(N):\n",
    "        mask[j] = float(init_set(s[j]))\n",
    "    masks.append(mask)\n",
    "\n",
    "masks = np.array(masks)\n",
    "I_s = np.array(masks).transpose((1,0,2))[...,0] # initiation vector at s\n",
    "\n",
    "# generate next states\n",
    "s_prime = []\n",
    "for i, effect_dist in enumerate(effect_dists_params):\n",
    "    _s_prime = []\n",
    "    for j in range(N):\n",
    "        _s_prime.append(effect_dist(s[j]))\n",
    "    s_prime.append(np.array(_s_prime).squeeze())\n",
    "\n",
    "\n",
    "# s_prime = np.array(s_prime) * masks  + s[np.newaxis] * (1-masks)\n",
    "s_prime = np.array(s_prime)\n",
    "\n",
    "masks = masks.reshape((-1,))\n",
    "\n",
    "I_s_prime = []\n",
    "for action in range(len(initiation_sets)):\n",
    "    _m = np.zeros((N,len(initiation_sets)))\n",
    "    for i, init_set in enumerate(initiation_sets):\n",
    "        for j in range(N):\n",
    "            _m[j, i] = float(init_set(s_prime[action][j]))\n",
    "    I_s_prime.append(_m)\n",
    "\n",
    "# Random affine transformation\n",
    "x = np.einsum('ij, kj->ki', T, s)\n",
    "x_prime = np.einsum('ij, lkj->lki', T, s_prime)\n",
    "\n",
    "data = []\n",
    "for i in range(len(effect_dists_params)):\n",
    "    data.append((x, x_prime[i], I_s, I_s_prime[i], s, s_prime[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot to Check\n",
    "for r in range(4):\n",
    "    plt.subplot(2, 2, r+1) \n",
    "    plt.scatter(s[I_s[:, r]==1, 0], s[I_s[:, r]==1, 1])\n",
    "    plt.scatter(s_prime[r, I_s[:, r]==1,0], s_prime[r, I_s[:, r]==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "n_actions = 4\n",
    "hidden_size = 64\n",
    "latent_dim = 2\n",
    "\n",
    "def init_models():\n",
    "    encoder = nn.Sequential(\n",
    "                                nn.Linear(obs_dim, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size, latent_dim + 1)\n",
    "                        )\n",
    "\n",
    "    transition_model = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim + n_actions, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, latent_dim + 1)\n",
    "                                )\n",
    "\n",
    "    grounding_model = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, obs_dim + 1)\n",
    "                                )\n",
    "    \n",
    "    initiation_classifier = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, n_actions),\n",
    "                                    nn.Sigmoid()\n",
    "                                )\n",
    "    return encoder, transition_model, grounding_model, initiation_classifier\n",
    "\n",
    "encoder, transition_model, grounding_model, initiation_classifier = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training hyperparameters\n",
    "n_epochs = 60\n",
    "minibatch_size = 32\n",
    "n_samples = 30  # samples to approximate expectation\n",
    "learning_rate = 1e-4\n",
    "beta = 0.01  # hyperparameter to control information bottleneck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data to float torch tensor and add action one-hot encoding\n",
    "in_data = []\n",
    "out_data = []\n",
    "initiations_s = []\n",
    "initiations_s_prime = []\n",
    "for i in range(n_actions):\n",
    "    data[i] = [torch.from_numpy(_tensor).float() for _tensor in data[i]]\n",
    "    actions = nn.functional.one_hot(torch.ones(data[i][0].size(0)).long() * i, n_actions)\n",
    "    data[i] = (torch.cat((data[i][0], actions), dim=1), data[i][1], data[i][2], data[i][3], data[i][4], data[i][5]) \n",
    "    in_data.append(data[i][0])\n",
    "    out_data.append(data[i][1])\n",
    "    initiations_s.append(data[i][2])\n",
    "    initiations_s_prime.append(data[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = torch.cat(in_data, dim=0)[masks == 1]\n",
    "out_data = torch.cat(out_data, dim=0)[masks == 1]\n",
    "initiations_s = torch.cat(initiations_s, dim=0)[masks==1]\n",
    "initiations_s_prime = torch.cat(initiations_s_prime, dim=0)[masks==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "rand_order = torch.randperm(in_data.size(0))\n",
    "in_data = torch.index_select(in_data, 0, rand_order)\n",
    "out_data = torch.index_select(out_data, 0, rand_order)\n",
    "initiations_s = torch.index_select(initiations_s, 0, rand_order)\n",
    "initiations_s_prime = torch.index_select(initiations_s_prime, 0, rand_order)\n",
    "\n",
    "\n",
    "def minibatches(data_list, batch_size):\n",
    "    N = in_data.size(0)\n",
    "    n_batches = N//batch_size + int(N%batch_size != 0)\n",
    "    for i in range(n_batches):\n",
    "        minibatch = [d[i*batch_size:(i+1)*batch_size] for d in data_list]\n",
    "        # yield (in_data[i*batch_size:(i+1)*batch_size], out_data[i*batch_size:(i+1)*batch_size], initiations[i*batch_size:(i+1)*batch_size])\n",
    "        yield minibatch\n",
    "\n",
    "models = (encoder, transition_model, grounding_model, initiation_classifier)\n",
    "models_params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        models_params.append(param)\n",
    "optimizer_forward = torch.optim.RMSprop(models_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_loss = nn.BCELoss()\n",
    "epsilon=1e-5\n",
    "def loss(target, predicted_s_prime, predicted_z):\n",
    "    \n",
    "    target = target.unsqueeze(1).unsqueeze(1)\n",
    "    encoding_loss = 0.5 * predicted_z[..., 0:latent_dim].pow(2).sum(dim=-1) + latent_dim * (torch.exp(predicted_z[..., -1]) - predicted_z[..., -1])\n",
    "    prediction_loss = -0.5 * (target - predicted_s_prime[..., 0:obs_dim]).pow(2).sum(dim=-1, keepdim=True)/(torch.exp(2*predicted_s_prime[..., obs_dim:]) + epsilon) - obs_dim * predicted_s_prime[..., obs_dim:]\n",
    "    _loss = -((1-beta)*prediction_loss - beta * encoding_loss)\n",
    "    return _loss\n",
    "\n",
    "def consistency_loss(target, encoded_z_prime, n_samples=10):\n",
    "\n",
    "    noise = torch.normal(0, 1, (target.size(0), n_samples, latent_dim))\n",
    "    z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * noise + encoded_z_prime.unsqueeze(1)[:,:, :-1]  \n",
    "    s_prime_params = grounding_model(z_prime_samples)\n",
    "    _means = s_prime_params[..., :obs_dim]\n",
    "    _vars = torch.exp(2*s_prime_params[..., obs_dim:])\n",
    "    _target_prediction_sim = torch.einsum('...bj,...bk->...b' ,target.unsqueeze(-2), _means)\n",
    "    _mean_norm = torch.einsum('...j,...k->...' , _means, _means)\n",
    "    _target_norm = torch.einsum('...bj,...bk->...b', target, target).unsqueeze(1)\n",
    "    _losses = obs_dim * torch.log(_vars.squeeze()) + (2 * _target_prediction_sim + _mean_norm + _target_norm)/_vars.squeeze() \n",
    "    return _losses\n",
    "\n",
    "def contrained_transition(transition_params, encoded_z_prime): # mean-seeking\n",
    "    epsilon=1e-5\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_t  + ((mean_z.pow(2) + torch.exp(2*log_sigma_z))/(torch.exp(2*log_sigma_t) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon) + torch.einsum('...i, ...j->...', mean_t, mean_t).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "def transition_consistency(transition_params, encoded_z_prime): # mode-seeking\n",
    "    epsilon=1e-5\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_z  + ((mean_t.pow(2) + torch.exp(2*log_sigma_t))/(torch.exp(2*log_sigma_z) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon) + torch.einsum('...i, ...j->...', mean_z, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "\n",
    "def forward_loss(inp, target, predicted_s_prime, predicted_z, predicted_z_prime, s_prime_params):\n",
    "    return loss(target, predicted_s_prime, predicted_z)\n",
    "\n",
    "def training_loop(in_data, out_data, initiations_s, initiations_s_prime, n_epochs, optimizer, loss_fn=forward_loss, print_loss=False):\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for _in, _out, _init_s, _init_s_prime in minibatches((in_data, out_data, initiations_s, initiations_s_prime), minibatch_size):\n",
    "            ## Forward pass\n",
    "            actions = _in[..., obs_dim:].unsqueeze(1).repeat_interleave(n_samples, dim=1)\n",
    "            z = encoder(_in[...,:obs_dim])  # encode\n",
    "            # Predict next abstract state\n",
    "            noise = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_ = torch.exp(z.unsqueeze(1)[:, :, -2:-1]) * noise + z.unsqueeze(1)[:,:, :-1]   \n",
    "            z_prime = transition_model(torch.cat((z_, actions), dim=-1))\n",
    "            \n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_prime_samples = torch.exp(z_prime[:, :, -2:-1]) * epsilon + z_prime[:,:, :-1]  \n",
    "            \n",
    "            encoded_z_prime = encoder(_out)\n",
    "            # z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * epsilon + encoded_z_prime.unsqueeze(1)[:,:, :-1] \n",
    "\n",
    "            # predict next ground state\n",
    "            s_prime = grounding_model(z_prime_samples).unsqueeze(2)\n",
    "            # s_prime = grounding_model(encoded_z_prime)\n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, n_samples, obs_dim))\n",
    "            s_prime_samples = torch.exp(s_prime[..., :obs_dim]) * epsilon + s_prime[..., :obs_dim]\n",
    "            # s_prime = s_prime.squeeze(1)\n",
    "            \n",
    "            # Predict initiation vector from (z, z')\n",
    "            predicted_I_s = initiation_classifier(z_[:, 0])\n",
    "            predicted_I_s_prime = initiation_classifier(z_prime_samples[:, 0])\n",
    "\n",
    "            # initiation_prediction = predicted_I_s.squeeze()#torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            # initiation_target = _init_s#torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            initiation_prediction = torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            initiation_target = torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            \n",
    "            \n",
    "            # binary classifier loss\n",
    "            _classifier_loss = classifier_loss(initiation_prediction, initiation_target)\n",
    "            \n",
    "            _target = _out\n",
    "            # _transition_constraint = contrained_transition(z_prime.detach(), encoded_z_prime).mean()\n",
    "            _transition_constraint = transition_consistency(z_prime, encoded_z_prime.detach()).mean()\n",
    "            _consistency_loss = consistency_loss(_out, encoded_z_prime).mean()\n",
    "            _encoding_loss = loss_fn(_in, _target, s_prime, z, z_prime_samples, s_prime).mean()\n",
    "\n",
    "            trans_const = 0.2\n",
    "\n",
    "            _loss = (1-trans_const) * nn.functional.gelu(10 *_encoding_loss) + nn.functional.relu(0 * _consistency_loss) + 100 * _classifier_loss + trans_const * nn.functional.leaky_relu(1 * _transition_constraint, negative_slope=0.001)\n",
    "            # _loss = _encoding_loss + _consistency_loss + _transition_constraint\n",
    "\n",
    "            \n",
    "            if print_loss:\n",
    "                print(f\"{i}: Encoding Loss {_encoding_loss}, Transition Loss {_transition_constraint}, Classifier Loss {_classifier_loss}, Total Loss {_loss}\")\n",
    "            \n",
    "            ### zero grads \n",
    "            optimizer.zero_grad()\n",
    "            ### backward pass\n",
    "            _loss.backward()\n",
    "            \n",
    "            ### update\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Encoding Loss 228.77984619140625, Transition Loss 28.74698257446289, Classifier Loss 0.6936162114143372, Total Loss 1905.349609375\n",
      "0: Encoding Loss 209.55184936523438, Transition Loss 20.45785903930664, Classifier Loss 0.6808907985687256, Total Loss 1748.5955810546875\n",
      "0: Encoding Loss 207.1597442626953, Transition Loss 23.131498336791992, Classifier Loss 0.6813125014305115, Total Loss 1730.0355224609375\n",
      "0: Encoding Loss 231.65765380859375, Transition Loss 17.21235466003418, Classifier Loss 0.6821011304855347, Total Loss 1924.9139404296875\n",
      "0: Encoding Loss 195.00869750976562, Transition Loss 20.467105865478516, Classifier Loss 0.6718468070030212, Total Loss 1631.34765625\n",
      "0: Encoding Loss 234.5523223876953, Transition Loss 28.379669189453125, Classifier Loss 0.6805892586708069, Total Loss 1950.1534423828125\n",
      "0: Encoding Loss 212.2755584716797, Transition Loss 27.569091796875, Classifier Loss 0.6684984564781189, Total Loss 1770.568115234375\n",
      "0: Encoding Loss 201.72427368164062, Transition Loss 34.88249588012695, Classifier Loss 0.6741903424263, Total Loss 1688.189697265625\n",
      "0: Encoding Loss 208.33297729492188, Transition Loss 29.48294448852539, Classifier Loss 0.6682348251342773, Total Loss 1739.384033203125\n",
      "0: Encoding Loss 178.04736328125, Transition Loss 25.83698844909668, Classifier Loss 0.659464955329895, Total Loss 1495.4927978515625\n",
      "0: Encoding Loss 177.4889678955078, Transition Loss 30.40673065185547, Classifier Loss 0.6531047224998474, Total Loss 1491.303466796875\n",
      "0: Encoding Loss 158.05238342285156, Transition Loss 25.917957305908203, Classifier Loss 0.6541175246238708, Total Loss 1335.014404296875\n",
      "0: Encoding Loss 171.017578125, Transition Loss 30.86448860168457, Classifier Loss 0.648598313331604, Total Loss 1439.17333984375\n",
      "0: Encoding Loss 149.340087890625, Transition Loss 18.61742401123047, Classifier Loss 0.6453565359115601, Total Loss 1262.9798583984375\n",
      "0: Encoding Loss 206.9376983642578, Transition Loss 33.56507873535156, Classifier Loss 0.6526738405227661, Total Loss 1727.48193359375\n",
      "0: Encoding Loss 218.1863555908203, Transition Loss 54.435951232910156, Classifier Loss 0.6660569906234741, Total Loss 1822.9837646484375\n",
      "0: Encoding Loss 186.75401306152344, Transition Loss 42.30337142944336, Classifier Loss 0.6418503522872925, Total Loss 1566.6778564453125\n",
      "0: Encoding Loss 135.5653839111328, Transition Loss 25.002817153930664, Classifier Loss 0.6256899237632751, Total Loss 1152.0926513671875\n",
      "0: Encoding Loss 163.4037322998047, Transition Loss 35.69773483276367, Classifier Loss 0.6440873146057129, Total Loss 1378.778076171875\n",
      "0: Encoding Loss 161.2615509033203, Transition Loss 37.818077087402344, Classifier Loss 0.6271716952323914, Total Loss 1360.3731689453125\n",
      "0: Encoding Loss 165.03347778320312, Transition Loss 30.9222354888916, Classifier Loss 0.6399273872375488, Total Loss 1390.4449462890625\n",
      "0: Encoding Loss 169.11956787109375, Transition Loss 38.86882019042969, Classifier Loss 0.6312841773033142, Total Loss 1423.8587646484375\n",
      "0: Encoding Loss 192.255126953125, Transition Loss 54.5703125, Classifier Loss 0.6356940865516663, Total Loss 1612.5245361328125\n",
      "0: Encoding Loss 144.29010009765625, Transition Loss 39.94426345825195, Classifier Loss 0.6194949150085449, Total Loss 1224.2591552734375\n",
      "0: Encoding Loss 154.66441345214844, Transition Loss 35.3100471496582, Classifier Loss 0.6181955337524414, Total Loss 1306.1968994140625\n",
      "0: Encoding Loss 124.25821685791016, Transition Loss 22.82796859741211, Classifier Loss 0.6059030294418335, Total Loss 1059.2215576171875\n",
      "0: Encoding Loss 135.6876983642578, Transition Loss 26.610401153564453, Classifier Loss 0.6025851368904114, Total Loss 1151.0821533203125\n",
      "0: Encoding Loss 188.4943084716797, Transition Loss 44.72012710571289, Classifier Loss 0.6385414600372314, Total Loss 1580.7525634765625\n",
      "0: Encoding Loss 136.64381408691406, Transition Loss 38.78213882446289, Classifier Loss 0.6145893335342407, Total Loss 1162.365966796875\n",
      "0: Encoding Loss 125.59941101074219, Transition Loss 28.335464477539062, Classifier Loss 0.5958976745605469, Total Loss 1070.05224609375\n",
      "0: Encoding Loss 130.68505859375, Transition Loss 29.494836807250977, Classifier Loss 0.6085183620452881, Total Loss 1112.231201171875\n",
      "0: Encoding Loss 98.29949188232422, Transition Loss 29.585819244384766, Classifier Loss 0.595009982585907, Total Loss 851.8140869140625\n",
      "0: Encoding Loss 120.38255310058594, Transition Loss 30.181320190429688, Classifier Loss 0.6087306141853333, Total Loss 1029.9698486328125\n",
      "0: Encoding Loss 115.56049346923828, Transition Loss 37.31203079223633, Classifier Loss 0.6012074947357178, Total Loss 992.067138671875\n",
      "0: Encoding Loss 117.12886810302734, Transition Loss 23.845304489135742, Classifier Loss 0.6047307252883911, Total Loss 1002.2730712890625\n",
      "0: Encoding Loss 97.52995300292969, Transition Loss 23.195024490356445, Classifier Loss 0.5941644906997681, Total Loss 844.2951049804688\n",
      "0: Encoding Loss 113.7571029663086, Transition Loss 42.572139739990234, Classifier Loss 0.5899461507797241, Total Loss 977.5658569335938\n",
      "0: Encoding Loss 91.1966323852539, Transition Loss 27.433177947998047, Classifier Loss 0.5892408490180969, Total Loss 793.9837646484375\n",
      "0: Encoding Loss 120.3104476928711, Transition Loss 33.99226379394531, Classifier Loss 0.5916212797164917, Total Loss 1028.444091796875\n",
      "0: Encoding Loss 104.01366424560547, Transition Loss 21.159486770629883, Classifier Loss 0.5857193470001221, Total Loss 894.9131469726562\n",
      "0: Encoding Loss 92.467529296875, Transition Loss 24.809934616088867, Classifier Loss 0.5829181671142578, Total Loss 802.9940185546875\n",
      "0: Encoding Loss 114.44944763183594, Transition Loss 32.04254150390625, Classifier Loss 0.5967787504196167, Total Loss 981.6820068359375\n",
      "0: Encoding Loss 103.0564193725586, Transition Loss 21.94817543029785, Classifier Loss 0.5915119647979736, Total Loss 887.9921875\n",
      "0: Encoding Loss 74.9459457397461, Transition Loss 8.22449016571045, Classifier Loss 0.57065749168396, Total Loss 658.2781982421875\n",
      "0: Encoding Loss 91.5486068725586, Transition Loss 22.689851760864258, Classifier Loss 0.5972365140914917, Total Loss 796.6504516601562\n",
      "0: Encoding Loss 80.49056243896484, Transition Loss 18.671924591064453, Classifier Loss 0.5705891251564026, Total Loss 704.7177734375\n",
      "0: Encoding Loss 93.39424133300781, Transition Loss 12.09846019744873, Classifier Loss 0.5956774353981018, Total Loss 809.141357421875\n",
      "0: Encoding Loss 64.88568115234375, Transition Loss 8.348697662353516, Classifier Loss 0.5676512122154236, Total Loss 577.5203247070312\n",
      "0: Encoding Loss 82.41795349121094, Transition Loss 13.301025390625, Classifier Loss 0.5733680725097656, Total Loss 719.3406982421875\n",
      "0: Encoding Loss 84.84297943115234, Transition Loss 15.600997924804688, Classifier Loss 0.585307240486145, Total Loss 740.3947143554688\n",
      "0: Encoding Loss 87.718017578125, Transition Loss 26.82540512084961, Classifier Loss 0.5969058275222778, Total Loss 766.7997436523438\n",
      "0: Encoding Loss 82.76016235351562, Transition Loss 18.360679626464844, Classifier Loss 0.5802881717681885, Total Loss 723.7822265625\n",
      "0: Encoding Loss 62.36688232421875, Transition Loss 17.081722259521484, Classifier Loss 0.5600681900978088, Total Loss 558.3582153320312\n",
      "0: Encoding Loss 69.27976989746094, Transition Loss 17.07949447631836, Classifier Loss 0.57591313123703, Total Loss 615.2454223632812\n",
      "0: Encoding Loss 67.15677642822266, Transition Loss 17.53938865661621, Classifier Loss 0.5720154047012329, Total Loss 597.963623046875\n",
      "0: Encoding Loss 54.65810012817383, Transition Loss 5.204935073852539, Classifier Loss 0.562947690486908, Total Loss 494.6005554199219\n",
      "0: Encoding Loss 64.93460083007812, Transition Loss 13.103498458862305, Classifier Loss 0.5707648992538452, Total Loss 579.1740112304688\n",
      "0: Encoding Loss 71.28492736816406, Transition Loss 18.68058967590332, Classifier Loss 0.5758365392684937, Total Loss 631.5992431640625\n",
      "0: Encoding Loss 62.27244186401367, Transition Loss 17.67451286315918, Classifier Loss 0.5847426652908325, Total Loss 560.188720703125\n",
      "0: Encoding Loss 69.47748565673828, Transition Loss 16.858531951904297, Classifier Loss 0.5773971080780029, Total Loss 616.9312744140625\n",
      "0: Encoding Loss 57.387939453125, Transition Loss 14.6040678024292, Classifier Loss 0.5660391449928284, Total Loss 518.6282958984375\n",
      "0: Encoding Loss 54.83903884887695, Transition Loss 10.87607192993164, Classifier Loss 0.5635457038879395, Total Loss 497.2420959472656\n",
      "0: Encoding Loss 53.70616912841797, Transition Loss 13.508026123046875, Classifier Loss 0.567452609539032, Total Loss 489.09625244140625\n",
      "0: Encoding Loss 45.4572868347168, Transition Loss 6.340800762176514, Classifier Loss 0.5623555183410645, Total Loss 421.1620178222656\n",
      "0: Encoding Loss 48.317684173583984, Transition Loss 14.641936302185059, Classifier Loss 0.5703703761100769, Total Loss 446.50689697265625\n",
      "0: Encoding Loss 44.97120666503906, Transition Loss 6.574719429016113, Classifier Loss 0.5569009780883789, Total Loss 416.7746887207031\n",
      "0: Encoding Loss 37.04866409301758, Transition Loss 0.47113358974456787, Classifier Loss 0.5455119013786316, Total Loss 351.03472900390625\n",
      "0: Encoding Loss 63.13517379760742, Transition Loss 14.355010986328125, Classifier Loss 0.5826937556266785, Total Loss 566.2218017578125\n",
      "0: Encoding Loss 53.59593963623047, Transition Loss 3.014282703399658, Classifier Loss 0.5628219246864319, Total Loss 485.652587890625\n",
      "0: Encoding Loss 44.45990753173828, Transition Loss 9.052352905273438, Classifier Loss 0.5589004158973694, Total Loss 413.3797607421875\n",
      "0: Encoding Loss 40.208763122558594, Transition Loss 5.475257873535156, Classifier Loss 0.5564592480659485, Total Loss 378.4111328125\n",
      "0: Encoding Loss 39.167823791503906, Transition Loss 2.9884116649627686, Classifier Loss 0.5605863332748413, Total Loss 369.9989013671875\n",
      "0: Encoding Loss 40.900333404541016, Transition Loss 3.801287889480591, Classifier Loss 0.5495414137840271, Total Loss 382.91705322265625\n",
      "0: Encoding Loss 46.39542007446289, Transition Loss 4.182553291320801, Classifier Loss 0.5587033033370972, Total Loss 427.8702087402344\n",
      "0: Encoding Loss 43.08780288696289, Transition Loss 2.703490734100342, Classifier Loss 0.5566034913063049, Total Loss 400.9034729003906\n",
      "0: Encoding Loss 42.505550384521484, Transition Loss 2.9968631267547607, Classifier Loss 0.5706281661987305, Total Loss 397.7065734863281\n",
      "0: Encoding Loss 41.48264694213867, Transition Loss 1.109740972518921, Classifier Loss 0.5447851419448853, Total Loss 386.5616455078125\n",
      "0: Encoding Loss 46.51829528808594, Transition Loss 7.150853157043457, Classifier Loss 0.5709913969039917, Total Loss 430.6756896972656\n",
      "0: Encoding Loss 46.70168685913086, Transition Loss 4.833252429962158, Classifier Loss 0.5585973262786865, Total Loss 430.43988037109375\n",
      "0: Encoding Loss 37.5603141784668, Transition Loss -0.5708418488502502, Classifier Loss 0.552078366279602, Total Loss 355.6902160644531\n",
      "0: Encoding Loss 43.15965270996094, Transition Loss 3.0764808654785156, Classifier Loss 0.5652918815612793, Total Loss 402.42169189453125\n",
      "0: Encoding Loss 37.38429260253906, Transition Loss -0.8581178188323975, Classifier Loss 0.553554892539978, Total Loss 354.4296569824219\n",
      "0: Encoding Loss 38.97841262817383, Transition Loss 0.7153980731964111, Classifier Loss 0.5487620830535889, Total Loss 366.8465881347656\n",
      "0: Encoding Loss 35.354270935058594, Transition Loss 3.5447819232940674, Classifier Loss 0.5756062865257263, Total Loss 341.1037902832031\n",
      "0: Encoding Loss 35.87229537963867, Transition Loss -2.1465630531311035, Classifier Loss 0.5577914714813232, Total Loss 342.757080078125\n",
      "0: Encoding Loss 35.6363525390625, Transition Loss -3.9491677284240723, Classifier Loss 0.5521469712257385, Total Loss 340.3047180175781\n",
      "0: Encoding Loss 39.0325927734375, Transition Loss 8.520440101623535, Classifier Loss 0.5679301023483276, Total Loss 370.7578430175781\n",
      "0: Encoding Loss 38.19755172729492, Transition Loss -3.212296962738037, Classifier Loss 0.5434088706970215, Total Loss 359.920654296875\n",
      "0: Encoding Loss 34.11697006225586, Transition Loss -2.1718251705169678, Classifier Loss 0.5622062683105469, Total Loss 329.15594482421875\n",
      "0: Encoding Loss 41.10787582397461, Transition Loss 2.563845634460449, Classifier Loss 0.5733585357666016, Total Loss 386.71160888671875\n",
      "0: Encoding Loss 38.668678283691406, Transition Loss 6.674744129180908, Classifier Loss 0.5777245759963989, Total Loss 368.45684814453125\n",
      "0: Encoding Loss 28.001745223999023, Transition Loss -4.7017621994018555, Classifier Loss 0.5636692047119141, Total Loss 280.37994384765625\n",
      "0: Encoding Loss 35.138633728027344, Transition Loss -1.2041471004486084, Classifier Loss 0.546402633190155, Total Loss 335.7491149902344\n",
      "0: Encoding Loss 32.83529281616211, Transition Loss 0.7035197019577026, Classifier Loss 0.5570614337921143, Total Loss 318.5292053222656\n",
      "0: Encoding Loss 28.7529239654541, Transition Loss -5.724972724914551, Classifier Loss 0.5535022020339966, Total Loss 285.3724365234375\n",
      "0: Encoding Loss 37.50905227661133, Transition Loss -0.7696807384490967, Classifier Loss 0.5674448013305664, Total Loss 356.8167419433594\n",
      "0: Encoding Loss 35.885440826416016, Transition Loss -1.0030255317687988, Classifier Loss 0.5641362071037292, Total Loss 343.4969177246094\n",
      "0: Encoding Loss 30.373516082763672, Transition Loss -5.924077033996582, Classifier Loss 0.5543991923332214, Total Loss 298.4268798828125\n",
      "0: Encoding Loss 31.4621524810791, Transition Loss -10.73452377319336, Classifier Loss 0.5431070923805237, Total Loss 306.00579833984375\n",
      "0: Encoding Loss 29.05891990661621, Transition Loss -2.8016481399536133, Classifier Loss 0.5497909784317017, Total Loss 287.4499206542969\n",
      "0: Encoding Loss 36.29685974121094, Transition Loss 1.2399799823760986, Classifier Loss 0.5622574090957642, Total Loss 346.8486022949219\n",
      "0: Encoding Loss 26.223608016967773, Transition Loss -10.479228019714355, Classifier Loss 0.5433834791183472, Total Loss 264.1250915527344\n",
      "0: Encoding Loss 36.50352478027344, Transition Loss -4.881645202636719, Classifier Loss 0.5552980899810791, Total Loss 347.5570373535156\n",
      "0: Encoding Loss 28.04635238647461, Transition Loss -2.9962852001190186, Classifier Loss 0.554145872592926, Total Loss 279.7848205566406\n",
      "0: Encoding Loss 27.706514358520508, Transition Loss -4.455971717834473, Classifier Loss 0.570936381816864, Total Loss 278.744873046875\n",
      "0: Encoding Loss 34.71236801147461, Transition Loss -0.8895303010940552, Classifier Loss 0.5599766373634338, Total Loss 333.6964111328125\n",
      "0: Encoding Loss 28.82486915588379, Transition Loss -7.942236423492432, Classifier Loss 0.5545594096183777, Total Loss 286.0533142089844\n",
      "0: Encoding Loss 36.09341049194336, Transition Loss -11.584759712219238, Classifier Loss 0.5507819056510925, Total Loss 343.8231506347656\n",
      "0: Encoding Loss 28.12605094909668, Transition Loss -8.064155578613281, Classifier Loss 0.5476878881454468, Total Loss 279.77557373046875\n",
      "0: Encoding Loss 30.128284454345703, Transition Loss -2.915799140930176, Classifier Loss 0.56156325340271, Total Loss 297.1820373535156\n",
      "0: Encoding Loss 29.540037155151367, Transition Loss -1.2406721115112305, Classifier Loss 0.5653003454208374, Total Loss 292.85009765625\n",
      "0: Encoding Loss 28.734283447265625, Transition Loss -9.900348663330078, Classifier Loss 0.5494121313095093, Total Loss 284.8135070800781\n",
      "0: Encoding Loss 38.89616012573242, Transition Loss 2.090085029602051, Classifier Loss 0.5639537572860718, Total Loss 367.9826965332031\n",
      "0: Encoding Loss 36.31541442871094, Transition Loss 0.945592999458313, Classifier Loss 0.5684763193130493, Total Loss 347.56005859375\n",
      "0: Encoding Loss 23.89557647705078, Transition Loss -12.838288307189941, Classifier Loss 0.5357061624526978, Total Loss 244.732666015625\n",
      "0: Encoding Loss 30.190176010131836, Transition Loss -12.235211372375488, Classifier Loss 0.5489814877510071, Total Loss 296.4171142578125\n",
      "0: Encoding Loss 34.68317413330078, Transition Loss -4.204269886016846, Classifier Loss 0.5519053936004639, Total Loss 332.65509033203125\n",
      "0: Encoding Loss 27.79781723022461, Transition Loss -7.632241725921631, Classifier Loss 0.5576533675193787, Total Loss 278.1463623046875\n",
      "0: Encoding Loss 32.497108459472656, Transition Loss -7.711832523345947, Classifier Loss 0.5513187050819397, Total Loss 315.107177734375\n",
      "0: Encoding Loss 26.68988800048828, Transition Loss -10.119729995727539, Classifier Loss 0.5443851947784424, Total Loss 267.9555969238281\n",
      "0: Encoding Loss 35.386634826660156, Transition Loss -2.4213433265686035, Classifier Loss 0.5611250400543213, Total Loss 339.205078125\n",
      "0: Encoding Loss 30.87197494506836, Transition Loss -14.85161304473877, Classifier Loss 0.5461916327476501, Total Loss 301.5920104980469\n",
      "0: Encoding Loss 32.65936279296875, Transition Loss -3.2385711669921875, Classifier Loss 0.561305046081543, Total Loss 317.4047546386719\n",
      "0: Encoding Loss 33.971954345703125, Transition Loss -4.297962188720703, Classifier Loss 0.566328763961792, Total Loss 328.40765380859375\n",
      "0: Encoding Loss 36.0621337890625, Transition Loss -8.893292427062988, Classifier Loss 0.5478348731994629, Total Loss 343.2787780761719\n",
      "0: Encoding Loss 28.24932098388672, Transition Loss -5.540200233459473, Classifier Loss 0.5578306317329407, Total Loss 281.77655029296875\n",
      "0: Encoding Loss 28.80299186706543, Transition Loss -3.5590758323669434, Classifier Loss 0.5527969002723694, Total Loss 285.7029113769531\n",
      "0: Encoding Loss 35.4019889831543, Transition Loss -12.236306190490723, Classifier Loss 0.5443506836891174, Total Loss 337.6485290527344\n",
      "0: Encoding Loss 25.71993064880371, Transition Loss -7.858983516693115, Classifier Loss 0.5550607442855835, Total Loss 261.263916015625\n",
      "0: Encoding Loss 25.49587059020996, Transition Loss -15.600865364074707, Classifier Loss 0.5501723885536194, Total Loss 258.9810791015625\n",
      "0: Encoding Loss 28.03741455078125, Transition Loss -11.169910430908203, Classifier Loss 0.5517273545265198, Total Loss 279.4698181152344\n",
      "0: Encoding Loss 29.20989227294922, Transition Loss -7.5299601554870605, Classifier Loss 0.5534210801124573, Total Loss 289.019775390625\n",
      "0: Encoding Loss 27.95622444152832, Transition Loss -11.281881332397461, Classifier Loss 0.5409362316131592, Total Loss 277.7411804199219\n",
      "0: Encoding Loss 26.27126121520996, Transition Loss -8.260289192199707, Classifier Loss 0.5474160313606262, Total Loss 264.9100341796875\n",
      "0: Encoding Loss 30.17576789855957, Transition Loss -5.1455397605896, Classifier Loss 0.558092474937439, Total Loss 297.21435546875\n",
      "0: Encoding Loss 29.334375381469727, Transition Loss -1.2864274978637695, Classifier Loss 0.5573678016662598, Total Loss 290.4115295410156\n",
      "0: Encoding Loss 25.57064437866211, Transition Loss -14.107410430908203, Classifier Loss 0.5443269610404968, Total Loss 258.99505615234375\n",
      "0: Encoding Loss 30.240671157836914, Transition Loss -10.862812042236328, Classifier Loss 0.5530609488487244, Total Loss 297.22930908203125\n",
      "0: Encoding Loss 29.84515380859375, Transition Loss -12.209966659545898, Classifier Loss 0.5518935918807983, Total Loss 293.9481506347656\n",
      "0: Encoding Loss 27.349098205566406, Transition Loss -5.6970720291137695, Classifier Loss 0.5648713111877441, Total Loss 275.2787780761719\n",
      "0: Encoding Loss 31.12619972229004, Transition Loss -7.787015914916992, Classifier Loss 0.5523576736450195, Total Loss 304.2438049316406\n",
      "0: Encoding Loss 28.746225357055664, Transition Loss -13.776925086975098, Classifier Loss 0.541953444480896, Total Loss 284.16241455078125\n",
      "0: Encoding Loss 30.409912109375, Transition Loss -9.733781814575195, Classifier Loss 0.5683881640434265, Total Loss 300.11614990234375\n",
      "0: Encoding Loss 23.639921188354492, Transition Loss -16.36042594909668, Classifier Loss 0.5369536876678467, Total Loss 242.8114776611328\n",
      "0: Encoding Loss 27.49709701538086, Transition Loss -11.914141654968262, Classifier Loss 0.5492810606956482, Total Loss 274.90252685546875\n",
      "0: Encoding Loss 29.900781631469727, Transition Loss -13.394083023071289, Classifier Loss 0.5442067980766296, Total Loss 293.6242370605469\n",
      "0: Encoding Loss 29.706493377685547, Transition Loss -10.216052055358887, Classifier Loss 0.5558242797851562, Total Loss 293.2323303222656\n",
      "0: Encoding Loss 29.665454864501953, Transition Loss -3.083340644836426, Classifier Loss 0.5711994767189026, Total Loss 294.4429931640625\n",
      "0: Encoding Loss 28.575061798095703, Transition Loss -7.661764144897461, Classifier Loss 0.5708715915679932, Total Loss 285.6861267089844\n",
      "0: Encoding Loss 25.85157012939453, Transition Loss -7.9167561531066895, Classifier Loss 0.5536801815032959, Total Loss 262.1789855957031\n",
      "0: Encoding Loss 30.825592041015625, Transition Loss -5.370844841003418, Classifier Loss 0.5568636059761047, Total Loss 302.2900390625\n",
      "0: Encoding Loss 29.05731964111328, Transition Loss -5.679083824157715, Classifier Loss 0.5600088834762573, Total Loss 288.45831298828125\n",
      "0: Encoding Loss 28.496402740478516, Transition Loss -10.65194320678711, Classifier Loss 0.5498675107955933, Total Loss 282.9558410644531\n",
      "0: Encoding Loss 28.841541290283203, Transition Loss -7.826967239379883, Classifier Loss 0.5502042770385742, Total Loss 285.7511901855469\n",
      "0: Encoding Loss 27.613513946533203, Transition Loss -6.814986228942871, Classifier Loss 0.5580512881278992, Total Loss 276.7118835449219\n",
      "0: Encoding Loss 26.272296905517578, Transition Loss -11.38230037689209, Classifier Loss 0.5630416870117188, Total Loss 266.4802551269531\n",
      "0: Encoding Loss 31.289106369018555, Transition Loss -3.1268954277038574, Classifier Loss 0.5609732270240784, Total Loss 306.4095764160156\n",
      "1: Encoding Loss 27.482606887817383, Transition Loss -19.376283645629883, Classifier Loss 0.542305052280426, Total Loss 274.0874938964844\n",
      "1: Encoding Loss 27.817699432373047, Transition Loss -10.944823265075684, Classifier Loss 0.554524302482605, Total Loss 277.9918518066406\n",
      "1: Encoding Loss 31.055004119873047, Transition Loss -15.769929885864258, Classifier Loss 0.5431342720985413, Total Loss 302.7503356933594\n",
      "1: Encoding Loss 27.84069061279297, Transition Loss -5.787192344665527, Classifier Loss 0.5627391338348389, Total Loss 278.998291015625\n",
      "1: Encoding Loss 29.042219161987305, Transition Loss -7.946281909942627, Classifier Loss 0.5617508292198181, Total Loss 288.5112609863281\n",
      "1: Encoding Loss 29.286823272705078, Transition Loss -10.899423599243164, Classifier Loss 0.5563110113143921, Total Loss 289.92352294921875\n",
      "1: Encoding Loss 27.428897857666016, Transition Loss -11.34022045135498, Classifier Loss 0.552453339099884, Total Loss 274.67425537109375\n",
      "1: Encoding Loss 25.8234920501709, Transition Loss -15.21157455444336, Classifier Loss 0.5480482578277588, Total Loss 261.38970947265625\n",
      "1: Encoding Loss 28.18583869934082, Transition Loss -11.054899215698242, Classifier Loss 0.5590713620185852, Total Loss 281.39166259765625\n",
      "1: Encoding Loss 28.97934913635254, Transition Loss -12.327529907226562, Classifier Loss 0.5561962127685547, Total Loss 287.4519348144531\n",
      "1: Encoding Loss 26.247968673706055, Transition Loss -7.797554016113281, Classifier Loss 0.5550227761268616, Total Loss 265.4844665527344\n",
      "1: Encoding Loss 25.215120315551758, Transition Loss -18.005573272705078, Classifier Loss 0.5468084812164307, Total Loss 256.398193359375\n",
      "1: Encoding Loss 26.147907257080078, Transition Loss -13.359615325927734, Classifier Loss 0.5487576723098755, Total Loss 264.05633544921875\n",
      "1: Encoding Loss 27.595125198364258, Transition Loss -12.660807609558105, Classifier Loss 0.5514030456542969, Total Loss 275.8987731933594\n",
      "1: Encoding Loss 27.486120223999023, Transition Loss -7.885972023010254, Classifier Loss 0.554782509803772, Total Loss 275.3656311035156\n",
      "1: Encoding Loss 28.744876861572266, Transition Loss -12.374890327453613, Classifier Loss 0.5544213652610779, Total Loss 285.398681640625\n",
      "1: Encoding Loss 33.877586364746094, Transition Loss -9.45893383026123, Classifier Loss 0.5553554892539978, Total Loss 326.55438232421875\n",
      "1: Encoding Loss 29.042924880981445, Transition Loss -8.534029006958008, Classifier Loss 0.5549660325050354, Total Loss 287.83831787109375\n",
      "1: Encoding Loss 27.934505462646484, Transition Loss -13.820953369140625, Classifier Loss 0.5563466548919678, Total Loss 279.1079406738281\n",
      "1: Encoding Loss 29.665773391723633, Transition Loss -10.732218742370605, Classifier Loss 0.5464912056922913, Total Loss 291.9731750488281\n",
      "1: Encoding Loss 28.490726470947266, Transition Loss -10.36372184753418, Classifier Loss 0.5613856315612793, Total Loss 284.06231689453125\n",
      "1: Encoding Loss 30.556140899658203, Transition Loss -8.43603515625, Classifier Loss 0.553554892539978, Total Loss 299.8029479980469\n",
      "1: Encoding Loss 30.38083267211914, Transition Loss -7.531723976135254, Classifier Loss 0.5547143816947937, Total Loss 298.5166015625\n",
      "1: Encoding Loss 28.425870895385742, Transition Loss -15.574496269226074, Classifier Loss 0.5440621376037598, Total Loss 281.81005859375\n",
      "1: Encoding Loss 26.475624084472656, Transition Loss -9.508662223815918, Classifier Loss 0.555549144744873, Total Loss 267.3580017089844\n",
      "1: Encoding Loss 29.810932159423828, Transition Loss -10.064549446105957, Classifier Loss 0.5467034578323364, Total Loss 293.1557922363281\n",
      "1: Encoding Loss 28.136878967285156, Transition Loss -12.928109169006348, Classifier Loss 0.5448808670043945, Total Loss 279.58050537109375\n",
      "1: Encoding Loss 29.33568000793457, Transition Loss -8.220309257507324, Classifier Loss 0.5603657960891724, Total Loss 290.72039794921875\n",
      "1: Encoding Loss 26.949106216430664, Transition Loss -16.1706485748291, Classifier Loss 0.5470283031463623, Total Loss 270.2924499511719\n",
      "1: Encoding Loss 29.406108856201172, Transition Loss -10.933826446533203, Classifier Loss 0.5441072583198547, Total Loss 289.65740966796875\n",
      "1: Encoding Loss 29.36848258972168, Transition Loss -6.142934799194336, Classifier Loss 0.5537264943122864, Total Loss 290.31927490234375\n",
      "1: Encoding Loss 25.11252212524414, Transition Loss -16.410402297973633, Classifier Loss 0.5469491481781006, Total Loss 255.59181213378906\n",
      "1: Encoding Loss 26.90236473083496, Transition Loss -8.169764518737793, Classifier Loss 0.5569738745689392, Total Loss 270.9146728515625\n",
      "1: Encoding Loss 24.040910720825195, Transition Loss -14.470706939697266, Classifier Loss 0.5446600317955017, Total Loss 246.79039001464844\n",
      "1: Encoding Loss 28.30006980895996, Transition Loss -9.802580833435059, Classifier Loss 0.5603863000869751, Total Loss 282.4372253417969\n",
      "1: Encoding Loss 27.542139053344727, Transition Loss -17.677749633789062, Classifier Loss 0.5447280406951904, Total Loss 274.8063659667969\n",
      "1: Encoding Loss 25.991533279418945, Transition Loss -10.517744064331055, Classifier Loss 0.5454559326171875, Total Loss 262.47576904296875\n",
      "1: Encoding Loss 24.041730880737305, Transition Loss -16.226734161376953, Classifier Loss 0.5441133379936218, Total Loss 246.74192810058594\n",
      "1: Encoding Loss 29.81249237060547, Transition Loss -9.719518661499023, Classifier Loss 0.5432778596878052, Total Loss 292.8257751464844\n",
      "1: Encoding Loss 32.15365982055664, Transition Loss -5.300804138183594, Classifier Loss 0.5585048198699951, Total Loss 313.0787048339844\n",
      "1: Encoding Loss 27.105058670043945, Transition Loss -14.762012481689453, Classifier Loss 0.5475896000862122, Total Loss 271.59649658203125\n",
      "1: Encoding Loss 29.23116111755371, Transition Loss -5.090559959411621, Classifier Loss 0.5549433827400208, Total Loss 289.3426208496094\n",
      "1: Encoding Loss 28.2015323638916, Transition Loss -8.681683540344238, Classifier Loss 0.5578230023384094, Total Loss 281.392822265625\n",
      "1: Encoding Loss 26.018590927124023, Transition Loss -13.150495529174805, Classifier Loss 0.5475013852119446, Total Loss 262.896240234375\n",
      "1: Encoding Loss 27.372873306274414, Transition Loss -9.897701263427734, Classifier Loss 0.562864363193512, Total Loss 275.2674255371094\n",
      "1: Encoding Loss 27.640846252441406, Transition Loss -10.201870918273926, Classifier Loss 0.5456809997558594, Total Loss 275.69281005859375\n",
      "1: Encoding Loss 29.01477813720703, Transition Loss -5.224719524383545, Classifier Loss 0.5656504034996033, Total Loss 288.6822204589844\n",
      "1: Encoding Loss 25.923175811767578, Transition Loss -12.796011924743652, Classifier Loss 0.5466597080230713, Total Loss 262.048828125\n",
      "1: Encoding Loss 28.231794357299805, Transition Loss -7.869288921356201, Classifier Loss 0.5504027605056763, Total Loss 280.8930358886719\n",
      "1: Encoding Loss 26.80095863342285, Transition Loss -8.31707763671875, Classifier Loss 0.5569220185279846, Total Loss 270.09820556640625\n",
      "1: Encoding Loss 26.869874954223633, Transition Loss -11.72969913482666, Classifier Loss 0.5598079562187195, Total Loss 270.9374694824219\n",
      "1: Encoding Loss 28.698436737060547, Transition Loss -5.9421281814575195, Classifier Loss 0.5548468232154846, Total Loss 285.0710144042969\n",
      "1: Encoding Loss 25.177513122558594, Transition Loss -20.53470230102539, Classifier Loss 0.5394281148910522, Total Loss 255.3588104248047\n",
      "1: Encoding Loss 27.52240562438965, Transition Loss -9.203246116638184, Classifier Loss 0.5533783435821533, Total Loss 275.5152587890625\n",
      "1: Encoding Loss 28.5671443939209, Transition Loss -10.388927459716797, Classifier Loss 0.545721173286438, Total Loss 283.1072082519531\n",
      "1: Encoding Loss 27.429771423339844, Transition Loss -14.05400276184082, Classifier Loss 0.5483907461166382, Total Loss 274.2744445800781\n",
      "1: Encoding Loss 29.160308837890625, Transition Loss -10.898500442504883, Classifier Loss 0.5491693019866943, Total Loss 288.1972351074219\n",
      "1: Encoding Loss 27.966320037841797, Transition Loss -8.74072551727295, Classifier Loss 0.5531873106956482, Total Loss 279.0475769042969\n",
      "1: Encoding Loss 25.350887298583984, Transition Loss -10.963141441345215, Classifier Loss 0.5603541135787964, Total Loss 258.84033203125\n",
      "1: Encoding Loss 28.595121383666992, Transition Loss -11.522485733032227, Classifier Loss 0.5528537034988403, Total Loss 284.04400634765625\n",
      "1: Encoding Loss 27.63573455810547, Transition Loss -10.269372940063477, Classifier Loss 0.5480821132659912, Total Loss 275.8920593261719\n",
      "1: Encoding Loss 27.587297439575195, Transition Loss -10.397976875305176, Classifier Loss 0.5463281869888306, Total Loss 275.3291320800781\n",
      "1: Encoding Loss 24.955381393432617, Transition Loss -11.872076034545898, Classifier Loss 0.5487260818481445, Total Loss 254.51327514648438\n",
      "1: Encoding Loss 26.49293327331543, Transition Loss -13.194016456604004, Classifier Loss 0.549041211605072, Total Loss 266.844970703125\n",
      "1: Encoding Loss 25.128530502319336, Transition Loss -13.429342269897461, Classifier Loss 0.5523591637611389, Total Loss 256.261474609375\n",
      "1: Encoding Loss 25.39004135131836, Transition Loss -13.05593490600586, Classifier Loss 0.5457953214645386, Total Loss 257.697265625\n",
      "1: Encoding Loss 24.3759765625, Transition Loss -17.303768157958984, Classifier Loss 0.535408616065979, Total Loss 248.5452117919922\n",
      "1: Encoding Loss 29.104290008544922, Transition Loss -3.7787084579467773, Classifier Loss 0.5601766705513, Total Loss 288.8512268066406\n",
      "1: Encoding Loss 30.432580947875977, Transition Loss -10.10523509979248, Classifier Loss 0.5470685958862305, Total Loss 298.1654968261719\n",
      "1: Encoding Loss 24.825237274169922, Transition Loss -11.444963455200195, Classifier Loss 0.5434828996658325, Total Loss 252.94790649414062\n",
      "1: Encoding Loss 24.920982360839844, Transition Loss -13.58940315246582, Classifier Loss 0.5423965454101562, Total Loss 253.60479736328125\n",
      "1: Encoding Loss 24.95517921447754, Transition Loss -13.566890716552734, Classifier Loss 0.5470383167266846, Total Loss 254.34254455566406\n",
      "1: Encoding Loss 25.604047775268555, Transition Loss -11.869538307189941, Classifier Loss 0.537087082862854, Total Loss 258.5386962890625\n",
      "1: Encoding Loss 27.481914520263672, Transition Loss -11.44476318359375, Classifier Loss 0.5429272055625916, Total Loss 274.145751953125\n",
      "1: Encoding Loss 27.186046600341797, Transition Loss -9.9036226272583, Classifier Loss 0.5449588298797607, Total Loss 271.9822998046875\n",
      "1: Encoding Loss 25.833717346191406, Transition Loss -9.419100761413574, Classifier Loss 0.553208589553833, Total Loss 261.9886779785156\n",
      "1: Encoding Loss 28.45989418029785, Transition Loss -16.003440856933594, Classifier Loss 0.5330212712287903, Total Loss 280.97808837890625\n",
      "1: Encoding Loss 28.005163192749023, Transition Loss -9.253270149230957, Classifier Loss 0.5533900856971741, Total Loss 279.3784484863281\n",
      "1: Encoding Loss 29.145185470581055, Transition Loss -11.164382934570312, Classifier Loss 0.5421755313873291, Total Loss 287.3768005371094\n",
      "1: Encoding Loss 25.67241668701172, Transition Loss -10.499300956726074, Classifier Loss 0.5392102003097534, Total Loss 259.2982482910156\n",
      "1: Encoding Loss 29.527788162231445, Transition Loss -8.15787410736084, Classifier Loss 0.5504558086395264, Total Loss 291.26629638671875\n",
      "1: Encoding Loss 26.355989456176758, Transition Loss -11.469395637512207, Classifier Loss 0.5398836135864258, Total Loss 264.8340148925781\n",
      "1: Encoding Loss 27.236692428588867, Transition Loss -9.394499778747559, Classifier Loss 0.5368484258651733, Total Loss 271.57647705078125\n",
      "1: Encoding Loss 25.65223503112793, Transition Loss -5.63195276260376, Classifier Loss 0.5587271451950073, Total Loss 261.0894775390625\n",
      "1: Encoding Loss 26.583900451660156, Transition Loss -18.253149032592773, Classifier Loss 0.5433604717254639, Total Loss 267.0035705566406\n",
      "1: Encoding Loss 27.189411163330078, Transition Loss -15.477890014648438, Classifier Loss 0.5414619445800781, Total Loss 271.6584167480469\n",
      "1: Encoding Loss 25.866376876831055, Transition Loss -8.348992347717285, Classifier Loss 0.552433431148529, Total Loss 262.17266845703125\n",
      "1: Encoding Loss 28.138164520263672, Transition Loss -14.108938217163086, Classifier Loss 0.5324862003326416, Total Loss 278.35113525390625\n",
      "1: Encoding Loss 26.15115737915039, Transition Loss -11.900338172912598, Classifier Loss 0.5469904541969299, Total Loss 263.9059143066406\n",
      "1: Encoding Loss 27.788230895996094, Transition Loss -4.9008259773254395, Classifier Loss 0.5562058687210083, Total Loss 277.92547607421875\n",
      "1: Encoding Loss 26.025270462036133, Transition Loss -9.644059181213379, Classifier Loss 0.5600312352180481, Total Loss 264.203369140625\n",
      "1: Encoding Loss 23.95231056213379, Transition Loss -13.032712936401367, Classifier Loss 0.5507853031158447, Total Loss 246.6944122314453\n",
      "1: Encoding Loss 27.921846389770508, Transition Loss -14.414137840270996, Classifier Loss 0.5364948511123657, Total Loss 277.0213928222656\n",
      "1: Encoding Loss 26.06813621520996, Transition Loss -12.672250747680664, Classifier Loss 0.5438706874847412, Total Loss 262.92962646484375\n",
      "1: Encoding Loss 25.25492286682129, Transition Loss -13.561164855957031, Classifier Loss 0.5420354008674622, Total Loss 256.2402038574219\n",
      "1: Encoding Loss 28.06148338317871, Transition Loss -8.66001033782959, Classifier Loss 0.5522004961967468, Total Loss 279.7101745605469\n",
      "1: Encoding Loss 27.148771286010742, Transition Loss -10.727314949035645, Classifier Loss 0.547497570514679, Total Loss 271.93780517578125\n",
      "1: Encoding Loss 25.81298828125, Transition Loss -16.927377700805664, Classifier Loss 0.5412002205848694, Total Loss 260.62054443359375\n",
      "1: Encoding Loss 27.25493621826172, Transition Loss -17.209156036376953, Classifier Loss 0.5327125787734985, Total Loss 271.30731201171875\n",
      "1: Encoding Loss 24.303531646728516, Transition Loss -12.574420928955078, Classifier Loss 0.5395852327346802, Total Loss 248.38426208496094\n",
      "1: Encoding Loss 27.127552032470703, Transition Loss -5.122991561889648, Classifier Loss 0.5487017631530762, Total Loss 271.8895568847656\n",
      "1: Encoding Loss 24.252582550048828, Transition Loss -14.610221862792969, Classifier Loss 0.5352184772491455, Total Loss 247.5395965576172\n",
      "1: Encoding Loss 29.41706085205078, Transition Loss -10.058059692382812, Classifier Loss 0.5424674153327942, Total Loss 289.5812072753906\n",
      "1: Encoding Loss 24.102216720581055, Transition Loss -8.104361534118652, Classifier Loss 0.5432177782058716, Total Loss 247.1378936767578\n",
      "1: Encoding Loss 23.633169174194336, Transition Loss -8.918033599853516, Classifier Loss 0.5547116994857788, Total Loss 244.5347442626953\n",
      "1: Encoding Loss 27.15231704711914, Transition Loss -8.182731628417969, Classifier Loss 0.545279324054718, Total Loss 271.74481201171875\n",
      "1: Encoding Loss 25.77906608581543, Transition Loss -12.560738563537598, Classifier Loss 0.5429795384407043, Total Loss 260.5279846191406\n",
      "1: Encoding Loss 30.47146987915039, Transition Loss -16.66581153869629, Classifier Loss 0.5379724502563477, Total Loss 297.565673828125\n",
      "1: Encoding Loss 24.748878479003906, Transition Loss -13.457906723022461, Classifier Loss 0.5358559489250183, Total Loss 251.57394409179688\n",
      "1: Encoding Loss 25.43976402282715, Transition Loss -9.664090156555176, Classifier Loss 0.5479574203491211, Total Loss 258.3119201660156\n",
      "1: Encoding Loss 24.589786529541016, Transition Loss -8.521623611450195, Classifier Loss 0.5512088537216187, Total Loss 251.83746337890625\n",
      "1: Encoding Loss 25.45305824279785, Transition Loss -11.488482475280762, Classifier Loss 0.5385175347328186, Total Loss 257.47393798828125\n",
      "1: Encoding Loss 30.42262840270996, Transition Loss -4.370299339294434, Classifier Loss 0.5488468408584595, Total Loss 298.26483154296875\n",
      "1: Encoding Loss 28.592023849487305, Transition Loss -3.3728690147399902, Classifier Loss 0.5528866648674011, Total Loss 284.024169921875\n",
      "1: Encoding Loss 23.13174057006836, Transition Loss -15.599635124206543, Classifier Loss 0.5276615023612976, Total Loss 237.81698608398438\n",
      "1: Encoding Loss 26.321706771850586, Transition Loss -11.62221908569336, Classifier Loss 0.5365126132965088, Total Loss 264.22259521484375\n",
      "1: Encoding Loss 29.1134090423584, Transition Loss -9.103144645690918, Classifier Loss 0.539762556552887, Total Loss 286.8817138671875\n",
      "1: Encoding Loss 24.678768157958984, Transition Loss -12.071086883544922, Classifier Loss 0.545526385307312, Total Loss 251.9803924560547\n",
      "1: Encoding Loss 27.532093048095703, Transition Loss -11.138327598571777, Classifier Loss 0.5393898487091064, Total Loss 274.1935119628906\n",
      "1: Encoding Loss 24.7132511138916, Transition Loss -13.493538856506348, Classifier Loss 0.5338019728660583, Total Loss 251.08351135253906\n",
      "1: Encoding Loss 29.05768585205078, Transition Loss -6.609335899353027, Classifier Loss 0.5468278527259827, Total Loss 287.1429443359375\n",
      "1: Encoding Loss 28.11504554748535, Transition Loss -16.47265625, Classifier Loss 0.5349987149238586, Total Loss 278.41693115234375\n",
      "1: Encoding Loss 27.097776412963867, Transition Loss -6.016694068908691, Classifier Loss 0.5458697080612183, Total Loss 271.36798095703125\n",
      "1: Encoding Loss 28.514026641845703, Transition Loss -9.141339302062988, Classifier Loss 0.5509627461433411, Total Loss 283.2066650390625\n",
      "1: Encoding Loss 30.341495513916016, Transition Loss -9.412067413330078, Classifier Loss 0.5345479249954224, Total Loss 296.18487548828125\n",
      "1: Encoding Loss 25.308801651000977, Transition Loss -8.699409484863281, Classifier Loss 0.5455427169799805, Total Loss 257.02294921875\n",
      "1: Encoding Loss 25.47051239013672, Transition Loss -9.787031173706055, Classifier Loss 0.5401701927185059, Total Loss 257.7791748046875\n",
      "1: Encoding Loss 31.068906784057617, Transition Loss -14.34524154663086, Classifier Loss 0.5325222015380859, Total Loss 301.80059814453125\n",
      "1: Encoding Loss 23.653215408325195, Transition Loss -10.823467254638672, Classifier Loss 0.5420711040496826, Total Loss 243.4306640625\n",
      "1: Encoding Loss 24.0413761138916, Transition Loss -15.72664737701416, Classifier Loss 0.5390658974647522, Total Loss 246.2344512939453\n",
      "1: Encoding Loss 25.6013240814209, Transition Loss -11.230973243713379, Classifier Loss 0.5387427806854248, Total Loss 258.6826171875\n",
      "1: Encoding Loss 25.897533416748047, Transition Loss -10.09329891204834, Classifier Loss 0.5415090918540955, Total Loss 261.32916259765625\n",
      "1: Encoding Loss 25.703907012939453, Transition Loss -11.389753341674805, Classifier Loss 0.5287624597549438, Total Loss 258.5052185058594\n",
      "1: Encoding Loss 23.92061996459961, Transition Loss -10.295858383178711, Classifier Loss 0.5363175272941589, Total Loss 244.9946746826172\n",
      "1: Encoding Loss 26.267183303833008, Transition Loss -8.357015609741211, Classifier Loss 0.5439969301223755, Total Loss 264.5354919433594\n",
      "1: Encoding Loss 25.610912322998047, Transition Loss -4.695774555206299, Classifier Loss 0.5446330308914185, Total Loss 259.34967041015625\n",
      "1: Encoding Loss 23.96670150756836, Transition Loss -13.138416290283203, Classifier Loss 0.5331119894981384, Total Loss 245.04220581054688\n",
      "1: Encoding Loss 26.96580696105957, Transition Loss -10.223457336425781, Classifier Loss 0.5391973257064819, Total Loss 269.6441650390625\n",
      "1: Encoding Loss 26.489898681640625, Transition Loss -12.481472969055176, Classifier Loss 0.5401903390884399, Total Loss 265.93572998046875\n",
      "1: Encoding Loss 24.40729331970215, Transition Loss -6.819957733154297, Classifier Loss 0.549560010433197, Total Loss 250.21299743652344\n",
      "1: Encoding Loss 27.416378021240234, Transition Loss -7.6883649826049805, Classifier Loss 0.5377522110939026, Total Loss 273.104736328125\n",
      "1: Encoding Loss 26.226953506469727, Transition Loss -12.25612735748291, Classifier Loss 0.5309495329856873, Total Loss 262.90814208984375\n",
      "1: Encoding Loss 26.736371994018555, Transition Loss -9.454793930053711, Classifier Loss 0.5511272549629211, Total Loss 269.0018005371094\n",
      "1: Encoding Loss 22.87420654296875, Transition Loss -15.71825122833252, Classifier Loss 0.5278224349021912, Total Loss 235.7727508544922\n",
      "1: Encoding Loss 24.82622528076172, Transition Loss -10.88392162322998, Classifier Loss 0.535605788230896, Total Loss 252.16819763183594\n",
      "1: Encoding Loss 26.943056106567383, Transition Loss -12.727706909179688, Classifier Loss 0.5311400294303894, Total Loss 268.65594482421875\n",
      "1: Encoding Loss 26.6011962890625, Transition Loss -10.262088775634766, Classifier Loss 0.5403731465339661, Total Loss 266.8448486328125\n",
      "1: Encoding Loss 25.484363555908203, Transition Loss -4.688882827758789, Classifier Loss 0.556733250617981, Total Loss 259.5472717285156\n",
      "1: Encoding Loss 25.488117218017578, Transition Loss -9.531624794006348, Classifier Loss 0.553398072719574, Total Loss 259.24285888671875\n",
      "1: Encoding Loss 23.764490127563477, Transition Loss -8.566482543945312, Classifier Loss 0.5417588353157043, Total Loss 244.29010009765625\n",
      "1: Encoding Loss 26.87980079650879, Transition Loss -5.928060531616211, Classifier Loss 0.5412602424621582, Total Loss 269.1632385253906\n",
      "1: Encoding Loss 26.007122039794922, Transition Loss -7.302964687347412, Classifier Loss 0.5438015460968018, Total Loss 262.4356689453125\n",
      "1: Encoding Loss 25.787809371948242, Transition Loss -10.415163040161133, Classifier Loss 0.5373014211654663, Total Loss 260.0305480957031\n",
      "1: Encoding Loss 26.217775344848633, Transition Loss -9.76826286315918, Classifier Loss 0.5361975431442261, Total Loss 263.3600158691406\n",
      "1: Encoding Loss 25.09270477294922, Transition Loss -8.6331205368042, Classifier Loss 0.542171061038971, Total Loss 254.95701599121094\n",
      "1: Encoding Loss 24.266008377075195, Transition Loss -12.260400772094727, Classifier Loss 0.5481597185134888, Total Loss 248.9415740966797\n",
      "1: Encoding Loss 27.38530731201172, Transition Loss -7.765487194061279, Classifier Loss 0.5416467189788818, Total Loss 273.2455749511719\n",
      "2: Encoding Loss 25.624441146850586, Transition Loss -17.169517517089844, Classifier Loss 0.528359591960907, Total Loss 257.8280334472656\n",
      "2: Encoding Loss 25.17888069152832, Transition Loss -9.289444923400879, Classifier Loss 0.5392981171607971, Total Loss 255.35899353027344\n",
      "2: Encoding Loss 28.099515914916992, Transition Loss -12.35802936553955, Classifier Loss 0.5318856835365295, Total Loss 277.9822082519531\n",
      "2: Encoding Loss 25.069536209106445, Transition Loss -7.2439866065979, Classifier Loss 0.5458495020866394, Total Loss 255.13978576660156\n",
      "2: Encoding Loss 25.9068546295166, Transition Loss -6.651456356048584, Classifier Loss 0.5453505516052246, Total Loss 261.7885437011719\n",
      "2: Encoding Loss 26.63531494140625, Transition Loss -11.907679557800293, Classifier Loss 0.5434860587120056, Total Loss 267.4287414550781\n",
      "2: Encoding Loss 25.206035614013672, Transition Loss -10.29122257232666, Classifier Loss 0.5379636287689209, Total Loss 255.44261169433594\n",
      "2: Encoding Loss 24.173051834106445, Transition Loss -14.463582992553711, Classifier Loss 0.5359106659889221, Total Loss 246.9725799560547\n",
      "2: Encoding Loss 25.1492919921875, Transition Loss -10.5617094039917, Classifier Loss 0.5423269271850586, Total Loss 255.4249267578125\n",
      "2: Encoding Loss 25.52412986755371, Transition Loss -10.852270126342773, Classifier Loss 0.5395898818969727, Total Loss 258.1498718261719\n",
      "2: Encoding Loss 24.001432418823242, Transition Loss -7.334967613220215, Classifier Loss 0.5418210625648499, Total Loss 246.19210815429688\n",
      "2: Encoding Loss 23.597028732299805, Transition Loss -15.133115768432617, Classifier Loss 0.5344764590263367, Total Loss 242.22085571289062\n",
      "2: Encoding Loss 24.10498046875, Transition Loss -11.521759033203125, Classifier Loss 0.5345304012298584, Total Loss 246.2905731201172\n",
      "2: Encoding Loss 24.91297149658203, Transition Loss -10.422456741333008, Classifier Loss 0.5363103151321411, Total Loss 252.93270874023438\n",
      "2: Encoding Loss 25.283447265625, Transition Loss -8.392678260803223, Classifier Loss 0.5384511351585388, Total Loss 256.11102294921875\n",
      "2: Encoding Loss 26.05205726623535, Transition Loss -12.202615737915039, Classifier Loss 0.5370306968688965, Total Loss 262.1170959472656\n",
      "2: Encoding Loss 29.34324836730957, Transition Loss -7.463598251342773, Classifier Loss 0.5387899875640869, Total Loss 288.6235046386719\n",
      "2: Encoding Loss 25.335508346557617, Transition Loss -6.766025066375732, Classifier Loss 0.5387997031211853, Total Loss 256.56268310546875\n",
      "2: Encoding Loss 25.214967727661133, Transition Loss -12.439260482788086, Classifier Loss 0.54012131690979, Total Loss 255.72938537597656\n",
      "2: Encoding Loss 26.40807342529297, Transition Loss -9.075737953186035, Classifier Loss 0.532558023929596, Total Loss 264.51861572265625\n",
      "2: Encoding Loss 25.660722732543945, Transition Loss -9.036375045776367, Classifier Loss 0.5444386005401611, Total Loss 259.72784423828125\n",
      "2: Encoding Loss 27.182445526123047, Transition Loss -7.647518157958984, Classifier Loss 0.5359441637992859, Total Loss 271.0524597167969\n",
      "2: Encoding Loss 27.312532424926758, Transition Loss -6.905519962310791, Classifier Loss 0.5387673377990723, Total Loss 272.3756408691406\n",
      "2: Encoding Loss 25.604217529296875, Transition Loss -12.859064102172852, Classifier Loss 0.5313935875892639, Total Loss 257.97052001953125\n",
      "2: Encoding Loss 24.440479278564453, Transition Loss -9.156469345092773, Classifier Loss 0.5401048064231873, Total Loss 249.53248596191406\n",
      "2: Encoding Loss 26.516754150390625, Transition Loss -7.8761372566223145, Classifier Loss 0.5328593254089355, Total Loss 265.41839599609375\n",
      "2: Encoding Loss 25.523494720458984, Transition Loss -10.457361221313477, Classifier Loss 0.5308042764663696, Total Loss 257.26629638671875\n",
      "2: Encoding Loss 26.79039764404297, Transition Loss -8.60972785949707, Classifier Loss 0.5431719422340393, Total Loss 268.638671875\n",
      "2: Encoding Loss 24.787715911865234, Transition Loss -14.17464828491211, Classifier Loss 0.5323565006256104, Total Loss 251.5345458984375\n",
      "2: Encoding Loss 26.225372314453125, Transition Loss -8.885071754455566, Classifier Loss 0.5297818779945374, Total Loss 262.7793884277344\n",
      "2: Encoding Loss 25.939044952392578, Transition Loss -5.859851837158203, Classifier Loss 0.5375738739967346, Total Loss 261.2685852050781\n",
      "2: Encoding Loss 23.11924171447754, Transition Loss -13.58874225616455, Classifier Loss 0.5345319509506226, Total Loss 238.4044189453125\n",
      "2: Encoding Loss 24.158628463745117, Transition Loss -7.513489723205566, Classifier Loss 0.5395981073379517, Total Loss 247.2273406982422\n",
      "2: Encoding Loss 22.471981048583984, Transition Loss -13.281046867370605, Classifier Loss 0.53326416015625, Total Loss 233.09962463378906\n",
      "2: Encoding Loss 25.1099796295166, Transition Loss -7.859111785888672, Classifier Loss 0.5425581336021423, Total Loss 255.1340789794922\n",
      "2: Encoding Loss 24.680461883544922, Transition Loss -14.24295425415039, Classifier Loss 0.5282092690467834, Total Loss 250.26177978515625\n",
      "2: Encoding Loss 23.8713321685791, Transition Loss -10.306310653686523, Classifier Loss 0.533340573310852, Total Loss 244.3026580810547\n",
      "2: Encoding Loss 22.33226776123047, Transition Loss -13.86743450164795, Classifier Loss 0.5311223268508911, Total Loss 231.76759338378906\n",
      "2: Encoding Loss 26.809467315673828, Transition Loss -8.315423965454102, Classifier Loss 0.525285542011261, Total Loss 267.0026550292969\n",
      "2: Encoding Loss 28.18828582763672, Transition Loss -4.52048397064209, Classifier Loss 0.5442042350769043, Total Loss 279.9258117675781\n",
      "2: Encoding Loss 24.638498306274414, Transition Loss -12.410017967224121, Classifier Loss 0.5312656164169312, Total Loss 250.2320556640625\n",
      "2: Encoding Loss 26.070974349975586, Transition Loss -5.099609851837158, Classifier Loss 0.5367752313613892, Total Loss 262.24432373046875\n",
      "2: Encoding Loss 25.32822036743164, Transition Loss -8.212810516357422, Classifier Loss 0.5409337282180786, Total Loss 256.7174987792969\n",
      "2: Encoding Loss 23.44053077697754, Transition Loss -10.938852310180664, Classifier Loss 0.5321258306503296, Total Loss 240.73464965820312\n",
      "2: Encoding Loss 24.530338287353516, Transition Loss -9.274460792541504, Classifier Loss 0.5446426868438721, Total Loss 250.70510864257812\n",
      "2: Encoding Loss 24.974178314208984, Transition Loss -8.319141387939453, Classifier Loss 0.5319439768791199, Total Loss 252.98617553710938\n",
      "2: Encoding Loss 25.60809326171875, Transition Loss -4.756903648376465, Classifier Loss 0.5472996234893799, Total Loss 259.59375\n",
      "2: Encoding Loss 23.311445236206055, Transition Loss -10.44184684753418, Classifier Loss 0.5328408479690552, Total Loss 239.7735595703125\n",
      "2: Encoding Loss 25.32994270324707, Transition Loss -6.417627811431885, Classifier Loss 0.5352663993835449, Total Loss 256.1649169921875\n",
      "2: Encoding Loss 24.28571319580078, Transition Loss -7.702786445617676, Classifier Loss 0.5397024154663086, Total Loss 248.25440979003906\n",
      "2: Encoding Loss 24.531145095825195, Transition Loss -11.048727035522461, Classifier Loss 0.5393431782722473, Total Loss 250.18125915527344\n",
      "2: Encoding Loss 25.63951873779297, Transition Loss -5.583736896514893, Classifier Loss 0.5376702547073364, Total Loss 258.8820495605469\n",
      "2: Encoding Loss 23.477705001831055, Transition Loss -17.516666412353516, Classifier Loss 0.5267046689987183, Total Loss 240.4886016845703\n",
      "2: Encoding Loss 24.775482177734375, Transition Loss -8.271005630493164, Classifier Loss 0.5362513661384583, Total Loss 251.8273468017578\n",
      "2: Encoding Loss 25.288597106933594, Transition Loss -9.381685256958008, Classifier Loss 0.5283108949661255, Total Loss 255.1379852294922\n",
      "2: Encoding Loss 24.4050235748291, Transition Loss -11.278599739074707, Classifier Loss 0.534820020198822, Total Loss 248.71994018554688\n",
      "2: Encoding Loss 25.489070892333984, Transition Loss -9.594033241271973, Classifier Loss 0.5316202640533447, Total Loss 257.07269287109375\n",
      "2: Encoding Loss 25.222185134887695, Transition Loss -7.956464767456055, Classifier Loss 0.5349828600883484, Total Loss 255.27418518066406\n",
      "2: Encoding Loss 23.22432518005371, Transition Loss -10.362918853759766, Classifier Loss 0.5426446795463562, Total Loss 240.05699157714844\n",
      "2: Encoding Loss 25.827192306518555, Transition Loss -10.297844886779785, Classifier Loss 0.5335806608200073, Total Loss 259.97357177734375\n",
      "2: Encoding Loss 24.722402572631836, Transition Loss -8.993635177612305, Classifier Loss 0.5321887731552124, Total Loss 250.9962921142578\n",
      "2: Encoding Loss 24.69839096069336, Transition Loss -8.871707916259766, Classifier Loss 0.532230794429779, Total Loss 250.80845642089844\n",
      "2: Encoding Loss 23.070240020751953, Transition Loss -10.71435260772705, Classifier Loss 0.5331149697303772, Total Loss 237.87127685546875\n",
      "2: Encoding Loss 23.58148956298828, Transition Loss -10.50178050994873, Classifier Loss 0.5356909036636353, Total Loss 242.21890258789062\n",
      "2: Encoding Loss 22.833093643188477, Transition Loss -11.959065437316895, Classifier Loss 0.5368685126304626, Total Loss 236.3491973876953\n",
      "2: Encoding Loss 23.190685272216797, Transition Loss -10.411097526550293, Classifier Loss 0.5338921546936035, Total Loss 238.91262817382812\n",
      "2: Encoding Loss 22.24922752380371, Transition Loss -13.494518280029297, Classifier Loss 0.5241066813468933, Total Loss 230.4017791748047\n",
      "2: Encoding Loss 25.93890380859375, Transition Loss -3.9238929748535156, Classifier Loss 0.5406571626663208, Total Loss 261.5761413574219\n",
      "2: Encoding Loss 26.447282791137695, Transition Loss -7.956044673919678, Classifier Loss 0.5318326354026794, Total Loss 264.75994873046875\n",
      "2: Encoding Loss 22.848997116088867, Transition Loss -10.25224494934082, Classifier Loss 0.5293197631835938, Total Loss 235.72190856933594\n",
      "2: Encoding Loss 22.613195419311523, Transition Loss -11.756317138671875, Classifier Loss 0.5278956890106201, Total Loss 233.69277954101562\n",
      "2: Encoding Loss 22.71405029296875, Transition Loss -11.446822166442871, Classifier Loss 0.5321677327156067, Total Loss 234.9268798828125\n",
      "2: Encoding Loss 23.019025802612305, Transition Loss -10.174811363220215, Classifier Loss 0.5223433971405029, Total Loss 236.384521484375\n",
      "2: Encoding Loss 24.750934600830078, Transition Loss -9.329713821411133, Classifier Loss 0.5252033472061157, Total Loss 250.5259552001953\n",
      "2: Encoding Loss 24.04075050354004, Transition Loss -8.226228713989258, Classifier Loss 0.530456006526947, Total Loss 245.36996459960938\n",
      "2: Encoding Loss 23.416873931884766, Transition Loss -8.124669075012207, Classifier Loss 0.5355022549629211, Total Loss 240.88360595703125\n",
      "2: Encoding Loss 25.209009170532227, Transition Loss -13.231185913085938, Classifier Loss 0.5159383416175842, Total Loss 253.2632598876953\n",
      "2: Encoding Loss 25.104957580566406, Transition Loss -7.992586612701416, Classifier Loss 0.5339152216911316, Total Loss 254.22958374023438\n",
      "2: Encoding Loss 26.131336212158203, Transition Loss -10.430810928344727, Classifier Loss 0.5213894844055176, Total Loss 261.18756103515625\n",
      "2: Encoding Loss 23.377355575561523, Transition Loss -9.191048622131348, Classifier Loss 0.5251306295394897, Total Loss 239.5300750732422\n",
      "2: Encoding Loss 25.654407501220703, Transition Loss -6.93631649017334, Classifier Loss 0.5333423018455505, Total Loss 258.568115234375\n",
      "2: Encoding Loss 23.611133575439453, Transition Loss -9.814260482788086, Classifier Loss 0.522783637046814, Total Loss 241.16546630859375\n",
      "2: Encoding Loss 24.38551902770996, Transition Loss -7.684476375579834, Classifier Loss 0.522970974445343, Total Loss 247.3797149658203\n",
      "2: Encoding Loss 22.337507247924805, Transition Loss -5.547730445861816, Classifier Loss 0.5425660014152527, Total Loss 232.95555114746094\n",
      "2: Encoding Loss 24.061649322509766, Transition Loss -14.591073036193848, Classifier Loss 0.5253873467445374, Total Loss 245.0290069580078\n",
      "2: Encoding Loss 23.868513107299805, Transition Loss -12.246353149414062, Classifier Loss 0.5255429148674011, Total Loss 243.49993896484375\n",
      "2: Encoding Loss 23.522613525390625, Transition Loss -8.40002155303955, Classifier Loss 0.5321630835533142, Total Loss 241.39553833007812\n",
      "2: Encoding Loss 25.20619010925293, Transition Loss -12.504283905029297, Classifier Loss 0.5133812427520752, Total Loss 252.98513793945312\n",
      "2: Encoding Loss 23.30684471130371, Transition Loss -9.306608200073242, Classifier Loss 0.5297445058822632, Total Loss 239.42733764648438\n",
      "2: Encoding Loss 24.385194778442383, Transition Loss -4.584944725036621, Classifier Loss 0.537356436252594, Total Loss 248.8162841796875\n",
      "2: Encoding Loss 23.797163009643555, Transition Loss -9.869865417480469, Classifier Loss 0.5377007126808167, Total Loss 244.14540100097656\n",
      "2: Encoding Loss 21.32894515991211, Transition Loss -11.014810562133789, Classifier Loss 0.5359777212142944, Total Loss 224.22714233398438\n",
      "2: Encoding Loss 24.651309967041016, Transition Loss -12.239705085754395, Classifier Loss 0.5196592807769775, Total Loss 249.17396545410156\n",
      "2: Encoding Loss 23.574047088623047, Transition Loss -10.722620964050293, Classifier Loss 0.5262177586555481, Total Loss 241.21202087402344\n",
      "2: Encoding Loss 22.453086853027344, Transition Loss -11.23575210571289, Classifier Loss 0.5278435945510864, Total Loss 232.4068145751953\n",
      "2: Encoding Loss 24.768938064575195, Transition Loss -7.736171722412109, Classifier Loss 0.5318269729614258, Total Loss 251.33265686035156\n",
      "2: Encoding Loss 24.35828971862793, Transition Loss -9.046464920043945, Classifier Loss 0.5257207751274109, Total Loss 247.43658447265625\n",
      "2: Encoding Loss 23.670616149902344, Transition Loss -15.001276969909668, Classifier Loss 0.5191795825958252, Total Loss 241.27987670898438\n",
      "2: Encoding Loss 24.166208267211914, Transition Loss -13.179402351379395, Classifier Loss 0.5150327086448669, Total Loss 244.83029174804688\n",
      "2: Encoding Loss 22.229352951049805, Transition Loss -11.575582504272461, Classifier Loss 0.5226746797561646, Total Loss 230.0999755859375\n",
      "2: Encoding Loss 24.41414451599121, Transition Loss -5.909295082092285, Classifier Loss 0.528868556022644, Total Loss 248.19883728027344\n",
      "2: Encoding Loss 21.945674896240234, Transition Loss -11.567938804626465, Classifier Loss 0.5224913954734802, Total Loss 227.81222534179688\n",
      "2: Encoding Loss 25.476207733154297, Transition Loss -8.291033744812012, Classifier Loss 0.5234436392784119, Total Loss 256.15240478515625\n",
      "2: Encoding Loss 21.68516731262207, Transition Loss -7.118764877319336, Classifier Loss 0.530840277671814, Total Loss 226.56394958496094\n",
      "2: Encoding Loss 21.398191452026367, Transition Loss -7.94739294052124, Classifier Loss 0.5377508401870728, Total Loss 224.9590301513672\n",
      "2: Encoding Loss 24.41261100769043, Transition Loss -7.915515899658203, Classifier Loss 0.5227097868919373, Total Loss 247.57028198242188\n",
      "2: Encoding Loss 22.548595428466797, Transition Loss -10.363213539123535, Classifier Loss 0.5289159417152405, Total Loss 233.27828979492188\n",
      "2: Encoding Loss 26.3633975982666, Transition Loss -13.874638557434082, Classifier Loss 0.5154205560684204, Total Loss 262.44647216796875\n",
      "2: Encoding Loss 22.46571922302246, Transition Loss -11.934514999389648, Classifier Loss 0.5156919360160828, Total Loss 231.29257202148438\n",
      "2: Encoding Loss 23.192344665527344, Transition Loss -9.274773597717285, Classifier Loss 0.5271495580673218, Total Loss 238.25184631347656\n",
      "2: Encoding Loss 22.194171905517578, Transition Loss -8.87203311920166, Classifier Loss 0.5302408337593079, Total Loss 230.57568359375\n",
      "2: Encoding Loss 22.49848175048828, Transition Loss -9.320338249206543, Classifier Loss 0.5238099694252014, Total Loss 232.3669891357422\n",
      "2: Encoding Loss 26.629419326782227, Transition Loss -5.652387619018555, Classifier Loss 0.5238699913024902, Total Loss 265.4212341308594\n",
      "2: Encoding Loss 24.97085189819336, Transition Loss -4.2631754875183105, Classifier Loss 0.5299458503723145, Total Loss 252.76055908203125\n",
      "2: Encoding Loss 20.92300033569336, Transition Loss -13.36212158203125, Classifier Loss 0.5132914185523987, Total Loss 218.7104949951172\n",
      "2: Encoding Loss 22.713274002075195, Transition Loss -8.851573944091797, Classifier Loss 0.5233601927757263, Total Loss 234.04043579101562\n",
      "2: Encoding Loss 25.43605613708496, Transition Loss -8.853944778442383, Classifier Loss 0.5170450210571289, Total Loss 255.19117736816406\n",
      "2: Encoding Loss 22.301908493041992, Transition Loss -11.244453430175781, Classifier Loss 0.5253169536590576, Total Loss 230.94471740722656\n",
      "2: Encoding Loss 24.8402099609375, Transition Loss -9.809489250183105, Classifier Loss 0.5195515155792236, Total Loss 250.67486572265625\n",
      "2: Encoding Loss 22.328336715698242, Transition Loss -11.866913795471191, Classifier Loss 0.5160027742385864, Total Loss 230.22459411621094\n",
      "2: Encoding Loss 25.691661834716797, Transition Loss -6.940045356750488, Classifier Loss 0.5226253271102905, Total Loss 257.7944641113281\n",
      "2: Encoding Loss 24.76821517944336, Transition Loss -13.792980194091797, Classifier Loss 0.5148118734359741, Total Loss 249.62416076660156\n",
      "2: Encoding Loss 23.702226638793945, Transition Loss -6.087916374206543, Classifier Loss 0.5256175398826599, Total Loss 242.1783447265625\n",
      "2: Encoding Loss 25.17276954650879, Transition Loss -9.355257034301758, Classifier Loss 0.5244930386543274, Total Loss 253.82957458496094\n",
      "2: Encoding Loss 26.117483139038086, Transition Loss -8.242270469665527, Classifier Loss 0.5125105977058411, Total Loss 260.18927001953125\n",
      "2: Encoding Loss 22.67768669128418, Transition Loss -8.183518409729004, Classifier Loss 0.5277608633041382, Total Loss 234.19593811035156\n",
      "2: Encoding Loss 23.15545082092285, Transition Loss -10.630443572998047, Classifier Loss 0.5173097252845764, Total Loss 236.9724578857422\n",
      "2: Encoding Loss 26.019838333129883, Transition Loss -13.039217948913574, Classifier Loss 0.5059041976928711, Total Loss 258.7465515136719\n",
      "2: Encoding Loss 21.374996185302734, Transition Loss -10.628071784973145, Classifier Loss 0.5212337970733643, Total Loss 223.12124633789062\n",
      "2: Encoding Loss 21.485185623168945, Transition Loss -13.079501152038574, Classifier Loss 0.5205235481262207, Total Loss 223.9312286376953\n",
      "2: Encoding Loss 22.475440979003906, Transition Loss -9.835004806518555, Classifier Loss 0.5198118686676025, Total Loss 231.78274536132812\n",
      "2: Encoding Loss 22.941783905029297, Transition Loss -9.378890037536621, Classifier Loss 0.5225349068641663, Total Loss 235.78590393066406\n",
      "2: Encoding Loss 22.724977493286133, Transition Loss -9.870777130126953, Classifier Loss 0.5087758898735046, Total Loss 232.67544555664062\n",
      "2: Encoding Loss 21.64085578918457, Transition Loss -9.908021926879883, Classifier Loss 0.5155140161514282, Total Loss 224.67626953125\n",
      "2: Encoding Loss 23.254575729370117, Transition Loss -8.794086456298828, Classifier Loss 0.5207513570785522, Total Loss 238.1099853515625\n",
      "2: Encoding Loss 22.829744338989258, Transition Loss -5.970734596252441, Classifier Loss 0.5226303339004517, Total Loss 234.89979553222656\n",
      "2: Encoding Loss 21.349599838256836, Transition Loss -10.942998886108398, Classifier Loss 0.5172896385192871, Total Loss 222.52357482910156\n",
      "2: Encoding Loss 23.31862449645996, Transition Loss -9.071797370910645, Classifier Loss 0.5198673009872437, Total Loss 238.53390502929688\n",
      "2: Encoding Loss 23.724376678466797, Transition Loss -10.999977111816406, Classifier Loss 0.5183958411216736, Total Loss 241.63241577148438\n",
      "2: Encoding Loss 21.470523834228516, Transition Loss -6.687526702880859, Classifier Loss 0.5290623307228088, Total Loss 224.66908264160156\n",
      "2: Encoding Loss 24.000314712524414, Transition Loss -7.310356140136719, Classifier Loss 0.5151879787445068, Total Loss 243.5198516845703\n",
      "2: Encoding Loss 23.223674774169922, Transition Loss -10.338517189025879, Classifier Loss 0.5145780444145203, Total Loss 237.24514770507812\n",
      "2: Encoding Loss 23.41950225830078, Transition Loss -8.914588928222656, Classifier Loss 0.5269379615783691, Total Loss 240.0480194091797\n",
      "2: Encoding Loss 20.664772033691406, Transition Loss -13.082674026489258, Classifier Loss 0.5116670727729797, Total Loss 216.48226928710938\n",
      "2: Encoding Loss 21.967893600463867, Transition Loss -9.79088020324707, Classifier Loss 0.5145551562309265, Total Loss 227.19671630859375\n",
      "2: Encoding Loss 23.486852645874023, Transition Loss -11.470083236694336, Classifier Loss 0.5086861252784729, Total Loss 238.76113891601562\n",
      "2: Encoding Loss 23.577999114990234, Transition Loss -10.306081771850586, Classifier Loss 0.5144892930984497, Total Loss 240.0708770751953\n",
      "2: Encoding Loss 22.05542755126953, Transition Loss -5.8120222091674805, Classifier Loss 0.5358375310897827, Total Loss 230.02601623535156\n",
      "2: Encoding Loss 22.83980369567871, Transition Loss -10.45768928527832, Classifier Loss 0.5243289470672607, Total Loss 235.14923095703125\n",
      "2: Encoding Loss 21.35150909423828, Transition Loss -8.85091495513916, Classifier Loss 0.5249462127685547, Total Loss 223.304931640625\n",
      "2: Encoding Loss 23.4125919342041, Transition Loss -6.640035152435303, Classifier Loss 0.5216836333274841, Total Loss 239.4677734375\n",
      "2: Encoding Loss 23.267181396484375, Transition Loss -8.774467468261719, Classifier Loss 0.5177381038665771, Total Loss 237.9095001220703\n",
      "2: Encoding Loss 22.902751922607422, Transition Loss -9.934980392456055, Classifier Loss 0.5184863805770874, Total Loss 235.0686798095703\n",
      "2: Encoding Loss 23.55357551574707, Transition Loss -10.924757957458496, Classifier Loss 0.5105687379837036, Total Loss 239.48329162597656\n",
      "2: Encoding Loss 22.51549530029297, Transition Loss -9.733789443969727, Classifier Loss 0.5175812244415283, Total Loss 231.880126953125\n",
      "2: Encoding Loss 21.8703556060791, Transition Loss -12.520089149475098, Classifier Loss 0.5236431956291199, Total Loss 227.3246612548828\n",
      "2: Encoding Loss 24.449201583862305, Transition Loss -10.557632446289062, Classifier Loss 0.5072275996208191, Total Loss 246.31427001953125\n",
      "3: Encoding Loss 22.60527801513672, Transition Loss -14.309432983398438, Classifier Loss 0.5058976411819458, Total Loss 231.4291229248047\n",
      "3: Encoding Loss 22.146923065185547, Transition Loss -8.006675720214844, Classifier Loss 0.5217821598052979, Total Loss 229.3520050048828\n",
      "3: Encoding Loss 23.79718017578125, Transition Loss -9.904544830322266, Classifier Loss 0.5152336359024048, Total Loss 241.89881896972656\n",
      "3: Encoding Loss 22.186594009399414, Transition Loss -8.555991172790527, Classifier Loss 0.5194349884986877, Total Loss 229.43453979492188\n",
      "3: Encoding Loss 22.529687881469727, Transition Loss -6.286364555358887, Classifier Loss 0.5246861577033997, Total Loss 232.70486450195312\n",
      "3: Encoding Loss 23.062639236450195, Transition Loss -12.050955772399902, Classifier Loss 0.5183917880058289, Total Loss 236.337890625\n",
      "3: Encoding Loss 22.524744033813477, Transition Loss -9.93787956237793, Classifier Loss 0.5181087255477905, Total Loss 232.0068359375\n",
      "3: Encoding Loss 21.581424713134766, Transition Loss -12.974958419799805, Classifier Loss 0.5135334134101868, Total Loss 224.00213623046875\n",
      "3: Encoding Loss 22.191770553588867, Transition Loss -10.538446426391602, Classifier Loss 0.5148906707763672, Total Loss 229.0211181640625\n",
      "3: Encoding Loss 22.04819107055664, Transition Loss -9.960467338562012, Classifier Loss 0.5156359672546387, Total Loss 227.94712829589844\n",
      "3: Encoding Loss 21.373355865478516, Transition Loss -7.64556360244751, Classifier Loss 0.5252561569213867, Total Loss 223.5109405517578\n",
      "3: Encoding Loss 20.871004104614258, Transition Loss -12.722359657287598, Classifier Loss 0.5161293745040894, Total Loss 218.5784149169922\n",
      "3: Encoding Loss 21.41716766357422, Transition Loss -10.145609855651855, Classifier Loss 0.5131403803825378, Total Loss 222.64935302734375\n",
      "3: Encoding Loss 21.344877243041992, Transition Loss -9.240485191345215, Classifier Loss 0.5158713459968567, Total Loss 222.3443145751953\n",
      "3: Encoding Loss 22.74810791015625, Transition Loss -9.741905212402344, Classifier Loss 0.5115411877632141, Total Loss 233.13702392578125\n",
      "3: Encoding Loss 23.282390594482422, Transition Loss -12.825471878051758, Classifier Loss 0.5038385391235352, Total Loss 236.6404266357422\n",
      "3: Encoding Loss 24.966840744018555, Transition Loss -7.325990676879883, Classifier Loss 0.5168501138687134, Total Loss 251.41827392578125\n",
      "3: Encoding Loss 21.6154727935791, Transition Loss -6.2892303466796875, Classifier Loss 0.5190741419792175, Total Loss 224.82994079589844\n",
      "3: Encoding Loss 22.025596618652344, Transition Loss -11.869680404663086, Classifier Loss 0.5146711468696594, Total Loss 227.6695098876953\n",
      "3: Encoding Loss 22.668209075927734, Transition Loss -8.557661056518555, Classifier Loss 0.5135868191719055, Total Loss 232.70266723632812\n",
      "3: Encoding Loss 22.464570999145508, Transition Loss -9.272055625915527, Classifier Loss 0.519274890422821, Total Loss 231.64219665527344\n",
      "3: Encoding Loss 23.647127151489258, Transition Loss -8.125321388244629, Classifier Loss 0.510158360004425, Total Loss 240.19122314453125\n",
      "3: Encoding Loss 24.14344024658203, Transition Loss -8.023228645324707, Classifier Loss 0.5128114223480225, Total Loss 244.42706298828125\n",
      "3: Encoding Loss 22.123905181884766, Transition Loss -11.179401397705078, Classifier Loss 0.5109250545501709, Total Loss 228.0814971923828\n",
      "3: Encoding Loss 21.772302627563477, Transition Loss -9.602986335754395, Classifier Loss 0.5177689790725708, Total Loss 225.95339965820312\n",
      "3: Encoding Loss 22.24481201171875, Transition Loss -7.040268898010254, Classifier Loss 0.5143599510192871, Total Loss 229.39309692382812\n",
      "3: Encoding Loss 22.336387634277344, Transition Loss -9.477862358093262, Classifier Loss 0.5148903727531433, Total Loss 230.17825317382812\n",
      "3: Encoding Loss 23.98002815246582, Transition Loss -10.626429557800293, Classifier Loss 0.5122971534729004, Total Loss 243.0678253173828\n",
      "3: Encoding Loss 21.800642013549805, Transition Loss -13.175355911254883, Classifier Loss 0.5057864785194397, Total Loss 224.98114013671875\n",
      "3: Encoding Loss 22.66144371032715, Transition Loss -8.129712104797363, Classifier Loss 0.5096275806427002, Total Loss 232.25267028808594\n",
      "3: Encoding Loss 22.384654998779297, Transition Loss -6.8430891036987305, Classifier Loss 0.5145927667617798, Total Loss 230.53515625\n",
      "3: Encoding Loss 20.18977165222168, Transition Loss -11.637471199035645, Classifier Loss 0.5166177749633789, Total Loss 213.1776123046875\n",
      "3: Encoding Loss 20.911373138427734, Transition Loss -8.28423023223877, Classifier Loss 0.5133640170097351, Total Loss 218.62574768066406\n",
      "3: Encoding Loss 20.208879470825195, Transition Loss -12.891464233398438, Classifier Loss 0.5114631056785583, Total Loss 212.8147735595703\n",
      "3: Encoding Loss 21.6562557220459, Transition Loss -8.174832344055176, Classifier Loss 0.5152649879455566, Total Loss 224.77491760253906\n",
      "3: Encoding Loss 21.05263328552246, Transition Loss -12.41687297821045, Classifier Loss 0.5012010931968689, Total Loss 218.53868103027344\n",
      "3: Encoding Loss 21.21580696105957, Transition Loss -10.664493560791016, Classifier Loss 0.5100545287132263, Total Loss 220.72976684570312\n",
      "3: Encoding Loss 19.89236068725586, Transition Loss -12.61699390411377, Classifier Loss 0.5063779354095459, Total Loss 209.77418518066406\n",
      "3: Encoding Loss 23.48029327392578, Transition Loss -8.95069694519043, Classifier Loss 0.496442049741745, Total Loss 237.48475646972656\n",
      "3: Encoding Loss 23.498506546020508, Transition Loss -5.565342903137207, Classifier Loss 0.5253844261169434, Total Loss 240.52537536621094\n",
      "3: Encoding Loss 21.456132888793945, Transition Loss -11.563080787658691, Classifier Loss 0.5033267736434937, Total Loss 221.9794158935547\n",
      "3: Encoding Loss 22.677324295043945, Transition Loss -7.2327961921691895, Classifier Loss 0.5090591907501221, Total Loss 232.32305908203125\n",
      "3: Encoding Loss 22.22102928161621, Transition Loss -9.73920726776123, Classifier Loss 0.5142887234687805, Total Loss 229.19515991210938\n",
      "3: Encoding Loss 20.405094146728516, Transition Loss -10.1483736038208, Classifier Loss 0.5129051804542542, Total Loss 214.5292510986328\n",
      "3: Encoding Loss 21.29317855834961, Transition Loss -10.465892791748047, Classifier Loss 0.5141470432281494, Total Loss 221.758056640625\n",
      "3: Encoding Loss 21.712926864624023, Transition Loss -8.140884399414062, Classifier Loss 0.5131964087486267, Total Loss 225.02142333984375\n",
      "3: Encoding Loss 22.03842544555664, Transition Loss -6.934482097625732, Classifier Loss 0.5240647792816162, Total Loss 228.71249389648438\n",
      "3: Encoding Loss 20.16193199157715, Transition Loss -9.554496765136719, Classifier Loss 0.5116655230522156, Total Loss 212.46009826660156\n",
      "3: Encoding Loss 21.92380714416504, Transition Loss -7.231943607330322, Classifier Loss 0.5173360109329224, Total Loss 227.1226043701172\n",
      "3: Encoding Loss 21.430395126342773, Transition Loss -9.388388633728027, Classifier Loss 0.5142287015914917, Total Loss 222.86415100097656\n",
      "3: Encoding Loss 21.76089859008789, Transition Loss -12.63447380065918, Classifier Loss 0.5018939971923828, Total Loss 224.2740478515625\n",
      "3: Encoding Loss 22.26434326171875, Transition Loss -7.349291801452637, Classifier Loss 0.5107405185699463, Total Loss 229.1873321533203\n",
      "3: Encoding Loss 20.991777420043945, Transition Loss -15.575342178344727, Classifier Loss 0.5025365352630615, Total Loss 218.18475341796875\n",
      "3: Encoding Loss 21.58298683166504, Transition Loss -9.160972595214844, Classifier Loss 0.5086889863014221, Total Loss 223.5309600830078\n",
      "3: Encoding Loss 21.528316497802734, Transition Loss -10.022378921508789, Classifier Loss 0.49613404273986816, Total Loss 221.83795166015625\n",
      "3: Encoding Loss 20.719379425048828, Transition Loss -10.012205123901367, Classifier Loss 0.5163292288780212, Total Loss 217.38595581054688\n",
      "3: Encoding Loss 21.90878677368164, Transition Loss -10.242298126220703, Classifier Loss 0.5019059181213379, Total Loss 225.45883178710938\n",
      "3: Encoding Loss 22.06721305847168, Transition Loss -9.59745979309082, Classifier Loss 0.5063832402229309, Total Loss 227.17410278320312\n",
      "3: Encoding Loss 20.527427673339844, Transition Loss -12.049826622009277, Classifier Loss 0.5106567144393921, Total Loss 215.28268432617188\n",
      "3: Encoding Loss 22.729463577270508, Transition Loss -11.9451322555542, Classifier Loss 0.5020412802696228, Total Loss 232.03744506835938\n",
      "3: Encoding Loss 21.19460105895996, Transition Loss -9.57673168182373, Classifier Loss 0.50554358959198, Total Loss 220.1092529296875\n",
      "3: Encoding Loss 21.292015075683594, Transition Loss -9.24036693572998, Classifier Loss 0.5109117031097412, Total Loss 221.42544555664062\n",
      "3: Encoding Loss 20.598608016967773, Transition Loss -11.719918251037598, Classifier Loss 0.506142258644104, Total Loss 215.40074157714844\n",
      "3: Encoding Loss 20.254247665405273, Transition Loss -9.743770599365234, Classifier Loss 0.5138250589370728, Total Loss 213.41453552246094\n",
      "3: Encoding Loss 20.005468368530273, Transition Loss -12.54711627960205, Classifier Loss 0.5059429407119751, Total Loss 210.6355438232422\n",
      "3: Encoding Loss 20.43471908569336, Transition Loss -9.75306510925293, Classifier Loss 0.5167652368545532, Total Loss 215.15234375\n",
      "3: Encoding Loss 19.46845817565918, Transition Loss -11.390274047851562, Classifier Loss 0.5058853626251221, Total Loss 206.33392333984375\n",
      "3: Encoding Loss 22.382457733154297, Transition Loss -7.159709453582764, Classifier Loss 0.5104892253875732, Total Loss 230.10716247558594\n",
      "3: Encoding Loss 22.354915618896484, Transition Loss -8.366167068481445, Classifier Loss 0.5109579563140869, Total Loss 229.93345642089844\n",
      "3: Encoding Loss 20.348291397094727, Transition Loss -10.972305297851562, Classifier Loss 0.5062181949615479, Total Loss 213.40594482421875\n",
      "3: Encoding Loss 19.814069747924805, Transition Loss -11.901016235351562, Classifier Loss 0.5022560358047485, Total Loss 208.73577880859375\n",
      "3: Encoding Loss 19.935901641845703, Transition Loss -11.582136154174805, Classifier Loss 0.505170464515686, Total Loss 210.00193786621094\n",
      "3: Encoding Loss 20.04201889038086, Transition Loss -10.472846031188965, Classifier Loss 0.4996222257614136, Total Loss 210.29629516601562\n",
      "3: Encoding Loss 21.64573097229004, Transition Loss -10.47113037109375, Classifier Loss 0.49659061431884766, Total Loss 222.82281494140625\n",
      "3: Encoding Loss 20.7004451751709, Transition Loss -9.255342483520508, Classifier Loss 0.5083146095275879, Total Loss 216.4331817626953\n",
      "3: Encoding Loss 20.475818634033203, Transition Loss -9.687891960144043, Classifier Loss 0.5064214468002319, Total Loss 214.44676208496094\n",
      "3: Encoding Loss 21.88862419128418, Transition Loss -13.084040641784668, Classifier Loss 0.486789733171463, Total Loss 223.7853546142578\n",
      "3: Encoding Loss 21.857425689697266, Transition Loss -9.84386920928955, Classifier Loss 0.503703773021698, Total Loss 225.22781372070312\n",
      "3: Encoding Loss 22.57172966003418, Transition Loss -12.880316734313965, Classifier Loss 0.4831281304359436, Total Loss 228.8840789794922\n",
      "3: Encoding Loss 20.568029403686523, Transition Loss -10.464327812194824, Classifier Loss 0.5032200813293457, Total Loss 214.86415100097656\n",
      "3: Encoding Loss 21.74213409423828, Transition Loss -8.591475486755371, Classifier Loss 0.5068871378898621, Total Loss 224.6240692138672\n",
      "3: Encoding Loss 20.218486785888672, Transition Loss -10.559114456176758, Classifier Loss 0.49619555473327637, Total Loss 211.3653564453125\n",
      "3: Encoding Loss 21.129812240600586, Transition Loss -8.205389976501465, Classifier Loss 0.5037392973899841, Total Loss 219.41078186035156\n",
      "3: Encoding Loss 18.839860916137695, Transition Loss -7.2025861740112305, Classifier Loss 0.518330991268158, Total Loss 202.55055236816406\n",
      "3: Encoding Loss 21.073219299316406, Transition Loss -14.31772232055664, Classifier Loss 0.491629034280777, Total Loss 217.74578857421875\n",
      "3: Encoding Loss 20.584321975708008, Transition Loss -11.715555191040039, Classifier Loss 0.4990812838077545, Total Loss 214.58035278320312\n",
      "3: Encoding Loss 20.67559051513672, Transition Loss -11.2227783203125, Classifier Loss 0.49527066946029663, Total Loss 214.92955017089844\n",
      "3: Encoding Loss 22.03802490234375, Transition Loss -14.154582023620605, Classifier Loss 0.47895169258117676, Total Loss 224.196533203125\n",
      "3: Encoding Loss 19.979747772216797, Transition Loss -9.43659496307373, Classifier Loss 0.5012555122375488, Total Loss 209.96165466308594\n",
      "3: Encoding Loss 20.678585052490234, Transition Loss -7.120326042175293, Classifier Loss 0.5101061463356018, Total Loss 216.43789672851562\n",
      "3: Encoding Loss 21.030160903930664, Transition Loss -13.717265129089355, Classifier Loss 0.4954840838909149, Total Loss 217.78695678710938\n",
      "3: Encoding Loss 18.23390769958496, Transition Loss -10.845138549804688, Classifier Loss 0.509945273399353, Total Loss 196.86361694335938\n",
      "3: Encoding Loss 21.24165916442871, Transition Loss -12.471108436584473, Classifier Loss 0.4874705672264099, Total Loss 218.67784118652344\n",
      "3: Encoding Loss 20.64473533630371, Transition Loss -11.17790699005127, Classifier Loss 0.492137610912323, Total Loss 214.36940002441406\n",
      "3: Encoding Loss 19.194368362426758, Transition Loss -10.63644790649414, Classifier Loss 0.5046223998069763, Total Loss 204.0150604248047\n",
      "3: Encoding Loss 21.338390350341797, Transition Loss -9.920339584350586, Classifier Loss 0.5020754933357239, Total Loss 220.91270446777344\n",
      "3: Encoding Loss 21.008455276489258, Transition Loss -10.9666748046875, Classifier Loss 0.48696714639663696, Total Loss 216.7621612548828\n",
      "3: Encoding Loss 20.88741111755371, Transition Loss -16.141828536987305, Classifier Loss 0.47666501998901367, Total Loss 214.76255798339844\n",
      "3: Encoding Loss 20.577816009521484, Transition Loss -12.142245292663574, Classifier Loss 0.48541349172592163, Total Loss 213.16146850585938\n",
      "3: Encoding Loss 19.617612838745117, Transition Loss -12.880891799926758, Classifier Loss 0.4923897981643677, Total Loss 206.17730712890625\n",
      "3: Encoding Loss 21.28499984741211, Transition Loss -10.144258499145508, Classifier Loss 0.49673891067504883, Total Loss 219.95187377929688\n",
      "3: Encoding Loss 19.197895050048828, Transition Loss -10.902856826782227, Classifier Loss 0.5048505663871765, Total Loss 204.0660400390625\n",
      "3: Encoding Loss 21.450504302978516, Transition Loss -9.612432479858398, Classifier Loss 0.4912169277667999, Total Loss 220.7238006591797\n",
      "3: Encoding Loss 18.798442840576172, Transition Loss -8.061158180236816, Classifier Loss 0.5103960037231445, Total Loss 201.425537109375\n",
      "3: Encoding Loss 18.552831649780273, Transition Loss -9.904204368591309, Classifier Loss 0.5067317485809326, Total Loss 199.09384155273438\n",
      "3: Encoding Loss 21.181995391845703, Transition Loss -11.446100234985352, Classifier Loss 0.48660844564437866, Total Loss 218.11451721191406\n",
      "3: Encoding Loss 19.054439544677734, Transition Loss -10.452367782592773, Classifier Loss 0.5029974579811096, Total Loss 202.73318481445312\n",
      "3: Encoding Loss 22.18691635131836, Transition Loss -14.686247825622559, Classifier Loss 0.47386330366134644, Total Loss 224.87875366210938\n",
      "3: Encoding Loss 19.64348793029785, Transition Loss -13.346731185913086, Classifier Loss 0.47873321175575256, Total Loss 205.0185546875\n",
      "3: Encoding Loss 20.27077293395996, Transition Loss -11.861339569091797, Classifier Loss 0.4896202087402344, Total Loss 211.12583923339844\n",
      "3: Encoding Loss 19.27878761291504, Transition Loss -11.776298522949219, Classifier Loss 0.4926719665527344, Total Loss 203.49514770507812\n",
      "3: Encoding Loss 19.178030014038086, Transition Loss -9.455588340759277, Classifier Loss 0.5010510087013245, Total Loss 203.52745056152344\n",
      "3: Encoding Loss 22.638582229614258, Transition Loss -10.307934761047363, Classifier Loss 0.4806094169616699, Total Loss 229.16754150390625\n",
      "3: Encoding Loss 21.200326919555664, Transition Loss -8.50818157196045, Classifier Loss 0.4957088828086853, Total Loss 219.1717987060547\n",
      "3: Encoding Loss 18.15947151184082, Transition Loss -12.548497200012207, Classifier Loss 0.4871818721294403, Total Loss 193.991455078125\n",
      "3: Encoding Loss 18.984811782836914, Transition Loss -8.200939178466797, Classifier Loss 0.5078150033950806, Total Loss 202.6583709716797\n",
      "3: Encoding Loss 21.502798080444336, Transition Loss -11.68774700164795, Classifier Loss 0.47786203026771545, Total Loss 219.80625915527344\n",
      "3: Encoding Loss 19.347206115722656, Transition Loss -13.227616310119629, Classifier Loss 0.48822492361068726, Total Loss 203.59750366210938\n",
      "3: Encoding Loss 21.608667373657227, Transition Loss -12.304676055908203, Classifier Loss 0.4890348017215729, Total Loss 221.77037048339844\n",
      "3: Encoding Loss 19.33269691467285, Transition Loss -12.600141525268555, Classifier Loss 0.4835730791091919, Total Loss 203.01637268066406\n",
      "3: Encoding Loss 21.892534255981445, Transition Loss -10.714049339294434, Classifier Loss 0.4822773337364197, Total Loss 223.36587524414062\n",
      "3: Encoding Loss 20.919939041137695, Transition Loss -14.158803939819336, Classifier Loss 0.4791177809238434, Total Loss 215.26844787597656\n",
      "3: Encoding Loss 20.04632568359375, Transition Loss -8.799299240112305, Classifier Loss 0.4957016706466675, Total Loss 209.9390106201172\n",
      "3: Encoding Loss 21.50859260559082, Transition Loss -13.280531883239746, Classifier Loss 0.4783037006855011, Total Loss 219.89645385742188\n",
      "3: Encoding Loss 21.99103546142578, Transition Loss -10.026351928710938, Classifier Loss 0.48059430718421936, Total Loss 223.9857177734375\n",
      "3: Encoding Loss 19.60904884338379, Transition Loss -10.055002212524414, Classifier Loss 0.4995754063129425, Total Loss 206.82791137695312\n",
      "3: Encoding Loss 20.184619903564453, Transition Loss -14.248825073242188, Classifier Loss 0.4763280153274536, Total Loss 209.10690307617188\n",
      "3: Encoding Loss 22.000774383544922, Transition Loss -15.00666332244873, Classifier Loss 0.46098992228507996, Total Loss 222.10218811035156\n",
      "3: Encoding Loss 18.53851890563965, Transition Loss -13.127449989318848, Classifier Loss 0.48195135593414307, Total Loss 196.50067138671875\n",
      "3: Encoding Loss 18.39630889892578, Transition Loss -13.294075965881348, Classifier Loss 0.48857951164245605, Total Loss 196.02577209472656\n",
      "3: Encoding Loss 19.100032806396484, Transition Loss -11.228577613830566, Classifier Loss 0.4873911738395691, Total Loss 201.5371551513672\n",
      "3: Encoding Loss 19.68674087524414, Transition Loss -11.36141586303711, Classifier Loss 0.49204763770103455, Total Loss 206.69642639160156\n",
      "3: Encoding Loss 19.270402908325195, Transition Loss -10.59971809387207, Classifier Loss 0.4770159423351288, Total Loss 201.86270141601562\n",
      "3: Encoding Loss 18.7508544921875, Transition Loss -12.399922370910645, Classifier Loss 0.4761626124382019, Total Loss 197.62060546875\n",
      "3: Encoding Loss 19.81818199157715, Transition Loss -12.161398887634277, Classifier Loss 0.48020732402801514, Total Loss 206.5637664794922\n",
      "3: Encoding Loss 19.466846466064453, Transition Loss -9.769018173217773, Classifier Loss 0.48550471663475037, Total Loss 204.28329467773438\n",
      "3: Encoding Loss 18.304197311401367, Transition Loss -11.245823860168457, Classifier Loss 0.49199169874191284, Total Loss 195.63050842285156\n",
      "3: Encoding Loss 19.497655868530273, Transition Loss -11.042499542236328, Classifier Loss 0.48936229944229126, Total Loss 204.91526794433594\n",
      "3: Encoding Loss 20.383548736572266, Transition Loss -13.412527084350586, Classifier Loss 0.4839400351047516, Total Loss 211.459716796875\n",
      "3: Encoding Loss 18.12148094177246, Transition Loss -8.983656883239746, Classifier Loss 0.4942217171192169, Total Loss 194.3922119140625\n",
      "3: Encoding Loss 20.113245010375977, Transition Loss -10.372997283935547, Classifier Loss 0.4800850450992584, Total Loss 208.91238403320312\n",
      "3: Encoding Loss 19.784130096435547, Transition Loss -11.511335372924805, Classifier Loss 0.49550825357437134, Total Loss 207.8215789794922\n",
      "3: Encoding Loss 19.74134063720703, Transition Loss -12.113441467285156, Classifier Loss 0.4897771179676056, Total Loss 206.906005859375\n",
      "3: Encoding Loss 17.872257232666016, Transition Loss -12.447513580322266, Classifier Loss 0.48197174072265625, Total Loss 191.17274475097656\n",
      "3: Encoding Loss 18.622507095336914, Transition Loss -11.654048919677734, Classifier Loss 0.48050153255462646, Total Loss 197.0278778076172\n",
      "3: Encoding Loss 19.797040939331055, Transition Loss -13.226672172546387, Classifier Loss 0.47150418162345886, Total Loss 205.52410888671875\n",
      "3: Encoding Loss 20.104564666748047, Transition Loss -14.08212947845459, Classifier Loss 0.4717475175857544, Total Loss 208.0084686279297\n",
      "3: Encoding Loss 18.43120765686035, Transition Loss -10.195168495178223, Classifier Loss 0.5011457204818726, Total Loss 197.56219482421875\n",
      "3: Encoding Loss 19.435544967651367, Transition Loss -15.532641410827637, Classifier Loss 0.4723212718963623, Total Loss 202.71337890625\n",
      "3: Encoding Loss 18.26418113708496, Transition Loss -11.72416877746582, Classifier Loss 0.4978783428668976, Total Loss 195.89892578125\n",
      "3: Encoding Loss 19.59136962890625, Transition Loss -10.265164375305176, Classifier Loss 0.4896734356880188, Total Loss 205.6962432861328\n",
      "3: Encoding Loss 19.930280685424805, Transition Loss -13.990806579589844, Classifier Loss 0.4722439646720886, Total Loss 206.66384887695312\n",
      "3: Encoding Loss 19.253597259521484, Transition Loss -12.067684173583984, Classifier Loss 0.49280375242233276, Total Loss 203.3067626953125\n",
      "3: Encoding Loss 20.19765853881836, Transition Loss -15.161588668823242, Classifier Loss 0.46707674860954285, Total Loss 208.28591918945312\n",
      "3: Encoding Loss 19.184070587158203, Transition Loss -14.164091110229492, Classifier Loss 0.47215449810028076, Total Loss 200.6851806640625\n",
      "3: Encoding Loss 18.59593963623047, Transition Loss -16.52228355407715, Classifier Loss 0.4776786267757416, Total Loss 196.53207397460938\n",
      "3: Encoding Loss 20.897136688232422, Transition Loss -16.98324966430664, Classifier Loss 0.4459553062915802, Total Loss 211.7692413330078\n",
      "4: Encoding Loss 19.081968307495117, Transition Loss -13.531148910522461, Classifier Loss 0.46787723898887634, Total Loss 199.44076538085938\n",
      "4: Encoding Loss 18.678377151489258, Transition Loss -9.192432403564453, Classifier Loss 0.498018354177475, Total Loss 199.22702026367188\n",
      "4: Encoding Loss 19.857051849365234, Transition Loss -10.319146156311035, Classifier Loss 0.495791494846344, Total Loss 208.4335174560547\n",
      "4: Encoding Loss 18.662927627563477, Transition Loss -13.664018630981445, Classifier Loss 0.47456443309783936, Total Loss 196.7571258544922\n",
      "4: Encoding Loss 18.761171340942383, Transition Loss -9.281712532043457, Classifier Loss 0.4923049807548523, Total Loss 199.31800842285156\n",
      "4: Encoding Loss 19.27223777770996, Transition Loss -15.682868003845215, Classifier Loss 0.47416338324546814, Total Loss 201.59109497070312\n",
      "4: Encoding Loss 19.18000602722168, Transition Loss -13.220937728881836, Classifier Loss 0.4883970022201538, Total Loss 202.27711486816406\n",
      "4: Encoding Loss 18.16608428955078, Transition Loss -14.281411170959473, Classifier Loss 0.4720950126647949, Total Loss 192.5353240966797\n",
      "4: Encoding Loss 18.64304542541504, Transition Loss -14.251968383789062, Classifier Loss 0.46352237462997437, Total Loss 195.49374389648438\n",
      "4: Encoding Loss 18.249210357666016, Transition Loss -12.494668960571289, Classifier Loss 0.4720166325569153, Total Loss 193.19284057617188\n",
      "4: Encoding Loss 18.20825958251953, Transition Loss -10.971263885498047, Classifier Loss 0.5021107196807861, Total Loss 195.8749542236328\n",
      "4: Encoding Loss 17.415245056152344, Transition Loss -13.004608154296875, Classifier Loss 0.48556190729141235, Total Loss 187.87554931640625\n",
      "4: Encoding Loss 18.09202766418457, Transition Loss -11.81715202331543, Classifier Loss 0.4758228659629822, Total Loss 192.31614685058594\n",
      "4: Encoding Loss 17.52767562866211, Transition Loss -11.032588005065918, Classifier Loss 0.4789683222770691, Total Loss 188.1160430908203\n",
      "4: Encoding Loss 19.30603790283203, Transition Loss -15.055816650390625, Classifier Loss 0.46683239936828613, Total Loss 201.1285400390625\n",
      "4: Encoding Loss 19.833789825439453, Transition Loss -17.585342407226562, Classifier Loss 0.4465404450893402, Total Loss 203.32086181640625\n",
      "4: Encoding Loss 20.529296875, Transition Loss -11.142289161682129, Classifier Loss 0.48117637634277344, Total Loss 212.34979248046875\n",
      "4: Encoding Loss 17.6802978515625, Transition Loss -8.78797435760498, Classifier Loss 0.48462167382240295, Total Loss 189.9027862548828\n",
      "4: Encoding Loss 18.305843353271484, Transition Loss -14.711273193359375, Classifier Loss 0.4706079363822937, Total Loss 193.50460815429688\n",
      "4: Encoding Loss 18.80661392211914, Transition Loss -11.207696914672852, Classifier Loss 0.48264095187187195, Total Loss 198.7147674560547\n",
      "4: Encoding Loss 18.623451232910156, Transition Loss -13.296587944030762, Classifier Loss 0.47915419936180115, Total Loss 196.90037536621094\n",
      "4: Encoding Loss 19.556560516357422, Transition Loss -12.078502655029297, Classifier Loss 0.46983781456947327, Total Loss 203.43386840820312\n",
      "4: Encoding Loss 20.503782272338867, Transition Loss -13.155867576599121, Classifier Loss 0.47583115100860596, Total Loss 211.61074829101562\n",
      "4: Encoding Loss 18.311697006225586, Transition Loss -12.399213790893555, Classifier Loss 0.47610920667648315, Total Loss 194.1020050048828\n",
      "4: Encoding Loss 18.216753005981445, Transition Loss -13.630675315856934, Classifier Loss 0.48164841532707214, Total Loss 193.89613342285156\n",
      "4: Encoding Loss 18.21344757080078, Transition Loss -8.841911315917969, Classifier Loss 0.4888673424720764, Total Loss 194.59254455566406\n",
      "4: Encoding Loss 18.732667922973633, Transition Loss -11.81513786315918, Classifier Loss 0.4973849058151245, Total Loss 199.59747314453125\n",
      "4: Encoding Loss 20.34756851196289, Transition Loss -17.468841552734375, Classifier Loss 0.46631452441215515, Total Loss 209.40850830078125\n",
      "4: Encoding Loss 18.157066345214844, Transition Loss -15.492545127868652, Classifier Loss 0.4589027166366577, Total Loss 191.14370727539062\n",
      "4: Encoding Loss 18.74768829345703, Transition Loss -10.197029113769531, Classifier Loss 0.479579359292984, Total Loss 197.93739318847656\n",
      "4: Encoding Loss 18.45486068725586, Transition Loss -11.114092826843262, Classifier Loss 0.47797465324401855, Total Loss 195.43414306640625\n",
      "4: Encoding Loss 16.657081604003906, Transition Loss -11.936700820922852, Classifier Loss 0.48545488715171814, Total Loss 181.7997589111328\n",
      "4: Encoding Loss 17.200305938720703, Transition Loss -12.254677772521973, Classifier Loss 0.46767744421958923, Total Loss 184.36773681640625\n",
      "4: Encoding Loss 16.97126579284668, Transition Loss -15.506423950195312, Classifier Loss 0.47553327679634094, Total Loss 183.3203582763672\n",
      "4: Encoding Loss 17.694225311279297, Transition Loss -12.813769340515137, Classifier Loss 0.46754565834999084, Total Loss 188.30581665039062\n",
      "4: Encoding Loss 17.024330139160156, Transition Loss -13.853658676147461, Classifier Loss 0.45075753331184387, Total Loss 181.26760864257812\n",
      "4: Encoding Loss 17.82566261291504, Transition Loss -13.524380683898926, Classifier Loss 0.4683653712272644, Total Loss 189.4391326904297\n",
      "4: Encoding Loss 16.426395416259766, Transition Loss -14.362499237060547, Classifier Loss 0.46175509691238403, Total Loss 177.58380126953125\n",
      "4: Encoding Loss 19.51063346862793, Transition Loss -12.771342277526855, Classifier Loss 0.45237797498703003, Total Loss 201.3203125\n",
      "4: Encoding Loss 19.00640106201172, Transition Loss -10.054839134216309, Classifier Loss 0.4978103041648865, Total Loss 201.83023071289062\n",
      "4: Encoding Loss 17.54187774658203, Transition Loss -14.045975685119629, Classifier Loss 0.45324552059173584, Total Loss 185.65676879882812\n",
      "4: Encoding Loss 18.644197463989258, Transition Loss -13.621784210205078, Classifier Loss 0.4681178331375122, Total Loss 195.96263122558594\n",
      "4: Encoding Loss 18.220792770385742, Transition Loss -15.881692886352539, Classifier Loss 0.4702116847038269, Total Loss 192.78433227539062\n",
      "4: Encoding Loss 16.67940902709961, Transition Loss -12.52774429321289, Classifier Loss 0.48188668489456177, Total Loss 181.6214599609375\n",
      "4: Encoding Loss 17.18593406677246, Transition Loss -16.142017364501953, Classifier Loss 0.46086400747299194, Total Loss 183.5706329345703\n",
      "4: Encoding Loss 18.033977508544922, Transition Loss -10.350086212158203, Classifier Loss 0.4876036047935486, Total Loss 193.03012084960938\n",
      "4: Encoding Loss 18.025114059448242, Transition Loss -13.959239959716797, Classifier Loss 0.4879911541938782, Total Loss 192.9972381591797\n",
      "4: Encoding Loss 16.238374710083008, Transition Loss -11.917061805725098, Classifier Loss 0.47348546981811523, Total Loss 177.253173828125\n",
      "4: Encoding Loss 18.041784286499023, Transition Loss -11.997450828552246, Classifier Loss 0.4940185546875, Total Loss 193.73373413085938\n",
      "4: Encoding Loss 17.517610549926758, Transition Loss -15.80360221862793, Classifier Loss 0.4732820391654968, Total Loss 187.46592712402344\n",
      "4: Encoding Loss 17.66071319580078, Transition Loss -18.87017250061035, Classifier Loss 0.441384494304657, Total Loss 185.42039489746094\n",
      "4: Encoding Loss 18.34463119506836, Transition Loss -12.487833023071289, Classifier Loss 0.46935245394706726, Total Loss 193.68980407714844\n",
      "4: Encoding Loss 17.399126052856445, Transition Loss -16.266387939453125, Classifier Loss 0.4684511423110962, Total Loss 186.0348663330078\n",
      "4: Encoding Loss 17.626638412475586, Transition Loss -13.667534828186035, Classifier Loss 0.46056798100471497, Total Loss 187.06716918945312\n",
      "4: Encoding Loss 17.3487548828125, Transition Loss -13.943391799926758, Classifier Loss 0.4392848610877991, Total Loss 182.71572875976562\n",
      "4: Encoding Loss 16.62526512145996, Transition Loss -11.757865905761719, Classifier Loss 0.4873150587081909, Total Loss 181.7312774658203\n",
      "4: Encoding Loss 17.67926597595215, Transition Loss -14.290407180786133, Classifier Loss 0.45162850618362427, Total Loss 186.59413146972656\n",
      "4: Encoding Loss 18.1306095123291, Transition Loss -15.237817764282227, Classifier Loss 0.4609336256980896, Total Loss 191.13519287109375\n",
      "4: Encoding Loss 16.407060623168945, Transition Loss -18.64221954345703, Classifier Loss 0.45251697301864624, Total Loss 176.50445556640625\n",
      "4: Encoding Loss 18.794099807739258, Transition Loss -17.938228607177734, Classifier Loss 0.4592297077178955, Total Loss 196.27218627929688\n",
      "4: Encoding Loss 17.27295684814453, Transition Loss -13.518205642700195, Classifier Loss 0.4641302227973938, Total Loss 184.59397888183594\n",
      "4: Encoding Loss 17.316713333129883, Transition Loss -12.831927299499512, Classifier Loss 0.47824233770370483, Total Loss 186.35537719726562\n",
      "4: Encoding Loss 16.992095947265625, Transition Loss -16.907930374145508, Classifier Loss 0.46582406759262085, Total Loss 182.51577758789062\n",
      "4: Encoding Loss 16.29092788696289, Transition Loss -11.754862785339355, Classifier Loss 0.4771292805671692, Total Loss 178.03799438476562\n",
      "4: Encoding Loss 15.93942928314209, Transition Loss -16.692495346069336, Classifier Loss 0.4505422115325928, Total Loss 172.56631469726562\n",
      "4: Encoding Loss 16.820270538330078, Transition Loss -11.656452178955078, Classifier Loss 0.4935482144355774, Total Loss 183.91465759277344\n",
      "4: Encoding Loss 15.673310279846191, Transition Loss -11.396677017211914, Classifier Loss 0.47723013162612915, Total Loss 173.1072235107422\n",
      "4: Encoding Loss 18.07172393798828, Transition Loss -15.289678573608398, Classifier Loss 0.46328282356262207, Total Loss 190.89901733398438\n",
      "4: Encoding Loss 18.131704330444336, Transition Loss -12.174331665039062, Classifier Loss 0.4838590919971466, Total Loss 193.43710327148438\n",
      "4: Encoding Loss 16.689279556274414, Transition Loss -15.122604370117188, Classifier Loss 0.47345027327537537, Total Loss 180.8562469482422\n",
      "4: Encoding Loss 15.929981231689453, Transition Loss -15.314075469970703, Classifier Loss 0.4592995047569275, Total Loss 173.36672973632812\n",
      "4: Encoding Loss 16.00815773010254, Transition Loss -14.715738296508789, Classifier Loss 0.46146559715270996, Total Loss 174.20887756347656\n",
      "4: Encoding Loss 16.328121185302734, Transition Loss -13.345454216003418, Classifier Loss 0.4642314314842224, Total Loss 177.0454559326172\n",
      "4: Encoding Loss 17.62138557434082, Transition Loss -15.362702369689941, Classifier Loss 0.45469579100608826, Total Loss 186.43759155273438\n",
      "4: Encoding Loss 16.92563819885254, Transition Loss -13.334549903869629, Classifier Loss 0.47671401500701904, Total Loss 183.07383728027344\n",
      "4: Encoding Loss 16.439533233642578, Transition Loss -14.913881301879883, Classifier Loss 0.4576181173324585, Total Loss 177.2751007080078\n",
      "4: Encoding Loss 17.732431411743164, Transition Loss -15.550641059875488, Classifier Loss 0.4479731321334839, Total Loss 186.65365600585938\n",
      "4: Encoding Loss 17.62506866455078, Transition Loss -15.737133026123047, Classifier Loss 0.45791786909103394, Total Loss 186.7891845703125\n",
      "4: Encoding Loss 18.255828857421875, Transition Loss -18.921092987060547, Classifier Loss 0.42798370122909546, Total Loss 188.84121704101562\n",
      "4: Encoding Loss 16.672454833984375, Transition Loss -15.120241165161133, Classifier Loss 0.4763804078102112, Total Loss 181.01466369628906\n",
      "4: Encoding Loss 17.46088218688965, Transition Loss -13.928346633911133, Classifier Loss 0.4664381444454193, Total Loss 186.3280792236328\n",
      "4: Encoding Loss 16.120271682739258, Transition Loss -13.733513832092285, Classifier Loss 0.4584770202636719, Total Loss 174.80712890625\n",
      "4: Encoding Loss 17.291406631469727, Transition Loss -11.074987411499023, Classifier Loss 0.4788043200969696, Total Loss 186.20947265625\n",
      "4: Encoding Loss 14.801268577575684, Transition Loss -11.615365982055664, Classifier Loss 0.47012239694595337, Total Loss 165.42007446289062\n",
      "4: Encoding Loss 17.179332733154297, Transition Loss -17.190261840820312, Classifier Loss 0.4456245005130768, Total Loss 181.9936981201172\n",
      "4: Encoding Loss 16.506298065185547, Transition Loss -14.115962028503418, Classifier Loss 0.46339553594589233, Total Loss 178.3871307373047\n",
      "4: Encoding Loss 16.35100555419922, Transition Loss -17.83605194091797, Classifier Loss 0.44102242588996887, Total Loss 174.90670776367188\n",
      "4: Encoding Loss 18.03705596923828, Transition Loss -18.708547592163086, Classifier Loss 0.43431591987609863, Total Loss 187.72430419921875\n",
      "4: Encoding Loss 15.820878028869629, Transition Loss -12.28765869140625, Classifier Loss 0.45859014987945557, Total Loss 172.42359924316406\n",
      "4: Encoding Loss 16.35922622680664, Transition Loss -13.691612243652344, Classifier Loss 0.46786823868751526, Total Loss 177.65789794921875\n",
      "4: Encoding Loss 16.513952255249023, Transition Loss -22.805206298828125, Classifier Loss 0.43171894550323486, Total Loss 175.27894592285156\n",
      "4: Encoding Loss 14.193034172058105, Transition Loss -12.778977394104004, Classifier Loss 0.4642140865325928, Total Loss 159.963134765625\n",
      "4: Encoding Loss 16.869287490844727, Transition Loss -15.004339218139648, Classifier Loss 0.4471057057380676, Total Loss 179.661865234375\n",
      "4: Encoding Loss 16.5308780670166, Transition Loss -14.083288192749023, Classifier Loss 0.4438627362251282, Total Loss 176.63047790527344\n",
      "4: Encoding Loss 15.149456977844238, Transition Loss -11.558510780334473, Classifier Loss 0.4670119285583496, Total Loss 167.89454650878906\n",
      "4: Encoding Loss 17.011720657348633, Transition Loss -15.815974235534668, Classifier Loss 0.45689764618873596, Total Loss 181.78038024902344\n",
      "4: Encoding Loss 16.53508949279785, Transition Loss -16.682090759277344, Classifier Loss 0.4322335720062256, Total Loss 175.500732421875\n",
      "4: Encoding Loss 16.447792053222656, Transition Loss -19.43484878540039, Classifier Loss 0.41978079080581665, Total Loss 173.55653381347656\n",
      "4: Encoding Loss 16.521522521972656, Transition Loss -13.331796646118164, Classifier Loss 0.44815289974212646, Total Loss 176.98480224609375\n",
      "4: Encoding Loss 15.81620979309082, Transition Loss -17.162002563476562, Classifier Loss 0.4530034065246582, Total Loss 171.8265838623047\n",
      "4: Encoding Loss 17.218780517578125, Transition Loss -18.462806701660156, Classifier Loss 0.4537588357925415, Total Loss 183.1224365234375\n",
      "4: Encoding Loss 15.852189064025879, Transition Loss -11.40072250366211, Classifier Loss 0.48452645540237427, Total Loss 175.2678985595703\n",
      "4: Encoding Loss 17.211681365966797, Transition Loss -13.221769332885742, Classifier Loss 0.44884660840034485, Total Loss 182.5754852294922\n",
      "4: Encoding Loss 14.903796195983887, Transition Loss -10.817380905151367, Classifier Loss 0.48054391145706177, Total Loss 167.2825927734375\n",
      "4: Encoding Loss 14.322406768798828, Transition Loss -14.87281322479248, Classifier Loss 0.4563286304473877, Total Loss 160.20913696289062\n",
      "4: Encoding Loss 16.886615753173828, Transition Loss -18.609500885009766, Classifier Loss 0.4395916759967804, Total Loss 179.04837036132812\n",
      "4: Encoding Loss 14.99808406829834, Transition Loss -12.337718963623047, Classifier Loss 0.4640505611896515, Total Loss 166.3872528076172\n",
      "4: Encoding Loss 17.665185928344727, Transition Loss -17.72578239440918, Classifier Loss 0.42434969544410706, Total Loss 183.75291442871094\n",
      "4: Encoding Loss 15.667405128479004, Transition Loss -16.79230499267578, Classifier Loss 0.4316919147968292, Total Loss 168.5050811767578\n",
      "4: Encoding Loss 15.968954086303711, Transition Loss -16.70673179626465, Classifier Loss 0.4369661808013916, Total Loss 171.44491577148438\n",
      "4: Encoding Loss 14.900776863098145, Transition Loss -17.602447509765625, Classifier Loss 0.440940260887146, Total Loss 163.2967071533203\n",
      "4: Encoding Loss 15.192788124084473, Transition Loss -11.320186614990234, Classifier Loss 0.4676654636859894, Total Loss 168.30661010742188\n",
      "4: Encoding Loss 17.784894943237305, Transition Loss -18.149513244628906, Classifier Loss 0.4277295172214508, Total Loss 185.04847717285156\n",
      "4: Encoding Loss 16.815425872802734, Transition Loss -16.607236862182617, Classifier Loss 0.4491937756538391, Total Loss 179.43946838378906\n",
      "4: Encoding Loss 14.553035736083984, Transition Loss -12.053839683532715, Classifier Loss 0.4559840261936188, Total Loss 162.02029418945312\n",
      "4: Encoding Loss 15.595076560974121, Transition Loss -9.178154945373535, Classifier Loss 0.4887107312679291, Total Loss 173.62985229492188\n",
      "4: Encoding Loss 17.266939163208008, Transition Loss -16.580881118774414, Classifier Loss 0.42707088589668274, Total Loss 180.83929443359375\n",
      "4: Encoding Loss 15.006272315979004, Transition Loss -17.93755340576172, Classifier Loss 0.43978750705718994, Total Loss 164.0253448486328\n",
      "4: Encoding Loss 17.430768966674805, Transition Loss -17.908222198486328, Classifier Loss 0.4590613842010498, Total Loss 185.3487091064453\n",
      "4: Encoding Loss 15.212474822998047, Transition Loss -14.362333297729492, Classifier Loss 0.4422876834869385, Total Loss 165.9257049560547\n",
      "4: Encoding Loss 17.275239944458008, Transition Loss -17.80186653137207, Classifier Loss 0.4335876405239105, Total Loss 181.55712890625\n",
      "4: Encoding Loss 16.417339324951172, Transition Loss -15.831440925598145, Classifier Loss 0.43750250339508057, Total Loss 175.0858154296875\n",
      "4: Encoding Loss 16.008140563964844, Transition Loss -14.084482192993164, Classifier Loss 0.456255167722702, Total Loss 173.6878204345703\n",
      "4: Encoding Loss 16.479368209838867, Transition Loss -20.52161979675293, Classifier Loss 0.4190889596939087, Total Loss 173.73973083496094\n",
      "4: Encoding Loss 17.984262466430664, Transition Loss -13.336713790893555, Classifier Loss 0.44720458984375, Total Loss 188.59188842773438\n",
      "4: Encoding Loss 15.676070213317871, Transition Loss -13.572885513305664, Classifier Loss 0.4597284197807312, Total Loss 171.37869262695312\n",
      "4: Encoding Loss 15.803054809570312, Transition Loss -20.166852951049805, Classifier Loss 0.42460405826568604, Total Loss 168.8808135986328\n",
      "4: Encoding Loss 17.23735237121582, Transition Loss -18.387144088745117, Classifier Loss 0.40948331356048584, Total Loss 178.84347534179688\n",
      "4: Encoding Loss 14.609309196472168, Transition Loss -17.6114444732666, Classifier Loss 0.4276992678642273, Total Loss 159.64088439941406\n",
      "4: Encoding Loss 14.589564323425293, Transition Loss -14.53736686706543, Classifier Loss 0.44720977544784546, Total Loss 161.43458557128906\n",
      "4: Encoding Loss 15.223532676696777, Transition Loss -13.449512481689453, Classifier Loss 0.446313738822937, Total Loss 166.4169464111328\n",
      "4: Encoding Loss 15.591858863830566, Transition Loss -15.202184677124023, Classifier Loss 0.4532565474510193, Total Loss 170.0574951171875\n",
      "4: Encoding Loss 15.625707626342773, Transition Loss -10.75231647491455, Classifier Loss 0.4362640082836151, Total Loss 168.62991333007812\n",
      "4: Encoding Loss 14.714885711669922, Transition Loss -16.536453247070312, Classifier Loss 0.4290814697742462, Total Loss 160.62393188476562\n",
      "4: Encoding Loss 15.314458847045898, Transition Loss -17.557636260986328, Classifier Loss 0.43110960721969604, Total Loss 165.62313842773438\n",
      "4: Encoding Loss 15.335205078125, Transition Loss -15.38244342803955, Classifier Loss 0.4367273151874542, Total Loss 166.35128784179688\n",
      "4: Encoding Loss 15.319340705871582, Transition Loss -11.592172622680664, Classifier Loss 0.4613616168498993, Total Loss 168.68856811523438\n",
      "4: Encoding Loss 15.567151069641113, Transition Loss -14.384827613830566, Classifier Loss 0.45152518153190613, Total Loss 169.6868438720703\n",
      "4: Encoding Loss 16.259946823120117, Transition Loss -17.11204719543457, Classifier Loss 0.4452800154685974, Total Loss 174.60415649414062\n",
      "4: Encoding Loss 14.254467010498047, Transition Loss -12.792791366577148, Classifier Loss 0.43778297305107117, Total Loss 157.8114776611328\n",
      "4: Encoding Loss 16.02857780456543, Transition Loss -14.21108341217041, Classifier Loss 0.43530574440956116, Total Loss 171.75636291503906\n",
      "4: Encoding Loss 16.250520706176758, Transition Loss -12.893657684326172, Classifier Loss 0.47761863470077515, Total Loss 177.76344299316406\n",
      "4: Encoding Loss 15.425175666809082, Transition Loss -17.72466468811035, Classifier Loss 0.4393821656703949, Total Loss 167.33609008789062\n",
      "4: Encoding Loss 14.529662132263184, Transition Loss -11.15406322479248, Classifier Loss 0.4488540291786194, Total Loss 161.12046813964844\n",
      "4: Encoding Loss 14.93543815612793, Transition Loss -14.038280487060547, Classifier Loss 0.4363033175468445, Total Loss 163.1110382080078\n",
      "4: Encoding Loss 15.911543846130371, Transition Loss -15.124746322631836, Classifier Loss 0.4268760085105896, Total Loss 169.9769287109375\n",
      "4: Encoding Loss 15.51882553100586, Transition Loss -18.22566032409668, Classifier Loss 0.4237401783466339, Total Loss 166.52098083496094\n",
      "4: Encoding Loss 14.064876556396484, Transition Loss -16.746543884277344, Classifier Loss 0.44773122668266296, Total Loss 157.28878784179688\n",
      "4: Encoding Loss 14.606534004211426, Transition Loss -22.77513313293457, Classifier Loss 0.4057801365852356, Total Loss 157.42572021484375\n",
      "4: Encoding Loss 14.34550952911377, Transition Loss -14.592207908630371, Classifier Loss 0.4613529145717621, Total Loss 160.89645385742188\n",
      "4: Encoding Loss 15.365433692932129, Transition Loss -14.428982734680176, Classifier Loss 0.44665318727493286, Total Loss 167.58590698242188\n",
      "4: Encoding Loss 15.610126495361328, Transition Loss -20.121803283691406, Classifier Loss 0.4207364618778229, Total Loss 166.95062255859375\n",
      "4: Encoding Loss 15.72179889678955, Transition Loss -14.044240951538086, Classifier Loss 0.4640924334526062, Total Loss 172.1808319091797\n",
      "4: Encoding Loss 15.74281120300293, Transition Loss -19.10445785522461, Classifier Loss 0.42189791798591614, Total Loss 168.12847900390625\n",
      "4: Encoding Loss 14.730191230773926, Transition Loss -19.022741317749023, Classifier Loss 0.4172266125679016, Total Loss 159.56039428710938\n",
      "4: Encoding Loss 13.759018898010254, Transition Loss -21.366294860839844, Classifier Loss 0.41707777976989746, Total Loss 151.77566528320312\n",
      "4: Encoding Loss 15.587957382202148, Transition Loss -25.40410614013672, Classifier Loss 0.3793146014213562, Total Loss 162.6300506591797\n",
      "5: Encoding Loss 15.38939380645752, Transition Loss -11.236074447631836, Classifier Loss 0.4310871362686157, Total Loss 166.22161865234375\n",
      "5: Encoding Loss 15.763627052307129, Transition Loss -10.706087112426758, Classifier Loss 0.47146934270858765, Total Loss 173.25381469726562\n",
      "5: Encoding Loss 16.88985824584961, Transition Loss -9.763046264648438, Classifier Loss 0.4761124849319458, Total Loss 182.72817993164062\n",
      "5: Encoding Loss 14.320324897766113, Transition Loss -19.628259658813477, Classifier Loss 0.4163007438182831, Total Loss 156.18875122070312\n",
      "5: Encoding Loss 14.997950553894043, Transition Loss -13.273018836975098, Classifier Loss 0.44751107692718506, Total Loss 164.7320556640625\n",
      "5: Encoding Loss 14.721077919006348, Transition Loss -19.281230926513672, Classifier Loss 0.4205816388130188, Total Loss 159.82293701171875\n",
      "5: Encoding Loss 15.45045280456543, Transition Loss -16.083885192871094, Classifier Loss 0.4532303214073181, Total Loss 168.92344665527344\n",
      "5: Encoding Loss 13.976279258728027, Transition Loss -14.69906234741211, Classifier Loss 0.42430579662323, Total Loss 154.2378692626953\n",
      "5: Encoding Loss 14.369053840637207, Transition Loss -18.2988338470459, Classifier Loss 0.40589964389801025, Total Loss 155.53872680664062\n",
      "5: Encoding Loss 14.218239784240723, Transition Loss -15.175048828125, Classifier Loss 0.4169105589389801, Total Loss 155.43394470214844\n",
      "5: Encoding Loss 15.319591522216797, Transition Loss -13.878204345703125, Classifier Loss 0.4708403944969177, Total Loss 169.63800048828125\n",
      "5: Encoding Loss 13.794493675231934, Transition Loss -12.036015510559082, Classifier Loss 0.44514143466949463, Total Loss 154.86767578125\n",
      "5: Encoding Loss 14.653757095336914, Transition Loss -12.512848854064941, Classifier Loss 0.4317586421966553, Total Loss 160.40341186523438\n",
      "5: Encoding Loss 13.91714859008789, Transition Loss -12.375462532043457, Classifier Loss 0.4279578924179077, Total Loss 154.1304931640625\n",
      "5: Encoding Loss 15.176868438720703, Transition Loss -19.098472595214844, Classifier Loss 0.41757768392562866, Total Loss 163.16888427734375\n",
      "5: Encoding Loss 15.274155616760254, Transition Loss -21.600017547607422, Classifier Loss 0.38914379477500916, Total Loss 161.1033172607422\n",
      "5: Encoding Loss 16.615192413330078, Transition Loss -14.9179048538208, Classifier Loss 0.43902361392974854, Total Loss 176.82090759277344\n",
      "5: Encoding Loss 14.345658302307129, Transition Loss -11.14464282989502, Classifier Loss 0.43287307024002075, Total Loss 158.05035400390625\n",
      "5: Encoding Loss 14.27740478515625, Transition Loss -16.58197784423828, Classifier Loss 0.4135274589061737, Total Loss 155.5686798095703\n",
      "5: Encoding Loss 15.165359497070312, Transition Loss -12.78226089477539, Classifier Loss 0.44990843534469604, Total Loss 166.3111572265625\n",
      "5: Encoding Loss 14.382081031799316, Transition Loss -16.089868545532227, Classifier Loss 0.42568016052246094, Total Loss 157.62144470214844\n",
      "5: Encoding Loss 15.412293434143066, Transition Loss -14.835168838500977, Classifier Loss 0.4201139509677887, Total Loss 165.30679321289062\n",
      "5: Encoding Loss 16.40998649597168, Transition Loss -17.184295654296875, Classifier Loss 0.43953937292099, Total Loss 175.2303924560547\n",
      "5: Encoding Loss 14.648226737976074, Transition Loss -11.826634407043457, Classifier Loss 0.43825602531433105, Total Loss 161.00904846191406\n",
      "5: Encoding Loss 14.511798858642578, Transition Loss -16.168533325195312, Classifier Loss 0.4326094388961792, Total Loss 159.3520965576172\n",
      "5: Encoding Loss 14.667258262634277, Transition Loss -9.644063949584961, Classifier Loss 0.4510052800178528, Total Loss 162.43667602539062\n",
      "5: Encoding Loss 15.950840950012207, Transition Loss -12.198893547058105, Classifier Loss 0.4739883542060852, Total Loss 175.00311279296875\n",
      "5: Encoding Loss 16.085695266723633, Transition Loss -22.106924057006836, Classifier Loss 0.42184972763061523, Total Loss 170.86611938476562\n",
      "5: Encoding Loss 14.369179725646973, Transition Loss -15.005290985107422, Classifier Loss 0.41160261631011963, Total Loss 156.11070251464844\n",
      "5: Encoding Loss 15.121598243713379, Transition Loss -10.585063934326172, Classifier Loss 0.44238516688346863, Total Loss 165.2091827392578\n",
      "5: Encoding Loss 14.771454811096191, Transition Loss -13.714861869812012, Classifier Loss 0.4303828775882721, Total Loss 161.20718383789062\n",
      "5: Encoding Loss 13.463995933532715, Transition Loss -10.193323135375977, Classifier Loss 0.43738752603530884, Total Loss 151.44866943359375\n",
      "5: Encoding Loss 13.640828132629395, Transition Loss -14.515172004699707, Classifier Loss 0.4105498790740967, Total Loss 150.1787109375\n",
      "5: Encoding Loss 13.72507095336914, Transition Loss -14.889411926269531, Classifier Loss 0.4392026960849762, Total Loss 153.7178497314453\n",
      "5: Encoding Loss 13.788415908813477, Transition Loss -15.685097694396973, Classifier Loss 0.40804392099380493, Total Loss 151.10858154296875\n",
      "5: Encoding Loss 13.458611488342285, Transition Loss -12.810081481933594, Classifier Loss 0.39460456371307373, Total Loss 147.12680053710938\n",
      "5: Encoding Loss 14.449420928955078, Transition Loss -13.624566078186035, Classifier Loss 0.4262339174747467, Total Loss 158.2160186767578\n",
      "5: Encoding Loss 12.772543907165527, Transition Loss -13.060501098632812, Classifier Loss 0.40970057249069214, Total Loss 143.14779663085938\n",
      "5: Encoding Loss 15.868343353271484, Transition Loss -13.999420166015625, Classifier Loss 0.4126022160053253, Total Loss 168.2041778564453\n",
      "5: Encoding Loss 15.126031875610352, Transition Loss -12.846956253051758, Classifier Loss 0.4524490237236023, Total Loss 166.25059509277344\n",
      "5: Encoding Loss 13.868133544921875, Transition Loss -13.440226554870605, Classifier Loss 0.3948219418525696, Total Loss 150.42457580566406\n",
      "5: Encoding Loss 14.837306022644043, Transition Loss -17.605392456054688, Classifier Loss 0.4217422902584076, Total Loss 160.86915588378906\n",
      "5: Encoding Loss 14.409557342529297, Transition Loss -18.572843551635742, Classifier Loss 0.4153723418712616, Total Loss 156.80999755859375\n",
      "5: Encoding Loss 14.024127960205078, Transition Loss -11.96455192565918, Classifier Loss 0.43971264362335205, Total Loss 156.16188049316406\n",
      "5: Encoding Loss 13.125487327575684, Transition Loss -18.89560317993164, Classifier Loss 0.3903881311416626, Total Loss 144.03892517089844\n",
      "5: Encoding Loss 14.922070503234863, Transition Loss -10.163128852844238, Classifier Loss 0.451438844203949, Total Loss 164.51841735839844\n",
      "5: Encoding Loss 14.721940040588379, Transition Loss -18.68513298034668, Classifier Loss 0.43322229385375977, Total Loss 161.09400939941406\n",
      "5: Encoding Loss 12.867300033569336, Transition Loss -11.726673126220703, Classifier Loss 0.4206472933292389, Total Loss 145.00079345703125\n",
      "5: Encoding Loss 15.030477523803711, Transition Loss -14.50765609741211, Classifier Loss 0.45432615280151367, Total Loss 165.67355346679688\n",
      "5: Encoding Loss 14.03603458404541, Transition Loss -18.347930908203125, Classifier Loss 0.422252357006073, Total Loss 154.50985717773438\n",
      "5: Encoding Loss 13.337488174438477, Transition Loss -20.301074981689453, Classifier Loss 0.37751418352127075, Total Loss 144.447265625\n",
      "5: Encoding Loss 14.808786392211914, Transition Loss -14.714924812316895, Classifier Loss 0.42006033658981323, Total Loss 160.47337341308594\n",
      "5: Encoding Loss 14.67869758605957, Transition Loss -11.772212982177734, Classifier Loss 0.4459812641143799, Total Loss 162.02536010742188\n",
      "5: Encoding Loss 14.196686744689941, Transition Loss -14.728205680847168, Classifier Loss 0.40026378631591797, Total Loss 153.59693908691406\n",
      "5: Encoding Loss 13.867956161499023, Transition Loss -14.435270309448242, Classifier Loss 0.3788880705833435, Total Loss 148.82957458496094\n",
      "5: Encoding Loss 13.697370529174805, Transition Loss -10.898866653442383, Classifier Loss 0.436564564704895, Total Loss 153.23324584960938\n",
      "5: Encoding Loss 14.1548433303833, Transition Loss -14.325661659240723, Classifier Loss 0.3968687653541565, Total Loss 152.92276000976562\n",
      "5: Encoding Loss 14.901835441589355, Transition Loss -16.825664520263672, Classifier Loss 0.4134339690208435, Total Loss 160.55470275878906\n",
      "5: Encoding Loss 12.37999439239502, Transition Loss -19.857376098632812, Classifier Loss 0.38002702593803406, Total Loss 137.0386962890625\n",
      "5: Encoding Loss 15.401549339294434, Transition Loss -18.745649337768555, Classifier Loss 0.4177290201187134, Total Loss 164.98153686523438\n",
      "5: Encoding Loss 14.050729751586914, Transition Loss -13.553937911987305, Classifier Loss 0.41688311100006104, Total Loss 154.0914306640625\n",
      "5: Encoding Loss 14.25607681274414, Transition Loss -12.741429328918457, Classifier Loss 0.4327346384525299, Total Loss 157.31951904296875\n",
      "5: Encoding Loss 14.25800609588623, Transition Loss -16.864620208740234, Classifier Loss 0.42346733808517456, Total Loss 156.40740966796875\n",
      "5: Encoding Loss 13.059685707092285, Transition Loss -10.76241397857666, Classifier Loss 0.4180271625518799, Total Loss 146.27806091308594\n",
      "5: Encoding Loss 12.41714859008789, Transition Loss -15.606759071350098, Classifier Loss 0.38978615403175354, Total Loss 138.3126678466797\n",
      "5: Encoding Loss 14.165629386901855, Transition Loss -10.425836563110352, Classifier Loss 0.46333086490631104, Total Loss 159.65603637695312\n",
      "5: Encoding Loss 13.305669784545898, Transition Loss -8.205738067626953, Classifier Loss 0.4435276985168457, Total Loss 150.79647827148438\n",
      "5: Encoding Loss 14.623881340026855, Transition Loss -18.731748580932617, Classifier Loss 0.4062650203704834, Total Loss 157.61380004882812\n",
      "5: Encoding Loss 15.023848533630371, Transition Loss -12.703656196594238, Classifier Loss 0.4479405879974365, Total Loss 164.9822998046875\n",
      "5: Encoding Loss 14.164668083190918, Transition Loss -14.260392189025879, Classifier Loss 0.43815213441848755, Total Loss 157.12969970703125\n",
      "5: Encoding Loss 12.97753620147705, Transition Loss -13.800505638122559, Classifier Loss 0.40909773111343384, Total Loss 144.72731018066406\n",
      "5: Encoding Loss 13.09080982208252, Transition Loss -12.942924499511719, Classifier Loss 0.41005241870880127, Total Loss 145.7291259765625\n",
      "5: Encoding Loss 14.138882637023926, Transition Loss -11.771807670593262, Classifier Loss 0.42725422978401184, Total Loss 155.83413696289062\n",
      "5: Encoding Loss 15.015596389770508, Transition Loss -15.096744537353516, Classifier Loss 0.4149459898471832, Total Loss 161.61634826660156\n",
      "5: Encoding Loss 14.505667686462402, Transition Loss -13.056861877441406, Classifier Loss 0.43787628412246704, Total Loss 159.8303680419922\n",
      "5: Encoding Loss 13.414031028747559, Transition Loss -14.85855484008789, Classifier Loss 0.39987999200820923, Total Loss 147.29727172851562\n",
      "5: Encoding Loss 15.010284423828125, Transition Loss -12.75175666809082, Classifier Loss 0.41952523589134216, Total Loss 162.03224182128906\n",
      "5: Encoding Loss 14.703927040100098, Transition Loss -16.519411087036133, Classifier Loss 0.4088280498981476, Total Loss 158.5109100341797\n",
      "5: Encoding Loss 15.242294311523438, Transition Loss -17.819469451904297, Classifier Loss 0.38177257776260376, Total Loss 160.11204528808594\n",
      "5: Encoding Loss 14.236031532287598, Transition Loss -14.180015563964844, Classifier Loss 0.4425499439239502, Total Loss 158.14041137695312\n",
      "5: Encoding Loss 14.437128067016602, Transition Loss -14.553007125854492, Classifier Loss 0.41484156250953674, Total Loss 156.978271484375\n",
      "5: Encoding Loss 13.657983779907227, Transition Loss -12.64543342590332, Classifier Loss 0.4107523560523987, Total Loss 150.33657836914062\n",
      "5: Encoding Loss 14.651932716369629, Transition Loss -10.593588829040527, Classifier Loss 0.4453871250152588, Total Loss 161.75205993652344\n",
      "5: Encoding Loss 11.777959823608398, Transition Loss -12.23687744140625, Classifier Loss 0.3897841274738312, Total Loss 133.1996612548828\n",
      "5: Encoding Loss 14.672324180603027, Transition Loss -15.169584274291992, Classifier Loss 0.4077886939048767, Total Loss 158.15443420410156\n",
      "5: Encoding Loss 14.184029579162598, Transition Loss -12.423640251159668, Classifier Loss 0.4241717755794525, Total Loss 155.88693237304688\n",
      "5: Encoding Loss 13.435521125793457, Transition Loss -17.813453674316406, Classifier Loss 0.38900765776634216, Total Loss 146.38137817382812\n",
      "5: Encoding Loss 15.594761848449707, Transition Loss -16.019752502441406, Classifier Loss 0.40644609928131104, Total Loss 165.39950561523438\n",
      "5: Encoding Loss 13.150179862976074, Transition Loss -11.365265846252441, Classifier Loss 0.40107738971710205, Total Loss 145.30690002441406\n",
      "5: Encoding Loss 13.494257926940918, Transition Loss -15.280364036560059, Classifier Loss 0.4100668132305145, Total Loss 148.9576873779297\n",
      "5: Encoding Loss 13.395831108093262, Transition Loss -22.42748260498047, Classifier Loss 0.3724392056465149, Total Loss 144.4060821533203\n",
      "5: Encoding Loss 11.58806324005127, Transition Loss -10.957096099853516, Classifier Loss 0.3991720676422119, Total Loss 132.6195068359375\n",
      "5: Encoding Loss 14.108597755432129, Transition Loss -13.155317306518555, Classifier Loss 0.40963754057884216, Total Loss 153.82992553710938\n",
      "5: Encoding Loss 13.858455657958984, Transition Loss -12.948183059692383, Classifier Loss 0.3955772817134857, Total Loss 150.42279052734375\n",
      "5: Encoding Loss 12.664366722106934, Transition Loss -9.580307960510254, Classifier Loss 0.41059643030166626, Total Loss 142.37265014648438\n",
      "5: Encoding Loss 14.218920707702637, Transition Loss -16.223920822143555, Classifier Loss 0.4034329950809479, Total Loss 154.09141540527344\n",
      "5: Encoding Loss 13.452024459838867, Transition Loss -16.75248908996582, Classifier Loss 0.37637025117874146, Total Loss 145.2498779296875\n",
      "5: Encoding Loss 13.858431816101074, Transition Loss -16.36615562438965, Classifier Loss 0.3706084191799164, Total Loss 147.92501831054688\n",
      "5: Encoding Loss 14.096616744995117, Transition Loss -11.269287109375, Classifier Loss 0.4078088402748108, Total Loss 153.55157470703125\n",
      "5: Encoding Loss 14.331869125366211, Transition Loss -14.657135009765625, Classifier Loss 0.4138539731502533, Total Loss 156.0374298095703\n",
      "5: Encoding Loss 14.83742618560791, Transition Loss -18.15294647216797, Classifier Loss 0.4089597463607788, Total Loss 159.59176635742188\n",
      "5: Encoding Loss 14.204802513122559, Transition Loss -8.855700492858887, Classifier Loss 0.4492585361003876, Total Loss 158.5625\n",
      "5: Encoding Loss 14.448834419250488, Transition Loss -12.684955596923828, Classifier Loss 0.3986251950263977, Total Loss 155.45065307617188\n",
      "5: Encoding Loss 12.663987159729004, Transition Loss -10.120819091796875, Classifier Loss 0.43219876289367676, Total Loss 144.5297393798828\n",
      "5: Encoding Loss 11.797107696533203, Transition Loss -14.0745210647583, Classifier Loss 0.39152446389198303, Total Loss 133.52650451660156\n",
      "5: Encoding Loss 14.392292976379395, Transition Loss -18.599470138549805, Classifier Loss 0.39121079444885254, Total Loss 154.25570678710938\n",
      "5: Encoding Loss 12.924702644348145, Transition Loss -11.041213989257812, Classifier Loss 0.4095861613750458, Total Loss 144.35401916503906\n",
      "5: Encoding Loss 15.153212547302246, Transition Loss -15.445442199707031, Classifier Loss 0.37657278776168823, Total Loss 158.87989807128906\n",
      "5: Encoding Loss 14.015419960021973, Transition Loss -14.31485366821289, Classifier Loss 0.3893654942512512, Total Loss 151.0570526123047\n",
      "5: Encoding Loss 13.364805221557617, Transition Loss -15.535057067871094, Classifier Loss 0.3858363628387451, Total Loss 145.4989776611328\n",
      "5: Encoding Loss 12.792815208435059, Transition Loss -16.31316566467285, Classifier Loss 0.38583672046661377, Total Loss 140.9229278564453\n",
      "5: Encoding Loss 13.348814964294434, Transition Loss -10.370307922363281, Classifier Loss 0.41905903816223145, Total Loss 148.69435119628906\n",
      "5: Encoding Loss 14.333023071289062, Transition Loss -18.805164337158203, Classifier Loss 0.37762659788131714, Total Loss 152.423095703125\n",
      "5: Encoding Loss 13.878739356994629, Transition Loss -18.131547927856445, Classifier Loss 0.3949161469936371, Total Loss 150.51791381835938\n",
      "5: Encoding Loss 12.99899673461914, Transition Loss -9.158161163330078, Classifier Loss 0.4196227192878723, Total Loss 145.95240783691406\n",
      "5: Encoding Loss 14.477422714233398, Transition Loss -8.879838943481445, Classifier Loss 0.4457292854785919, Total Loss 160.3905487060547\n",
      "5: Encoding Loss 14.51800537109375, Transition Loss -16.020854949951172, Classifier Loss 0.38202714920043945, Total Loss 154.3435516357422\n",
      "5: Encoding Loss 12.879378318786621, Transition Loss -16.204439163208008, Classifier Loss 0.38549837470054626, Total Loss 141.58163452148438\n",
      "5: Encoding Loss 15.04196548461914, Transition Loss -18.278274536132812, Classifier Loss 0.42379510402679443, Total Loss 162.71156311035156\n",
      "5: Encoding Loss 13.13900375366211, Transition Loss -12.310916900634766, Classifier Loss 0.3946498930454254, Total Loss 144.57456970214844\n",
      "5: Encoding Loss 14.59949016571045, Transition Loss -18.117401123046875, Classifier Loss 0.3799932897090912, Total Loss 154.79164123535156\n",
      "5: Encoding Loss 13.842597007751465, Transition Loss -14.160377502441406, Classifier Loss 0.3911471366882324, Total Loss 149.8526611328125\n",
      "5: Encoding Loss 14.06396484375, Transition Loss -14.517172813415527, Classifier Loss 0.39988023042678833, Total Loss 152.49684143066406\n",
      "5: Encoding Loss 13.548449516296387, Transition Loss -20.14400863647461, Classifier Loss 0.3633495271205902, Total Loss 144.71852111816406\n",
      "5: Encoding Loss 15.522144317626953, Transition Loss -13.858592987060547, Classifier Loss 0.40525296330451965, Total Loss 164.6996612548828\n",
      "5: Encoding Loss 13.281097412109375, Transition Loss -13.419315338134766, Classifier Loss 0.40759360790252686, Total Loss 147.00546264648438\n",
      "5: Encoding Loss 13.571474075317383, Transition Loss -18.59442710876465, Classifier Loss 0.3725462257862091, Total Loss 145.82269287109375\n",
      "5: Encoding Loss 14.696044921875, Transition Loss -16.661148071289062, Classifier Loss 0.36483871936798096, Total Loss 154.0489044189453\n",
      "5: Encoding Loss 12.728517532348633, Transition Loss -15.592141151428223, Classifier Loss 0.37215691804885864, Total Loss 139.04071044921875\n",
      "5: Encoding Loss 12.957784652709961, Transition Loss -12.661370277404785, Classifier Loss 0.4004571735858917, Total Loss 143.70547485351562\n",
      "5: Encoding Loss 13.144600868225098, Transition Loss -12.833742141723633, Classifier Loss 0.3958524167537689, Total Loss 144.739501953125\n",
      "5: Encoding Loss 13.733624458312988, Transition Loss -14.820303916931152, Classifier Loss 0.40667903423309326, Total Loss 150.533935546875\n",
      "5: Encoding Loss 13.52100658416748, Transition Loss -10.548837661743164, Classifier Loss 0.3872087299823761, Total Loss 146.88682556152344\n",
      "5: Encoding Loss 13.14236068725586, Transition Loss -15.477365493774414, Classifier Loss 0.38174939155578613, Total Loss 143.31072998046875\n",
      "5: Encoding Loss 13.375283241271973, Transition Loss -16.78011703491211, Classifier Loss 0.37706735730171204, Total Loss 144.70565795898438\n",
      "5: Encoding Loss 12.99557876586914, Transition Loss -15.70396614074707, Classifier Loss 0.3806391656398773, Total Loss 142.025390625\n",
      "5: Encoding Loss 14.088834762573242, Transition Loss -10.761380195617676, Classifier Loss 0.4156761169433594, Total Loss 154.27613830566406\n",
      "5: Encoding Loss 13.627361297607422, Transition Loss -14.237546920776367, Classifier Loss 0.39789143204689026, Total Loss 148.80519104003906\n",
      "5: Encoding Loss 13.973458290100098, Transition Loss -16.420650482177734, Classifier Loss 0.4007439911365509, Total Loss 151.85879516601562\n",
      "5: Encoding Loss 12.58082103729248, Transition Loss -12.780794143676758, Classifier Loss 0.37429291009902954, Total Loss 138.07330322265625\n",
      "5: Encoding Loss 13.974596977233887, Transition Loss -14.878190994262695, Classifier Loss 0.38083818554878235, Total Loss 149.87762451171875\n",
      "5: Encoding Loss 14.346915245056152, Transition Loss -13.787315368652344, Classifier Loss 0.43802329897880554, Total Loss 158.57489013671875\n",
      "5: Encoding Loss 13.68709659576416, Transition Loss -17.557649612426758, Classifier Loss 0.3773445188999176, Total Loss 147.22772216796875\n",
      "5: Encoding Loss 13.242059707641602, Transition Loss -10.025090217590332, Classifier Loss 0.40559831261634827, Total Loss 146.4943084716797\n",
      "5: Encoding Loss 13.247424125671387, Transition Loss -13.499737739562988, Classifier Loss 0.3809452950954437, Total Loss 144.0712127685547\n",
      "5: Encoding Loss 14.222830772399902, Transition Loss -14.52394962310791, Classifier Loss 0.37631314992904663, Total Loss 151.4110565185547\n",
      "5: Encoding Loss 12.947460174560547, Transition Loss -17.762245178222656, Classifier Loss 0.36633744835853577, Total Loss 140.2098846435547\n",
      "5: Encoding Loss 11.951238632202148, Transition Loss -17.010353088378906, Classifier Loss 0.37860095500946045, Total Loss 133.46661376953125\n",
      "5: Encoding Loss 12.493000984191895, Transition Loss -22.04431915283203, Classifier Loss 0.34308889508247375, Total Loss 134.2484893798828\n",
      "5: Encoding Loss 12.317747116088867, Transition Loss -14.55813980102539, Classifier Loss 0.4088754653930664, Total Loss 139.42662048339844\n",
      "5: Encoding Loss 13.018019676208496, Transition Loss -15.335984230041504, Classifier Loss 0.38628819584846497, Total Loss 142.76991271972656\n",
      "5: Encoding Loss 13.235250473022461, Transition Loss -19.856721878051758, Classifier Loss 0.3679872155189514, Total Loss 142.6767578125\n",
      "5: Encoding Loss 14.128894805908203, Transition Loss -14.365595817565918, Classifier Loss 0.4173377454280853, Total Loss 154.76205444335938\n",
      "5: Encoding Loss 13.307287216186523, Transition Loss -18.914989471435547, Classifier Loss 0.37759944796562195, Total Loss 144.21446228027344\n",
      "5: Encoding Loss 12.743647575378418, Transition Loss -18.63117790222168, Classifier Loss 0.36029940843582153, Total Loss 137.97540283203125\n",
      "5: Encoding Loss 11.292523384094238, Transition Loss -20.15584373474121, Classifier Loss 0.35421037673950195, Total Loss 125.75719451904297\n",
      "5: Encoding Loss 13.859128952026367, Transition Loss -24.886472702026367, Classifier Loss 0.3216659128665924, Total Loss 143.03465270996094\n",
      "6: Encoding Loss 13.442313194274902, Transition Loss -10.950592041015625, Classifier Loss 0.38433051109313965, Total Loss 145.9693603515625\n",
      "6: Encoding Loss 14.51298713684082, Transition Loss -12.43497371673584, Classifier Loss 0.419354647397995, Total Loss 158.03688049316406\n",
      "6: Encoding Loss 15.415843963623047, Transition Loss -11.282708168029785, Classifier Loss 0.4304482638835907, Total Loss 166.36932373046875\n",
      "6: Encoding Loss 12.446474075317383, Transition Loss -19.203357696533203, Classifier Loss 0.3572211265563965, Total Loss 135.29006958007812\n",
      "6: Encoding Loss 13.455877304077148, Transition Loss -14.475650787353516, Classifier Loss 0.387589693069458, Total Loss 146.40309143066406\n",
      "6: Encoding Loss 12.635208129882812, Transition Loss -18.44255256652832, Classifier Loss 0.3650558590888977, Total Loss 137.58355712890625\n",
      "6: Encoding Loss 13.583938598632812, Transition Loss -17.16478157043457, Classifier Loss 0.40251365303993225, Total Loss 148.91944885253906\n",
      "6: Encoding Loss 12.105175971984863, Transition Loss -14.488569259643555, Classifier Loss 0.3727625906467438, Total Loss 134.11477661132812\n",
      "6: Encoding Loss 12.812480926513672, Transition Loss -18.171621322631836, Classifier Loss 0.34604349732398987, Total Loss 137.10057067871094\n",
      "6: Encoding Loss 12.369047164916992, Transition Loss -15.389684677124023, Classifier Loss 0.35377803444862366, Total Loss 134.3271026611328\n",
      "6: Encoding Loss 14.380910873413086, Transition Loss -14.581012725830078, Classifier Loss 0.41668015718460083, Total Loss 156.71238708496094\n",
      "6: Encoding Loss 12.000659942626953, Transition Loss -12.16478157043457, Classifier Loss 0.3887377977371216, Total Loss 134.8766326904297\n",
      "6: Encoding Loss 13.12501049041748, Transition Loss -12.853158950805664, Classifier Loss 0.38061344623565674, Total Loss 143.05886840820312\n",
      "6: Encoding Loss 12.62038803100586, Transition Loss -12.739459037780762, Classifier Loss 0.36727428436279297, Total Loss 137.68798828125\n",
      "6: Encoding Loss 13.377455711364746, Transition Loss -19.68853187561035, Classifier Loss 0.3687626123428345, Total Loss 143.8919677734375\n",
      "6: Encoding Loss 13.083955764770508, Transition Loss -21.908180236816406, Classifier Loss 0.3381304144859314, Total Loss 138.4803009033203\n",
      "6: Encoding Loss 14.646852493286133, Transition Loss -16.981731414794922, Classifier Loss 0.38309675455093384, Total Loss 155.48109436035156\n",
      "6: Encoding Loss 12.856152534484863, Transition Loss -12.777260780334473, Classifier Loss 0.36632609367370605, Total Loss 139.47927856445312\n",
      "6: Encoding Loss 12.232449531555176, Transition Loss -16.777219772338867, Classifier Loss 0.3486284017562866, Total Loss 132.71908569335938\n",
      "6: Encoding Loss 13.515145301818848, Transition Loss -14.209640502929688, Classifier Loss 0.39876800775527954, Total Loss 147.99513244628906\n",
      "6: Encoding Loss 12.14495849609375, Transition Loss -17.141931533813477, Classifier Loss 0.36415764689445496, Total Loss 133.57200622558594\n",
      "6: Encoding Loss 13.444604873657227, Transition Loss -16.6614933013916, Classifier Loss 0.3584359884262085, Total Loss 143.39710998535156\n",
      "6: Encoding Loss 14.312148094177246, Transition Loss -19.642948150634766, Classifier Loss 0.39431387186050415, Total Loss 153.92465209960938\n",
      "6: Encoding Loss 12.937557220458984, Transition Loss -13.117206573486328, Classifier Loss 0.3881851136684418, Total Loss 142.31634521484375\n",
      "6: Encoding Loss 12.92914867401123, Transition Loss -17.506549835205078, Classifier Loss 0.3719676434993744, Total Loss 140.62646484375\n",
      "6: Encoding Loss 12.697368621826172, Transition Loss -12.208357810974121, Classifier Loss 0.38722124695777893, Total Loss 140.2986297607422\n",
      "6: Encoding Loss 14.783888816833496, Transition Loss -14.195676803588867, Classifier Loss 0.42265328764915466, Total Loss 160.53359985351562\n",
      "6: Encoding Loss 14.044556617736816, Transition Loss -23.364376068115234, Classifier Loss 0.3685300052165985, Total Loss 149.2047882080078\n",
      "6: Encoding Loss 12.642830848693848, Transition Loss -15.427102088928223, Classifier Loss 0.35904645919799805, Total Loss 137.04421997070312\n",
      "6: Encoding Loss 13.19332504272461, Transition Loss -13.022565841674805, Classifier Loss 0.388063907623291, Total Loss 144.3503875732422\n",
      "6: Encoding Loss 12.92874813079834, Transition Loss -16.053951263427734, Classifier Loss 0.3711814284324646, Total Loss 140.544921875\n",
      "6: Encoding Loss 11.933272361755371, Transition Loss -11.357393264770508, Classifier Loss 0.3715943992137909, Total Loss 132.6233367919922\n",
      "6: Encoding Loss 12.461262702941895, Transition Loss -15.441482543945312, Classifier Loss 0.3413325548171997, Total Loss 133.82028198242188\n",
      "6: Encoding Loss 12.691943168640137, Transition Loss -14.661850929260254, Classifier Loss 0.39261865615844727, Total Loss 140.7944793701172\n",
      "6: Encoding Loss 12.260110855102539, Transition Loss -17.028751373291016, Classifier Loss 0.3393261432647705, Total Loss 132.01010131835938\n",
      "6: Encoding Loss 12.13296127319336, Transition Loss -14.036808013916016, Classifier Loss 0.33004137873649597, Total Loss 130.0650177001953\n",
      "6: Encoding Loss 13.181559562683105, Transition Loss -14.576668739318848, Classifier Loss 0.3799123466014862, Total Loss 143.4407958984375\n",
      "6: Encoding Loss 10.904065132141113, Transition Loss -13.896524429321289, Classifier Loss 0.35376718640327454, Total Loss 122.60646057128906\n",
      "6: Encoding Loss 14.103445053100586, Transition Loss -16.921382904052734, Classifier Loss 0.3587988317012787, Total Loss 148.70407104492188\n",
      "6: Encoding Loss 13.08373737335205, Transition Loss -16.153900146484375, Classifier Loss 0.3903510272502899, Total Loss 143.70176696777344\n",
      "6: Encoding Loss 12.381570816040039, Transition Loss -14.871726989746094, Classifier Loss 0.3320137858390808, Total Loss 132.25096130371094\n",
      "6: Encoding Loss 13.355263710021973, Transition Loss -20.255659103393555, Classifier Loss 0.36565491557121277, Total Loss 143.403564453125\n",
      "6: Encoding Loss 13.034473419189453, Transition Loss -20.43846893310547, Classifier Loss 0.35347533226013184, Total Loss 139.61923217773438\n",
      "6: Encoding Loss 13.258988380432129, Transition Loss -13.528327941894531, Classifier Loss 0.37851279973983765, Total Loss 143.9204864501953\n",
      "6: Encoding Loss 11.803370475769043, Transition Loss -20.184242248535156, Classifier Loss 0.32004407048225403, Total Loss 126.42733001708984\n",
      "6: Encoding Loss 13.08874225616455, Transition Loss -13.46376895904541, Classifier Loss 0.3914972245693207, Total Loss 143.8569793701172\n",
      "6: Encoding Loss 13.714542388916016, Transition Loss -20.587427139282227, Classifier Loss 0.3592214584350586, Total Loss 145.6343536376953\n",
      "6: Encoding Loss 11.467889785766602, Transition Loss -13.285966873168945, Classifier Loss 0.35303404927253723, Total Loss 127.04386901855469\n",
      "6: Encoding Loss 13.811115264892578, Transition Loss -17.27599334716797, Classifier Loss 0.39294376969337463, Total Loss 149.77984619140625\n",
      "6: Encoding Loss 12.753279685974121, Transition Loss -20.774620056152344, Classifier Loss 0.3577091693878174, Total Loss 137.79299926757812\n",
      "6: Encoding Loss 11.68330192565918, Transition Loss -21.70953369140625, Classifier Loss 0.31401050090789795, Total Loss 124.8631362915039\n",
      "6: Encoding Loss 13.307003021240234, Transition Loss -17.025957107543945, Classifier Loss 0.35778987407684326, Total Loss 142.2316131591797\n",
      "6: Encoding Loss 13.532760620117188, Transition Loss -13.389908790588379, Classifier Loss 0.40217453241348267, Total Loss 148.4768524169922\n",
      "6: Encoding Loss 12.676352500915527, Transition Loss -17.146638870239258, Classifier Loss 0.3370044529438019, Total Loss 135.1078338623047\n",
      "6: Encoding Loss 12.518567085266113, Transition Loss -16.95583152770996, Classifier Loss 0.3162557780742645, Total Loss 131.77072143554688\n",
      "6: Encoding Loss 12.525205612182617, Transition Loss -13.188916206359863, Classifier Loss 0.36843040585517883, Total Loss 137.04205322265625\n",
      "6: Encoding Loss 12.723528861999512, Transition Loss -16.3524112701416, Classifier Loss 0.3309767544269562, Total Loss 134.8826446533203\n",
      "6: Encoding Loss 13.716313362121582, Transition Loss -19.289039611816406, Classifier Loss 0.3542639911174774, Total Loss 145.15304565429688\n",
      "6: Encoding Loss 10.934748649597168, Transition Loss -21.32684898376465, Classifier Loss 0.3126535713672638, Total Loss 118.73908233642578\n",
      "6: Encoding Loss 13.838970184326172, Transition Loss -21.526412963867188, Classifier Loss 0.3611820340156555, Total Loss 146.82566833496094\n",
      "6: Encoding Loss 12.675488471984863, Transition Loss -16.07437515258789, Classifier Loss 0.35612213611602783, Total Loss 137.0128936767578\n",
      "6: Encoding Loss 12.759382247924805, Transition Loss -15.538125991821289, Classifier Loss 0.37258976697921753, Total Loss 139.3309326171875\n",
      "6: Encoding Loss 13.38868522644043, Transition Loss -19.105670928955078, Classifier Loss 0.3700679540634155, Total Loss 144.1124725341797\n",
      "6: Encoding Loss 11.635697364807129, Transition Loss -13.498908996582031, Classifier Loss 0.3537818193435669, Total Loss 128.4610595703125\n",
      "6: Encoding Loss 11.222859382629395, Transition Loss -16.89145278930664, Classifier Loss 0.33197253942489624, Total Loss 122.97675323486328\n",
      "6: Encoding Loss 12.668168067932129, Transition Loss -13.749382019042969, Classifier Loss 0.40764057636260986, Total Loss 142.10665893554688\n",
      "6: Encoding Loss 12.187472343444824, Transition Loss -11.30371379852295, Classifier Loss 0.3824290335178375, Total Loss 135.74041748046875\n",
      "6: Encoding Loss 13.553160667419434, Transition Loss -21.179706573486328, Classifier Loss 0.34173211455345154, Total Loss 142.59425354003906\n",
      "6: Encoding Loss 13.609030723571777, Transition Loss -16.478788375854492, Classifier Loss 0.3803175687789917, Total Loss 146.9007110595703\n",
      "6: Encoding Loss 13.187066078186035, Transition Loss -17.193540573120117, Classifier Loss 0.3844778537750244, Total Loss 143.94088745117188\n",
      "6: Encoding Loss 11.778528213500977, Transition Loss -15.993276596069336, Classifier Loss 0.350815087556839, Total Loss 129.30653381347656\n",
      "6: Encoding Loss 11.926447868347168, Transition Loss -15.300739288330078, Classifier Loss 0.348264217376709, Total Loss 130.2349395751953\n",
      "6: Encoding Loss 13.349514961242676, Transition Loss -14.135027885437012, Classifier Loss 0.3680022954940796, Total Loss 143.59352111816406\n",
      "6: Encoding Loss 14.160717010498047, Transition Loss -18.41543197631836, Classifier Loss 0.3537106513977051, Total Loss 148.6531219482422\n",
      "6: Encoding Loss 13.540838241577148, Transition Loss -15.772613525390625, Classifier Loss 0.3799848258495331, Total Loss 146.32203674316406\n",
      "6: Encoding Loss 12.16154670715332, Transition Loss -17.574865341186523, Classifier Loss 0.33460769057273865, Total Loss 130.7496337890625\n",
      "6: Encoding Loss 13.572668075561523, Transition Loss -16.579811096191406, Classifier Loss 0.36875662207603455, Total Loss 145.45370483398438\n",
      "6: Encoding Loss 13.704351425170898, Transition Loss -20.32756805419922, Classifier Loss 0.34887656569480896, Total Loss 144.51841735839844\n",
      "6: Encoding Loss 14.022345542907715, Transition Loss -20.746646881103516, Classifier Loss 0.32296791672706604, Total Loss 144.47140502929688\n",
      "6: Encoding Loss 12.837349891662598, Transition Loss -18.08011817932129, Classifier Loss 0.3860805332660675, Total Loss 141.30323791503906\n",
      "6: Encoding Loss 13.246076583862305, Transition Loss -17.659578323364258, Classifier Loss 0.346963495016098, Total Loss 140.6614532470703\n",
      "6: Encoding Loss 12.481477737426758, Transition Loss -15.772041320800781, Classifier Loss 0.34153103828430176, Total Loss 134.00177001953125\n",
      "6: Encoding Loss 13.221883773803711, Transition Loss -14.728365898132324, Classifier Loss 0.38622909784317017, Total Loss 144.39503479003906\n",
      "6: Encoding Loss 10.654282569885254, Transition Loss -14.096004486083984, Classifier Loss 0.3157438635826111, Total Loss 116.80582427978516\n",
      "6: Encoding Loss 13.640336990356445, Transition Loss -19.144638061523438, Classifier Loss 0.3505706489086151, Total Loss 144.17593383789062\n",
      "6: Encoding Loss 13.435343742370605, Transition Loss -15.958812713623047, Classifier Loss 0.3632963299751282, Total Loss 143.80918884277344\n",
      "6: Encoding Loss 12.402100563049316, Transition Loss -20.907272338867188, Classifier Loss 0.3271632492542267, Total Loss 131.928955078125\n",
      "6: Encoding Loss 14.470118522644043, Transition Loss -19.198436737060547, Classifier Loss 0.3562188148498535, Total Loss 151.3789825439453\n",
      "6: Encoding Loss 12.089592933654785, Transition Loss -15.196577072143555, Classifier Loss 0.33021798729896545, Total Loss 129.73550415039062\n",
      "6: Encoding Loss 12.563004493713379, Transition Loss -18.817293167114258, Classifier Loss 0.3357829451560974, Total Loss 134.0785675048828\n",
      "6: Encoding Loss 12.37856674194336, Transition Loss -25.44995880126953, Classifier Loss 0.30961281061172485, Total Loss 129.98471069335938\n",
      "6: Encoding Loss 10.495878219604492, Transition Loss -13.034370422363281, Classifier Loss 0.3306453824043274, Total Loss 117.02896118164062\n",
      "6: Encoding Loss 12.818831443786621, Transition Loss -17.278926849365234, Classifier Loss 0.3494689464569092, Total Loss 137.4940948486328\n",
      "6: Encoding Loss 12.515440940856934, Transition Loss -17.569589614868164, Classifier Loss 0.33202627301216125, Total Loss 133.32264709472656\n",
      "6: Encoding Loss 11.593244552612305, Transition Loss -13.305458068847656, Classifier Loss 0.34374380111694336, Total Loss 127.11768341064453\n",
      "6: Encoding Loss 13.176056861877441, Transition Loss -20.054218292236328, Classifier Loss 0.3354196846485138, Total Loss 138.94642639160156\n",
      "6: Encoding Loss 12.135863304138184, Transition Loss -21.008779525756836, Classifier Loss 0.311603307723999, Total Loss 128.2430419921875\n",
      "6: Encoding Loss 12.637137413024902, Transition Loss -20.722488403320312, Classifier Loss 0.30929702520370483, Total Loss 132.02264404296875\n",
      "6: Encoding Loss 13.01347827911377, Transition Loss -16.244718551635742, Classifier Loss 0.3384186029434204, Total Loss 137.94644165039062\n",
      "6: Encoding Loss 13.866044044494629, Transition Loss -17.613977432250977, Classifier Loss 0.3568149507045746, Total Loss 146.60633850097656\n",
      "6: Encoding Loss 13.972444534301758, Transition Loss -21.015172958374023, Classifier Loss 0.3457018733024597, Total Loss 146.34555053710938\n",
      "6: Encoding Loss 13.2762451171875, Transition Loss -12.690263748168945, Classifier Loss 0.3852589726448059, Total Loss 144.7333221435547\n",
      "6: Encoding Loss 13.400336265563965, Transition Loss -16.60670280456543, Classifier Loss 0.32860833406448364, Total Loss 140.06019592285156\n",
      "6: Encoding Loss 11.565975189208984, Transition Loss -13.621491432189941, Classifier Loss 0.36654725670814514, Total Loss 129.17979431152344\n",
      "6: Encoding Loss 10.783707618713379, Transition Loss -16.977216720581055, Classifier Loss 0.3237658441066742, Total Loss 118.6428451538086\n",
      "6: Encoding Loss 13.55780029296875, Transition Loss -23.30335807800293, Classifier Loss 0.3257021903991699, Total Loss 141.02796936035156\n",
      "6: Encoding Loss 12.249073028564453, Transition Loss -14.327035903930664, Classifier Loss 0.3453139066696167, Total Loss 132.52110290527344\n",
      "6: Encoding Loss 14.299215316772461, Transition Loss -19.700504302978516, Classifier Loss 0.3065023720264435, Total Loss 145.0400390625\n",
      "6: Encoding Loss 13.352217674255371, Transition Loss -18.158945083618164, Classifier Loss 0.32730865478515625, Total Loss 139.54498291015625\n",
      "6: Encoding Loss 12.172236442565918, Transition Loss -20.324243545532227, Classifier Loss 0.3215331733226776, Total Loss 129.5271453857422\n",
      "6: Encoding Loss 12.112811088562012, Transition Loss -19.320323944091797, Classifier Loss 0.31974637508392334, Total Loss 128.87326049804688\n",
      "6: Encoding Loss 12.782817840576172, Transition Loss -14.053227424621582, Classifier Loss 0.34800392389297485, Total Loss 137.06011962890625\n",
      "6: Encoding Loss 12.882342338562012, Transition Loss -23.286470413208008, Classifier Loss 0.3101942837238312, Total Loss 134.07351684570312\n",
      "6: Encoding Loss 12.760478019714355, Transition Loss -22.87545394897461, Classifier Loss 0.32181188464164734, Total Loss 134.26043701171875\n",
      "6: Encoding Loss 12.14389419555664, Transition Loss -12.89185905456543, Classifier Loss 0.35521966218948364, Total Loss 132.67054748535156\n",
      "6: Encoding Loss 14.310548782348633, Transition Loss -12.497410774230957, Classifier Loss 0.3729214668273926, Total Loss 151.77403259277344\n",
      "6: Encoding Loss 13.317244529724121, Transition Loss -20.546113967895508, Classifier Loss 0.31408047676086426, Total Loss 137.94189453125\n",
      "6: Encoding Loss 12.149555206298828, Transition Loss -20.176259994506836, Classifier Loss 0.3180341124534607, Total Loss 128.99581909179688\n",
      "6: Encoding Loss 13.974575996398926, Transition Loss -24.556360244750977, Classifier Loss 0.3555748164653778, Total Loss 147.34918212890625\n",
      "6: Encoding Loss 12.11602783203125, Transition Loss -16.478334426879883, Classifier Loss 0.3278619050979614, Total Loss 129.71112060546875\n",
      "6: Encoding Loss 13.694430351257324, Transition Loss -23.442712783813477, Classifier Loss 0.3075922727584839, Total Loss 140.3099822998047\n",
      "6: Encoding Loss 12.56005573272705, Transition Loss -19.070724487304688, Classifier Loss 0.3253575265407562, Total Loss 133.01239013671875\n",
      "6: Encoding Loss 13.424504280090332, Transition Loss -18.622364044189453, Classifier Loss 0.3306334912776947, Total Loss 140.45565795898438\n",
      "6: Encoding Loss 12.558985710144043, Transition Loss -25.184738159179688, Classifier Loss 0.2975682020187378, Total Loss 130.22366333007812\n",
      "6: Encoding Loss 14.387487411499023, Transition Loss -18.806161880493164, Classifier Loss 0.33412328362464905, Total Loss 148.50848388671875\n",
      "6: Encoding Loss 12.078116416931152, Transition Loss -18.272829055786133, Classifier Loss 0.340234637260437, Total Loss 130.6447296142578\n",
      "6: Encoding Loss 12.549026489257812, Transition Loss -23.13903045654297, Classifier Loss 0.3092458248138428, Total Loss 131.3121795654297\n",
      "6: Encoding Loss 13.709986686706543, Transition Loss -21.768754959106445, Classifier Loss 0.2942042350769043, Total Loss 139.09596252441406\n",
      "6: Encoding Loss 11.962274551391602, Transition Loss -18.481826782226562, Classifier Loss 0.30815955996513367, Total Loss 126.51045989990234\n",
      "6: Encoding Loss 12.342021942138672, Transition Loss -16.517337799072266, Classifier Loss 0.3319295048713684, Total Loss 131.9258270263672\n",
      "6: Encoding Loss 12.13591194152832, Transition Loss -16.962003707885742, Classifier Loss 0.32701000571250916, Total Loss 129.784912109375\n",
      "6: Encoding Loss 13.014975547790527, Transition Loss -19.497163772583008, Classifier Loss 0.3394443690776825, Total Loss 138.06033325195312\n",
      "6: Encoding Loss 12.47578239440918, Transition Loss -15.216745376586914, Classifier Loss 0.3108900189399719, Total Loss 130.89222717285156\n",
      "6: Encoding Loss 12.504207611083984, Transition Loss -19.808828353881836, Classifier Loss 0.3142131567001343, Total Loss 131.4510040283203\n",
      "6: Encoding Loss 12.738910675048828, Transition Loss -21.485950469970703, Classifier Loss 0.3082437515258789, Total Loss 132.73135375976562\n",
      "6: Encoding Loss 12.009173393249512, Transition Loss -20.2237606048584, Classifier Loss 0.31366240978240967, Total Loss 127.43558502197266\n",
      "6: Encoding Loss 13.593080520629883, Transition Loss -14.7904691696167, Classifier Loss 0.34933674335479736, Total Loss 143.67535400390625\n",
      "6: Encoding Loss 12.898387908935547, Transition Loss -18.384517669677734, Classifier Loss 0.3223799169063568, Total Loss 135.4214324951172\n",
      "6: Encoding Loss 13.001712799072266, Transition Loss -21.959571838378906, Classifier Loss 0.32943636178970337, Total Loss 136.95294189453125\n",
      "6: Encoding Loss 12.083044052124023, Transition Loss -16.075050354003906, Classifier Loss 0.30456337332725525, Total Loss 127.11748504638672\n",
      "6: Encoding Loss 13.177945137023926, Transition Loss -19.119169235229492, Classifier Loss 0.3042709231376648, Total Loss 135.84681701660156\n",
      "6: Encoding Loss 13.373373031616211, Transition Loss -19.702369689941406, Classifier Loss 0.36401867866516113, Total Loss 143.38491821289062\n",
      "6: Encoding Loss 13.237701416015625, Transition Loss -21.96087646484375, Classifier Loss 0.3005492091178894, Total Loss 135.95213317871094\n",
      "6: Encoding Loss 12.65235424041748, Transition Loss -14.399877548217773, Classifier Loss 0.33526140451431274, Total Loss 134.74209594726562\n",
      "6: Encoding Loss 12.663715362548828, Transition Loss -17.403593063354492, Classifier Loss 0.3065638542175293, Total Loss 131.96263122558594\n",
      "6: Encoding Loss 13.598554611206055, Transition Loss -18.907175064086914, Classifier Loss 0.3046570122241974, Total Loss 139.2503662109375\n",
      "6: Encoding Loss 11.94071102142334, Transition Loss -22.559682846069336, Classifier Loss 0.29781973361968994, Total Loss 125.30315399169922\n",
      "6: Encoding Loss 11.30407428741455, Transition Loss -20.286468505859375, Classifier Loss 0.3075951039791107, Total Loss 121.18804931640625\n",
      "6: Encoding Loss 11.798249244689941, Transition Loss -26.195404052734375, Classifier Loss 0.27679091691970825, Total Loss 122.05984497070312\n",
      "6: Encoding Loss 11.352591514587402, Transition Loss -18.546375274658203, Classifier Loss 0.3422948122024536, Total Loss 125.0465087890625\n",
      "6: Encoding Loss 12.092829704284668, Transition Loss -19.282575607299805, Classifier Loss 0.30881208181381226, Total Loss 127.6199951171875\n",
      "6: Encoding Loss 12.269868850708008, Transition Loss -24.412683486938477, Classifier Loss 0.2995230257511139, Total Loss 128.1063690185547\n",
      "6: Encoding Loss 13.447622299194336, Transition Loss -18.687511444091797, Classifier Loss 0.3413955271244049, Total Loss 141.716796875\n",
      "6: Encoding Loss 12.073104858398438, Transition Loss -23.90479278564453, Classifier Loss 0.3142196238040924, Total Loss 128.0020294189453\n",
      "6: Encoding Loss 11.9767484664917, Transition Loss -23.31067657470703, Classifier Loss 0.28806570172309875, Total Loss 124.61589813232422\n",
      "6: Encoding Loss 10.318984031677246, Transition Loss -24.6348819732666, Classifier Loss 0.2869552969932556, Total Loss 111.24247741699219\n",
      "6: Encoding Loss 13.450607299804688, Transition Loss -30.991682052612305, Classifier Loss 0.25014206767082214, Total Loss 132.6128692626953\n",
      "7: Encoding Loss 12.440881729125977, Transition Loss -15.727457046508789, Classifier Loss 0.3067891597747803, Total Loss 130.20281982421875\n",
      "7: Encoding Loss 14.024194717407227, Transition Loss -17.46832847595215, Classifier Loss 0.34499242901802063, Total Loss 146.68931579589844\n",
      "7: Encoding Loss 14.856152534484863, Transition Loss -16.35567855834961, Classifier Loss 0.3441583812236786, Total Loss 153.2617950439453\n",
      "7: Encoding Loss 11.773148536682129, Transition Loss -23.40780258178711, Classifier Loss 0.2903256416320801, Total Loss 123.21306610107422\n",
      "7: Encoding Loss 12.976001739501953, Transition Loss -18.71171760559082, Classifier Loss 0.3110877573490143, Total Loss 134.9130401611328\n",
      "7: Encoding Loss 11.813617706298828, Transition Loss -23.38063621520996, Classifier Loss 0.3007740378379822, Total Loss 124.58167266845703\n",
      "7: Encoding Loss 12.82623291015625, Transition Loss -22.445693969726562, Classifier Loss 0.3299005329608917, Total Loss 135.59542846679688\n",
      "7: Encoding Loss 11.296995162963867, Transition Loss -19.274822235107422, Classifier Loss 0.3037186563014984, Total Loss 120.74398040771484\n",
      "7: Encoding Loss 12.39283275604248, Transition Loss -22.376489639282227, Classifier Loss 0.27394092082977295, Total Loss 126.53227996826172\n",
      "7: Encoding Loss 11.73505687713623, Transition Loss -19.327180862426758, Classifier Loss 0.28261324763298035, Total Loss 122.13790893554688\n",
      "7: Encoding Loss 14.022899627685547, Transition Loss -18.731861114501953, Classifier Loss 0.3388035297393799, Total Loss 146.05979919433594\n",
      "7: Encoding Loss 11.171842575073242, Transition Loss -16.244680404663086, Classifier Loss 0.31225061416625977, Total Loss 120.5965576171875\n",
      "7: Encoding Loss 12.438162803649902, Transition Loss -17.0394229888916, Classifier Loss 0.30600178241729736, Total Loss 130.10208129882812\n",
      "7: Encoding Loss 12.249422073364258, Transition Loss -16.191965103149414, Classifier Loss 0.2962445616722107, Total Loss 127.6166000366211\n",
      "7: Encoding Loss 12.645040512084961, Transition Loss -24.35437774658203, Classifier Loss 0.2956167161464691, Total Loss 130.71713256835938\n",
      "7: Encoding Loss 12.315935134887695, Transition Loss -27.605831146240234, Classifier Loss 0.27114176750183105, Total Loss 125.63613891601562\n",
      "7: Encoding Loss 14.027239799499512, Transition Loss -22.281150817871094, Classifier Loss 0.29890772700309753, Total Loss 142.10423278808594\n",
      "7: Encoding Loss 12.378450393676758, Transition Loss -16.414669036865234, Classifier Loss 0.2900061011314392, Total Loss 128.02493286132812\n",
      "7: Encoding Loss 11.4822416305542, Transition Loss -21.049285888671875, Classifier Loss 0.28158020973205566, Total Loss 120.0117416381836\n",
      "7: Encoding Loss 12.798527717590332, Transition Loss -18.439071655273438, Classifier Loss 0.3211560845375061, Total Loss 134.50013732910156\n",
      "7: Encoding Loss 11.305402755737305, Transition Loss -21.931669235229492, Classifier Loss 0.29226624965667725, Total Loss 119.66546630859375\n",
      "7: Encoding Loss 12.769342422485352, Transition Loss -21.209228515625, Classifier Loss 0.27654024958610535, Total Loss 129.8045196533203\n",
      "7: Encoding Loss 13.4622802734375, Transition Loss -26.331226348876953, Classifier Loss 0.31950852274894714, Total Loss 139.64382934570312\n",
      "7: Encoding Loss 12.202805519104004, Transition Loss -18.360660552978516, Classifier Loss 0.3150186538696289, Total Loss 129.12063598632812\n",
      "7: Encoding Loss 12.27348518371582, Transition Loss -22.461233139038086, Classifier Loss 0.2969144880771637, Total Loss 127.87483978271484\n",
      "7: Encoding Loss 11.892739295959473, Transition Loss -16.677845001220703, Classifier Loss 0.30084991455078125, Total Loss 125.22357177734375\n",
      "7: Encoding Loss 14.356486320495605, Transition Loss -18.814773559570312, Classifier Loss 0.3407606780529022, Total Loss 148.9241943359375\n",
      "7: Encoding Loss 13.228668212890625, Transition Loss -29.795455932617188, Classifier Loss 0.29718703031539917, Total Loss 135.54208374023438\n",
      "7: Encoding Loss 11.827932357788086, Transition Loss -20.12015151977539, Classifier Loss 0.2883376181125641, Total Loss 123.45320892333984\n",
      "7: Encoding Loss 12.396879196166992, Transition Loss -17.605173110961914, Classifier Loss 0.30723631381988525, Total Loss 129.8951416015625\n",
      "7: Encoding Loss 12.183263778686523, Transition Loss -20.883840560913086, Classifier Loss 0.2957347333431244, Total Loss 127.03541564941406\n",
      "7: Encoding Loss 11.30866527557373, Transition Loss -15.053999900817871, Classifier Loss 0.2944410741329193, Total Loss 119.91041564941406\n",
      "7: Encoding Loss 12.051100730895996, Transition Loss -19.177242279052734, Classifier Loss 0.26720571517944336, Total Loss 123.12554168701172\n",
      "7: Encoding Loss 12.11732006072998, Transition Loss -18.707782745361328, Classifier Loss 0.33241134881973267, Total Loss 130.1759490966797\n",
      "7: Encoding Loss 11.784897804260254, Transition Loss -21.320722579956055, Classifier Loss 0.2688397467136383, Total Loss 121.15888977050781\n",
      "7: Encoding Loss 11.626009941101074, Transition Loss -18.196691513061523, Classifier Loss 0.25195127725601196, Total Loss 118.19956970214844\n",
      "7: Encoding Loss 12.418172836303711, Transition Loss -18.83226776123047, Classifier Loss 0.31342676281929016, Total Loss 130.68429565429688\n",
      "7: Encoding Loss 10.102519035339355, Transition Loss -18.512325286865234, Classifier Loss 0.28453052043914795, Total Loss 109.2695083618164\n",
      "7: Encoding Loss 13.396932601928711, Transition Loss -22.289844512939453, Classifier Loss 0.27877718210220337, Total Loss 135.04873657226562\n",
      "7: Encoding Loss 12.222965240478516, Transition Loss -21.02103042602539, Classifier Loss 0.31140947341918945, Total Loss 128.9204559326172\n",
      "7: Encoding Loss 11.766495704650879, Transition Loss -19.659704208374023, Classifier Loss 0.2564603090286255, Total Loss 119.7740707397461\n",
      "7: Encoding Loss 12.881434440612793, Transition Loss -25.57470703125, Classifier Loss 0.29057416319847107, Total Loss 132.10377502441406\n",
      "7: Encoding Loss 12.573527336120605, Transition Loss -25.083887100219727, Classifier Loss 0.27836915850639343, Total Loss 128.4201202392578\n",
      "7: Encoding Loss 12.897798538208008, Transition Loss -16.994285583496094, Classifier Loss 0.3025018274784088, Total Loss 133.42916870117188\n",
      "7: Encoding Loss 11.394583702087402, Transition Loss -24.59592056274414, Classifier Loss 0.2531310021877289, Total Loss 116.46485137939453\n",
      "7: Encoding Loss 12.247665405273438, Transition Loss -17.870485305786133, Classifier Loss 0.30460068583488464, Total Loss 128.4378204345703\n",
      "7: Encoding Loss 13.548894882202148, Transition Loss -24.788272857666016, Classifier Loss 0.2862548232078552, Total Loss 137.01168823242188\n",
      "7: Encoding Loss 10.878338813781738, Transition Loss -16.49766731262207, Classifier Loss 0.2725279927253723, Total Loss 114.27621459960938\n",
      "7: Encoding Loss 13.305254936218262, Transition Loss -21.573043823242188, Classifier Loss 0.3109438419342041, Total Loss 137.5321044921875\n",
      "7: Encoding Loss 12.207358360290527, Transition Loss -25.491214752197266, Classifier Loss 0.28471431136131287, Total Loss 126.12519836425781\n",
      "7: Encoding Loss 11.06081485748291, Transition Loss -27.049758911132812, Classifier Loss 0.24446696043014526, Total Loss 112.92780303955078\n",
      "7: Encoding Loss 12.68228530883789, Transition Loss -21.56818199157715, Classifier Loss 0.28055307269096375, Total Loss 129.50927734375\n",
      "7: Encoding Loss 12.786707878112793, Transition Loss -18.025691986083984, Classifier Loss 0.3175925016403198, Total Loss 134.04931640625\n",
      "7: Encoding Loss 12.088479995727539, Transition Loss -21.45003890991211, Classifier Loss 0.2622520923614502, Total Loss 122.92875671386719\n",
      "7: Encoding Loss 12.015089988708496, Transition Loss -21.264039993286133, Classifier Loss 0.24389536678791046, Total Loss 120.5060043334961\n",
      "7: Encoding Loss 12.012086868286133, Transition Loss -16.560611724853516, Classifier Loss 0.29575926065444946, Total Loss 125.6693115234375\n",
      "7: Encoding Loss 12.162772178649902, Transition Loss -20.147329330444336, Classifier Loss 0.25166094303131104, Total Loss 122.46424102783203\n",
      "7: Encoding Loss 13.20903491973877, Transition Loss -23.898500442504883, Classifier Loss 0.2818189263343811, Total Loss 133.84939575195312\n",
      "7: Encoding Loss 10.399561882019043, Transition Loss -25.6461124420166, Classifier Loss 0.2483471930027008, Total Loss 108.02608489990234\n",
      "7: Encoding Loss 13.088475227355957, Transition Loss -26.459854125976562, Classifier Loss 0.28118211030960083, Total Loss 132.8207244873047\n",
      "7: Encoding Loss 12.054669380187988, Transition Loss -20.294418334960938, Classifier Loss 0.27575933933258057, Total Loss 124.00923156738281\n",
      "7: Encoding Loss 12.120091438293457, Transition Loss -19.482385635375977, Classifier Loss 0.2951298952102661, Total Loss 126.4698257446289\n",
      "7: Encoding Loss 12.978177070617676, Transition Loss -23.615755081176758, Classifier Loss 0.29406028985977173, Total Loss 133.22671508789062\n",
      "7: Encoding Loss 11.003679275512695, Transition Loss -17.15865707397461, Classifier Loss 0.282469242811203, Total Loss 116.2729263305664\n",
      "7: Encoding Loss 10.711931228637695, Transition Loss -20.45794677734375, Classifier Loss 0.2684904932975769, Total Loss 112.5404052734375\n",
      "7: Encoding Loss 11.853153228759766, Transition Loss -18.0255126953125, Classifier Loss 0.3272155523300171, Total Loss 127.54317474365234\n",
      "7: Encoding Loss 11.441879272460938, Transition Loss -14.789592742919922, Classifier Loss 0.2966235280036926, Total Loss 121.19442749023438\n",
      "7: Encoding Loss 13.143798828125, Transition Loss -25.523725509643555, Classifier Loss 0.2746851444244385, Total Loss 132.61378479003906\n",
      "7: Encoding Loss 12.958383560180664, Transition Loss -20.635379791259766, Classifier Loss 0.29157379269599915, Total Loss 132.82032775878906\n",
      "7: Encoding Loss 12.671710014343262, Transition Loss -21.396366119384766, Classifier Loss 0.30665165185928345, Total Loss 132.03457641601562\n",
      "7: Encoding Loss 11.17164134979248, Transition Loss -19.815073013305664, Classifier Loss 0.2794351279735565, Total Loss 117.31268310546875\n",
      "7: Encoding Loss 11.380208969116211, Transition Loss -18.74338150024414, Classifier Loss 0.2769346237182617, Total Loss 118.73139190673828\n",
      "7: Encoding Loss 12.92564868927002, Transition Loss -17.427396774291992, Classifier Loss 0.29139408469200134, Total Loss 132.54112243652344\n",
      "7: Encoding Loss 13.668038368225098, Transition Loss -22.989334106445312, Classifier Loss 0.2654305696487427, Total Loss 135.88278198242188\n",
      "7: Encoding Loss 13.0076322555542, Transition Loss -19.310142517089844, Classifier Loss 0.3064408004283905, Total Loss 134.70127868652344\n",
      "7: Encoding Loss 11.560260772705078, Transition Loss -20.909120559692383, Classifier Loss 0.2563753128051758, Total Loss 118.11543273925781\n",
      "7: Encoding Loss 12.681670188903809, Transition Loss -20.391677856445312, Classifier Loss 0.28344598412513733, Total Loss 129.79388427734375\n",
      "7: Encoding Loss 13.197142601013184, Transition Loss -24.531455993652344, Classifier Loss 0.27219700813293457, Total Loss 132.79193115234375\n",
      "7: Encoding Loss 13.430191993713379, Transition Loss -25.05670166015625, Classifier Loss 0.23421861231327057, Total Loss 130.8583984375\n",
      "7: Encoding Loss 11.94163703918457, Transition Loss -21.43387794494629, Classifier Loss 0.30956190824508667, Total Loss 126.48500061035156\n",
      "7: Encoding Loss 12.671215057373047, Transition Loss -21.07965850830078, Classifier Loss 0.26398536562919617, Total Loss 127.7640380859375\n",
      "7: Encoding Loss 11.772616386413574, Transition Loss -18.569171905517578, Classifier Loss 0.2561592757701874, Total Loss 119.79314422607422\n",
      "7: Encoding Loss 12.409236907958984, Transition Loss -18.392047882080078, Classifier Loss 0.29086822271347046, Total Loss 128.35704040527344\n",
      "7: Encoding Loss 10.231508255004883, Transition Loss -15.792640686035156, Classifier Loss 0.2555713951587677, Total Loss 107.40605163574219\n",
      "7: Encoding Loss 13.045553207397461, Transition Loss -23.46337890625, Classifier Loss 0.2706676721572876, Total Loss 131.42649841308594\n",
      "7: Encoding Loss 12.935155868530273, Transition Loss -19.840105056762695, Classifier Loss 0.2765553295612335, Total Loss 131.1328125\n",
      "7: Encoding Loss 11.780953407287598, Transition Loss -24.92328643798828, Classifier Loss 0.2519204616546631, Total Loss 119.4346923828125\n",
      "7: Encoding Loss 13.792838096618652, Transition Loss -22.89373207092285, Classifier Loss 0.2760922908782959, Total Loss 137.94735717773438\n",
      "7: Encoding Loss 11.55283260345459, Transition Loss -18.342626571655273, Classifier Loss 0.24535970389842987, Total Loss 116.95496368408203\n",
      "7: Encoding Loss 12.176268577575684, Transition Loss -21.85185432434082, Classifier Loss 0.2533473074436188, Total Loss 122.74050903320312\n",
      "7: Encoding Loss 11.921921730041504, Transition Loss -29.45498275756836, Classifier Loss 0.24401524662971497, Total Loss 119.77101135253906\n",
      "7: Encoding Loss 9.925337791442871, Transition Loss -14.954020500183105, Classifier Loss 0.26049116253852844, Total Loss 105.4488296508789\n",
      "7: Encoding Loss 11.995542526245117, Transition Loss -20.660472869873047, Classifier Loss 0.25629767775535583, Total Loss 121.58998107910156\n",
      "7: Encoding Loss 11.70908260345459, Transition Loss -21.568737030029297, Classifier Loss 0.24705612659454346, Total Loss 118.37396240234375\n",
      "7: Encoding Loss 10.934948921203613, Transition Loss -15.824380874633789, Classifier Loss 0.26297613978385925, Total Loss 113.77404022216797\n",
      "7: Encoding Loss 12.652484893798828, Transition Loss -23.149730682373047, Classifier Loss 0.2592068314552307, Total Loss 127.13593292236328\n",
      "7: Encoding Loss 11.454574584960938, Transition Loss -24.47613525390625, Classifier Loss 0.23834823071956635, Total Loss 115.46652221679688\n",
      "7: Encoding Loss 11.82555103302002, Transition Loss -24.385841369628906, Classifier Loss 0.22443203628063202, Total Loss 117.04273223876953\n",
      "7: Encoding Loss 12.273588180541992, Transition Loss -18.972278594970703, Classifier Loss 0.23694607615470886, Total Loss 121.87952423095703\n",
      "7: Encoding Loss 13.353185653686523, Transition Loss -20.418493270874023, Classifier Loss 0.2780191898345947, Total Loss 134.62332153320312\n",
      "7: Encoding Loss 13.490970611572266, Transition Loss -23.56475830078125, Classifier Loss 0.26732730865478516, Total Loss 134.65577697753906\n",
      "7: Encoding Loss 12.532485961914062, Transition Loss -14.877340316772461, Classifier Loss 0.2882919907569885, Total Loss 129.0861053466797\n",
      "7: Encoding Loss 12.752304077148438, Transition Loss -19.425762176513672, Classifier Loss 0.23495589196681976, Total Loss 125.51013946533203\n",
      "7: Encoding Loss 10.769440650939941, Transition Loss -15.299428939819336, Classifier Loss 0.2750384509563446, Total Loss 113.65631103515625\n",
      "7: Encoding Loss 10.148055076599121, Transition Loss -18.79060935974121, Classifier Loss 0.25012925267219543, Total Loss 106.193603515625\n",
      "7: Encoding Loss 13.00640869140625, Transition Loss -26.645950317382812, Classifier Loss 0.24522823095321655, Total Loss 128.5687713623047\n",
      "7: Encoding Loss 11.626463890075684, Transition Loss -16.078319549560547, Classifier Loss 0.2680560350418091, Total Loss 119.81410217285156\n",
      "7: Encoding Loss 13.595660209655762, Transition Loss -22.1127872467041, Classifier Loss 0.2118108719587326, Total Loss 129.9419403076172\n",
      "7: Encoding Loss 12.677188873291016, Transition Loss -20.2071533203125, Classifier Loss 0.24340328574180603, Total Loss 125.75379943847656\n",
      "7: Encoding Loss 11.41909122467041, Transition Loss -22.84260368347168, Classifier Loss 0.24154327809810638, Total Loss 115.50248718261719\n",
      "7: Encoding Loss 11.612882614135742, Transition Loss -21.821767807006836, Classifier Loss 0.24685019254684448, Total Loss 117.58372497558594\n",
      "7: Encoding Loss 12.166876792907715, Transition Loss -15.474284172058105, Classifier Loss 0.25517553091049194, Total Loss 122.84947204589844\n",
      "7: Encoding Loss 12.037904739379883, Transition Loss -26.399978637695312, Classifier Loss 0.21662157773971558, Total Loss 117.96011352539062\n",
      "7: Encoding Loss 12.153070449829102, Transition Loss -25.827190399169922, Classifier Loss 0.23990461230278015, Total Loss 121.2098617553711\n",
      "7: Encoding Loss 11.380069732666016, Transition Loss -14.298126220703125, Classifier Loss 0.25243887305259705, Total Loss 116.28158569335938\n",
      "7: Encoding Loss 13.858006477355957, Transition Loss -13.496810913085938, Classifier Loss 0.2724490463733673, Total Loss 138.10626220703125\n",
      "7: Encoding Loss 12.458684921264648, Transition Loss -22.592533111572266, Classifier Loss 0.2211202085018158, Total Loss 121.77699279785156\n",
      "7: Encoding Loss 11.511707305908203, Transition Loss -22.083669662475586, Classifier Loss 0.23874245584011078, Total Loss 115.96348571777344\n",
      "7: Encoding Loss 13.020207405090332, Transition Loss -27.197872161865234, Classifier Loss 0.25718554854393005, Total Loss 129.87478637695312\n",
      "7: Encoding Loss 11.226374626159668, Transition Loss -17.48349952697754, Classifier Loss 0.23318977653980255, Total Loss 113.12648010253906\n",
      "7: Encoding Loss 12.991072654724121, Transition Loss -26.45893096923828, Classifier Loss 0.22315561771392822, Total Loss 126.23884582519531\n",
      "7: Encoding Loss 11.51458740234375, Transition Loss -20.61774444580078, Classifier Loss 0.24001774191856384, Total Loss 116.11434936523438\n",
      "7: Encoding Loss 12.88762092590332, Transition Loss -20.021413803100586, Classifier Loss 0.25649523735046387, Total Loss 128.74649047851562\n",
      "7: Encoding Loss 11.858697891235352, Transition Loss -27.641450881958008, Classifier Loss 0.22999414801597595, Total Loss 117.86346435546875\n",
      "7: Encoding Loss 13.45666217803955, Transition Loss -20.92735481262207, Classifier Loss 0.23772406578063965, Total Loss 131.42152404785156\n",
      "7: Encoding Loss 11.251788139343262, Transition Loss -20.00464630126953, Classifier Loss 0.2574404180049896, Total Loss 115.75434875488281\n",
      "7: Encoding Loss 11.832620620727539, Transition Loss -25.19113540649414, Classifier Loss 0.2381146401166916, Total Loss 118.46739196777344\n",
      "7: Encoding Loss 12.834855079650879, Transition Loss -24.399215698242188, Classifier Loss 0.19634605944156647, Total Loss 122.3085708618164\n",
      "7: Encoding Loss 11.332881927490234, Transition Loss -19.448514938354492, Classifier Loss 0.2352847307920456, Total Loss 114.18763732910156\n",
      "7: Encoding Loss 11.666163444519043, Transition Loss -17.71293067932129, Classifier Loss 0.24307014048099518, Total Loss 117.63278198242188\n",
      "7: Encoding Loss 11.277585983276367, Transition Loss -16.883949279785156, Classifier Loss 0.23617884516716003, Total Loss 113.83519744873047\n",
      "7: Encoding Loss 12.261366844177246, Transition Loss -20.481584548950195, Classifier Loss 0.2547186017036438, Total Loss 123.55870056152344\n",
      "7: Encoding Loss 11.479348182678223, Transition Loss -16.274375915527344, Classifier Loss 0.19993151724338531, Total Loss 111.8246841430664\n",
      "7: Encoding Loss 11.737594604492188, Transition Loss -20.655685424804688, Classifier Loss 0.21975421905517578, Total Loss 115.8720474243164\n",
      "7: Encoding Loss 12.048872947692871, Transition Loss -23.086782455444336, Classifier Loss 0.22223760187625885, Total Loss 118.6101303100586\n",
      "7: Encoding Loss 11.230600357055664, Transition Loss -21.847299575805664, Classifier Loss 0.2348526269197464, Total Loss 113.32569122314453\n",
      "7: Encoding Loss 12.914412498474121, Transition Loss -14.854360580444336, Classifier Loss 0.25883299112319946, Total Loss 129.19561767578125\n",
      "7: Encoding Loss 12.148028373718262, Transition Loss -18.913175582885742, Classifier Loss 0.22980712354183197, Total Loss 120.1611557006836\n",
      "7: Encoding Loss 12.108779907226562, Transition Loss -22.737260818481445, Classifier Loss 0.2440173327922821, Total Loss 121.26742553710938\n",
      "7: Encoding Loss 11.520893096923828, Transition Loss -17.165401458740234, Classifier Loss 0.22992795705795288, Total Loss 115.15650939941406\n",
      "7: Encoding Loss 12.264252662658691, Transition Loss -20.042057037353516, Classifier Loss 0.2117888629436493, Total Loss 119.28890228271484\n",
      "7: Encoding Loss 12.306305885314941, Transition Loss -19.463794708251953, Classifier Loss 0.2651239335536957, Total Loss 124.95895385742188\n",
      "7: Encoding Loss 12.610864639282227, Transition Loss -22.39316749572754, Classifier Loss 0.22929875552654266, Total Loss 123.81231689453125\n",
      "7: Encoding Loss 11.852327346801758, Transition Loss -14.013626098632812, Classifier Loss 0.23251831531524658, Total Loss 118.0676498413086\n",
      "7: Encoding Loss 11.90178394317627, Transition Loss -17.833011627197266, Classifier Loss 0.2127017229795456, Total Loss 116.48088073730469\n",
      "7: Encoding Loss 12.786127090454102, Transition Loss -18.889413833618164, Classifier Loss 0.21536093950271606, Total Loss 123.82133483886719\n",
      "7: Encoding Loss 11.033781051635742, Transition Loss -22.972549438476562, Classifier Loss 0.22708120942115784, Total Loss 110.97378540039062\n",
      "7: Encoding Loss 10.779240608215332, Transition Loss -20.46031379699707, Classifier Loss 0.25039973855018616, Total Loss 111.26980590820312\n",
      "7: Encoding Loss 11.118216514587402, Transition Loss -27.674793243408203, Classifier Loss 0.21587149798870087, Total Loss 110.52735137939453\n",
      "7: Encoding Loss 10.425646781921387, Transition Loss -18.663606643676758, Classifier Loss 0.2625455856323242, Total Loss 109.65599822998047\n",
      "7: Encoding Loss 11.21340274810791, Transition Loss -19.385149002075195, Classifier Loss 0.22379979491233826, Total Loss 112.08332824707031\n",
      "7: Encoding Loss 11.391706466674805, Transition Loss -26.013275146484375, Classifier Loss 0.22389420866966248, Total Loss 113.51787567138672\n",
      "7: Encoding Loss 12.434354782104492, Transition Loss -19.084693908691406, Classifier Loss 0.24395130574703217, Total Loss 123.86616516113281\n",
      "7: Encoding Loss 10.899468421936035, Transition Loss -24.31376075744629, Classifier Loss 0.23011581599712372, Total Loss 110.20246887207031\n",
      "7: Encoding Loss 11.02283000946045, Transition Loss -24.018592834472656, Classifier Loss 0.20464836061000824, Total Loss 108.64266967773438\n",
      "7: Encoding Loss 9.432328224182129, Transition Loss -25.053428649902344, Classifier Loss 0.2287442982196808, Total Loss 98.32804107666016\n",
      "7: Encoding Loss 12.829488754272461, Transition Loss -33.67888641357422, Classifier Loss 0.18851110339164734, Total Loss 121.48029327392578\n",
      "8: Encoding Loss 11.310898780822754, Transition Loss -16.67529296875, Classifier Loss 0.19609379768371582, Total Loss 110.0932388305664\n",
      "8: Encoding Loss 13.13437557220459, Transition Loss -17.57137680053711, Classifier Loss 0.25143149495124817, Total Loss 130.2146453857422\n",
      "8: Encoding Loss 13.780152320861816, Transition Loss -14.911443710327148, Classifier Loss 0.22405216097831726, Total Loss 132.64346313476562\n",
      "8: Encoding Loss 10.901609420776367, Transition Loss -23.89125633239746, Classifier Loss 0.22631406784057617, Total Loss 109.83951568603516\n",
      "8: Encoding Loss 12.160371780395508, Transition Loss -18.85460662841797, Classifier Loss 0.2243950515985489, Total Loss 119.71871185302734\n",
      "8: Encoding Loss 10.870409965515137, Transition Loss -23.5756893157959, Classifier Loss 0.2301286906003952, Total Loss 109.971435546875\n",
      "8: Encoding Loss 11.847844123840332, Transition Loss -22.02065086364746, Classifier Loss 0.2411293089389801, Total Loss 118.89128112792969\n",
      "8: Encoding Loss 10.2194185256958, Transition Loss -19.24343490600586, Classifier Loss 0.21129031479358673, Total Loss 102.88053894042969\n",
      "8: Encoding Loss 11.709308624267578, Transition Loss -23.213911056518555, Classifier Loss 0.20450058579444885, Total Loss 114.11988067626953\n",
      "8: Encoding Loss 10.970006942749023, Transition Loss -19.608203887939453, Classifier Loss 0.21831952035427094, Total Loss 109.58808898925781\n",
      "8: Encoding Loss 13.170670509338379, Transition Loss -18.58345603942871, Classifier Loss 0.2472129464149475, Total Loss 130.08294677734375\n",
      "8: Encoding Loss 10.134679794311523, Transition Loss -14.832319259643555, Classifier Loss 0.2269725799560547, Total Loss 103.77173614501953\n",
      "8: Encoding Loss 11.368903160095215, Transition Loss -16.432180404663086, Classifier Loss 0.21198305487632751, Total Loss 112.146240234375\n",
      "8: Encoding Loss 11.420832633972168, Transition Loss -15.439794540405273, Classifier Loss 0.217189222574234, Total Loss 113.0824966430664\n",
      "8: Encoding Loss 11.44973087310791, Transition Loss -23.58981704711914, Classifier Loss 0.20542828738689423, Total Loss 112.1359634399414\n",
      "8: Encoding Loss 11.37423324584961, Transition Loss -28.64299964904785, Classifier Loss 0.20091679692268372, Total Loss 111.07981872558594\n",
      "8: Encoding Loss 12.89552116394043, Transition Loss -22.669885635375977, Classifier Loss 0.19902855157852173, Total Loss 123.0625\n",
      "8: Encoding Loss 11.613611221313477, Transition Loss -15.701805114746094, Classifier Loss 0.20862434804439545, Total Loss 113.76818084716797\n",
      "8: Encoding Loss 10.62726879119873, Transition Loss -20.334672927856445, Classifier Loss 0.2275092899799347, Total Loss 107.7650146484375\n",
      "8: Encoding Loss 11.650654792785645, Transition Loss -17.504941940307617, Classifier Loss 0.22137576341629028, Total Loss 115.33931732177734\n",
      "8: Encoding Loss 10.207999229431152, Transition Loss -22.209426879882812, Classifier Loss 0.22627437114715576, Total Loss 104.2869873046875\n",
      "8: Encoding Loss 11.534225463867188, Transition Loss -20.61260986328125, Classifier Loss 0.18619254231452942, Total Loss 110.8889389038086\n",
      "8: Encoding Loss 12.245728492736816, Transition Loss -26.8098087310791, Classifier Loss 0.22517862915992737, Total Loss 120.47832489013672\n",
      "8: Encoding Loss 10.957291603088379, Transition Loss -17.506959915161133, Classifier Loss 0.22057855129241943, Total Loss 109.71268463134766\n",
      "8: Encoding Loss 11.063323020935059, Transition Loss -20.98955535888672, Classifier Loss 0.21819370985031128, Total Loss 110.32176208496094\n",
      "8: Encoding Loss 10.563506126403809, Transition Loss -15.24384593963623, Classifier Loss 0.20886898040771484, Total Loss 105.39189147949219\n",
      "8: Encoding Loss 13.288267135620117, Transition Loss -16.156869888305664, Classifier Loss 0.24612325429916382, Total Loss 130.9152374267578\n",
      "8: Encoding Loss 11.953508377075195, Transition Loss -29.390138626098633, Classifier Loss 0.22328248620033264, Total Loss 117.950439453125\n",
      "8: Encoding Loss 10.636006355285645, Transition Loss -19.466732025146484, Classifier Loss 0.2039049118757248, Total Loss 105.47465515136719\n",
      "8: Encoding Loss 10.992419242858887, Transition Loss -16.02396011352539, Classifier Loss 0.2092348039150238, Total Loss 108.85963439941406\n",
      "8: Encoding Loss 10.99665355682373, Transition Loss -19.67778968811035, Classifier Loss 0.21730473637580872, Total Loss 109.69976806640625\n",
      "8: Encoding Loss 10.38428783416748, Transition Loss -12.467106819152832, Classifier Loss 0.22563588619232178, Total Loss 105.6353988647461\n",
      "8: Encoding Loss 11.194123268127441, Transition Loss -18.91748809814453, Classifier Loss 0.20063138008117676, Total Loss 109.61233520507812\n",
      "8: Encoding Loss 11.162229537963867, Transition Loss -17.964515686035156, Classifier Loss 0.2631176710128784, Total Loss 115.60601806640625\n",
      "8: Encoding Loss 10.76784610748291, Transition Loss -20.955781936645508, Classifier Loss 0.20296534895896912, Total Loss 106.43511199951172\n",
      "8: Encoding Loss 10.471954345703125, Transition Loss -16.553068161010742, Classifier Loss 0.17194008827209473, Total Loss 100.9663314819336\n",
      "8: Encoding Loss 11.298327445983887, Transition Loss -17.440608978271484, Classifier Loss 0.22266773879528046, Total Loss 112.64990997314453\n",
      "8: Encoding Loss 8.983954429626465, Transition Loss -17.214319229125977, Classifier Loss 0.21672679483890533, Total Loss 93.5408706665039\n",
      "8: Encoding Loss 11.93614387512207, Transition Loss -20.96654510498047, Classifier Loss 0.18209980428218842, Total Loss 113.69493103027344\n",
      "8: Encoding Loss 10.767804145812988, Transition Loss -18.687374114990234, Classifier Loss 0.24179288744926453, Total Loss 110.31798553466797\n",
      "8: Encoding Loss 10.371500968933105, Transition Loss -17.529539108276367, Classifier Loss 0.17305180430412292, Total Loss 100.273681640625\n",
      "8: Encoding Loss 11.753873825073242, Transition Loss -24.337478637695312, Classifier Loss 0.22131143510341644, Total Loss 116.15727233886719\n",
      "8: Encoding Loss 11.274969100952148, Transition Loss -23.368154525756836, Classifier Loss 0.2108772248029709, Total Loss 111.28280639648438\n",
      "8: Encoding Loss 11.787282943725586, Transition Loss -14.457876205444336, Classifier Loss 0.22549954056739807, Total Loss 116.84532928466797\n",
      "8: Encoding Loss 10.456488609313965, Transition Loss -23.515886306762695, Classifier Loss 0.20066778361797333, Total Loss 103.7139892578125\n",
      "8: Encoding Loss 10.812079429626465, Transition Loss -15.37077522277832, Classifier Loss 0.21421775221824646, Total Loss 107.91533660888672\n",
      "8: Encoding Loss 12.907184600830078, Transition Loss -24.272274017333984, Classifier Loss 0.24191561341285706, Total Loss 127.44417572021484\n",
      "8: Encoding Loss 9.611482620239258, Transition Loss -14.484928131103516, Classifier Loss 0.19009806215763092, Total Loss 95.89876556396484\n",
      "8: Encoding Loss 12.119820594787598, Transition Loss -19.425073623657227, Classifier Loss 0.22852863371372223, Total Loss 119.80754852294922\n",
      "8: Encoding Loss 10.941080093383789, Transition Loss -23.88877296447754, Classifier Loss 0.21928921341896057, Total Loss 109.45278930664062\n",
      "8: Encoding Loss 9.77877426147461, Transition Loss -27.263166427612305, Classifier Loss 0.18218842148780823, Total Loss 96.4435806274414\n",
      "8: Encoding Loss 11.43630313873291, Transition Loss -21.841663360595703, Classifier Loss 0.20296545326709747, Total Loss 111.78260040283203\n",
      "8: Encoding Loss 11.043848991394043, Transition Loss -15.230027198791504, Classifier Loss 0.20407246053218842, Total Loss 108.75499725341797\n",
      "8: Encoding Loss 10.940057754516602, Transition Loss -19.725547790527344, Classifier Loss 0.19267646968364716, Total Loss 106.78416442871094\n",
      "8: Encoding Loss 10.81850528717041, Transition Loss -19.916154861450195, Classifier Loss 0.17580413818359375, Total Loss 104.12447357177734\n",
      "8: Encoding Loss 10.734415054321289, Transition Loss -13.062911987304688, Classifier Loss 0.23262877762317657, Total Loss 109.13558959960938\n",
      "8: Encoding Loss 10.915814399719238, Transition Loss -18.041465759277344, Classifier Loss 0.16959825158119202, Total Loss 104.28273010253906\n",
      "8: Encoding Loss 11.935647010803223, Transition Loss -22.383651733398438, Classifier Loss 0.2060137689113617, Total Loss 116.08207702636719\n",
      "8: Encoding Loss 9.26093578338623, Transition Loss -23.250219345092773, Classifier Loss 0.21172688901424408, Total Loss 95.25553131103516\n",
      "8: Encoding Loss 11.541397094726562, Transition Loss -25.190292358398438, Classifier Loss 0.2025345414876938, Total Loss 112.57959747314453\n",
      "8: Encoding Loss 10.719712257385254, Transition Loss -18.358116149902344, Classifier Loss 0.1885385513305664, Total Loss 104.6078872680664\n",
      "8: Encoding Loss 10.789933204650879, Transition Loss -16.783557891845703, Classifier Loss 0.22259041666984558, Total Loss 108.57514953613281\n",
      "8: Encoding Loss 11.688187599182129, Transition Loss -20.928943634033203, Classifier Loss 0.20641405880451202, Total Loss 114.14271545410156\n",
      "8: Encoding Loss 9.63399887084961, Transition Loss -14.046772956848145, Classifier Loss 0.2196045070886612, Total Loss 99.02963256835938\n",
      "8: Encoding Loss 9.445955276489258, Transition Loss -18.54747200012207, Classifier Loss 0.20656132698059082, Total Loss 96.22006225585938\n",
      "8: Encoding Loss 10.243860244750977, Transition Loss -14.744211196899414, Classifier Loss 0.2433914840221405, Total Loss 106.28707885742188\n",
      "8: Encoding Loss 9.63471508026123, Transition Loss -9.271406173706055, Classifier Loss 0.2061956822872162, Total Loss 97.6954345703125\n",
      "8: Encoding Loss 11.99514389038086, Transition Loss -25.19979476928711, Classifier Loss 0.2162807136774063, Total Loss 117.58417510986328\n",
      "8: Encoding Loss 11.318989753723145, Transition Loss -17.610132217407227, Classifier Loss 0.20835231244564056, Total Loss 111.38362121582031\n",
      "8: Encoding Loss 11.253021240234375, Transition Loss -17.95155906677246, Classifier Loss 0.22870995104312897, Total Loss 112.89157104492188\n",
      "8: Encoding Loss 9.825685501098633, Transition Loss -16.43086051940918, Classifier Loss 0.2154988795518875, Total Loss 100.15208435058594\n",
      "8: Encoding Loss 9.851597785949707, Transition Loss -16.000341415405273, Classifier Loss 0.20671969652175903, Total Loss 99.48155975341797\n",
      "8: Encoding Loss 11.86849308013916, Transition Loss -13.981162071228027, Classifier Loss 0.21498370170593262, Total Loss 116.44351959228516\n",
      "8: Encoding Loss 11.883587837219238, Transition Loss -19.843202590942383, Classifier Loss 0.16604851186275482, Total Loss 111.66958618164062\n",
      "8: Encoding Loss 11.6455078125, Transition Loss -15.287700653076172, Classifier Loss 0.23837414383888245, Total Loss 116.99842071533203\n",
      "8: Encoding Loss 9.889031410217285, Transition Loss -17.89739418029785, Classifier Loss 0.19112741947174072, Total Loss 98.2214126586914\n",
      "8: Encoding Loss 10.656160354614258, Transition Loss -15.392576217651367, Classifier Loss 0.1911790519952774, Total Loss 104.36410522460938\n",
      "8: Encoding Loss 11.62634563446045, Transition Loss -22.412879943847656, Classifier Loss 0.2012958526611328, Total Loss 113.1358642578125\n",
      "8: Encoding Loss 11.606066703796387, Transition Loss -22.36777687072754, Classifier Loss 0.14573103189468384, Total Loss 107.41716766357422\n",
      "8: Encoding Loss 10.114127159118652, Transition Loss -15.52267837524414, Classifier Loss 0.24129143357276917, Total Loss 105.03905487060547\n",
      "8: Encoding Loss 11.222173690795898, Transition Loss -18.05931282043457, Classifier Loss 0.19951429963111877, Total Loss 109.7252197265625\n",
      "8: Encoding Loss 10.110841751098633, Transition Loss -14.654801368713379, Classifier Loss 0.18374782800674438, Total Loss 99.25859069824219\n",
      "8: Encoding Loss 10.583815574645996, Transition Loss -14.446727752685547, Classifier Loss 0.20384928584098816, Total Loss 105.05255889892578\n",
      "8: Encoding Loss 9.469676971435547, Transition Loss -13.438383102416992, Classifier Loss 0.2353459894657135, Total Loss 99.2893295288086\n",
      "8: Encoding Loss 11.48608684539795, Transition Loss -20.6735782623291, Classifier Loss 0.1875435709953308, Total Loss 110.638916015625\n",
      "8: Encoding Loss 11.066205978393555, Transition Loss -16.227859497070312, Classifier Loss 0.18969646096229553, Total Loss 107.49605560302734\n",
      "8: Encoding Loss 10.156777381896973, Transition Loss -22.827312469482422, Classifier Loss 0.19110414385795593, Total Loss 100.36006927490234\n",
      "8: Encoding Loss 11.732006072998047, Transition Loss -19.764204025268555, Classifier Loss 0.18523116409778595, Total Loss 112.37521362304688\n",
      "8: Encoding Loss 9.796740531921387, Transition Loss -13.681333541870117, Classifier Loss 0.1797380894422531, Total Loss 96.3449935913086\n",
      "8: Encoding Loss 10.91384506225586, Transition Loss -19.307626724243164, Classifier Loss 0.20170649886131287, Total Loss 107.47755432128906\n",
      "8: Encoding Loss 10.427072525024414, Transition Loss -28.20208168029785, Classifier Loss 0.19782611727714539, Total Loss 103.19355010986328\n",
      "8: Encoding Loss 8.697710990905762, Transition Loss -12.30678653717041, Classifier Loss 0.21623331308364868, Total Loss 91.20255279541016\n",
      "8: Encoding Loss 9.919740676879883, Transition Loss -16.59905242919922, Classifier Loss 0.17509213089942932, Total Loss 96.86382293701172\n",
      "8: Encoding Loss 9.88467025756836, Transition Loss -17.75244140625, Classifier Loss 0.17847593128681183, Total Loss 96.9214096069336\n",
      "8: Encoding Loss 9.213431358337402, Transition Loss -10.336034774780273, Classifier Loss 0.20808909833431244, Total Loss 94.51428985595703\n",
      "8: Encoding Loss 10.976170539855957, Transition Loss -19.970157623291016, Classifier Loss 0.20397423207759857, Total Loss 108.20278930664062\n",
      "8: Encoding Loss 9.694391250610352, Transition Loss -21.611557006835938, Classifier Loss 0.18559114634990692, Total Loss 96.10991668701172\n",
      "8: Encoding Loss 9.28900146484375, Transition Loss -20.09461212158203, Classifier Loss 0.15800844132900238, Total Loss 90.10883331298828\n",
      "8: Encoding Loss 9.9291353225708, Transition Loss -11.55160903930664, Classifier Loss 0.1554049700498581, Total Loss 94.97126770019531\n",
      "8: Encoding Loss 11.80321979522705, Transition Loss -16.168668746948242, Classifier Loss 0.2106972187757492, Total Loss 115.49224853515625\n",
      "8: Encoding Loss 11.801287651062012, Transition Loss -20.17763328552246, Classifier Loss 0.20379549264907837, Total Loss 114.78581237792969\n",
      "8: Encoding Loss 10.541043281555176, Transition Loss -7.532825469970703, Classifier Loss 0.20873600244522095, Total Loss 105.20044708251953\n",
      "8: Encoding Loss 10.734574317932129, Transition Loss -14.405567169189453, Classifier Loss 0.16743510961532593, Total Loss 102.61721801757812\n",
      "8: Encoding Loss 8.879593849182129, Transition Loss -8.955098152160645, Classifier Loss 0.2120514214038849, Total Loss 92.24010467529297\n",
      "8: Encoding Loss 8.314111709594727, Transition Loss -14.719598770141602, Classifier Loss 0.20284724235534668, Total Loss 86.794677734375\n",
      "8: Encoding Loss 11.072815895080566, Transition Loss -22.34880256652832, Classifier Loss 0.20058727264404297, Total Loss 108.63677978515625\n",
      "8: Encoding Loss 9.750268936157227, Transition Loss -9.634214401245117, Classifier Loss 0.22729237377643585, Total Loss 100.72945404052734\n",
      "8: Encoding Loss 10.836991310119629, Transition Loss -15.238197326660156, Classifier Loss 0.14623035490512848, Total Loss 101.31592559814453\n",
      "8: Encoding Loss 10.53167724609375, Transition Loss -14.195749282836914, Classifier Loss 0.1710835099220276, Total Loss 101.35893249511719\n",
      "8: Encoding Loss 9.28191089630127, Transition Loss -17.628679275512695, Classifier Loss 0.19217664003372192, Total Loss 93.46942901611328\n",
      "8: Encoding Loss 10.033305168151855, Transition Loss -18.904287338256836, Classifier Loss 0.18904340267181396, Total Loss 99.16699981689453\n",
      "8: Encoding Loss 9.850266456604004, Transition Loss -8.452106475830078, Classifier Loss 0.18909329175949097, Total Loss 97.70976257324219\n",
      "8: Encoding Loss 9.56338882446289, Transition Loss -22.67816925048828, Classifier Loss 0.14410077035427094, Total Loss 90.91265869140625\n",
      "8: Encoding Loss 10.366108894348145, Transition Loss -22.940391540527344, Classifier Loss 0.18564899265766144, Total Loss 101.48918914794922\n",
      "8: Encoding Loss 9.394116401672363, Transition Loss -6.568085193634033, Classifier Loss 0.16970054805278778, Total Loss 92.12167358398438\n",
      "8: Encoding Loss 12.007092475891113, Transition Loss -5.338981628417969, Classifier Loss 0.20169171690940857, Total Loss 116.22484588623047\n",
      "8: Encoding Loss 10.070560455322266, Transition Loss -16.347532272338867, Classifier Loss 0.15337395668029785, Total Loss 95.89860534667969\n",
      "8: Encoding Loss 9.254351615905762, Transition Loss -16.574237823486328, Classifier Loss 0.18723586201667786, Total Loss 92.75508880615234\n",
      "8: Encoding Loss 10.158011436462402, Transition Loss -19.469396591186523, Classifier Loss 0.18613401055335999, Total Loss 99.87360382080078\n",
      "8: Encoding Loss 8.674500465393066, Transition Loss -9.808253288269043, Classifier Loss 0.17707332968711853, Total Loss 87.10137176513672\n",
      "8: Encoding Loss 10.492504119873047, Transition Loss -22.23143196105957, Classifier Loss 0.1725786030292511, Total Loss 101.19344329833984\n",
      "8: Encoding Loss 8.795202255249023, Transition Loss -11.879781723022461, Classifier Loss 0.19487224519252777, Total Loss 89.84647369384766\n",
      "8: Encoding Loss 11.204923629760742, Transition Loss -16.607511520385742, Classifier Loss 0.21592578291893005, Total Loss 111.2286605834961\n",
      "8: Encoding Loss 9.371969223022461, Transition Loss -24.669206619262695, Classifier Loss 0.1950846016407013, Total Loss 94.47928619384766\n",
      "8: Encoding Loss 10.963540077209473, Transition Loss -15.777678489685059, Classifier Loss 0.17527996003627777, Total Loss 105.233154296875\n",
      "8: Encoding Loss 9.339713096618652, Transition Loss -13.541940689086914, Classifier Loss 0.22202813625335693, Total Loss 96.91780853271484\n",
      "8: Encoding Loss 10.114130973815918, Transition Loss -20.83452033996582, Classifier Loss 0.20046204328536987, Total Loss 100.95508575439453\n",
      "8: Encoding Loss 9.640375137329102, Transition Loss -19.208389282226562, Classifier Loss 0.13197027146816254, Total Loss 90.3161849975586\n",
      "8: Encoding Loss 9.481836318969727, Transition Loss -14.939852714538574, Classifier Loss 0.1863621473312378, Total Loss 94.4879150390625\n",
      "8: Encoding Loss 9.352632522583008, Transition Loss -10.19188117980957, Classifier Loss 0.18013139069080353, Total Loss 92.83216094970703\n",
      "8: Encoding Loss 8.818215370178223, Transition Loss -9.719179153442383, Classifier Loss 0.19296015799045563, Total Loss 89.83979034423828\n",
      "8: Encoding Loss 10.158312797546387, Transition Loss -14.18191146850586, Classifier Loss 0.19925017654895782, Total Loss 101.18868255615234\n",
      "8: Encoding Loss 8.623149871826172, Transition Loss -8.661693572998047, Classifier Loss 0.13770458102226257, Total Loss 82.7539291381836\n",
      "8: Encoding Loss 8.973811149597168, Transition Loss -14.505498886108398, Classifier Loss 0.15150821208953857, Total Loss 86.93841552734375\n",
      "8: Encoding Loss 9.684447288513184, Transition Loss -18.002965927124023, Classifier Loss 0.17249402403831482, Total Loss 94.72138214111328\n",
      "8: Encoding Loss 9.262956619262695, Transition Loss -18.12068748474121, Classifier Loss 0.1853373646736145, Total Loss 92.6337661743164\n",
      "8: Encoding Loss 10.480826377868652, Transition Loss -5.857900619506836, Classifier Loss 0.2182023525238037, Total Loss 105.6656723022461\n",
      "8: Encoding Loss 9.877866744995117, Transition Loss -10.109989166259766, Classifier Loss 0.18450413644313812, Total Loss 97.4713363647461\n",
      "8: Encoding Loss 9.471117973327637, Transition Loss -15.890990257263184, Classifier Loss 0.19917485117912292, Total Loss 95.6832504272461\n",
      "8: Encoding Loss 9.631182670593262, Transition Loss -13.399698257446289, Classifier Loss 0.1897868812084198, Total Loss 96.02547454833984\n",
      "8: Encoding Loss 9.322515487670898, Transition Loss -13.110617637634277, Classifier Loss 0.1564447283744812, Total Loss 90.22197723388672\n",
      "8: Encoding Loss 9.612679481506348, Transition Loss -7.976656913757324, Classifier Loss 0.2206951081752777, Total Loss 98.96935272216797\n",
      "8: Encoding Loss 10.328939437866211, Transition Loss -17.861141204833984, Classifier Loss 0.1851339042186737, Total Loss 101.14134216308594\n",
      "8: Encoding Loss 9.452265739440918, Transition Loss -5.447543144226074, Classifier Loss 0.17126069962978363, Total Loss 92.74310302734375\n",
      "8: Encoding Loss 9.086807250976562, Transition Loss -10.581930160522461, Classifier Loss 0.16559307277202606, Total Loss 89.25165557861328\n",
      "8: Encoding Loss 9.825241088867188, Transition Loss -10.489873886108398, Classifier Loss 0.17827153205871582, Total Loss 96.4269790649414\n",
      "8: Encoding Loss 8.015470504760742, Transition Loss -17.95416831970215, Classifier Loss 0.1976384073495865, Total Loss 83.88401794433594\n",
      "8: Encoding Loss 9.108437538146973, Transition Loss -17.368215560913086, Classifier Loss 0.23942752182483673, Total Loss 96.8067855834961\n",
      "8: Encoding Loss 8.692757606506348, Transition Loss -27.6629638671875, Classifier Loss 0.17816738784313202, Total Loss 87.35326385498047\n",
      "8: Encoding Loss 8.087957382202148, Transition Loss -14.112271308898926, Classifier Loss 0.22526410222053528, Total Loss 87.22725677490234\n",
      "8: Encoding Loss 8.789422035217285, Transition Loss -14.566630363464355, Classifier Loss 0.19501571357250214, Total Loss 89.81403350830078\n",
      "8: Encoding Loss 8.712614059448242, Transition Loss -24.911571502685547, Classifier Loss 0.17733360826969147, Total Loss 87.4292984008789\n",
      "8: Encoding Loss 9.465333938598633, Transition Loss -11.824013710021973, Classifier Loss 0.1896134614944458, Total Loss 94.68165588378906\n",
      "8: Encoding Loss 8.072548866271973, Transition Loss -18.78960609436035, Classifier Loss 0.18399831652641296, Total Loss 82.9764633178711\n",
      "8: Encoding Loss 7.995886325836182, Transition Loss -21.444913864135742, Classifier Loss 0.1518665999174118, Total Loss 79.14945983886719\n",
      "8: Encoding Loss 6.854546070098877, Transition Loss -21.731557846069336, Classifier Loss 0.21084348857402802, Total Loss 75.91637420654297\n",
      "8: Encoding Loss 10.93211841583252, Transition Loss -34.65671920776367, Classifier Loss 0.14542129635810852, Total Loss 101.99214172363281\n",
      "9: Encoding Loss 8.17074966430664, Transition Loss -9.267330169677734, Classifier Loss 0.1228971779346466, Total Loss 77.65386199951172\n",
      "9: Encoding Loss 10.500833511352539, Transition Loss -9.660224914550781, Classifier Loss 0.20366764068603516, Total Loss 104.37149810791016\n",
      "9: Encoding Loss 10.005407333374023, Transition Loss -3.0986335277557373, Classifier Loss 0.16520698368549347, Total Loss 96.56334686279297\n",
      "9: Encoding Loss 8.122434616088867, Transition Loss -20.787506103515625, Classifier Loss 0.1919333040714264, Total Loss 84.16865539550781\n",
      "9: Encoding Loss 9.639458656311035, Transition Loss -14.342214584350586, Classifier Loss 0.16882555186748505, Total Loss 93.99535369873047\n",
      "9: Encoding Loss 8.057599067687988, Transition Loss -17.964385986328125, Classifier Loss 0.20705047249794006, Total Loss 85.16224670410156\n",
      "9: Encoding Loss 9.155537605285645, Transition Loss -16.233179092407227, Classifier Loss 0.1895856410264969, Total Loss 92.19961547851562\n",
      "9: Encoding Loss 7.29351806640625, Transition Loss -11.764819145202637, Classifier Loss 0.1633549928665161, Total Loss 74.68128967285156\n",
      "9: Encoding Loss 9.134607315063477, Transition Loss -21.83353042602539, Classifier Loss 0.16625329852104187, Total Loss 89.69782257080078\n",
      "9: Encoding Loss 8.624552726745605, Transition Loss -15.09945297241211, Classifier Loss 0.19361519813537598, Total Loss 88.35491943359375\n",
      "9: Encoding Loss 10.301523208618164, Transition Loss -12.962526321411133, Classifier Loss 0.19797059893608093, Total Loss 102.20664978027344\n",
      "9: Encoding Loss 7.653996467590332, Transition Loss -4.20112943649292, Classifier Loss 0.19931860268115997, Total Loss 81.16299438476562\n",
      "9: Encoding Loss 8.345429420471191, Transition Loss -10.68728256225586, Classifier Loss 0.15660177171230316, Total Loss 82.42147827148438\n",
      "9: Encoding Loss 8.621540069580078, Transition Loss -9.010064125061035, Classifier Loss 0.18127837777137756, Total Loss 87.09835815429688\n",
      "9: Encoding Loss 7.728729248046875, Transition Loss -18.135112762451172, Classifier Loss 0.15997833013534546, Total Loss 77.82404327392578\n",
      "9: Encoding Loss 8.614851951599121, Transition Loss -25.783409118652344, Classifier Loss 0.15831366181373596, Total Loss 84.74502563476562\n",
      "9: Encoding Loss 9.353960037231445, Transition Loss -15.381284713745117, Classifier Loss 0.1504882276058197, Total Loss 89.87742614746094\n",
      "9: Encoding Loss 9.05897045135498, Transition Loss -9.555004119873047, Classifier Loss 0.17395268380641937, Total Loss 89.86512756347656\n",
      "9: Encoding Loss 8.498429298400879, Transition Loss -14.00784683227539, Classifier Loss 0.23202601075172424, Total Loss 91.1872329711914\n",
      "9: Encoding Loss 8.696533203125, Transition Loss -10.455476760864258, Classifier Loss 0.16280712187290192, Total Loss 85.85089111328125\n",
      "9: Encoding Loss 7.351832389831543, Transition Loss -17.77066421508789, Classifier Loss 0.21102102100849152, Total Loss 79.9132080078125\n",
      "9: Encoding Loss 8.021086692810059, Transition Loss -14.179693222045898, Classifier Loss 0.15354862809181213, Total Loss 79.52072143554688\n",
      "9: Encoding Loss 9.002030372619629, Transition Loss -21.129106521606445, Classifier Loss 0.17405027151107788, Total Loss 89.41704559326172\n",
      "9: Encoding Loss 7.414038181304932, Transition Loss -7.354070663452148, Classifier Loss 0.18816420435905457, Total Loss 78.12725067138672\n",
      "9: Encoding Loss 8.046563148498535, Transition Loss -13.777342796325684, Classifier Loss 0.18533049523830414, Total Loss 82.90280151367188\n",
      "9: Encoding Loss 7.373364448547363, Transition Loss -6.547369480133057, Classifier Loss 0.1772127002477646, Total Loss 76.70687103271484\n",
      "9: Encoding Loss 10.141263961791992, Transition Loss -5.786169528961182, Classifier Loss 0.20763394236564636, Total Loss 101.89234924316406\n",
      "9: Encoding Loss 8.449889183044434, Transition Loss -24.495223999023438, Classifier Loss 0.19148099422454834, Total Loss 86.74231719970703\n",
      "9: Encoding Loss 7.6972575187683105, Transition Loss -14.51728343963623, Classifier Loss 0.16078495979309082, Total Loss 77.65364837646484\n",
      "9: Encoding Loss 7.7251362800598145, Transition Loss -6.206326484680176, Classifier Loss 0.16979826986789703, Total Loss 78.77967834472656\n",
      "9: Encoding Loss 8.17430591583252, Transition Loss -14.310577392578125, Classifier Loss 0.18854261934757233, Total Loss 84.245849609375\n",
      "9: Encoding Loss 8.135045051574707, Transition Loss -3.323582649230957, Classifier Loss 0.2231045365333557, Total Loss 87.39015197753906\n",
      "9: Encoding Loss 8.742612838745117, Transition Loss -15.505892753601074, Classifier Loss 0.17519164085388184, Total Loss 87.45697784423828\n",
      "9: Encoding Loss 9.023478507995605, Transition Loss -11.170252799987793, Classifier Loss 0.24046485126018524, Total Loss 96.2320785522461\n",
      "9: Encoding Loss 7.874321460723877, Transition Loss -17.568790435791016, Classifier Loss 0.17246174812316895, Total Loss 80.23723602294922\n",
      "9: Encoding Loss 7.14482307434082, Transition Loss -9.02152156829834, Classifier Loss 0.13997073471546173, Total Loss 71.15385437011719\n",
      "9: Encoding Loss 8.836811065673828, Transition Loss -10.766589164733887, Classifier Loss 0.18784767389297485, Total Loss 89.47710418701172\n",
      "9: Encoding Loss 6.520840644836426, Transition Loss -9.999712944030762, Classifier Loss 0.20322108268737793, Total Loss 72.48683166503906\n",
      "9: Encoding Loss 8.211987495422363, Transition Loss -12.630938529968262, Classifier Loss 0.1490745097398758, Total Loss 80.60082244873047\n",
      "9: Encoding Loss 7.660504341125488, Transition Loss -11.99045181274414, Classifier Loss 0.22651055455207825, Total Loss 83.93269348144531\n",
      "9: Encoding Loss 6.668013095855713, Transition Loss -10.490594863891602, Classifier Loss 0.14432555437088013, Total Loss 67.7745590209961\n",
      "9: Encoding Loss 8.970568656921387, Transition Loss -19.635112762451172, Classifier Loss 0.1892509162425995, Total Loss 90.68570709228516\n",
      "9: Encoding Loss 7.69068717956543, Transition Loss -19.156295776367188, Classifier Loss 0.18259258568286896, Total Loss 79.78092956542969\n",
      "9: Encoding Loss 8.59329605102539, Transition Loss -6.523922920227051, Classifier Loss 0.19681456685066223, Total Loss 88.42652130126953\n",
      "9: Encoding Loss 8.09992790222168, Transition Loss -19.698448181152344, Classifier Loss 0.17730778455734253, Total Loss 82.52627563476562\n",
      "9: Encoding Loss 7.726669788360596, Transition Loss -7.1207275390625, Classifier Loss 0.18742772936820984, Total Loss 80.5547103881836\n",
      "9: Encoding Loss 10.692743301391602, Transition Loss -22.639686584472656, Classifier Loss 0.22716332972049713, Total Loss 108.25375366210938\n",
      "9: Encoding Loss 6.286875247955322, Transition Loss -7.442825794219971, Classifier Loss 0.15926234424114227, Total Loss 66.2197494506836\n",
      "9: Encoding Loss 9.392293930053711, Transition Loss -13.20793342590332, Classifier Loss 0.19161278009414673, Total Loss 94.2969970703125\n",
      "9: Encoding Loss 7.876656532287598, Transition Loss -19.958925247192383, Classifier Loss 0.18933223187923431, Total Loss 81.9424819946289\n",
      "9: Encoding Loss 6.805018901824951, Transition Loss -25.525461196899414, Classifier Loss 0.15257318317890167, Total Loss 69.69236755371094\n",
      "9: Encoding Loss 8.120111465454102, Transition Loss -18.106496810913086, Classifier Loss 0.17425692081451416, Total Loss 82.3829574584961\n",
      "9: Encoding Loss 6.678858280181885, Transition Loss -3.2184696197509766, Classifier Loss 0.16446170210838318, Total Loss 69.87639617919922\n",
      "9: Encoding Loss 8.0640230178833, Transition Loss -14.400646209716797, Classifier Loss 0.16916239261627197, Total Loss 81.425537109375\n",
      "9: Encoding Loss 7.649057865142822, Transition Loss -14.653087615966797, Classifier Loss 0.15024538338184357, Total Loss 76.21407318115234\n",
      "9: Encoding Loss 7.877392768859863, Transition Loss -2.7529892921447754, Classifier Loss 0.2276870757341385, Total Loss 85.78730010986328\n",
      "9: Encoding Loss 7.665137767791748, Transition Loss -12.54458236694336, Classifier Loss 0.1388804018497467, Total Loss 75.20663452148438\n",
      "9: Encoding Loss 8.944693565368652, Transition Loss -16.304677963256836, Classifier Loss 0.17491689324378967, Total Loss 89.04598236083984\n",
      "9: Encoding Loss 6.731037616729736, Transition Loss -19.636812210083008, Classifier Loss 0.20822231471538544, Total Loss 74.6666030883789\n",
      "9: Encoding Loss 8.12172794342041, Transition Loss -20.04363250732422, Classifier Loss 0.16968244314193726, Total Loss 81.93806457519531\n",
      "9: Encoding Loss 7.620641708374023, Transition Loss -11.171204566955566, Classifier Loss 0.14997850358486176, Total Loss 75.96075439453125\n",
      "9: Encoding Loss 8.027994155883789, Transition Loss -8.302033424377441, Classifier Loss 0.21068912744522095, Total Loss 85.29119873046875\n",
      "9: Encoding Loss 8.219232559204102, Transition Loss -13.801252365112305, Classifier Loss 0.1685701161623001, Total Loss 82.60810852050781\n",
      "9: Encoding Loss 6.728543758392334, Transition Loss -6.461557388305664, Classifier Loss 0.20222854614257812, Total Loss 74.04991912841797\n",
      "9: Encoding Loss 6.551412582397461, Transition Loss -12.863829612731934, Classifier Loss 0.18460753560066223, Total Loss 70.8694839477539\n",
      "9: Encoding Loss 7.156057357788086, Transition Loss -4.013404846191406, Classifier Loss 0.22521138191223145, Total Loss 79.768798828125\n",
      "9: Encoding Loss 6.251716613769531, Transition Loss 4.352685928344727, Classifier Loss 0.18617913126945496, Total Loss 69.50218200683594\n",
      "9: Encoding Loss 9.194177627563477, Transition Loss -25.020610809326172, Classifier Loss 0.18754926323890686, Total Loss 92.3033447265625\n",
      "9: Encoding Loss 7.944371223449707, Transition Loss -8.533026695251465, Classifier Loss 0.1827876716852188, Total Loss 81.83203125\n",
      "9: Encoding Loss 8.647625923156738, Transition Loss -10.12234878540039, Classifier Loss 0.19931095838546753, Total Loss 89.1100845336914\n",
      "9: Encoding Loss 7.027595043182373, Transition Loss -8.662538528442383, Classifier Loss 0.19828510284423828, Total Loss 76.04753875732422\n",
      "9: Encoding Loss 6.585258483886719, Transition Loss -9.183371543884277, Classifier Loss 0.17948906123638153, Total Loss 70.62913513183594\n",
      "9: Encoding Loss 9.557546615600586, Transition Loss -5.912336349487305, Classifier Loss 0.18856093287467957, Total Loss 95.31529235839844\n",
      "9: Encoding Loss 7.44974946975708, Transition Loss -11.406739234924316, Classifier Loss 0.12591031193733215, Total Loss 72.18675231933594\n",
      "9: Encoding Loss 8.73980712890625, Transition Loss -7.574254512786865, Classifier Loss 0.22107526659965515, Total Loss 92.02446746826172\n",
      "9: Encoding Loss 6.496988296508789, Transition Loss -13.477762222290039, Classifier Loss 0.16359490156173706, Total Loss 68.33269500732422\n",
      "9: Encoding Loss 6.941158771514893, Transition Loss -3.9640464782714844, Classifier Loss 0.1627035140991211, Total Loss 71.798828125\n",
      "9: Encoding Loss 8.53925609588623, Transition Loss -16.943323135375977, Classifier Loss 0.1687564104795456, Total Loss 85.1863021850586\n",
      "9: Encoding Loss 7.73958158493042, Transition Loss -15.120834350585938, Classifier Loss 0.11006291210651398, Total Loss 72.919921875\n",
      "9: Encoding Loss 7.121933937072754, Transition Loss -5.4012556076049805, Classifier Loss 0.22509121894836426, Total Loss 79.48350524902344\n",
      "9: Encoding Loss 8.245343208312988, Transition Loss -13.009936332702637, Classifier Loss 0.17705339193344116, Total Loss 83.66548156738281\n",
      "9: Encoding Loss 7.160446643829346, Transition Loss -6.995604515075684, Classifier Loss 0.15677863359451294, Total Loss 72.96004486083984\n",
      "9: Encoding Loss 7.48267936706543, Transition Loss -5.777566432952881, Classifier Loss 0.1751820147037506, Total Loss 77.37848663330078\n",
      "9: Encoding Loss 8.004372596740723, Transition Loss -11.55355453491211, Classifier Loss 0.23576489090919495, Total Loss 87.6091537475586\n",
      "9: Encoding Loss 8.368776321411133, Transition Loss -14.070984840393066, Classifier Loss 0.14853757619857788, Total Loss 81.80115509033203\n",
      "9: Encoding Loss 7.284517288208008, Transition Loss -5.348238945007324, Classifier Loss 0.15942150354385376, Total Loss 74.21721649169922\n",
      "9: Encoding Loss 7.403812885284424, Transition Loss -19.30592155456543, Classifier Loss 0.16744208335876465, Total Loss 75.97085571289062\n",
      "9: Encoding Loss 7.226995468139648, Transition Loss -11.036871910095215, Classifier Loss 0.15446631610393524, Total Loss 73.26039123535156\n",
      "9: Encoding Loss 6.637486457824707, Transition Loss -5.302545547485352, Classifier Loss 0.16053789854049683, Total Loss 69.15261840820312\n",
      "9: Encoding Loss 8.461881637573242, Transition Loss -14.344705581665039, Classifier Loss 0.18183425068855286, Total Loss 85.87561798095703\n",
      "9: Encoding Loss 7.589231014251709, Transition Loss -25.96016502380371, Classifier Loss 0.1727866232395172, Total Loss 77.98731231689453\n",
      "9: Encoding Loss 6.589758396148682, Transition Loss -5.979499816894531, Classifier Loss 0.20913082361221313, Total Loss 73.62995147705078\n",
      "9: Encoding Loss 6.61322021484375, Transition Loss -5.982454299926758, Classifier Loss 0.1500273048877716, Total Loss 67.90729522705078\n",
      "9: Encoding Loss 7.206558704376221, Transition Loss -10.619902610778809, Classifier Loss 0.15610191226005554, Total Loss 73.26054382324219\n",
      "9: Encoding Loss 6.587006092071533, Transition Loss 1.1977319717407227, Classifier Loss 0.1967434585094452, Total Loss 72.60994720458984\n",
      "9: Encoding Loss 7.748840808868408, Transition Loss -14.920836448669434, Classifier Loss 0.18181975185871124, Total Loss 80.16972351074219\n",
      "9: Encoding Loss 6.871735572814941, Transition Loss -16.768951416015625, Classifier Loss 0.16547399759292603, Total Loss 71.51792907714844\n",
      "9: Encoding Loss 5.124135971069336, Transition Loss -10.169445991516113, Classifier Loss 0.13825947046279907, Total Loss 54.81700134277344\n",
      "9: Encoding Loss 6.219038009643555, Transition Loss 1.0583600997924805, Classifier Loss 0.1333005279302597, Total Loss 63.294029235839844\n",
      "9: Encoding Loss 9.051626205444336, Transition Loss -9.494379997253418, Classifier Loss 0.18878647685050964, Total Loss 91.28976440429688\n",
      "9: Encoding Loss 8.324820518493652, Transition Loss -15.708590507507324, Classifier Loss 0.1770978569984436, Total Loss 84.30520629882812\n",
      "9: Encoding Loss 7.381483554840088, Transition Loss 4.344601154327393, Classifier Loss 0.1931179165840149, Total Loss 79.23258209228516\n",
      "9: Encoding Loss 7.4633002281188965, Transition Loss -8.1837158203125, Classifier Loss 0.14701896905899048, Total Loss 74.40665435791016\n",
      "9: Encoding Loss 6.547407150268555, Transition Loss -0.5462029576301575, Classifier Loss 0.1912718117237091, Total Loss 71.50633239746094\n",
      "9: Encoding Loss 5.626563549041748, Transition Loss -11.492704391479492, Classifier Loss 0.17557284235954285, Total Loss 62.5674934387207\n",
      "9: Encoding Loss 8.003849983215332, Transition Loss -16.645668029785156, Classifier Loss 0.18196962773799896, Total Loss 82.22443389892578\n",
      "9: Encoding Loss 6.755403518676758, Transition Loss -1.7109295129776, Classifier Loss 0.21787285804748535, Total Loss 75.83016204833984\n",
      "9: Encoding Loss 6.040456771850586, Transition Loss -7.096199989318848, Classifier Loss 0.12400786578655243, Total Loss 60.7230224609375\n",
      "9: Encoding Loss 7.186211109161377, Transition Loss -7.807318687438965, Classifier Loss 0.1426553875207901, Total Loss 71.75366973876953\n",
      "9: Encoding Loss 6.782738208770752, Transition Loss -12.147225379943848, Classifier Loss 0.16962821781635284, Total Loss 71.22230529785156\n",
      "9: Encoding Loss 7.559282302856445, Transition Loss -16.255088806152344, Classifier Loss 0.1656656414270401, Total Loss 77.03756713867188\n",
      "9: Encoding Loss 6.069354057312012, Transition Loss 1.2503924369812012, Classifier Loss 0.16998617351055145, Total Loss 65.80352783203125\n",
      "9: Encoding Loss 6.307406425476074, Transition Loss -19.913089752197266, Classifier Loss 0.11498276889324188, Total Loss 61.953548431396484\n",
      "9: Encoding Loss 7.818314075469971, Transition Loss -19.69607162475586, Classifier Loss 0.15668372809886932, Total Loss 78.21095275878906\n",
      "9: Encoding Loss 6.738580226898193, Transition Loss 4.555664539337158, Classifier Loss 0.14406995475292206, Total Loss 69.22676849365234\n",
      "9: Encoding Loss 8.959087371826172, Transition Loss 3.4453136920928955, Classifier Loss 0.17336779832839966, Total Loss 89.69853973388672\n",
      "9: Encoding Loss 7.1528849601745605, Transition Loss -11.868474960327148, Classifier Loss 0.12787234783172607, Total Loss 70.00794219970703\n",
      "9: Encoding Loss 6.139753341674805, Transition Loss -10.927772521972656, Classifier Loss 0.1607924848794937, Total Loss 65.1950912475586\n",
      "9: Encoding Loss 6.556674480438232, Transition Loss -11.544403076171875, Classifier Loss 0.15202973783016205, Total Loss 67.65406036376953\n",
      "9: Encoding Loss 5.668445110321045, Transition Loss -2.6799914836883545, Classifier Loss 0.15497830510139465, Total Loss 60.844852447509766\n",
      "9: Encoding Loss 7.1824421882629395, Transition Loss -18.05980110168457, Classifier Loss 0.15063326060771942, Total Loss 72.51925659179688\n",
      "9: Encoding Loss 5.680656433105469, Transition Loss -3.366788864135742, Classifier Loss 0.1718684881925583, Total Loss 62.63142776489258\n",
      "9: Encoding Loss 8.891223907470703, Transition Loss -14.113468170166016, Classifier Loss 0.19160616397857666, Total Loss 90.28758239746094\n",
      "9: Encoding Loss 6.49152946472168, Transition Loss -23.078968048095703, Classifier Loss 0.17257143557071686, Total Loss 69.18476867675781\n",
      "9: Encoding Loss 7.636708736419678, Transition Loss -7.955559253692627, Classifier Loss 0.1551009714603424, Total Loss 76.6021728515625\n",
      "9: Encoding Loss 7.679722785949707, Transition Loss -8.974895477294922, Classifier Loss 0.20308159291744232, Total Loss 81.74414825439453\n",
      "9: Encoding Loss 8.595793724060059, Transition Loss -17.882339477539062, Classifier Loss 0.17843014001846313, Total Loss 86.60578155517578\n",
      "9: Encoding Loss 5.706306457519531, Transition Loss -12.104717254638672, Classifier Loss 0.10699619352817535, Total Loss 56.34764862060547\n",
      "9: Encoding Loss 7.066787242889404, Transition Loss -12.080470085144043, Classifier Loss 0.1623585969209671, Total Loss 72.76773834228516\n",
      "9: Encoding Loss 6.342766761779785, Transition Loss -3.956772565841675, Classifier Loss 0.15243306756019592, Total Loss 65.98464965820312\n",
      "9: Encoding Loss 6.06882381439209, Transition Loss -3.870990037918091, Classifier Loss 0.16657482087612152, Total Loss 65.20730590820312\n",
      "9: Encoding Loss 7.991369247436523, Transition Loss -9.464458465576172, Classifier Loss 0.16745330393314362, Total Loss 80.67439270019531\n",
      "9: Encoding Loss 5.7886505126953125, Transition Loss -0.8842366337776184, Classifier Loss 0.11784221231937408, Total Loss 58.0932502746582\n",
      "9: Encoding Loss 6.083229064941406, Transition Loss -11.050264358520508, Classifier Loss 0.12424425035715103, Total Loss 61.08804702758789\n",
      "9: Encoding Loss 7.173781394958496, Transition Loss -13.864090919494629, Classifier Loss 0.1467570960521698, Total Loss 72.06319427490234\n",
      "9: Encoding Loss 7.563193321228027, Transition Loss -15.315204620361328, Classifier Loss 0.1636611521244049, Total Loss 76.86860656738281\n",
      "9: Encoding Loss 7.494232177734375, Transition Loss 2.9145750999450684, Classifier Loss 0.20403167605400085, Total Loss 80.93994140625\n",
      "9: Encoding Loss 7.253173828125, Transition Loss -6.0025153160095215, Classifier Loss 0.15212494134902954, Total Loss 73.23668670654297\n",
      "9: Encoding Loss 7.108968257904053, Transition Loss -11.3248291015625, Classifier Loss 0.17374970018863678, Total Loss 74.24445343017578\n",
      "9: Encoding Loss 7.509420871734619, Transition Loss -11.758548736572266, Classifier Loss 0.16429004073143005, Total Loss 76.50202178955078\n",
      "9: Encoding Loss 6.248762130737305, Transition Loss -8.21147632598877, Classifier Loss 0.13482674956321716, Total Loss 63.471126556396484\n",
      "9: Encoding Loss 7.114196300506592, Transition Loss -0.32357168197631836, Classifier Loss 0.18984271585941315, Total Loss 75.89778137207031\n",
      "9: Encoding Loss 7.354023456573486, Transition Loss -15.974871635437012, Classifier Loss 0.1489335298538208, Total Loss 73.72234344482422\n",
      "9: Encoding Loss 7.251460075378418, Transition Loss 0.4429242014884949, Classifier Loss 0.14552518725395203, Total Loss 72.65278625488281\n",
      "9: Encoding Loss 6.050929546356201, Transition Loss -6.3114752769470215, Classifier Loss 0.13761751353740692, Total Loss 62.16792678833008\n",
      "9: Encoding Loss 6.734599590301514, Transition Loss -5.5946784019470215, Classifier Loss 0.15896029770374298, Total Loss 69.7717056274414\n",
      "9: Encoding Loss 5.352478504180908, Transition Loss -15.564988136291504, Classifier Loss 0.18215544521808624, Total Loss 61.032257080078125\n",
      "9: Encoding Loss 7.579274654388428, Transition Loss -16.623409271240234, Classifier Loss 0.21671974658966064, Total Loss 82.30284118652344\n",
      "9: Encoding Loss 6.319299221038818, Transition Loss -26.889814376831055, Classifier Loss 0.14592258632183075, Total Loss 65.14127349853516\n",
      "9: Encoding Loss 6.0166239738464355, Transition Loss -10.173480987548828, Classifier Loss 0.20165570080280304, Total Loss 68.29652404785156\n",
      "9: Encoding Loss 6.85161018371582, Transition Loss -11.239879608154297, Classifier Loss 0.17134448885917664, Total Loss 71.94507598876953\n",
      "9: Encoding Loss 6.252678394317627, Transition Loss -23.382728576660156, Classifier Loss 0.15125910937786102, Total Loss 65.14266204833984\n",
      "9: Encoding Loss 6.7101850509643555, Transition Loss -5.870607376098633, Classifier Loss 0.16877083480358124, Total Loss 70.55738830566406\n",
      "9: Encoding Loss 6.016505241394043, Transition Loss -16.212549209594727, Classifier Loss 0.16322806477546692, Total Loss 64.45160675048828\n",
      "9: Encoding Loss 5.52177095413208, Transition Loss -19.143020629882812, Classifier Loss 0.12143753468990326, Total Loss 56.314090728759766\n",
      "9: Encoding Loss 4.916215896606445, Transition Loss -20.606670379638672, Classifier Loss 0.1840347945690155, Total Loss 57.729087829589844\n",
      "9: Encoding Loss 9.95780086517334, Transition Loss -34.2351188659668, Classifier Loss 0.1197662353515625, Total Loss 91.63218688964844\n",
      "10: Encoding Loss 5.194502830505371, Transition Loss -0.2570861577987671, Classifier Loss 0.09850163757801056, Total Loss 51.4061393737793\n",
      "10: Encoding Loss 8.055436134338379, Transition Loss -4.325112342834473, Classifier Loss 0.18286404013633728, Total Loss 82.7290267944336\n",
      "10: Encoding Loss 6.483570575714111, Transition Loss 4.469400405883789, Classifier Loss 0.13600215315818787, Total Loss 66.36266326904297\n",
      "10: Encoding Loss 5.919760227203369, Transition Loss -21.422475814819336, Classifier Loss 0.15980781614780426, Total Loss 63.33457946777344\n",
      "10: Encoding Loss 7.231820583343506, Transition Loss -11.015552520751953, Classifier Loss 0.13928434252738953, Total Loss 71.78079223632812\n",
      "10: Encoding Loss 5.922736644744873, Transition Loss -15.120831489562988, Classifier Loss 0.17917320132255554, Total Loss 65.29618835449219\n",
      "10: Encoding Loss 6.758254051208496, Transition Loss -11.93659782409668, Classifier Loss 0.15964582562446594, Total Loss 70.02822875976562\n",
      "10: Encoding Loss 5.380622386932373, Transition Loss -7.02845573425293, Classifier Loss 0.1375516951084137, Total Loss 56.798744201660156\n",
      "10: Encoding Loss 6.894198894500732, Transition Loss -20.26357650756836, Classifier Loss 0.14161908626556396, Total Loss 69.31144714355469\n",
      "10: Encoding Loss 6.601560592651367, Transition Loss -12.180892944335938, Classifier Loss 0.17193283140659332, Total Loss 70.00333404541016\n",
      "10: Encoding Loss 7.959636211395264, Transition Loss -9.880402565002441, Classifier Loss 0.1681288778781891, Total Loss 80.48799896240234\n",
      "10: Encoding Loss 5.838065147399902, Transition Loss 0.7701811790466309, Classifier Loss 0.174815833568573, Total Loss 64.34014892578125\n",
      "10: Encoding Loss 6.049863815307617, Transition Loss -8.326887130737305, Classifier Loss 0.13074587285518646, Total Loss 61.47182846069336\n",
      "10: Encoding Loss 5.89011812210083, Transition Loss -6.6802496910095215, Classifier Loss 0.15514540672302246, Total Loss 62.634151458740234\n",
      "10: Encoding Loss 5.0979390144348145, Transition Loss -16.07938575744629, Classifier Loss 0.13213005661964417, Total Loss 53.99330139160156\n",
      "10: Encoding Loss 6.88226318359375, Transition Loss -24.305694580078125, Classifier Loss 0.13501933217048645, Total Loss 68.55517578125\n",
      "10: Encoding Loss 6.65959358215332, Transition Loss -10.60287857055664, Classifier Loss 0.11857367306947708, Total Loss 65.13198852539062\n",
      "10: Encoding Loss 6.743851661682129, Transition Loss -6.807986259460449, Classifier Loss 0.15004181861877441, Total Loss 68.9536361694336\n",
      "10: Encoding Loss 7.318639278411865, Transition Loss -11.483962059020996, Classifier Loss 0.20969045162200928, Total Loss 79.51586151123047\n",
      "10: Encoding Loss 6.739907264709473, Transition Loss -6.446864604949951, Classifier Loss 0.1363128423690796, Total Loss 67.54925537109375\n",
      "10: Encoding Loss 5.6704792976379395, Transition Loss -16.33104133605957, Classifier Loss 0.18076938390731812, Total Loss 63.43750762939453\n",
      "10: Encoding Loss 5.877714157104492, Transition Loss -10.861860275268555, Classifier Loss 0.12914308905601501, Total Loss 59.9338493347168\n",
      "10: Encoding Loss 6.9885711669921875, Transition Loss -18.507614135742188, Classifier Loss 0.15191076695919037, Total Loss 71.095947265625\n",
      "10: Encoding Loss 5.3840742111206055, Transition Loss -2.795602798461914, Classifier Loss 0.17086681723594666, Total Loss 60.15871810913086\n",
      "10: Encoding Loss 6.264420032501221, Transition Loss -11.561542510986328, Classifier Loss 0.14617855846881866, Total Loss 64.73090362548828\n",
      "10: Encoding Loss 5.570475101470947, Transition Loss -2.470930576324463, Classifier Loss 0.15407101809978485, Total Loss 59.97040557861328\n",
      "10: Encoding Loss 7.658163547515869, Transition Loss -2.2408690452575684, Classifier Loss 0.18133904039859772, Total Loss 79.39876556396484\n",
      "10: Encoding Loss 6.391285419464111, Transition Loss -23.782196044921875, Classifier Loss 0.1638713777065277, Total Loss 67.5126724243164\n",
      "10: Encoding Loss 5.845065593719482, Transition Loss -11.102487564086914, Classifier Loss 0.13920986652374268, Total Loss 60.679290771484375\n",
      "10: Encoding Loss 5.978339195251465, Transition Loss -2.801790714263916, Classifier Loss 0.14555461704730988, Total Loss 62.38161849975586\n",
      "10: Encoding Loss 6.65312385559082, Transition Loss -12.888079643249512, Classifier Loss 0.16471701860427856, Total Loss 69.69410705566406\n",
      "10: Encoding Loss 6.737890243530273, Transition Loss -0.594978928565979, Classifier Loss 0.19601386785507202, Total Loss 73.50439453125\n",
      "10: Encoding Loss 7.295983791351318, Transition Loss -14.027275085449219, Classifier Loss 0.15672197937965393, Total Loss 74.03726196289062\n",
      "10: Encoding Loss 7.779939651489258, Transition Loss -8.410762786865234, Classifier Loss 0.2281462550163269, Total Loss 85.05245971679688\n",
      "10: Encoding Loss 5.745339870452881, Transition Loss -16.325241088867188, Classifier Loss 0.14722663164138794, Total Loss 60.6821174621582\n",
      "10: Encoding Loss 5.122045040130615, Transition Loss -6.010342597961426, Classifier Loss 0.1167604848742485, Total Loss 52.651206970214844\n",
      "10: Encoding Loss 7.721240520477295, Transition Loss -8.995404243469238, Classifier Loss 0.1656663715839386, Total Loss 78.33476257324219\n",
      "10: Encoding Loss 5.560984134674072, Transition Loss -7.339524269104004, Classifier Loss 0.18355143070220947, Total Loss 62.841548919677734\n",
      "10: Encoding Loss 6.368462085723877, Transition Loss -10.371710777282715, Classifier Loss 0.130976140499115, Total Loss 64.0432357788086\n",
      "10: Encoding Loss 6.082244396209717, Transition Loss -10.930830001831055, Classifier Loss 0.19091758131980896, Total Loss 67.74752044677734\n",
      "10: Encoding Loss 4.732688903808594, Transition Loss -8.061736106872559, Classifier Loss 0.11740051209926605, Total Loss 49.59994888305664\n",
      "10: Encoding Loss 7.422092914581299, Transition Loss -18.759592056274414, Classifier Loss 0.16454264521598816, Total Loss 75.82725524902344\n",
      "10: Encoding Loss 5.466717720031738, Transition Loss -17.561986923217773, Classifier Loss 0.14615535736083984, Total Loss 58.34576416015625\n",
      "10: Encoding Loss 6.52170991897583, Transition Loss -3.871708393096924, Classifier Loss 0.17070630192756653, Total Loss 69.24354553222656\n",
      "10: Encoding Loss 6.54929256439209, Transition Loss -19.933807373046875, Classifier Loss 0.14461274445056915, Total Loss 66.85163116455078\n",
      "10: Encoding Loss 6.5196533203125, Transition Loss -4.615440845489502, Classifier Loss 0.15977612137794495, Total Loss 68.13391876220703\n",
      "10: Encoding Loss 8.834916114807129, Transition Loss -22.932199478149414, Classifier Loss 0.19141258299350739, Total Loss 89.81600189208984\n",
      "10: Encoding Loss 4.379771709442139, Transition Loss -5.660952091217041, Classifier Loss 0.13017834722995758, Total Loss 48.054874420166016\n",
      "10: Encoding Loss 7.560540199279785, Transition Loss -11.939493179321289, Classifier Loss 0.16075769066810608, Total Loss 76.55770111083984\n",
      "10: Encoding Loss 6.191764831542969, Transition Loss -19.55504035949707, Classifier Loss 0.1557869166135788, Total Loss 65.10889434814453\n",
      "10: Encoding Loss 5.491234302520752, Transition Loss -24.9980525970459, Classifier Loss 0.12903407216072083, Total Loss 56.82828140258789\n",
      "10: Encoding Loss 6.360705852508545, Transition Loss -16.414390563964844, Classifier Loss 0.15250274538993835, Total Loss 66.13264465332031\n",
      "10: Encoding Loss 4.745226860046387, Transition Loss 0.28155040740966797, Classifier Loss 0.15174558758735657, Total Loss 53.192684173583984\n",
      "10: Encoding Loss 6.53123664855957, Transition Loss -14.637371063232422, Classifier Loss 0.14048437774181366, Total Loss 66.29539489746094\n",
      "10: Encoding Loss 6.039381504058838, Transition Loss -13.412242889404297, Classifier Loss 0.1294669657945633, Total Loss 61.25906753540039\n",
      "10: Encoding Loss 6.284486770629883, Transition Loss -1.6291272640228271, Classifier Loss 0.19390279054641724, Total Loss 69.66584777832031\n",
      "10: Encoding Loss 6.051957130432129, Transition Loss -11.726349830627441, Classifier Loss 0.11553838849067688, Total Loss 59.9671516418457\n",
      "10: Encoding Loss 7.479249477386475, Transition Loss -15.737401962280273, Classifier Loss 0.14949890971183777, Total Loss 74.7807388305664\n",
      "10: Encoding Loss 5.330773830413818, Transition Loss -19.90033721923828, Classifier Loss 0.1705653965473175, Total Loss 59.69874954223633\n",
      "10: Encoding Loss 6.537961959838867, Transition Loss -17.95431137084961, Classifier Loss 0.14678911864757538, Total Loss 66.97901916503906\n",
      "10: Encoding Loss 6.173502445220947, Transition Loss -10.361006736755371, Classifier Loss 0.1262020617723465, Total Loss 62.00615310668945\n",
      "10: Encoding Loss 6.831490516662598, Transition Loss -7.237265586853027, Classifier Loss 0.18566358089447021, Total Loss 73.21683502197266\n",
      "10: Encoding Loss 6.456752300262451, Transition Loss -12.894071578979492, Classifier Loss 0.1419868767261505, Total Loss 65.85012817382812\n",
      "10: Encoding Loss 5.316812992095947, Transition Loss -5.94729471206665, Classifier Loss 0.17105838656425476, Total Loss 59.63915252685547\n",
      "10: Encoding Loss 5.126236915588379, Transition Loss -12.138951301574707, Classifier Loss 0.15897348523139954, Total Loss 56.904815673828125\n",
      "10: Encoding Loss 6.031794548034668, Transition Loss -1.7627321481704712, Classifier Loss 0.20734167098999023, Total Loss 68.98817443847656\n",
      "10: Encoding Loss 5.194954872131348, Transition Loss 5.978492736816406, Classifier Loss 0.1565450131893158, Total Loss 58.40983963012695\n",
      "10: Encoding Loss 7.55034875869751, Transition Loss -24.663909912109375, Classifier Loss 0.16090156137943268, Total Loss 76.4880142211914\n",
      "10: Encoding Loss 6.516306400299072, Transition Loss -6.799625873565674, Classifier Loss 0.15684695541858673, Total Loss 67.81378936767578\n",
      "10: Encoding Loss 7.715016841888428, Transition Loss -9.335113525390625, Classifier Loss 0.17913900315761566, Total Loss 79.63216400146484\n",
      "10: Encoding Loss 5.801513671875, Transition Loss -7.577917098999023, Classifier Loss 0.17194850742816925, Total Loss 63.605445861816406\n",
      "10: Encoding Loss 5.1073832511901855, Transition Loss -8.296299934387207, Classifier Loss 0.15477889776229858, Total Loss 56.335296630859375\n",
      "10: Encoding Loss 8.278404235839844, Transition Loss -4.747188091278076, Classifier Loss 0.16958196461200714, Total Loss 83.18448638916016\n",
      "10: Encoding Loss 5.485877513885498, Transition Loss -10.580812454223633, Classifier Loss 0.10029403120279312, Total Loss 53.914306640625\n",
      "10: Encoding Loss 7.142913818359375, Transition Loss -6.925045013427734, Classifier Loss 0.20142020285129547, Total Loss 77.28394317626953\n",
      "10: Encoding Loss 5.0543012619018555, Transition Loss -13.55389404296875, Classifier Loss 0.13007904589176178, Total Loss 53.439605712890625\n",
      "10: Encoding Loss 5.734105587005615, Transition Loss -2.305344820022583, Classifier Loss 0.14397048950195312, Total Loss 60.269432067871094\n",
      "10: Encoding Loss 7.108694076538086, Transition Loss -16.896482467651367, Classifier Loss 0.142192080616951, Total Loss 71.08538055419922\n",
      "10: Encoding Loss 6.18947696685791, Transition Loss -13.793973922729492, Classifier Loss 0.08956587314605713, Total Loss 58.46964645385742\n",
      "10: Encoding Loss 6.0323381423950195, Transition Loss -5.849761009216309, Classifier Loss 0.19724048674106598, Total Loss 67.9815902709961\n",
      "10: Encoding Loss 6.818190574645996, Transition Loss -13.248109817504883, Classifier Loss 0.14564606547355652, Total Loss 69.10749053955078\n",
      "10: Encoding Loss 6.118497848510742, Transition Loss -5.780576705932617, Classifier Loss 0.13019104301929474, Total Loss 61.9659309387207\n",
      "10: Encoding Loss 6.328738212585449, Transition Loss -5.424755573272705, Classifier Loss 0.1549876481294632, Total Loss 66.12759399414062\n",
      "10: Encoding Loss 7.006644248962402, Transition Loss -12.417245864868164, Classifier Loss 0.20363615453243256, Total Loss 76.4142837524414\n",
      "10: Encoding Loss 6.706145286560059, Transition Loss -12.179765701293945, Classifier Loss 0.12773048877716064, Total Loss 66.4197769165039\n",
      "10: Encoding Loss 5.786379337310791, Transition Loss -4.695977687835693, Classifier Loss 0.13606925308704376, Total Loss 59.89702224731445\n",
      "10: Encoding Loss 6.538773536682129, Transition Loss -18.473310470581055, Classifier Loss 0.14473263919353485, Total Loss 66.7797622680664\n",
      "10: Encoding Loss 5.428201198577881, Transition Loss -9.106683731079102, Classifier Loss 0.1487707495689392, Total Loss 58.300865173339844\n",
      "10: Encoding Loss 5.423584938049316, Transition Loss -5.992166996002197, Classifier Loss 0.1334337592124939, Total Loss 56.730857849121094\n",
      "10: Encoding Loss 7.350010871887207, Transition Loss -13.995784759521484, Classifier Loss 0.15470796823501587, Total Loss 74.26808166503906\n",
      "10: Encoding Loss 6.231222629547119, Transition Loss -25.466493606567383, Classifier Loss 0.14456439018249512, Total Loss 64.3011245727539\n",
      "10: Encoding Loss 5.623354911804199, Transition Loss -5.478310585021973, Classifier Loss 0.18557114899158478, Total Loss 63.54286575317383\n",
      "10: Encoding Loss 5.640284061431885, Transition Loss -4.787561416625977, Classifier Loss 0.1336057186126709, Total Loss 58.48188781738281\n",
      "10: Encoding Loss 6.4316253662109375, Transition Loss -10.034490585327148, Classifier Loss 0.13795697689056396, Total Loss 65.24669647216797\n",
      "10: Encoding Loss 5.754933834075928, Transition Loss 1.230147361755371, Classifier Loss 0.16932959854602814, Total Loss 63.21846008300781\n",
      "10: Encoding Loss 6.2312703132629395, Transition Loss -14.637654304504395, Classifier Loss 0.15768970549106598, Total Loss 65.61620330810547\n",
      "10: Encoding Loss 5.668985843658447, Transition Loss -16.176164627075195, Classifier Loss 0.1491362750530243, Total Loss 60.26227951049805\n",
      "10: Encoding Loss 3.947227954864502, Transition Loss -9.133399963378906, Classifier Loss 0.12215308845043182, Total Loss 43.79130554199219\n",
      "10: Encoding Loss 5.1059041023254395, Transition Loss 0.9942278861999512, Classifier Loss 0.110638827085495, Total Loss 52.109962463378906\n",
      "10: Encoding Loss 7.847333908081055, Transition Loss -8.389801025390625, Classifier Loss 0.17247235774993896, Total Loss 80.02423095703125\n",
      "10: Encoding Loss 6.605659484863281, Transition Loss -16.516695022583008, Classifier Loss 0.1514713019132614, Total Loss 67.98910522460938\n",
      "10: Encoding Loss 6.310395240783691, Transition Loss 4.214926719665527, Classifier Loss 0.17645932734012604, Total Loss 68.97208404541016\n",
      "10: Encoding Loss 6.30727481842041, Transition Loss -7.876469612121582, Classifier Loss 0.1279831826686859, Total Loss 63.254940032958984\n",
      "10: Encoding Loss 5.89042329788208, Transition Loss -1.5181961059570312, Classifier Loss 0.16389769315719604, Total Loss 63.512847900390625\n",
      "10: Encoding Loss 4.524550914764404, Transition Loss -11.71362018585205, Classifier Loss 0.1435600370168686, Total Loss 50.55006790161133\n",
      "10: Encoding Loss 6.6586456298828125, Transition Loss -16.778806686401367, Classifier Loss 0.1581326723098755, Total Loss 69.07907104492188\n",
      "10: Encoding Loss 5.6792449951171875, Transition Loss -1.2784600257873535, Classifier Loss 0.1929589956998825, Total Loss 64.72959899902344\n",
      "10: Encoding Loss 4.5168609619140625, Transition Loss -6.131269454956055, Classifier Loss 0.10575855523347855, Total Loss 46.70951843261719\n",
      "10: Encoding Loss 5.967836856842041, Transition Loss -8.1826753616333, Classifier Loss 0.12347682565450668, Total Loss 60.088741302490234\n",
      "10: Encoding Loss 6.008580684661865, Transition Loss -12.599180221557617, Classifier Loss 0.14518141746520996, Total Loss 62.584266662597656\n",
      "10: Encoding Loss 6.5520243644714355, Transition Loss -15.837201118469238, Classifier Loss 0.1438111662864685, Total Loss 66.79414367675781\n",
      "10: Encoding Loss 4.67004919052124, Transition Loss 0.5835453271865845, Classifier Loss 0.14165866374969482, Total Loss 51.64297103881836\n",
      "10: Encoding Loss 5.32004976272583, Transition Loss -19.41088104248047, Classifier Loss 0.09947800636291504, Total Loss 52.50431442260742\n",
      "10: Encoding Loss 6.646859169006348, Transition Loss -20.000837326049805, Classifier Loss 0.13301001489162445, Total Loss 66.47187805175781\n",
      "10: Encoding Loss 5.916310787200928, Transition Loss 4.708766937255859, Classifier Loss 0.1246919259428978, Total Loss 60.741432189941406\n",
      "10: Encoding Loss 7.6635050773620605, Transition Loss 3.270679235458374, Classifier Loss 0.1477823704481125, Total Loss 76.74041748046875\n",
      "10: Encoding Loss 6.2560272216796875, Transition Loss -12.11374282836914, Classifier Loss 0.1108512133359909, Total Loss 61.130916595458984\n",
      "10: Encoding Loss 5.059358596801758, Transition Loss -10.985748291015625, Classifier Loss 0.13448818027973175, Total Loss 53.92148971557617\n",
      "10: Encoding Loss 5.315311908721924, Transition Loss -11.612781524658203, Classifier Loss 0.1260693520307541, Total Loss 55.12710952758789\n",
      "10: Encoding Loss 4.832379341125488, Transition Loss -3.096839427947998, Classifier Loss 0.13198179006576538, Total Loss 51.856597900390625\n",
      "10: Encoding Loss 5.88095235824585, Transition Loss -18.19883155822754, Classifier Loss 0.12610532343387604, Total Loss 59.654510498046875\n",
      "10: Encoding Loss 4.864809036254883, Transition Loss -2.8357763290405273, Classifier Loss 0.14937730133533478, Total Loss 53.85563278198242\n",
      "10: Encoding Loss 7.8902506828308105, Transition Loss -14.64748764038086, Classifier Loss 0.17022092640399933, Total Loss 80.14116668701172\n",
      "10: Encoding Loss 5.339684963226318, Transition Loss -22.54800033569336, Classifier Loss 0.14755916595458984, Total Loss 57.46888732910156\n",
      "10: Encoding Loss 6.3845744132995605, Transition Loss -7.1186418533325195, Classifier Loss 0.13525603711605072, Total Loss 64.60076904296875\n",
      "10: Encoding Loss 6.8838677406311035, Transition Loss -10.253671646118164, Classifier Loss 0.18155863881111145, Total Loss 73.2247543334961\n",
      "10: Encoding Loss 7.824202537536621, Transition Loss -16.94230842590332, Classifier Loss 0.15595567226409912, Total Loss 78.18580627441406\n",
      "10: Encoding Loss 4.4573516845703125, Transition Loss -11.220786094665527, Classifier Loss 0.08836395293474197, Total Loss 44.49296569824219\n",
      "10: Encoding Loss 5.884673118591309, Transition Loss -12.364360809326172, Classifier Loss 0.13977383077144623, Total Loss 61.05229949951172\n",
      "10: Encoding Loss 5.136175632476807, Transition Loss -4.0008344650268555, Classifier Loss 0.12679056823253632, Total Loss 53.767662048339844\n",
      "10: Encoding Loss 5.006300449371338, Transition Loss -4.7765703201293945, Classifier Loss 0.13946835696697235, Total Loss 53.99628448486328\n",
      "10: Encoding Loss 7.129392623901367, Transition Loss -10.093730926513672, Classifier Loss 0.13940879702568054, Total Loss 70.97400665283203\n",
      "10: Encoding Loss 4.846902370452881, Transition Loss -0.8214138150215149, Classifier Loss 0.10298354923725128, Total Loss 49.07341003417969\n",
      "10: Encoding Loss 5.04749870300293, Transition Loss -11.52643871307373, Classifier Loss 0.10392968356609344, Total Loss 50.770652770996094\n",
      "10: Encoding Loss 6.241465091705322, Transition Loss -13.723419189453125, Classifier Loss 0.13261963427066803, Total Loss 63.19093704223633\n",
      "10: Encoding Loss 6.764982223510742, Transition Loss -15.22677230834961, Classifier Loss 0.143723726272583, Total Loss 68.48918914794922\n",
      "10: Encoding Loss 6.325132369995117, Transition Loss 2.173906087875366, Classifier Loss 0.18256136775016785, Total Loss 69.29197692871094\n",
      "10: Encoding Loss 6.111005783081055, Transition Loss -6.129707336425781, Classifier Loss 0.12645655870437622, Total Loss 61.53247833251953\n",
      "10: Encoding Loss 6.060715675354004, Transition Loss -11.578462600708008, Classifier Loss 0.15435586869716644, Total Loss 63.91899490356445\n",
      "10: Encoding Loss 6.507336616516113, Transition Loss -11.680274963378906, Classifier Loss 0.14072833955287933, Total Loss 66.12919616699219\n",
      "10: Encoding Loss 5.243624210357666, Transition Loss -8.155649185180664, Classifier Loss 0.1206601932644844, Total Loss 54.01338195800781\n",
      "10: Encoding Loss 6.069101333618164, Transition Loss -1.5559806823730469, Classifier Loss 0.16011744737625122, Total Loss 64.56424713134766\n",
      "10: Encoding Loss 6.107212543487549, Transition Loss -15.489348411560059, Classifier Loss 0.11987237632274628, Total Loss 60.84184265136719\n",
      "10: Encoding Loss 6.373117446899414, Transition Loss -0.2490496039390564, Classifier Loss 0.12544119358062744, Total Loss 63.52901077270508\n",
      "10: Encoding Loss 4.8396315574646, Transition Loss -5.887940883636475, Classifier Loss 0.11407864838838577, Total Loss 50.12373733520508\n",
      "10: Encoding Loss 5.594094753265381, Transition Loss -5.943913459777832, Classifier Loss 0.14306512475013733, Total Loss 59.058082580566406\n",
      "10: Encoding Loss 4.332006454467773, Transition Loss -15.436053276062012, Classifier Loss 0.17569600045681, Total Loss 52.222564697265625\n",
      "10: Encoding Loss 6.550857067108154, Transition Loss -16.98734474182129, Classifier Loss 0.18593475222587585, Total Loss 70.99694061279297\n",
      "10: Encoding Loss 5.16084623336792, Transition Loss -25.083890914916992, Classifier Loss 0.120976522564888, Total Loss 53.3794059753418\n",
      "10: Encoding Loss 5.16088342666626, Transition Loss -10.534303665161133, Classifier Loss 0.17574545741081238, Total Loss 58.85950469970703\n",
      "10: Encoding Loss 5.917935848236084, Transition Loss -11.372623443603516, Classifier Loss 0.14762192964553833, Total Loss 62.1034049987793\n",
      "10: Encoding Loss 5.200735092163086, Transition Loss -22.366804122924805, Classifier Loss 0.13109028339385986, Total Loss 54.71043395996094\n",
      "10: Encoding Loss 5.851704120635986, Transition Loss -5.8400068283081055, Classifier Loss 0.1550048291683197, Total Loss 62.312950134277344\n",
      "10: Encoding Loss 5.286897659301758, Transition Loss -16.211017608642578, Classifier Loss 0.1475331038236618, Total Loss 57.045249938964844\n",
      "10: Encoding Loss 4.656092643737793, Transition Loss -18.69666862487793, Classifier Loss 0.09790746867656708, Total Loss 47.03575134277344\n",
      "10: Encoding Loss 3.9567644596099854, Transition Loss -20.428407669067383, Classifier Loss 0.1527174562215805, Total Loss 46.92177963256836\n",
      "10: Encoding Loss 9.134041786193848, Transition Loss -32.260772705078125, Classifier Loss 0.09746082872152328, Total Loss 82.81196594238281\n",
      "11: Encoding Loss 4.385676383972168, Transition Loss 0.7616074085235596, Classifier Loss 0.08399812877178192, Total Loss 43.637550354003906\n",
      "11: Encoding Loss 7.046916484832764, Transition Loss -5.212731838226318, Classifier Loss 0.16952089965343475, Total Loss 73.32637786865234\n",
      "11: Encoding Loss 5.235172748565674, Transition Loss 3.2748000621795654, Classifier Loss 0.11179398000240326, Total Loss 53.71574020385742\n",
      "11: Encoding Loss 4.95155143737793, Transition Loss -20.80628204345703, Classifier Loss 0.1368815302848816, Total Loss 53.29640197753906\n",
      "11: Encoding Loss 6.120423316955566, Transition Loss -10.811273574829102, Classifier Loss 0.11763185262680054, Total Loss 60.72440719604492\n",
      "11: Encoding Loss 4.915873050689697, Transition Loss -15.405571937561035, Classifier Loss 0.14619891345500946, Total Loss 53.94379425048828\n",
      "11: Encoding Loss 5.613060474395752, Transition Loss -11.794021606445312, Classifier Loss 0.137695774435997, Total Loss 58.67170333862305\n",
      "11: Encoding Loss 4.712822914123535, Transition Loss -6.903911113739014, Classifier Loss 0.11634795367717743, Total Loss 49.33599853515625\n",
      "11: Encoding Loss 5.831848621368408, Transition Loss -18.88637924194336, Classifier Loss 0.1194801852107048, Total Loss 58.599029541015625\n",
      "11: Encoding Loss 5.518205165863037, Transition Loss -11.766690254211426, Classifier Loss 0.14670443534851074, Total Loss 58.8137321472168\n",
      "11: Encoding Loss 6.928371906280518, Transition Loss -9.766142845153809, Classifier Loss 0.14461761713027954, Total Loss 69.88678741455078\n",
      "11: Encoding Loss 4.895512104034424, Transition Loss -0.12588703632354736, Classifier Loss 0.15201576054096222, Total Loss 54.36564636230469\n",
      "11: Encoding Loss 5.162186145782471, Transition Loss -7.710555076599121, Classifier Loss 0.11177818477153778, Total Loss 52.4737663269043\n",
      "11: Encoding Loss 4.789366722106934, Transition Loss -6.806025981903076, Classifier Loss 0.13247975707054138, Total Loss 51.56155014038086\n",
      "11: Encoding Loss 4.184242248535156, Transition Loss -16.265762329101562, Classifier Loss 0.11309991031885147, Total Loss 44.78067398071289\n",
      "11: Encoding Loss 5.802750587463379, Transition Loss -23.20217514038086, Classifier Loss 0.11896943300962448, Total Loss 58.314308166503906\n",
      "11: Encoding Loss 5.5105743408203125, Transition Loss -10.375337600708008, Classifier Loss 0.09634003043174744, Total Loss 53.716522216796875\n",
      "11: Encoding Loss 5.589053153991699, Transition Loss -6.8590593338012695, Classifier Loss 0.1285478174686432, Total Loss 57.56583786010742\n",
      "11: Encoding Loss 6.510890483856201, Transition Loss -11.086006164550781, Classifier Loss 0.1835528314113617, Total Loss 70.440185546875\n",
      "11: Encoding Loss 5.833807468414307, Transition Loss -6.34665584564209, Classifier Loss 0.11859036982059479, Total Loss 58.52822494506836\n",
      "11: Encoding Loss 4.817656517028809, Transition Loss -15.9806547164917, Classifier Loss 0.15699362754821777, Total Loss 54.237422943115234\n",
      "11: Encoding Loss 5.009483337402344, Transition Loss -11.042644500732422, Classifier Loss 0.10958104580640793, Total Loss 51.031761169433594\n",
      "11: Encoding Loss 5.939218997955322, Transition Loss -18.011686325073242, Classifier Loss 0.1383567452430725, Total Loss 61.3458251953125\n",
      "11: Encoding Loss 4.473340034484863, Transition Loss -2.7075352668762207, Classifier Loss 0.15131737291812897, Total Loss 50.91791534423828\n",
      "11: Encoding Loss 5.364230632781982, Transition Loss -11.715751647949219, Classifier Loss 0.11875782161951065, Total Loss 54.78728485107422\n",
      "11: Encoding Loss 4.796113967895508, Transition Loss -2.882617712020874, Classifier Loss 0.13465146720409393, Total Loss 51.8334846496582\n",
      "11: Encoding Loss 6.522133827209473, Transition Loss -2.9763150215148926, Classifier Loss 0.16497710347175598, Total Loss 68.67418670654297\n",
      "11: Encoding Loss 5.4017720222473145, Transition Loss -22.879648208618164, Classifier Loss 0.14632980525493622, Total Loss 57.84257888793945\n",
      "11: Encoding Loss 4.7892165184021, Transition Loss -10.164730072021484, Classifier Loss 0.12481202930212021, Total Loss 50.792903900146484\n",
      "11: Encoding Loss 5.193548202514648, Transition Loss -3.712830066680908, Classifier Loss 0.12637029588222504, Total Loss 54.18467330932617\n",
      "11: Encoding Loss 5.80731725692749, Transition Loss -12.873224258422852, Classifier Loss 0.14276137948036194, Total Loss 60.73210144042969\n",
      "11: Encoding Loss 5.775566577911377, Transition Loss -1.184760570526123, Classifier Loss 0.17210924625396729, Total Loss 63.41522216796875\n",
      "11: Encoding Loss 6.447192192077637, Transition Loss -13.235615730285645, Classifier Loss 0.13625532388687134, Total Loss 65.20042419433594\n",
      "11: Encoding Loss 6.943465709686279, Transition Loss -8.094470977783203, Classifier Loss 0.21604937314987183, Total Loss 77.15104675292969\n",
      "11: Encoding Loss 4.610427379608154, Transition Loss -15.41666030883789, Classifier Loss 0.12443255633115768, Total Loss 49.32359313964844\n",
      "11: Encoding Loss 4.201003074645996, Transition Loss -5.889091491699219, Classifier Loss 0.09842721372842789, Total Loss 43.44956970214844\n",
      "11: Encoding Loss 6.952835559844971, Transition Loss -9.122017860412598, Classifier Loss 0.15073633193969727, Total Loss 70.69449615478516\n",
      "11: Encoding Loss 4.938809394836426, Transition Loss -7.229950904846191, Classifier Loss 0.16862782835960388, Total Loss 56.3718147277832\n",
      "11: Encoding Loss 5.436004638671875, Transition Loss -10.438032150268555, Classifier Loss 0.11999990046024323, Total Loss 55.48594284057617\n",
      "11: Encoding Loss 5.123135089874268, Transition Loss -11.481807708740234, Classifier Loss 0.16235114634037018, Total Loss 57.217899322509766\n",
      "11: Encoding Loss 3.839470863342285, Transition Loss -8.139391899108887, Classifier Loss 0.09733258932828903, Total Loss 40.44739532470703\n",
      "11: Encoding Loss 6.381917476654053, Transition Loss -18.345657348632812, Classifier Loss 0.147667795419693, Total Loss 65.81845092773438\n",
      "11: Encoding Loss 4.318264961242676, Transition Loss -17.16218376159668, Classifier Loss 0.1187983825802803, Total Loss 46.422523498535156\n",
      "11: Encoding Loss 5.5265302658081055, Transition Loss -3.913329601287842, Classifier Loss 0.1496042162179947, Total Loss 59.1718864440918\n",
      "11: Encoding Loss 5.507130146026611, Transition Loss -19.244861602783203, Classifier Loss 0.12182384729385376, Total Loss 56.23557662963867\n",
      "11: Encoding Loss 5.840020656585693, Transition Loss -5.16294002532959, Classifier Loss 0.1406773179769516, Total Loss 60.786861419677734\n",
      "11: Encoding Loss 7.492905616760254, Transition Loss -21.726970672607422, Classifier Loss 0.1630147099494934, Total Loss 76.24037170410156\n",
      "11: Encoding Loss 3.4142608642578125, Transition Loss -5.627270698547363, Classifier Loss 0.10784319788217545, Total Loss 38.09728240966797\n",
      "11: Encoding Loss 6.327119827270508, Transition Loss -11.763211250305176, Classifier Loss 0.1391402781009674, Total Loss 64.52864074707031\n",
      "11: Encoding Loss 5.178495407104492, Transition Loss -18.943870544433594, Classifier Loss 0.13145995140075684, Total Loss 54.57017135620117\n",
      "11: Encoding Loss 4.613205909729004, Transition Loss -23.572328567504883, Classifier Loss 0.11223811656236649, Total Loss 48.1247444152832\n",
      "11: Encoding Loss 5.37265157699585, Transition Loss -15.315557479858398, Classifier Loss 0.13580290973186493, Total Loss 56.558441162109375\n",
      "11: Encoding Loss 3.9517250061035156, Transition Loss 0.17744684219360352, Classifier Loss 0.14465385675430298, Total Loss 46.11467361450195\n",
      "11: Encoding Loss 5.523530960083008, Transition Loss -14.540362358093262, Classifier Loss 0.12103358656167984, Total Loss 56.288700103759766\n",
      "11: Encoding Loss 5.072685241699219, Transition Loss -12.571318626403809, Classifier Loss 0.11446492373943329, Total Loss 52.02545928955078\n",
      "11: Encoding Loss 5.310893535614014, Transition Loss -2.275613307952881, Classifier Loss 0.16920097172260284, Total Loss 59.40679168701172\n",
      "11: Encoding Loss 5.028617858886719, Transition Loss -11.213269233703613, Classifier Loss 0.09681308269500732, Total Loss 49.90800857543945\n",
      "11: Encoding Loss 6.3986687660217285, Transition Loss -15.323149681091309, Classifier Loss 0.131068617105484, Total Loss 64.29314422607422\n",
      "11: Encoding Loss 4.359888553619385, Transition Loss -19.451831817626953, Classifier Loss 0.14177006483078003, Total Loss 49.052223205566406\n",
      "11: Encoding Loss 5.515356063842773, Transition Loss -16.99285125732422, Classifier Loss 0.13149742782115936, Total Loss 57.26919174194336\n",
      "11: Encoding Loss 5.300027370452881, Transition Loss -10.353826522827148, Classifier Loss 0.11186483502388, Total Loss 53.58462905883789\n",
      "11: Encoding Loss 5.976057052612305, Transition Loss -7.822086334228516, Classifier Loss 0.17000673711299896, Total Loss 64.80756378173828\n",
      "11: Encoding Loss 5.361030101776123, Transition Loss -12.589374542236328, Classifier Loss 0.12620922923088074, Total Loss 55.50664520263672\n",
      "11: Encoding Loss 4.341795921325684, Transition Loss -6.330591201782227, Classifier Loss 0.1462796926498413, Total Loss 49.36107635498047\n",
      "11: Encoding Loss 4.215653896331787, Transition Loss -11.905566215515137, Classifier Loss 0.1397039294242859, Total Loss 47.69324493408203\n",
      "11: Encoding Loss 5.191807746887207, Transition Loss -2.460515260696411, Classifier Loss 0.19496920704841614, Total Loss 61.03089141845703\n",
      "11: Encoding Loss 4.647843360900879, Transition Loss 5.105233192443848, Classifier Loss 0.13310222327709198, Total Loss 51.514015197753906\n",
      "11: Encoding Loss 6.340868949890137, Transition Loss -23.176912307739258, Classifier Loss 0.14444047212600708, Total Loss 65.16636657714844\n",
      "11: Encoding Loss 5.567692756652832, Transition Loss -6.657317638397217, Classifier Loss 0.13718044757843018, Total Loss 58.25825500488281\n",
      "11: Encoding Loss 6.950772762298584, Transition Loss -9.297828674316406, Classifier Loss 0.16815172135829926, Total Loss 72.41949462890625\n",
      "11: Encoding Loss 4.928646564483643, Transition Loss -7.572880268096924, Classifier Loss 0.15161830186843872, Total Loss 54.5894889831543\n",
      "11: Encoding Loss 4.233491897583008, Transition Loss -8.34290599822998, Classifier Loss 0.13657823204994202, Total Loss 47.52408981323242\n",
      "11: Encoding Loss 7.26248025894165, Transition Loss -4.760744094848633, Classifier Loss 0.15611597895622253, Total Loss 73.71048736572266\n",
      "11: Encoding Loss 4.373865127563477, Transition Loss -10.246657371520996, Classifier Loss 0.08379735052585602, Total Loss 43.36860656738281\n",
      "11: Encoding Loss 5.9408369064331055, Transition Loss -6.860857963562012, Classifier Loss 0.1861504316329956, Total Loss 66.14036560058594\n",
      "11: Encoding Loss 4.0960869789123535, Transition Loss -13.359552383422852, Classifier Loss 0.10794948786497116, Total Loss 43.56097412109375\n",
      "11: Encoding Loss 4.9222259521484375, Transition Loss -2.804637908935547, Classifier Loss 0.13038428127765656, Total Loss 52.41567611694336\n",
      "11: Encoding Loss 5.899877071380615, Transition Loss -16.115650177001953, Classifier Loss 0.1272059977054596, Total Loss 59.9163932800293\n",
      "11: Encoding Loss 5.187420845031738, Transition Loss -12.954120635986328, Classifier Loss 0.07810834795236588, Total Loss 49.307613372802734\n",
      "11: Encoding Loss 5.1574625968933105, Transition Loss -6.7649126052856445, Classifier Loss 0.1773608922958374, Total Loss 58.99443435668945\n",
      "11: Encoding Loss 5.667737007141113, Transition Loss -12.849892616271973, Classifier Loss 0.1242818757891655, Total Loss 57.767513275146484\n",
      "11: Encoding Loss 5.197489261627197, Transition Loss -5.722579002380371, Classifier Loss 0.11188387870788574, Total Loss 52.76715850830078\n",
      "11: Encoding Loss 5.471858024597168, Transition Loss -5.654574871063232, Classifier Loss 0.1433565318584442, Total Loss 58.10939025878906\n",
      "11: Encoding Loss 6.075369834899902, Transition Loss -12.171808242797852, Classifier Loss 0.17943155765533447, Total Loss 66.54368591308594\n",
      "11: Encoding Loss 5.353914737701416, Transition Loss -10.8279390335083, Classifier Loss 0.11394670605659485, Total Loss 54.22382354736328\n",
      "11: Encoding Loss 4.725600719451904, Transition Loss -4.528733253479004, Classifier Loss 0.11945938318967819, Total Loss 49.749839782714844\n",
      "11: Encoding Loss 5.704233646392822, Transition Loss -17.634273529052734, Classifier Loss 0.12916740775108337, Total Loss 58.547080993652344\n",
      "11: Encoding Loss 4.422276496887207, Transition Loss -8.331348419189453, Classifier Loss 0.1472322791814804, Total Loss 50.09977340698242\n",
      "11: Encoding Loss 4.427345275878906, Transition Loss -6.444586277008057, Classifier Loss 0.11480031162500381, Total Loss 46.89750289916992\n",
      "11: Encoding Loss 6.431595802307129, Transition Loss -13.180715560913086, Classifier Loss 0.13483330607414246, Total Loss 64.93345642089844\n",
      "11: Encoding Loss 5.024659156799316, Transition Loss -24.102033615112305, Classifier Loss 0.1268375962972641, Total Loss 52.87621307373047\n",
      "11: Encoding Loss 4.769619941711426, Transition Loss -5.549989700317383, Classifier Loss 0.16842766106128693, Total Loss 54.99861526489258\n",
      "11: Encoding Loss 4.743974208831787, Transition Loss -4.994972229003906, Classifier Loss 0.1220480352640152, Total Loss 50.15559768676758\n",
      "11: Encoding Loss 5.6450581550598145, Transition Loss -9.76711368560791, Classifier Loss 0.12632039189338684, Total Loss 57.790550231933594\n",
      "11: Encoding Loss 4.967864036560059, Transition Loss 0.43703150749206543, Classifier Loss 0.14795365929603577, Total Loss 54.62569046020508\n",
      "11: Encoding Loss 5.134499549865723, Transition Loss -13.909337997436523, Classifier Loss 0.14137396216392517, Total Loss 55.21061325073242\n",
      "11: Encoding Loss 4.555322647094727, Transition Loss -15.35783576965332, Classifier Loss 0.13915766775608063, Total Loss 50.35527801513672\n",
      "11: Encoding Loss 3.2079451084136963, Transition Loss -8.95928955078125, Classifier Loss 0.11136887222528458, Total Loss 36.79865646362305\n",
      "11: Encoding Loss 4.21666145324707, Transition Loss 0.1422567367553711, Classifier Loss 0.09622722864151001, Total Loss 43.384464263916016\n",
      "11: Encoding Loss 6.824673175811768, Transition Loss -7.685318946838379, Classifier Loss 0.15996231138706207, Total Loss 70.59208679199219\n",
      "11: Encoding Loss 5.368020057678223, Transition Loss -16.370567321777344, Classifier Loss 0.1349453330039978, Total Loss 56.435420989990234\n",
      "11: Encoding Loss 5.568266868591309, Transition Loss 3.145115375518799, Classifier Loss 0.16587680578231812, Total Loss 61.76284408569336\n",
      "11: Encoding Loss 5.309494495391846, Transition Loss -7.493402004241943, Classifier Loss 0.1167198121547699, Total Loss 54.14643859863281\n",
      "11: Encoding Loss 5.255434036254883, Transition Loss -2.2404561042785645, Classifier Loss 0.14415547251701355, Total Loss 56.45857238769531\n",
      "11: Encoding Loss 3.685685634613037, Transition Loss -11.615056037902832, Classifier Loss 0.12303218245506287, Total Loss 41.786380767822266\n",
      "11: Encoding Loss 5.444808483123779, Transition Loss -16.225324630737305, Classifier Loss 0.14358074963092804, Total Loss 57.91329574584961\n",
      "11: Encoding Loss 4.750607013702393, Transition Loss -1.2515525817871094, Classifier Loss 0.17287373542785645, Total Loss 55.29197692871094\n",
      "11: Encoding Loss 3.57255220413208, Transition Loss -5.4892578125, Classifier Loss 0.09054836630821228, Total Loss 37.6341552734375\n",
      "11: Encoding Loss 4.967667102813721, Transition Loss -8.34024429321289, Classifier Loss 0.11248020827770233, Total Loss 50.98768997192383\n",
      "11: Encoding Loss 5.27699089050293, Transition Loss -12.723219871520996, Classifier Loss 0.12922737002372742, Total Loss 55.1361198425293\n",
      "11: Encoding Loss 5.587196350097656, Transition Loss -14.923032760620117, Classifier Loss 0.13219335675239563, Total Loss 57.91392517089844\n",
      "11: Encoding Loss 3.644770860671997, Transition Loss 0.05743229389190674, Classifier Loss 0.12054015696048737, Total Loss 41.22366714477539\n",
      "11: Encoding Loss 4.437045097351074, Transition Loss -18.59310531616211, Classifier Loss 0.09245455265045166, Total Loss 44.738101959228516\n",
      "11: Encoding Loss 5.446901321411133, Transition Loss -19.10209846496582, Classifier Loss 0.11648476868867874, Total Loss 55.21986389160156\n",
      "11: Encoding Loss 5.139444351196289, Transition Loss 3.948807954788208, Classifier Loss 0.11016339063644409, Total Loss 52.921653747558594\n",
      "11: Encoding Loss 6.5838775634765625, Transition Loss 3.218427896499634, Classifier Loss 0.1261080503463745, Total Loss 65.9255142211914\n",
      "11: Encoding Loss 5.337316989898682, Transition Loss -11.541097640991211, Classifier Loss 0.09905938059091568, Total Loss 52.60216522216797\n",
      "11: Encoding Loss 4.118852138519287, Transition Loss -10.83888053894043, Classifier Loss 0.1186477318406105, Total Loss 44.81342315673828\n",
      "11: Encoding Loss 4.1880669593811035, Transition Loss -11.293641090393066, Classifier Loss 0.11002066731452942, Total Loss 44.50434494018555\n",
      "11: Encoding Loss 4.056643486022949, Transition Loss -3.543759346008301, Classifier Loss 0.11390481889247894, Total Loss 43.842926025390625\n",
      "11: Encoding Loss 4.699014663696289, Transition Loss -17.196409225463867, Classifier Loss 0.10955865681171417, Total Loss 48.54454040527344\n",
      "11: Encoding Loss 4.049528121948242, Transition Loss -2.878960609436035, Classifier Loss 0.1314396858215332, Total Loss 45.539615631103516\n",
      "11: Encoding Loss 6.878092288970947, Transition Loss -13.902582168579102, Classifier Loss 0.15376611053943634, Total Loss 70.39857482910156\n",
      "11: Encoding Loss 4.2989912033081055, Transition Loss -21.358688354492188, Classifier Loss 0.1349528580904007, Total Loss 47.8829460144043\n",
      "11: Encoding Loss 5.2085394859313965, Transition Loss -6.652021408081055, Classifier Loss 0.12647879123687744, Total Loss 54.31486129760742\n",
      "11: Encoding Loss 5.947768211364746, Transition Loss -10.134394645690918, Classifier Loss 0.16810260713100433, Total Loss 64.390380859375\n",
      "11: Encoding Loss 6.717596530914307, Transition Loss -16.063880920410156, Classifier Loss 0.14084206521511078, Total Loss 67.82176971435547\n",
      "11: Encoding Loss 3.4024498462677, Transition Loss -10.418783187866211, Classifier Loss 0.07543609291315079, Total Loss 34.76112365722656\n",
      "11: Encoding Loss 4.872706890106201, Transition Loss -11.817234992980957, Classifier Loss 0.12423352897167206, Total Loss 51.40264129638672\n",
      "11: Encoding Loss 4.1003923416137695, Transition Loss -3.9312636852264404, Classifier Loss 0.10955949872732162, Total Loss 43.75830078125\n",
      "11: Encoding Loss 4.138876914978027, Transition Loss -5.068309307098389, Classifier Loss 0.12309278547763824, Total Loss 45.419281005859375\n",
      "11: Encoding Loss 6.125035762786865, Transition Loss -9.668790817260742, Classifier Loss 0.12046229094266891, Total Loss 61.04458236694336\n",
      "11: Encoding Loss 4.020435810089111, Transition Loss -0.9379417896270752, Classifier Loss 0.09570888429880142, Total Loss 41.734188079833984\n",
      "11: Encoding Loss 4.099968433380127, Transition Loss -11.032540321350098, Classifier Loss 0.09010559320449829, Total Loss 41.808101654052734\n",
      "11: Encoding Loss 5.11828088760376, Transition Loss -12.860629081726074, Classifier Loss 0.12366030365228653, Total Loss 53.30970764160156\n",
      "11: Encoding Loss 5.7730536460876465, Transition Loss -14.273563385009766, Classifier Loss 0.12914954125881195, Total Loss 59.09653091430664\n",
      "11: Encoding Loss 5.377021312713623, Transition Loss 1.9586820602416992, Classifier Loss 0.16609452664852142, Total Loss 60.01736068725586\n",
      "11: Encoding Loss 5.003773212432861, Transition Loss -5.680927276611328, Classifier Loss 0.1064191460609436, Total Loss 50.670963287353516\n",
      "11: Encoding Loss 5.123198986053467, Transition Loss -10.845526695251465, Classifier Loss 0.14084823429584503, Total Loss 55.06824493408203\n",
      "11: Encoding Loss 5.5397562980651855, Transition Loss -10.714214324951172, Classifier Loss 0.12717385590076447, Total Loss 57.03329086303711\n",
      "11: Encoding Loss 4.222507953643799, Transition Loss -7.569286346435547, Classifier Loss 0.11111251264810562, Total Loss 44.889801025390625\n",
      "11: Encoding Loss 5.040925025939941, Transition Loss -1.580643653869629, Classifier Loss 0.14097000658512115, Total Loss 54.4240837097168\n",
      "11: Encoding Loss 4.970900535583496, Transition Loss -14.055964469909668, Classifier Loss 0.10116096585988998, Total Loss 49.8804931640625\n",
      "11: Encoding Loss 5.5163254737854, Transition Loss -0.7826828956604004, Classifier Loss 0.11223241686820984, Total Loss 55.35368728637695\n",
      "11: Encoding Loss 3.885566234588623, Transition Loss -5.719761848449707, Classifier Loss 0.09734640270471573, Total Loss 40.81802749633789\n",
      "11: Encoding Loss 4.564000606536865, Transition Loss -5.577278137207031, Classifier Loss 0.1312006711959839, Total Loss 49.630958557128906\n",
      "11: Encoding Loss 3.5958070755004883, Transition Loss -14.750207901000977, Classifier Loss 0.17134609818458557, Total Loss 45.89811325073242\n",
      "11: Encoding Loss 5.423069477081299, Transition Loss -16.3936710357666, Classifier Loss 0.16699784994125366, Total Loss 60.08106231689453\n",
      "11: Encoding Loss 3.9962103366851807, Transition Loss -23.01788902282715, Classifier Loss 0.10409148782491684, Total Loss 42.374229431152344\n",
      "11: Encoding Loss 4.482609748840332, Transition Loss -10.445243835449219, Classifier Loss 0.15742740035057068, Total Loss 51.60152816772461\n",
      "11: Encoding Loss 4.84616756439209, Transition Loss -10.953117370605469, Classifier Loss 0.1326163113117218, Total Loss 52.028785705566406\n",
      "11: Encoding Loss 4.208496570587158, Transition Loss -20.822261810302734, Classifier Loss 0.11832348257303238, Total Loss 45.49615478515625\n",
      "11: Encoding Loss 4.965907573699951, Transition Loss -5.432608604431152, Classifier Loss 0.1469050794839859, Total Loss 54.416683197021484\n",
      "11: Encoding Loss 4.512186050415039, Transition Loss -15.421512603759766, Classifier Loss 0.13832472264766693, Total Loss 49.926876068115234\n",
      "11: Encoding Loss 3.834517002105713, Transition Loss -17.74175262451172, Classifier Loss 0.08304963260889053, Total Loss 38.9775505065918\n",
      "11: Encoding Loss 3.042708396911621, Transition Loss -19.460630416870117, Classifier Loss 0.1342228353023529, Total Loss 37.76005935668945\n",
      "11: Encoding Loss 7.592583179473877, Transition Loss -29.80016326904297, Classifier Loss 0.09021510928869247, Total Loss 69.75621795654297\n",
      "12: Encoding Loss 3.737942934036255, Transition Loss 0.8632028102874756, Classifier Loss 0.07322224229574203, Total Loss 37.39841079711914\n",
      "12: Encoding Loss 5.857520580291748, Transition Loss -4.702651023864746, Classifier Loss 0.15905094146728516, Total Loss 62.76431655883789\n",
      "12: Encoding Loss 4.20020055770874, Transition Loss 2.9581985473632812, Classifier Loss 0.09668946266174316, Total Loss 43.86219024658203\n",
      "12: Encoding Loss 3.978959798812866, Transition Loss -19.648052215576172, Classifier Loss 0.1229831725358963, Total Loss 44.126068115234375\n",
      "12: Encoding Loss 4.971739768981934, Transition Loss -9.812783241271973, Classifier Loss 0.10278849303722382, Total Loss 50.050811767578125\n",
      "12: Encoding Loss 3.86491322517395, Transition Loss -15.03562068939209, Classifier Loss 0.12348964065313339, Total Loss 43.265262603759766\n",
      "12: Encoding Loss 4.539025783538818, Transition Loss -10.998270034790039, Classifier Loss 0.12334397435188293, Total Loss 48.64440155029297\n",
      "12: Encoding Loss 3.886314868927002, Transition Loss -6.565845012664795, Classifier Loss 0.10112307220697403, Total Loss 41.201515197753906\n",
      "12: Encoding Loss 4.832997798919678, Transition Loss -17.224464416503906, Classifier Loss 0.10608690977096558, Total Loss 49.269229888916016\n",
      "12: Encoding Loss 4.384457588195801, Transition Loss -10.910579681396484, Classifier Loss 0.13033729791641235, Total Loss 48.107208251953125\n",
      "12: Encoding Loss 6.020866394042969, Transition Loss -9.053414344787598, Classifier Loss 0.1281982660293579, Total Loss 60.98494338989258\n",
      "12: Encoding Loss 3.984025478363037, Transition Loss -0.6444981098175049, Classifier Loss 0.13426288962364197, Total Loss 45.298362731933594\n",
      "12: Encoding Loss 4.443286418914795, Transition Loss -7.322149276733398, Classifier Loss 0.09915871173143387, Total Loss 45.460697174072266\n",
      "12: Encoding Loss 3.8469924926757812, Transition Loss -6.299980163574219, Classifier Loss 0.11852996051311493, Total Loss 42.62767791748047\n",
      "12: Encoding Loss 3.325802803039551, Transition Loss -15.771419525146484, Classifier Loss 0.104822538793087, Total Loss 37.08552169799805\n",
      "12: Encoding Loss 4.554720401763916, Transition Loss -21.507770538330078, Classifier Loss 0.11012712121009827, Total Loss 47.44617462158203\n",
      "12: Encoding Loss 4.326485633850098, Transition Loss -9.537278175354004, Classifier Loss 0.08115268498659134, Total Loss 42.72524642944336\n",
      "12: Encoding Loss 4.6019487380981445, Transition Loss -6.145547866821289, Classifier Loss 0.11218966543674469, Total Loss 48.033329010009766\n",
      "12: Encoding Loss 5.694596290588379, Transition Loss -10.678224563598633, Classifier Loss 0.16355468332767487, Total Loss 61.91010284423828\n",
      "12: Encoding Loss 4.850503444671631, Transition Loss -5.694148063659668, Classifier Loss 0.10673753917217255, Total Loss 49.47664260864258\n",
      "12: Encoding Loss 4.025257110595703, Transition Loss -15.079514503479004, Classifier Loss 0.1414773166179657, Total Loss 46.346771240234375\n",
      "12: Encoding Loss 4.010088920593262, Transition Loss -10.590559005737305, Classifier Loss 0.1003943383693695, Total Loss 42.1180305480957\n",
      "12: Encoding Loss 4.858273029327393, Transition Loss -16.728662490844727, Classifier Loss 0.13096286356449127, Total Loss 51.95912551879883\n",
      "12: Encoding Loss 3.5077009201049805, Transition Loss -2.4264824390411377, Classifier Loss 0.14180052280426025, Total Loss 42.24117660522461\n",
      "12: Encoding Loss 4.3738579750061035, Transition Loss -11.128996849060059, Classifier Loss 0.09872467815876007, Total Loss 44.861106872558594\n",
      "12: Encoding Loss 3.9719719886779785, Transition Loss -2.8720664978027344, Classifier Loss 0.12175653874874115, Total Loss 43.95085525512695\n",
      "12: Encoding Loss 5.488066673278809, Transition Loss -2.7889394760131836, Classifier Loss 0.15051014721393585, Total Loss 58.954994201660156\n",
      "12: Encoding Loss 4.295895099639893, Transition Loss -21.603302001953125, Classifier Loss 0.13547992706298828, Total Loss 47.910831451416016\n",
      "12: Encoding Loss 3.5843775272369385, Transition Loss -9.08428955078125, Classifier Loss 0.11381871998310089, Total Loss 40.055076599121094\n",
      "12: Encoding Loss 4.36079216003418, Transition Loss -3.787715435028076, Classifier Loss 0.11519938707351685, Total Loss 46.405517578125\n",
      "12: Encoding Loss 4.880058765411377, Transition Loss -12.192211151123047, Classifier Loss 0.13088014721870422, Total Loss 52.12604904174805\n",
      "12: Encoding Loss 4.748678684234619, Transition Loss -1.0780143737792969, Classifier Loss 0.15389087796211243, Total Loss 53.378299713134766\n",
      "12: Encoding Loss 5.495665550231934, Transition Loss -12.037200927734375, Classifier Loss 0.12750981748104095, Total Loss 56.71390151977539\n",
      "12: Encoding Loss 5.89118766784668, Transition Loss -7.3870391845703125, Classifier Loss 0.20952332019805908, Total Loss 68.08035278320312\n",
      "12: Encoding Loss 3.5730974674224854, Transition Loss -14.19680118560791, Classifier Loss 0.11056177318096161, Total Loss 39.63812255859375\n",
      "12: Encoding Loss 3.2740437984466553, Transition Loss -5.399178504943848, Classifier Loss 0.08244186639785767, Total Loss 34.43545913696289\n",
      "12: Encoding Loss 5.94334602355957, Transition Loss -8.740821838378906, Classifier Loss 0.1434321403503418, Total Loss 61.88823699951172\n",
      "12: Encoding Loss 4.293900012969971, Transition Loss -7.205029487609863, Classifier Loss 0.15958833694458008, Total Loss 50.30859375\n",
      "12: Encoding Loss 4.395822048187256, Transition Loss -9.63546371459961, Classifier Loss 0.11513333022594452, Total Loss 46.677982330322266\n",
      "12: Encoding Loss 4.050757884979248, Transition Loss -11.044501304626465, Classifier Loss 0.14168910682201385, Total Loss 46.5727653503418\n",
      "12: Encoding Loss 2.8861031532287598, Transition Loss -7.634189128875732, Classifier Loss 0.08528043329715729, Total Loss 31.615341186523438\n",
      "12: Encoding Loss 5.195050239562988, Transition Loss -16.959091186523438, Classifier Loss 0.1364268958568573, Total Loss 55.19969940185547\n",
      "12: Encoding Loss 3.258655071258545, Transition Loss -16.08950424194336, Classifier Loss 0.10006587207317352, Total Loss 36.072608947753906\n",
      "12: Encoding Loss 4.507591247558594, Transition Loss -3.6325368881225586, Classifier Loss 0.1350824534893036, Total Loss 49.56825256347656\n",
      "12: Encoding Loss 4.424403190612793, Transition Loss -17.777578353881836, Classifier Loss 0.10711373388767242, Total Loss 46.10304641723633\n",
      "12: Encoding Loss 5.111395359039307, Transition Loss -5.106741905212402, Classifier Loss 0.12835155427455902, Total Loss 53.72529602050781\n",
      "12: Encoding Loss 6.297940731048584, Transition Loss -19.522552490234375, Classifier Loss 0.14557838439941406, Total Loss 64.93745422363281\n",
      "12: Encoding Loss 2.5254242420196533, Transition Loss -5.158598899841309, Classifier Loss 0.09221048653125763, Total Loss 29.423410415649414\n",
      "12: Encoding Loss 5.104024410247803, Transition Loss -10.505023956298828, Classifier Loss 0.12420560419559479, Total Loss 53.25065231323242\n",
      "12: Encoding Loss 4.195071220397949, Transition Loss -17.855247497558594, Classifier Loss 0.1144532784819603, Total Loss 45.0023307800293\n",
      "12: Encoding Loss 3.674476146697998, Transition Loss -21.642393112182617, Classifier Loss 0.1020248681306839, Total Loss 39.59396743774414\n",
      "12: Encoding Loss 4.450403213500977, Transition Loss -14.036108016967773, Classifier Loss 0.128384068608284, Total Loss 48.43882369995117\n",
      "12: Encoding Loss 3.066805601119995, Transition Loss 0.10233068466186523, Classifier Loss 0.14132484793663025, Total Loss 38.68739700317383\n",
      "12: Encoding Loss 4.481489181518555, Transition Loss -13.506296157836914, Classifier Loss 0.10827284306287766, Total Loss 46.67649841308594\n",
      "12: Encoding Loss 4.01798152923584, Transition Loss -11.401211738586426, Classifier Loss 0.10632085055112839, Total Loss 42.773658752441406\n",
      "12: Encoding Loss 4.30715274810791, Transition Loss -2.2132797241210938, Classifier Loss 0.1489323079586029, Total Loss 49.35000991821289\n",
      "12: Encoding Loss 4.059848308563232, Transition Loss -10.520174026489258, Classifier Loss 0.08783823251724243, Total Loss 41.26050567626953\n",
      "12: Encoding Loss 5.2149977684021, Transition Loss -14.098304748535156, Classifier Loss 0.12078364938497543, Total Loss 53.795528411865234\n",
      "12: Encoding Loss 3.4174411296844482, Transition Loss -18.737537384033203, Classifier Loss 0.12300394475460052, Total Loss 39.63617706298828\n",
      "12: Encoding Loss 4.432113170623779, Transition Loss -15.746051788330078, Classifier Loss 0.12231951951980591, Total Loss 47.685707092285156\n",
      "12: Encoding Loss 4.388546466827393, Transition Loss -9.763134002685547, Classifier Loss 0.10223302245140076, Total Loss 45.32971954345703\n",
      "12: Encoding Loss 5.036462306976318, Transition Loss -7.365738868713379, Classifier Loss 0.1571645736694336, Total Loss 56.006683349609375\n",
      "12: Encoding Loss 4.268274307250977, Transition Loss -11.6322603225708, Classifier Loss 0.11759405583143234, Total Loss 45.90327453613281\n",
      "12: Encoding Loss 3.3996856212615967, Transition Loss -5.951022148132324, Classifier Loss 0.12989914417266846, Total Loss 40.18621063232422\n",
      "12: Encoding Loss 3.2127532958984375, Transition Loss -11.340738296508789, Classifier Loss 0.12633714079856873, Total Loss 38.33346939086914\n",
      "12: Encoding Loss 4.260228633880615, Transition Loss -2.628575325012207, Classifier Loss 0.18597042560577393, Total Loss 52.6783447265625\n",
      "12: Encoding Loss 4.009531021118164, Transition Loss 4.223374366760254, Classifier Loss 0.11620438098907471, Total Loss 44.541358947753906\n",
      "12: Encoding Loss 5.007425785064697, Transition Loss -20.623252868652344, Classifier Loss 0.13403594493865967, Total Loss 53.45887756347656\n",
      "12: Encoding Loss 4.707261562347412, Transition Loss -6.2082366943359375, Classifier Loss 0.12272056937217712, Total Loss 49.92890930175781\n",
      "12: Encoding Loss 6.019825458526611, Transition Loss -8.559436798095703, Classifier Loss 0.16210274398326874, Total Loss 64.36717224121094\n",
      "12: Encoding Loss 3.97076416015625, Transition Loss -7.0233025550842285, Classifier Loss 0.13888245820999146, Total Loss 45.6529541015625\n",
      "12: Encoding Loss 3.3766119480133057, Transition Loss -7.831482887268066, Classifier Loss 0.12562735378742218, Total Loss 39.574066162109375\n",
      "12: Encoding Loss 6.124139308929443, Transition Loss -4.188470840454102, Classifier Loss 0.14875489473342896, Total Loss 63.86776351928711\n",
      "12: Encoding Loss 3.381370782852173, Transition Loss -9.328208923339844, Classifier Loss 0.07393690198659897, Total Loss 34.44279098510742\n",
      "12: Encoding Loss 4.760758876800537, Transition Loss -6.186981678009033, Classifier Loss 0.1741231083869934, Total Loss 55.49714660644531\n",
      "12: Encoding Loss 3.1833550930023193, Transition Loss -12.843194007873535, Classifier Loss 0.09511099755764008, Total Loss 34.975372314453125\n",
      "12: Encoding Loss 3.903496503829956, Transition Loss -2.7176120281219482, Classifier Loss 0.12305540591478348, Total Loss 43.5329704284668\n",
      "12: Encoding Loss 4.532761573791504, Transition Loss -14.316401481628418, Classifier Loss 0.11812958866357803, Total Loss 48.07218551635742\n",
      "12: Encoding Loss 4.144659042358398, Transition Loss -12.156044006347656, Classifier Loss 0.07015250623226166, Total Loss 40.17009353637695\n",
      "12: Encoding Loss 4.243488788604736, Transition Loss -6.9064130783081055, Classifier Loss 0.1643112450838089, Total Loss 50.377655029296875\n",
      "12: Encoding Loss 4.498713493347168, Transition Loss -11.649026870727539, Classifier Loss 0.11079533398151398, Total Loss 47.06691360473633\n",
      "12: Encoding Loss 4.044723987579346, Transition Loss -5.370623588562012, Classifier Loss 0.10018537938594818, Total Loss 42.37525177001953\n",
      "12: Encoding Loss 4.600921154022217, Transition Loss -5.169055938720703, Classifier Loss 0.1360585242509842, Total Loss 50.41218948364258\n",
      "12: Encoding Loss 5.054872989654541, Transition Loss -11.140213966369629, Classifier Loss 0.16335347294807434, Total Loss 56.77210235595703\n",
      "12: Encoding Loss 3.988762617111206, Transition Loss -9.233530044555664, Classifier Loss 0.10582686215639114, Total Loss 42.49094009399414\n",
      "12: Encoding Loss 3.5857956409454346, Transition Loss -3.576054811477661, Classifier Loss 0.1077396497130394, Total Loss 39.459617614746094\n",
      "12: Encoding Loss 4.655820369720459, Transition Loss -16.34931755065918, Classifier Loss 0.12090190500020981, Total Loss 49.3334846496582\n",
      "12: Encoding Loss 3.4515981674194336, Transition Loss -7.434665203094482, Classifier Loss 0.14458248019218445, Total Loss 42.069549560546875\n",
      "12: Encoding Loss 3.4790749549865723, Transition Loss -5.989049911499023, Classifier Loss 0.10231374204158783, Total Loss 38.062774658203125\n",
      "12: Encoding Loss 5.501065254211426, Transition Loss -11.90812873840332, Classifier Loss 0.12026333063840866, Total Loss 56.032474517822266\n",
      "12: Encoding Loss 3.6864876747131348, Transition Loss -22.27001953125, Classifier Loss 0.11447162926197052, Total Loss 40.934608459472656\n",
      "12: Encoding Loss 3.8357582092285156, Transition Loss -5.156418323516846, Classifier Loss 0.15753872692584991, Total Loss 46.438907623291016\n",
      "12: Encoding Loss 3.670933961868286, Transition Loss -4.7653045654296875, Classifier Loss 0.11508174240589142, Total Loss 40.874691009521484\n",
      "12: Encoding Loss 4.685953140258789, Transition Loss -9.156044960021973, Classifier Loss 0.11982636153697968, Total Loss 49.46842956542969\n",
      "12: Encoding Loss 4.083226203918457, Transition Loss 0.4068046808242798, Classifier Loss 0.13256192207336426, Total Loss 46.003360748291016\n",
      "12: Encoding Loss 4.036584377288818, Transition Loss -12.517556190490723, Classifier Loss 0.12959164381027222, Total Loss 45.24933624267578\n",
      "12: Encoding Loss 3.4669268131256104, Transition Loss -14.289055824279785, Classifier Loss 0.132927805185318, Total Loss 41.02534103393555\n",
      "12: Encoding Loss 2.4326400756835938, Transition Loss -8.625420570373535, Classifier Loss 0.10272406041622162, Total Loss 29.731801986694336\n",
      "12: Encoding Loss 3.296316623687744, Transition Loss 0.20070767402648926, Classifier Loss 0.08669779449701309, Total Loss 35.0804557800293\n",
      "12: Encoding Loss 5.520431041717529, Transition Loss -6.496997833251953, Classifier Loss 0.15345188975334167, Total Loss 59.5073356628418\n",
      "12: Encoding Loss 4.247680187225342, Transition Loss -15.237892150878906, Classifier Loss 0.12988808751106262, Total Loss 46.967201232910156\n",
      "12: Encoding Loss 4.790830135345459, Transition Loss 2.8608198165893555, Classifier Loss 0.15774020552635193, Total Loss 54.67282485961914\n",
      "12: Encoding Loss 4.277432918548584, Transition Loss -6.490036964416504, Classifier Loss 0.1092212125658989, Total Loss 45.140289306640625\n",
      "12: Encoding Loss 4.4317216873168945, Transition Loss -2.109659194946289, Classifier Loss 0.13339684903621674, Total Loss 48.793033599853516\n",
      "12: Encoding Loss 2.8632900714874268, Transition Loss -10.873271942138672, Classifier Loss 0.11085507273674011, Total Loss 33.989654541015625\n",
      "12: Encoding Loss 4.215120315551758, Transition Loss -14.853803634643555, Classifier Loss 0.13462164998054504, Total Loss 47.18015670776367\n",
      "12: Encoding Loss 3.7365479469299316, Transition Loss -0.8245788812637329, Classifier Loss 0.15717273950576782, Total Loss 45.609493255615234\n",
      "12: Encoding Loss 2.5871646404266357, Transition Loss -4.810983657836914, Classifier Loss 0.08393684774637222, Total Loss 29.09004020690918\n",
      "12: Encoding Loss 3.797828435897827, Transition Loss -7.5353193283081055, Classifier Loss 0.10847961157560349, Total Loss 41.22908401489258\n",
      "12: Encoding Loss 4.490880012512207, Transition Loss -12.483078002929688, Classifier Loss 0.11894138902425766, Total Loss 47.81868362426758\n",
      "12: Encoding Loss 4.483633041381836, Transition Loss -13.394133567810059, Classifier Loss 0.12333135306835175, Total Loss 48.19952392578125\n",
      "12: Encoding Loss 2.5939691066741943, Transition Loss 0.31568658351898193, Classifier Loss 0.10935002565383911, Total Loss 31.749893188476562\n",
      "12: Encoding Loss 3.4341094493865967, Transition Loss -17.11440086364746, Classifier Loss 0.08756579458713531, Total Loss 36.22603225708008\n",
      "12: Encoding Loss 4.32159423828125, Transition Loss -17.38629722595215, Classifier Loss 0.10805781185626984, Total Loss 45.375057220458984\n",
      "12: Encoding Loss 4.217165470123291, Transition Loss 3.659348964691162, Classifier Loss 0.10250531136989594, Total Loss 44.719722747802734\n",
      "12: Encoding Loss 5.348261833190918, Transition Loss 4.061823844909668, Classifier Loss 0.11247741430997849, Total Loss 54.84620666503906\n",
      "12: Encoding Loss 4.26594352722168, Transition Loss -10.468005180358887, Classifier Loss 0.09255341440439224, Total Loss 43.380794525146484\n",
      "12: Encoding Loss 3.1537108421325684, Transition Loss -9.941022872924805, Classifier Loss 0.10752299427986145, Total Loss 35.97999954223633\n",
      "12: Encoding Loss 3.0434226989746094, Transition Loss -10.296908378601074, Classifier Loss 0.0999661311507225, Total Loss 34.34193420410156\n",
      "12: Encoding Loss 3.166588306427002, Transition Loss -3.4863123893737793, Classifier Loss 0.10381918400526047, Total Loss 35.713924407958984\n",
      "12: Encoding Loss 3.4889111518859863, Transition Loss -15.27139949798584, Classifier Loss 0.10012207925319672, Total Loss 37.920440673828125\n",
      "12: Encoding Loss 3.105839967727661, Transition Loss -2.747450828552246, Classifier Loss 0.11974094063043594, Total Loss 36.82026672363281\n",
      "12: Encoding Loss 5.749425888061523, Transition Loss -12.009303092956543, Classifier Loss 0.14402318000793457, Total Loss 60.395320892333984\n",
      "12: Encoding Loss 3.265101194381714, Transition Loss -19.458629608154297, Classifier Loss 0.12696708738803864, Total Loss 38.813629150390625\n",
      "12: Encoding Loss 3.9586617946624756, Transition Loss -5.3822021484375, Classifier Loss 0.12291882932186127, Total Loss 43.96010208129883\n",
      "12: Encoding Loss 4.902647972106934, Transition Loss -9.339049339294434, Classifier Loss 0.16068829596042633, Total Loss 55.28814697265625\n",
      "12: Encoding Loss 5.39888334274292, Transition Loss -14.878162384033203, Classifier Loss 0.13248983025550842, Total Loss 56.43707275390625\n",
      "12: Encoding Loss 2.3270246982574463, Transition Loss -9.401362419128418, Classifier Loss 0.06828704476356506, Total Loss 25.443021774291992\n",
      "12: Encoding Loss 3.8756139278411865, Transition Loss -10.77650260925293, Classifier Loss 0.11284743249416351, Total Loss 42.287498474121094\n",
      "12: Encoding Loss 2.964123010635376, Transition Loss -3.2192134857177734, Classifier Loss 0.09829005599021912, Total Loss 33.541343688964844\n",
      "12: Encoding Loss 3.295844554901123, Transition Loss -4.923150539398193, Classifier Loss 0.11357999593019485, Total Loss 37.72377395629883\n",
      "12: Encoding Loss 4.979279041290283, Transition Loss -8.408188819885254, Classifier Loss 0.10753631591796875, Total Loss 50.586181640625\n",
      "12: Encoding Loss 3.1546530723571777, Transition Loss -0.8659922480583191, Classifier Loss 0.09234344959259033, Total Loss 34.47140121459961\n",
      "12: Encoding Loss 3.2054107189178467, Transition Loss -10.042978286743164, Classifier Loss 0.08181241154670715, Total Loss 33.82251739501953\n",
      "12: Encoding Loss 3.8246352672576904, Transition Loss -11.382902145385742, Classifier Loss 0.11895738542079926, Total Loss 42.490543365478516\n",
      "12: Encoding Loss 4.720128059387207, Transition Loss -12.845986366271973, Classifier Loss 0.12247724831104279, Total Loss 50.00618362426758\n",
      "12: Encoding Loss 4.363615036010742, Transition Loss 2.315889358520508, Classifier Loss 0.15611772239208221, Total Loss 50.98386764526367\n",
      "12: Encoding Loss 3.8810386657714844, Transition Loss -4.962096214294434, Classifier Loss 0.09361304342746735, Total Loss 40.40862274169922\n",
      "12: Encoding Loss 4.237271785736084, Transition Loss -9.799064636230469, Classifier Loss 0.1306726187467575, Total Loss 46.96347427368164\n",
      "12: Encoding Loss 4.539328575134277, Transition Loss -9.160208702087402, Classifier Loss 0.11915560066699982, Total Loss 48.228363037109375\n",
      "12: Encoding Loss 3.1194281578063965, Transition Loss -6.564691543579102, Classifier Loss 0.1044296994805336, Total Loss 35.39708709716797\n",
      "12: Encoding Loss 3.9483706951141357, Transition Loss -1.148674726486206, Classifier Loss 0.12802131474018097, Total Loss 44.38887023925781\n",
      "12: Encoding Loss 3.7145280838012695, Transition Loss -12.0791015625, Classifier Loss 0.08839181065559387, Total Loss 38.5529899597168\n",
      "12: Encoding Loss 4.568554878234863, Transition Loss -0.608238160610199, Classifier Loss 0.10457508265972137, Total Loss 47.00582504272461\n",
      "12: Encoding Loss 3.016134738922119, Transition Loss -5.089315414428711, Classifier Loss 0.08775448799133301, Total Loss 32.903507232666016\n",
      "12: Encoding Loss 3.4928431510925293, Transition Loss -4.721749782562256, Classifier Loss 0.12198871374130249, Total Loss 40.14067077636719\n",
      "12: Encoding Loss 3.021000862121582, Transition Loss -13.674979209899902, Classifier Loss 0.16734610497951508, Total Loss 40.89988327026367\n",
      "12: Encoding Loss 4.212949275970459, Transition Loss -14.859878540039062, Classifier Loss 0.15510886907577515, Total Loss 49.211509704589844\n",
      "12: Encoding Loss 2.8214337825775146, Transition Loss -20.69076919555664, Classifier Loss 0.09446986019611359, Total Loss 32.01431655883789\n",
      "12: Encoding Loss 3.8667099475860596, Transition Loss -9.772043228149414, Classifier Loss 0.1458427608013153, Total Loss 45.5160026550293\n",
      "12: Encoding Loss 3.7519032955169678, Transition Loss -9.85816478729248, Classifier Loss 0.1232081726193428, Total Loss 42.33407211303711\n",
      "12: Encoding Loss 3.2321996688842773, Transition Loss -18.914798736572266, Classifier Loss 0.11264066398143768, Total Loss 37.117881774902344\n",
      "12: Encoding Loss 3.8612725734710693, Transition Loss -4.481152534484863, Classifier Loss 0.1400299072265625, Total Loss 44.892276763916016\n",
      "12: Encoding Loss 3.6504018306732178, Transition Loss -14.449682235717773, Classifier Loss 0.13105714321136475, Total Loss 42.30603790283203\n",
      "12: Encoding Loss 2.987172842025757, Transition Loss -16.15776252746582, Classifier Loss 0.07466817647218704, Total Loss 31.36096954345703\n",
      "12: Encoding Loss 2.22758150100708, Transition Loss -18.269933700561523, Classifier Loss 0.1211717426776886, Total Loss 29.934171676635742\n",
      "12: Encoding Loss 5.740823268890381, Transition Loss -26.68735694885254, Classifier Loss 0.07708211243152618, Total Loss 53.629459381103516\n",
      "13: Encoding Loss 2.9716849327087402, Transition Loss 0.8877463936805725, Classifier Loss 0.06714452058076859, Total Loss 30.665483474731445\n",
      "13: Encoding Loss 4.533663749694824, Transition Loss -3.203434944152832, Classifier Loss 0.15459893643856049, Total Loss 51.72856521606445\n",
      "13: Encoding Loss 3.1934385299682617, Transition Loss 3.3432323932647705, Classifier Loss 0.0870734453201294, Total Loss 34.923500061035156\n",
      "13: Encoding Loss 2.95182728767395, Transition Loss -17.957054138183594, Classifier Loss 0.11571702361106873, Total Loss 35.18273162841797\n",
      "13: Encoding Loss 3.8490192890167236, Transition Loss -7.999517440795898, Classifier Loss 0.0946597009897232, Total Loss 40.256526947021484\n",
      "13: Encoding Loss 2.736642599105835, Transition Loss -14.01032543182373, Classifier Loss 0.1127898320555687, Total Loss 33.16931915283203\n",
      "13: Encoding Loss 3.5151329040527344, Transition Loss -9.62185287475586, Classifier Loss 0.11472681164741516, Total Loss 39.591819763183594\n",
      "13: Encoding Loss 2.8469488620758057, Transition Loss -5.88453483581543, Classifier Loss 0.09247241914272308, Total Loss 32.02165603637695\n",
      "13: Encoding Loss 3.8758769035339355, Transition Loss -15.238879203796387, Classifier Loss 0.09901703894138336, Total Loss 40.90567398071289\n",
      "13: Encoding Loss 3.2206034660339355, Transition Loss -9.68117904663086, Classifier Loss 0.11982104927301407, Total Loss 37.7449951171875\n",
      "13: Encoding Loss 5.172884941101074, Transition Loss -7.7222113609313965, Classifier Loss 0.12039195001125336, Total Loss 53.42073440551758\n",
      "13: Encoding Loss 3.0558254718780518, Transition Loss -0.6649700999259949, Classifier Loss 0.12179049104452133, Total Loss 36.625518798828125\n",
      "13: Encoding Loss 3.654837131500244, Transition Loss -6.16550874710083, Classifier Loss 0.09248696267604828, Total Loss 38.48616027832031\n",
      "13: Encoding Loss 2.840451717376709, Transition Loss -5.2042083740234375, Classifier Loss 0.1086558848619461, Total Loss 33.58816146850586\n",
      "13: Encoding Loss 2.3375000953674316, Transition Loss -14.316041946411133, Classifier Loss 0.10033591091632843, Total Loss 28.730730056762695\n",
      "13: Encoding Loss 3.333655595779419, Transition Loss -19.457984924316406, Classifier Loss 0.10107696801424026, Total Loss 36.773048400878906\n",
      "13: Encoding Loss 3.186004161834717, Transition Loss -8.03268814086914, Classifier Loss 0.07188812643289566, Total Loss 32.67524337768555\n",
      "13: Encoding Loss 3.663717031478882, Transition Loss -4.766111373901367, Classifier Loss 0.10294363647699356, Total Loss 39.603145599365234\n",
      "13: Encoding Loss 4.830322742462158, Transition Loss -9.93423080444336, Classifier Loss 0.15125876665115356, Total Loss 53.76647186279297\n",
      "13: Encoding Loss 3.9328694343566895, Transition Loss -4.426224708557129, Classifier Loss 0.09963119029998779, Total Loss 41.42518997192383\n",
      "13: Encoding Loss 3.304781436920166, Transition Loss -13.807135581970215, Classifier Loss 0.13026311993598938, Total Loss 39.46179962158203\n",
      "13: Encoding Loss 2.9899752140045166, Transition Loss -9.26697826385498, Classifier Loss 0.09217513352632523, Total Loss 33.135459899902344\n",
      "13: Encoding Loss 3.6740570068359375, Transition Loss -14.536102294921875, Classifier Loss 0.12778379023075104, Total Loss 42.167930603027344\n",
      "13: Encoding Loss 2.550690174102783, Transition Loss -1.6861684322357178, Classifier Loss 0.13371062278747559, Total Loss 33.776248931884766\n",
      "13: Encoding Loss 3.2697126865386963, Transition Loss -9.852728843688965, Classifier Loss 0.08791768550872803, Total Loss 34.9474983215332\n",
      "13: Encoding Loss 3.1013424396514893, Transition Loss -2.2091619968414307, Classifier Loss 0.11305755376815796, Total Loss 36.116050720214844\n",
      "13: Encoding Loss 4.362438678741455, Transition Loss -1.6769845485687256, Classifier Loss 0.1401534527540207, Total Loss 48.914520263671875\n",
      "13: Encoding Loss 3.043886423110962, Transition Loss -19.359390258789062, Classifier Loss 0.12867823243141174, Total Loss 37.21504211425781\n",
      "13: Encoding Loss 2.2497828006744385, Transition Loss -7.89301061630249, Classifier Loss 0.10723409056663513, Total Loss 28.7200927734375\n",
      "13: Encoding Loss 3.4492337703704834, Transition Loss -3.0153467655181885, Classifier Loss 0.10875819623470306, Total Loss 38.469085693359375\n",
      "13: Encoding Loss 3.9083211421966553, Transition Loss -10.649249076843262, Classifier Loss 0.12504638731479645, Total Loss 43.769081115722656\n",
      "13: Encoding Loss 3.599512815475464, Transition Loss -0.6468449831008911, Classifier Loss 0.14240393042564392, Total Loss 43.0363655090332\n",
      "13: Encoding Loss 4.519349575042725, Transition Loss -10.383312225341797, Classifier Loss 0.1246199682354927, Total Loss 48.61471939086914\n",
      "13: Encoding Loss 4.739431858062744, Transition Loss -6.402697563171387, Classifier Loss 0.20303675532341003, Total Loss 58.21784973144531\n",
      "13: Encoding Loss 2.526383638381958, Transition Loss -12.460220336914062, Classifier Loss 0.10101044178009033, Total Loss 30.309619903564453\n",
      "13: Encoding Loss 2.235766887664795, Transition Loss -4.776211261749268, Classifier Loss 0.07483389228582382, Total Loss 25.36857032775879\n",
      "13: Encoding Loss 4.721513748168945, Transition Loss -7.587676048278809, Classifier Loss 0.13956786692142487, Total Loss 51.727378845214844\n",
      "13: Encoding Loss 3.499448299407959, Transition Loss -7.0831193923950195, Classifier Loss 0.1528494507074356, Total Loss 43.279117584228516\n",
      "13: Encoding Loss 3.264657497406006, Transition Loss -7.960753440856934, Classifier Loss 0.11822492629289627, Total Loss 37.93816375732422\n",
      "13: Encoding Loss 2.869541883468628, Transition Loss -9.793115615844727, Classifier Loss 0.13048499822616577, Total Loss 36.00288009643555\n",
      "13: Encoding Loss 1.839605450630188, Transition Loss -6.579383373260498, Classifier Loss 0.07873363792896271, Total Loss 22.588891983032227\n",
      "13: Encoding Loss 3.9905846118927, Transition Loss -14.936769485473633, Classifier Loss 0.13573549687862396, Total Loss 45.4952392578125\n",
      "13: Encoding Loss 2.1995291709899902, Transition Loss -14.394332885742188, Classifier Loss 0.08890930563211441, Total Loss 26.48428726196289\n",
      "13: Encoding Loss 3.335808277130127, Transition Loss -2.657132387161255, Classifier Loss 0.12440204620361328, Total Loss 39.12614059448242\n",
      "13: Encoding Loss 3.209567070007324, Transition Loss -15.980077743530273, Classifier Loss 0.09842254966497421, Total Loss 35.515594482421875\n",
      "13: Encoding Loss 4.245246887207031, Transition Loss -4.4912519454956055, Classifier Loss 0.124020054936409, Total Loss 46.36308288574219\n",
      "13: Encoding Loss 5.182037353515625, Transition Loss -16.81618309020996, Classifier Loss 0.13143527507781982, Total Loss 54.59646224975586\n",
      "13: Encoding Loss 1.6032017469406128, Transition Loss -4.179254531860352, Classifier Loss 0.0864931046962738, Total Loss 21.474088668823242\n",
      "13: Encoding Loss 3.8679072856903076, Transition Loss -8.539983749389648, Classifier Loss 0.118024080991745, Total Loss 42.74395751953125\n",
      "13: Encoding Loss 3.1800692081451416, Transition Loss -15.872392654418945, Classifier Loss 0.10508927702903748, Total Loss 35.94630813598633\n",
      "13: Encoding Loss 2.685683250427246, Transition Loss -19.566198348999023, Classifier Loss 0.09622874110937119, Total Loss 31.104427337646484\n",
      "13: Encoding Loss 3.572556257247925, Transition Loss -12.009714126586914, Classifier Loss 0.12097170948982239, Total Loss 40.67521667480469\n",
      "13: Encoding Loss 1.9978079795837402, Transition Loss 0.4334239959716797, Classifier Loss 0.1346028745174408, Total Loss 29.529438018798828\n",
      "13: Encoding Loss 3.2633538246154785, Transition Loss -11.702333450317383, Classifier Loss 0.10248326510190964, Total Loss 36.352813720703125\n",
      "13: Encoding Loss 2.891894817352295, Transition Loss -9.912936210632324, Classifier Loss 0.10013680160045624, Total Loss 33.14685821533203\n",
      "13: Encoding Loss 3.1475768089294434, Transition Loss -1.5178390741348267, Classifier Loss 0.13517068326473236, Total Loss 38.69738006591797\n",
      "13: Encoding Loss 3.0880348682403564, Transition Loss -9.280220985412598, Classifier Loss 0.08106137067079544, Total Loss 32.80855941772461\n",
      "13: Encoding Loss 3.878798484802246, Transition Loss -11.989532470703125, Classifier Loss 0.1117033064365387, Total Loss 42.19832229614258\n",
      "13: Encoding Loss 2.3308298587799072, Transition Loss -17.629032135009766, Classifier Loss 0.1129659041762352, Total Loss 29.9397029876709\n",
      "13: Encoding Loss 3.1906120777130127, Transition Loss -13.919336318969727, Classifier Loss 0.11654271930456161, Total Loss 37.17638397216797\n",
      "13: Encoding Loss 3.3407952785491943, Transition Loss -8.366424560546875, Classifier Loss 0.09794045984745026, Total Loss 36.518733978271484\n",
      "13: Encoding Loss 3.795788526535034, Transition Loss -6.189093112945557, Classifier Loss 0.15076331794261932, Total Loss 45.44140625\n",
      "13: Encoding Loss 3.0040388107299805, Transition Loss -9.996341705322266, Classifier Loss 0.11266237497329712, Total Loss 35.296546936035156\n",
      "13: Encoding Loss 2.385249376296997, Transition Loss -5.173229694366455, Classifier Loss 0.1179252564907074, Total Loss 30.873485565185547\n",
      "13: Encoding Loss 2.0797390937805176, Transition Loss -10.229948043823242, Classifier Loss 0.11805228143930435, Total Loss 28.44109535217285\n",
      "13: Encoding Loss 3.1154942512512207, Transition Loss -2.0757222175598145, Classifier Loss 0.17889520525932312, Total Loss 42.81306076049805\n",
      "13: Encoding Loss 3.2265968322753906, Transition Loss 3.78493595123291, Classifier Loss 0.102943554520607, Total Loss 36.86412048339844\n",
      "13: Encoding Loss 3.6760122776031494, Transition Loss -17.750749588012695, Classifier Loss 0.13024473190307617, Total Loss 42.429019927978516\n",
      "13: Encoding Loss 3.7635271549224854, Transition Loss -5.062961101531982, Classifier Loss 0.11386938393115997, Total Loss 41.494144439697266\n",
      "13: Encoding Loss 4.839222431182861, Transition Loss -7.305073261260986, Classifier Loss 0.15850861370563507, Total Loss 54.56317901611328\n",
      "13: Encoding Loss 2.8227527141571045, Transition Loss -6.277964115142822, Classifier Loss 0.13003578782081604, Total Loss 35.584346771240234\n",
      "13: Encoding Loss 2.3802239894866943, Transition Loss -6.925500392913818, Classifier Loss 0.11844072490930557, Total Loss 30.884479522705078\n",
      "13: Encoding Loss 4.761507034301758, Transition Loss -2.9820449352264404, Classifier Loss 0.14375121891498566, Total Loss 52.466583251953125\n",
      "13: Encoding Loss 2.39034366607666, Transition Loss -7.697577476501465, Classifier Loss 0.06912869960069656, Total Loss 26.034080505371094\n",
      "13: Encoding Loss 3.406372308731079, Transition Loss -4.816637992858887, Classifier Loss 0.16928474605083466, Total Loss 44.178489685058594\n",
      "13: Encoding Loss 2.1809356212615967, Transition Loss -11.654757499694824, Classifier Loss 0.08714041113853455, Total Loss 26.159194946289062\n",
      "13: Encoding Loss 2.667966365814209, Transition Loss -2.013904571533203, Classifier Loss 0.11830966919660568, Total Loss 33.174293518066406\n",
      "13: Encoding Loss 2.9729416370391846, Transition Loss -11.890885353088379, Classifier Loss 0.11427301168441772, Total Loss 35.208457946777344\n",
      "13: Encoding Loss 2.887364387512207, Transition Loss -10.646392822265625, Classifier Loss 0.06723674386739731, Total Loss 29.82046127319336\n",
      "13: Encoding Loss 3.1694180965423584, Transition Loss -6.223304748535156, Classifier Loss 0.15614363551139832, Total Loss 40.96846389770508\n",
      "13: Encoding Loss 3.288470983505249, Transition Loss -9.750120162963867, Classifier Loss 0.10615305602550507, Total Loss 36.92112350463867\n",
      "13: Encoding Loss 2.750732421875, Transition Loss -4.561652183532715, Classifier Loss 0.09446872770786285, Total Loss 31.451820373535156\n",
      "13: Encoding Loss 3.5382957458496094, Transition Loss -3.856813430786133, Classifier Loss 0.13035902380943298, Total Loss 41.34149932861328\n",
      "13: Encoding Loss 3.831511974334717, Transition Loss -9.681082725524902, Classifier Loss 0.15279936790466309, Total Loss 45.93009567260742\n",
      "13: Encoding Loss 2.4123833179473877, Transition Loss -7.42763614654541, Classifier Loss 0.10017378628253937, Total Loss 29.314958572387695\n",
      "13: Encoding Loss 2.237621307373047, Transition Loss -2.114323377609253, Classifier Loss 0.10072863101959229, Total Loss 27.97340965270996\n",
      "13: Encoding Loss 3.3149592876434326, Transition Loss -14.449015617370605, Classifier Loss 0.12006766349077225, Total Loss 38.5235481262207\n",
      "13: Encoding Loss 2.3116049766540527, Transition Loss -6.171319961547852, Classifier Loss 0.14475734531879425, Total Loss 32.96733856201172\n",
      "13: Encoding Loss 2.5194971561431885, Transition Loss -4.8894500732421875, Classifier Loss 0.09425228834152222, Total Loss 29.580228805541992\n",
      "13: Encoding Loss 4.579737663269043, Transition Loss -9.946228981018066, Classifier Loss 0.11339087784290314, Total Loss 47.975006103515625\n",
      "13: Encoding Loss 2.103902578353882, Transition Loss -19.844526290893555, Classifier Loss 0.10725431144237518, Total Loss 27.552682876586914\n",
      "13: Encoding Loss 2.67761492729187, Transition Loss -4.488574981689453, Classifier Loss 0.14920440316200256, Total Loss 36.3404655456543\n",
      "13: Encoding Loss 2.3323724269866943, Transition Loss -3.9839181900024414, Classifier Loss 0.11341492831707001, Total Loss 29.999675750732422\n",
      "13: Encoding Loss 3.480696678161621, Transition Loss -8.047866821289062, Classifier Loss 0.11867053806781769, Total Loss 39.711021423339844\n",
      "13: Encoding Loss 3.037205219268799, Transition Loss 0.5213379859924316, Classifier Loss 0.11816193163394928, Total Loss 36.218101501464844\n",
      "13: Encoding Loss 2.8001723289489746, Transition Loss -10.519320487976074, Classifier Loss 0.12555783987045288, Total Loss 34.95505905151367\n",
      "13: Encoding Loss 2.179865598678589, Transition Loss -12.671463012695312, Classifier Loss 0.12915948033332825, Total Loss 30.352338790893555\n",
      "13: Encoding Loss 1.3446881771087646, Transition Loss -8.138311386108398, Classifier Loss 0.0975119024515152, Total Loss 20.507070541381836\n",
      "13: Encoding Loss 2.2717983722686768, Transition Loss 0.8387538194656372, Classifier Loss 0.08339642733335495, Total Loss 26.681781768798828\n",
      "13: Encoding Loss 3.8893473148345947, Transition Loss -4.808463096618652, Classifier Loss 0.14709554612636566, Total Loss 45.82337188720703\n",
      "13: Encoding Loss 2.941195249557495, Transition Loss -13.114969253540039, Classifier Loss 0.13513845205307007, Total Loss 37.0407829284668\n",
      "13: Encoding Loss 3.7949299812316895, Transition Loss 3.250955104827881, Classifier Loss 0.15028323233127594, Total Loss 46.03795623779297\n",
      "13: Encoding Loss 3.071709632873535, Transition Loss -4.831457614898682, Classifier Loss 0.10597465187311172, Total Loss 35.1701774597168\n",
      "13: Encoding Loss 3.407958745956421, Transition Loss -1.3338983058929443, Classifier Loss 0.1275942623615265, Total Loss 40.0228271484375\n",
      "13: Encoding Loss 1.8568718433380127, Transition Loss -9.801186561584473, Classifier Loss 0.10409136116504669, Total Loss 25.262149810791016\n",
      "13: Encoding Loss 2.8320326805114746, Transition Loss -12.734216690063477, Classifier Loss 0.12855491042137146, Total Loss 35.50920486450195\n",
      "13: Encoding Loss 2.5394327640533447, Transition Loss -0.04515409469604492, Classifier Loss 0.14560329914093018, Total Loss 34.87578582763672\n",
      "13: Encoding Loss 1.5045092105865479, Transition Loss -3.756199359893799, Classifier Loss 0.07505404949188232, Total Loss 19.540727615356445\n",
      "13: Encoding Loss 2.3598551750183105, Transition Loss -6.204192161560059, Classifier Loss 0.1095830425620079, Total Loss 29.835905075073242\n",
      "13: Encoding Loss 3.502678632736206, Transition Loss -11.64852523803711, Classifier Loss 0.1165064349770546, Total Loss 39.669742584228516\n",
      "13: Encoding Loss 3.1039652824401855, Transition Loss -11.589319229125977, Classifier Loss 0.11982440948486328, Total Loss 36.81184387207031\n",
      "13: Encoding Loss 1.4418731927871704, Transition Loss 1.3320682048797607, Classifier Loss 0.09930836409330368, Total Loss 21.732234954833984\n",
      "13: Encoding Loss 2.165353298187256, Transition Loss -15.01476001739502, Classifier Loss 0.08770014345645905, Total Loss 26.089839935302734\n",
      "13: Encoding Loss 3.078964948654175, Transition Loss -14.930203437805176, Classifier Loss 0.10532526671886444, Total Loss 35.161258697509766\n",
      "13: Encoding Loss 3.0994863510131836, Transition Loss 3.6493608951568604, Classifier Loss 0.09720239043235779, Total Loss 35.246002197265625\n",
      "13: Encoding Loss 3.8989803791046143, Transition Loss 5.473762035369873, Classifier Loss 0.10240544378757477, Total Loss 42.52714157104492\n",
      "13: Encoding Loss 2.965202808380127, Transition Loss -9.025490760803223, Classifier Loss 0.08967069536447525, Total Loss 32.686885833740234\n",
      "13: Encoding Loss 2.042771339416504, Transition Loss -8.512348175048828, Classifier Loss 0.09762060642242432, Total Loss 26.102527618408203\n",
      "13: Encoding Loss 1.7400612831115723, Transition Loss -8.685136795043945, Classifier Loss 0.09560955315828323, Total Loss 23.479707717895508\n",
      "13: Encoding Loss 2.040025472640991, Transition Loss -2.909572124481201, Classifier Loss 0.09641310572624207, Total Loss 25.9609317779541\n",
      "13: Encoding Loss 2.177238941192627, Transition Loss -12.816801071166992, Classifier Loss 0.09815075248479843, Total Loss 27.230422973632812\n",
      "13: Encoding Loss 1.977002501487732, Transition Loss -2.493784189224243, Classifier Loss 0.11126135289669037, Total Loss 26.9416561126709\n",
      "13: Encoding Loss 4.433745861053467, Transition Loss -9.602694511413574, Classifier Loss 0.1360662579536438, Total Loss 49.07467269897461\n",
      "13: Encoding Loss 2.1804072856903076, Transition Loss -17.250503540039062, Classifier Loss 0.11764414608478546, Total Loss 29.204221725463867\n",
      "13: Encoding Loss 2.5977065563201904, Transition Loss -3.684328556060791, Classifier Loss 0.11988860368728638, Total Loss 32.769775390625\n",
      "13: Encoding Loss 3.512303113937378, Transition Loss -8.05735969543457, Classifier Loss 0.15604785084724426, Total Loss 43.70159912109375\n",
      "13: Encoding Loss 3.7737691402435303, Transition Loss -13.293575286865234, Classifier Loss 0.125020369887352, Total Loss 42.68953323364258\n",
      "13: Encoding Loss 1.057719111442566, Transition Loss -7.995870590209961, Classifier Loss 0.06489749997854233, Total Loss 14.94990348815918\n",
      "13: Encoding Loss 2.621098279953003, Transition Loss -9.467859268188477, Classifier Loss 0.10660675913095474, Total Loss 31.6275691986084\n",
      "13: Encoding Loss 1.6353822946548462, Transition Loss -2.1413919925689697, Classifier Loss 0.09503771364688873, Total Loss 22.586400985717773\n",
      "13: Encoding Loss 2.24474835395813, Transition Loss -4.188408851623535, Classifier Loss 0.108242467045784, Total Loss 28.781396865844727\n",
      "13: Encoding Loss 3.623030662536621, Transition Loss -6.562504768371582, Classifier Loss 0.10125277936458588, Total Loss 39.108211517333984\n",
      "13: Encoding Loss 2.083240509033203, Transition Loss -0.4372847080230713, Classifier Loss 0.0903109461069107, Total Loss 25.696929931640625\n",
      "13: Encoding Loss 2.1662073135375977, Transition Loss -8.707780838012695, Classifier Loss 0.07979332655668259, Total Loss 25.307249069213867\n",
      "13: Encoding Loss 2.266561269760132, Transition Loss -9.33543586730957, Classifier Loss 0.11414525657892227, Total Loss 29.545148849487305\n",
      "13: Encoding Loss 3.509434700012207, Transition Loss -10.981619834899902, Classifier Loss 0.12463946640491486, Total Loss 40.53722381591797\n",
      "13: Encoding Loss 3.061645030975342, Transition Loss 3.269918441772461, Classifier Loss 0.15163609385490417, Total Loss 40.31075668334961\n",
      "13: Encoding Loss 2.6597936153411865, Transition Loss -3.672929525375366, Classifier Loss 0.08681923896074295, Total Loss 29.95953941345215\n",
      "13: Encoding Loss 3.1860697269439697, Transition Loss -8.354783058166504, Classifier Loss 0.11771857738494873, Total Loss 37.25874328613281\n",
      "13: Encoding Loss 3.379282236099243, Transition Loss -7.19793701171875, Classifier Loss 0.11640949547290802, Total Loss 38.673770904541016\n",
      "13: Encoding Loss 1.853031873703003, Transition Loss -4.934504985809326, Classifier Loss 0.10246845334768295, Total Loss 25.07011604309082\n",
      "13: Encoding Loss 2.5919861793518066, Transition Loss -0.26956498622894287, Classifier Loss 0.12079446017742157, Total Loss 32.81528091430664\n",
      "13: Encoding Loss 2.2349207401275635, Transition Loss -9.666322708129883, Classifier Loss 0.08023526519536972, Total Loss 25.900959014892578\n",
      "13: Encoding Loss 3.4544830322265625, Transition Loss -0.03575342893600464, Classifier Loss 0.10182268172502518, Total Loss 37.81812286376953\n",
      "13: Encoding Loss 2.067904472351074, Transition Loss -3.8379411697387695, Classifier Loss 0.08277420699596405, Total Loss 24.819889068603516\n",
      "13: Encoding Loss 2.261335849761963, Transition Loss -3.2824578285217285, Classifier Loss 0.11944174766540527, Total Loss 30.03420639038086\n",
      "13: Encoding Loss 2.3234174251556396, Transition Loss -12.096588134765625, Classifier Loss 0.16170130670070648, Total Loss 34.75505065917969\n",
      "13: Encoding Loss 2.8868303298950195, Transition Loss -12.586074829101562, Classifier Loss 0.1508011817932129, Total Loss 38.1722412109375\n",
      "13: Encoding Loss 1.400044322013855, Transition Loss -18.278966903686523, Classifier Loss 0.09238077700138092, Total Loss 20.434776306152344\n",
      "13: Encoding Loss 3.0766711235046387, Transition Loss -8.459607124328613, Classifier Loss 0.13942141830921173, Total Loss 38.55381774902344\n",
      "13: Encoding Loss 2.4444761276245117, Transition Loss -8.143257141113281, Classifier Loss 0.11860200017690659, Total Loss 31.414379119873047\n",
      "13: Encoding Loss 1.9265462160110474, Transition Loss -16.4508056640625, Classifier Loss 0.10922139883041382, Total Loss 26.331220626831055\n",
      "13: Encoding Loss 2.4422271251678467, Transition Loss -2.837661027908325, Classifier Loss 0.14030802249908447, Total Loss 33.568050384521484\n",
      "13: Encoding Loss 2.4999587535858154, Transition Loss -12.83234977722168, Classifier Loss 0.12764033675193787, Total Loss 32.76113510131836\n",
      "13: Encoding Loss 1.9367015361785889, Transition Loss -14.049067497253418, Classifier Loss 0.0703381597995758, Total Loss 22.524620056152344\n",
      "13: Encoding Loss 1.3636730909347534, Transition Loss -16.555477142333984, Classifier Loss 0.11386518180370331, Total Loss 22.292591094970703\n",
      "13: Encoding Loss 3.6914381980895996, Transition Loss -22.853809356689453, Classifier Loss 0.06986953318119049, Total Loss 36.51388931274414\n",
      "14: Encoding Loss 2.0073342323303223, Transition Loss 1.1002755165100098, Classifier Loss 0.06414332985877991, Total Loss 22.69306182861328\n",
      "14: Encoding Loss 2.9559319019317627, Transition Loss -1.1445367336273193, Classifier Loss 0.14901401102542877, Total Loss 38.54862976074219\n",
      "14: Encoding Loss 2.04272723197937, Transition Loss 4.2883710861206055, Classifier Loss 0.0788651555776596, Total Loss 25.08600616455078\n",
      "14: Encoding Loss 1.690090537071228, Transition Loss -15.619306564331055, Classifier Loss 0.11525140702724457, Total Loss 25.042739868164062\n",
      "14: Encoding Loss 2.5247740745544434, Transition Loss -5.68881893157959, Classifier Loss 0.08763834089040756, Total Loss 28.96088981628418\n",
      "14: Encoding Loss 1.4652936458587646, Transition Loss -12.383614540100098, Classifier Loss 0.1071767657995224, Total Loss 22.43754768371582\n",
      "14: Encoding Loss 2.2177822589874268, Transition Loss -7.926373481750488, Classifier Loss 0.11054681986570358, Total Loss 28.79535484313965\n",
      "14: Encoding Loss 1.632726788520813, Transition Loss -5.209691047668457, Classifier Loss 0.08755379170179367, Total Loss 21.816152572631836\n",
      "14: Encoding Loss 2.7821438312530518, Transition Loss -12.930072784423828, Classifier Loss 0.10050181299448013, Total Loss 32.304744720458984\n",
      "14: Encoding Loss 1.812923550605774, Transition Loss -8.195748329162598, Classifier Loss 0.1145944595336914, Total Loss 25.961196899414062\n",
      "14: Encoding Loss 4.189583778381348, Transition Loss -5.8792524337768555, Classifier Loss 0.10915657132863998, Total Loss 44.43115234375\n",
      "14: Encoding Loss 1.9970282316207886, Transition Loss -0.3632691502571106, Classifier Loss 0.11304808408021927, Total Loss 27.280961990356445\n",
      "14: Encoding Loss 2.5618016719818115, Transition Loss -4.725032806396484, Classifier Loss 0.08861127495765686, Total Loss 29.354597091674805\n",
      "14: Encoding Loss 1.7892149686813354, Transition Loss -3.7019245624542236, Classifier Loss 0.11154460161924362, Total Loss 25.467439651489258\n",
      "14: Encoding Loss 0.998232901096344, Transition Loss -12.302000045776367, Classifier Loss 0.10028079152107239, Total Loss 18.01148223876953\n",
      "14: Encoding Loss 1.9703869819641113, Transition Loss -17.169466018676758, Classifier Loss 0.09595043212175369, Total Loss 25.354705810546875\n",
      "14: Encoding Loss 1.9606939554214478, Transition Loss -5.6826171875, Classifier Loss 0.06510882824659348, Total Loss 22.195297241210938\n",
      "14: Encoding Loss 2.6912357807159424, Transition Loss -3.1023950576782227, Classifier Loss 0.09837856888771057, Total Loss 31.367124557495117\n",
      "14: Encoding Loss 3.91243052482605, Transition Loss -8.826050758361816, Classifier Loss 0.14350683987140656, Total Loss 45.64836120605469\n",
      "14: Encoding Loss 2.8676023483276367, Transition Loss -2.860024929046631, Classifier Loss 0.09411917626857758, Total Loss 32.35216522216797\n",
      "14: Encoding Loss 2.3820769786834717, Transition Loss -11.934311866760254, Classifier Loss 0.1214500442147255, Total Loss 31.199234008789062\n",
      "14: Encoding Loss 1.7942917346954346, Transition Loss -7.43708610534668, Classifier Loss 0.09298393875360489, Total Loss 23.651241302490234\n",
      "14: Encoding Loss 2.1437900066375732, Transition Loss -11.713552474975586, Classifier Loss 0.1281166523694992, Total Loss 29.959644317626953\n",
      "14: Encoding Loss 1.466583251953125, Transition Loss -0.7267286777496338, Classifier Loss 0.12964043021202087, Total Loss 24.696563720703125\n",
      "14: Encoding Loss 1.8982956409454346, Transition Loss -8.265932083129883, Classifier Loss 0.08033803850412369, Total Loss 23.218515396118164\n",
      "14: Encoding Loss 2.0023396015167236, Transition Loss -1.1366634368896484, Classifier Loss 0.10558797419071198, Total Loss 26.577287673950195\n",
      "14: Encoding Loss 2.926943063735962, Transition Loss -0.1561189889907837, Classifier Loss 0.13527996838092804, Total Loss 36.943511962890625\n",
      "14: Encoding Loss 1.3819550275802612, Transition Loss -16.655841827392578, Classifier Loss 0.12406674027442932, Total Loss 23.458984375\n",
      "14: Encoding Loss 0.6767153143882751, Transition Loss -6.646694183349609, Classifier Loss 0.10119983553886414, Total Loss 15.532377243041992\n",
      "14: Encoding Loss 2.3080780506134033, Transition Loss -1.7770051956176758, Classifier Loss 0.10686975717544556, Total Loss 29.1512451171875\n",
      "14: Encoding Loss 2.690523624420166, Transition Loss -8.49707317352295, Classifier Loss 0.1236429288983345, Total Loss 33.886783599853516\n",
      "14: Encoding Loss 2.2093241214752197, Transition Loss -0.09156811237335205, Classifier Loss 0.13497759401798248, Total Loss 31.172332763671875\n",
      "14: Encoding Loss 3.3355886936187744, Transition Loss -8.389177322387695, Classifier Loss 0.12053211778402328, Total Loss 38.736244201660156\n",
      "14: Encoding Loss 3.4059038162231445, Transition Loss -5.085874557495117, Classifier Loss 0.1992221623659134, Total Loss 47.168426513671875\n",
      "14: Encoding Loss 1.2832105159759521, Transition Loss -10.060623168945312, Classifier Loss 0.09545377641916275, Total Loss 19.809049606323242\n",
      "14: Encoding Loss 1.0702401399612427, Transition Loss -3.758998394012451, Classifier Loss 0.07122591137886047, Total Loss 15.683760643005371\n",
      "14: Encoding Loss 3.210275411605835, Transition Loss -6.144892692565918, Classifier Loss 0.14039967954158783, Total Loss 39.720943450927734\n",
      "14: Encoding Loss 2.442913293838501, Transition Loss -6.6681976318359375, Classifier Loss 0.14799551665782928, Total Loss 34.341522216796875\n",
      "14: Encoding Loss 1.9733489751815796, Transition Loss -6.044602870941162, Classifier Loss 0.11482804268598557, Total Loss 27.268386840820312\n",
      "14: Encoding Loss 1.493347406387329, Transition Loss -7.809439182281494, Classifier Loss 0.1223972737789154, Total Loss 24.18494415283203\n",
      "14: Encoding Loss 0.6154918670654297, Transition Loss -5.224258899688721, Classifier Loss 0.07612871378660202, Total Loss 12.535760879516602\n",
      "14: Encoding Loss 2.510640859603882, Transition Loss -12.218188285827637, Classifier Loss 0.13275763392448425, Total Loss 33.35844421386719\n",
      "14: Encoding Loss 1.002224326133728, Transition Loss -12.12460994720459, Classifier Loss 0.08260331302881241, Total Loss 16.27570152282715\n",
      "14: Encoding Loss 1.9135589599609375, Transition Loss -1.2341461181640625, Classifier Loss 0.12229040265083313, Total Loss 27.53726577758789\n",
      "14: Encoding Loss 1.8357661962509155, Transition Loss -13.66546630859375, Classifier Loss 0.09370749443769455, Total Loss 24.05414581298828\n",
      "14: Encoding Loss 3.1843926906585693, Transition Loss -3.419586181640625, Classifier Loss 0.11875475943088531, Total Loss 37.34993362426758\n",
      "14: Encoding Loss 3.952316999435425, Transition Loss -13.478501319885254, Classifier Loss 0.12143988907337189, Total Loss 43.75982666015625\n",
      "14: Encoding Loss 0.5434890389442444, Transition Loss -2.828510284423828, Classifier Loss 0.08104436099529266, Total Loss 12.451783180236816\n",
      "14: Encoding Loss 2.3880774974823, Transition Loss -6.022392749786377, Classifier Loss 0.11405254155397415, Total Loss 30.508670806884766\n",
      "14: Encoding Loss 1.9953463077545166, Transition Loss -13.343085289001465, Classifier Loss 0.09799704700708389, Total Loss 25.75980567932129\n",
      "14: Encoding Loss 1.522620439529419, Transition Loss -16.9862060546875, Classifier Loss 0.08940330892801285, Total Loss 21.117897033691406\n",
      "14: Encoding Loss 2.5983405113220215, Transition Loss -9.560233116149902, Classifier Loss 0.1151360422372818, Total Loss 32.29841995239258\n",
      "14: Encoding Loss 0.7305468916893005, Transition Loss 0.8814042806625366, Classifier Loss 0.1353418231010437, Total Loss 19.554838180541992\n",
      "14: Encoding Loss 1.7314666509628296, Transition Loss -9.532214164733887, Classifier Loss 0.09620645642280579, Total Loss 23.470470428466797\n",
      "14: Encoding Loss 1.5793817043304443, Transition Loss -8.167874336242676, Classifier Loss 0.10099738836288452, Total Loss 22.73316192626953\n",
      "14: Encoding Loss 1.7315160036087036, Transition Loss -0.46886467933654785, Classifier Loss 0.12347586452960968, Total Loss 26.199621200561523\n",
      "14: Encoding Loss 2.035672187805176, Transition Loss -7.75973653793335, Classifier Loss 0.08168820291757584, Total Loss 24.45264434814453\n",
      "14: Encoding Loss 2.1790969371795654, Transition Loss -9.533012390136719, Classifier Loss 0.108214370906353, Total Loss 28.252304077148438\n",
      "14: Encoding Loss 1.056715488433838, Transition Loss -15.893880844116211, Classifier Loss 0.09973764419555664, Total Loss 18.42430877685547\n",
      "14: Encoding Loss 1.677405834197998, Transition Loss -11.724742889404297, Classifier Loss 0.11313141882419586, Total Loss 24.730045318603516\n",
      "14: Encoding Loss 1.9352184534072876, Transition Loss -6.591940879821777, Classifier Loss 0.09779830276966095, Total Loss 25.2602596282959\n",
      "14: Encoding Loss 2.1271767616271973, Transition Loss -4.6194071769714355, Classifier Loss 0.1524457037448883, Total Loss 32.26106262207031\n",
      "14: Encoding Loss 1.514398217201233, Transition Loss -7.876045227050781, Classifier Loss 0.11016693711280823, Total Loss 23.13030433654785\n",
      "14: Encoding Loss 1.1902602910995483, Transition Loss -4.27100944519043, Classifier Loss 0.10876692831516266, Total Loss 20.397920608520508\n",
      "14: Encoding Loss 0.7393252849578857, Transition Loss -8.894665718078613, Classifier Loss 0.11262810230255127, Total Loss 17.17563247680664\n",
      "14: Encoding Loss 1.6148310899734497, Transition Loss -1.110670566558838, Classifier Loss 0.17531700432300568, Total Loss 30.45012855529785\n",
      "14: Encoding Loss 2.3166894912719727, Transition Loss 3.645998001098633, Classifier Loss 0.09820973873138428, Total Loss 29.083690643310547\n",
      "14: Encoding Loss 2.019249200820923, Transition Loss -14.45223617553711, Classifier Loss 0.12613889575004578, Total Loss 28.76499366760254\n",
      "14: Encoding Loss 2.7294745445251465, Transition Loss -3.4491701126098633, Classifier Loss 0.1087690144777298, Total Loss 32.71200942993164\n",
      "14: Encoding Loss 3.3672006130218506, Transition Loss -5.7212677001953125, Classifier Loss 0.15294435620307922, Total Loss 42.23089599609375\n",
      "14: Encoding Loss 1.448723554611206, Transition Loss -5.059858322143555, Classifier Loss 0.12699784338474274, Total Loss 24.28856086730957\n",
      "14: Encoding Loss 1.1647313833236694, Transition Loss -5.752730369567871, Classifier Loss 0.1175757497549057, Total Loss 21.074275970458984\n",
      "14: Encoding Loss 3.1670024394989014, Transition Loss -1.4945448637008667, Classifier Loss 0.14413222670555115, Total Loss 39.74894332885742\n",
      "14: Encoding Loss 1.290624976158142, Transition Loss -5.584820747375488, Classifier Loss 0.0664065033197403, Total Loss 16.96453285217285\n",
      "14: Encoding Loss 1.7109646797180176, Transition Loss -3.1676955223083496, Classifier Loss 0.1683157980442047, Total Loss 30.518661499023438\n",
      "14: Encoding Loss 0.9826586246490479, Transition Loss -10.073750495910645, Classifier Loss 0.08423130214214325, Total Loss 16.282384872436523\n",
      "14: Encoding Loss 1.1480698585510254, Transition Loss -1.0575157403945923, Classifier Loss 0.11932972073554993, Total Loss 21.117319107055664\n",
      "14: Encoding Loss 1.1556929349899292, Transition Loss -9.24771785736084, Classifier Loss 0.11609025299549103, Total Loss 20.852718353271484\n",
      "14: Encoding Loss 1.3130412101745605, Transition Loss -8.777444839477539, Classifier Loss 0.06894343346357346, Total Loss 17.39691925048828\n",
      "14: Encoding Loss 1.90411376953125, Transition Loss -5.077396392822266, Classifier Loss 0.15357428789138794, Total Loss 30.589324951171875\n",
      "14: Encoding Loss 1.9260218143463135, Transition Loss -7.589610576629639, Classifier Loss 0.10185786336660385, Total Loss 25.59244155883789\n",
      "14: Encoding Loss 1.199622631072998, Transition Loss -3.3832430839538574, Classifier Loss 0.09279227256774902, Total Loss 18.875532150268555\n",
      "14: Encoding Loss 2.10646915435791, Transition Loss -2.153646469116211, Classifier Loss 0.12424977123737335, Total Loss 29.27629852294922\n",
      "14: Encoding Loss 2.3347930908203125, Transition Loss -8.039106369018555, Classifier Loss 0.1497359573841095, Total Loss 33.650333404541016\n",
      "14: Encoding Loss 0.5919211506843567, Transition Loss -5.377534866333008, Classifier Loss 0.09730343520641327, Total Loss 14.464637756347656\n",
      "14: Encoding Loss 0.7650666832923889, Transition Loss -0.629417896270752, Classifier Loss 0.09550081938505173, Total Loss 15.670490264892578\n",
      "14: Encoding Loss 1.4793154001235962, Transition Loss -12.231019020080566, Classifier Loss 0.12685394287109375, Total Loss 24.51746940612793\n",
      "14: Encoding Loss 0.9741666913032532, Transition Loss -4.832986831665039, Classifier Loss 0.1413554698228836, Total Loss 21.927913665771484\n",
      "14: Encoding Loss 1.5728442668914795, Transition Loss -3.5597243309020996, Classifier Loss 0.08953173458576202, Total Loss 21.53521728515625\n",
      "14: Encoding Loss 3.5353174209594727, Transition Loss -7.71108865737915, Classifier Loss 0.11111828684806824, Total Loss 39.392826080322266\n",
      "14: Encoding Loss 0.07820413261651993, Transition Loss -17.103334426879883, Classifier Loss 0.1069364994764328, Total Loss 11.18004035949707\n",
      "14: Encoding Loss 1.3039599657058716, Transition Loss -3.7971935272216797, Classifier Loss 0.14826487004756927, Total Loss 25.257408142089844\n",
      "14: Encoding Loss 0.6871219873428345, Transition Loss -3.046980857849121, Classifier Loss 0.12239691615104675, Total Loss 17.736059188842773\n",
      "14: Encoding Loss 2.1015100479125977, Transition Loss -6.738020896911621, Classifier Loss 0.11958863586187363, Total Loss 28.769594192504883\n",
      "14: Encoding Loss 1.7572448253631592, Transition Loss 0.8664740324020386, Classifier Loss 0.1100916713476181, Total Loss 25.240421295166016\n",
      "14: Encoding Loss 1.2569972276687622, Transition Loss -8.453343391418457, Classifier Loss 0.1186007410287857, Total Loss 21.91436195373535\n",
      "14: Encoding Loss 0.7612618803977966, Transition Loss -10.83875846862793, Classifier Loss 0.12273536622524261, Total Loss 18.36146354675293\n",
      "14: Encoding Loss 0.02586333639919758, Transition Loss -7.506355285644531, Classifier Loss 0.09352414309978485, Total Loss 9.475479125976562\n",
      "14: Encoding Loss 1.176628828048706, Transition Loss 1.616713523864746, Classifier Loss 0.07703062891960144, Total Loss 17.439437866210938\n",
      "14: Encoding Loss 1.8988556861877441, Transition Loss -3.2461516857147217, Classifier Loss 0.14503782987594604, Total Loss 29.693979263305664\n",
      "14: Encoding Loss 1.434569001197815, Transition Loss -10.702445983886719, Classifier Loss 0.1293843537569046, Total Loss 24.4128475189209\n",
      "14: Encoding Loss 2.6029398441314697, Transition Loss 3.7603728771209717, Classifier Loss 0.14803971350193024, Total Loss 36.37956619262695\n",
      "14: Encoding Loss 1.7417566776275635, Transition Loss -3.1999423503875732, Classifier Loss 0.10389786958694458, Total Loss 24.323200225830078\n",
      "14: Encoding Loss 2.1219048500061035, Transition Loss -0.4607429504394531, Classifier Loss 0.12518185377120972, Total Loss 29.493331909179688\n",
      "14: Encoding Loss 0.6605854034423828, Transition Loss -8.435702323913574, Classifier Loss 0.09820562601089478, Total Loss 15.103558540344238\n",
      "14: Encoding Loss 1.2970852851867676, Transition Loss -10.488395690917969, Classifier Loss 0.1299605816602707, Total Loss 23.370643615722656\n",
      "14: Encoding Loss 1.2289363145828247, Transition Loss 0.6499912142753601, Classifier Loss 0.14371076226234436, Total Loss 24.33256721496582\n",
      "14: Encoding Loss 0.3162391483783722, Transition Loss -2.6820521354675293, Classifier Loss 0.07172615826129913, Total Loss 9.700014114379883\n",
      "14: Encoding Loss 0.6101433038711548, Transition Loss -4.978069305419922, Classifier Loss 0.10911699384450912, Total Loss 15.791850090026855\n",
      "14: Encoding Loss 2.3517327308654785, Transition Loss -10.644497871398926, Classifier Loss 0.1147100031375885, Total Loss 30.282733917236328\n",
      "14: Encoding Loss 1.3549854755401611, Transition Loss -9.604351043701172, Classifier Loss 0.11694461107254028, Total Loss 22.53242301940918\n",
      "14: Encoding Loss 0.13897228240966797, Transition Loss 2.398411750793457, Classifier Loss 0.0875692144036293, Total Loss 10.256875038146973\n",
      "14: Encoding Loss 0.4228966534137726, Transition Loss -12.80030632019043, Classifier Loss 0.08562994003295898, Total Loss 11.943568229675293\n",
      "14: Encoding Loss 1.5509955883026123, Transition Loss -12.277714729309082, Classifier Loss 0.09986838698387146, Total Loss 22.392349243164062\n",
      "14: Encoding Loss 1.8606698513031006, Transition Loss 3.7323577404022217, Classifier Loss 0.09616619348526001, Total Loss 25.248451232910156\n",
      "14: Encoding Loss 2.165015935897827, Transition Loss 7.09361457824707, Classifier Loss 0.09297772496938705, Total Loss 28.036623001098633\n",
      "14: Encoding Loss 1.4854058027267456, Transition Loss -7.402634620666504, Classifier Loss 0.08984674513339996, Total Loss 20.866439819335938\n",
      "14: Encoding Loss 0.7488813996315002, Transition Loss -7.036988258361816, Classifier Loss 0.0985850840806961, Total Loss 15.848152160644531\n",
      "14: Encoding Loss 0.11102262139320374, Transition Loss -6.810537815093994, Classifier Loss 0.08981669694185257, Total Loss 9.749959945678711\n",
      "14: Encoding Loss 0.776385486125946, Transition Loss -2.4790167808532715, Classifier Loss 0.09564949572086334, Total Loss 15.775537490844727\n",
      "14: Encoding Loss 0.5979265570640564, Transition Loss -10.295957565307617, Classifier Loss 0.09114527702331543, Total Loss 13.895880699157715\n",
      "14: Encoding Loss 0.5871500372886658, Transition Loss -2.3475100994110107, Classifier Loss 0.10820671916007996, Total Loss 15.517402648925781\n",
      "14: Encoding Loss 2.812255382537842, Transition Loss -7.28621768951416, Classifier Loss 0.1325884610414505, Total Loss 35.75543212890625\n",
      "14: Encoding Loss 0.8464680314064026, Transition Loss -14.814583778381348, Classifier Loss 0.12168951332569122, Total Loss 18.937734603881836\n",
      "14: Encoding Loss 0.9718537926673889, Transition Loss -1.9522535800933838, Classifier Loss 0.1153746098279953, Total Loss 19.311901092529297\n",
      "14: Encoding Loss 1.7631205320358276, Transition Loss -6.718088626861572, Classifier Loss 0.15737216174602509, Total Loss 29.840837478637695\n",
      "14: Encoding Loss 1.7545552253723145, Transition Loss -11.505763053894043, Classifier Loss 0.12613841891288757, Total Loss 26.64798355102539\n",
      "14: Encoding Loss -0.2930549383163452, Transition Loss -6.5401506423950195, Classifier Loss 0.06732241809368134, Total Loss 6.7269673347473145\n",
      "14: Encoding Loss 1.1081836223602295, Transition Loss -8.399578094482422, Classifier Loss 0.10369154810905457, Total Loss 19.23294448852539\n",
      "14: Encoding Loss 0.2605055868625641, Transition Loss -0.8540127277374268, Classifier Loss 0.09095007181167603, Total Loss 11.16930866241455\n",
      "14: Encoding Loss 1.1514049768447876, Transition Loss -3.2817463874816895, Classifier Loss 0.1073509082198143, Total Loss 19.945674896240234\n",
      "14: Encoding Loss 1.9449342489242554, Transition Loss -4.8612494468688965, Classifier Loss 0.09708067774772644, Total Loss 25.266569137573242\n",
      "14: Encoding Loss 0.7461450099945068, Transition Loss 0.19267761707305908, Classifier Loss 0.09018716961145401, Total Loss 15.026412010192871\n",
      "14: Encoding Loss 0.975000262260437, Transition Loss -7.2792205810546875, Classifier Loss 0.07775884866714478, Total Loss 15.574431419372559\n",
      "14: Encoding Loss 0.4814917743206024, Transition Loss -7.296370983123779, Classifier Loss 0.1087748184800148, Total Loss 14.727953910827637\n",
      "14: Encoding Loss 2.0139975547790527, Transition Loss -9.164163589477539, Classifier Loss 0.11864805966615677, Total Loss 27.97495460510254\n",
      "14: Encoding Loss 1.517223834991455, Transition Loss 3.937248706817627, Classifier Loss 0.15298673510551453, Total Loss 28.223913192749023\n",
      "14: Encoding Loss 1.3376463651657104, Transition Loss -2.3576807975769043, Classifier Loss 0.08425641059875488, Total Loss 19.126340866088867\n",
      "14: Encoding Loss 2.1811063289642334, Transition Loss -6.987685680389404, Classifier Loss 0.11375483125448227, Total Loss 28.822935104370117\n",
      "14: Encoding Loss 2.0186269283294678, Transition Loss -5.377314567565918, Classifier Loss 0.11516129970550537, Total Loss 27.66407012939453\n",
      "14: Encoding Loss 0.37311094999313354, Transition Loss -3.3145246505737305, Classifier Loss 0.10149729996919632, Total Loss 13.13366985321045\n",
      "14: Encoding Loss 1.0541319847106934, Transition Loss 0.49429428577423096, Classifier Loss 0.11769884079694748, Total Loss 20.301799774169922\n",
      "14: Encoding Loss 0.51003497838974, Transition Loss -7.400054931640625, Classifier Loss 0.0768444687128067, Total Loss 11.763246536254883\n",
      "14: Encoding Loss 2.238513708114624, Transition Loss 0.4500298798084259, Classifier Loss 0.09995827823877335, Total Loss 27.99394416809082\n",
      "14: Encoding Loss 1.0175492763519287, Transition Loss -2.7798912525177, Classifier Loss 0.08276307582855225, Total Loss 16.416147232055664\n",
      "14: Encoding Loss 1.0540614128112793, Transition Loss -2.0766987800598145, Classifier Loss 0.11040326207876205, Total Loss 19.472400665283203\n",
      "14: Encoding Loss 1.5697804689407349, Transition Loss -10.717044830322266, Classifier Loss 0.16278746724128723, Total Loss 28.83484649658203\n",
      "14: Encoding Loss 1.232816457748413, Transition Loss -10.442593574523926, Classifier Loss 0.14758595824241638, Total Loss 24.61903953552246\n",
      "14: Encoding Loss -0.3626900613307953, Transition Loss -15.88620376586914, Classifier Loss 0.0899958610534668, Total Loss 8.995992660522461\n",
      "14: Encoding Loss 1.9840121269226074, Transition Loss -7.304506778717041, Classifier Loss 0.13455215096473694, Total Loss 29.325851440429688\n",
      "14: Encoding Loss 1.0501078367233276, Transition Loss -6.630031585693359, Classifier Loss 0.11676815897226334, Total Loss 20.076353073120117\n",
      "14: Encoding Loss 0.2667039930820465, Transition Loss -14.219644546508789, Classifier Loss 0.11066044867038727, Total Loss 13.188669204711914\n",
      "14: Encoding Loss 0.7579225897789001, Transition Loss -1.4706170558929443, Classifier Loss 0.1366950422525406, Total Loss 19.73259162902832\n",
      "14: Encoding Loss 1.2619953155517578, Transition Loss -11.491279602050781, Classifier Loss 0.13141053915023804, Total Loss 23.234718322753906\n",
      "14: Encoding Loss 0.5944015979766846, Transition Loss -12.107146263122559, Classifier Loss 0.0676509216427803, Total Loss 11.517884254455566\n",
      "14: Encoding Loss 0.5025219321250916, Transition Loss -15.127935409545898, Classifier Loss 0.11243389546871185, Total Loss 15.260537147521973\n",
      "14: Encoding Loss 1.238567590713501, Transition Loss -19.46408462524414, Classifier Loss 0.06685119867324829, Total Loss 16.589767456054688\n",
      "15: Encoding Loss 0.8173539638519287, Transition Loss 1.0623376369476318, Classifier Loss 0.06342844665050507, Total Loss 13.094143867492676\n",
      "15: Encoding Loss 1.2041529417037964, Transition Loss 0.5283749103546143, Classifier Loss 0.15034259855747223, Total Loss 24.77315902709961\n",
      "15: Encoding Loss 0.6679365634918213, Transition Loss 4.9050493240356445, Classifier Loss 0.07672008872032166, Total Loss 13.99651050567627\n",
      "15: Encoding Loss 0.11777558922767639, Transition Loss -13.611358642578125, Classifier Loss 0.11540406942367554, Total Loss 12.367345809936523\n",
      "15: Encoding Loss 1.0016663074493408, Transition Loss -3.683547258377075, Classifier Loss 0.08152793347835541, Total Loss 16.165386199951172\n",
      "15: Encoding Loss 0.03538723662495613, Transition Loss -10.849081993103027, Classifier Loss 0.10592503845691681, Total Loss 10.77103042602539\n",
      "15: Encoding Loss 0.6340907216072083, Transition Loss -6.436511039733887, Classifier Loss 0.10752913355827332, Total Loss 15.824352264404297\n",
      "15: Encoding Loss 0.33947813510894775, Transition Loss -4.641627311706543, Classifier Loss 0.09020078182220459, Total Loss 11.73404312133789\n",
      "15: Encoding Loss 1.7445746660232544, Transition Loss -10.78825569152832, Classifier Loss 0.09664864838123322, Total Loss 23.619306564331055\n",
      "15: Encoding Loss 0.17850551009178162, Transition Loss -6.800946235656738, Classifier Loss 0.11847437918186188, Total Loss 13.221104621887207\n",
      "15: Encoding Loss 3.0609028339385986, Transition Loss -4.239072799682617, Classifier Loss 0.10623325407505035, Total Loss 35.109703063964844\n",
      "15: Encoding Loss 0.8939759731292725, Transition Loss -0.23352313041687012, Classifier Loss 0.11153746396303177, Total Loss 18.305509567260742\n",
      "15: Encoding Loss 1.0819847583770752, Transition Loss -3.452620267868042, Classifier Loss 0.08684930950403214, Total Loss 17.340118408203125\n",
      "15: Encoding Loss 0.5928170084953308, Transition Loss -2.4632105827331543, Classifier Loss 0.1064811497926712, Total Loss 15.390158653259277\n",
      "15: Encoding Loss -0.6297483444213867, Transition Loss -10.619497299194336, Classifier Loss 0.09899857640266418, Total Loss 9.897733688354492\n",
      "15: Encoding Loss 0.5744651556015015, Transition Loss -15.1840181350708, Classifier Loss 0.09327173978090286, Total Loss 13.919858932495117\n",
      "15: Encoding Loss 0.6209176182746887, Transition Loss -3.9083824157714844, Classifier Loss 0.06273511797189713, Total Loss 11.240070343017578\n",
      "15: Encoding Loss 1.5524177551269531, Transition Loss -1.756227970123291, Classifier Loss 0.09726372361183167, Total Loss 22.145362854003906\n",
      "15: Encoding Loss 2.7877395153045654, Transition Loss -7.865619659423828, Classifier Loss 0.1415640264749527, Total Loss 36.456748962402344\n",
      "15: Encoding Loss 1.669094204902649, Transition Loss -1.7013392448425293, Classifier Loss 0.08986195921897888, Total Loss 22.338611602783203\n",
      "15: Encoding Loss 1.432005524635315, Transition Loss -10.393470764160156, Classifier Loss 0.11345558613538742, Total Loss 22.799522399902344\n",
      "15: Encoding Loss 0.5095421671867371, Transition Loss -5.95283317565918, Classifier Loss 0.09118511527776718, Total Loss 13.193657875061035\n",
      "15: Encoding Loss 0.49208322167396545, Transition Loss -9.407122611999512, Classifier Loss 0.13358643651008606, Total Loss 17.293426513671875\n",
      "15: Encoding Loss 0.42088568210601807, Transition Loss 0.004878520965576172, Classifier Loss 0.1311638355255127, Total Loss 16.48440170288086\n",
      "15: Encoding Loss 0.2612641155719757, Transition Loss -7.036585330963135, Classifier Loss 0.07540179044008255, Total Loss 9.619495391845703\n",
      "15: Encoding Loss 0.6988620758056641, Transition Loss -0.4370541572570801, Classifier Loss 0.09866635501384735, Total Loss 15.457444190979004\n",
      "15: Encoding Loss 1.1578232049942017, Transition Loss 0.9467277526855469, Classifier Loss 0.1256696581840515, Total Loss 22.018898010253906\n",
      "15: Encoding Loss -0.5761434435844421, Transition Loss -14.4962158203125, Classifier Loss 0.12339481711387634, Total Loss 12.33658218383789\n",
      "15: Encoding Loss -0.9355965852737427, Transition Loss -5.631911754608154, Classifier Loss 0.10268427431583405, Total Loss 10.267301559448242\n",
      "15: Encoding Loss 0.8814643621444702, Transition Loss -1.2695915699005127, Classifier Loss 0.10694403201341629, Total Loss 17.745864868164062\n",
      "15: Encoding Loss 1.292895793914795, Transition Loss -6.95145320892334, Classifier Loss 0.12558338046073914, Total Loss 22.900114059448242\n",
      "15: Encoding Loss 0.8392106890678406, Transition Loss 0.09839332103729248, Classifier Loss 0.13165263831615448, Total Loss 19.89862823486328\n",
      "15: Encoding Loss 1.9774843454360962, Transition Loss -7.015279293060303, Classifier Loss 0.11595507711172104, Total Loss 27.413978576660156\n",
      "15: Encoding Loss 1.9512087106704712, Transition Loss -4.113593578338623, Classifier Loss 0.20234544575214386, Total Loss 35.84339141845703\n",
      "15: Encoding Loss 0.007246685214340687, Transition Loss -8.394327163696289, Classifier Loss 0.09839137643575668, Total Loss 9.868121147155762\n",
      "15: Encoding Loss -0.019169429317116737, Transition Loss -3.074138641357422, Classifier Loss 0.06872215121984482, Total Loss 6.806579113006592\n",
      "15: Encoding Loss 1.1046916246414185, Transition Loss -4.840524673461914, Classifier Loss 0.13397371768951416, Total Loss 22.233936309814453\n",
      "15: Encoding Loss 1.3769556283950806, Transition Loss -6.349956512451172, Classifier Loss 0.14496460556983948, Total Loss 25.510835647583008\n",
      "15: Encoding Loss 0.5136569142341614, Transition Loss -4.733341693878174, Classifier Loss 0.11356880515813828, Total Loss 15.465188026428223\n",
      "15: Encoding Loss 0.022723780944943428, Transition Loss -6.3994340896606445, Classifier Loss 0.11562156677246094, Total Loss 11.668111801147461\n",
      "15: Encoding Loss -0.5436691045761108, Transition Loss -4.3700127601623535, Classifier Loss 0.07646292448043823, Total Loss 7.645418643951416\n",
      "15: Encoding Loss 0.6572145223617554, Transition Loss -11.152666091918945, Classifier Loss 0.1355043202638626, Total Loss 18.805919647216797\n",
      "15: Encoding Loss -0.23951014876365662, Transition Loss -10.518415451049805, Classifier Loss 0.07475880533456802, Total Loss 7.457858085632324\n",
      "15: Encoding Loss 0.6380689144134521, Transition Loss -0.44951921701431274, Classifier Loss 0.11535979062318802, Total Loss 16.640439987182617\n",
      "15: Encoding Loss 0.37164440751075745, Transition Loss -11.968024253845215, Classifier Loss 0.09276138991117477, Total Loss 12.246600151062012\n",
      "15: Encoding Loss 1.9365462064743042, Transition Loss -2.9126791954040527, Classifier Loss 0.11861260235309601, Total Loss 27.35304832458496\n",
      "15: Encoding Loss 2.77536678314209, Transition Loss -11.132004737854004, Classifier Loss 0.1191093921661377, Total Loss 34.11164474487305\n",
      "15: Encoding Loss -0.3890206515789032, Transition Loss -2.0309300422668457, Classifier Loss 0.07451760023832321, Total Loss 7.451197624206543\n",
      "15: Encoding Loss 1.0957306623458862, Transition Loss -3.7443718910217285, Classifier Loss 0.10373110324144363, Total Loss 19.138206481933594\n",
      "15: Encoding Loss 0.8530673384666443, Transition Loss -11.326166152954102, Classifier Loss 0.0943072959780693, Total Loss 16.253002166748047\n",
      "15: Encoding Loss 0.3698405623435974, Transition Loss -15.151177406311035, Classifier Loss 0.08670013397932053, Total Loss 11.625387191772461\n",
      "15: Encoding Loss 1.444577932357788, Transition Loss -7.8125200271606445, Classifier Loss 0.10878373682498932, Total Loss 22.433435440063477\n",
      "15: Encoding Loss -0.31417614221572876, Transition Loss 0.9935641288757324, Classifier Loss 0.12772700190544128, Total Loss 12.969303131103516\n",
      "15: Encoding Loss 0.05172134190797806, Transition Loss -7.518970012664795, Classifier Loss 0.09480193257331848, Total Loss 9.767292976379395\n",
      "15: Encoding Loss 0.25702229142189026, Transition Loss -7.023905277252197, Classifier Loss 0.10037226974964142, Total Loss 12.081551551818848\n",
      "15: Encoding Loss 0.3048194646835327, Transition Loss 0.058208346366882324, Classifier Loss 0.11609293520450592, Total Loss 14.056684494018555\n",
      "15: Encoding Loss 1.1213241815567017, Transition Loss -6.837075710296631, Classifier Loss 0.07910469174385071, Total Loss 16.87969398498535\n",
      "15: Encoding Loss 0.3413265645503998, Transition Loss -7.8729681968688965, Classifier Loss 0.1001284271478653, Total Loss 12.741004943847656\n",
      "15: Encoding Loss -0.31119224429130554, Transition Loss -14.781142234802246, Classifier Loss 0.10037156939506531, Total Loss 10.03188705444336\n",
      "15: Encoding Loss 0.03322112187743187, Transition Loss -10.410510063171387, Classifier Loss 0.11221574246883392, Total Loss 11.386961936950684\n",
      "15: Encoding Loss 0.35913264751434326, Transition Loss -5.22548246383667, Classifier Loss 0.09742534160614014, Total Loss 12.614078521728516\n",
      "15: Encoding Loss 0.3711042106151581, Transition Loss -3.5536320209503174, Classifier Loss 0.14513088762760162, Total Loss 17.480905532836914\n",
      "15: Encoding Loss -0.043350979685783386, Transition Loss -6.41461181640625, Classifier Loss 0.10896095633506775, Total Loss 10.779561042785645\n",
      "15: Encoding Loss 0.019856803119182587, Transition Loss -3.945509195327759, Classifier Loss 0.09972448647022247, Total Loss 10.063589096069336\n",
      "15: Encoding Loss -0.4467450976371765, Transition Loss -8.283273696899414, Classifier Loss 0.10547299683094025, Total Loss 10.545628547668457\n",
      "15: Encoding Loss 0.015224114060401917, Transition Loss -0.5962727069854736, Classifier Loss 0.17392149567604065, Total Loss 17.460294723510742\n",
      "15: Encoding Loss 1.4768621921539307, Transition Loss 3.1963107585906982, Classifier Loss 0.09365444630384445, Total Loss 21.819602966308594\n",
      "15: Encoding Loss 0.27932825684547424, Transition Loss -12.262736320495605, Classifier Loss 0.11680473387241364, Total Loss 13.906816482543945\n",
      "15: Encoding Loss 1.8087371587753296, Transition Loss -2.632401704788208, Classifier Loss 0.10543568432331085, Total Loss 25.012939453125\n",
      "15: Encoding Loss 1.840087652206421, Transition Loss -4.76280403137207, Classifier Loss 0.1511882096529007, Total Loss 29.838571548461914\n",
      "15: Encoding Loss 0.18271033465862274, Transition Loss -4.409131050109863, Classifier Loss 0.12340814620256424, Total Loss 13.752148628234863\n",
      "15: Encoding Loss 0.04610578343272209, Transition Loss -5.06379508972168, Classifier Loss 0.11177151650190353, Total Loss 11.426076889038086\n",
      "15: Encoding Loss 1.4861910343170166, Transition Loss -0.8039263486862183, Classifier Loss 0.14381396770477295, Total Loss 26.27076530456543\n",
      "15: Encoding Loss 0.20262305438518524, Transition Loss -4.373679161071777, Classifier Loss 0.06345512717962265, Total Loss 7.930981636047363\n",
      "15: Encoding Loss 0.10573694109916687, Transition Loss -2.185894727706909, Classifier Loss 0.16822955012321472, Total Loss 17.545612335205078\n",
      "15: Encoding Loss -0.13220921158790588, Transition Loss -9.33692741394043, Classifier Loss 0.08397307991981506, Total Loss 8.297004699707031\n",
      "15: Encoding Loss -0.26109617948532104, Transition Loss -0.9794681668281555, Classifier Loss 0.11784666031599045, Total Loss 11.775040626525879\n",
      "15: Encoding Loss -0.823350191116333, Transition Loss -7.067093372344971, Classifier Loss 0.10263114422559738, Total Loss 10.261700630187988\n",
      "15: Encoding Loss 0.10531212389469147, Transition Loss -7.845708847045898, Classifier Loss 0.07067200541496277, Total Loss 7.785003662109375\n",
      "15: Encoding Loss 0.6960058212280273, Transition Loss -4.81040096282959, Classifier Loss 0.15324713289737701, Total Loss 20.891799926757812\n",
      "15: Encoding Loss 0.7700476050376892, Transition Loss -6.5381646156311035, Classifier Loss 0.1006103903055191, Total Loss 16.220111846923828\n",
      "15: Encoding Loss -0.42338669300079346, Transition Loss -3.034973382949829, Classifier Loss 0.09449402242898941, Total Loss 9.448756217956543\n",
      "15: Encoding Loss 0.7009462714195251, Transition Loss -0.7710099220275879, Classifier Loss 0.12213683873414993, Total Loss 17.82110023498535\n",
      "15: Encoding Loss 0.8584514856338501, Transition Loss -7.280205726623535, Classifier Loss 0.1484971046447754, Total Loss 21.71586799621582\n",
      "15: Encoding Loss -0.675605833530426, Transition Loss -4.300451755523682, Classifier Loss 0.09485606849193573, Total Loss 9.484746932983398\n",
      "15: Encoding Loss -0.042648911476135254, Transition Loss -0.914630651473999, Classifier Loss 0.08856312930583954, Total Loss 8.741872787475586\n",
      "15: Encoding Loss -0.1582963615655899, Transition Loss -11.723941802978516, Classifier Loss 0.11531171202659607, Total Loss 11.457003593444824\n",
      "15: Encoding Loss -0.03258815407752991, Transition Loss -5.338787078857422, Classifier Loss 0.1344279795885086, Total Loss 13.344679832458496\n",
      "15: Encoding Loss 1.0468837022781372, Transition Loss -3.5720324516296387, Classifier Loss 0.09132499992847443, Total Loss 17.506853103637695\n",
      "15: Encoding Loss 2.4567983150482178, Transition Loss -6.693886756896973, Classifier Loss 0.11375556886196136, Total Loss 31.02860450744629\n",
      "15: Encoding Loss -1.628447413444519, Transition Loss -15.774477005004883, Classifier Loss 0.10597386956214905, Total Loss 10.594232559204102\n",
      "15: Encoding Loss 0.21048346161842346, Transition Loss -3.4198756217956543, Classifier Loss 0.13599467277526855, Total Loss 15.252925872802734\n",
      "15: Encoding Loss -0.7146235108375549, Transition Loss -2.8736817836761475, Classifier Loss 0.10437782108783722, Total Loss 10.437207221984863\n",
      "15: Encoding Loss 0.6355676054954529, Transition Loss -5.921604633331299, Classifier Loss 0.12074238806962967, Total Loss 17.157594680786133\n",
      "15: Encoding Loss 0.3813757598400116, Transition Loss 0.05740344524383545, Classifier Loss 0.10052087903022766, Total Loss 13.114365577697754\n",
      "15: Encoding Loss 0.10427824407815933, Transition Loss -7.747004508972168, Classifier Loss 0.11271098256111145, Total Loss 11.97987174987793\n",
      "15: Encoding Loss -0.25883859395980835, Transition Loss -10.275659561157227, Classifier Loss 0.12522479891777039, Total Loss 12.510440826416016\n",
      "15: Encoding Loss -0.8505958914756775, Transition Loss -8.109708786010742, Classifier Loss 0.08612716197967529, Total Loss 8.61109447479248\n",
      "15: Encoding Loss 0.3885926902294159, Transition Loss 0.9972444772720337, Classifier Loss 0.07610210031270981, Total Loss 10.918241500854492\n",
      "15: Encoding Loss 0.3303961157798767, Transition Loss -2.65925931930542, Classifier Loss 0.14387920498847961, Total Loss 17.029296875\n",
      "15: Encoding Loss 0.04089634492993355, Transition Loss -9.791431427001953, Classifier Loss 0.12001034617424011, Total Loss 12.214590072631836\n",
      "15: Encoding Loss 1.4573349952697754, Transition Loss 3.3109350204467773, Classifier Loss 0.14433129131793976, Total Loss 26.753995895385742\n",
      "15: Encoding Loss 0.678646445274353, Transition Loss -2.7962636947631836, Classifier Loss 0.10077961534261703, Total Loss 15.506574630737305\n",
      "15: Encoding Loss 1.0695533752441406, Transition Loss -0.43648433685302734, Classifier Loss 0.12298639863729477, Total Loss 20.85498046875\n",
      "15: Encoding Loss -0.21251191198825836, Transition Loss -8.01976490020752, Classifier Loss 0.09319151937961578, Total Loss 9.289006233215332\n",
      "15: Encoding Loss 0.49618977308273315, Transition Loss -9.897358894348145, Classifier Loss 0.131867915391922, Total Loss 17.154327392578125\n",
      "15: Encoding Loss 0.1472279280424118, Transition Loss 0.8447486758232117, Classifier Loss 0.12968289852142334, Total Loss 14.232059478759766\n",
      "15: Encoding Loss -0.5959938168525696, Transition Loss -2.5663645267486572, Classifier Loss 0.06553436070680618, Total Loss 6.552923202514648\n",
      "15: Encoding Loss -0.7036173939704895, Transition Loss -4.074955940246582, Classifier Loss 0.10003544390201569, Total Loss 10.002729415893555\n",
      "15: Encoding Loss 0.8814834356307983, Transition Loss -9.895625114440918, Classifier Loss 0.10940787196159363, Total Loss 17.99067497253418\n",
      "15: Encoding Loss 0.031052114441990852, Transition Loss -8.79720687866211, Classifier Loss 0.11575861275196075, Total Loss 11.728596687316895\n",
      "15: Encoding Loss -0.6708386540412903, Transition Loss 2.3723888397216797, Classifier Loss 0.08453812450170517, Total Loss 8.928290367126465\n",
      "15: Encoding Loss -0.8880212306976318, Transition Loss -11.556405067443848, Classifier Loss 0.08539554476737976, Total Loss 8.537242889404297\n",
      "15: Encoding Loss -0.05813773348927498, Transition Loss -10.837066650390625, Classifier Loss 0.09532091021537781, Total Loss 9.399465560913086\n",
      "15: Encoding Loss 0.8312200307846069, Transition Loss 2.942620277404785, Classifier Loss 0.0985562652349472, Total Loss 17.093910217285156\n",
      "15: Encoding Loss 0.9464548826217651, Transition Loss 7.330537796020508, Classifier Loss 0.08795879781246185, Total Loss 17.833627700805664\n",
      "15: Encoding Loss 0.31253424286842346, Transition Loss -7.026351451873779, Classifier Loss 0.08432795107364655, Total Loss 10.929443359375\n",
      "15: Encoding Loss -0.1873190850019455, Transition Loss -6.5806965827941895, Classifier Loss 0.09439468383789062, Total Loss 9.392415046691895\n",
      "15: Encoding Loss -1.145060658454895, Transition Loss -7.026637077331543, Classifier Loss 0.09199727326631546, Total Loss 9.198321342468262\n",
      "15: Encoding Loss 0.17074865102767944, Transition Loss -2.3113303184509277, Classifier Loss 0.09171399474143982, Total Loss 10.477005958557129\n",
      "15: Encoding Loss -0.6810826063156128, Transition Loss -9.591814994812012, Classifier Loss 0.08387237787246704, Total Loss 8.385318756103516\n",
      "15: Encoding Loss -0.36767956614494324, Transition Loss -3.2719497680664062, Classifier Loss 0.10044259577989578, Total Loss 10.043258666992188\n",
      "15: Encoding Loss 1.1197314262390137, Transition Loss -6.357308387756348, Classifier Loss 0.13568949699401855, Total Loss 22.525529861450195\n",
      "15: Encoding Loss -0.6334626078605652, Transition Loss -13.918960571289062, Classifier Loss 0.11625269055366516, Total Loss 11.622485160827637\n",
      "15: Encoding Loss -0.3476318418979645, Transition Loss -2.2633543014526367, Classifier Loss 0.11312656104564667, Total Loss 11.31149673461914\n",
      "15: Encoding Loss 0.526493489742279, Transition Loss -6.428620338439941, Classifier Loss 0.15769344568252563, Total Loss 19.98000717163086\n",
      "15: Encoding Loss 0.29647961258888245, Transition Loss -10.987112045288086, Classifier Loss 0.11873269826173782, Total Loss 14.239317893981934\n",
      "15: Encoding Loss -0.9639976620674133, Transition Loss -6.3637309074401855, Classifier Loss 0.059964295476675034, Total Loss 5.995156764984131\n",
      "15: Encoding Loss 0.08546849340200424, Transition Loss -8.043438911437988, Classifier Loss 0.09924270212650299, Total Loss 10.472146987915039\n",
      "15: Encoding Loss -0.7511124014854431, Transition Loss -0.6707391738891602, Classifier Loss 0.08712410926818848, Total Loss 8.712276458740234\n",
      "15: Encoding Loss 0.5617684721946716, Transition Loss -3.5519895553588867, Classifier Loss 0.10682868957519531, Total Loss 15.17630672454834\n",
      "15: Encoding Loss 0.6223307251930237, Transition Loss -4.311738014221191, Classifier Loss 0.09234707802534103, Total Loss 14.212491989135742\n",
      "15: Encoding Loss -0.1348823755979538, Transition Loss 0.04702109098434448, Classifier Loss 0.0869535356760025, Total Loss 8.609048843383789\n",
      "15: Encoding Loss 0.21642683446407318, Transition Loss -6.361838340759277, Classifier Loss 0.07228721678256989, Total Loss 8.93250846862793\n",
      "15: Encoding Loss -0.5868640542030334, Transition Loss -6.533071517944336, Classifier Loss 0.10929527878761292, Total Loss 10.928221702575684\n",
      "15: Encoding Loss 1.0053951740264893, Transition Loss -7.791010856628418, Classifier Loss 0.11932426691055298, Total Loss 19.974029541015625\n",
      "15: Encoding Loss 0.339335173368454, Transition Loss 3.9569172859191895, Classifier Loss 0.1524311751127243, Total Loss 18.748245239257812\n",
      "15: Encoding Loss 0.4261142611503601, Transition Loss -2.0378780364990234, Classifier Loss 0.08319904655218124, Total Loss 11.728376388549805\n",
      "15: Encoding Loss 1.4951609373092651, Transition Loss -6.665314197540283, Classifier Loss 0.10885214805603027, Total Loss 22.845169067382812\n",
      "15: Encoding Loss 1.0069091320037842, Transition Loss -4.674939155578613, Classifier Loss 0.1141693964600563, Total Loss 19.47127914428711\n",
      "15: Encoding Loss -0.5096561908721924, Transition Loss -2.863400936126709, Classifier Loss 0.09393145143985748, Total Loss 9.392572402954102\n",
      "15: Encoding Loss -0.0041361404582858086, Transition Loss 0.9461907744407654, Classifier Loss 0.11198769509792328, Total Loss 11.37200927734375\n",
      "15: Encoding Loss -0.6384233832359314, Transition Loss -6.840567111968994, Classifier Loss 0.07732025533914566, Total Loss 7.730657577514648\n",
      "15: Encoding Loss 1.1071220636367798, Transition Loss 1.4156062602996826, Classifier Loss 0.09537574648857117, Total Loss 18.677671432495117\n",
      "15: Encoding Loss 0.6004209518432617, Transition Loss -2.3094370365142822, Classifier Loss 0.07783906906843185, Total Loss 12.586812973022461\n",
      "15: Encoding Loss 0.30793657898902893, Transition Loss -1.721289873123169, Classifier Loss 0.10189187526702881, Total Loss 12.649781227111816\n",
      "15: Encoding Loss 0.9586794972419739, Transition Loss -10.264225006103516, Classifier Loss 0.14776886999607086, Total Loss 22.444271087646484\n",
      "15: Encoding Loss 0.006982108578085899, Transition Loss -9.902057647705078, Classifier Loss 0.1624053418636322, Total Loss 16.268037796020508\n",
      "15: Encoding Loss -1.3244670629501343, Transition Loss -15.064167976379395, Classifier Loss 0.08910171687602997, Total Loss 8.907158851623535\n",
      "15: Encoding Loss 1.1399158239364624, Transition Loss -6.87495756149292, Classifier Loss 0.12156589329242706, Total Loss 21.2745418548584\n",
      "15: Encoding Loss 0.09439866989850998, Transition Loss -6.1981964111328125, Classifier Loss 0.11512669175863266, Total Loss 12.136281967163086\n",
      "15: Encoding Loss -1.002865195274353, Transition Loss -13.763090133666992, Classifier Loss 0.10716851055622101, Total Loss 10.714098930358887\n",
      "15: Encoding Loss -0.41600435972213745, Transition Loss -1.2936620712280273, Classifier Loss 0.12967097759246826, Total Loss 12.96678638458252\n",
      "15: Encoding Loss 0.3338157832622528, Transition Loss -11.218633651733398, Classifier Loss 0.13422593474388123, Total Loss 16.089750289916992\n",
      "15: Encoding Loss -0.24622152745723724, Transition Loss -11.543087005615234, Classifier Loss 0.057744234800338745, Total Loss 5.758515357971191\n",
      "15: Encoding Loss -0.33521461486816406, Transition Loss -15.038040161132812, Classifier Loss 0.10627537965774536, Total Loss 10.623455047607422\n",
      "15: Encoding Loss -1.5930168628692627, Transition Loss -19.697021484375, Classifier Loss 0.06853067129850388, Total Loss 6.849127292633057\n",
      "16: Encoding Loss -0.0006810483755543828, Transition Loss 0.13395839929580688, Classifier Loss 0.06455093622207642, Total Loss 6.479175567626953\n",
      "16: Encoding Loss -0.04908376559615135, Transition Loss 0.5568398237228394, Classifier Loss 0.14606599509716034, Total Loss 14.595544815063477\n",
      "16: Encoding Loss -0.20215535163879395, Transition Loss 4.635800838470459, Classifier Loss 0.07524468004703522, Total Loss 8.416677474975586\n",
      "16: Encoding Loss -0.6734840869903564, Transition Loss -13.322378158569336, Classifier Loss 0.11079825460910797, Total Loss 11.077160835266113\n",
      "16: Encoding Loss -0.09317125380039215, Transition Loss -3.5971338748931885, Classifier Loss 0.08005722612142563, Total Loss 7.87401008605957\n",
      "16: Encoding Loss -0.698811948299408, Transition Loss -10.836079597473145, Classifier Loss 0.09859724342823029, Total Loss 9.85755729675293\n",
      "16: Encoding Loss -0.31248828768730164, Transition Loss -6.607524871826172, Classifier Loss 0.10555650293827057, Total Loss 10.552105903625488\n",
      "16: Encoding Loss -0.6446155309677124, Transition Loss -4.80219841003418, Classifier Loss 0.08372218906879425, Total Loss 8.371258735656738\n",
      "16: Encoding Loss 0.5759884715080261, Transition Loss -10.44224739074707, Classifier Loss 0.08969959616661072, Total Loss 13.57577896118164\n",
      "16: Encoding Loss -0.7139923572540283, Transition Loss -6.828446388244629, Classifier Loss 0.11449258029460907, Total Loss 11.447892189025879\n",
      "16: Encoding Loss 2.1447386741638184, Transition Loss -3.577826499938965, Classifier Loss 0.09781716763973236, Total Loss 26.938913345336914\n",
      "16: Encoding Loss 0.44511446356773376, Transition Loss -0.4662286639213562, Classifier Loss 0.10881072282791138, Total Loss 14.441879272460938\n",
      "16: Encoding Loss 0.049716439098119736, Transition Loss -3.2973101139068604, Classifier Loss 0.08145534247159958, Total Loss 8.41949462890625\n",
      "16: Encoding Loss -0.12241923809051514, Transition Loss -2.45772647857666, Classifier Loss 0.10105808079242706, Total Loss 9.997157096862793\n",
      "16: Encoding Loss -1.5096455812454224, Transition Loss -10.110939979553223, Classifier Loss 0.09020838141441345, Total Loss 9.018815994262695\n",
      "16: Encoding Loss -0.44684943556785583, Transition Loss -14.671525001525879, Classifier Loss 0.08708761632442474, Total Loss 8.705812454223633\n",
      "16: Encoding Loss 0.2537228763103485, Transition Loss -4.540903091430664, Classifier Loss 0.056237563490867615, Total Loss 7.64129114151001\n",
      "16: Encoding Loss 0.8665602207183838, Transition Loss -1.6374011039733887, Classifier Loss 0.09690214693546295, Total Loss 16.62236785888672\n",
      "16: Encoding Loss 1.6847914457321167, Transition Loss -8.164881706237793, Classifier Loss 0.13889876008033752, Total Loss 27.366575241088867\n",
      "16: Encoding Loss 1.0276535749435425, Transition Loss -1.7503641843795776, Classifier Loss 0.08572875708341599, Total Loss 16.79375457763672\n",
      "16: Encoding Loss 0.9634897708892822, Transition Loss -10.462261199951172, Classifier Loss 0.1128840520977974, Total Loss 18.994230270385742\n",
      "16: Encoding Loss -0.025585804134607315, Transition Loss -5.85419225692749, Classifier Loss 0.08051904290914536, Total Loss 7.969058036804199\n",
      "16: Encoding Loss -0.13469187915325165, Transition Loss -8.662397384643555, Classifier Loss 0.12616020441055298, Total Loss 12.518383026123047\n",
      "16: Encoding Loss -0.08075638115406036, Transition Loss -0.4620233178138733, Classifier Loss 0.13742119073867798, Total Loss 13.606568336486816\n",
      "16: Encoding Loss -0.29205322265625, Transition Loss -7.188304901123047, Classifier Loss 0.06950383633375168, Total Loss 6.944863796234131\n",
      "16: Encoding Loss 0.1601019948720932, Transition Loss -0.5579777956008911, Classifier Loss 0.09750566631555557, Total Loss 10.961228370666504\n",
      "16: Encoding Loss 0.10691262781620026, Transition Loss 0.6169556975364685, Classifier Loss 0.12588568031787872, Total Loss 13.445374488830566\n",
      "16: Encoding Loss -1.5244672298431396, Transition Loss -14.284614562988281, Classifier Loss 0.11916754394769669, Total Loss 11.913897514343262\n",
      "16: Encoding Loss -1.5990276336669922, Transition Loss -5.757893085479736, Classifier Loss 0.10128611326217651, Total Loss 10.127459526062012\n",
      "16: Encoding Loss 0.1523953080177307, Transition Loss -1.5318870544433594, Classifier Loss 0.10699863731861115, Total Loss 11.840985298156738\n",
      "16: Encoding Loss 0.4686192274093628, Transition Loss -7.065108299255371, Classifier Loss 0.12009935081005096, Total Loss 15.757471084594727\n",
      "16: Encoding Loss -0.06789359450340271, Transition Loss -0.2911851406097412, Classifier Loss 0.13068175315856934, Total Loss 12.933095932006836\n",
      "16: Encoding Loss 1.0444120168685913, Transition Loss -7.274275779724121, Classifier Loss 0.11343080550432205, Total Loss 19.69692039489746\n",
      "16: Encoding Loss 1.3044397830963135, Transition Loss -3.7425079345703125, Classifier Loss 0.19572895765304565, Total Loss 30.007665634155273\n",
      "16: Encoding Loss -0.6912802457809448, Transition Loss -8.296253204345703, Classifier Loss 0.08214832097291946, Total Loss 8.213172912597656\n",
      "16: Encoding Loss -0.4439399540424347, Transition Loss -3.2593581676483154, Classifier Loss 0.06641542166471481, Total Loss 6.640873908996582\n",
      "16: Encoding Loss -0.01632082834839821, Transition Loss -4.443892002105713, Classifier Loss 0.12270462512969971, Total Loss 12.212753295898438\n",
      "16: Encoding Loss 0.407893568277359, Transition Loss -6.485508918762207, Classifier Loss 0.13944830000400543, Total Loss 17.206607818603516\n",
      "16: Encoding Loss -0.23229371011257172, Transition Loss -4.9694390296936035, Classifier Loss 0.10766004770994186, Total Loss 10.746257781982422\n",
      "16: Encoding Loss -0.5258325934410095, Transition Loss -6.289153099060059, Classifier Loss 0.10866665095090866, Total Loss 10.86540699005127\n",
      "16: Encoding Loss -1.0871552228927612, Transition Loss -4.163478374481201, Classifier Loss 0.06871548295021057, Total Loss 6.870715618133545\n",
      "16: Encoding Loss -0.0071504502557218075, Transition Loss -10.301338195800781, Classifier Loss 0.13566356897354126, Total Loss 13.537324905395508\n",
      "16: Encoding Loss -0.946997344493866, Transition Loss -10.52956485748291, Classifier Loss 0.06699792295694351, Total Loss 6.697686672210693\n",
      "16: Encoding Loss -0.37409457564353943, Transition Loss -0.7280356884002686, Classifier Loss 0.1100461408495903, Total Loss 11.004193305969238\n",
      "16: Encoding Loss -0.40564432740211487, Transition Loss -11.5184326171875, Classifier Loss 0.08981010317802429, Total Loss 8.978625297546387\n",
      "16: Encoding Loss 1.2104856967926025, Transition Loss -3.1117770671844482, Classifier Loss 0.11713860929012299, Total Loss 21.397125244140625\n",
      "16: Encoding Loss 2.067528486251831, Transition Loss -10.890997886657715, Classifier Loss 0.11138208210468292, Total Loss 27.676258087158203\n",
      "16: Encoding Loss -0.7083995342254639, Transition Loss -2.0511629581451416, Classifier Loss 0.07026036083698273, Total Loss 7.025626182556152\n",
      "16: Encoding Loss 0.4213469624519348, Transition Loss -3.5751335620880127, Classifier Loss 0.10371316969394684, Total Loss 13.741334915161133\n",
      "16: Encoding Loss 0.11389628797769547, Transition Loss -11.405424118041992, Classifier Loss 0.09828014671802521, Total Loss 10.620857238769531\n",
      "16: Encoding Loss -0.38516440987586975, Transition Loss -14.79910659790039, Classifier Loss 0.08668813109397888, Total Loss 8.665671348571777\n",
      "16: Encoding Loss 0.5347220301628113, Transition Loss -7.93715763092041, Classifier Loss 0.10557064414024353, Total Loss 14.833252906799316\n",
      "16: Encoding Loss -0.8779412508010864, Transition Loss 0.5198955535888672, Classifier Loss 0.12781086564064026, Total Loss 12.885066032409668\n",
      "16: Encoding Loss -0.7566947937011719, Transition Loss -7.507989406585693, Classifier Loss 0.08874501287937164, Total Loss 8.87299919128418\n",
      "16: Encoding Loss -0.1590367704629898, Transition Loss -6.873328685760498, Classifier Loss 0.09496969729661942, Total Loss 9.424505233764648\n",
      "16: Encoding Loss -0.5978588461875916, Transition Loss -0.5004143118858337, Classifier Loss 0.1203014999628067, Total Loss 12.030050277709961\n",
      "16: Encoding Loss 0.7449245452880859, Transition Loss -6.324467658996582, Classifier Loss 0.07239330559968948, Total Loss 13.19746208190918\n",
      "16: Encoding Loss -0.5905219912528992, Transition Loss -7.629727363586426, Classifier Loss 0.09346812963485718, Total Loss 9.345287322998047\n",
      "16: Encoding Loss -0.7471961379051208, Transition Loss -14.741485595703125, Classifier Loss 0.093207947909832, Total Loss 9.317846298217773\n",
      "16: Encoding Loss -0.6196748614311218, Transition Loss -10.55494499206543, Classifier Loss 0.10639531165361404, Total Loss 10.637419700622559\n",
      "16: Encoding Loss -0.06716987490653992, Transition Loss -5.160097122192383, Classifier Loss 0.09538881480693817, Total Loss 9.403032302856445\n",
      "16: Encoding Loss -0.2777872681617737, Transition Loss -3.440842390060425, Classifier Loss 0.14628030359745026, Total Loss 14.621262550354004\n",
      "16: Encoding Loss -0.4076917767524719, Transition Loss -5.850295066833496, Classifier Loss 0.10807967931032181, Total Loss 10.806723594665527\n",
      "16: Encoding Loss -0.3147960603237152, Transition Loss -3.6662685871124268, Classifier Loss 0.0982000082731247, Total Loss 9.8171968460083\n",
      "16: Encoding Loss -0.48410844802856445, Transition Loss -8.399628639221191, Classifier Loss 0.09990590810775757, Total Loss 9.988907814025879\n",
      "16: Encoding Loss -0.4233654737472534, Transition Loss -0.8419902324676514, Classifier Loss 0.171661376953125, Total Loss 17.165931701660156\n",
      "16: Encoding Loss 1.1908963918685913, Transition Loss 3.0281758308410645, Classifier Loss 0.08810591697692871, Total Loss 18.943397521972656\n",
      "16: Encoding Loss -0.7224925756454468, Transition Loss -12.158418655395508, Classifier Loss 0.11355932801961899, Total Loss 11.353501319885254\n",
      "16: Encoding Loss 0.7976095676422119, Transition Loss -2.943375587463379, Classifier Loss 0.09904403984546661, Total Loss 16.284690856933594\n",
      "16: Encoding Loss 1.1969469785690308, Transition Loss -5.531777381896973, Classifier Loss 0.15314476191997528, Total Loss 24.888946533203125\n",
      "16: Encoding Loss -0.47878947854042053, Transition Loss -4.915106296539307, Classifier Loss 0.12122847139835358, Total Loss 12.121861457824707\n",
      "16: Encoding Loss -0.40641921758651733, Transition Loss -5.349899768829346, Classifier Loss 0.11224028468132019, Total Loss 11.222880363464355\n",
      "16: Encoding Loss 0.5324199199676514, Transition Loss -1.2166101932525635, Classifier Loss 0.14111800491809845, Total Loss 18.370914459228516\n",
      "16: Encoding Loss -0.13761472702026367, Transition Loss -4.858035087585449, Classifier Loss 0.05978616327047348, Total Loss 5.884740352630615\n",
      "16: Encoding Loss -0.2724998891353607, Transition Loss -3.549335241317749, Classifier Loss 0.16643373668193817, Total Loss 16.63565444946289\n",
      "16: Encoding Loss -0.33793461322784424, Transition Loss -10.325732231140137, Classifier Loss 0.08472494781017303, Total Loss 8.46944808959961\n",
      "16: Encoding Loss -0.22356633841991425, Transition Loss -2.1179139614105225, Classifier Loss 0.11250923573970795, Total Loss 11.227808952331543\n",
      "16: Encoding Loss -1.4221153259277344, Transition Loss -8.006063461303711, Classifier Loss 0.10080642253160477, Total Loss 10.079041481018066\n",
      "16: Encoding Loss -0.4053103029727936, Transition Loss -8.824626922607422, Classifier Loss 0.061398137360811234, Total Loss 6.137967109680176\n",
      "16: Encoding Loss 0.1587645262479782, Transition Loss -6.0244855880737305, Classifier Loss 0.13963033258914948, Total Loss 15.1605863571167\n",
      "16: Encoding Loss 0.2713008522987366, Transition Loss -7.126678466796875, Classifier Loss 0.09746001660823822, Total Loss 11.907746315002441\n",
      "16: Encoding Loss -0.983218252658844, Transition Loss -3.834672212600708, Classifier Loss 0.0948902815580368, Total Loss 9.488261222839355\n",
      "16: Encoding Loss 0.05556102469563484, Transition Loss -1.5263478755950928, Classifier Loss 0.11715355515480042, Total Loss 12.030975341796875\n",
      "16: Encoding Loss 0.24176433682441711, Transition Loss -7.737896919250488, Classifier Loss 0.14494702219963074, Total Loss 16.41216278076172\n",
      "16: Encoding Loss -1.2704968452453613, Transition Loss -4.821983337402344, Classifier Loss 0.09098633378744125, Total Loss 9.09766960144043\n",
      "16: Encoding Loss -0.4498893916606903, Transition Loss -1.5982425212860107, Classifier Loss 0.08756473660469055, Total Loss 8.756141662597656\n",
      "16: Encoding Loss -0.7073640823364258, Transition Loss -12.21301555633545, Classifier Loss 0.10413695871829987, Total Loss 10.411253929138184\n",
      "16: Encoding Loss -0.34833160042762756, Transition Loss -6.035490989685059, Classifier Loss 0.13865268230438232, Total Loss 13.863370895385742\n",
      "16: Encoding Loss 0.9592220783233643, Transition Loss -4.408148288726807, Classifier Loss 0.08020567893981934, Total Loss 15.693462371826172\n",
      "16: Encoding Loss 1.9990241527557373, Transition Loss -6.844855308532715, Classifier Loss 0.09137238562107086, Total Loss 25.128063201904297\n",
      "16: Encoding Loss -2.1810038089752197, Transition Loss -16.014490127563477, Classifier Loss 0.10628978908061981, Total Loss 10.625776290893555\n",
      "16: Encoding Loss -0.2297952026128769, Transition Loss -3.83375883102417, Classifier Loss 0.1286071538925171, Total Loss 12.840126991271973\n",
      "16: Encoding Loss -1.1918079853057861, Transition Loss -3.073338747024536, Classifier Loss 0.10351927578449249, Total Loss 10.351312637329102\n",
      "16: Encoding Loss 0.055366404354572296, Transition Loss -5.913227081298828, Classifier Loss 0.11703850328922272, Total Loss 12.017191886901855\n",
      "16: Encoding Loss 0.04679423198103905, Transition Loss -0.3094221353530884, Classifier Loss 0.10358507931232452, Total Loss 10.613039016723633\n",
      "16: Encoding Loss -0.4740302860736847, Transition Loss -8.005979537963867, Classifier Loss 0.11461442708969116, Total Loss 11.459837913513184\n",
      "16: Encoding Loss -0.5794137120246887, Transition Loss -10.508832931518555, Classifier Loss 0.1230853945016861, Total Loss 12.306437492370605\n",
      "16: Encoding Loss -1.0116633176803589, Transition Loss -8.365472793579102, Classifier Loss 0.08183488994836807, Total Loss 8.181816101074219\n",
      "16: Encoding Loss 0.16373522579669952, Transition Loss 0.25500011444091797, Classifier Loss 0.06903834640979767, Total Loss 8.198202133178711\n",
      "16: Encoding Loss -0.07291091233491898, Transition Loss -2.9993207454681396, Classifier Loss 0.14840348064899445, Total Loss 14.703861236572266\n",
      "16: Encoding Loss 0.16136136651039124, Transition Loss -9.70388412475586, Classifier Loss 0.11133763939142227, Total Loss 12.353902816772461\n",
      "16: Encoding Loss 0.8405876159667969, Transition Loss 2.713865280151367, Classifier Loss 0.1407269835472107, Total Loss 21.340171813964844\n",
      "16: Encoding Loss 0.20661436021327972, Transition Loss -3.277017593383789, Classifier Loss 0.09740407019853592, Total Loss 11.360588073730469\n",
      "16: Encoding Loss 0.5244496464729309, Transition Loss -0.7235906720161438, Classifier Loss 0.11830206960439682, Total Loss 16.025657653808594\n",
      "16: Encoding Loss -0.5410501956939697, Transition Loss -8.257400512695312, Classifier Loss 0.08932921290397644, Total Loss 8.931269645690918\n",
      "16: Encoding Loss -0.16777849197387695, Transition Loss -10.166420936584473, Classifier Loss 0.12898308038711548, Total Loss 12.833600044250488\n",
      "16: Encoding Loss -0.3604094386100769, Transition Loss -0.6021571159362793, Classifier Loss 0.12670835852622986, Total Loss 12.67026424407959\n",
      "16: Encoding Loss -0.40484440326690674, Transition Loss -3.3299624919891357, Classifier Loss 0.0629509687423706, Total Loss 6.294347286224365\n",
      "16: Encoding Loss -0.8991739749908447, Transition Loss -4.59800910949707, Classifier Loss 0.0904003158211708, Total Loss 9.039112091064453\n",
      "16: Encoding Loss 0.1931367963552475, Transition Loss -10.307669639587402, Classifier Loss 0.10549050569534302, Total Loss 12.050800323486328\n",
      "16: Encoding Loss -0.39558693766593933, Transition Loss -8.67983627319336, Classifier Loss 0.10930079966783524, Total Loss 10.928224563598633\n",
      "16: Encoding Loss -0.8417139053344727, Transition Loss 2.3874478340148926, Classifier Loss 0.07670856267213821, Total Loss 8.148345947265625\n",
      "16: Encoding Loss -1.1735609769821167, Transition Loss -11.152531623840332, Classifier Loss 0.07853814959526062, Total Loss 7.851584434509277\n",
      "16: Encoding Loss -0.7081791162490845, Transition Loss -10.635915756225586, Classifier Loss 0.08665097504854202, Total Loss 8.662969589233398\n",
      "16: Encoding Loss 0.5293731689453125, Transition Loss 2.8637807369232178, Classifier Loss 0.09401663392782211, Total Loss 14.209403991699219\n",
      "16: Encoding Loss 0.25744351744651794, Transition Loss 6.999945640563965, Classifier Loss 0.08778104186058044, Total Loss 12.227301597595215\n",
      "16: Encoding Loss -0.2875136733055115, Transition Loss -7.332646369934082, Classifier Loss 0.08047543466091156, Total Loss 8.04143238067627\n",
      "16: Encoding Loss -0.5720281004905701, Transition Loss -7.031035423278809, Classifier Loss 0.0915977954864502, Total Loss 9.15837287902832\n",
      "16: Encoding Loss -1.6133308410644531, Transition Loss -7.850732326507568, Classifier Loss 0.08771707862615585, Total Loss 8.770137786865234\n",
      "16: Encoding Loss -0.11951937526464462, Transition Loss -2.7988014221191406, Classifier Loss 0.09016715735197067, Total Loss 8.905237197875977\n",
      "16: Encoding Loss -1.4005531072616577, Transition Loss -9.91028118133545, Classifier Loss 0.07262159138917923, Total Loss 7.260177135467529\n",
      "16: Encoding Loss -0.01676822453737259, Transition Loss -3.5431594848632812, Classifier Loss 0.10546792298555374, Total Loss 10.487942695617676\n",
      "16: Encoding Loss 0.388994038105011, Transition Loss -6.796658992767334, Classifier Loss 0.13294662535190582, Total Loss 16.40509796142578\n",
      "16: Encoding Loss -1.1495307683944702, Transition Loss -14.36412239074707, Classifier Loss 0.10320034623146057, Total Loss 10.31716251373291\n",
      "16: Encoding Loss -0.9552843570709229, Transition Loss -2.9972305297851562, Classifier Loss 0.10845165699720383, Total Loss 10.844565391540527\n",
      "16: Encoding Loss -0.11137144267559052, Transition Loss -6.8917436599731445, Classifier Loss 0.15564356744289398, Total Loss 15.444746017456055\n",
      "16: Encoding Loss -0.5763384103775024, Transition Loss -11.931818962097168, Classifier Loss 0.11626681685447693, Total Loss 11.624295234680176\n",
      "16: Encoding Loss -1.4254794120788574, Transition Loss -7.3042473793029785, Classifier Loss 0.059439752250909805, Total Loss 5.942514419555664\n",
      "16: Encoding Loss -0.3716168999671936, Transition Loss -8.502503395080566, Classifier Loss 0.09728352725505829, Total Loss 9.726351737976074\n",
      "16: Encoding Loss -0.7190598845481873, Transition Loss -1.5907316207885742, Classifier Loss 0.0833335816860199, Total Loss 8.333039283752441\n",
      "16: Encoding Loss 0.5263316631317139, Transition Loss -4.4221110343933105, Classifier Loss 0.10279018431901932, Total Loss 14.488787651062012\n",
      "16: Encoding Loss 0.07724098116159439, Transition Loss -5.214280128479004, Classifier Loss 0.08906397968530655, Total Loss 9.387377738952637\n",
      "16: Encoding Loss -0.29140713810920715, Transition Loss -0.9222333431243896, Classifier Loss 0.08401530981063843, Total Loss 8.397189140319824\n",
      "16: Encoding Loss -0.05225823074579239, Transition Loss -6.922118663787842, Classifier Loss 0.06152893975377083, Total Loss 6.025825500488281\n",
      "16: Encoding Loss -1.287453532218933, Transition Loss -7.560237884521484, Classifier Loss 0.11103541404008865, Total Loss 11.102029800415039\n",
      "16: Encoding Loss 0.3805851638317108, Transition Loss -8.601410865783691, Classifier Loss 0.10698285698890686, Total Loss 13.741031646728516\n",
      "16: Encoding Loss -0.06180742755532265, Transition Loss 3.2245545387268066, Classifier Loss 0.1456385999917984, Total Loss 15.076125144958496\n",
      "16: Encoding Loss 0.17923682928085327, Transition Loss -3.0745151042938232, Classifier Loss 0.07525123655796051, Total Loss 8.906012535095215\n",
      "16: Encoding Loss 1.1514649391174316, Transition Loss -7.158207893371582, Classifier Loss 0.10357867181301117, Total Loss 19.56815528869629\n",
      "16: Encoding Loss 0.4660893678665161, Transition Loss -5.093825340270996, Classifier Loss 0.10579293966293335, Total Loss 14.306984901428223\n",
      "16: Encoding Loss -0.864724338054657, Transition Loss -3.659973621368408, Classifier Loss 0.0834091380238533, Total Loss 8.340181350708008\n",
      "16: Encoding Loss -0.6284304857254028, Transition Loss -0.04413634538650513, Classifier Loss 0.11086953431367874, Total Loss 11.086944580078125\n",
      "16: Encoding Loss -1.0692769289016724, Transition Loss -7.524013519287109, Classifier Loss 0.06669988483190536, Total Loss 6.668483734130859\n",
      "16: Encoding Loss 0.8112141489982605, Transition Loss 1.0990242958068848, Classifier Loss 0.08993537724018097, Total Loss 15.703056335449219\n",
      "16: Encoding Loss 0.2845483720302582, Transition Loss -2.8307361602783203, Classifier Loss 0.07134223729372025, Total Loss 9.404996871948242\n",
      "16: Encoding Loss -0.06088419631123543, Transition Loss -2.324155569076538, Classifier Loss 0.09192106872797012, Total Loss 9.059492111206055\n",
      "16: Encoding Loss 0.4090054929256439, Transition Loss -11.417135238647461, Classifier Loss 0.1672319918870926, Total Loss 19.992889404296875\n",
      "16: Encoding Loss -0.5540830492973328, Transition Loss -10.080142974853516, Classifier Loss 0.14104148745536804, Total Loss 14.102132797241211\n",
      "16: Encoding Loss -1.4082552194595337, Transition Loss -14.881816864013672, Classifier Loss 0.0848783552646637, Total Loss 8.484859466552734\n",
      "16: Encoding Loss 0.6624101996421814, Transition Loss -6.7535200119018555, Classifier Loss 0.11724773794412613, Total Loss 17.022705078125\n",
      "16: Encoding Loss -0.45696550607681274, Transition Loss -6.705498695373535, Classifier Loss 0.10896468907594681, Total Loss 10.895119667053223\n",
      "16: Encoding Loss -1.3166954517364502, Transition Loss -13.106544494628906, Classifier Loss 0.10376391559839249, Total Loss 10.373769760131836\n",
      "16: Encoding Loss -0.8456000685691833, Transition Loss -1.1321237087249756, Classifier Loss 0.12612192332744598, Total Loss 12.611966133117676\n",
      "16: Encoding Loss 0.3563387989997864, Transition Loss -10.426353454589844, Classifier Loss 0.11465437710285187, Total Loss 14.3135404586792\n",
      "16: Encoding Loss -0.6299046277999878, Transition Loss -12.541415214538574, Classifier Loss 0.05889427289366722, Total Loss 5.886919021606445\n",
      "16: Encoding Loss -0.6167553663253784, Transition Loss -16.120437622070312, Classifier Loss 0.09827268123626709, Total Loss 9.824044227600098\n",
      "16: Encoding Loss -2.394021511077881, Transition Loss -20.848901748657227, Classifier Loss 0.053848665207624435, Total Loss 5.380696773529053\n",
      "17: Encoding Loss -0.3042830526828766, Transition Loss -0.7695221900939941, Classifier Loss 0.06295009702444077, Total Loss 6.292003154754639\n",
      "17: Encoding Loss -0.228921040892601, Transition Loss -0.15992605686187744, Classifier Loss 0.15066063404083252, Total Loss 15.04582405090332\n",
      "17: Encoding Loss -0.30975499749183655, Transition Loss 4.092780113220215, Classifier Loss 0.07066259533166885, Total Loss 7.882397651672363\n",
      "17: Encoding Loss -0.7745051980018616, Transition Loss -14.29558277130127, Classifier Loss 0.10490015149116516, Total Loss 10.48715591430664\n",
      "17: Encoding Loss -0.5945106148719788, Transition Loss -4.211986541748047, Classifier Loss 0.07518105208873749, Total Loss 7.5172624588012695\n",
      "17: Encoding Loss -0.9077245593070984, Transition Loss -11.802671432495117, Classifier Loss 0.09444180130958557, Total Loss 9.44182014465332\n",
      "17: Encoding Loss -0.4596507251262665, Transition Loss -7.427050590515137, Classifier Loss 0.10171771794557571, Total Loss 10.170278549194336\n",
      "17: Encoding Loss -1.0091569423675537, Transition Loss -5.5811591148376465, Classifier Loss 0.07683197408914566, Total Loss 7.68208122253418\n",
      "17: Encoding Loss 0.31919702887535095, Transition Loss -11.27406120300293, Classifier Loss 0.08224593102931976, Total Loss 10.774110794067383\n",
      "17: Encoding Loss -0.9122276902198792, Transition Loss -7.167140007019043, Classifier Loss 0.11063752323389053, Total Loss 11.062318801879883\n",
      "17: Encoding Loss 1.800913691520691, Transition Loss -3.8208305835723877, Classifier Loss 0.09667766839265823, Total Loss 24.074312210083008\n",
      "17: Encoding Loss 0.18325594067573547, Transition Loss -0.9952583909034729, Classifier Loss 0.10703552514314651, Total Loss 12.120384216308594\n",
      "17: Encoding Loss -0.5032143592834473, Transition Loss -3.724874496459961, Classifier Loss 0.07585939764976501, Total Loss 7.585194110870361\n",
      "17: Encoding Loss -0.27529212832450867, Transition Loss -2.9013867378234863, Classifier Loss 0.1015574261546135, Total Loss 10.148658752441406\n",
      "17: Encoding Loss -1.6609591245651245, Transition Loss -10.471731185913086, Classifier Loss 0.08284909278154373, Total Loss 8.282814979553223\n",
      "17: Encoding Loss -1.1075018644332886, Transition Loss -14.951744079589844, Classifier Loss 0.08119886368513107, Total Loss 8.11689567565918\n",
      "17: Encoding Loss -0.2610326111316681, Transition Loss -5.007847785949707, Classifier Loss 0.04930981248617172, Total Loss 4.920535087585449\n",
      "17: Encoding Loss 0.6362277269363403, Transition Loss -2.6850075721740723, Classifier Loss 0.09422285109758377, Total Loss 14.51156997680664\n",
      "17: Encoding Loss 0.9283909201622009, Transition Loss -8.990622520446777, Classifier Loss 0.13193345069885254, Total Loss 20.61867332458496\n",
      "17: Encoding Loss 0.7075291872024536, Transition Loss -2.3546857833862305, Classifier Loss 0.08348314464092255, Total Loss 14.008076667785645\n",
      "17: Encoding Loss 0.6956689953804016, Transition Loss -11.026350975036621, Classifier Loss 0.111024871468544, Total Loss 16.665634155273438\n",
      "17: Encoding Loss -0.18037988245487213, Transition Loss -6.350936412811279, Classifier Loss 0.0813756063580513, Total Loss 8.08487319946289\n",
      "17: Encoding Loss 0.039005886763334274, Transition Loss -8.579642295837402, Classifier Loss 0.11277738213539124, Total Loss 11.479400634765625\n",
      "17: Encoding Loss -0.5491113662719727, Transition Loss -0.9567673206329346, Classifier Loss 0.11617255955934525, Total Loss 11.617064476013184\n",
      "17: Encoding Loss -1.0200916528701782, Transition Loss -7.67476749420166, Classifier Loss 0.06669505685567856, Total Loss 6.667970657348633\n",
      "17: Encoding Loss -0.4076370894908905, Transition Loss -1.1642284393310547, Classifier Loss 0.09287026524543762, Total Loss 9.28671932220459\n",
      "17: Encoding Loss -0.3958311975002289, Transition Loss 0.4939640760421753, Classifier Loss 0.12691111862659454, Total Loss 12.789785385131836\n",
      "17: Encoding Loss -1.9005241394042969, Transition Loss -14.698400497436523, Classifier Loss 0.1111302301287651, Total Loss 11.11008358001709\n",
      "17: Encoding Loss -1.72873055934906, Transition Loss -6.002574920654297, Classifier Loss 0.0939849466085434, Total Loss 9.397294044494629\n",
      "17: Encoding Loss -0.22015409171581268, Transition Loss -1.720505714416504, Classifier Loss 0.09925348311662674, Total Loss 9.900612831115723\n",
      "17: Encoding Loss 0.031872134655714035, Transition Loss -7.230967044830322, Classifier Loss 0.11787240207195282, Total Loss 11.945162773132324\n",
      "17: Encoding Loss -0.42099228501319885, Transition Loss -0.5980637669563293, Classifier Loss 0.12644708156585693, Total Loss 12.644546508789062\n",
      "17: Encoding Loss 0.4746851921081543, Transition Loss -7.5603814125061035, Classifier Loss 0.11018366366624832, Total Loss 14.814332008361816\n",
      "17: Encoding Loss 0.865185558795929, Transition Loss -3.9725985527038574, Classifier Loss 0.19013601541519165, Total Loss 25.934289932250977\n",
      "17: Encoding Loss -1.0233314037322998, Transition Loss -8.863659858703613, Classifier Loss 0.08380577713251114, Total Loss 8.378805160522461\n",
      "17: Encoding Loss -0.5340278744697571, Transition Loss -3.911172866821289, Classifier Loss 0.058127861469984055, Total Loss 5.8120036125183105\n",
      "17: Encoding Loss -0.40186557173728943, Transition Loss -4.822122573852539, Classifier Loss 0.10922803729772568, Total Loss 10.921745300292969\n",
      "17: Encoding Loss 0.0063948561437428, Transition Loss -6.878280162811279, Classifier Loss 0.13728389143943787, Total Loss 13.753897666931152\n",
      "17: Encoding Loss -0.7078666090965271, Transition Loss -5.604391574859619, Classifier Loss 0.09735478460788727, Total Loss 9.734357833862305\n",
      "17: Encoding Loss -0.8891945481300354, Transition Loss -6.932238578796387, Classifier Loss 0.09933532774448395, Total Loss 9.932146072387695\n",
      "17: Encoding Loss -1.2285593748092651, Transition Loss -4.735256195068359, Classifier Loss 0.06776303797960281, Total Loss 6.775356769561768\n",
      "17: Encoding Loss -0.5358978509902954, Transition Loss -10.994379043579102, Classifier Loss 0.1287875920534134, Total Loss 12.87656021118164\n",
      "17: Encoding Loss -1.193861722946167, Transition Loss -10.97147274017334, Classifier Loss 0.062400057911872864, Total Loss 6.23781156539917\n",
      "17: Encoding Loss -0.7140796184539795, Transition Loss -1.1801177263259888, Classifier Loss 0.10801412910223007, Total Loss 10.801177024841309\n",
      "17: Encoding Loss -0.6972187161445618, Transition Loss -11.961694717407227, Classifier Loss 0.08375055342912674, Total Loss 8.372662544250488\n",
      "17: Encoding Loss 0.9945881962776184, Transition Loss -3.5434370040893555, Classifier Loss 0.11556480824947357, Total Loss 19.512475967407227\n",
      "17: Encoding Loss 1.5551973581314087, Transition Loss -11.519031524658203, Classifier Loss 0.10806091129779816, Total Loss 23.245365142822266\n",
      "17: Encoding Loss -0.9754473567008972, Transition Loss -2.492737293243408, Classifier Loss 0.0669960305094719, Total Loss 6.699104309082031\n",
      "17: Encoding Loss 0.13570207357406616, Transition Loss -4.087850093841553, Classifier Loss 0.09660384804010391, Total Loss 10.650315284729004\n",
      "17: Encoding Loss -0.2578711211681366, Transition Loss -12.204110145568848, Classifier Loss 0.08631864935159683, Total Loss 8.619194984436035\n",
      "17: Encoding Loss -0.6015275120735168, Transition Loss -16.306896209716797, Classifier Loss 0.07081557810306549, Total Loss 7.078296184539795\n",
      "17: Encoding Loss 0.34195441007614136, Transition Loss -9.093725204467773, Classifier Loss 0.09845319390296936, Total Loss 12.578278541564941\n",
      "17: Encoding Loss -0.9897480607032776, Transition Loss -0.03479766845703125, Classifier Loss 0.11876369267702103, Total Loss 11.876362800598145\n",
      "17: Encoding Loss -1.0705522298812866, Transition Loss -7.8657917976379395, Classifier Loss 0.08357943594455719, Total Loss 8.356369972229004\n",
      "17: Encoding Loss -0.29584741592407227, Transition Loss -7.302412509918213, Classifier Loss 0.09301873296499252, Total Loss 9.296754837036133\n",
      "17: Encoding Loss -0.8976336717605591, Transition Loss -1.0002291202545166, Classifier Loss 0.10858666151762009, Total Loss 10.858466148376465\n",
      "17: Encoding Loss 0.4498143494129181, Transition Loss -6.737795829772949, Classifier Loss 0.06804075837135315, Total Loss 10.401230812072754\n",
      "17: Encoding Loss -1.1372171640396118, Transition Loss -8.02811336517334, Classifier Loss 0.09130188822746277, Total Loss 9.128582954406738\n",
      "17: Encoding Loss -1.0968282222747803, Transition Loss -15.154458999633789, Classifier Loss 0.08262588828802109, Total Loss 8.259557723999023\n",
      "17: Encoding Loss -0.8939908146858215, Transition Loss -11.07768440246582, Classifier Loss 0.10115794837474823, Total Loss 10.113579750061035\n",
      "17: Encoding Loss -0.44654881954193115, Transition Loss -5.473581314086914, Classifier Loss 0.09070190787315369, Total Loss 9.06908130645752\n",
      "17: Encoding Loss -0.6944259405136108, Transition Loss -3.7828421592712402, Classifier Loss 0.13337360322475433, Total Loss 13.336604118347168\n",
      "17: Encoding Loss -0.7203294038772583, Transition Loss -6.147678375244141, Classifier Loss 0.10264314711093903, Total Loss 10.26308536529541\n",
      "17: Encoding Loss -0.5590024590492249, Transition Loss -3.9544122219085693, Classifier Loss 0.08978979289531708, Total Loss 8.978188514709473\n",
      "17: Encoding Loss -0.6889067888259888, Transition Loss -8.571686744689941, Classifier Loss 0.09653826057910919, Total Loss 9.652111053466797\n",
      "17: Encoding Loss -0.7781832814216614, Transition Loss -1.156448483467102, Classifier Loss 0.14883878827095032, Total Loss 14.883646965026855\n",
      "17: Encoding Loss 0.8870846629142761, Transition Loss 2.531954765319824, Classifier Loss 0.08501049876213074, Total Loss 16.10411834716797\n",
      "17: Encoding Loss -1.1495263576507568, Transition Loss -12.808430671691895, Classifier Loss 0.11160207539796829, Total Loss 11.157646179199219\n",
      "17: Encoding Loss 0.5214826464653015, Transition Loss -3.6237001419067383, Classifier Loss 0.09672234207391739, Total Loss 13.84337043762207\n",
      "17: Encoding Loss 0.706088125705719, Transition Loss -5.886730670928955, Classifier Loss 0.1495690494775772, Total Loss 20.604433059692383\n",
      "17: Encoding Loss -0.8128425478935242, Transition Loss -5.368185520172119, Classifier Loss 0.11780547350645065, Total Loss 11.779473304748535\n",
      "17: Encoding Loss -0.7310792803764343, Transition Loss -5.740436553955078, Classifier Loss 0.11113117635250092, Total Loss 11.111968994140625\n",
      "17: Encoding Loss 0.018109822645783424, Transition Loss -1.6323275566101074, Classifier Loss 0.1400003433227539, Total Loss 14.082557678222656\n",
      "17: Encoding Loss -0.5260252952575684, Transition Loss -5.127392768859863, Classifier Loss 0.04808903485536575, Total Loss 4.807877540588379\n",
      "17: Encoding Loss -0.8652162551879883, Transition Loss -3.7846081256866455, Classifier Loss 0.1586928814649582, Total Loss 15.868531227111816\n",
      "17: Encoding Loss -0.781524658203125, Transition Loss -10.437256813049316, Classifier Loss 0.07716448605060577, Total Loss 7.714361190795898\n",
      "17: Encoding Loss -0.6510403156280518, Transition Loss -2.383594512939453, Classifier Loss 0.11136498302221298, Total Loss 11.136021614074707\n",
      "17: Encoding Loss -2.0041630268096924, Transition Loss -8.081422805786133, Classifier Loss 0.09820204973220825, Total Loss 9.818588256835938\n",
      "17: Encoding Loss -0.9643346071243286, Transition Loss -8.983154296875, Classifier Loss 0.0638817846775055, Total Loss 6.38638162612915\n",
      "17: Encoding Loss -0.3368261456489563, Transition Loss -6.222533702850342, Classifier Loss 0.13409122824668884, Total Loss 13.406858444213867\n",
      "17: Encoding Loss -0.031532950699329376, Transition Loss -7.221475601196289, Classifier Loss 0.08870630711317062, Total Loss 8.774271011352539\n",
      "17: Encoding Loss -1.0779564380645752, Transition Loss -4.3188910484313965, Classifier Loss 0.08612819761037827, Total Loss 8.611955642700195\n",
      "17: Encoding Loss -0.3690333366394043, Transition Loss -1.6857514381408691, Classifier Loss 0.11589129269123077, Total Loss 11.588460922241211\n",
      "17: Encoding Loss -0.008599711582064629, Transition Loss -7.462729454040527, Classifier Loss 0.13615302741527557, Total Loss 13.581768989562988\n",
      "17: Encoding Loss -1.6139169931411743, Transition Loss -4.642930507659912, Classifier Loss 0.09040530771017075, Total Loss 9.039602279663086\n",
      "17: Encoding Loss -0.8049574494361877, Transition Loss -1.6373419761657715, Classifier Loss 0.0814933329820633, Total Loss 8.149005889892578\n",
      "17: Encoding Loss -1.2195779085159302, Transition Loss -12.250992774963379, Classifier Loss 0.10057265311479568, Total Loss 10.054815292358398\n",
      "17: Encoding Loss -0.7901536822319031, Transition Loss -5.913475036621094, Classifier Loss 0.1384233832359314, Total Loss 13.841156005859375\n",
      "17: Encoding Loss 0.10600879788398743, Transition Loss -4.394299030303955, Classifier Loss 0.0804249495267868, Total Loss 8.767095565795898\n",
      "17: Encoding Loss 1.7014378309249878, Transition Loss -7.122524261474609, Classifier Loss 0.09921866655349731, Total Loss 23.531944274902344\n",
      "17: Encoding Loss -2.4227850437164307, Transition Loss -16.44708251953125, Classifier Loss 0.09516145288944244, Total Loss 9.512856483459473\n",
      "17: Encoding Loss -0.4204091727733612, Transition Loss -4.106595039367676, Classifier Loss 0.13586725294589996, Total Loss 13.585860252380371\n",
      "17: Encoding Loss -1.3736729621887207, Transition Loss -3.461841106414795, Classifier Loss 0.09782072901725769, Total Loss 9.781380653381348\n",
      "17: Encoding Loss -0.16685859858989716, Transition Loss -6.204897403717041, Classifier Loss 0.11629025638103485, Total Loss 11.564245223999023\n",
      "17: Encoding Loss -0.3461214601993561, Transition Loss -0.8786191940307617, Classifier Loss 0.09983371198177338, Total Loss 9.982450485229492\n",
      "17: Encoding Loss -0.6995382905006409, Transition Loss -8.435200691223145, Classifier Loss 0.10750725120306015, Total Loss 10.749037742614746\n",
      "17: Encoding Loss -0.6865559816360474, Transition Loss -11.041983604431152, Classifier Loss 0.11365249752998352, Total Loss 11.363040924072266\n",
      "17: Encoding Loss -0.9203998446464539, Transition Loss -8.673072814941406, Classifier Loss 0.08323009312152863, Total Loss 8.321274757385254\n",
      "17: Encoding Loss 0.05614040791988373, Transition Loss -0.1271378993988037, Classifier Loss 0.060725416988134384, Total Loss 6.3926239013671875\n",
      "17: Encoding Loss -0.49194449186325073, Transition Loss -3.3567774295806885, Classifier Loss 0.14512121677398682, Total Loss 14.51144790649414\n",
      "17: Encoding Loss -0.32108983397483826, Transition Loss -10.138692855834961, Classifier Loss 0.09516551345586777, Total Loss 9.512824058532715\n",
      "17: Encoding Loss 0.9191659092903137, Transition Loss 3.2341532707214355, Classifier Loss 0.13273195922374725, Total Loss 21.27335548400879\n",
      "17: Encoding Loss 0.0422251895070076, Transition Loss -3.7741689682006836, Classifier Loss 0.08928214758634567, Total Loss 9.151618003845215\n",
      "17: Encoding Loss 0.18497519195079803, Transition Loss -1.2947391271591187, Classifier Loss 0.11547078937292099, Total Loss 12.979008674621582\n",
      "17: Encoding Loss -0.6191590428352356, Transition Loss -8.624959945678711, Classifier Loss 0.08473451435565948, Total Loss 8.471726417541504\n",
      "17: Encoding Loss -0.3493317663669586, Transition Loss -10.67905330657959, Classifier Loss 0.11423144489526749, Total Loss 11.420341491699219\n",
      "17: Encoding Loss -0.5800104141235352, Transition Loss -1.1168144941329956, Classifier Loss 0.12346810847520828, Total Loss 12.346588134765625\n",
      "17: Encoding Loss -0.7277320623397827, Transition Loss -4.071467876434326, Classifier Loss 0.05929706618189812, Total Loss 5.928892135620117\n",
      "17: Encoding Loss -1.2484221458435059, Transition Loss -5.074156761169434, Classifier Loss 0.08762624114751816, Total Loss 8.761609077453613\n",
      "17: Encoding Loss -0.15745505690574646, Transition Loss -10.555362701416016, Classifier Loss 0.09669558703899384, Total Loss 9.5947904586792\n",
      "17: Encoding Loss -0.7498733997344971, Transition Loss -9.89272689819336, Classifier Loss 0.10730807483196259, Total Loss 10.728828430175781\n",
      "17: Encoding Loss -1.1980342864990234, Transition Loss 1.1102104187011719, Classifier Loss 0.07507659494876862, Total Loss 7.729701519012451\n",
      "17: Encoding Loss -1.8605294227600098, Transition Loss -12.679136276245117, Classifier Loss 0.07512900233268738, Total Loss 7.510364532470703\n",
      "17: Encoding Loss -0.8444331288337708, Transition Loss -12.019002914428711, Classifier Loss 0.08313987404108047, Total Loss 8.311583518981934\n",
      "17: Encoding Loss 0.4253673851490021, Transition Loss 1.6200244426727295, Classifier Loss 0.09417424350976944, Total Loss 13.144332885742188\n",
      "17: Encoding Loss -0.022767456248402596, Transition Loss 6.357378959655762, Classifier Loss 0.08512261509895325, Total Loss 9.70906925201416\n",
      "17: Encoding Loss -0.33118730783462524, Transition Loss -7.76954460144043, Classifier Loss 0.07436706870794296, Total Loss 7.433925151824951\n",
      "17: Encoding Loss -0.8059843182563782, Transition Loss -7.366852283477783, Classifier Loss 0.08443470299243927, Total Loss 8.441996574401855\n",
      "17: Encoding Loss -1.710692286491394, Transition Loss -8.413422584533691, Classifier Loss 0.09049156308174133, Total Loss 9.047473907470703\n",
      "17: Encoding Loss -0.20664732158184052, Transition Loss -3.1269705295562744, Classifier Loss 0.08517143130302429, Total Loss 8.484458923339844\n",
      "17: Encoding Loss -1.4553622007369995, Transition Loss -10.381123542785645, Classifier Loss 0.0736573338508606, Total Loss 7.363656997680664\n",
      "17: Encoding Loss -0.02138921618461609, Transition Loss -4.026852130889893, Classifier Loss 0.10435087978839874, Total Loss 10.3632173538208\n",
      "17: Encoding Loss 0.041998039931058884, Transition Loss -7.424408912658691, Classifier Loss 0.1244351714849472, Total Loss 12.664705276489258\n",
      "17: Encoding Loss -1.680866003036499, Transition Loss -14.977928161621094, Classifier Loss 0.09893284738063812, Total Loss 9.890289306640625\n",
      "17: Encoding Loss -1.290037989616394, Transition Loss -3.6848816871643066, Classifier Loss 0.11476857960224152, Total Loss 11.476120948791504\n",
      "17: Encoding Loss -0.5563405752182007, Transition Loss -7.302375793457031, Classifier Loss 0.15217876434326172, Total Loss 15.216416358947754\n",
      "17: Encoding Loss -0.9438658952713013, Transition Loss -12.259214401245117, Classifier Loss 0.11195997893810272, Total Loss 11.193546295166016\n",
      "17: Encoding Loss -1.7492698431015015, Transition Loss -7.819718837738037, Classifier Loss 0.050096575170755386, Total Loss 5.008093357086182\n",
      "17: Encoding Loss -0.4796917736530304, Transition Loss -8.757287979125977, Classifier Loss 0.08841612190008163, Total Loss 8.83985710144043\n",
      "17: Encoding Loss -1.0554510354995728, Transition Loss -1.9552392959594727, Classifier Loss 0.07889062911272049, Total Loss 7.888671875\n",
      "17: Encoding Loss 0.12122531980276108, Transition Loss -4.894529342651367, Classifier Loss 0.09447909891605377, Total Loss 10.307430267333984\n",
      "17: Encoding Loss -0.06322275847196579, Transition Loss -5.819859504699707, Classifier Loss 0.08641141653060913, Total Loss 8.506643295288086\n",
      "17: Encoding Loss -0.31292369961738586, Transition Loss -1.4369947910308838, Classifier Loss 0.07889758795499802, Total Loss 7.887277126312256\n",
      "17: Encoding Loss -0.03623855859041214, Transition Loss -7.465780258178711, Classifier Loss 0.06107958033680916, Total Loss 6.002523422241211\n",
      "17: Encoding Loss -0.7809625267982483, Transition Loss -8.175257682800293, Classifier Loss 0.10064715892076492, Total Loss 10.063081741333008\n",
      "17: Encoding Loss 0.4665440618991852, Transition Loss -9.147438049316406, Classifier Loss 0.10150343179702759, Total Loss 13.880860328674316\n",
      "17: Encoding Loss -0.17810453474521637, Transition Loss 2.2835958003997803, Classifier Loss 0.14028345048427582, Total Loss 14.431700706481934\n",
      "17: Encoding Loss 0.13821634650230408, Transition Loss -4.151040077209473, Classifier Loss 0.07503416389226913, Total Loss 8.516031265258789\n",
      "17: Encoding Loss 0.8840781450271606, Transition Loss -7.5474853515625, Classifier Loss 0.10468343645334244, Total Loss 17.539461135864258\n",
      "17: Encoding Loss 0.2685322165489197, Transition Loss -5.732662677764893, Classifier Loss 0.10269058495759964, Total Loss 12.40838623046875\n",
      "17: Encoding Loss -1.070029377937317, Transition Loss -4.351493835449219, Classifier Loss 0.08458388596773148, Total Loss 8.457517623901367\n",
      "17: Encoding Loss -0.9100332260131836, Transition Loss -0.7203419208526611, Classifier Loss 0.11034154891967773, Total Loss 11.034010887145996\n",
      "17: Encoding Loss -1.4820687770843506, Transition Loss -8.297135353088379, Classifier Loss 0.06820541620254517, Total Loss 6.818881988525391\n",
      "17: Encoding Loss 0.692651629447937, Transition Loss 0.7209105491638184, Classifier Loss 0.08770600706338882, Total Loss 14.455995559692383\n",
      "17: Encoding Loss 0.0157703198492527, Transition Loss -3.315880298614502, Classifier Loss 0.07023058831691742, Total Loss 7.093381404876709\n",
      "17: Encoding Loss -0.2956183850765228, Transition Loss -2.721759796142578, Classifier Loss 0.0948639065027237, Total Loss 9.482163429260254\n",
      "17: Encoding Loss 0.16943474113941193, Transition Loss -11.672836303710938, Classifier Loss 0.16058897972106934, Total Loss 17.350910186767578\n",
      "17: Encoding Loss -0.6421034932136536, Transition Loss -10.467547416687012, Classifier Loss 0.12174826860427856, Total Loss 12.172733306884766\n",
      "17: Encoding Loss -1.334542155265808, Transition Loss -15.274978637695312, Classifier Loss 0.08121292293071747, Total Loss 8.118237495422363\n",
      "17: Encoding Loss 0.4628187119960785, Transition Loss -7.0526580810546875, Classifier Loss 0.12069657444953918, Total Loss 15.770790100097656\n",
      "17: Encoding Loss -0.7885238528251648, Transition Loss -7.276838302612305, Classifier Loss 0.10159345716238022, Total Loss 10.157890319824219\n",
      "17: Encoding Loss -1.6558868885040283, Transition Loss -13.561247825622559, Classifier Loss 0.09765517711639404, Total Loss 9.762805938720703\n",
      "17: Encoding Loss -1.1072463989257812, Transition Loss -1.6811566352844238, Classifier Loss 0.1264006346464157, Total Loss 12.639726638793945\n",
      "17: Encoding Loss -0.048306483775377274, Transition Loss -10.839377403259277, Classifier Loss 0.10711309313774109, Total Loss 10.587593078613281\n",
      "17: Encoding Loss -0.4779682755470276, Transition Loss -11.697693824768066, Classifier Loss 0.04967210814356804, Total Loss 4.964868068695068\n",
      "17: Encoding Loss -1.2627276182174683, Transition Loss -15.137200355529785, Classifier Loss 0.09343484044075012, Total Loss 9.340456008911133\n",
      "17: Encoding Loss -2.1471502780914307, Transition Loss -19.66813087463379, Classifier Loss 0.05415242537856102, Total Loss 5.411309242248535\n",
      "18: Encoding Loss -0.4974971413612366, Transition Loss -0.4520260691642761, Classifier Loss 0.05776270478963852, Total Loss 5.776178359985352\n",
      "18: Encoding Loss -0.832281231880188, Transition Loss -0.13854706287384033, Classifier Loss 0.14110471308231354, Total Loss 14.110444068908691\n",
      "18: Encoding Loss -0.6428735256195068, Transition Loss 4.024467468261719, Classifier Loss 0.06650912761688232, Total Loss 7.455806255340576\n",
      "18: Encoding Loss -1.0440610647201538, Transition Loss -13.442399978637695, Classifier Loss 0.10354230552911758, Total Loss 10.351542472839355\n",
      "18: Encoding Loss -0.824319064617157, Transition Loss -3.926652193069458, Classifier Loss 0.07014523446559906, Total Loss 7.01373815536499\n",
      "18: Encoding Loss -0.9708036780357361, Transition Loss -10.940591812133789, Classifier Loss 0.08809912204742432, Total Loss 8.807723999023438\n",
      "18: Encoding Loss -0.9314108490943909, Transition Loss -6.887561321258545, Classifier Loss 0.0985192283987999, Total Loss 9.850545883178711\n",
      "18: Encoding Loss -0.7578552961349487, Transition Loss -4.923357963562012, Classifier Loss 0.07470598816871643, Total Loss 7.469614028930664\n",
      "18: Encoding Loss -0.38936442136764526, Transition Loss -10.602036476135254, Classifier Loss 0.07260295003652573, Total Loss 7.258020401000977\n",
      "18: Encoding Loss -1.3003671169281006, Transition Loss -7.833731651306152, Classifier Loss 0.10313526540994644, Total Loss 10.311959266662598\n",
      "18: Encoding Loss 1.138587474822998, Transition Loss -4.3878984451293945, Classifier Loss 0.08950916677713394, Total Loss 18.058738708496094\n",
      "18: Encoding Loss 0.2625581622123718, Transition Loss -1.2575998306274414, Classifier Loss 0.09730055183172226, Total Loss 11.821184158325195\n",
      "18: Encoding Loss -0.8288441896438599, Transition Loss -4.167296886444092, Classifier Loss 0.07387804985046387, Total Loss 7.386971473693848\n",
      "18: Encoding Loss -0.6337403059005737, Transition Loss -3.4971494674682617, Classifier Loss 0.09557431936264038, Total Loss 9.556733131408691\n",
      "18: Encoding Loss -1.9435820579528809, Transition Loss -11.058065414428711, Classifier Loss 0.08173519372940063, Total Loss 8.171307563781738\n",
      "18: Encoding Loss -1.4596294164657593, Transition Loss -15.522760391235352, Classifier Loss 0.07475964725017548, Total Loss 7.472859859466553\n",
      "18: Encoding Loss -0.5952521562576294, Transition Loss -5.725842475891113, Classifier Loss 0.044328801333904266, Total Loss 4.431734561920166\n",
      "18: Encoding Loss 0.47804784774780273, Transition Loss -3.2483224868774414, Classifier Loss 0.08779589831829071, Total Loss 12.603320121765137\n",
      "18: Encoding Loss 0.6539708971977234, Transition Loss -9.44668197631836, Classifier Loss 0.12635059654712677, Total Loss 17.86493682861328\n",
      "18: Encoding Loss 0.40916743874549866, Transition Loss -2.8375837802886963, Classifier Loss 0.07694831490516663, Total Loss 10.967533111572266\n",
      "18: Encoding Loss 0.7215534448623657, Transition Loss -11.455184936523438, Classifier Loss 0.11077911406755447, Total Loss 16.848047256469727\n",
      "18: Encoding Loss -0.2264082282781601, Transition Loss -6.946244716644287, Classifier Loss 0.07261615991592407, Total Loss 7.238881587982178\n",
      "18: Encoding Loss 0.07444347441196442, Transition Loss -9.050253868103027, Classifier Loss 0.11162253469228745, Total Loss 11.620023727416992\n",
      "18: Encoding Loss -0.6353703737258911, Transition Loss -1.4733537435531616, Classifier Loss 0.11139988899230957, Total Loss 11.139694213867188\n",
      "18: Encoding Loss -1.3787915706634521, Transition Loss -8.229763984680176, Classifier Loss 0.06182410940527916, Total Loss 6.180764675140381\n",
      "18: Encoding Loss -0.7409597039222717, Transition Loss -1.7904930114746094, Classifier Loss 0.08651012182235718, Total Loss 8.650654792785645\n",
      "18: Encoding Loss -0.728985071182251, Transition Loss -0.19186365604400635, Classifier Loss 0.1183655858039856, Total Loss 11.836520195007324\n",
      "18: Encoding Loss -2.1807570457458496, Transition Loss -15.109363555908203, Classifier Loss 0.11098457872867584, Total Loss 11.095436096191406\n",
      "18: Encoding Loss -1.7801545858383179, Transition Loss -6.442714214324951, Classifier Loss 0.09174793213605881, Total Loss 9.173504829406738\n",
      "18: Encoding Loss -0.541340172290802, Transition Loss -2.2266037464141846, Classifier Loss 0.09811224788427353, Total Loss 9.810779571533203\n",
      "18: Encoding Loss -0.2820548713207245, Transition Loss -7.72689962387085, Classifier Loss 0.10920586436986923, Total Loss 10.9136323928833\n",
      "18: Encoding Loss -0.44042810797691345, Transition Loss -0.8591145873069763, Classifier Loss 0.12274515628814697, Total Loss 12.274324417114258\n",
      "18: Encoding Loss 0.1560121476650238, Transition Loss -7.773818492889404, Classifier Loss 0.09639912098646164, Total Loss 10.812360763549805\n",
      "18: Encoding Loss 0.6434423923492432, Transition Loss -4.260514736175537, Classifier Loss 0.1827855408191681, Total Loss 23.425241470336914\n",
      "18: Encoding Loss -1.3046705722808838, Transition Loss -9.25928783416748, Classifier Loss 0.07443857938051224, Total Loss 7.4420061111450195\n",
      "18: Encoding Loss -0.7743618488311768, Transition Loss -4.4169416427612305, Classifier Loss 0.05370341241359711, Total Loss 5.369457721710205\n",
      "18: Encoding Loss -0.5910892486572266, Transition Loss -5.054102897644043, Classifier Loss 0.10863754898309708, Total Loss 10.862744331359863\n",
      "18: Encoding Loss -0.2725790739059448, Transition Loss -7.117778778076172, Classifier Loss 0.13386869430541992, Total Loss 13.378451347351074\n",
      "18: Encoding Loss -0.9489648342132568, Transition Loss -5.918313980102539, Classifier Loss 0.10590872913599014, Total Loss 10.589689254760742\n",
      "18: Encoding Loss -1.0937798023223877, Transition Loss -7.251099586486816, Classifier Loss 0.09843286871910095, Total Loss 9.841835975646973\n",
      "18: Encoding Loss -1.3513176441192627, Transition Loss -4.9184250831604, Classifier Loss 0.062143370509147644, Total Loss 6.213353157043457\n",
      "18: Encoding Loss -0.752015233039856, Transition Loss -11.364599227905273, Classifier Loss 0.12576040625572205, Total Loss 12.57376766204834\n",
      "18: Encoding Loss -1.3880805969238281, Transition Loss -11.251648902893066, Classifier Loss 0.060721635818481445, Total Loss 6.069913387298584\n",
      "18: Encoding Loss -0.9907819628715515, Transition Loss -1.4559427499771118, Classifier Loss 0.09688462316989899, Total Loss 9.68817138671875\n",
      "18: Encoding Loss -0.911483108997345, Transition Loss -12.356400489807129, Classifier Loss 0.07880917936563492, Total Loss 7.878446578979492\n",
      "18: Encoding Loss 0.7285624146461487, Transition Loss -3.8190135955810547, Classifier Loss 0.11069968342781067, Total Loss 16.897705078125\n",
      "18: Encoding Loss 1.2490025758743286, Transition Loss -12.175095558166504, Classifier Loss 0.10258210450410843, Total Loss 20.24779510498047\n",
      "18: Encoding Loss -1.102178692817688, Transition Loss -2.848001718521118, Classifier Loss 0.06006641685962677, Total Loss 6.006072044372559\n",
      "18: Encoding Loss -0.08292002975940704, Transition Loss -4.5226593017578125, Classifier Loss 0.09517388790845871, Total Loss 9.38149356842041\n",
      "18: Encoding Loss 0.23495270311832428, Transition Loss -11.245885848999023, Classifier Loss 0.08973396569490433, Total Loss 10.83310317993164\n",
      "18: Encoding Loss -1.033362627029419, Transition Loss -16.293394088745117, Classifier Loss 0.0724644586443901, Total Loss 7.243186950683594\n",
      "18: Encoding Loss 0.040700480341911316, Transition Loss -9.24880313873291, Classifier Loss 0.09432029724121094, Total Loss 9.644426345825195\n",
      "18: Encoding Loss -1.1897794008255005, Transition Loss -0.5186069011688232, Classifier Loss 0.11013087630271912, Total Loss 11.012983322143555\n",
      "18: Encoding Loss -1.2931263446807861, Transition Loss -8.433459281921387, Classifier Loss 0.07879078388214111, Total Loss 7.877391815185547\n",
      "18: Encoding Loss -0.5091474652290344, Transition Loss -7.939199924468994, Classifier Loss 0.09052801132202148, Total Loss 9.051212310791016\n",
      "18: Encoding Loss -1.1690629720687866, Transition Loss -1.553450345993042, Classifier Loss 0.11563990265130997, Total Loss 11.563679695129395\n",
      "18: Encoding Loss 0.2564360201358795, Transition Loss -7.295159339904785, Classifier Loss 0.06581835448741913, Total Loss 8.621261596679688\n",
      "18: Encoding Loss -1.3669341802597046, Transition Loss -8.46253490447998, Classifier Loss 0.0934860035777092, Total Loss 9.346907615661621\n",
      "18: Encoding Loss -1.355708360671997, Transition Loss -15.544839859008789, Classifier Loss 0.08017002046108246, Total Loss 8.013893127441406\n",
      "18: Encoding Loss -1.1699997186660767, Transition Loss -11.546656608581543, Classifier Loss 0.09745969623327255, Total Loss 9.743659973144531\n",
      "18: Encoding Loss -0.7394042611122131, Transition Loss -5.8370490074157715, Classifier Loss 0.08589670807123184, Total Loss 8.58850383758545\n",
      "18: Encoding Loss -1.03853178024292, Transition Loss -4.2069549560546875, Classifier Loss 0.13534627854824066, Total Loss 13.53378677368164\n",
      "18: Encoding Loss -0.8487333655357361, Transition Loss -6.465632915496826, Classifier Loss 0.09947805851697922, Total Loss 9.946512222290039\n",
      "18: Encoding Loss -0.7045470476150513, Transition Loss -4.261595249176025, Classifier Loss 0.09092487394809723, Total Loss 9.091634750366211\n",
      "18: Encoding Loss -0.8795225620269775, Transition Loss -8.808496475219727, Classifier Loss 0.08925581723451614, Total Loss 8.923820495605469\n",
      "18: Encoding Loss -1.015098214149475, Transition Loss -1.4715757369995117, Classifier Loss 0.14592228829860687, Total Loss 14.591934204101562\n",
      "18: Encoding Loss 0.7436392307281494, Transition Loss 2.151127815246582, Classifier Loss 0.08135643601417542, Total Loss 14.514983177185059\n",
      "18: Encoding Loss -1.5970160961151123, Transition Loss -13.469326972961426, Classifier Loss 0.11319418996572495, Total Loss 11.31672477722168\n",
      "18: Encoding Loss 0.2821473181247711, Transition Loss -4.362250328063965, Classifier Loss 0.09164644777774811, Total Loss 11.415555953979492\n",
      "18: Encoding Loss 0.45551568269729614, Transition Loss -6.458209991455078, Classifier Loss 0.14397189021110535, Total Loss 18.040014266967773\n",
      "18: Encoding Loss -0.9493974447250366, Transition Loss -5.635375022888184, Classifier Loss 0.11220520734786987, Total Loss 11.219393730163574\n",
      "18: Encoding Loss -0.9654694199562073, Transition Loss -5.984375953674316, Classifier Loss 0.10286854207515717, Total Loss 10.285656929016113\n",
      "18: Encoding Loss -0.287072092294693, Transition Loss -2.029805898666382, Classifier Loss 0.13460694253444672, Total Loss 13.455585479736328\n",
      "18: Encoding Loss -0.7919016480445862, Transition Loss -5.356109619140625, Classifier Loss 0.04620145633816719, Total Loss 4.61907434463501\n",
      "18: Encoding Loss -1.0366147756576538, Transition Loss -4.07678747177124, Classifier Loss 0.15075363218784332, Total Loss 15.07454776763916\n",
      "18: Encoding Loss -1.0765745639801025, Transition Loss -10.519113540649414, Classifier Loss 0.07107614725828171, Total Loss 7.105510711669922\n",
      "18: Encoding Loss -0.9472019076347351, Transition Loss -2.663179636001587, Classifier Loss 0.10692745447158813, Total Loss 10.692212104797363\n",
      "18: Encoding Loss -2.2538113594055176, Transition Loss -8.249675750732422, Classifier Loss 0.0879335105419159, Total Loss 8.791701316833496\n",
      "18: Encoding Loss -1.2253934144973755, Transition Loss -9.168411254882812, Classifier Loss 0.05552125722169876, Total Loss 5.550292015075684\n",
      "18: Encoding Loss -0.6023593544960022, Transition Loss -6.493928909301758, Classifier Loss 0.1331179291009903, Total Loss 13.310494422912598\n",
      "18: Encoding Loss -0.3106067478656769, Transition Loss -7.472146987915039, Classifier Loss 0.08744646608829498, Total Loss 8.74079704284668\n",
      "18: Encoding Loss -1.1901196241378784, Transition Loss -4.67087984085083, Classifier Loss 0.08397635817527771, Total Loss 8.396700859069824\n",
      "18: Encoding Loss -0.5416618585586548, Transition Loss -2.034806728363037, Classifier Loss 0.11398802697658539, Total Loss 11.398395538330078\n",
      "18: Encoding Loss -0.050445254892110825, Transition Loss -7.632855415344238, Classifier Loss 0.13063167035579681, Total Loss 12.93775749206543\n",
      "18: Encoding Loss -1.8987241983413696, Transition Loss -4.623532295227051, Classifier Loss 0.08416364341974258, Total Loss 8.41543960571289\n",
      "18: Encoding Loss -1.1819674968719482, Transition Loss -1.8549909591674805, Classifier Loss 0.08608167618513107, Total Loss 8.607796669006348\n",
      "18: Encoding Loss -1.601523995399475, Transition Loss -12.020344734191895, Classifier Loss 0.09393232315778732, Total Loss 9.390828132629395\n",
      "18: Encoding Loss -0.9533445835113525, Transition Loss -5.912921905517578, Classifier Loss 0.12537027895450592, Total Loss 12.535845756530762\n",
      "18: Encoding Loss -0.40370529890060425, Transition Loss -4.490625381469727, Classifier Loss 0.07355941087007523, Total Loss 7.354955673217773\n",
      "18: Encoding Loss 0.0661795437335968, Transition Loss -8.832094192504883, Classifier Loss 0.09577266871929169, Total Loss 9.970433235168457\n",
      "18: Encoding Loss -2.9393417835235596, Transition Loss -17.45195960998535, Classifier Loss 0.09025957435369492, Total Loss 9.022466659545898\n",
      "18: Encoding Loss -0.693000853061676, Transition Loss -4.854218482971191, Classifier Loss 0.12087700515985489, Total Loss 12.086730003356934\n",
      "18: Encoding Loss -1.6888383626937866, Transition Loss -4.575183868408203, Classifier Loss 0.09551770985126495, Total Loss 9.550856590270996\n",
      "18: Encoding Loss -0.3608206510543823, Transition Loss -7.15943717956543, Classifier Loss 0.1068468764424324, Total Loss 10.68281078338623\n",
      "18: Encoding Loss -0.5548660755157471, Transition Loss -1.7167304754257202, Classifier Loss 0.09433306753635406, Total Loss 9.432963371276855\n",
      "18: Encoding Loss -1.0048025846481323, Transition Loss -9.432188034057617, Classifier Loss 0.10551901906728745, Total Loss 10.550015449523926\n",
      "18: Encoding Loss -0.8741121292114258, Transition Loss -12.08654499053955, Classifier Loss 0.11333028227090836, Total Loss 11.330610275268555\n",
      "18: Encoding Loss -1.3050532341003418, Transition Loss -9.808826446533203, Classifier Loss 0.07799328863620758, Total Loss 7.797367095947266\n",
      "18: Encoding Loss -0.3341609835624695, Transition Loss -1.2517871856689453, Classifier Loss 0.05948645994067192, Total Loss 5.947282314300537\n",
      "18: Encoding Loss -0.8741876482963562, Transition Loss -4.247664451599121, Classifier Loss 0.13384926319122314, Total Loss 13.384076118469238\n",
      "18: Encoding Loss -1.1491780281066895, Transition Loss -10.956089973449707, Classifier Loss 0.0971161499619484, Total Loss 9.709423065185547\n",
      "18: Encoding Loss 0.21062402427196503, Transition Loss 2.1891467571258545, Classifier Loss 0.1322965919971466, Total Loss 15.32283878326416\n",
      "18: Encoding Loss -0.35859742760658264, Transition Loss -4.47197961807251, Classifier Loss 0.08654799312353134, Total Loss 8.653423309326172\n",
      "18: Encoding Loss -0.25153619050979614, Transition Loss -2.141690731048584, Classifier Loss 0.11163293570280075, Total Loss 11.150901794433594\n",
      "18: Encoding Loss -1.0259754657745361, Transition Loss -9.159292221069336, Classifier Loss 0.0792309120297432, Total Loss 7.92125940322876\n",
      "18: Encoding Loss -0.9626649022102356, Transition Loss -11.684792518615723, Classifier Loss 0.11411845684051514, Total Loss 11.409509658813477\n",
      "18: Encoding Loss -0.8847489953041077, Transition Loss -1.4662508964538574, Classifier Loss 0.11714162677526474, Total Loss 11.71387004852295\n",
      "18: Encoding Loss -1.3488484621047974, Transition Loss -4.876398086547852, Classifier Loss 0.05600358545780182, Total Loss 5.599383354187012\n",
      "18: Encoding Loss -1.7876209020614624, Transition Loss -5.735223293304443, Classifier Loss 0.08406288921833038, Total Loss 8.405141830444336\n",
      "18: Encoding Loss -0.38228291273117065, Transition Loss -11.395895004272461, Classifier Loss 0.09732881188392639, Total Loss 9.730401039123535\n",
      "18: Encoding Loss -0.7312812805175781, Transition Loss -10.615220069885254, Classifier Loss 0.1011534035205841, Total Loss 10.1132173538208\n",
      "18: Encoding Loss -1.1083413362503052, Transition Loss 1.1185088157653809, Classifier Loss 0.061569929122924805, Total Loss 6.38069486618042\n",
      "18: Encoding Loss -1.9265015125274658, Transition Loss -13.732858657836914, Classifier Loss 0.0712471604347229, Total Loss 7.121969223022461\n",
      "18: Encoding Loss -1.2516608238220215, Transition Loss -13.001066207885742, Classifier Loss 0.07527131587266922, Total Loss 7.524531364440918\n",
      "18: Encoding Loss 0.25749385356903076, Transition Loss 1.522977352142334, Classifier Loss 0.0893450602889061, Total Loss 11.288726806640625\n",
      "18: Encoding Loss -0.23758751153945923, Transition Loss 5.48249626159668, Classifier Loss 0.07947254925966263, Total Loss 9.027115821838379\n",
      "18: Encoding Loss -0.5872770547866821, Transition Loss -7.6806864738464355, Classifier Loss 0.07198378443717957, Total Loss 7.196842193603516\n",
      "18: Encoding Loss -0.9471710324287415, Transition Loss -7.2962141036987305, Classifier Loss 0.08050929009914398, Total Loss 8.049469947814941\n",
      "18: Encoding Loss -1.9749282598495483, Transition Loss -8.3851957321167, Classifier Loss 0.08407814800739288, Total Loss 8.406137466430664\n",
      "18: Encoding Loss -0.39510536193847656, Transition Loss -3.2207586765289307, Classifier Loss 0.08541814982891083, Total Loss 8.541048049926758\n",
      "18: Encoding Loss -1.8000084161758423, Transition Loss -10.301251411437988, Classifier Loss 0.06722970306873322, Total Loss 6.72091007232666\n",
      "18: Encoding Loss -0.37694084644317627, Transition Loss -4.318694591522217, Classifier Loss 0.09850200265645981, Total Loss 9.849089622497559\n",
      "18: Encoding Loss -0.02391575090587139, Transition Loss -7.440677165985107, Classifier Loss 0.12419711798429489, Total Loss 12.340642929077148\n",
      "18: Encoding Loss -2.046370267868042, Transition Loss -15.090773582458496, Classifier Loss 0.08707527816295624, Total Loss 8.704509735107422\n",
      "18: Encoding Loss -1.411771535873413, Transition Loss -3.9513676166534424, Classifier Loss 0.09999427944421768, Total Loss 9.998637199401855\n",
      "18: Encoding Loss -1.043580412864685, Transition Loss -7.2602081298828125, Classifier Loss 0.14697740972042084, Total Loss 14.696288108825684\n",
      "18: Encoding Loss -1.1624126434326172, Transition Loss -12.271395683288574, Classifier Loss 0.10795058310031891, Total Loss 10.792604446411133\n",
      "18: Encoding Loss -1.9971702098846436, Transition Loss -7.943863391876221, Classifier Loss 0.048248954117298126, Total Loss 4.823306560516357\n",
      "18: Encoding Loss -0.6268265247344971, Transition Loss -8.768495559692383, Classifier Loss 0.08546725660562515, Total Loss 8.544971466064453\n",
      "18: Encoding Loss -1.4357330799102783, Transition Loss -1.9588921070098877, Classifier Loss 0.07159718871116638, Total Loss 7.159327030181885\n",
      "18: Encoding Loss -0.09082861244678497, Transition Loss -4.988210201263428, Classifier Loss 0.09106317162513733, Total Loss 8.973173141479492\n",
      "18: Encoding Loss -0.7915493249893188, Transition Loss -5.5480756759643555, Classifier Loss 0.08162645995616913, Total Loss 8.16153621673584\n",
      "18: Encoding Loss -0.5970314145088196, Transition Loss -1.164536714553833, Classifier Loss 0.07297085225582123, Total Loss 7.2968525886535645\n",
      "18: Encoding Loss -0.3529142737388611, Transition Loss -7.0628814697265625, Classifier Loss 0.05044841393828392, Total Loss 5.042840480804443\n",
      "18: Encoding Loss -1.3679072856903076, Transition Loss -7.993782043457031, Classifier Loss 0.09940249472856522, Total Loss 9.938651084899902\n",
      "18: Encoding Loss -0.09198837727308273, Transition Loss -8.913907051086426, Classifier Loss 0.09228190034627914, Total Loss 9.094815254211426\n",
      "18: Encoding Loss -0.5843927264213562, Transition Loss 2.2396700382232666, Classifier Loss 0.13687409460544586, Total Loss 14.135343551635742\n",
      "18: Encoding Loss -0.2761439383029938, Transition Loss -4.100721836090088, Classifier Loss 0.06667358428239822, Total Loss 6.660181522369385\n",
      "18: Encoding Loss 0.9434745907783508, Transition Loss -7.704246997833252, Classifier Loss 0.10706104338169098, Total Loss 18.25235939025879\n",
      "18: Encoding Loss 0.6193972229957581, Transition Loss -5.772190570831299, Classifier Loss 0.09604059159755707, Total Loss 14.55808162689209\n",
      "18: Encoding Loss -1.1465741395950317, Transition Loss -5.3628973960876465, Classifier Loss 0.07652393728494644, Total Loss 7.6513214111328125\n",
      "18: Encoding Loss -0.9427246451377869, Transition Loss -1.6803971529006958, Classifier Loss 0.10783666372299194, Total Loss 10.783330917358398\n",
      "18: Encoding Loss -1.6727882623672485, Transition Loss -9.372586250305176, Classifier Loss 0.06492206454277039, Total Loss 6.490332126617432\n",
      "18: Encoding Loss 0.6480839252471924, Transition Loss 0.10519558191299438, Classifier Loss 0.08433762192726135, Total Loss 13.639472961425781\n",
      "18: Encoding Loss -0.2025672346353531, Transition Loss -3.664320468902588, Classifier Loss 0.06357784569263458, Total Loss 6.322373867034912\n",
      "18: Encoding Loss -0.011718683876097202, Transition Loss -2.6584298610687256, Classifier Loss 0.09631980955600739, Total Loss 9.588947296142578\n",
      "18: Encoding Loss -0.057880982756614685, Transition Loss -12.02108097076416, Classifier Loss 0.16247928142547607, Total Loss 16.1152400970459\n",
      "18: Encoding Loss -1.3339500427246094, Transition Loss -11.81408405303955, Classifier Loss 0.11580075323581696, Total Loss 11.577712059020996\n",
      "18: Encoding Loss -2.2302212715148926, Transition Loss -16.98160743713379, Classifier Loss 0.07223767042160034, Total Loss 7.220370769500732\n",
      "18: Encoding Loss 0.08834990859031677, Transition Loss -7.985497951507568, Classifier Loss 0.11502639949321747, Total Loss 12.07462215423584\n",
      "18: Encoding Loss -0.8782721161842346, Transition Loss -7.782188892364502, Classifier Loss 0.09720005840063095, Total Loss 9.718449592590332\n",
      "18: Encoding Loss -1.8893381357192993, Transition Loss -13.991077423095703, Classifier Loss 0.09027183055877686, Total Loss 9.024385452270508\n",
      "18: Encoding Loss -1.3095077276229858, Transition Loss -2.1910836696624756, Classifier Loss 0.12095171213150024, Total Loss 12.094732284545898\n",
      "18: Encoding Loss -0.06332603096961975, Transition Loss -11.048995971679688, Classifier Loss 0.11434124410152435, Total Loss 11.29853343963623\n",
      "18: Encoding Loss -0.7455666661262512, Transition Loss -11.992836952209473, Classifier Loss 0.04121815040707588, Total Loss 4.1194167137146\n",
      "18: Encoding Loss -1.3807967901229858, Transition Loss -15.348883628845215, Classifier Loss 0.08651650696992874, Total Loss 8.648580551147461\n",
      "18: Encoding Loss -2.352057933807373, Transition Loss -19.98929786682129, Classifier Loss 0.05010561645030975, Total Loss 5.006563663482666\n",
      "19: Encoding Loss -0.6338427066802979, Transition Loss -0.8155221343040466, Classifier Loss 0.053589820861816406, Total Loss 5.358819007873535\n",
      "19: Encoding Loss -1.0816080570220947, Transition Loss -0.7774562239646912, Classifier Loss 0.1356751024723053, Total Loss 13.567355155944824\n",
      "19: Encoding Loss -0.82518070936203, Transition Loss 3.4964041709899902, Classifier Loss 0.06297540664672852, Total Loss 6.996821403503418\n",
      "19: Encoding Loss -1.2299689054489136, Transition Loss -13.771465301513672, Classifier Loss 0.09639428555965424, Total Loss 9.636673927307129\n",
      "19: Encoding Loss -0.8339632153511047, Transition Loss -4.428057670593262, Classifier Loss 0.06646603345870972, Total Loss 6.645718097686768\n",
      "19: Encoding Loss -1.0483787059783936, Transition Loss -11.128508567810059, Classifier Loss 0.08030581474304199, Total Loss 8.028355598449707\n",
      "19: Encoding Loss -1.022657871246338, Transition Loss -7.243849277496338, Classifier Loss 0.09430544823408127, Total Loss 9.429096221923828\n",
      "19: Encoding Loss -0.853192925453186, Transition Loss -5.074506759643555, Classifier Loss 0.07181017100811005, Total Loss 7.180002212524414\n",
      "19: Encoding Loss -0.5225169658660889, Transition Loss -10.989409446716309, Classifier Loss 0.07085246592760086, Total Loss 7.083048343658447\n",
      "19: Encoding Loss -1.4602491855621338, Transition Loss -8.263907432556152, Classifier Loss 0.0989392027258873, Total Loss 9.892267227172852\n",
      "19: Encoding Loss 0.808557391166687, Transition Loss -4.807465553283691, Classifier Loss 0.08616548776626587, Total Loss 15.084047317504883\n",
      "19: Encoding Loss 0.47665292024612427, Transition Loss -1.8826497793197632, Classifier Loss 0.09579136967658997, Total Loss 13.391980171203613\n",
      "19: Encoding Loss -0.8215738534927368, Transition Loss -4.219743728637695, Classifier Loss 0.06882824003696442, Total Loss 6.881979942321777\n",
      "19: Encoding Loss -0.5550746321678162, Transition Loss -3.754225492477417, Classifier Loss 0.09537665545940399, Total Loss 9.536914825439453\n",
      "19: Encoding Loss -1.8611690998077393, Transition Loss -10.72690200805664, Classifier Loss 0.07838604599237442, Total Loss 7.836459159851074\n",
      "19: Encoding Loss -1.6025183200836182, Transition Loss -14.93765926361084, Classifier Loss 0.07746758311986923, Total Loss 7.743771076202393\n",
      "19: Encoding Loss -0.4522605538368225, Transition Loss -5.895625591278076, Classifier Loss 0.041660793125629425, Total Loss 4.164889335632324\n",
      "19: Encoding Loss 0.2824360132217407, Transition Loss -3.5002312660217285, Classifier Loss 0.08208389580249786, Total Loss 10.461825370788574\n",
      "19: Encoding Loss 0.5950567126274109, Transition Loss -10.71310806274414, Classifier Loss 0.12196584045886993, Total Loss 16.95489501953125\n",
      "19: Encoding Loss 0.8334188461303711, Transition Loss -2.603966236114502, Classifier Loss 0.07385531067848206, Total Loss 14.052360534667969\n",
      "19: Encoding Loss 1.3598164319992065, Transition Loss -12.820746421813965, Classifier Loss 0.10589601844549179, Total Loss 21.46556854248047\n",
      "19: Encoding Loss 0.0910901129245758, Transition Loss -7.085353374481201, Classifier Loss 0.0648844912648201, Total Loss 7.083727836608887\n",
      "19: Encoding Loss -0.9084555506706238, Transition Loss -10.790669441223145, Classifier Loss 0.11489197611808777, Total Loss 11.487039566040039\n",
      "19: Encoding Loss -0.5972509384155273, Transition Loss -2.4159460067749023, Classifier Loss 0.10760606825351715, Total Loss 10.760123252868652\n",
      "19: Encoding Loss -1.390344262123108, Transition Loss -9.757203102111816, Classifier Loss 0.05702400207519531, Total Loss 5.700448989868164\n",
      "19: Encoding Loss -0.5780567526817322, Transition Loss -2.918208599090576, Classifier Loss 0.08470343798398972, Total Loss 8.469759941101074\n",
      "19: Encoding Loss -0.8000453114509583, Transition Loss -1.4905709028244019, Classifier Loss 0.11796636879444122, Total Loss 11.796338081359863\n",
      "19: Encoding Loss -2.0647358894348145, Transition Loss -16.80476188659668, Classifier Loss 0.10731324553489685, Total Loss 10.7279634475708\n",
      "19: Encoding Loss -1.629285454750061, Transition Loss -7.677356243133545, Classifier Loss 0.08637446910142899, Total Loss 8.63591194152832\n",
      "19: Encoding Loss -0.4902075529098511, Transition Loss -3.385134220123291, Classifier Loss 0.09399070590734482, Total Loss 9.398391723632812\n",
      "19: Encoding Loss -0.2974991500377655, Transition Loss -9.105486869812012, Classifier Loss 0.10958045721054077, Total Loss 10.952737808227539\n",
      "19: Encoding Loss -0.4175001084804535, Transition Loss -1.6988766193389893, Classifier Loss 0.11582647264003754, Total Loss 11.582258224487305\n",
      "19: Encoding Loss -0.06417520344257355, Transition Loss -9.071237564086914, Classifier Loss 0.09460650384426117, Total Loss 9.32508659362793\n",
      "19: Encoding Loss 0.5988386273384094, Transition Loss -5.000866889953613, Classifier Loss 0.18729764223098755, Total Loss 23.519474029541016\n",
      "19: Encoding Loss -1.3556993007659912, Transition Loss -9.82840633392334, Classifier Loss 0.06406022608280182, Total Loss 6.404057025909424\n",
      "19: Encoding Loss -0.810463011264801, Transition Loss -5.0102643966674805, Classifier Loss 0.052540943026542664, Total Loss 5.253092288970947\n",
      "19: Encoding Loss -0.7149749994277954, Transition Loss -5.530849456787109, Classifier Loss 0.1125868409872055, Total Loss 11.257577896118164\n",
      "19: Encoding Loss -0.34907835721969604, Transition Loss -7.447209358215332, Classifier Loss 0.13179415464401245, Total Loss 13.177253723144531\n",
      "19: Encoding Loss -0.983536422252655, Transition Loss -6.685128688812256, Classifier Loss 0.0937720313668251, Total Loss 9.375865936279297\n",
      "19: Encoding Loss -1.1088480949401855, Transition Loss -8.02373218536377, Classifier Loss 0.09496874362230301, Total Loss 9.495269775390625\n",
      "19: Encoding Loss -1.2735563516616821, Transition Loss -5.544610500335693, Classifier Loss 0.0577852800488472, Total Loss 5.777419090270996\n",
      "19: Encoding Loss -0.9542396664619446, Transition Loss -12.009910583496094, Classifier Loss 0.11685006320476532, Total Loss 11.68260383605957\n",
      "19: Encoding Loss -1.5345324277877808, Transition Loss -11.970207214355469, Classifier Loss 0.05697432532906532, Total Loss 5.695038318634033\n",
      "19: Encoding Loss -1.1679261922836304, Transition Loss -2.3570146560668945, Classifier Loss 0.09460704028606415, Total Loss 9.460232734680176\n",
      "19: Encoding Loss -1.1337895393371582, Transition Loss -12.895807266235352, Classifier Loss 0.07565250992774963, Total Loss 7.562671661376953\n",
      "19: Encoding Loss 0.6148030757904053, Transition Loss -4.503190994262695, Classifier Loss 0.108067587018013, Total Loss 15.724283218383789\n",
      "19: Encoding Loss 0.8104236721992493, Transition Loss -13.47476577758789, Classifier Loss 0.09736593812704086, Total Loss 16.217288970947266\n",
      "19: Encoding Loss -1.269789218902588, Transition Loss -3.511953353881836, Classifier Loss 0.059203919023275375, Total Loss 5.919689655303955\n",
      "19: Encoding Loss -0.2410261631011963, Transition Loss -5.3644256591796875, Classifier Loss 0.08760020136833191, Total Loss 8.743578910827637\n",
      "19: Encoding Loss -0.029208052903413773, Transition Loss -12.002720832824707, Classifier Loss 0.07724516093730927, Total Loss 7.6321282386779785\n",
      "19: Encoding Loss -0.9962867498397827, Transition Loss -15.870238304138184, Classifier Loss 0.06435970962047577, Total Loss 6.432796955108643\n",
      "19: Encoding Loss 0.01119063887745142, Transition Loss -9.122334480285645, Classifier Loss 0.0860629603266716, Total Loss 8.65322208404541\n",
      "19: Encoding Loss -1.115168809890747, Transition Loss -1.132751703262329, Classifier Loss 0.09612661600112915, Total Loss 9.612434387207031\n",
      "19: Encoding Loss -1.2660800218582153, Transition Loss -9.184688568115234, Classifier Loss 0.07395181804895401, Total Loss 7.393344879150391\n",
      "19: Encoding Loss -0.40504521131515503, Transition Loss -8.720744132995605, Classifier Loss 0.08638278394937515, Total Loss 8.63645076751709\n",
      "19: Encoding Loss -1.21433687210083, Transition Loss -2.1803321838378906, Classifier Loss 0.10520963370800018, Total Loss 10.520527839660645\n",
      "19: Encoding Loss 0.18122892081737518, Transition Loss -8.056450843811035, Classifier Loss 0.05906209722161293, Total Loss 7.3037285804748535\n",
      "19: Encoding Loss -1.543686032295227, Transition Loss -8.945960998535156, Classifier Loss 0.08880221098661423, Total Loss 8.878432273864746\n",
      "19: Encoding Loss -1.4458670616149902, Transition Loss -15.932348251342773, Classifier Loss 0.07280530035495758, Total Loss 7.277343273162842\n",
      "19: Encoding Loss -1.193889856338501, Transition Loss -12.097731590270996, Classifier Loss 0.09353652596473694, Total Loss 9.35123348236084\n",
      "19: Encoding Loss -0.7730085849761963, Transition Loss -6.235721111297607, Classifier Loss 0.08541329205036163, Total Loss 8.540081977844238\n",
      "19: Encoding Loss -1.0076580047607422, Transition Loss -4.6115031242370605, Classifier Loss 0.13098777830600739, Total Loss 13.097855567932129\n",
      "19: Encoding Loss -0.897710919380188, Transition Loss -6.873733997344971, Classifier Loss 0.09376475214958191, Total Loss 9.375100135803223\n",
      "19: Encoding Loss -0.8220759034156799, Transition Loss -4.5491719245910645, Classifier Loss 0.09262802451848984, Total Loss 9.261892318725586\n",
      "19: Encoding Loss -0.8980716466903687, Transition Loss -9.011822700500488, Classifier Loss 0.09862369298934937, Total Loss 9.860567092895508\n",
      "19: Encoding Loss -1.0662949085235596, Transition Loss -1.8440784215927124, Classifier Loss 0.14616285264492035, Total Loss 14.61591625213623\n",
      "19: Encoding Loss 0.6449839472770691, Transition Loss 1.7665443420410156, Classifier Loss 0.07739904522895813, Total Loss 13.253085136413574\n",
      "19: Encoding Loss -1.745337724685669, Transition Loss -14.402706146240234, Classifier Loss 0.1035645455121994, Total Loss 10.353574752807617\n",
      "19: Encoding Loss 0.2390713393688202, Transition Loss -5.236586093902588, Classifier Loss 0.08720647543668747, Total Loss 10.616090774536133\n",
      "19: Encoding Loss 0.25701794028282166, Transition Loss -6.775615692138672, Classifier Loss 0.14135852456092834, Total Loss 16.180192947387695\n",
      "19: Encoding Loss -1.0875297784805298, Transition Loss -6.163740634918213, Classifier Loss 0.11061622947454453, Total Loss 11.060389518737793\n",
      "19: Encoding Loss -1.015458345413208, Transition Loss -6.498359680175781, Classifier Loss 0.09890805929899216, Total Loss 9.889506340026855\n",
      "19: Encoding Loss -0.5347234606742859, Transition Loss -2.5783300399780273, Classifier Loss 0.1306343674659729, Total Loss 13.062920570373535\n",
      "19: Encoding Loss -0.7892786860466003, Transition Loss -5.9787187576293945, Classifier Loss 0.04423501342535019, Total Loss 4.422305583953857\n",
      "19: Encoding Loss -1.138023853302002, Transition Loss -4.696390628814697, Classifier Loss 0.14432889223098755, Total Loss 14.431949615478516\n",
      "19: Encoding Loss -1.188643217086792, Transition Loss -11.131908416748047, Classifier Loss 0.0690743699669838, Total Loss 6.905210494995117\n",
      "19: Encoding Loss -0.8498489260673523, Transition Loss -3.2258715629577637, Classifier Loss 0.10517096519470215, Total Loss 10.516450881958008\n",
      "19: Encoding Loss -2.3470966815948486, Transition Loss -8.926614761352539, Classifier Loss 0.09022647142410278, Total Loss 9.020861625671387\n",
      "19: Encoding Loss -1.3961732387542725, Transition Loss -9.841577529907227, Classifier Loss 0.050730206072330475, Total Loss 5.071052074432373\n",
      "19: Encoding Loss -0.7171871066093445, Transition Loss -7.07099723815918, Classifier Loss 0.12719637155532837, Total Loss 12.718222618103027\n",
      "19: Encoding Loss -0.43348729610443115, Transition Loss -8.091557502746582, Classifier Loss 0.08148501813411713, Total Loss 8.146857261657715\n",
      "19: Encoding Loss -1.3644673824310303, Transition Loss -5.337883949279785, Classifier Loss 0.08075786381959915, Total Loss 8.074719429016113\n",
      "19: Encoding Loss -0.6646713018417358, Transition Loss -2.552701473236084, Classifier Loss 0.10942082107067108, Total Loss 10.941572189331055\n",
      "19: Encoding Loss -0.1959698498249054, Transition Loss -8.153217315673828, Classifier Loss 0.12700232863426208, Total Loss 12.659383773803711\n",
      "19: Encoding Loss -1.8248382806777954, Transition Loss -4.867522239685059, Classifier Loss 0.08083883672952652, Total Loss 8.08290958404541\n",
      "19: Encoding Loss -1.0882742404937744, Transition Loss -2.233726978302002, Classifier Loss 0.08586151897907257, Total Loss 8.585705757141113\n",
      "19: Encoding Loss -1.5891839265823364, Transition Loss -12.262533187866211, Classifier Loss 0.09451921284198761, Total Loss 9.449468612670898\n",
      "19: Encoding Loss -0.878270149230957, Transition Loss -6.224596977233887, Classifier Loss 0.13392411172389984, Total Loss 13.391166687011719\n",
      "19: Encoding Loss -0.5482031106948853, Transition Loss -4.811196327209473, Classifier Loss 0.06628027558326721, Total Loss 6.627065181732178\n",
      "19: Encoding Loss -0.14564695954322815, Transition Loss -9.326688766479492, Classifier Loss 0.08345840871334076, Total Loss 8.259346961975098\n",
      "19: Encoding Loss -2.785653591156006, Transition Loss -17.4266357421875, Classifier Loss 0.08699074387550354, Total Loss 8.695589065551758\n",
      "19: Encoding Loss -0.8098449110984802, Transition Loss -4.586609840393066, Classifier Loss 0.11773111671209335, Total Loss 11.772193908691406\n",
      "19: Encoding Loss -1.5520390272140503, Transition Loss -4.377702713012695, Classifier Loss 0.09091196954250336, Total Loss 9.09032154083252\n",
      "19: Encoding Loss -0.5973538756370544, Transition Loss -6.852524757385254, Classifier Loss 0.10337740182876587, Total Loss 10.336369514465332\n",
      "19: Encoding Loss -0.47045403718948364, Transition Loss -1.406524896621704, Classifier Loss 0.0907297134399414, Total Loss 9.072685241699219\n",
      "19: Encoding Loss -1.0917035341262817, Transition Loss -9.349967956542969, Classifier Loss 0.09883590042591095, Total Loss 9.881719589233398\n",
      "19: Encoding Loss -1.1677978038787842, Transition Loss -12.073240280151367, Classifier Loss 0.10224518924951553, Total Loss 10.2221040725708\n",
      "19: Encoding Loss -1.594062089920044, Transition Loss -9.593135833740234, Classifier Loss 0.07390810549259186, Total Loss 7.388891696929932\n",
      "19: Encoding Loss -0.6653741002082825, Transition Loss -1.1878961324691772, Classifier Loss 0.05737913027405739, Total Loss 5.737675666809082\n",
      "19: Encoding Loss -0.88792484998703, Transition Loss -4.13510274887085, Classifier Loss 0.13103535771369934, Total Loss 13.10270881652832\n",
      "19: Encoding Loss -1.0304465293884277, Transition Loss -10.92319107055664, Classifier Loss 0.09607580304145813, Total Loss 9.605395317077637\n",
      "19: Encoding Loss 0.19471845030784607, Transition Loss 2.450289726257324, Classifier Loss 0.12745322287082672, Total Loss 14.753005981445312\n",
      "19: Encoding Loss -0.2655225396156311, Transition Loss -5.363191604614258, Classifier Loss 0.08419270813465118, Total Loss 8.40977954864502\n",
      "19: Encoding Loss -0.47522732615470886, Transition Loss -2.8381471633911133, Classifier Loss 0.10974129289388657, Total Loss 10.97355842590332\n",
      "19: Encoding Loss -1.0639632940292358, Transition Loss -9.966115951538086, Classifier Loss 0.07519856840372086, Total Loss 7.517863750457764\n",
      "19: Encoding Loss -0.8878945708274841, Transition Loss -12.732278823852539, Classifier Loss 0.10955797135829926, Total Loss 10.953250885009766\n",
      "19: Encoding Loss -0.9979095458984375, Transition Loss -2.248481273651123, Classifier Loss 0.10787273198366165, Total Loss 10.786823272705078\n",
      "19: Encoding Loss -1.2791434526443481, Transition Loss -5.8731865882873535, Classifier Loss 0.05289720371365547, Total Loss 5.288546085357666\n",
      "19: Encoding Loss -1.7826683521270752, Transition Loss -6.596498012542725, Classifier Loss 0.07987731695175171, Total Loss 7.986412525177002\n",
      "19: Encoding Loss -0.5154673457145691, Transition Loss -12.3043794631958, Classifier Loss 0.09220898151397705, Total Loss 9.218436241149902\n",
      "19: Encoding Loss -0.9146536588668823, Transition Loss -11.49105167388916, Classifier Loss 0.09819285571575165, Total Loss 9.816987037658691\n",
      "19: Encoding Loss -1.4091761112213135, Transition Loss 0.5138822197914124, Classifier Loss 0.06103673204779625, Total Loss 6.206449508666992\n",
      "19: Encoding Loss -2.226630687713623, Transition Loss -14.751416206359863, Classifier Loss 0.06639158725738525, Total Loss 6.636208534240723\n",
      "19: Encoding Loss -1.1669459342956543, Transition Loss -13.992544174194336, Classifier Loss 0.07271812856197357, Total Loss 7.269014358520508\n",
      "19: Encoding Loss 0.1639527678489685, Transition Loss 0.8673175573348999, Classifier Loss 0.08875143527984619, Total Loss 10.293924331665039\n",
      "19: Encoding Loss -0.22022010385990143, Transition Loss 5.114004611968994, Classifier Loss 0.07722338289022446, Total Loss 8.720781326293945\n",
      "19: Encoding Loss -0.4340147376060486, Transition Loss -7.634331703186035, Classifier Loss 0.06748861074447632, Total Loss 6.747309684753418\n",
      "19: Encoding Loss -1.038289189338684, Transition Loss -7.200822830200195, Classifier Loss 0.07033468782901764, Total Loss 7.032028675079346\n",
      "19: Encoding Loss -1.7888014316558838, Transition Loss -8.345402717590332, Classifier Loss 0.08068587630987167, Total Loss 8.06691837310791\n",
      "19: Encoding Loss -0.17707844078540802, Transition Loss -3.1499626636505127, Classifier Loss 0.07876534014940262, Total Loss 7.821649551391602\n",
      "19: Encoding Loss -1.3665554523468018, Transition Loss -9.806113243103027, Classifier Loss 0.06629213690757751, Total Loss 6.627252578735352\n",
      "19: Encoding Loss 0.2290024757385254, Transition Loss -3.9707236289978027, Classifier Loss 0.09544744342565536, Total Loss 11.355799674987793\n",
      "19: Encoding Loss 0.29680773615837097, Transition Loss -11.078354835510254, Classifier Loss 0.12926752865314484, Total Loss 15.295441627502441\n",
      "19: Encoding Loss -2.0273852348327637, Transition Loss -15.74461555480957, Classifier Loss 0.08542189002037048, Total Loss 8.539039611816406\n",
      "19: Encoding Loss -1.5349631309509277, Transition Loss -5.030253887176514, Classifier Loss 0.09924902021884918, Total Loss 9.923895835876465\n",
      "19: Encoding Loss -0.9617795348167419, Transition Loss -8.033836364746094, Classifier Loss 0.14503590762615204, Total Loss 14.501983642578125\n",
      "19: Encoding Loss -1.051953673362732, Transition Loss -12.95174789428711, Classifier Loss 0.10472185909748077, Total Loss 10.469595909118652\n",
      "19: Encoding Loss -2.1469035148620605, Transition Loss -9.007752418518066, Classifier Loss 0.046475380659103394, Total Loss 4.6457366943359375\n",
      "19: Encoding Loss -0.8114529848098755, Transition Loss -9.314260482788086, Classifier Loss 0.08427785336971283, Total Loss 8.425922393798828\n",
      "19: Encoding Loss -1.5121114253997803, Transition Loss -2.914255380630493, Classifier Loss 0.07612369954586029, Total Loss 7.61178731918335\n",
      "19: Encoding Loss -0.4739132225513458, Transition Loss -5.843383312225342, Classifier Loss 0.08841201663017273, Total Loss 8.8400297164917\n",
      "19: Encoding Loss -0.9473081231117249, Transition Loss -6.423931121826172, Classifier Loss 0.07874532788991928, Total Loss 7.873248100280762\n",
      "19: Encoding Loss -0.6893594264984131, Transition Loss -2.3225624561309814, Classifier Loss 0.06961029767990112, Total Loss 6.960565567016602\n",
      "19: Encoding Loss -0.48995494842529297, Transition Loss -7.88460111618042, Classifier Loss 0.04971040040254593, Total Loss 4.969461441040039\n",
      "19: Encoding Loss -1.419860601425171, Transition Loss -8.88801383972168, Classifier Loss 0.09440992772579193, Total Loss 9.439214706420898\n",
      "19: Encoding Loss -0.06473369896411896, Transition Loss -9.539740562438965, Classifier Loss 0.08950222283601761, Total Loss 8.814337730407715\n",
      "19: Encoding Loss -0.7607163786888123, Transition Loss 1.0349907875061035, Classifier Loss 0.13229839503765106, Total Loss 13.436837196350098\n",
      "19: Encoding Loss -0.6828526258468628, Transition Loss -5.172427654266357, Classifier Loss 0.06127141788601875, Total Loss 6.126107692718506\n",
      "19: Encoding Loss 0.5304734706878662, Transition Loss -8.85679817199707, Classifier Loss 0.10147860646247864, Total Loss 14.389876365661621\n",
      "19: Encoding Loss 0.2779756486415863, Transition Loss -6.283294200897217, Classifier Loss 0.09261680394411087, Total Loss 11.478179931640625\n",
      "19: Encoding Loss -1.2343957424163818, Transition Loss -5.888775825500488, Classifier Loss 0.07750978320837021, Total Loss 7.749800682067871\n",
      "19: Encoding Loss -1.236971378326416, Transition Loss -2.152435064315796, Classifier Loss 0.10488040745258331, Total Loss 10.487610816955566\n",
      "19: Encoding Loss -1.9179917573928833, Transition Loss -10.038819313049316, Classifier Loss 0.06271076202392578, Total Loss 6.269068241119385\n",
      "19: Encoding Loss 0.5293235182762146, Transition Loss -0.14722847938537598, Classifier Loss 0.07900033891201019, Total Loss 12.13459300994873\n",
      "19: Encoding Loss -0.34204915165901184, Transition Loss -4.255593299865723, Classifier Loss 0.05919326841831207, Total Loss 5.9176201820373535\n",
      "19: Encoding Loss -0.20008334517478943, Transition Loss -3.418522596359253, Classifier Loss 0.08882229775190353, Total Loss 8.845202445983887\n",
      "19: Encoding Loss 0.13399364054203033, Transition Loss -11.99703598022461, Classifier Loss 0.16022615134716034, Total Loss 16.995546340942383\n",
      "19: Encoding Loss -0.1683734953403473, Transition Loss -12.031047821044922, Classifier Loss 0.10739053785800934, Total Loss 10.674529075622559\n",
      "19: Encoding Loss -0.25563180446624756, Transition Loss -17.24064064025879, Classifier Loss 0.07089436799287796, Total Loss 7.075171947479248\n",
      "19: Encoding Loss 0.9354442358016968, Transition Loss -7.978019714355469, Classifier Loss 0.1097479984164238, Total Loss 18.456758499145508\n",
      "19: Encoding Loss -1.0931442975997925, Transition Loss -8.415933609008789, Classifier Loss 0.09189130365848541, Total Loss 9.187447547912598\n",
      "19: Encoding Loss -1.9113904237747192, Transition Loss -14.508584022521973, Classifier Loss 0.09090836346149445, Total Loss 9.087934494018555\n",
      "19: Encoding Loss -1.3049975633621216, Transition Loss -2.937835693359375, Classifier Loss 0.11766639351844788, Total Loss 11.76605224609375\n",
      "19: Encoding Loss -0.39846163988113403, Transition Loss -11.653315544128418, Classifier Loss 0.11493860930204391, Total Loss 11.491422653198242\n",
      "19: Encoding Loss -0.6798781752586365, Transition Loss -12.313291549682617, Classifier Loss 0.04401204362511635, Total Loss 4.398741722106934\n",
      "19: Encoding Loss -1.5296204090118408, Transition Loss -15.655694007873535, Classifier Loss 0.08321040123701096, Total Loss 8.317909240722656\n",
      "19: Encoding Loss -2.2064898014068604, Transition Loss -20.313339233398438, Classifier Loss 0.04545045644044876, Total Loss 4.540983200073242\n",
      "20: Encoding Loss -0.6658018827438354, Transition Loss -1.3985238075256348, Classifier Loss 0.054249320179224014, Total Loss 5.424652099609375\n",
      "20: Encoding Loss -0.9389026165008545, Transition Loss -1.3744276762008667, Classifier Loss 0.13239556550979614, Total Loss 13.23928165435791\n",
      "20: Encoding Loss -0.8165519833564758, Transition Loss 2.7785301208496094, Classifier Loss 0.06106175482273102, Total Loss 6.661881446838379\n",
      "20: Encoding Loss -1.2149393558502197, Transition Loss -14.06321907043457, Classifier Loss 0.09492743760347366, Total Loss 9.489931106567383\n",
      "20: Encoding Loss -1.062395453453064, Transition Loss -4.8661088943481445, Classifier Loss 0.06589063256978989, Total Loss 6.588089942932129\n",
      "20: Encoding Loss -1.0662963390350342, Transition Loss -11.51174545288086, Classifier Loss 0.07931286096572876, Total Loss 7.928983688354492\n",
      "20: Encoding Loss -1.1542093753814697, Transition Loss -7.6939191818237305, Classifier Loss 0.09262565523386002, Total Loss 9.261026382446289\n",
      "20: Encoding Loss -0.9328033924102783, Transition Loss -5.571648597717285, Classifier Loss 0.0728343054652214, Total Loss 7.282316207885742\n",
      "20: Encoding Loss -0.8545931577682495, Transition Loss -11.327466011047363, Classifier Loss 0.06644995510578156, Total Loss 6.642730236053467\n",
      "20: Encoding Loss -1.6069843769073486, Transition Loss -8.661593437194824, Classifier Loss 0.09701791405677795, Total Loss 9.70005989074707\n",
      "20: Encoding Loss 0.8142277598381042, Transition Loss -5.253865718841553, Classifier Loss 0.08328734338283539, Total Loss 14.84150505065918\n",
      "20: Encoding Loss -0.06983363628387451, Transition Loss -2.203084707260132, Classifier Loss 0.09392602741718292, Total Loss 9.256694793701172\n",
      "20: Encoding Loss -0.8891791105270386, Transition Loss -3.691790819168091, Classifier Loss 0.06888851523399353, Total Loss 6.888113498687744\n",
      "20: Encoding Loss -0.25208449363708496, Transition Loss -3.6839895248413086, Classifier Loss 0.07817751169204712, Total Loss 7.805209159851074\n",
      "20: Encoding Loss -1.6721388101577759, Transition Loss -11.228100776672363, Classifier Loss 0.06999664753675461, Total Loss 6.997419357299805\n",
      "20: Encoding Loss -2.105302095413208, Transition Loss -15.908323287963867, Classifier Loss 0.07311887294054031, Total Loss 7.308705806732178\n",
      "20: Encoding Loss -1.0811896324157715, Transition Loss -6.241457462310791, Classifier Loss 0.04032805189490318, Total Loss 4.031556606292725\n",
      "20: Encoding Loss 0.03298008069396019, Transition Loss -3.5110158920288086, Classifier Loss 0.0739244595170021, Total Loss 7.557758808135986\n",
      "20: Encoding Loss 0.30668920278549194, Transition Loss -10.362319946289062, Classifier Loss 0.11879529058933258, Total Loss 14.328316688537598\n",
      "20: Encoding Loss 0.06623364239931107, Transition Loss -3.6566193103790283, Classifier Loss 0.07416512072086334, Total Loss 7.811127662658691\n",
      "20: Encoding Loss 0.6561165452003479, Transition Loss -12.800474166870117, Classifier Loss 0.10968887805938721, Total Loss 16.215259552001953\n",
      "20: Encoding Loss -0.11910638958215714, Transition Loss -8.0687255859375, Classifier Loss 0.06503293663263321, Total Loss 6.390373229980469\n",
      "20: Encoding Loss 0.4099109470844269, Transition Loss -9.714173316955566, Classifier Loss 0.10818838328123093, Total Loss 14.096115112304688\n",
      "20: Encoding Loss -0.7322207093238831, Transition Loss -2.356748104095459, Classifier Loss 0.10605725646018982, Total Loss 10.605254173278809\n",
      "20: Encoding Loss -1.57785165309906, Transition Loss -9.478642463684082, Classifier Loss 0.0555860698223114, Total Loss 5.556711196899414\n",
      "20: Encoding Loss -0.8057027459144592, Transition Loss -2.904822587966919, Classifier Loss 0.08312662690877914, Total Loss 8.312082290649414\n",
      "20: Encoding Loss -1.0178064107894897, Transition Loss -1.2980544567108154, Classifier Loss 0.11703887581825256, Total Loss 11.703628540039062\n",
      "20: Encoding Loss -2.3226845264434814, Transition Loss -16.4425106048584, Classifier Loss 0.10217422246932983, Total Loss 10.214134216308594\n",
      "20: Encoding Loss -1.7809170484542847, Transition Loss -7.6103057861328125, Classifier Loss 0.08548523485660553, Total Loss 8.547001838684082\n",
      "20: Encoding Loss -0.6875618696212769, Transition Loss -3.235180616378784, Classifier Loss 0.09167546033859253, Total Loss 9.166899681091309\n",
      "20: Encoding Loss -0.516503095626831, Transition Loss -8.952073097229004, Classifier Loss 0.1021936908364296, Total Loss 10.217577934265137\n",
      "20: Encoding Loss -0.5162436366081238, Transition Loss -1.6851186752319336, Classifier Loss 0.1131858229637146, Total Loss 11.318244934082031\n",
      "20: Encoding Loss -0.10605953633785248, Transition Loss -8.918571472167969, Classifier Loss 0.09448806196451187, Total Loss 9.324471473693848\n",
      "20: Encoding Loss 0.4071507453918457, Transition Loss -4.842654228210449, Classifier Loss 0.18348747491836548, Total Loss 21.604907989501953\n",
      "20: Encoding Loss -1.4496090412139893, Transition Loss -10.433610916137695, Classifier Loss 0.06425407528877258, Total Loss 6.423320770263672\n",
      "20: Encoding Loss -0.8104234933853149, Transition Loss -5.545726776123047, Classifier Loss 0.054512765258550644, Total Loss 5.450167179107666\n",
      "20: Encoding Loss -0.7851054072380066, Transition Loss -5.826990127563477, Classifier Loss 0.10268183052539825, Total Loss 10.267017364501953\n",
      "20: Encoding Loss -0.5720311999320984, Transition Loss -7.951604843139648, Classifier Loss 0.12921132147312164, Total Loss 12.919541358947754\n",
      "20: Encoding Loss -1.1596192121505737, Transition Loss -7.286410331726074, Classifier Loss 0.09039755165576935, Total Loss 9.038297653198242\n",
      "20: Encoding Loss -1.1729265451431274, Transition Loss -8.558307647705078, Classifier Loss 0.09006714820861816, Total Loss 9.005002975463867\n",
      "20: Encoding Loss -1.4399100542068481, Transition Loss -5.882419586181641, Classifier Loss 0.055180542171001434, Total Loss 5.5168776512146\n",
      "20: Encoding Loss -1.0677602291107178, Transition Loss -13.003936767578125, Classifier Loss 0.11715377122163773, Total Loss 11.712776184082031\n",
      "20: Encoding Loss -1.5963689088821411, Transition Loss -12.653173446655273, Classifier Loss 0.05210770294070244, Total Loss 5.208239555358887\n",
      "20: Encoding Loss -1.2099337577819824, Transition Loss -2.6141085624694824, Classifier Loss 0.09096164256334305, Total Loss 9.09564208984375\n",
      "20: Encoding Loss -1.1357706785202026, Transition Loss -13.883180618286133, Classifier Loss 0.07152672111988068, Total Loss 7.149895668029785\n",
      "20: Encoding Loss 0.454631507396698, Transition Loss -4.934818744659424, Classifier Loss 0.10690179467201233, Total Loss 14.326234817504883\n",
      "20: Encoding Loss 0.7200964093208313, Transition Loss -13.385604858398438, Classifier Loss 0.0969352126121521, Total Loss 15.451615333557129\n",
      "20: Encoding Loss -1.3650082349777222, Transition Loss -3.845167636871338, Classifier Loss 0.05714472383260727, Total Loss 5.713703155517578\n",
      "20: Encoding Loss -0.28437551856040955, Transition Loss -5.791908264160156, Classifier Loss 0.0881652683019638, Total Loss 8.810296058654785\n",
      "20: Encoding Loss -0.14949335157871246, Transition Loss -12.541244506835938, Classifier Loss 0.07623583823442459, Total Loss 7.5403900146484375\n",
      "20: Encoding Loss -0.5250158905982971, Transition Loss -15.875353813171387, Classifier Loss 0.06595208495855331, Total Loss 6.5920329093933105\n",
      "20: Encoding Loss 0.43947550654411316, Transition Loss -9.102032661437988, Classifier Loss 0.08511172980070114, Total Loss 12.025136947631836\n",
      "20: Encoding Loss -1.2414957284927368, Transition Loss -1.7011030912399292, Classifier Loss 0.10149259865283966, Total Loss 10.148919105529785\n",
      "20: Encoding Loss -1.2826348543167114, Transition Loss -9.613550186157227, Classifier Loss 0.07269584387540817, Total Loss 7.2676615715026855\n",
      "20: Encoding Loss -0.3271646797657013, Transition Loss -9.234609603881836, Classifier Loss 0.08441854268312454, Total Loss 8.438608169555664\n",
      "20: Encoding Loss -1.2318894863128662, Transition Loss -2.686657428741455, Classifier Loss 0.09777864068746567, Total Loss 9.777327537536621\n",
      "20: Encoding Loss 0.27211621403694153, Transition Loss -8.521407127380371, Classifier Loss 0.061059169471263885, Total Loss 8.274062156677246\n",
      "20: Encoding Loss -1.6020543575286865, Transition Loss -9.42930793762207, Classifier Loss 0.08390780538320541, Total Loss 8.388895034790039\n",
      "20: Encoding Loss -1.485741138458252, Transition Loss -16.398412704467773, Classifier Loss 0.06905800104141235, Total Loss 6.902520179748535\n",
      "20: Encoding Loss -1.307556390762329, Transition Loss -12.707071304321289, Classifier Loss 0.0880114734172821, Total Loss 8.798605918884277\n",
      "20: Encoding Loss -0.8935850262641907, Transition Loss -6.612348556518555, Classifier Loss 0.08143135160207748, Total Loss 8.141812324523926\n",
      "20: Encoding Loss -1.1673535108566284, Transition Loss -5.050602912902832, Classifier Loss 0.1255742609500885, Total Loss 12.556416511535645\n",
      "20: Encoding Loss -1.0526440143585205, Transition Loss -7.278280258178711, Classifier Loss 0.09295377135276794, Total Loss 9.29392147064209\n",
      "20: Encoding Loss -0.9232495427131653, Transition Loss -4.878242492675781, Classifier Loss 0.09422147274017334, Total Loss 9.421172142028809\n",
      "20: Encoding Loss -1.0495343208312988, Transition Loss -9.378793716430664, Classifier Loss 0.08496152609586716, Total Loss 8.494277000427246\n",
      "20: Encoding Loss -1.1945809125900269, Transition Loss -2.224118709564209, Classifier Loss 0.14120015501976013, Total Loss 14.1195707321167\n",
      "20: Encoding Loss 0.3976157605648041, Transition Loss 1.291867971420288, Classifier Loss 0.07567127794027328, Total Loss 11.006315231323242\n",
      "20: Encoding Loss -1.8011701107025146, Transition Loss -14.808664321899414, Classifier Loss 0.09885488450527191, Total Loss 9.882526397705078\n",
      "20: Encoding Loss 0.05307355150580406, Transition Loss -5.680498123168945, Classifier Loss 0.08695986866950989, Total Loss 8.992996215820312\n",
      "20: Encoding Loss 0.09217433631420135, Transition Loss -7.304449081420898, Classifier Loss 0.14060968160629272, Total Loss 14.665401458740234\n",
      "20: Encoding Loss -1.2614704370498657, Transition Loss -6.44467306137085, Classifier Loss 0.10466203093528748, Total Loss 10.464913368225098\n",
      "20: Encoding Loss -1.1652895212173462, Transition Loss -6.769578456878662, Classifier Loss 0.09832190722227097, Total Loss 9.830836296081543\n",
      "20: Encoding Loss -0.6646378636360168, Transition Loss -2.923114776611328, Classifier Loss 0.12587164342403412, Total Loss 12.586579322814941\n",
      "20: Encoding Loss -0.9363492131233215, Transition Loss -6.3466997146606445, Classifier Loss 0.039708107709884644, Total Loss 3.969541549682617\n",
      "20: Encoding Loss -1.3140686750411987, Transition Loss -5.088929176330566, Classifier Loss 0.13726504147052765, Total Loss 13.725486755371094\n",
      "20: Encoding Loss -1.3265939950942993, Transition Loss -11.542415618896484, Classifier Loss 0.06694912910461426, Total Loss 6.6926045417785645\n",
      "20: Encoding Loss -1.080486536026001, Transition Loss -3.6550395488739014, Classifier Loss 0.10256285220384598, Total Loss 10.25555419921875\n",
      "20: Encoding Loss -2.4976954460144043, Transition Loss -9.368165969848633, Classifier Loss 0.0851430743932724, Total Loss 8.512433052062988\n",
      "20: Encoding Loss -1.5855287313461304, Transition Loss -10.358139038085938, Classifier Loss 0.048146702349185944, Total Loss 4.81259822845459\n",
      "20: Encoding Loss -0.9129213690757751, Transition Loss -7.4771246910095215, Classifier Loss 0.12754365801811218, Total Loss 12.752870559692383\n",
      "20: Encoding Loss -0.4957932233810425, Transition Loss -8.512015342712402, Classifier Loss 0.08032046258449554, Total Loss 8.030343055725098\n",
      "20: Encoding Loss -1.5410810708999634, Transition Loss -5.82349967956543, Classifier Loss 0.07723792642354965, Total Loss 7.722627639770508\n",
      "20: Encoding Loss -0.7998513579368591, Transition Loss -2.921708583831787, Classifier Loss 0.11166209727525711, Total Loss 11.16562557220459\n",
      "20: Encoding Loss -0.2527856230735779, Transition Loss -8.538126945495605, Classifier Loss 0.1219906210899353, Total Loss 12.185750007629395\n",
      "20: Encoding Loss -2.032862663269043, Transition Loss -5.371823310852051, Classifier Loss 0.07869049906730652, Total Loss 7.86797571182251\n",
      "20: Encoding Loss -1.1916452646255493, Transition Loss -2.764633893966675, Classifier Loss 0.08893090486526489, Total Loss 8.892537117004395\n",
      "20: Encoding Loss -1.7055937051773071, Transition Loss -12.850257873535156, Classifier Loss 0.0869411751627922, Total Loss 8.691547393798828\n",
      "20: Encoding Loss -1.1035270690917969, Transition Loss -6.756012916564941, Classifier Loss 0.1262470930814743, Total Loss 12.623357772827148\n",
      "20: Encoding Loss -0.6330131888389587, Transition Loss -5.299497604370117, Classifier Loss 0.07081009447574615, Total Loss 7.079949378967285\n",
      "20: Encoding Loss -0.3640372157096863, Transition Loss -9.931194305419922, Classifier Loss 0.07988003641366959, Total Loss 7.985621452331543\n",
      "20: Encoding Loss -3.051666736602783, Transition Loss -18.24517250061035, Classifier Loss 0.08264564722776413, Total Loss 8.260915756225586\n",
      "20: Encoding Loss -0.9749851226806641, Transition Loss -5.071131229400635, Classifier Loss 0.11115893721580505, Total Loss 11.114879608154297\n",
      "20: Encoding Loss -1.7378381490707397, Transition Loss -5.0450239181518555, Classifier Loss 0.08905891329050064, Total Loss 8.904882431030273\n",
      "20: Encoding Loss -0.7735092043876648, Transition Loss -7.475658416748047, Classifier Loss 0.09876342117786407, Total Loss 9.874846458435059\n",
      "20: Encoding Loss -0.6774747967720032, Transition Loss -1.9395112991333008, Classifier Loss 0.08391433954238892, Total Loss 8.391045570373535\n",
      "20: Encoding Loss -1.2755036354064941, Transition Loss -10.015799522399902, Classifier Loss 0.09552009403705597, Total Loss 9.550006866455078\n",
      "20: Encoding Loss -1.401262640953064, Transition Loss -12.825368881225586, Classifier Loss 0.10858488827943802, Total Loss 10.855923652648926\n",
      "20: Encoding Loss -1.7657856941223145, Transition Loss -10.311445236206055, Classifier Loss 0.06873287260532379, Total Loss 6.871224880218506\n",
      "20: Encoding Loss -0.7455013990402222, Transition Loss -1.832457184791565, Classifier Loss 0.05472484976053238, Total Loss 5.472118377685547\n",
      "20: Encoding Loss -1.1397855281829834, Transition Loss -4.646017074584961, Classifier Loss 0.12848304212093353, Total Loss 12.84737491607666\n",
      "20: Encoding Loss -1.373826503753662, Transition Loss -11.48556137084961, Classifier Loss 0.09062720835208893, Total Loss 9.060423851013184\n",
      "20: Encoding Loss -0.06177697703242302, Transition Loss 2.062636613845825, Classifier Loss 0.12518805265426636, Total Loss 12.79870319366455\n",
      "20: Encoding Loss -0.7607404589653015, Transition Loss -4.9260711669921875, Classifier Loss 0.08174674957990646, Total Loss 8.173689842224121\n",
      "20: Encoding Loss -0.45584776997566223, Transition Loss -2.34700083732605, Classifier Loss 0.10479100048542023, Total Loss 10.478621482849121\n",
      "20: Encoding Loss -1.1919898986816406, Transition Loss -9.362255096435547, Classifier Loss 0.07225662469863892, Total Loss 7.223789691925049\n",
      "20: Encoding Loss -1.3758684396743774, Transition Loss -12.164480209350586, Classifier Loss 0.10836384445428848, Total Loss 10.833951950073242\n",
      "20: Encoding Loss -1.0166640281677246, Transition Loss -1.861058235168457, Classifier Loss 0.10625119507312775, Total Loss 10.624747276306152\n",
      "20: Encoding Loss -1.1795684099197388, Transition Loss -5.439882278442383, Classifier Loss 0.051806624978780746, Total Loss 5.179574489593506\n",
      "20: Encoding Loss -1.8168208599090576, Transition Loss -6.134040355682373, Classifier Loss 0.07586400210857391, Total Loss 7.5851731300354\n",
      "20: Encoding Loss -0.650714635848999, Transition Loss -11.618340492248535, Classifier Loss 0.09315569698810577, Total Loss 9.31324577331543\n",
      "20: Encoding Loss -0.9560577869415283, Transition Loss -10.971909523010254, Classifier Loss 0.09497110545635223, Total Loss 9.494915962219238\n",
      "20: Encoding Loss -1.1530191898345947, Transition Loss 0.5924320220947266, Classifier Loss 0.06000226363539696, Total Loss 6.118712902069092\n",
      "20: Encoding Loss -1.965223789215088, Transition Loss -14.238126754760742, Classifier Loss 0.06538044661283493, Total Loss 6.535196781158447\n",
      "20: Encoding Loss -1.4155553579330444, Transition Loss -13.67179012298584, Classifier Loss 0.06538952887058258, Total Loss 6.536218643188477\n",
      "20: Encoding Loss -0.1987379640340805, Transition Loss 0.9476408362388611, Classifier Loss 0.08281706273555756, Total Loss 8.433966636657715\n",
      "20: Encoding Loss -0.4221492409706116, Transition Loss 4.867165565490723, Classifier Loss 0.07356198132038116, Total Loss 8.329590797424316\n",
      "20: Encoding Loss -0.7985096573829651, Transition Loss -8.97964859008789, Classifier Loss 0.06138250231742859, Total Loss 6.1364545822143555\n",
      "20: Encoding Loss -0.9451286792755127, Transition Loss -8.383223533630371, Classifier Loss 0.07367005944252014, Total Loss 7.365329265594482\n",
      "20: Encoding Loss -2.0687098503112793, Transition Loss -9.873236656188965, Classifier Loss 0.08194608986377716, Total Loss 8.192633628845215\n",
      "20: Encoding Loss -0.48068705201148987, Transition Loss -4.035233497619629, Classifier Loss 0.07670328766107559, Total Loss 7.669518947601318\n",
      "20: Encoding Loss -2.0697808265686035, Transition Loss -11.740859985351562, Classifier Loss 0.06358616799116135, Total Loss 6.356268882751465\n",
      "20: Encoding Loss -0.4740019142627716, Transition Loss -5.592533111572266, Classifier Loss 0.09551229327917099, Total Loss 9.5501070022583\n",
      "20: Encoding Loss -0.41654446721076965, Transition Loss -8.738686561584473, Classifier Loss 0.12189464271068573, Total Loss 12.187664985656738\n",
      "20: Encoding Loss -2.2743847370147705, Transition Loss -16.128332138061523, Classifier Loss 0.08216805756092072, Total Loss 8.213580131530762\n",
      "20: Encoding Loss -1.505969524383545, Transition Loss -4.95680046081543, Classifier Loss 0.09955912828445435, Total Loss 9.954920768737793\n",
      "20: Encoding Loss -1.3083806037902832, Transition Loss -7.69984769821167, Classifier Loss 0.14009860157966614, Total Loss 14.008319854736328\n",
      "20: Encoding Loss -1.1981934309005737, Transition Loss -13.003850936889648, Classifier Loss 0.10634978115558624, Total Loss 10.632377624511719\n",
      "20: Encoding Loss -1.9840350151062012, Transition Loss -9.006439208984375, Classifier Loss 0.0420009009540081, Total Loss 4.198288440704346\n",
      "20: Encoding Loss -0.7088690400123596, Transition Loss -9.229284286499023, Classifier Loss 0.08066186308860779, Total Loss 8.064339637756348\n",
      "20: Encoding Loss -1.4823298454284668, Transition Loss -2.4984915256500244, Classifier Loss 0.06371179223060608, Total Loss 6.3706793785095215\n",
      "20: Encoding Loss -0.1592007279396057, Transition Loss -5.692793846130371, Classifier Loss 0.08567619323730469, Total Loss 8.495551109313965\n",
      "20: Encoding Loss -0.8594790101051331, Transition Loss -6.15431022644043, Classifier Loss 0.07597696781158447, Total Loss 7.596466064453125\n",
      "20: Encoding Loss -0.5155556797981262, Transition Loss -1.8166191577911377, Classifier Loss 0.0637151226401329, Total Loss 6.371148586273193\n",
      "20: Encoding Loss -0.4329650104045868, Transition Loss -7.628578186035156, Classifier Loss 0.04503178596496582, Total Loss 4.501626968383789\n",
      "20: Encoding Loss -1.2411644458770752, Transition Loss -8.870247840881348, Classifier Loss 0.09755437821149826, Total Loss 9.753664016723633\n",
      "20: Encoding Loss -0.20245462656021118, Transition Loss -9.563664436340332, Classifier Loss 0.0833306834101677, Total Loss 8.296401977539062\n",
      "20: Encoding Loss -0.6631051301956177, Transition Loss 1.7533667087554932, Classifier Loss 0.12800756096839905, Total Loss 13.151430130004883\n",
      "20: Encoding Loss -0.29074448347091675, Transition Loss -5.036144256591797, Classifier Loss 0.059777501970529556, Total Loss 5.972505569458008\n",
      "20: Encoding Loss 0.6843355298042297, Transition Loss -8.376394271850586, Classifier Loss 0.10547112673521042, Total Loss 16.020122528076172\n",
      "20: Encoding Loss 0.29216328263282776, Transition Loss -6.69365930557251, Classifier Loss 0.08687499165534973, Total Loss 11.019396781921387\n",
      "20: Encoding Loss -1.2296764850616455, Transition Loss -6.699516296386719, Classifier Loss 0.06982903927564621, Total Loss 6.981564044952393\n",
      "20: Encoding Loss -1.2419339418411255, Transition Loss -2.865729808807373, Classifier Loss 0.10511357337236404, Total Loss 10.510784149169922\n",
      "20: Encoding Loss -1.9310282468795776, Transition Loss -10.909473419189453, Classifier Loss 0.05732958763837814, Total Loss 5.730776786804199\n",
      "20: Encoding Loss 0.4892955720424652, Transition Loss -0.516814649105072, Classifier Loss 0.07813282310962677, Total Loss 11.72754192352295\n",
      "20: Encoding Loss -0.4531879127025604, Transition Loss -4.333920001983643, Classifier Loss 0.05688147619366646, Total Loss 5.687270164489746\n",
      "20: Encoding Loss -0.25129520893096924, Transition Loss -3.4730234146118164, Classifier Loss 0.08862890303134918, Total Loss 8.8501615524292\n",
      "20: Encoding Loss -0.08866335451602936, Transition Loss -12.108996391296387, Classifier Loss 0.16038145124912262, Total Loss 15.902630805969238\n",
      "20: Encoding Loss -1.6735559701919556, Transition Loss -12.059226036071777, Classifier Loss 0.10690870881080627, Total Loss 10.688459396362305\n",
      "20: Encoding Loss -2.392512798309326, Transition Loss -17.112178802490234, Classifier Loss 0.06943100690841675, Total Loss 6.93967866897583\n",
      "20: Encoding Loss -0.28528130054473877, Transition Loss -8.058061599731445, Classifier Loss 0.10778295993804932, Total Loss 10.77173900604248\n",
      "20: Encoding Loss -1.1522835493087769, Transition Loss -8.541231155395508, Classifier Loss 0.08730408549308777, Total Loss 8.728700637817383\n",
      "20: Encoding Loss -2.186727285385132, Transition Loss -14.785711288452148, Classifier Loss 0.08390164375305176, Total Loss 8.38720703125\n",
      "20: Encoding Loss -1.4835330247879028, Transition Loss -2.7863564491271973, Classifier Loss 0.1055055558681488, Total Loss 10.54999828338623\n",
      "20: Encoding Loss -0.13590635359287262, Transition Loss -11.660372734069824, Classifier Loss 0.10214013606309891, Total Loss 10.117022514343262\n",
      "20: Encoding Loss -0.9034384489059448, Transition Loss -11.918288230895996, Classifier Loss 0.040131088346242905, Total Loss 4.010725021362305\n",
      "20: Encoding Loss -1.4262984991073608, Transition Loss -15.268340110778809, Classifier Loss 0.0781431645154953, Total Loss 7.811262607574463\n",
      "20: Encoding Loss -2.2879016399383545, Transition Loss -20.235553741455078, Classifier Loss 0.04527490586042404, Total Loss 4.523443698883057\n",
      "21: Encoding Loss -0.7459077835083008, Transition Loss -0.8138529658317566, Classifier Loss 0.04831109941005707, Total Loss 4.830947399139404\n",
      "21: Encoding Loss -1.185737133026123, Transition Loss -1.1116762161254883, Classifier Loss 0.13172200322151184, Total Loss 13.171977996826172\n",
      "21: Encoding Loss -0.9200077056884766, Transition Loss 3.4173455238342285, Classifier Loss 0.057447344064712524, Total Loss 6.428203582763672\n",
      "21: Encoding Loss -1.2056756019592285, Transition Loss -13.85992431640625, Classifier Loss 0.08679526299238205, Total Loss 8.676753997802734\n",
      "21: Encoding Loss -0.9326827526092529, Transition Loss -4.553894519805908, Classifier Loss 0.0637403279542923, Total Loss 6.373122215270996\n",
      "21: Encoding Loss -0.891284167766571, Transition Loss -11.044922828674316, Classifier Loss 0.07469411194324493, Total Loss 7.467202186584473\n",
      "21: Encoding Loss -1.1386550664901733, Transition Loss -7.13585901260376, Classifier Loss 0.08873524516820908, Total Loss 8.872097969055176\n",
      "21: Encoding Loss -0.7601537108421326, Transition Loss -5.011198997497559, Classifier Loss 0.06577876955270767, Total Loss 6.576874732971191\n",
      "21: Encoding Loss -0.5963971018791199, Transition Loss -11.103475570678711, Classifier Loss 0.06296943128108978, Total Loss 6.294722557067871\n",
      "21: Encoding Loss -1.5598703622817993, Transition Loss -8.421234130859375, Classifier Loss 0.09403535723686218, Total Loss 9.401851654052734\n",
      "21: Encoding Loss 0.4080348312854767, Transition Loss -4.870623588562012, Classifier Loss 0.07842589914798737, Total Loss 11.10582160949707\n",
      "21: Encoding Loss 0.6440874934196472, Transition Loss -2.452152967453003, Classifier Loss 0.08814690262079239, Total Loss 13.966899871826172\n",
      "21: Encoding Loss -0.8760149478912354, Transition Loss -4.526591777801514, Classifier Loss 0.06553720682859421, Total Loss 6.552814960479736\n",
      "21: Encoding Loss -0.44291695952415466, Transition Loss -4.3546462059021, Classifier Loss 0.08484023809432983, Total Loss 8.483136177062988\n",
      "21: Encoding Loss -1.8509243726730347, Transition Loss -11.34176254272461, Classifier Loss 0.06923777610063553, Total Loss 6.921509265899658\n",
      "21: Encoding Loss -1.783078908920288, Transition Loss -15.603333473205566, Classifier Loss 0.06775657832622528, Total Loss 6.772536754608154\n",
      "21: Encoding Loss -0.42642295360565186, Transition Loss -6.639025688171387, Classifier Loss 0.03756541386246681, Total Loss 3.7551794052124023\n",
      "21: Encoding Loss 0.19668631255626678, Transition Loss -4.008731842041016, Classifier Loss 0.07618308812379837, Total Loss 9.152290344238281\n",
      "21: Encoding Loss 0.41772305965423584, Transition Loss -11.58348274230957, Classifier Loss 0.11259926855564117, Total Loss 14.599346160888672\n",
      "21: Encoding Loss 0.8371103405952454, Transition Loss -3.2218704223632812, Classifier Loss 0.0691726803779602, Total Loss 13.613506317138672\n",
      "21: Encoding Loss 0.874930739402771, Transition Loss -13.483377456665039, Classifier Loss 0.10007355362176895, Total Loss 17.004104614257812\n",
      "21: Encoding Loss -0.25529226660728455, Transition Loss -8.40827751159668, Classifier Loss 0.060067933052778244, Total Loss 5.994203090667725\n",
      "21: Encoding Loss 0.014531021937727928, Transition Loss -10.133794784545898, Classifier Loss 0.10400798916816711, Total Loss 10.463611602783203\n",
      "21: Encoding Loss -0.8272745609283447, Transition Loss -2.374917507171631, Classifier Loss 0.10246549546718597, Total Loss 10.246074676513672\n",
      "21: Encoding Loss -1.4859493970870972, Transition Loss -9.64238452911377, Classifier Loss 0.050586920231580734, Total Loss 5.056763648986816\n",
      "21: Encoding Loss -0.727208137512207, Transition Loss -3.059542655944824, Classifier Loss 0.08042718470096588, Total Loss 8.042106628417969\n",
      "21: Encoding Loss -1.1714088916778564, Transition Loss -1.473555326461792, Classifier Loss 0.1122141182422638, Total Loss 11.22111701965332\n",
      "21: Encoding Loss -2.1945157051086426, Transition Loss -16.578838348388672, Classifier Loss 0.0984620749950409, Total Loss 9.842891693115234\n",
      "21: Encoding Loss -1.8579758405685425, Transition Loss -7.726444721221924, Classifier Loss 0.08460298925638199, Total Loss 8.45875358581543\n",
      "21: Encoding Loss -0.5086338520050049, Transition Loss -3.3551547527313232, Classifier Loss 0.08701322227716446, Total Loss 8.700650215148926\n",
      "21: Encoding Loss -0.4633291959762573, Transition Loss -9.180548667907715, Classifier Loss 0.10027880221605301, Total Loss 10.02603816986084\n",
      "21: Encoding Loss -0.7847622036933899, Transition Loss -1.7557915449142456, Classifier Loss 0.10750743001699448, Total Loss 10.750391960144043\n",
      "21: Encoding Loss -0.11555229127407074, Transition Loss -9.146228790283203, Classifier Loss 0.0851166620850563, Total Loss 8.39526653289795\n",
      "21: Encoding Loss 0.30425122380256653, Transition Loss -4.816917419433594, Classifier Loss 0.18861845135688782, Total Loss 21.292036056518555\n",
      "21: Encoding Loss -1.619078516960144, Transition Loss -11.121108055114746, Classifier Loss 0.061899080872535706, Total Loss 6.187683582305908\n",
      "21: Encoding Loss -0.840213418006897, Transition Loss -6.249942302703857, Classifier Loss 0.05153878778219223, Total Loss 5.1526288986206055\n",
      "21: Encoding Loss -1.0266870260238647, Transition Loss -6.507188320159912, Classifier Loss 0.09898310899734497, Total Loss 9.897008895874023\n",
      "21: Encoding Loss -0.5621787905693054, Transition Loss -8.537747383117676, Classifier Loss 0.12710633873939514, Total Loss 12.708927154541016\n",
      "21: Encoding Loss -1.3109713792800903, Transition Loss -8.092438697814941, Classifier Loss 0.09202347695827484, Total Loss 9.200729370117188\n",
      "21: Encoding Loss -1.2615829706192017, Transition Loss -9.268913269042969, Classifier Loss 0.0886816531419754, Total Loss 8.866311073303223\n",
      "21: Encoding Loss -1.3849482536315918, Transition Loss -6.593486785888672, Classifier Loss 0.053495846688747406, Total Loss 5.348265647888184\n",
      "21: Encoding Loss -1.2303332090377808, Transition Loss -13.770539283752441, Classifier Loss 0.11552178859710693, Total Loss 11.54942512512207\n",
      "21: Encoding Loss -1.7567914724349976, Transition Loss -13.404834747314453, Classifier Loss 0.052504897117614746, Total Loss 5.247808933258057\n",
      "21: Encoding Loss -1.3483161926269531, Transition Loss -3.3216183185577393, Classifier Loss 0.0923648253083229, Total Loss 9.235817909240723\n",
      "21: Encoding Loss -1.287022352218628, Transition Loss -14.557594299316406, Classifier Loss 0.06723151355981827, Total Loss 6.720239639282227\n",
      "21: Encoding Loss 0.3615068197250366, Transition Loss -5.5805182456970215, Classifier Loss 0.10441560298204422, Total Loss 13.33206558227539\n",
      "21: Encoding Loss 0.49867376685142517, Transition Loss -14.059436798095703, Classifier Loss 0.09447771310806274, Total Loss 13.434349060058594\n",
      "21: Encoding Loss -1.420481562614441, Transition Loss -4.208194732666016, Classifier Loss 0.05219216272234917, Total Loss 5.218374729156494\n",
      "21: Encoding Loss -0.32912567257881165, Transition Loss -6.2562642097473145, Classifier Loss 0.08277200162410736, Total Loss 8.274635314941406\n",
      "21: Encoding Loss -0.2734297513961792, Transition Loss -12.982305526733398, Classifier Loss 0.07156860083341599, Total Loss 7.147426128387451\n",
      "21: Encoding Loss -0.7796692252159119, Transition Loss -16.32337188720703, Classifier Loss 0.06442488729953766, Total Loss 6.439223766326904\n",
      "21: Encoding Loss 0.09114024043083191, Transition Loss -9.629590034484863, Classifier Loss 0.08659525215625763, Total Loss 9.254719734191895\n",
      "21: Encoding Loss -1.2591639757156372, Transition Loss -2.3112502098083496, Classifier Loss 0.09405698627233505, Total Loss 9.40523624420166\n",
      "21: Encoding Loss -1.3585675954818726, Transition Loss -10.29443359375, Classifier Loss 0.069828100502491, Total Loss 6.980751037597656\n",
      "21: Encoding Loss -0.4515168368816376, Transition Loss -9.918438911437988, Classifier Loss 0.08386078476905823, Total Loss 8.38408374786377\n",
      "21: Encoding Loss -1.3214980363845825, Transition Loss -3.2444558143615723, Classifier Loss 0.1009247899055481, Total Loss 10.091830253601074\n",
      "21: Encoding Loss 0.3864431083202362, Transition Loss -9.200078010559082, Classifier Loss 0.05672409385442734, Total Loss 8.761942863464355\n",
      "21: Encoding Loss -1.6045587062835693, Transition Loss -9.721959114074707, Classifier Loss 0.07782552391290665, Total Loss 7.7806077003479\n",
      "21: Encoding Loss -1.4096633195877075, Transition Loss -16.631994247436523, Classifier Loss 0.0657811164855957, Total Loss 6.574785232543945\n",
      "21: Encoding Loss -1.313810110092163, Transition Loss -13.069358825683594, Classifier Loss 0.08680343627929688, Total Loss 8.677729606628418\n",
      "21: Encoding Loss -0.7377859354019165, Transition Loss -6.846463203430176, Classifier Loss 0.07890094816684723, Total Loss 7.888725280761719\n",
      "21: Encoding Loss -1.059178113937378, Transition Loss -5.280414581298828, Classifier Loss 0.1319294422864914, Total Loss 13.191888809204102\n",
      "21: Encoding Loss -1.0090644359588623, Transition Loss -7.551955223083496, Classifier Loss 0.08889805525541306, Total Loss 8.88829517364502\n",
      "21: Encoding Loss -0.8505979180335999, Transition Loss -5.086636543273926, Classifier Loss 0.0876493826508522, Total Loss 8.763920783996582\n",
      "21: Encoding Loss -0.9209325909614563, Transition Loss -9.548273086547852, Classifier Loss 0.08657687157392502, Total Loss 8.655777931213379\n",
      "21: Encoding Loss -1.1615554094314575, Transition Loss -2.390801429748535, Classifier Loss 0.1331138014793396, Total Loss 13.31090259552002\n",
      "21: Encoding Loss 0.42525091767311096, Transition Loss 1.1016852855682373, Classifier Loss 0.07391742616891861, Total Loss 11.014050483703613\n",
      "21: Encoding Loss -1.9961129426956177, Transition Loss -15.29909610748291, Classifier Loss 0.10053904354572296, Total Loss 10.0508451461792\n",
      "21: Encoding Loss 0.07303404808044434, Transition Loss -6.231563091278076, Classifier Loss 0.08121046423912048, Total Loss 8.568175315856934\n",
      "21: Encoding Loss 0.05361218750476837, Transition Loss -7.8854827880859375, Classifier Loss 0.1343332827091217, Total Loss 13.733721733093262\n",
      "21: Encoding Loss -1.408491849899292, Transition Loss -6.787160396575928, Classifier Loss 0.10075543075799942, Total Loss 10.074186325073242\n",
      "21: Encoding Loss -1.2587928771972656, Transition Loss -7.083395004272461, Classifier Loss 0.09303254634141922, Total Loss 9.301838874816895\n",
      "21: Encoding Loss -0.7088531255722046, Transition Loss -3.367663860321045, Classifier Loss 0.12564979493618011, Total Loss 12.564306259155273\n",
      "21: Encoding Loss -1.039302945137024, Transition Loss -6.74147891998291, Classifier Loss 0.03631361201405525, Total Loss 3.6300129890441895\n",
      "21: Encoding Loss -1.4451899528503418, Transition Loss -5.532543659210205, Classifier Loss 0.13550838828086853, Total Loss 13.549732208251953\n",
      "21: Encoding Loss -1.4222737550735474, Transition Loss -11.841352462768555, Classifier Loss 0.061010800302028656, Total Loss 6.0987114906311035\n",
      "21: Encoding Loss -1.137699007987976, Transition Loss -4.060052871704102, Classifier Loss 0.09990809857845306, Total Loss 9.989997863769531\n",
      "21: Encoding Loss -2.6021037101745605, Transition Loss -9.764904022216797, Classifier Loss 0.08165468275547028, Total Loss 8.163515090942383\n",
      "21: Encoding Loss -1.543277621269226, Transition Loss -10.71552848815918, Classifier Loss 0.050536010414361954, Total Loss 5.051458358764648\n",
      "21: Encoding Loss -1.081798791885376, Transition Loss -7.824106216430664, Classifier Loss 0.1262282133102417, Total Loss 12.621256828308105\n",
      "21: Encoding Loss -0.6253232955932617, Transition Loss -8.823192596435547, Classifier Loss 0.0774964764714241, Total Loss 7.747882843017578\n",
      "21: Encoding Loss -1.6489266157150269, Transition Loss -6.182930946350098, Classifier Loss 0.0765569657087326, Total Loss 7.6544599533081055\n",
      "21: Encoding Loss -0.9129409790039062, Transition Loss -3.253260850906372, Classifier Loss 0.10528884083032608, Total Loss 10.528233528137207\n",
      "21: Encoding Loss -0.38065484166145325, Transition Loss -8.799120903015137, Classifier Loss 0.12062608450651169, Total Loss 12.06063461303711\n",
      "21: Encoding Loss -2.2438197135925293, Transition Loss -5.745139122009277, Classifier Loss 0.07972922176122665, Total Loss 7.971773147583008\n",
      "21: Encoding Loss -1.3249964714050293, Transition Loss -3.218695878982544, Classifier Loss 0.07910968363285065, Total Loss 7.910324573516846\n",
      "21: Encoding Loss -1.783913493156433, Transition Loss -13.221787452697754, Classifier Loss 0.08860430866479874, Total Loss 8.857786178588867\n",
      "21: Encoding Loss -1.1887069940567017, Transition Loss -7.163801193237305, Classifier Loss 0.12349199503660202, Total Loss 12.347766876220703\n",
      "21: Encoding Loss -0.6997277140617371, Transition Loss -5.727226257324219, Classifier Loss 0.06257729232311249, Total Loss 6.2565836906433105\n",
      "21: Encoding Loss -0.5815533995628357, Transition Loss -10.309835433959961, Classifier Loss 0.07821650803089142, Total Loss 7.819589138031006\n",
      "21: Encoding Loss -3.156304359436035, Transition Loss -18.58469009399414, Classifier Loss 0.07923630625009537, Total Loss 7.9199137687683105\n",
      "21: Encoding Loss -1.1118561029434204, Transition Loss -5.378278732299805, Classifier Loss 0.11011239886283875, Total Loss 11.010164260864258\n",
      "21: Encoding Loss -1.7183775901794434, Transition Loss -5.414198875427246, Classifier Loss 0.09011481702327728, Total Loss 9.010398864746094\n",
      "21: Encoding Loss -0.9040135741233826, Transition Loss -7.7885613441467285, Classifier Loss 0.09690617024898529, Total Loss 9.689059257507324\n",
      "21: Encoding Loss -0.8301661610603333, Transition Loss -2.2325289249420166, Classifier Loss 0.08943544328212738, Total Loss 8.943098068237305\n",
      "21: Encoding Loss -1.464703917503357, Transition Loss -10.397883415222168, Classifier Loss 0.09492683410644531, Total Loss 9.49060344696045\n",
      "21: Encoding Loss -1.4143115282058716, Transition Loss -13.213776588439941, Classifier Loss 0.10091114789247513, Total Loss 10.088472366333008\n",
      "21: Encoding Loss -1.8405519723892212, Transition Loss -10.697778701782227, Classifier Loss 0.06621254235506058, Total Loss 6.619114875793457\n",
      "21: Encoding Loss -0.8701398968696594, Transition Loss -2.266646385192871, Classifier Loss 0.05221681669354439, Total Loss 5.221228122711182\n",
      "21: Encoding Loss -1.2396458387374878, Transition Loss -4.998454570770264, Classifier Loss 0.12746326625347137, Total Loss 12.74532699584961\n",
      "21: Encoding Loss -1.5578866004943848, Transition Loss -11.793237686157227, Classifier Loss 0.08713807910680771, Total Loss 8.71144962310791\n",
      "21: Encoding Loss -0.19308622181415558, Transition Loss 1.7082982063293457, Classifier Loss 0.12246076762676239, Total Loss 12.546415328979492\n",
      "21: Encoding Loss -0.7861769199371338, Transition Loss -5.285913944244385, Classifier Loss 0.08044003695249557, Total Loss 8.042945861816406\n",
      "21: Encoding Loss -0.47819384932518005, Transition Loss -2.5979762077331543, Classifier Loss 0.10275144875049591, Total Loss 10.274621963500977\n",
      "21: Encoding Loss -1.2287393808364868, Transition Loss -9.534683227539062, Classifier Loss 0.06991183757781982, Total Loss 6.989276885986328\n",
      "21: Encoding Loss -1.4210801124572754, Transition Loss -12.455949783325195, Classifier Loss 0.10527338087558746, Total Loss 10.524847030639648\n",
      "21: Encoding Loss -1.143066644668579, Transition Loss -2.1592555046081543, Classifier Loss 0.1012975424528122, Total Loss 10.129322052001953\n",
      "21: Encoding Loss -1.3136045932769775, Transition Loss -5.794344425201416, Classifier Loss 0.0510026179254055, Total Loss 5.099102973937988\n",
      "21: Encoding Loss -1.8432344198226929, Transition Loss -6.356109619140625, Classifier Loss 0.0719214677810669, Total Loss 7.19087553024292\n",
      "21: Encoding Loss -0.7238496541976929, Transition Loss -11.775303840637207, Classifier Loss 0.08601485192775726, Total Loss 8.599130630493164\n",
      "21: Encoding Loss -1.130444884300232, Transition Loss -11.13442611694336, Classifier Loss 0.09347867220640182, Total Loss 9.345640182495117\n",
      "21: Encoding Loss -1.165362000465393, Transition Loss 0.32892972230911255, Classifier Loss 0.053708527237176895, Total Loss 5.436638832092285\n",
      "21: Encoding Loss -1.9516490697860718, Transition Loss -14.454578399658203, Classifier Loss 0.06260814517736435, Total Loss 6.257923603057861\n",
      "21: Encoding Loss -1.513234257698059, Transition Loss -13.901652336120605, Classifier Loss 0.0640493854880333, Total Loss 6.402158260345459\n",
      "21: Encoding Loss -0.269040048122406, Transition Loss 0.648569643497467, Classifier Loss 0.08153713494539261, Total Loss 8.275747299194336\n",
      "21: Encoding Loss -0.5588701963424683, Transition Loss 4.4638261795043945, Classifier Loss 0.07884448021650314, Total Loss 8.777213096618652\n",
      "21: Encoding Loss -0.8231046199798584, Transition Loss -9.200408935546875, Classifier Loss 0.06091741472482681, Total Loss 6.089901447296143\n",
      "21: Encoding Loss -1.115233063697815, Transition Loss -8.561484336853027, Classifier Loss 0.07035434246063232, Total Loss 7.033721923828125\n",
      "21: Encoding Loss -2.0794835090637207, Transition Loss -10.148295402526855, Classifier Loss 0.07777231931686401, Total Loss 7.77520227432251\n",
      "21: Encoding Loss -0.5615513920783997, Transition Loss -4.290194511413574, Classifier Loss 0.07513206452131271, Total Loss 7.512348651885986\n",
      "21: Encoding Loss -2.1483378410339355, Transition Loss -12.009819030761719, Classifier Loss 0.0594768188893795, Total Loss 5.945280075073242\n",
      "21: Encoding Loss -0.5935186147689819, Transition Loss -5.879277229309082, Classifier Loss 0.09423014521598816, Total Loss 9.421838760375977\n",
      "21: Encoding Loss -0.6524056196212769, Transition Loss -9.009577751159668, Classifier Loss 0.11863507330417633, Total Loss 11.861705780029297\n",
      "21: Encoding Loss -2.317509889602661, Transition Loss -16.35110092163086, Classifier Loss 0.0821480080485344, Total Loss 8.211530685424805\n",
      "21: Encoding Loss -1.5684491395950317, Transition Loss -5.271205425262451, Classifier Loss 0.09285947680473328, Total Loss 9.284893989562988\n",
      "21: Encoding Loss -1.3558381795883179, Transition Loss -7.898251056671143, Classifier Loss 0.13610851764678955, Total Loss 13.609272003173828\n",
      "21: Encoding Loss -1.1549170017242432, Transition Loss -13.18681526184082, Classifier Loss 0.10354222357273102, Total Loss 10.351585388183594\n",
      "21: Encoding Loss -1.9801660776138306, Transition Loss -9.290966987609863, Classifier Loss 0.03929932042956352, Total Loss 3.9280738830566406\n",
      "21: Encoding Loss -0.7589768171310425, Transition Loss -9.366808891296387, Classifier Loss 0.078684002161026, Total Loss 7.8665266036987305\n",
      "21: Encoding Loss -1.5604172945022583, Transition Loss -2.7451767921447754, Classifier Loss 0.0625220388174057, Total Loss 6.251655101776123\n",
      "21: Encoding Loss -0.249803826212883, Transition Loss -5.943479537963867, Classifier Loss 0.08247651904821396, Total Loss 8.23398494720459\n",
      "21: Encoding Loss -1.0063163042068481, Transition Loss -6.511728286743164, Classifier Loss 0.07514187693595886, Total Loss 7.512885570526123\n",
      "21: Encoding Loss -0.5491309762001038, Transition Loss -2.23765230178833, Classifier Loss 0.06054847687482834, Total Loss 6.0543999671936035\n",
      "21: Encoding Loss -0.5478354692459106, Transition Loss -7.939629077911377, Classifier Loss 0.04446536302566528, Total Loss 4.444948196411133\n",
      "21: Encoding Loss -1.2953412532806396, Transition Loss -9.2424955368042, Classifier Loss 0.09187076985836029, Total Loss 9.18522834777832\n",
      "21: Encoding Loss -0.18415972590446472, Transition Loss -9.831618309020996, Classifier Loss 0.08224042505025864, Total Loss 8.17380142211914\n",
      "21: Encoding Loss -0.7444964647293091, Transition Loss 1.3473000526428223, Classifier Loss 0.12379951775074005, Total Loss 12.64941120147705\n",
      "21: Encoding Loss -0.5138981342315674, Transition Loss -5.384686470031738, Classifier Loss 0.056385233998298645, Total Loss 5.63744592666626\n",
      "21: Encoding Loss 0.42386457324028015, Transition Loss -8.661686897277832, Classifier Loss 0.10283121466636658, Total Loss 13.67226791381836\n",
      "21: Encoding Loss -0.03480202704668045, Transition Loss -7.384795188903809, Classifier Loss 0.08403382450342178, Total Loss 8.300585746765137\n",
      "21: Encoding Loss -0.9473946690559387, Transition Loss -6.665167331695557, Classifier Loss 0.0746329203248024, Total Loss 7.461958885192871\n",
      "21: Encoding Loss -1.480141282081604, Transition Loss -2.815223217010498, Classifier Loss 0.10283064842224121, Total Loss 10.282502174377441\n",
      "21: Encoding Loss -1.845702052116394, Transition Loss -10.825288772583008, Classifier Loss 0.0573250874876976, Total Loss 5.730343818664551\n",
      "21: Encoding Loss 0.5868927836418152, Transition Loss -0.28527605533599854, Classifier Loss 0.07504268735647202, Total Loss 12.19935417175293\n",
      "21: Encoding Loss 0.30969735980033875, Transition Loss -5.5419511795043945, Classifier Loss 0.05628134310245514, Total Loss 8.10218334197998\n",
      "21: Encoding Loss -0.07942347228527069, Transition Loss -4.832754611968994, Classifier Loss 0.08650331199169159, Total Loss 8.513690948486328\n",
      "21: Encoding Loss -0.23436109721660614, Transition Loss -14.36754322052002, Classifier Loss 0.14184542000293732, Total Loss 14.163764953613281\n",
      "21: Encoding Loss -0.47695115208625793, Transition Loss -14.103343963623047, Classifier Loss 0.10765543580055237, Total Loss 10.76271915435791\n",
      "21: Encoding Loss -0.9558942914009094, Transition Loss -19.950382232666016, Classifier Loss 0.06778358668088913, Total Loss 6.7743682861328125\n",
      "21: Encoding Loss 0.5292066931724548, Transition Loss -9.814517974853516, Classifier Loss 0.10339608043432236, Total Loss 14.571298599243164\n",
      "21: Encoding Loss -1.5367939472198486, Transition Loss -9.392438888549805, Classifier Loss 0.08510179817676544, Total Loss 8.50830078125\n",
      "21: Encoding Loss -2.2754228115081787, Transition Loss -15.356535911560059, Classifier Loss 0.0867474228143692, Total Loss 8.671671867370605\n",
      "21: Encoding Loss -1.548926591873169, Transition Loss -4.132206916809082, Classifier Loss 0.11201634258031845, Total Loss 11.200807571411133\n",
      "21: Encoding Loss -0.638660192489624, Transition Loss -12.476327896118164, Classifier Loss 0.10182467103004456, Total Loss 10.179972648620605\n",
      "21: Encoding Loss -1.2943655252456665, Transition Loss -13.152663230895996, Classifier Loss 0.036906227469444275, Total Loss 3.6879923343658447\n",
      "21: Encoding Loss -1.7400093078613281, Transition Loss -16.303695678710938, Classifier Loss 0.07641087472438812, Total Loss 7.637826919555664\n",
      "21: Encoding Loss -2.758676052093506, Transition Loss -21.045141220092773, Classifier Loss 0.04162643477320671, Total Loss 4.1584343910217285\n",
      "22: Encoding Loss -0.9785199761390686, Transition Loss -2.387402057647705, Classifier Loss 0.04709690064191818, Total Loss 4.709212779998779\n",
      "22: Encoding Loss -1.2809053659439087, Transition Loss -2.6916093826293945, Classifier Loss 0.1285298615694046, Total Loss 12.852448463439941\n",
      "22: Encoding Loss -1.1886180639266968, Transition Loss 1.4961668252944946, Classifier Loss 0.05648119002580643, Total Loss 5.947352409362793\n",
      "22: Encoding Loss -1.4515210390090942, Transition Loss -14.954282760620117, Classifier Loss 0.0844765305519104, Total Loss 8.444662094116211\n",
      "22: Encoding Loss -1.395518183708191, Transition Loss -6.034530162811279, Classifier Loss 0.06198558211326599, Total Loss 6.197351455688477\n",
      "22: Encoding Loss -1.5231400728225708, Transition Loss -12.34102725982666, Classifier Loss 0.07276015728712082, Total Loss 7.273547649383545\n",
      "22: Encoding Loss -1.4960885047912598, Transition Loss -8.69524097442627, Classifier Loss 0.08678402751684189, Total Loss 8.676663398742676\n",
      "22: Encoding Loss -1.4142869710922241, Transition Loss -6.332767486572266, Classifier Loss 0.06471496820449829, Total Loss 6.4702301025390625\n",
      "22: Encoding Loss -1.022535800933838, Transition Loss -12.21505355834961, Classifier Loss 0.06132708862423897, Total Loss 6.130266189575195\n",
      "22: Encoding Loss -1.9594939947128296, Transition Loss -9.494136810302734, Classifier Loss 0.08997657895088196, Total Loss 8.995759010314941\n",
      "22: Encoding Loss 0.3380315601825714, Transition Loss -6.238353729248047, Classifier Loss 0.07316356897354126, Total Loss 10.018383026123047\n",
      "22: Encoding Loss 0.06627114117145538, Transition Loss -2.6733527183532715, Classifier Loss 0.08518792688846588, Total Loss 8.91389274597168\n",
      "22: Encoding Loss -1.6849591732025146, Transition Loss -5.938368320465088, Classifier Loss 0.06544090807437897, Total Loss 6.542902946472168\n",
      "22: Encoding Loss -0.7931610345840454, Transition Loss -5.672995567321777, Classifier Loss 0.08687352389097214, Total Loss 8.686217308044434\n",
      "22: Encoding Loss -2.574472665786743, Transition Loss -13.331287384033203, Classifier Loss 0.0689786896109581, Total Loss 6.89520263671875\n",
      "22: Encoding Loss -2.027365207672119, Transition Loss -17.80074691772461, Classifier Loss 0.07245951145887375, Total Loss 7.242391109466553\n",
      "22: Encoding Loss -0.6314395070075989, Transition Loss -8.389610290527344, Classifier Loss 0.0348803848028183, Total Loss 3.486360549926758\n",
      "22: Encoding Loss 0.021120449528098106, Transition Loss -5.274293422698975, Classifier Loss 0.07146456837654114, Total Loss 7.244015216827393\n",
      "22: Encoding Loss -0.06482415646314621, Transition Loss -11.823931694030762, Classifier Loss 0.10941509157419205, Total Loss 10.805131912231445\n",
      "22: Encoding Loss -0.5478106737136841, Transition Loss -4.676060199737549, Classifier Loss 0.06758488714694977, Total Loss 6.757553577423096\n",
      "22: Encoding Loss -1.3262981176376343, Transition Loss -13.403087615966797, Classifier Loss 0.09502936154603958, Total Loss 9.500255584716797\n",
      "22: Encoding Loss -1.4863662719726562, Transition Loss -9.434045791625977, Classifier Loss 0.0579010434448719, Total Loss 5.788217544555664\n",
      "22: Encoding Loss -1.3488162755966187, Transition Loss -11.091032028198242, Classifier Loss 0.09433241188526154, Total Loss 9.431022644042969\n",
      "22: Encoding Loss -1.19082772731781, Transition Loss -2.976494789123535, Classifier Loss 0.0972350686788559, Total Loss 9.722911834716797\n",
      "22: Encoding Loss -1.9484775066375732, Transition Loss -10.370841026306152, Classifier Loss 0.0484108105301857, Total Loss 4.8390069007873535\n",
      "22: Encoding Loss -1.0106650590896606, Transition Loss -3.7545108795166016, Classifier Loss 0.07699133455753326, Total Loss 7.698382377624512\n",
      "22: Encoding Loss -1.5688128471374512, Transition Loss -2.1398627758026123, Classifier Loss 0.11303560435771942, Total Loss 11.303132057189941\n",
      "22: Encoding Loss -2.955080509185791, Transition Loss -17.327802658081055, Classifier Loss 0.09259908646345139, Total Loss 9.25644302368164\n",
      "22: Encoding Loss -2.438549757003784, Transition Loss -8.365730285644531, Classifier Loss 0.07562263309955597, Total Loss 7.5605902671813965\n",
      "22: Encoding Loss -0.5953876376152039, Transition Loss -3.962207317352295, Classifier Loss 0.08539074659347534, Total Loss 8.53828239440918\n",
      "22: Encoding Loss -0.802871584892273, Transition Loss -9.797783851623535, Classifier Loss 0.09660929441452026, Total Loss 9.65896987915039\n",
      "22: Encoding Loss -1.2581433057785034, Transition Loss -2.243281841278076, Classifier Loss 0.10734628140926361, Total Loss 10.734179496765137\n",
      "22: Encoding Loss -0.3673301339149475, Transition Loss -9.66201400756836, Classifier Loss 0.08355381339788437, Total Loss 8.353096961975098\n",
      "22: Encoding Loss -0.011666719801723957, Transition Loss -5.231324672698975, Classifier Loss 0.17700064182281494, Total Loss 17.65668487548828\n",
      "22: Encoding Loss -1.918365716934204, Transition Loss -11.51982307434082, Classifier Loss 0.056603122502565384, Total Loss 5.658008098602295\n",
      "22: Encoding Loss -1.1653485298156738, Transition Loss -6.53986930847168, Classifier Loss 0.047473687678575516, Total Loss 4.746060848236084\n",
      "22: Encoding Loss -1.2517319917678833, Transition Loss -6.545503616333008, Classifier Loss 0.09780879318714142, Total Loss 9.779569625854492\n",
      "22: Encoding Loss -0.8324686884880066, Transition Loss -8.663533210754395, Classifier Loss 0.12515082955360413, Total Loss 12.513350486755371\n",
      "22: Encoding Loss -1.4288686513900757, Transition Loss -8.496665954589844, Classifier Loss 0.08384378254413605, Total Loss 8.382678985595703\n",
      "22: Encoding Loss -1.618304967880249, Transition Loss -9.728886604309082, Classifier Loss 0.08487160503864288, Total Loss 8.485215187072754\n",
      "22: Encoding Loss -1.6823146343231201, Transition Loss -6.789852619171143, Classifier Loss 0.05022001639008522, Total Loss 5.020643711090088\n",
      "22: Encoding Loss -1.4233168363571167, Transition Loss -14.293801307678223, Classifier Loss 0.10722927004098892, Total Loss 10.720067977905273\n",
      "22: Encoding Loss -2.0659170150756836, Transition Loss -13.892504692077637, Classifier Loss 0.04632613807916641, Total Loss 4.62983512878418\n",
      "22: Encoding Loss -1.7764760255813599, Transition Loss -3.702632188796997, Classifier Loss 0.08630982786417007, Total Loss 8.630242347717285\n",
      "22: Encoding Loss -1.446334958076477, Transition Loss -15.050500869750977, Classifier Loss 0.06537610292434692, Total Loss 6.534599781036377\n",
      "22: Encoding Loss 0.35037192702293396, Transition Loss -5.856407165527344, Classifier Loss 0.10216930508613586, Total Loss 13.018092155456543\n",
      "22: Encoding Loss 1.042632818222046, Transition Loss -14.946757316589355, Classifier Loss 0.08743840456008911, Total Loss 17.0819149017334\n",
      "22: Encoding Loss -0.5973479151725769, Transition Loss -4.260758399963379, Classifier Loss 0.04736022651195526, Total Loss 4.735170364379883\n",
      "22: Encoding Loss 1.0066779851913452, Transition Loss -6.479377746582031, Classifier Loss 0.08179284632205963, Total Loss 16.231412887573242\n",
      "22: Encoding Loss -0.7226682901382446, Transition Loss -15.770218849182129, Classifier Loss 0.06997296959161758, Total Loss 6.994142532348633\n",
      "22: Encoding Loss -1.0056214332580566, Transition Loss -19.61896324157715, Classifier Loss 0.0655020922422409, Total Loss 6.546285152435303\n",
      "22: Encoding Loss -0.1648063212633133, Transition Loss -12.032899856567383, Classifier Loss 0.0827215239405632, Total Loss 8.204258918762207\n",
      "22: Encoding Loss -1.0589559078216553, Transition Loss -3.3965797424316406, Classifier Loss 0.10179231315851212, Total Loss 10.178552627563477\n",
      "22: Encoding Loss -1.003250002861023, Transition Loss -12.139418601989746, Classifier Loss 0.06701440364122391, Total Loss 6.699012279510498\n",
      "22: Encoding Loss -0.0850515216588974, Transition Loss -11.746525764465332, Classifier Loss 0.07968224585056305, Total Loss 7.831480503082275\n",
      "22: Encoding Loss -0.7115755677223206, Transition Loss -4.474842071533203, Classifier Loss 0.10075480490922928, Total Loss 10.074585914611816\n",
      "22: Encoding Loss 0.5897734761238098, Transition Loss -11.078390121459961, Classifier Loss 0.05038420483469963, Total Loss 9.754393577575684\n",
      "22: Encoding Loss -1.372747778892517, Transition Loss -9.337017059326172, Classifier Loss 0.07815874367952347, Total Loss 7.81400728225708\n",
      "22: Encoding Loss -1.1157724857330322, Transition Loss -15.852251052856445, Classifier Loss 0.061988480389118195, Total Loss 6.195677757263184\n",
      "22: Encoding Loss -1.0877493619918823, Transition Loss -12.537793159484863, Classifier Loss 0.08357565104961395, Total Loss 8.355057716369629\n",
      "22: Encoding Loss -0.5872777104377747, Transition Loss -6.516809463500977, Classifier Loss 0.0784018337726593, Total Loss 7.8388800621032715\n",
      "22: Encoding Loss -0.8557916283607483, Transition Loss -5.067531108856201, Classifier Loss 0.12072744965553284, Total Loss 12.071731567382812\n",
      "22: Encoding Loss -0.7726742625236511, Transition Loss -7.111752510070801, Classifier Loss 0.08262009173631668, Total Loss 8.260587692260742\n",
      "22: Encoding Loss -0.6874725222587585, Transition Loss -4.797111511230469, Classifier Loss 0.08570945262908936, Total Loss 8.569986343383789\n",
      "22: Encoding Loss -0.723203718662262, Transition Loss -8.971360206604004, Classifier Loss 0.07864772528409958, Total Loss 7.862977981567383\n",
      "22: Encoding Loss -0.8812472224235535, Transition Loss -2.2513608932495117, Classifier Loss 0.1298365294933319, Total Loss 12.983202934265137\n",
      "22: Encoding Loss 0.6254473924636841, Transition Loss 1.0535757541656494, Classifier Loss 0.07379972189664841, Total Loss 12.594265937805176\n",
      "22: Encoding Loss -1.8518458604812622, Transition Loss -17.14285659790039, Classifier Loss 0.1051144078373909, Total Loss 10.508012771606445\n",
      "22: Encoding Loss -0.052232079207897186, Transition Loss -7.52773904800415, Classifier Loss 0.07951055467128754, Total Loss 7.823890686035156\n",
      "22: Encoding Loss -0.23758326470851898, Transition Loss -8.667774200439453, Classifier Loss 0.12817442417144775, Total Loss 12.799068450927734\n",
      "22: Encoding Loss -1.4713575839996338, Transition Loss -8.390339851379395, Classifier Loss 0.10372044891119003, Total Loss 10.370366096496582\n",
      "22: Encoding Loss -1.2049553394317627, Transition Loss -8.56244945526123, Classifier Loss 0.08852890878915787, Total Loss 8.851178169250488\n",
      "22: Encoding Loss -0.479277104139328, Transition Loss -4.585235595703125, Classifier Loss 0.12599553167819977, Total Loss 12.5986328125\n",
      "22: Encoding Loss -0.9038872122764587, Transition Loss -8.185260772705078, Classifier Loss 0.03390026465058327, Total Loss 3.3883895874023438\n",
      "22: Encoding Loss -1.4025026559829712, Transition Loss -6.883382797241211, Classifier Loss 0.1327638477087021, Total Loss 13.275007247924805\n",
      "22: Encoding Loss -1.151987910270691, Transition Loss -13.465810775756836, Classifier Loss 0.06142786145210266, Total Loss 6.140092849731445\n",
      "22: Encoding Loss -1.5107887983322144, Transition Loss -5.61600399017334, Classifier Loss 0.09913121908903122, Total Loss 9.911998748779297\n",
      "22: Encoding Loss -2.212252616882324, Transition Loss -11.371139526367188, Classifier Loss 0.07850004732608795, Total Loss 7.84773063659668\n",
      "22: Encoding Loss -1.3571621179580688, Transition Loss -12.452390670776367, Classifier Loss 0.04903142526745796, Total Loss 4.900651931762695\n",
      "22: Encoding Loss -1.3142521381378174, Transition Loss -9.285228729248047, Classifier Loss 0.12573175132274628, Total Loss 12.571318626403809\n",
      "22: Encoding Loss -0.6665908098220825, Transition Loss -10.214986801147461, Classifier Loss 0.07403792440891266, Total Loss 7.401749610900879\n",
      "22: Encoding Loss -1.6630587577819824, Transition Loss -7.522635459899902, Classifier Loss 0.07415023446083069, Total Loss 7.413518905639648\n",
      "22: Encoding Loss -0.9024208784103394, Transition Loss -4.323128700256348, Classifier Loss 0.10219196230173111, Total Loss 10.218331336975098\n",
      "22: Encoding Loss -0.36329957842826843, Transition Loss -9.920666694641113, Classifier Loss 0.11498678475618362, Total Loss 11.496286392211914\n",
      "22: Encoding Loss -2.3448293209075928, Transition Loss -7.034536361694336, Classifier Loss 0.07234634459018707, Total Loss 7.233227729797363\n",
      "22: Encoding Loss -1.296221137046814, Transition Loss -4.350651741027832, Classifier Loss 0.07917279005050659, Total Loss 7.916409015655518\n",
      "22: Encoding Loss -1.5284026861190796, Transition Loss -14.996797561645508, Classifier Loss 0.08676724880933762, Total Loss 8.673725128173828\n",
      "22: Encoding Loss -1.3020774126052856, Transition Loss -8.5050048828125, Classifier Loss 0.1201997622847557, Total Loss 12.018275260925293\n",
      "22: Encoding Loss -0.44674035906791687, Transition Loss -6.855696678161621, Classifier Loss 0.062489524483680725, Total Loss 6.247567176818848\n",
      "22: Encoding Loss -0.45285919308662415, Transition Loss -11.700035095214844, Classifier Loss 0.08304113149642944, Total Loss 8.301762580871582\n",
      "22: Encoding Loss -2.9944911003112793, Transition Loss -20.699777603149414, Classifier Loss 0.07850011438131332, Total Loss 7.845871448516846\n",
      "22: Encoding Loss -1.0417994260787964, Transition Loss -6.34706449508667, Classifier Loss 0.10523495078086853, Total Loss 10.522225379943848\n",
      "22: Encoding Loss -1.5285238027572632, Transition Loss -6.788321495056152, Classifier Loss 0.08917944133281708, Total Loss 8.916585922241211\n",
      "22: Encoding Loss -0.734935462474823, Transition Loss -9.118719100952148, Classifier Loss 0.09325262904167175, Total Loss 9.323439598083496\n",
      "22: Encoding Loss -0.7083958387374878, Transition Loss -3.097931146621704, Classifier Loss 0.08642057329416275, Total Loss 8.641437530517578\n",
      "22: Encoding Loss -1.444811463356018, Transition Loss -11.748743057250977, Classifier Loss 0.09516602754592896, Total Loss 9.514252662658691\n",
      "22: Encoding Loss -1.3472923040390015, Transition Loss -14.916802406311035, Classifier Loss 0.10029660165309906, Total Loss 10.026677131652832\n",
      "22: Encoding Loss -1.744551181793213, Transition Loss -12.299964904785156, Classifier Loss 0.0685407966375351, Total Loss 6.851619720458984\n",
      "22: Encoding Loss -0.95487380027771, Transition Loss -3.317899227142334, Classifier Loss 0.049689047038555145, Total Loss 4.968240737915039\n",
      "22: Encoding Loss -1.3796418905258179, Transition Loss -6.080260276794434, Classifier Loss 0.12727856636047363, Total Loss 12.726640701293945\n",
      "22: Encoding Loss -1.5220521688461304, Transition Loss -13.088672637939453, Classifier Loss 0.08281325548887253, Total Loss 8.278707504272461\n",
      "22: Encoding Loss -0.23721863329410553, Transition Loss 1.1406294107437134, Classifier Loss 0.119158074259758, Total Loss 12.127154350280762\n",
      "22: Encoding Loss -0.5819636583328247, Transition Loss -6.553482532501221, Classifier Loss 0.07952210307121277, Total Loss 7.950899600982666\n",
      "22: Encoding Loss -0.3312329947948456, Transition Loss -3.6187918186187744, Classifier Loss 0.10200224071741104, Total Loss 10.198274612426758\n",
      "22: Encoding Loss -1.1445624828338623, Transition Loss -10.92990779876709, Classifier Loss 0.06793391704559326, Total Loss 6.791205883026123\n",
      "22: Encoding Loss -1.2958046197891235, Transition Loss -14.377825736999512, Classifier Loss 0.09992381930351257, Total Loss 9.989506721496582\n",
      "22: Encoding Loss -1.122619390487671, Transition Loss -3.20760178565979, Classifier Loss 0.09880580008029938, Total Loss 9.879938125610352\n",
      "22: Encoding Loss -1.5488431453704834, Transition Loss -7.343804359436035, Classifier Loss 0.04999813064932823, Total Loss 4.998344421386719\n",
      "22: Encoding Loss -1.7134004831314087, Transition Loss -7.735301494598389, Classifier Loss 0.0736546665430069, Total Loss 7.363919734954834\n",
      "22: Encoding Loss -0.4947255849838257, Transition Loss -13.53011417388916, Classifier Loss 0.08578404784202576, Total Loss 8.57569694519043\n",
      "22: Encoding Loss -0.95144122838974, Transition Loss -12.77413558959961, Classifier Loss 0.09170591831207275, Total Loss 9.168036460876465\n",
      "22: Encoding Loss -1.0702115297317505, Transition Loss -0.30728501081466675, Classifier Loss 0.054694805294275284, Total Loss 5.469419002532959\n",
      "22: Encoding Loss -1.8249648809432983, Transition Loss -16.44586944580078, Classifier Loss 0.061721015721559525, Total Loss 6.168812274932861\n",
      "22: Encoding Loss -1.4255725145339966, Transition Loss -15.705118179321289, Classifier Loss 0.06028885394334793, Total Loss 6.025744438171387\n",
      "22: Encoding Loss -0.20967744290828705, Transition Loss 0.020909518003463745, Classifier Loss 0.08030369132757187, Total Loss 8.004345893859863\n",
      "22: Encoding Loss -0.4628760516643524, Transition Loss 4.269063949584961, Classifier Loss 0.07739425450563431, Total Loss 8.593232154846191\n",
      "22: Encoding Loss -0.8893918395042419, Transition Loss -10.929521560668945, Classifier Loss 0.05836007744073868, Total Loss 5.833821773529053\n",
      "22: Encoding Loss -1.0255241394042969, Transition Loss -10.105596542358398, Classifier Loss 0.06530819088220596, Total Loss 6.528797626495361\n",
      "22: Encoding Loss -2.02227520942688, Transition Loss -12.087824821472168, Classifier Loss 0.0779762789607048, Total Loss 7.795210361480713\n",
      "22: Encoding Loss -0.647326648235321, Transition Loss -5.5328474044799805, Classifier Loss 0.07423081994056702, Total Loss 7.421975135803223\n",
      "22: Encoding Loss -2.021688222885132, Transition Loss -13.918721199035645, Classifier Loss 0.0597090907394886, Total Loss 5.968125343322754\n",
      "22: Encoding Loss -1.0504766702651978, Transition Loss -7.4930524826049805, Classifier Loss 0.09123184531927109, Total Loss 9.121685981750488\n",
      "22: Encoding Loss -0.641538679599762, Transition Loss -10.513060569763184, Classifier Loss 0.11634043604135513, Total Loss 11.631940841674805\n",
      "22: Encoding Loss -2.237302303314209, Transition Loss -18.798276901245117, Classifier Loss 0.08112223446369171, Total Loss 8.108464241027832\n",
      "22: Encoding Loss -1.5455012321472168, Transition Loss -6.542424201965332, Classifier Loss 0.08667132258415222, Total Loss 8.665823936462402\n",
      "22: Encoding Loss -1.1852835416793823, Transition Loss -9.286212921142578, Classifier Loss 0.134259432554245, Total Loss 13.424086570739746\n",
      "22: Encoding Loss -1.1987035274505615, Transition Loss -15.33895492553711, Classifier Loss 0.10405503958463669, Total Loss 10.402436256408691\n",
      "22: Encoding Loss -2.011733293533325, Transition Loss -11.159256935119629, Classifier Loss 0.039010655134916306, Total Loss 3.898833751678467\n",
      "22: Encoding Loss -0.8198941349983215, Transition Loss -10.763341903686523, Classifier Loss 0.07644905894994736, Total Loss 7.642753601074219\n",
      "22: Encoding Loss -1.6010031700134277, Transition Loss -3.6948771476745605, Classifier Loss 0.06030452251434326, Total Loss 6.029713153839111\n",
      "22: Encoding Loss -0.5078260898590088, Transition Loss -7.113794803619385, Classifier Loss 0.07983292639255524, Total Loss 7.981868743896484\n",
      "22: Encoding Loss -1.0214217901229858, Transition Loss -7.943182945251465, Classifier Loss 0.0740857943892479, Total Loss 7.4069905281066895\n",
      "22: Encoding Loss -0.6526896953582764, Transition Loss -3.218956470489502, Classifier Loss 0.05988527834415436, Total Loss 5.987884044647217\n",
      "22: Encoding Loss -0.6230066418647766, Transition Loss -9.409244537353516, Classifier Loss 0.04198293760418892, Total Loss 4.196411609649658\n",
      "22: Encoding Loss -1.2421733140945435, Transition Loss -10.925705909729004, Classifier Loss 0.08788614720106125, Total Loss 8.786429405212402\n",
      "22: Encoding Loss -0.2794340252876282, Transition Loss -11.477417945861816, Classifier Loss 0.08358291536569595, Total Loss 8.350183486938477\n",
      "22: Encoding Loss -0.8626713156700134, Transition Loss 1.0069339275360107, Classifier Loss 0.1205800473690033, Total Loss 12.259390830993652\n",
      "22: Encoding Loss -0.5961195230484009, Transition Loss -6.768585205078125, Classifier Loss 0.05488388612866402, Total Loss 5.487034797668457\n",
      "22: Encoding Loss 0.2677689492702484, Transition Loss -10.506234169006348, Classifier Loss 0.0961315855383873, Total Loss 11.745268821716309\n",
      "22: Encoding Loss 0.28835153579711914, Transition Loss -6.447858810424805, Classifier Loss 0.08102167397737503, Total Loss 10.403154373168945\n",
      "22: Encoding Loss -1.230059027671814, Transition Loss -7.5341291427612305, Classifier Loss 0.06881634891033173, Total Loss 6.880127906799316\n",
      "22: Encoding Loss -1.3077218532562256, Transition Loss -3.5334861278533936, Classifier Loss 0.1000780314207077, Total Loss 10.007096290588379\n",
      "22: Encoding Loss -2.0201783180236816, Transition Loss -12.103864669799805, Classifier Loss 0.05198720097541809, Total Loss 5.196299076080322\n",
      "22: Encoding Loss 0.3896484971046448, Transition Loss -0.8158326148986816, Classifier Loss 0.07407747209072113, Total Loss 10.524620056152344\n",
      "22: Encoding Loss -0.6356543898582458, Transition Loss -4.730247497558594, Classifier Loss 0.049803148955106735, Total Loss 4.979368686676025\n",
      "22: Encoding Loss -0.4599689543247223, Transition Loss -3.976590871810913, Classifier Loss 0.092793770134449, Total Loss 9.278573989868164\n",
      "22: Encoding Loss -0.47977060079574585, Transition Loss -12.828059196472168, Classifier Loss 0.15604731440544128, Total Loss 15.602163314819336\n",
      "22: Encoding Loss -1.766866683959961, Transition Loss -12.953635215759277, Classifier Loss 0.09608892351388931, Total Loss 9.606301307678223\n",
      "22: Encoding Loss -2.486720323562622, Transition Loss -18.183216094970703, Classifier Loss 0.06168479472398758, Total Loss 6.16484260559082\n",
      "22: Encoding Loss -0.5023930668830872, Transition Loss -8.765436172485352, Classifier Loss 0.10538133978843689, Total Loss 10.53637981414795\n",
      "22: Encoding Loss -1.1418957710266113, Transition Loss -9.439010620117188, Classifier Loss 0.08120163530111313, Total Loss 8.11827564239502\n",
      "22: Encoding Loss -2.3459155559539795, Transition Loss -15.675863265991211, Classifier Loss 0.0801108181476593, Total Loss 8.007946968078613\n",
      "22: Encoding Loss -1.6221386194229126, Transition Loss -3.519531011581421, Classifier Loss 0.10511171072721481, Total Loss 10.510467529296875\n",
      "22: Encoding Loss -0.18922504782676697, Transition Loss -12.506714820861816, Classifier Loss 0.10480724275112152, Total Loss 10.433976173400879\n",
      "22: Encoding Loss -0.5694587230682373, Transition Loss -12.912910461425781, Classifier Loss 0.03250870853662491, Total Loss 3.24828839302063\n",
      "22: Encoding Loss -1.5555282831192017, Transition Loss -16.40550422668457, Classifier Loss 0.07234641164541245, Total Loss 7.231359958648682\n",
      "22: Encoding Loss -1.57307767868042, Transition Loss -21.79623031616211, Classifier Loss 0.03782426565885544, Total Loss 3.7780673503875732\n",
      "23: Encoding Loss -0.7335336804389954, Transition Loss -1.539407730102539, Classifier Loss 0.04611751064658165, Total Loss 4.611443042755127\n",
      "23: Encoding Loss -1.3270663022994995, Transition Loss -1.9435802698135376, Classifier Loss 0.12811313569545746, Total Loss 12.810924530029297\n",
      "23: Encoding Loss -1.0450140237808228, Transition Loss 2.952770709991455, Classifier Loss 0.052755095064640045, Total Loss 5.866063594818115\n",
      "23: Encoding Loss -1.178473711013794, Transition Loss -15.04183578491211, Classifier Loss 0.08358146250247955, Total Loss 8.355137825012207\n",
      "23: Encoding Loss -0.8380938768386841, Transition Loss -5.313658237457275, Classifier Loss 0.05896713212132454, Total Loss 5.895650386810303\n",
      "23: Encoding Loss -0.9194252490997314, Transition Loss -12.106529235839844, Classifier Loss 0.06805001199245453, Total Loss 6.802579879760742\n",
      "23: Encoding Loss -1.3111748695373535, Transition Loss -7.979076862335205, Classifier Loss 0.08476226031780243, Total Loss 8.474630355834961\n",
      "23: Encoding Loss -0.5875836610794067, Transition Loss -5.829596519470215, Classifier Loss 0.0644991546869278, Total Loss 6.448749542236328\n",
      "23: Encoding Loss -0.8918628096580505, Transition Loss -12.102325439453125, Classifier Loss 0.05849869176745415, Total Loss 5.847448825836182\n",
      "23: Encoding Loss -1.6927849054336548, Transition Loss -9.3258638381958, Classifier Loss 0.08719643205404282, Total Loss 8.717778205871582\n",
      "23: Encoding Loss 0.4458966851234436, Transition Loss -5.596804618835449, Classifier Loss 0.07156771421432495, Total Loss 10.722810745239258\n",
      "23: Encoding Loss 0.5554109215736389, Transition Loss -3.030792236328125, Classifier Loss 0.08481422811746597, Total Loss 12.924102783203125\n",
      "23: Encoding Loss -0.845437228679657, Transition Loss -5.779067039489746, Classifier Loss 0.062437452375888824, Total Loss 6.242589473724365\n",
      "23: Encoding Loss -0.5260534286499023, Transition Loss -5.710110664367676, Classifier Loss 0.08613763749599457, Total Loss 8.612622261047363\n",
      "23: Encoding Loss -1.6389636993408203, Transition Loss -13.411972045898438, Classifier Loss 0.06346724182367325, Total Loss 6.34404182434082\n",
      "23: Encoding Loss -1.5851161479949951, Transition Loss -18.06902313232422, Classifier Loss 0.07155153155326843, Total Loss 7.151539325714111\n",
      "23: Encoding Loss -0.2111257165670395, Transition Loss -8.47216510772705, Classifier Loss 0.03566848859190941, Total Loss 3.5358076095581055\n",
      "23: Encoding Loss 0.30635547637939453, Transition Loss -5.190301895141602, Classifier Loss 0.06739921867847443, Total Loss 9.187047958374023\n",
      "23: Encoding Loss 0.21508386731147766, Transition Loss -12.150691986083984, Classifier Loss 0.10593540966510773, Total Loss 12.284690856933594\n",
      "23: Encoding Loss 0.9911352396011353, Transition Loss -4.195497512817383, Classifier Loss 0.06765198707580566, Total Loss 14.693441390991211\n",
      "23: Encoding Loss 0.7640405893325806, Transition Loss -14.449131965637207, Classifier Loss 0.09522409737110138, Total Loss 15.631844520568848\n",
      "23: Encoding Loss -0.2911693751811981, Transition Loss -9.472644805908203, Classifier Loss 0.056299664080142975, Total Loss 5.623885154724121\n",
      "23: Encoding Loss -0.2343742996454239, Transition Loss -10.980218887329102, Classifier Loss 0.10290683805942535, Total Loss 10.270589828491211\n",
      "23: Encoding Loss -0.16695506870746613, Transition Loss -2.708263874053955, Classifier Loss 0.09952326118946075, Total Loss 9.888335227966309\n",
      "23: Encoding Loss -0.07458925247192383, Transition Loss -10.290313720703125, Classifier Loss 0.04675450921058655, Total Loss 4.537421703338623\n",
      "23: Encoding Loss 0.3091462552547455, Transition Loss -3.6354005336761475, Classifier Loss 0.07702077925205231, Total Loss 10.17205810546875\n",
      "23: Encoding Loss -1.5193355083465576, Transition Loss -3.1245877742767334, Classifier Loss 0.11238855868577957, Total Loss 11.238231658935547\n",
      "23: Encoding Loss -2.528848886489868, Transition Loss -18.331348419189453, Classifier Loss 0.09425123035907745, Total Loss 9.421457290649414\n",
      "23: Encoding Loss -2.13832426071167, Transition Loss -9.28251838684082, Classifier Loss 0.07799862325191498, Total Loss 7.798006057739258\n",
      "23: Encoding Loss -0.6787701845169067, Transition Loss -4.961894512176514, Classifier Loss 0.08488167077302933, Total Loss 8.487174034118652\n",
      "23: Encoding Loss -0.7655063271522522, Transition Loss -10.780862808227539, Classifier Loss 0.09451839327812195, Total Loss 9.44968318939209\n",
      "23: Encoding Loss -1.108297348022461, Transition Loss -3.091557502746582, Classifier Loss 0.10158079117536545, Total Loss 10.157461166381836\n",
      "23: Encoding Loss -0.27848386764526367, Transition Loss -10.512861251831055, Classifier Loss 0.08646031469106674, Total Loss 8.63796329498291\n",
      "23: Encoding Loss -0.10706712305545807, Transition Loss -6.1869096755981445, Classifier Loss 0.1781981736421585, Total Loss 17.696815490722656\n",
      "23: Encoding Loss -1.5837763547897339, Transition Loss -11.886897087097168, Classifier Loss 0.05525042489171028, Total Loss 5.522665023803711\n",
      "23: Encoding Loss -1.0239999294281006, Transition Loss -6.810807704925537, Classifier Loss 0.04615345597267151, Total Loss 4.613983154296875\n",
      "23: Encoding Loss -0.9261295199394226, Transition Loss -6.832732200622559, Classifier Loss 0.09377988427877426, Total Loss 9.376622200012207\n",
      "23: Encoding Loss -0.7478316426277161, Transition Loss -8.946630477905273, Classifier Loss 0.12646308541297913, Total Loss 12.644519805908203\n",
      "23: Encoding Loss -1.393021821975708, Transition Loss -9.038832664489746, Classifier Loss 0.08144970238208771, Total Loss 8.14316177368164\n",
      "23: Encoding Loss -1.2929208278656006, Transition Loss -10.362727165222168, Classifier Loss 0.08389187604188919, Total Loss 8.387115478515625\n",
      "23: Encoding Loss -1.3719252347946167, Transition Loss -7.195036888122559, Classifier Loss 0.0492001548409462, Total Loss 4.918576240539551\n",
      "23: Encoding Loss -1.2877203226089478, Transition Loss -14.941655158996582, Classifier Loss 0.10455626249313354, Total Loss 10.452638626098633\n",
      "23: Encoding Loss -1.787509560585022, Transition Loss -14.568603515625, Classifier Loss 0.048590756952762604, Total Loss 4.856161594390869\n",
      "23: Encoding Loss -1.465111255645752, Transition Loss -4.341361045837402, Classifier Loss 0.08114924281835556, Total Loss 8.114056587219238\n",
      "23: Encoding Loss -1.4290167093276978, Transition Loss -15.619180679321289, Classifier Loss 0.06379493325948715, Total Loss 6.376369476318359\n",
      "23: Encoding Loss 0.1689349114894867, Transition Loss -6.497589111328125, Classifier Loss 0.1007474809885025, Total Loss 11.36333179473877\n",
      "23: Encoding Loss 0.3544858992099762, Transition Loss -15.042564392089844, Classifier Loss 0.0871155858039856, Total Loss 11.543879508972168\n",
      "23: Encoding Loss -1.637490153312683, Transition Loss -5.156247138977051, Classifier Loss 0.046784307807683945, Total Loss 4.677399158477783\n",
      "23: Encoding Loss -0.6341330409049988, Transition Loss -7.585063457489014, Classifier Loss 0.07978010177612305, Total Loss 7.9764933586120605\n",
      "23: Encoding Loss -0.5301712155342102, Transition Loss -14.320168495178223, Classifier Loss 0.06730077415704727, Total Loss 6.727212905883789\n",
      "23: Encoding Loss -1.128248691558838, Transition Loss -17.591867446899414, Classifier Loss 0.06056835874915123, Total Loss 6.053317070007324\n",
      "23: Encoding Loss -0.13764886558055878, Transition Loss -10.704771041870117, Classifier Loss 0.07995814830064774, Total Loss 7.90080451965332\n",
      "23: Encoding Loss -0.17423628270626068, Transition Loss -2.062558650970459, Classifier Loss 0.09149650484323502, Total Loss 9.092474937438965\n",
      "23: Encoding Loss -0.2326870858669281, Transition Loss -10.196507453918457, Classifier Loss 0.06667983531951904, Total Loss 6.647355079650879\n",
      "23: Encoding Loss 0.36072877049446106, Transition Loss -9.77907657623291, Classifier Loss 0.08196020126342773, Total Loss 11.079447746276855\n",
      "23: Encoding Loss -1.3610799312591553, Transition Loss -4.047978401184082, Classifier Loss 0.10175072401762009, Total Loss 10.174263000488281\n",
      "23: Encoding Loss -0.21832294762134552, Transition Loss -10.134498596191406, Classifier Loss 0.049723926931619644, Total Loss 4.945023536682129\n",
      "23: Encoding Loss -1.7198978662490845, Transition Loss -11.764135360717773, Classifier Loss 0.07530791312456131, Total Loss 7.528438568115234\n",
      "23: Encoding Loss -2.027721643447876, Transition Loss -18.457984924316406, Classifier Loss 0.06189320981502533, Total Loss 6.185629367828369\n",
      "23: Encoding Loss -1.5438861846923828, Transition Loss -15.148119926452637, Classifier Loss 0.08083991706371307, Total Loss 8.080962181091309\n",
      "23: Encoding Loss -1.411891222000122, Transition Loss -8.647375106811523, Classifier Loss 0.07805311679840088, Total Loss 7.803582191467285\n",
      "23: Encoding Loss -1.7290631532669067, Transition Loss -7.066524982452393, Classifier Loss 0.12097233533859253, Total Loss 12.095820426940918\n",
      "23: Encoding Loss -1.37299382686615, Transition Loss -9.300010681152344, Classifier Loss 0.07998014241456985, Total Loss 7.996154308319092\n",
      "23: Encoding Loss -1.1482073068618774, Transition Loss -6.6763763427734375, Classifier Loss 0.08370209485292435, Total Loss 8.368874549865723\n",
      "23: Encoding Loss -1.5195609331130981, Transition Loss -11.188517570495605, Classifier Loss 0.08061166107654572, Total Loss 8.058928489685059\n",
      "23: Encoding Loss -1.7250874042510986, Transition Loss -4.247690200805664, Classifier Loss 0.1253231316804886, Total Loss 12.531463623046875\n",
      "23: Encoding Loss -0.24627679586410522, Transition Loss -0.7639693021774292, Classifier Loss 0.07168139517307281, Total Loss 7.154405117034912\n",
      "23: Encoding Loss -1.904362440109253, Transition Loss -16.49672508239746, Classifier Loss 0.0965135395526886, Total Loss 9.648054122924805\n",
      "23: Encoding Loss -0.08999264985322952, Transition Loss -7.130392074584961, Classifier Loss 0.07689926028251648, Total Loss 7.555973529815674\n",
      "23: Encoding Loss -0.6281095743179321, Transition Loss -7.980541229248047, Classifier Loss 0.13049975037574768, Total Loss 13.048378944396973\n",
      "23: Encoding Loss -1.3517727851867676, Transition Loss -7.587921142578125, Classifier Loss 0.09539083391427994, Total Loss 9.537566184997559\n",
      "23: Encoding Loss -1.2224085330963135, Transition Loss -7.8594770431518555, Classifier Loss 0.08797252178192139, Total Loss 8.79568099975586\n",
      "23: Encoding Loss -0.8363254070281982, Transition Loss -4.218177795410156, Classifier Loss 0.11963821947574615, Total Loss 11.96297836303711\n",
      "23: Encoding Loss -1.101096510887146, Transition Loss -7.722682476043701, Classifier Loss 0.033773764967918396, Total Loss 3.3758320808410645\n",
      "23: Encoding Loss -1.4967738389968872, Transition Loss -6.558577537536621, Classifier Loss 0.12713468074798584, Total Loss 12.712157249450684\n",
      "23: Encoding Loss -1.3728084564208984, Transition Loss -13.001100540161133, Classifier Loss 0.05649580806493759, Total Loss 5.6469807624816895\n",
      "23: Encoding Loss -1.1172153949737549, Transition Loss -5.010478973388672, Classifier Loss 0.09674126654863358, Total Loss 9.673124313354492\n",
      "23: Encoding Loss -2.53705096244812, Transition Loss -10.899164199829102, Classifier Loss 0.07656245678663254, Total Loss 7.65406608581543\n",
      "23: Encoding Loss -1.5663039684295654, Transition Loss -12.005304336547852, Classifier Loss 0.0445091687142849, Total Loss 4.448515892028809\n",
      "23: Encoding Loss -1.061721920967102, Transition Loss -8.863204002380371, Classifier Loss 0.12285278737545013, Total Loss 12.2835054397583\n",
      "23: Encoding Loss -0.5762420296669006, Transition Loss -9.864527702331543, Classifier Loss 0.07135093212127686, Total Loss 7.133120536804199\n",
      "23: Encoding Loss -1.4804539680480957, Transition Loss -7.2388691902160645, Classifier Loss 0.06746240705251694, Total Loss 6.744792938232422\n",
      "23: Encoding Loss -0.853621780872345, Transition Loss -4.0438103675842285, Classifier Loss 0.10569936037063599, Total Loss 10.569127082824707\n",
      "23: Encoding Loss -0.27228930592536926, Transition Loss -9.64778995513916, Classifier Loss 0.10991056263446808, Total Loss 10.982078552246094\n",
      "23: Encoding Loss -2.241908073425293, Transition Loss -6.53889799118042, Classifier Loss 0.07031986117362976, Total Loss 7.0306782722473145\n",
      "23: Encoding Loss -1.363878846168518, Transition Loss -4.103126049041748, Classifier Loss 0.07535522431135178, Total Loss 7.534701824188232\n",
      "23: Encoding Loss -1.6458685398101807, Transition Loss -14.370266914367676, Classifier Loss 0.08325086534023285, Total Loss 8.322212219238281\n",
      "23: Encoding Loss -1.2124543190002441, Transition Loss -8.189610481262207, Classifier Loss 0.12245732545852661, Total Loss 12.244094848632812\n",
      "23: Encoding Loss -0.8527213931083679, Transition Loss -6.517745018005371, Classifier Loss 0.062471188604831696, Total Loss 6.245815277099609\n",
      "23: Encoding Loss -0.6326059699058533, Transition Loss -11.569866180419922, Classifier Loss 0.07580775767564774, Total Loss 7.578461647033691\n",
      "23: Encoding Loss -2.911546468734741, Transition Loss -20.08844757080078, Classifier Loss 0.07683221995830536, Total Loss 7.679203987121582\n",
      "23: Encoding Loss -0.9512401819229126, Transition Loss -6.021032333374023, Classifier Loss 0.10112117230892181, Total Loss 10.110912322998047\n",
      "23: Encoding Loss -1.5146889686584473, Transition Loss -6.360467910766602, Classifier Loss 0.08680218458175659, Total Loss 8.678946495056152\n",
      "23: Encoding Loss -0.8277560472488403, Transition Loss -8.602337837219238, Classifier Loss 0.0914769098162651, Total Loss 9.145970344543457\n",
      "23: Encoding Loss -0.8015986680984497, Transition Loss -2.8121938705444336, Classifier Loss 0.08399534225463867, Total Loss 8.398971557617188\n",
      "23: Encoding Loss -1.4176963567733765, Transition Loss -11.469766616821289, Classifier Loss 0.09157665073871613, Total Loss 9.15537166595459\n",
      "23: Encoding Loss -1.4134559631347656, Transition Loss -14.438618659973145, Classifier Loss 0.09419353306293488, Total Loss 9.416465759277344\n",
      "23: Encoding Loss -1.712228775024414, Transition Loss -11.770360946655273, Classifier Loss 0.06222301349043846, Total Loss 6.219947338104248\n",
      "23: Encoding Loss -0.8885981440544128, Transition Loss -3.137216567993164, Classifier Loss 0.047286778688430786, Total Loss 4.728050231933594\n",
      "23: Encoding Loss -1.1686640977859497, Transition Loss -5.869912147521973, Classifier Loss 0.12283588200807571, Total Loss 12.282414436340332\n",
      "23: Encoding Loss -1.5621888637542725, Transition Loss -12.944742202758789, Classifier Loss 0.07975658774375916, Total Loss 7.97307014465332\n",
      "23: Encoding Loss -0.2307799905538559, Transition Loss 1.150120735168457, Classifier Loss 0.118878573179245, Total Loss 12.09848690032959\n",
      "23: Encoding Loss -0.8748338222503662, Transition Loss -6.360342979431152, Classifier Loss 0.07862885296344757, Total Loss 7.8616132736206055\n",
      "23: Encoding Loss -0.3426710069179535, Transition Loss -3.4673197269439697, Classifier Loss 0.09838050603866577, Total Loss 9.836520195007324\n",
      "23: Encoding Loss -1.1215362548828125, Transition Loss -10.587705612182617, Classifier Loss 0.06513988226652145, Total Loss 6.511870384216309\n",
      "23: Encoding Loss -1.4317561388015747, Transition Loss -13.945474624633789, Classifier Loss 0.09705615043640137, Total Loss 9.702825546264648\n",
      "23: Encoding Loss -1.021669626235962, Transition Loss -3.0814998149871826, Classifier Loss 0.10009455680847168, Total Loss 10.00883960723877\n",
      "23: Encoding Loss -1.2991318702697754, Transition Loss -7.036833763122559, Classifier Loss 0.04869728162884712, Total Loss 4.868320941925049\n",
      "23: Encoding Loss -1.7204080820083618, Transition Loss -7.472317218780518, Classifier Loss 0.0707230269908905, Total Loss 7.070808410644531\n",
      "23: Encoding Loss -0.6675903797149658, Transition Loss -12.919943809509277, Classifier Loss 0.08073175698518753, Total Loss 8.07059097290039\n",
      "23: Encoding Loss -0.9793885350227356, Transition Loss -12.310152053833008, Classifier Loss 0.08828015625476837, Total Loss 8.825552940368652\n",
      "23: Encoding Loss -1.1271787881851196, Transition Loss -0.36884844303131104, Classifier Loss 0.0508648157119751, Total Loss 5.086407661437988\n",
      "23: Encoding Loss -1.6892292499542236, Transition Loss -15.910634994506836, Classifier Loss 0.05727968364953995, Total Loss 5.724786281585693\n",
      "23: Encoding Loss -1.5102176666259766, Transition Loss -15.291410446166992, Classifier Loss 0.056697018444538116, Total Loss 5.6666436195373535\n",
      "23: Encoding Loss -0.3598923087120056, Transition Loss 0.009748220443725586, Classifier Loss 0.07848839461803436, Total Loss 7.8503289222717285\n",
      "23: Encoding Loss -0.5313518047332764, Transition Loss 3.8073978424072266, Classifier Loss 0.06852535903453827, Total Loss 7.614014625549316\n",
      "23: Encoding Loss -0.7851454019546509, Transition Loss -10.417693138122559, Classifier Loss 0.056126274168491364, Total Loss 5.610544204711914\n",
      "23: Encoding Loss -1.0388550758361816, Transition Loss -9.647263526916504, Classifier Loss 0.0607793815433979, Total Loss 6.0760087966918945\n",
      "23: Encoding Loss -1.8946185111999512, Transition Loss -11.58780288696289, Classifier Loss 0.07478582859039307, Total Loss 7.4762654304504395\n",
      "23: Encoding Loss -0.5125973224639893, Transition Loss -5.188574314117432, Classifier Loss 0.07157272100448608, Total Loss 7.156234264373779\n",
      "23: Encoding Loss -2.0344481468200684, Transition Loss -13.315893173217773, Classifier Loss 0.054380498826503754, Total Loss 5.435386657714844\n",
      "23: Encoding Loss -0.6679063439369202, Transition Loss -7.060524940490723, Classifier Loss 0.08804772794246674, Total Loss 8.803359985351562\n",
      "23: Encoding Loss -0.644597053527832, Transition Loss -10.28661823272705, Classifier Loss 0.11601152271032333, Total Loss 11.599095344543457\n",
      "23: Encoding Loss -2.2264513969421387, Transition Loss -17.921903610229492, Classifier Loss 0.0750885009765625, Total Loss 7.505265712738037\n",
      "23: Encoding Loss -1.5129653215408325, Transition Loss -6.497391223907471, Classifier Loss 0.08774679899215698, Total Loss 8.773380279541016\n",
      "23: Encoding Loss -1.266454815864563, Transition Loss -8.811494827270508, Classifier Loss 0.13180077075958252, Total Loss 13.178314208984375\n",
      "23: Encoding Loss -1.11501944065094, Transition Loss -14.471704483032227, Classifier Loss 0.09855908900499344, Total Loss 9.853014945983887\n",
      "23: Encoding Loss -1.9782907962799072, Transition Loss -10.626511573791504, Classifier Loss 0.036884818226099014, Total Loss 3.686356544494629\n",
      "23: Encoding Loss -0.6749117970466614, Transition Loss -10.272268295288086, Classifier Loss 0.07615654915571213, Total Loss 7.613600254058838\n",
      "23: Encoding Loss -1.5548052787780762, Transition Loss -3.5496866703033447, Classifier Loss 0.05877649039030075, Total Loss 5.876938819885254\n",
      "23: Encoding Loss -0.4143570363521576, Transition Loss -6.940875053405762, Classifier Loss 0.07778091728687286, Total Loss 7.776647090911865\n",
      "23: Encoding Loss -1.0957458019256592, Transition Loss -7.5882248878479, Classifier Loss 0.07079867273569107, Total Loss 7.078349590301514\n",
      "23: Encoding Loss -0.491262286901474, Transition Loss -3.1898927688598633, Classifier Loss 0.05798540636897087, Total Loss 5.797900676727295\n",
      "23: Encoding Loss -0.6208772659301758, Transition Loss -8.980942726135254, Classifier Loss 0.03805123269557953, Total Loss 3.8033270835876465\n",
      "23: Encoding Loss -1.162327527999878, Transition Loss -10.390802383422852, Classifier Loss 0.08858120441436768, Total Loss 8.85604190826416\n",
      "23: Encoding Loss -0.11853065341711044, Transition Loss -10.960489273071289, Classifier Loss 0.07938843965530396, Total Loss 7.824808120727539\n",
      "23: Encoding Loss -0.8507048487663269, Transition Loss 0.8466455936431885, Classifier Loss 0.1168181300163269, Total Loss 11.851141929626465\n",
      "23: Encoding Loss -0.6759026050567627, Transition Loss -6.5604963302612305, Classifier Loss 0.052108194679021835, Total Loss 5.209506988525391\n",
      "23: Encoding Loss 0.19298404455184937, Transition Loss -9.82372760772705, Classifier Loss 0.09590549767017365, Total Loss 11.091060638427734\n",
      "23: Encoding Loss -0.1708979308605194, Transition Loss -7.575352191925049, Classifier Loss 0.07830613851547241, Total Loss 7.769315719604492\n",
      "23: Encoding Loss -0.6985731720924377, Transition Loss -6.8931379318237305, Classifier Loss 0.06330737471580505, Total Loss 6.32935905456543\n",
      "23: Encoding Loss -1.5096697807312012, Transition Loss -3.260836124420166, Classifier Loss 0.1012783795595169, Total Loss 10.127185821533203\n",
      "23: Encoding Loss -1.745082974433899, Transition Loss -10.721444129943848, Classifier Loss 0.05136784911155701, Total Loss 5.134640693664551\n",
      "23: Encoding Loss 0.6677761077880859, Transition Loss -0.5868333578109741, Classifier Loss 0.07222014665603638, Total Loss 12.564105987548828\n",
      "23: Encoding Loss 0.41845881938934326, Transition Loss -7.723374843597412, Classifier Loss 0.05027447268366814, Total Loss 8.37352466583252\n",
      "23: Encoding Loss -0.17659585177898407, Transition Loss -4.880842685699463, Classifier Loss 0.08150902390480042, Total Loss 8.095250129699707\n",
      "23: Encoding Loss -0.07626000791788101, Transition Loss -13.20004653930664, Classifier Loss 0.14684510231018066, Total Loss 14.545913696289062\n",
      "23: Encoding Loss -0.17305196821689606, Transition Loss -13.043813705444336, Classifier Loss 0.10242587327957153, Total Loss 10.182153701782227\n",
      "23: Encoding Loss 0.37545400857925415, Transition Loss -17.766897201538086, Classifier Loss 0.05969684571027756, Total Loss 8.969502449035645\n",
      "23: Encoding Loss 0.8247789740562439, Transition Loss -11.891195297241211, Classifier Loss 0.10278932005167007, Total Loss 16.874784469604492\n",
      "23: Encoding Loss -0.9437592625617981, Transition Loss -10.349860191345215, Classifier Loss 0.07924432307481766, Total Loss 7.922362327575684\n",
      "23: Encoding Loss -1.8240344524383545, Transition Loss -16.717937469482422, Classifier Loss 0.07953908294439316, Total Loss 7.950564861297607\n",
      "23: Encoding Loss -1.36724054813385, Transition Loss -4.726445198059082, Classifier Loss 0.10467305779457092, Total Loss 10.466361045837402\n",
      "23: Encoding Loss -0.16722585260868073, Transition Loss -13.505422592163086, Classifier Loss 0.10105305165052414, Total Loss 10.039410591125488\n",
      "23: Encoding Loss -0.44962799549102783, Transition Loss -13.970331192016602, Classifier Loss 0.03079112060368061, Total Loss 3.076305627822876\n",
      "23: Encoding Loss -1.2622532844543457, Transition Loss -17.280580520629883, Classifier Loss 0.06868104636669159, Total Loss 6.864648342132568\n",
      "23: Encoding Loss -1.162445306777954, Transition Loss -22.490116119384766, Classifier Loss 0.03863395005464554, Total Loss 3.8588969707489014\n",
      "24: Encoding Loss -0.5158605575561523, Transition Loss -2.569094181060791, Classifier Loss 0.0449516624212265, Total Loss 4.494651794433594\n",
      "24: Encoding Loss -1.1595596075057983, Transition Loss -3.1217851638793945, Classifier Loss 0.12263789027929306, Total Loss 12.263164520263672\n",
      "24: Encoding Loss -0.7415268421173096, Transition Loss 1.560678482055664, Classifier Loss 0.050290197134017944, Total Loss 5.341155529022217\n",
      "24: Encoding Loss -0.9950315952301025, Transition Loss -16.03652572631836, Classifier Loss 0.08024607598781586, Total Loss 8.021400451660156\n",
      "24: Encoding Loss -0.6699674129486084, Transition Loss -6.475253582000732, Classifier Loss 0.05826864019036293, Total Loss 5.825569152832031\n",
      "24: Encoding Loss -0.6538359522819519, Transition Loss -13.066871643066406, Classifier Loss 0.0674474686384201, Total Loss 6.742133140563965\n",
      "24: Encoding Loss -1.0816075801849365, Transition Loss -9.29111385345459, Classifier Loss 0.08118095248937607, Total Loss 8.11623764038086\n",
      "24: Encoding Loss -0.41720518469810486, Transition Loss -6.683384418487549, Classifier Loss 0.06210792064666748, Total Loss 6.209404945373535\n",
      "24: Encoding Loss -0.8220853209495544, Transition Loss -13.09109115600586, Classifier Loss 0.05708160996437073, Total Loss 5.70554256439209\n",
      "24: Encoding Loss -1.5234298706054688, Transition Loss -10.270057678222656, Classifier Loss 0.08374001830816269, Total Loss 8.371947288513184\n",
      "24: Encoding Loss 0.582714855670929, Transition Loss -6.821250915527344, Classifier Loss 0.0678010880947113, Total Loss 11.440463066101074\n",
      "24: Encoding Loss -0.12236560881137848, Transition Loss -3.365311622619629, Classifier Loss 0.08038666844367981, Total Loss 7.929781913757324\n",
      "24: Encoding Loss -0.9806325435638428, Transition Loss -4.660637378692627, Classifier Loss 0.06223504990339279, Total Loss 6.2225728034973145\n",
      "24: Encoding Loss -0.3077467978000641, Transition Loss -5.084139823913574, Classifier Loss 0.07009129226207733, Total Loss 7.005542755126953\n",
      "24: Encoding Loss -1.5667623281478882, Transition Loss -12.692943572998047, Classifier Loss 0.060384489595890045, Total Loss 6.035910129547119\n",
      "24: Encoding Loss -2.170077085494995, Transition Loss -17.057680130004883, Classifier Loss 0.07304184883832932, Total Loss 7.3007731437683105\n",
      "24: Encoding Loss -1.0455262660980225, Transition Loss -8.091331481933594, Classifier Loss 0.03204101696610451, Total Loss 3.2024834156036377\n",
      "24: Encoding Loss -0.21835818886756897, Transition Loss -4.899424076080322, Classifier Loss 0.06250570714473724, Total Loss 6.224267482757568\n",
      "24: Encoding Loss 0.16683155298233032, Transition Loss -11.875199317932129, Classifier Loss 0.10457261651754379, Total Loss 11.725975036621094\n",
      "24: Encoding Loss -0.18118731677532196, Transition Loss -5.129705905914307, Classifier Loss 0.06616386771202087, Total Loss 6.564624309539795\n",
      "24: Encoding Loss -1.1850796937942505, Transition Loss -13.871079444885254, Classifier Loss 0.08945927023887634, Total Loss 8.94315242767334\n",
      "24: Encoding Loss -0.9003499746322632, Transition Loss -9.941008567810059, Classifier Loss 0.054959215223789215, Total Loss 5.493933200836182\n",
      "24: Encoding Loss -0.847795307636261, Transition Loss -11.228324890136719, Classifier Loss 0.08920249342918396, Total Loss 8.91800308227539\n",
      "24: Encoding Loss -0.9172322154045105, Transition Loss -3.1077888011932373, Classifier Loss 0.09503994882106781, Total Loss 9.503373146057129\n",
      "24: Encoding Loss -1.3588483333587646, Transition Loss -10.780073165893555, Classifier Loss 0.045920614153146744, Total Loss 4.589905738830566\n",
      "24: Encoding Loss -0.5370970368385315, Transition Loss -4.111766338348389, Classifier Loss 0.07642500847578049, Total Loss 7.641678333282471\n",
      "24: Encoding Loss -1.27078115940094, Transition Loss -2.466501235961914, Classifier Loss 0.11205942928791046, Total Loss 11.205450057983398\n",
      "24: Encoding Loss -2.1737165451049805, Transition Loss -17.59994888305664, Classifier Loss 0.08882544934749603, Total Loss 8.879024505615234\n",
      "24: Encoding Loss -1.898050308227539, Transition Loss -8.62391471862793, Classifier Loss 0.07386039942502975, Total Loss 7.384315013885498\n",
      "24: Encoding Loss -0.14836536347866058, Transition Loss -4.242593765258789, Classifier Loss 0.08062393218278885, Total Loss 7.979706287384033\n",
      "24: Encoding Loss -0.4843744933605194, Transition Loss -9.992177963256836, Classifier Loss 0.08964478969573975, Total Loss 8.962477684020996\n",
      "24: Encoding Loss -0.8279522061347961, Transition Loss -2.1876120567321777, Classifier Loss 0.10091163218021393, Total Loss 10.090725898742676\n",
      "24: Encoding Loss 0.0902719497680664, Transition Loss -9.786075592041016, Classifier Loss 0.08344476670026779, Total Loss 8.932293891906738\n",
      "24: Encoding Loss 0.18924100697040558, Transition Loss -6.382413864135742, Classifier Loss 0.18051190674304962, Total Loss 19.519609451293945\n",
      "24: Encoding Loss -1.6984485387802124, Transition Loss -12.36825180053711, Classifier Loss 0.05736104026436806, Total Loss 5.733630180358887\n",
      "24: Encoding Loss -1.1463260650634766, Transition Loss -7.35012674331665, Classifier Loss 0.04894031956791878, Total Loss 4.892561912536621\n",
      "24: Encoding Loss -0.6367393732070923, Transition Loss -7.294388771057129, Classifier Loss 0.08837418258190155, Total Loss 8.835959434509277\n",
      "24: Encoding Loss -0.7187459468841553, Transition Loss -9.37632942199707, Classifier Loss 0.12320412695407867, Total Loss 12.318537712097168\n",
      "24: Encoding Loss -1.1560231447219849, Transition Loss -9.327438354492188, Classifier Loss 0.08226241916418076, Total Loss 8.224376678466797\n",
      "24: Encoding Loss -1.0313379764556885, Transition Loss -10.5545654296875, Classifier Loss 0.0804603099822998, Total Loss 8.043920516967773\n",
      "24: Encoding Loss -1.3395392894744873, Transition Loss -7.595992088317871, Classifier Loss 0.047886788845062256, Total Loss 4.7871599197387695\n",
      "24: Encoding Loss -0.9861484169960022, Transition Loss -15.001019477844238, Classifier Loss 0.10161083191633224, Total Loss 10.158082962036133\n",
      "24: Encoding Loss -1.835795521736145, Transition Loss -14.597001075744629, Classifier Loss 0.04466831311583519, Total Loss 4.463912010192871\n",
      "24: Encoding Loss -1.7644615173339844, Transition Loss -4.595463752746582, Classifier Loss 0.07996167242527008, Total Loss 7.995248317718506\n",
      "24: Encoding Loss -1.2675503492355347, Transition Loss -15.7815580368042, Classifier Loss 0.06378639489412308, Total Loss 6.375483512878418\n",
      "24: Encoding Loss 0.5033079385757446, Transition Loss -6.56984281539917, Classifier Loss 0.0988532304763794, Total Loss 13.910470962524414\n",
      "24: Encoding Loss 0.0621308870613575, Transition Loss -15.929910659790039, Classifier Loss 0.0841805636882782, Total Loss 8.779107093811035\n",
      "24: Encoding Loss -1.5307940244674683, Transition Loss -5.312149524688721, Classifier Loss 0.044347960501909256, Total Loss 4.4337334632873535\n",
      "24: Encoding Loss -0.4001244008541107, Transition Loss -7.759552955627441, Classifier Loss 0.07363100349903107, Total Loss 7.361447811126709\n",
      "24: Encoding Loss -0.4117423892021179, Transition Loss -14.530211448669434, Classifier Loss 0.0701296329498291, Total Loss 7.0099945068359375\n",
      "24: Encoding Loss -1.050347924232483, Transition Loss -17.769739151000977, Classifier Loss 0.06016768515110016, Total Loss 6.013214588165283\n",
      "24: Encoding Loss -0.12112592905759811, Transition Loss -10.803247451782227, Classifier Loss 0.07870864868164062, Total Loss 7.759305477142334\n",
      "24: Encoding Loss 0.095180943608284, Transition Loss -1.9580955505371094, Classifier Loss 0.09403432160615921, Total Loss 10.034586906433105\n",
      "24: Encoding Loss -1.2677562236785889, Transition Loss -13.014615058898926, Classifier Loss 0.06321048736572266, Total Loss 6.318445682525635\n",
      "24: Encoding Loss -0.3188849687576294, Transition Loss -12.606574058532715, Classifier Loss 0.07619369775056839, Total Loss 7.615026473999023\n",
      "24: Encoding Loss -0.7859678268432617, Transition Loss -5.307880878448486, Classifier Loss 0.10125380754470825, Total Loss 10.124319076538086\n",
      "24: Encoding Loss 0.8837651610374451, Transition Loss -11.906195640563965, Classifier Loss 0.04672838747501373, Total Loss 11.740578651428223\n",
      "24: Encoding Loss -1.5378060340881348, Transition Loss -10.074065208435059, Classifier Loss 0.0720340684056282, Total Loss 7.20139217376709\n",
      "24: Encoding Loss -1.1931897401809692, Transition Loss -16.492996215820312, Classifier Loss 0.0573611855506897, Total Loss 5.732819557189941\n",
      "24: Encoding Loss -1.1796150207519531, Transition Loss -13.360801696777344, Classifier Loss 0.0789838582277298, Total Loss 7.895713806152344\n",
      "24: Encoding Loss -0.7411351799964905, Transition Loss -7.12631893157959, Classifier Loss 0.07360150665044785, Total Loss 7.358725547790527\n",
      "24: Encoding Loss -0.9604705572128296, Transition Loss -5.653730392456055, Classifier Loss 0.11836092174053192, Total Loss 11.8349609375\n",
      "24: Encoding Loss -0.9149376153945923, Transition Loss -7.665788173675537, Classifier Loss 0.07613561302423477, Total Loss 7.612028121948242\n",
      "24: Encoding Loss -0.7966135740280151, Transition Loss -5.199594497680664, Classifier Loss 0.08294739574193954, Total Loss 8.293700218200684\n",
      "24: Encoding Loss -0.7770817279815674, Transition Loss -9.391399383544922, Classifier Loss 0.07718397676944733, Total Loss 7.716519355773926\n",
      "24: Encoding Loss -1.090112328529358, Transition Loss -2.7807841300964355, Classifier Loss 0.12351411581039429, Total Loss 12.350855827331543\n",
      "24: Encoding Loss 0.5519095063209534, Transition Loss 0.4672805070877075, Classifier Loss 0.0691632628440857, Total Loss 11.425058364868164\n",
      "24: Encoding Loss -1.9227125644683838, Transition Loss -18.000215530395508, Classifier Loss 0.09764356911182404, Total Loss 9.760756492614746\n",
      "24: Encoding Loss -0.263346403837204, Transition Loss -8.386675834655762, Classifier Loss 0.07770165055990219, Total Loss 7.759584426879883\n",
      "24: Encoding Loss -0.5062093734741211, Transition Loss -9.429716110229492, Classifier Loss 0.12450584024190903, Total Loss 12.448697090148926\n",
      "24: Encoding Loss -1.6300482749938965, Transition Loss -9.033037185668945, Classifier Loss 0.09693199396133423, Total Loss 9.69139289855957\n",
      "24: Encoding Loss -1.3842157125473022, Transition Loss -9.224360466003418, Classifier Loss 0.08362193405628204, Total Loss 8.36034870147705\n",
      "24: Encoding Loss -0.716084897518158, Transition Loss -5.327592849731445, Classifier Loss 0.12072058022022247, Total Loss 12.070992469787598\n",
      "24: Encoding Loss -1.1620651483535767, Transition Loss -9.060956954956055, Classifier Loss 0.03315206989645958, Total Loss 3.313394784927368\n",
      "24: Encoding Loss -1.712896704673767, Transition Loss -7.683563232421875, Classifier Loss 0.12395581603050232, Total Loss 12.394044876098633\n",
      "24: Encoding Loss -1.40937340259552, Transition Loss -14.38078498840332, Classifier Loss 0.05690890550613403, Total Loss 5.688014030456543\n",
      "24: Encoding Loss -1.6313759088516235, Transition Loss -6.477278709411621, Classifier Loss 0.09638561308383942, Total Loss 9.637266159057617\n",
      "24: Encoding Loss -2.4780492782592773, Transition Loss -12.304244041442871, Classifier Loss 0.07581761479377747, Total Loss 7.579300403594971\n",
      "24: Encoding Loss -1.5107721090316772, Transition Loss -13.46274185180664, Classifier Loss 0.04360004886984825, Total Loss 4.357312202453613\n",
      "24: Encoding Loss -1.5248457193374634, Transition Loss -10.202392578125, Classifier Loss 0.11963552236557007, Total Loss 11.961511611938477\n",
      "24: Encoding Loss -0.810703694820404, Transition Loss -11.016733169555664, Classifier Loss 0.06994140148162842, Total Loss 6.991936683654785\n",
      "24: Encoding Loss -1.8415378332138062, Transition Loss -8.477216720581055, Classifier Loss 0.0670577809214592, Total Loss 6.704082489013672\n",
      "24: Encoding Loss -1.1049033403396606, Transition Loss -5.178128719329834, Classifier Loss 0.10215841978788376, Total Loss 10.21480655670166\n",
      "24: Encoding Loss -0.573769748210907, Transition Loss -10.660100936889648, Classifier Loss 0.11075443029403687, Total Loss 11.073310852050781\n",
      "24: Encoding Loss -2.6299667358398438, Transition Loss -7.89851713180542, Classifier Loss 0.06863459199666977, Total Loss 6.861879348754883\n",
      "24: Encoding Loss -1.5105689764022827, Transition Loss -5.241391181945801, Classifier Loss 0.07430529594421387, Total Loss 7.429481506347656\n",
      "24: Encoding Loss -1.7877781391143799, Transition Loss -15.909259796142578, Classifier Loss 0.08446067571640015, Total Loss 8.442886352539062\n",
      "24: Encoding Loss -1.5125113725662231, Transition Loss -9.556147575378418, Classifier Loss 0.11508534848690033, Total Loss 11.506623268127441\n",
      "24: Encoding Loss -0.6903694868087769, Transition Loss -7.6152753829956055, Classifier Loss 0.06464826315641403, Total Loss 6.463303089141846\n",
      "24: Encoding Loss -0.6686906218528748, Transition Loss -12.69662857055664, Classifier Loss 0.07395526766777039, Total Loss 7.3929877281188965\n",
      "24: Encoding Loss -3.262151002883911, Transition Loss -21.907875061035156, Classifier Loss 0.0721154734492302, Total Loss 7.207165718078613\n",
      "24: Encoding Loss -1.1730246543884277, Transition Loss -7.087362289428711, Classifier Loss 0.1048816591501236, Total Loss 10.486748695373535\n",
      "24: Encoding Loss -1.6069129705429077, Transition Loss -7.772799015045166, Classifier Loss 0.08772606402635574, Total Loss 8.771052360534668\n",
      "24: Encoding Loss -0.9014624953269958, Transition Loss -10.002685546875, Classifier Loss 0.08868764340877533, Total Loss 8.8667631149292\n",
      "24: Encoding Loss -0.9646303653717041, Transition Loss -3.8557040691375732, Classifier Loss 0.08170754462480545, Total Loss 8.16998291015625\n",
      "24: Encoding Loss -1.7336280345916748, Transition Loss -12.753410339355469, Classifier Loss 0.08786088973283768, Total Loss 8.783537864685059\n",
      "24: Encoding Loss -1.4567512273788452, Transition Loss -15.955428123474121, Classifier Loss 0.09133630245923996, Total Loss 9.130438804626465\n",
      "24: Encoding Loss -1.9138545989990234, Transition Loss -13.322831153869629, Classifier Loss 0.06232130527496338, Total Loss 6.229465961456299\n",
      "24: Encoding Loss -1.0869057178497314, Transition Loss -4.181787014007568, Classifier Loss 0.04712545871734619, Total Loss 4.711709499359131\n",
      "24: Encoding Loss -1.5631972551345825, Transition Loss -6.910612106323242, Classifier Loss 0.12576419115066528, Total Loss 12.575037002563477\n",
      "24: Encoding Loss -1.846913456916809, Transition Loss -14.040369033813477, Classifier Loss 0.07578150182962418, Total Loss 7.575342178344727\n",
      "24: Encoding Loss -0.43793201446533203, Transition Loss 0.39446842670440674, Classifier Loss 0.11744124442338943, Total Loss 11.822997093200684\n",
      "24: Encoding Loss -1.03382408618927, Transition Loss -7.391749858856201, Classifier Loss 0.07908456027507782, Total Loss 7.906977653503418\n",
      "24: Encoding Loss -0.5017169713973999, Transition Loss -4.345846176147461, Classifier Loss 0.0981655865907669, Total Loss 9.815689086914062\n",
      "24: Encoding Loss -1.3331326246261597, Transition Loss -11.708534240722656, Classifier Loss 0.06375211477279663, Total Loss 6.372869968414307\n",
      "24: Encoding Loss -1.4647598266601562, Transition Loss -15.379119873046875, Classifier Loss 0.0965428277850151, Total Loss 9.651206970214844\n",
      "24: Encoding Loss -1.2608882188796997, Transition Loss -4.013582229614258, Classifier Loss 0.09655963629484177, Total Loss 9.655160903930664\n",
      "24: Encoding Loss -1.6883809566497803, Transition Loss -8.320301055908203, Classifier Loss 0.04778788611292839, Total Loss 4.777124404907227\n",
      "24: Encoding Loss -1.8866081237792969, Transition Loss -8.5933837890625, Classifier Loss 0.06924791634082794, Total Loss 6.9230732917785645\n",
      "24: Encoding Loss -0.7155385613441467, Transition Loss -14.327873229980469, Classifier Loss 0.08031047880649567, Total Loss 8.028182029724121\n",
      "24: Encoding Loss -1.1976208686828613, Transition Loss -13.58593463897705, Classifier Loss 0.08666977286338806, Total Loss 8.664259910583496\n",
      "24: Encoding Loss -1.3096522092819214, Transition Loss -1.0143682956695557, Classifier Loss 0.052986130118370056, Total Loss 5.298410415649414\n",
      "24: Encoding Loss -1.980294108390808, Transition Loss -17.452068328857422, Classifier Loss 0.056781526654958725, Total Loss 5.674662113189697\n",
      "24: Encoding Loss -1.7401608228683472, Transition Loss -16.583515167236328, Classifier Loss 0.056712377816438675, Total Loss 5.66792106628418\n",
      "24: Encoding Loss -0.4842984080314636, Transition Loss -0.7172805070877075, Classifier Loss 0.07755811512470245, Total Loss 7.7556657791137695\n",
      "24: Encoding Loss -0.7750904560089111, Transition Loss 3.5543477535247803, Classifier Loss 0.06928714364767075, Total Loss 7.639583587646484\n",
      "24: Encoding Loss -1.0309302806854248, Transition Loss -11.607036590576172, Classifier Loss 0.05612798407673836, Total Loss 5.610476970672607\n",
      "24: Encoding Loss -1.2765640020370483, Transition Loss -10.728797912597656, Classifier Loss 0.061917319893836975, Total Loss 6.189586162567139\n",
      "24: Encoding Loss -2.20194411277771, Transition Loss -12.884292602539062, Classifier Loss 0.0721881315112114, Total Loss 7.216236114501953\n",
      "24: Encoding Loss -0.8257488012313843, Transition Loss -6.124968528747559, Classifier Loss 0.07312837988138199, Total Loss 7.311613082885742\n",
      "24: Encoding Loss -2.345799684524536, Transition Loss -14.570104598999023, Classifier Loss 0.05612616240978241, Total Loss 5.609702110290527\n",
      "24: Encoding Loss -1.2036089897155762, Transition Loss -8.242685317993164, Classifier Loss 0.08774404972791672, Total Loss 8.77275562286377\n",
      "24: Encoding Loss -0.9013463258743286, Transition Loss -11.26274299621582, Classifier Loss 0.11486136168241501, Total Loss 11.48388385772705\n",
      "24: Encoding Loss -2.490065336227417, Transition Loss -19.56007194519043, Classifier Loss 0.07151850312948227, Total Loss 7.147938251495361\n",
      "24: Encoding Loss -1.8296083211898804, Transition Loss -7.378603458404541, Classifier Loss 0.08620190620422363, Total Loss 8.618715286254883\n",
      "24: Encoding Loss -1.4723212718963623, Transition Loss -9.800521850585938, Classifier Loss 0.1297142505645752, Total Loss 12.969465255737305\n",
      "24: Encoding Loss -1.4070136547088623, Transition Loss -15.924617767333984, Classifier Loss 0.1015940010547638, Total Loss 10.156214714050293\n",
      "24: Encoding Loss -2.26442551612854, Transition Loss -11.85638427734375, Classifier Loss 0.037441469728946686, Total Loss 3.7417757511138916\n",
      "24: Encoding Loss -1.0564053058624268, Transition Loss -11.252084732055664, Classifier Loss 0.0733308494091034, Total Loss 7.330834865570068\n",
      "24: Encoding Loss -1.8809832334518433, Transition Loss -4.245770454406738, Classifier Loss 0.057553671300411224, Total Loss 5.754518032073975\n",
      "24: Encoding Loss -0.6842377185821533, Transition Loss -7.779341697692871, Classifier Loss 0.07866048067808151, Total Loss 7.864491939544678\n",
      "24: Encoding Loss -1.3348249197006226, Transition Loss -8.581839561462402, Classifier Loss 0.06948641687631607, Total Loss 6.946925640106201\n",
      "24: Encoding Loss -0.8072728514671326, Transition Loss -3.916271686553955, Classifier Loss 0.05556778237223625, Total Loss 5.555994987487793\n",
      "24: Encoding Loss -0.8975311517715454, Transition Loss -9.99398136138916, Classifier Loss 0.039505280554294586, Total Loss 3.9485292434692383\n",
      "24: Encoding Loss -1.4536052942276, Transition Loss -11.543571472167969, Classifier Loss 0.08442167937755585, Total Loss 8.439859390258789\n",
      "24: Encoding Loss -0.3496491611003876, Transition Loss -12.009894371032715, Classifier Loss 0.07799244672060013, Total Loss 7.7961835861206055\n",
      "24: Encoding Loss -1.1842180490493774, Transition Loss 0.4441959857940674, Classifier Loss 0.11602554470300674, Total Loss 11.691393852233887\n",
      "24: Encoding Loss -0.8976388573646545, Transition Loss -7.529811382293701, Classifier Loss 0.050424423068761826, Total Loss 5.040936470031738\n",
      "24: Encoding Loss 0.04660327360033989, Transition Loss -11.107078552246094, Classifier Loss 0.09436153620481491, Total Loss 9.68723201751709\n",
      "24: Encoding Loss -0.12526173889636993, Transition Loss -6.929813861846924, Classifier Loss 0.0749664232134819, Total Loss 7.389863014221191\n",
      "24: Encoding Loss -0.5033949017524719, Transition Loss -6.132753849029541, Classifier Loss 0.06280629336833954, Total Loss 6.279401779174805\n",
      "24: Encoding Loss -1.4750087261199951, Transition Loss -2.5433473587036133, Classifier Loss 0.09864016622304916, Total Loss 9.863508224487305\n",
      "24: Encoding Loss -1.6707340478897095, Transition Loss -9.938962936401367, Classifier Loss 0.051042862236499786, Total Loss 5.102298259735107\n",
      "24: Encoding Loss 0.7511218786239624, Transition Loss 0.03249621391296387, Classifier Loss 0.07025612145662308, Total Loss 13.04108715057373\n",
      "24: Encoding Loss 0.2777932286262512, Transition Loss -7.648072719573975, Classifier Loss 0.04822976142168045, Total Loss 7.037713050842285\n",
      "24: Encoding Loss -0.5737589001655579, Transition Loss -5.675166606903076, Classifier Loss 0.08148825913667679, Total Loss 8.147690773010254\n",
      "24: Encoding Loss -0.5755977034568787, Transition Loss -14.560933113098145, Classifier Loss 0.13857465982437134, Total Loss 13.85455322265625\n",
      "24: Encoding Loss -1.0112260580062866, Transition Loss -14.21458911895752, Classifier Loss 0.10471445322036743, Total Loss 10.468602180480957\n",
      "24: Encoding Loss -1.5450464487075806, Transition Loss -19.685802459716797, Classifier Loss 0.058122411370277405, Total Loss 5.8083038330078125\n",
      "24: Encoding Loss 0.012853355146944523, Transition Loss -10.127899169921875, Classifier Loss 0.09985367953777313, Total Loss 10.040014266967773\n",
      "24: Encoding Loss -1.3409920930862427, Transition Loss -11.007863998413086, Classifier Loss 0.07775775343179703, Total Loss 7.773573875427246\n",
      "24: Encoding Loss -2.3746347427368164, Transition Loss -17.412900924682617, Classifier Loss 0.07567241787910461, Total Loss 7.563758850097656\n",
      "24: Encoding Loss -1.7967103719711304, Transition Loss -5.373204708099365, Classifier Loss 0.10429632663726807, Total Loss 10.428557395935059\n",
      "24: Encoding Loss -0.3453197777271271, Transition Loss -14.25449275970459, Classifier Loss 0.10200639814138412, Total Loss 10.19702434539795\n",
      "24: Encoding Loss -0.869510293006897, Transition Loss -14.886900901794434, Classifier Loss 0.034739334136247635, Total Loss 3.4709560871124268\n",
      "24: Encoding Loss -1.7933765649795532, Transition Loss -18.221033096313477, Classifier Loss 0.06802795827388763, Total Loss 6.79915189743042\n",
      "24: Encoding Loss -1.5282156467437744, Transition Loss -23.554834365844727, Classifier Loss 0.03657461702823639, Total Loss 3.6527507305145264\n",
      "25: Encoding Loss -0.9005612134933472, Transition Loss -3.409166097640991, Classifier Loss 0.04385397955775261, Total Loss 4.384716033935547\n",
      "25: Encoding Loss -1.6577224731445312, Transition Loss -3.8658032417297363, Classifier Loss 0.11952632665634155, Total Loss 11.951859474182129\n",
      "25: Encoding Loss -1.322036623954773, Transition Loss 0.8551845550537109, Classifier Loss 0.050569407641887665, Total Loss 5.227977275848389\n",
      "25: Encoding Loss -1.3982658386230469, Transition Loss -16.91128921508789, Classifier Loss 0.07844667881727219, Total Loss 7.841285705566406\n",
      "25: Encoding Loss -1.1115281581878662, Transition Loss -7.247251510620117, Classifier Loss 0.05806208401918411, Total Loss 5.8047590255737305\n",
      "25: Encoding Loss -1.1464818716049194, Transition Loss -13.91082763671875, Classifier Loss 0.06589999049901962, Total Loss 6.587216854095459\n",
      "25: Encoding Loss -1.721006989479065, Transition Loss -10.101068496704102, Classifier Loss 0.07905209809541702, Total Loss 7.903189659118652\n",
      "25: Encoding Loss -0.9189357161521912, Transition Loss -7.495906352996826, Classifier Loss 0.060411013662815094, Total Loss 6.039602279663086\n",
      "25: Encoding Loss -1.444442868232727, Transition Loss -13.865429878234863, Classifier Loss 0.05559884011745453, Total Loss 5.557110786437988\n",
      "25: Encoding Loss -2.099135398864746, Transition Loss -10.996286392211914, Classifier Loss 0.08351271599531174, Total Loss 8.349072456359863\n",
      "25: Encoding Loss 0.4215603172779083, Transition Loss -7.4372992515563965, Classifier Loss 0.06902384757995605, Total Loss 10.273338317871094\n",
      "25: Encoding Loss -0.10984889417886734, Transition Loss -3.4758872985839844, Classifier Loss 0.07725165039300919, Total Loss 7.6049580574035645\n",
      "25: Encoding Loss -1.1118725538253784, Transition Loss -4.762059211730957, Classifier Loss 0.06107440963387489, Total Loss 6.1064887046813965\n",
      "25: Encoding Loss -0.18623623251914978, Transition Loss -5.295192241668701, Classifier Loss 0.06850328296422958, Total Loss 6.802671432495117\n",
      "25: Encoding Loss -1.570793628692627, Transition Loss -12.623035430908203, Classifier Loss 0.05930387228727341, Total Loss 5.927862644195557\n",
      "25: Encoding Loss -2.2456297874450684, Transition Loss -16.94432258605957, Classifier Loss 0.0743221789598465, Total Loss 7.428829193115234\n",
      "25: Encoding Loss -1.1444432735443115, Transition Loss -8.179892539978027, Classifier Loss 0.03167646378278732, Total Loss 3.1660103797912598\n",
      "25: Encoding Loss -0.23590262234210968, Transition Loss -4.99228572845459, Classifier Loss 0.06064751744270325, Total Loss 6.046463489532471\n",
      "25: Encoding Loss 0.5162026286125183, Transition Loss -11.913930892944336, Classifier Loss 0.10300420224666595, Total Loss 14.427657127380371\n",
      "25: Encoding Loss 0.3426184058189392, Transition Loss -5.2563910484313965, Classifier Loss 0.0672764927148819, Total Loss 9.466707229614258\n",
      "25: Encoding Loss 0.3593193590641022, Transition Loss -14.93658447265625, Classifier Loss 0.08831434696912766, Total Loss 11.702532768249512\n",
      "25: Encoding Loss -0.39912986755371094, Transition Loss -10.296384811401367, Classifier Loss 0.0576530396938324, Total Loss 5.763139724731445\n",
      "25: Encoding Loss -0.34778672456741333, Transition Loss -11.482778549194336, Classifier Loss 0.09154803305864334, Total Loss 9.151803970336914\n",
      "25: Encoding Loss -0.44222360849380493, Transition Loss -3.2264978885650635, Classifier Loss 0.0936366468667984, Total Loss 9.363001823425293\n",
      "25: Encoding Loss -0.7903924584388733, Transition Loss -11.113253593444824, Classifier Loss 0.044939037412405014, Total Loss 4.491681098937988\n",
      "25: Encoding Loss -0.20806655287742615, Transition Loss -4.336629390716553, Classifier Loss 0.07214377075433731, Total Loss 7.182329177856445\n",
      "25: Encoding Loss -0.8442974090576172, Transition Loss -2.6400434970855713, Classifier Loss 0.1035134419798851, Total Loss 10.350815773010254\n",
      "25: Encoding Loss -1.1416609287261963, Transition Loss -17.973499298095703, Classifier Loss 0.08571197092533112, Total Loss 8.56760311126709\n",
      "25: Encoding Loss -1.2488631010055542, Transition Loss -8.742767333984375, Classifier Loss 0.07210221141576767, Total Loss 7.208472728729248\n",
      "25: Encoding Loss 0.3977767825126648, Transition Loss -4.393887996673584, Classifier Loss 0.0812157392501831, Total Loss 11.302800178527832\n",
      "25: Encoding Loss -0.9223383665084839, Transition Loss -11.685870170593262, Classifier Loss 0.0901433676481247, Total Loss 9.011999130249023\n",
      "25: Encoding Loss -0.8946536183357239, Transition Loss -3.7884342670440674, Classifier Loss 0.09802426397800446, Total Loss 9.801669120788574\n",
      "25: Encoding Loss -0.42278924584388733, Transition Loss -11.312921524047852, Classifier Loss 0.0821913406252861, Total Loss 8.216832160949707\n",
      "25: Encoding Loss -0.09088930487632751, Transition Loss -6.75525426864624, Classifier Loss 0.1783895045518875, Total Loss 17.705482482910156\n",
      "25: Encoding Loss -1.5994205474853516, Transition Loss -12.85405445098877, Classifier Loss 0.05291777104139328, Total Loss 5.289206504821777\n",
      "25: Encoding Loss -0.8831403255462646, Transition Loss -7.742915153503418, Classifier Loss 0.04452156648039818, Total Loss 4.450607776641846\n",
      "25: Encoding Loss -1.2500239610671997, Transition Loss -7.607051849365234, Classifier Loss 0.08995502442121506, Total Loss 8.99398136138916\n",
      "25: Encoding Loss -0.7559284567832947, Transition Loss -9.709088325500488, Classifier Loss 0.12646621465682983, Total Loss 12.64468002319336\n",
      "25: Encoding Loss -1.5422354936599731, Transition Loss -10.059911727905273, Classifier Loss 0.0811944454908371, Total Loss 8.117432594299316\n",
      "25: Encoding Loss -1.551127314567566, Transition Loss -11.409090995788574, Classifier Loss 0.08071105927228928, Total Loss 8.06882381439209\n",
      "25: Encoding Loss -1.3210010528564453, Transition Loss -8.145554542541504, Classifier Loss 0.04582148790359497, Total Loss 4.580519676208496\n",
      "25: Encoding Loss -1.8044781684875488, Transition Loss -16.005340576171875, Classifier Loss 0.10051926970481873, Total Loss 10.048725128173828\n",
      "25: Encoding Loss -1.9610193967819214, Transition Loss -15.612407684326172, Classifier Loss 0.04289524257183075, Total Loss 4.286401748657227\n",
      "25: Encoding Loss -1.5225956439971924, Transition Loss -5.329362869262695, Classifier Loss 0.07579074054956436, Total Loss 7.57800817489624\n",
      "25: Encoding Loss -1.5698294639587402, Transition Loss -16.66263771057129, Classifier Loss 0.0602472648024559, Total Loss 6.021393775939941\n",
      "25: Encoding Loss -0.05962580814957619, Transition Loss -7.400335311889648, Classifier Loss 0.09485994279384613, Total Loss 9.353097915649414\n",
      "25: Encoding Loss -0.013941497541964054, Transition Loss -16.440532684326172, Classifier Loss 0.08120112866163254, Total Loss 8.067241668701172\n",
      "25: Encoding Loss -1.7948646545410156, Transition Loss -5.970132827758789, Classifier Loss 0.043534740805625916, Total Loss 4.352280139923096\n",
      "25: Encoding Loss -1.5388216972351074, Transition Loss -8.674302101135254, Classifier Loss 0.07234472781419754, Total Loss 7.232738018035889\n",
      "25: Encoding Loss -1.274299144744873, Transition Loss -15.542187690734863, Classifier Loss 0.06530025601387024, Total Loss 6.526916980743408\n",
      "25: Encoding Loss -1.84536874294281, Transition Loss -18.743125915527344, Classifier Loss 0.060111094266176224, Total Loss 6.007360935211182\n",
      "25: Encoding Loss -0.5462874174118042, Transition Loss -11.667498588562012, Classifier Loss 0.07418572902679443, Total Loss 7.416239261627197\n",
      "25: Encoding Loss -1.3994642496109009, Transition Loss -2.9821722507476807, Classifier Loss 0.08985074609518051, Total Loss 8.984478950500488\n",
      "25: Encoding Loss -1.6269195079803467, Transition Loss -11.531292915344238, Classifier Loss 0.05979422107338905, Total Loss 5.977115631103516\n",
      "25: Encoding Loss -0.8684009313583374, Transition Loss -11.17752456665039, Classifier Loss 0.0775819718837738, Total Loss 7.755961894989014\n",
      "25: Encoding Loss -1.4938890933990479, Transition Loss -4.320621490478516, Classifier Loss 0.09102759510278702, Total Loss 9.101895332336426\n",
      "25: Encoding Loss -0.4472077190876007, Transition Loss -10.544135093688965, Classifier Loss 0.045860156416893005, Total Loss 4.583892822265625\n",
      "25: Encoding Loss -1.797320008277893, Transition Loss -12.099594116210938, Classifier Loss 0.06958433240652084, Total Loss 6.956013202667236\n",
      "25: Encoding Loss -1.9505863189697266, Transition Loss -18.838409423828125, Classifier Loss 0.059212010353803635, Total Loss 5.917433738708496\n",
      "25: Encoding Loss -1.672534704208374, Transition Loss -15.607685089111328, Classifier Loss 0.07807285338640213, Total Loss 7.804163932800293\n",
      "25: Encoding Loss -1.3225880861282349, Transition Loss -8.825275421142578, Classifier Loss 0.0726218968629837, Total Loss 7.260424613952637\n",
      "25: Encoding Loss -1.7108466625213623, Transition Loss -7.282735347747803, Classifier Loss 0.11560214310884476, Total Loss 11.558757781982422\n",
      "25: Encoding Loss -1.493572473526001, Transition Loss -9.419400215148926, Classifier Loss 0.07431734353303909, Total Loss 7.4298505783081055\n",
      "25: Encoding Loss -1.133695125579834, Transition Loss -6.71995210647583, Classifier Loss 0.08317513018846512, Total Loss 8.316169738769531\n",
      "25: Encoding Loss -1.4211962223052979, Transition Loss -11.285120964050293, Classifier Loss 0.07288791984319687, Total Loss 7.286535263061523\n",
      "25: Encoding Loss -1.7682684659957886, Transition Loss -4.36494779586792, Classifier Loss 0.12107093632221222, Total Loss 12.106221199035645\n",
      "25: Encoding Loss -0.29868417978286743, Transition Loss -0.9006550312042236, Classifier Loss 0.0703115463256836, Total Loss 7.02760648727417\n",
      "25: Encoding Loss -1.546862244606018, Transition Loss -16.96387481689453, Classifier Loss 0.09418438374996185, Total Loss 9.415044784545898\n",
      "25: Encoding Loss -0.2650439143180847, Transition Loss -7.483691215515137, Classifier Loss 0.07515878975391388, Total Loss 7.505859851837158\n",
      "25: Encoding Loss -0.5711387395858765, Transition Loss -8.108488082885742, Classifier Loss 0.1262648105621338, Total Loss 12.624859809875488\n",
      "25: Encoding Loss -1.2381937503814697, Transition Loss -7.774169921875, Classifier Loss 0.09368277341127396, Total Loss 9.36672306060791\n",
      "25: Encoding Loss -1.2155821323394775, Transition Loss -7.945303916931152, Classifier Loss 0.08380409330129623, Total Loss 8.378820419311523\n",
      "25: Encoding Loss -0.6643559336662292, Transition Loss -4.363786220550537, Classifier Loss 0.11474838852882385, Total Loss 11.473966598510742\n",
      "25: Encoding Loss -1.2605812549591064, Transition Loss -7.853557586669922, Classifier Loss 0.031566161662340164, Total Loss 3.155045509338379\n",
      "25: Encoding Loss -1.787475347518921, Transition Loss -6.714380741119385, Classifier Loss 0.1214742511510849, Total Loss 12.146081924438477\n",
      "25: Encoding Loss -1.3546675443649292, Transition Loss -13.232723236083984, Classifier Loss 0.052514996379613876, Total Loss 5.2488532066345215\n",
      "25: Encoding Loss -1.2288590669631958, Transition Loss -5.165543556213379, Classifier Loss 0.09318123757839203, Total Loss 9.31709098815918\n",
      "25: Encoding Loss -2.3909850120544434, Transition Loss -11.055655479431152, Classifier Loss 0.07678233087062836, Total Loss 7.676022052764893\n",
      "25: Encoding Loss -1.4916714429855347, Transition Loss -12.236534118652344, Classifier Loss 0.03802725300192833, Total Loss 3.8002779483795166\n",
      "25: Encoding Loss -1.1551849842071533, Transition Loss -9.06007194519043, Classifier Loss 0.11910832673311234, Total Loss 11.90902042388916\n",
      "25: Encoding Loss -0.6003261804580688, Transition Loss -9.980985641479492, Classifier Loss 0.06747716665267944, Total Loss 6.745720863342285\n",
      "25: Encoding Loss -1.2797099351882935, Transition Loss -7.572813034057617, Classifier Loss 0.06516457349061966, Total Loss 6.5149431228637695\n",
      "25: Encoding Loss -0.885572075843811, Transition Loss -4.149576187133789, Classifier Loss 0.10379843413829803, Total Loss 10.379014015197754\n",
      "25: Encoding Loss -0.19776780903339386, Transition Loss -9.869678497314453, Classifier Loss 0.10649295151233673, Total Loss 10.609376907348633\n",
      "25: Encoding Loss -2.3148674964904785, Transition Loss -6.4258131980896, Classifier Loss 0.06989924609661102, Total Loss 6.9886393547058105\n",
      "25: Encoding Loss -1.4818722009658813, Transition Loss -4.1340227127075195, Classifier Loss 0.07634942978620529, Total Loss 7.634116172790527\n",
      "25: Encoding Loss -1.6190603971481323, Transition Loss -14.433741569519043, Classifier Loss 0.07939126342535019, Total Loss 7.936239719390869\n",
      "25: Encoding Loss -1.3601009845733643, Transition Loss -8.224059104919434, Classifier Loss 0.11868312209844589, Total Loss 11.866666793823242\n",
      "25: Encoding Loss -1.0370755195617676, Transition Loss -6.506264686584473, Classifier Loss 0.05703461915254593, Total Loss 5.702160835266113\n",
      "25: Encoding Loss -0.4205705225467682, Transition Loss -11.794384956359863, Classifier Loss 0.06870268285274506, Total Loss 6.867865562438965\n",
      "25: Encoding Loss -2.604954242706299, Transition Loss -20.311180114746094, Classifier Loss 0.06896016746759415, Total Loss 6.89195442199707\n",
      "25: Encoding Loss -0.8899954557418823, Transition Loss -6.0426788330078125, Classifier Loss 0.09767182171344757, Total Loss 9.765974044799805\n",
      "25: Encoding Loss -1.2348922491073608, Transition Loss -6.461745262145996, Classifier Loss 0.08247227966785431, Total Loss 8.245935440063477\n",
      "25: Encoding Loss -0.6729015707969666, Transition Loss -8.580347061157227, Classifier Loss 0.08476464450359344, Total Loss 8.474748611450195\n",
      "25: Encoding Loss -0.6343280673027039, Transition Loss -2.7763047218322754, Classifier Loss 0.07978975772857666, Total Loss 7.978420734405518\n",
      "25: Encoding Loss -1.3840070962905884, Transition Loss -11.632659912109375, Classifier Loss 0.086545929312706, Total Loss 8.652265548706055\n",
      "25: Encoding Loss -1.3550755977630615, Transition Loss -14.666431427001953, Classifier Loss 0.09096434712409973, Total Loss 9.093501091003418\n",
      "25: Encoding Loss -1.8081510066986084, Transition Loss -11.856388092041016, Classifier Loss 0.06221257150173187, Total Loss 6.218885898590088\n",
      "25: Encoding Loss -1.0261561870574951, Transition Loss -3.2417354583740234, Classifier Loss 0.04366437345743179, Total Loss 4.36578893661499\n",
      "25: Encoding Loss -1.1570099592208862, Transition Loss -5.916420936584473, Classifier Loss 0.1196591854095459, Total Loss 11.96473503112793\n",
      "25: Encoding Loss -1.3672547340393066, Transition Loss -12.967676162719727, Classifier Loss 0.07393631339073181, Total Loss 7.391037940979004\n",
      "25: Encoding Loss -0.15968288481235504, Transition Loss 1.2193498611450195, Classifier Loss 0.11606024950742722, Total Loss 11.779439926147461\n",
      "25: Encoding Loss -0.7536603212356567, Transition Loss -6.458898067474365, Classifier Loss 0.07592683285474777, Total Loss 7.591391563415527\n",
      "25: Encoding Loss 0.11379321664571762, Transition Loss -3.4838829040527344, Classifier Loss 0.09631481021642685, Total Loss 10.424992561340332\n",
      "25: Encoding Loss -0.6785982847213745, Transition Loss -11.623347282409668, Classifier Loss 0.06472375988960266, Total Loss 6.470051288604736\n",
      "25: Encoding Loss -0.18936088681221008, Transition Loss -15.019800186157227, Classifier Loss 0.09348420798778534, Total Loss 9.301275253295898\n",
      "25: Encoding Loss -0.2162686139345169, Transition Loss -4.687221050262451, Classifier Loss 0.09502874314785004, Total Loss 9.475495338439941\n",
      "25: Encoding Loss 0.4194740653038025, Transition Loss -8.946879386901855, Classifier Loss 0.04964315891265869, Total Loss 8.318273544311523\n",
      "25: Encoding Loss -0.11801023036241531, Transition Loss -7.871823787689209, Classifier Loss 0.07490348070859909, Total Loss 7.376446723937988\n",
      "25: Encoding Loss 1.8766871690750122, Transition Loss -13.124305725097656, Classifier Loss 0.07908312976360321, Total Loss 22.919185638427734\n",
      "25: Encoding Loss -1.0504744052886963, Transition Loss -12.325414657592773, Classifier Loss 0.08838281780481339, Total Loss 8.835816383361816\n",
      "25: Encoding Loss -1.269642949104309, Transition Loss -1.4639275074005127, Classifier Loss 0.050954777747392654, Total Loss 5.095184803009033\n",
      "25: Encoding Loss -1.8352503776550293, Transition Loss -15.67082405090332, Classifier Loss 0.05619825795292854, Total Loss 5.616691589355469\n",
      "25: Encoding Loss -1.0611282587051392, Transition Loss -15.093061447143555, Classifier Loss 0.05376382917165756, Total Loss 5.373364448547363\n",
      "25: Encoding Loss -0.3110993504524231, Transition Loss -1.0198737382888794, Classifier Loss 0.07456646859645844, Total Loss 7.454122543334961\n",
      "25: Encoding Loss -0.3356555700302124, Transition Loss 2.3985462188720703, Classifier Loss 0.07103022187948227, Total Loss 7.581671714782715\n",
      "25: Encoding Loss -0.6469678282737732, Transition Loss -10.473919868469238, Classifier Loss 0.05492737144231796, Total Loss 5.490642547607422\n",
      "25: Encoding Loss -0.8586333394050598, Transition Loss -9.858572006225586, Classifier Loss 0.06192540377378464, Total Loss 6.190568447113037\n",
      "25: Encoding Loss -1.800514817237854, Transition Loss -11.73001480102539, Classifier Loss 0.07104615867137909, Total Loss 7.102269649505615\n",
      "25: Encoding Loss -0.8436763882637024, Transition Loss -5.770680904388428, Classifier Loss 0.0698106586933136, Total Loss 6.979911804199219\n",
      "25: Encoding Loss -1.976279616355896, Transition Loss -13.248937606811523, Classifier Loss 0.05287294089794159, Total Loss 5.28464412689209\n",
      "25: Encoding Loss -0.9836662411689758, Transition Loss -7.5223388671875, Classifier Loss 0.08830863982439041, Total Loss 8.82935905456543\n",
      "25: Encoding Loss -0.8437076210975647, Transition Loss -10.441638946533203, Classifier Loss 0.11051766574382782, Total Loss 11.049677848815918\n",
      "25: Encoding Loss -2.0352089405059814, Transition Loss -17.51799964904785, Classifier Loss 0.07479078322649002, Total Loss 7.475574493408203\n",
      "25: Encoding Loss -1.3462756872177124, Transition Loss -7.104476451873779, Classifier Loss 0.08599614351987839, Total Loss 8.598193168640137\n",
      "25: Encoding Loss -1.0828852653503418, Transition Loss -9.079853057861328, Classifier Loss 0.12843000888824463, Total Loss 12.841184616088867\n",
      "25: Encoding Loss -1.2736036777496338, Transition Loss -14.184356689453125, Classifier Loss 0.09523124247789383, Total Loss 9.52028751373291\n",
      "25: Encoding Loss -1.5137163400650024, Transition Loss -10.77590560913086, Classifier Loss 0.035102635622024536, Total Loss 3.508108377456665\n",
      "25: Encoding Loss -0.5867006182670593, Transition Loss -10.312789916992188, Classifier Loss 0.07538734376430511, Total Loss 7.536672115325928\n",
      "25: Encoding Loss -1.0233324766159058, Transition Loss -4.154295444488525, Classifier Loss 0.05865984410047531, Total Loss 5.865153789520264\n",
      "25: Encoding Loss -0.0898362472653389, Transition Loss -7.347565174102783, Classifier Loss 0.07653054594993591, Total Loss 7.518989562988281\n",
      "25: Encoding Loss -0.9149652719497681, Transition Loss -7.838434219360352, Classifier Loss 0.06873911619186401, Total Loss 6.872344017028809\n",
      "25: Encoding Loss -0.22285491228103638, Transition Loss -3.8413686752319336, Classifier Loss 0.05396568030118942, Total Loss 5.3727617263793945\n",
      "25: Encoding Loss -0.4145764112472534, Transition Loss -8.843957901000977, Classifier Loss 0.03831615298986435, Total Loss 3.8297903537750244\n",
      "25: Encoding Loss -1.2660002708435059, Transition Loss -10.154040336608887, Classifier Loss 0.08992369472980499, Total Loss 8.990339279174805\n",
      "25: Encoding Loss -0.008656708523631096, Transition Loss -10.460356712341309, Classifier Loss 0.07602746784687042, Total Loss 7.568417072296143\n",
      "25: Encoding Loss -0.640821635723114, Transition Loss -0.09987187385559082, Classifier Loss 0.11366444081068039, Total Loss 11.366423606872559\n",
      "25: Encoding Loss -0.4772915542125702, Transition Loss -7.36190128326416, Classifier Loss 0.05063149705529213, Total Loss 5.061674118041992\n",
      "25: Encoding Loss 0.1924930214881897, Transition Loss -10.706924438476562, Classifier Loss 0.09321024268865585, Total Loss 10.817065238952637\n",
      "25: Encoding Loss -0.0745883509516716, Transition Loss -7.321796417236328, Classifier Loss 0.07376459985971451, Total Loss 7.239024639129639\n",
      "25: Encoding Loss -0.3209388554096222, Transition Loss -6.818233966827393, Classifier Loss 0.06643746048212051, Total Loss 6.640674591064453\n",
      "25: Encoding Loss -1.2660876512527466, Transition Loss -2.8734641075134277, Classifier Loss 0.09775598347187042, Total Loss 9.775023460388184\n",
      "25: Encoding Loss -1.3875038623809814, Transition Loss -10.97157096862793, Classifier Loss 0.05101734399795532, Total Loss 5.099539756774902\n",
      "25: Encoding Loss 0.8595739006996155, Transition Loss 0.013568758964538574, Classifier Loss 0.06882622838020325, Total Loss 13.76192855834961\n",
      "25: Encoding Loss -0.4703192412853241, Transition Loss -7.129254341125488, Classifier Loss 0.04647693783044815, Total Loss 4.646263122558594\n",
      "25: Encoding Loss -0.19018974900245667, Transition Loss -6.349978446960449, Classifier Loss 0.09066357463598251, Total Loss 9.021583557128906\n",
      "25: Encoding Loss 0.43097418546676636, Transition Loss -15.841516494750977, Classifier Loss 0.16134515404701233, Total Loss 19.579113006591797\n",
      "25: Encoding Loss -0.7240929007530212, Transition Loss -13.68049144744873, Classifier Loss 0.09594983607530594, Total Loss 9.59224796295166\n",
      "25: Encoding Loss -1.3288344144821167, Transition Loss -18.939054489135742, Classifier Loss 0.05740269273519516, Total Loss 5.736481189727783\n",
      "25: Encoding Loss -0.27446117997169495, Transition Loss -9.402933120727539, Classifier Loss 0.10239876061677933, Total Loss 10.231344223022461\n",
      "25: Encoding Loss -0.09364034980535507, Transition Loss -10.306708335876465, Classifier Loss 0.07585115730762482, Total Loss 7.452308177947998\n",
      "25: Encoding Loss -0.9037413001060486, Transition Loss -16.442277908325195, Classifier Loss 0.07528872042894363, Total Loss 7.525583744049072\n",
      "25: Encoding Loss -0.7125505208969116, Transition Loss -4.271437168121338, Classifier Loss 0.09858947992324829, Total Loss 9.85809326171875\n",
      "25: Encoding Loss 0.8896987438201904, Transition Loss -13.059887886047363, Classifier Loss 0.09834150969982147, Total Loss 16.94913101196289\n",
      "25: Encoding Loss -1.1746195554733276, Transition Loss -16.902502059936523, Classifier Loss 0.03207952156662941, Total Loss 3.2045717239379883\n",
      "25: Encoding Loss -1.3462388515472412, Transition Loss -20.40103530883789, Classifier Loss 0.07049112021923065, Total Loss 7.045031547546387\n",
      "25: Encoding Loss -2.301497459411621, Transition Loss -26.276981353759766, Classifier Loss 0.036429643630981445, Total Loss 3.637708902359009\n",
      "26: Encoding Loss -0.8266407251358032, Transition Loss -4.901699542999268, Classifier Loss 0.04370782524347305, Total Loss 4.369801998138428\n",
      "26: Encoding Loss -0.9345945715904236, Transition Loss -5.141464710235596, Classifier Loss 0.11837504804134369, Total Loss 11.83647632598877\n",
      "26: Encoding Loss -0.9706381559371948, Transition Loss -0.3061959743499756, Classifier Loss 0.04922162741422653, Total Loss 4.9221014976501465\n",
      "26: Encoding Loss -1.1706615686416626, Transition Loss -18.698036193847656, Classifier Loss 0.07540510594844818, Total Loss 7.536770820617676\n",
      "26: Encoding Loss -1.2670005559921265, Transition Loss -8.464438438415527, Classifier Loss 0.05862981453537941, Total Loss 5.861288547515869\n",
      "26: Encoding Loss -1.2724508047103882, Transition Loss -15.768770217895508, Classifier Loss 0.06402386724948883, Total Loss 6.399232864379883\n",
      "26: Encoding Loss -1.2778663635253906, Transition Loss -11.634461402893066, Classifier Loss 0.07729329913854599, Total Loss 7.72700309753418\n",
      "26: Encoding Loss -1.4291622638702393, Transition Loss -9.11884880065918, Classifier Loss 0.06220536306500435, Total Loss 6.218712329864502\n",
      "26: Encoding Loss -0.8476545810699463, Transition Loss -15.580421447753906, Classifier Loss 0.05374839901924133, Total Loss 5.371723651885986\n",
      "26: Encoding Loss -1.660803198814392, Transition Loss -12.477621078491211, Classifier Loss 0.08078936487436295, Total Loss 8.076440811157227\n",
      "26: Encoding Loss 0.4714566469192505, Transition Loss -8.668501853942871, Classifier Loss 0.06725969910621643, Total Loss 10.49588394165039\n",
      "26: Encoding Loss -0.2839750349521637, Transition Loss -3.322599411010742, Classifier Loss 0.07833017408847809, Total Loss 7.827224254608154\n",
      "26: Encoding Loss -1.2501220703125, Transition Loss -4.680844306945801, Classifier Loss 0.05855565518140793, Total Loss 5.8546295166015625\n",
      "26: Encoding Loss -0.5100005865097046, Transition Loss -5.23069429397583, Classifier Loss 0.06669419258832932, Total Loss 6.66837215423584\n",
      "26: Encoding Loss -1.758050799369812, Transition Loss -12.444778442382812, Classifier Loss 0.05750606209039688, Total Loss 5.748116970062256\n",
      "26: Encoding Loss -2.2829675674438477, Transition Loss -16.6566219329834, Classifier Loss 0.07250559329986572, Total Loss 7.247228145599365\n",
      "26: Encoding Loss -1.1155991554260254, Transition Loss -8.170694351196289, Classifier Loss 0.029686257243156433, Total Loss 2.966991662979126\n",
      "26: Encoding Loss -0.23006559908390045, Transition Loss -4.950301647186279, Classifier Loss 0.0588420033454895, Total Loss 5.863506317138672\n",
      "26: Encoding Loss -0.06294593960046768, Transition Loss -11.7365140914917, Classifier Loss 0.10080882906913757, Total Loss 9.945330619812012\n",
      "26: Encoding Loss -0.5618969202041626, Transition Loss -4.607285022735596, Classifier Loss 0.06282973289489746, Total Loss 6.282052040100098\n",
      "26: Encoding Loss -0.9186517596244812, Transition Loss -13.564423561096191, Classifier Loss 0.09032195806503296, Total Loss 9.0294828414917\n",
      "26: Encoding Loss -1.4274076223373413, Transition Loss -9.71552562713623, Classifier Loss 0.04807528108358383, Total Loss 4.805584907531738\n",
      "26: Encoding Loss -1.5219372510910034, Transition Loss -10.79622745513916, Classifier Loss 0.08392883092164993, Total Loss 8.390724182128906\n",
      "26: Encoding Loss -0.8814132809638977, Transition Loss -2.853875160217285, Classifier Loss 0.0884871631860733, Total Loss 8.848145484924316\n",
      "26: Encoding Loss -1.7738920450210571, Transition Loss -10.609986305236816, Classifier Loss 0.04313040152192116, Total Loss 4.31091833114624\n",
      "26: Encoding Loss -0.7630777955055237, Transition Loss -4.110998153686523, Classifier Loss 0.07615558803081512, Total Loss 7.614736557006836\n",
      "26: Encoding Loss -1.4850925207138062, Transition Loss -2.4953861236572266, Classifier Loss 0.10682431608438492, Total Loss 10.68193244934082\n",
      "26: Encoding Loss -2.2796692848205566, Transition Loss -17.3580322265625, Classifier Loss 0.085262730717659, Total Loss 8.522801399230957\n",
      "26: Encoding Loss -1.8824079036712646, Transition Loss -8.483150482177734, Classifier Loss 0.07243369519710541, Total Loss 7.241672992706299\n",
      "26: Encoding Loss -0.7045078873634338, Transition Loss -4.233302116394043, Classifier Loss 0.07731429487466812, Total Loss 7.7305827140808105\n",
      "26: Encoding Loss -0.8293175101280212, Transition Loss -10.22553539276123, Classifier Loss 0.08743719011545181, Total Loss 8.741674423217773\n",
      "26: Encoding Loss -0.8238299489021301, Transition Loss -2.3490517139434814, Classifier Loss 0.09742748737335205, Total Loss 9.742278099060059\n",
      "26: Encoding Loss -0.3858775794506073, Transition Loss -10.0, Classifier Loss 0.0802796334028244, Total Loss 8.025788307189941\n",
      "26: Encoding Loss 0.2211126983165741, Transition Loss -5.038661479949951, Classifier Loss 0.1815747320652008, Total Loss 19.90146255493164\n",
      "26: Encoding Loss -1.9486132860183716, Transition Loss -13.889664649963379, Classifier Loss 0.057956863194704056, Total Loss 5.792908191680908\n",
      "26: Encoding Loss -1.0421826839447021, Transition Loss -8.799018859863281, Classifier Loss 0.045879002660512924, Total Loss 4.586140155792236\n",
      "26: Encoding Loss -1.1153961420059204, Transition Loss -8.579527854919434, Classifier Loss 0.0901128500699997, Total Loss 9.00956916809082\n",
      "26: Encoding Loss -0.8776033520698547, Transition Loss -10.751175880432129, Classifier Loss 0.11849576234817505, Total Loss 11.84742546081543\n",
      "26: Encoding Loss -1.5469216108322144, Transition Loss -10.98491096496582, Classifier Loss 0.07702280580997467, Total Loss 7.7000837326049805\n",
      "26: Encoding Loss -1.4830029010772705, Transition Loss -12.077198028564453, Classifier Loss 0.07693402469158173, Total Loss 7.690986633300781\n",
      "26: Encoding Loss -1.4739221334457397, Transition Loss -8.947224617004395, Classifier Loss 0.044491592794656754, Total Loss 4.447369575500488\n",
      "26: Encoding Loss -1.4666742086410522, Transition Loss -16.943904876708984, Classifier Loss 0.09569713473320007, Total Loss 9.566325187683105\n",
      "26: Encoding Loss -1.9227023124694824, Transition Loss -16.368940353393555, Classifier Loss 0.04121584817767143, Total Loss 4.118310928344727\n",
      "26: Encoding Loss -1.7275999784469604, Transition Loss -5.729968070983887, Classifier Loss 0.07894591242074966, Total Loss 7.8934454917907715\n",
      "26: Encoding Loss -1.462825894355774, Transition Loss -17.54208755493164, Classifier Loss 0.057916224002838135, Total Loss 5.788113594055176\n",
      "26: Encoding Loss 0.014274334535002708, Transition Loss -7.873626708984375, Classifier Loss 0.09408458322286606, Total Loss 9.470462799072266\n",
      "26: Encoding Loss -0.14826582372188568, Transition Loss -15.947224617004395, Classifier Loss 0.07981763780117035, Total Loss 7.896633625030518\n",
      "26: Encoding Loss -1.6830543279647827, Transition Loss -5.507617473602295, Classifier Loss 0.04172229766845703, Total Loss 4.171128273010254\n",
      "26: Encoding Loss -1.8216179609298706, Transition Loss -8.231821060180664, Classifier Loss 0.06943781673908234, Total Loss 6.942135334014893\n",
      "26: Encoding Loss -1.4410616159439087, Transition Loss -15.08205795288086, Classifier Loss 0.06305822730064392, Total Loss 6.302806377410889\n",
      "26: Encoding Loss -1.9272443056106567, Transition Loss -18.445331573486328, Classifier Loss 0.0567840039730072, Total Loss 5.674711227416992\n",
      "26: Encoding Loss -0.49590280652046204, Transition Loss -11.282393455505371, Classifier Loss 0.07355643808841705, Total Loss 7.353385925292969\n",
      "26: Encoding Loss -1.6894782781600952, Transition Loss -2.615774154663086, Classifier Loss 0.08807840943336487, Total Loss 8.807317733764648\n",
      "26: Encoding Loss -1.9269486665725708, Transition Loss -11.174725532531738, Classifier Loss 0.058328934013843536, Total Loss 5.830658435821533\n",
      "26: Encoding Loss -0.959891140460968, Transition Loss -10.975037574768066, Classifier Loss 0.07846692204475403, Total Loss 7.844497203826904\n",
      "26: Encoding Loss -1.4084376096725464, Transition Loss -4.114465713500977, Classifier Loss 0.09370958060026169, Total Loss 9.370135307312012\n",
      "26: Encoding Loss -0.5817576050758362, Transition Loss -10.253612518310547, Classifier Loss 0.043293438851833344, Total Loss 4.3272929191589355\n",
      "26: Encoding Loss -2.093989849090576, Transition Loss -11.794232368469238, Classifier Loss 0.06923645734786987, Total Loss 6.9212870597839355\n",
      "26: Encoding Loss -2.2428812980651855, Transition Loss -18.568111419677734, Classifier Loss 0.05749636888504028, Total Loss 5.745923042297363\n",
      "26: Encoding Loss -1.7460352182388306, Transition Loss -15.281960487365723, Classifier Loss 0.07508370280265808, Total Loss 7.505313873291016\n",
      "26: Encoding Loss -1.7748544216156006, Transition Loss -8.55688190460205, Classifier Loss 0.07098595052957535, Total Loss 7.096883773803711\n",
      "26: Encoding Loss -2.044663667678833, Transition Loss -7.012508392333984, Classifier Loss 0.11777699738740921, Total Loss 11.776296615600586\n",
      "26: Encoding Loss -1.6997162103652954, Transition Loss -8.993898391723633, Classifier Loss 0.07010018080472946, Total Loss 7.008219242095947\n",
      "26: Encoding Loss -1.2200284004211426, Transition Loss -6.5263519287109375, Classifier Loss 0.08420081436634064, Total Loss 8.41877555847168\n",
      "26: Encoding Loss -1.7480255365371704, Transition Loss -11.090607643127441, Classifier Loss 0.07131844013929367, Total Loss 7.1296257972717285\n",
      "26: Encoding Loss -1.9764491319656372, Transition Loss -4.120880126953125, Classifier Loss 0.11776623129844666, Total Loss 11.775798797607422\n",
      "26: Encoding Loss -0.36700376868247986, Transition Loss -0.8137115240097046, Classifier Loss 0.07112616300582886, Total Loss 7.11209774017334\n",
      "26: Encoding Loss -2.116513967514038, Transition Loss -16.797739028930664, Classifier Loss 0.08856934309005737, Total Loss 8.853574752807617\n",
      "26: Encoding Loss -0.170662984251976, Transition Loss -7.349610328674316, Classifier Loss 0.07343529164791107, Total Loss 7.282060146331787\n",
      "26: Encoding Loss -0.889013946056366, Transition Loss -7.769996166229248, Classifier Loss 0.12837325036525726, Total Loss 12.835771560668945\n",
      "26: Encoding Loss -1.487908959388733, Transition Loss -7.528505325317383, Classifier Loss 0.09011238813400269, Total Loss 9.009733200073242\n",
      "26: Encoding Loss -1.3360496759414673, Transition Loss -7.637333393096924, Classifier Loss 0.08396100997924805, Total Loss 8.394573211669922\n",
      "26: Encoding Loss -0.8211372494697571, Transition Loss -4.260305881500244, Classifier Loss 0.11012747138738632, Total Loss 11.011895179748535\n",
      "26: Encoding Loss -1.521535038948059, Transition Loss -7.500418663024902, Classifier Loss 0.031014520674943924, Total Loss 3.099951982498169\n",
      "26: Encoding Loss -1.7361950874328613, Transition Loss -6.541393280029297, Classifier Loss 0.11713497340679169, Total Loss 11.712188720703125\n",
      "26: Encoding Loss -1.4815086126327515, Transition Loss -12.866134643554688, Classifier Loss 0.05024661123752594, Total Loss 5.022088050842285\n",
      "26: Encoding Loss -1.3353532552719116, Transition Loss -5.016511917114258, Classifier Loss 0.09215456992387772, Total Loss 9.21445369720459\n",
      "26: Encoding Loss -2.663811683654785, Transition Loss -10.746124267578125, Classifier Loss 0.07026497274637222, Total Loss 7.024348258972168\n",
      "26: Encoding Loss -1.766701340675354, Transition Loss -11.90757942199707, Classifier Loss 0.03811844065785408, Total Loss 3.809462547302246\n",
      "26: Encoding Loss -1.3849807977676392, Transition Loss -8.66182804107666, Classifier Loss 0.11325854063034058, Total Loss 11.324121475219727\n",
      "26: Encoding Loss -0.7625450491905212, Transition Loss -9.650158882141113, Classifier Loss 0.0656491070985794, Total Loss 6.562980651855469\n",
      "26: Encoding Loss -1.578170657157898, Transition Loss -7.469668388366699, Classifier Loss 0.06193334236741066, Total Loss 6.191840171813965\n",
      "26: Encoding Loss -1.157538652420044, Transition Loss -3.9023847579956055, Classifier Loss 0.10288430750370026, Total Loss 10.287651062011719\n",
      "26: Encoding Loss -0.3218923509120941, Transition Loss -9.714458465576172, Classifier Loss 0.10035427659749985, Total Loss 10.031828880310059\n",
      "26: Encoding Loss -2.4509613513946533, Transition Loss -6.489040374755859, Classifier Loss 0.06680162996053696, Total Loss 6.6788649559021\n",
      "26: Encoding Loss -1.2864012718200684, Transition Loss -4.269168853759766, Classifier Loss 0.07331005483865738, Total Loss 7.330151557922363\n",
      "26: Encoding Loss -1.8252371549606323, Transition Loss -14.352446556091309, Classifier Loss 0.07412661612033844, Total Loss 7.409790992736816\n",
      "26: Encoding Loss -1.4050438404083252, Transition Loss -8.091712951660156, Classifier Loss 0.117548368871212, Total Loss 11.753218650817871\n",
      "26: Encoding Loss -0.935642659664154, Transition Loss -6.515567302703857, Classifier Loss 0.055968835949897766, Total Loss 5.595580577850342\n",
      "26: Encoding Loss -0.7127956748008728, Transition Loss -11.722776412963867, Classifier Loss 0.0729614645242691, Total Loss 7.293801784515381\n",
      "26: Encoding Loss -3.2231976985931396, Transition Loss -20.139318466186523, Classifier Loss 0.07067455351352692, Total Loss 7.063427448272705\n",
      "26: Encoding Loss -1.1083720922470093, Transition Loss -6.069168567657471, Classifier Loss 0.09785590320825577, Total Loss 9.78437614440918\n",
      "26: Encoding Loss -1.80573308467865, Transition Loss -6.481247425079346, Classifier Loss 0.08465494215488434, Total Loss 8.464198112487793\n",
      "26: Encoding Loss -1.0143663883209229, Transition Loss -8.567047119140625, Classifier Loss 0.08463733643293381, Total Loss 8.462019920349121\n",
      "26: Encoding Loss -0.9649083614349365, Transition Loss -2.7788290977478027, Classifier Loss 0.08007165044546127, Total Loss 8.006608963012695\n",
      "26: Encoding Loss -1.5648845434188843, Transition Loss -11.419427871704102, Classifier Loss 0.08794967830181122, Total Loss 8.792683601379395\n",
      "26: Encoding Loss -1.4431438446044922, Transition Loss -14.521267890930176, Classifier Loss 0.09127910435199738, Total Loss 9.125006675720215\n",
      "26: Encoding Loss -2.0585150718688965, Transition Loss -11.608331680297852, Classifier Loss 0.060297008603811264, Total Loss 6.027379035949707\n",
      "26: Encoding Loss -1.0883429050445557, Transition Loss -3.19714093208313, Classifier Loss 0.043874915689229965, Total Loss 4.386852264404297\n",
      "26: Encoding Loss -1.5831630229949951, Transition Loss -5.828623294830322, Classifier Loss 0.12251000851392746, Total Loss 12.249835014343262\n",
      "26: Encoding Loss -1.8579254150390625, Transition Loss -12.525607109069824, Classifier Loss 0.07381689548492432, Total Loss 7.379184246063232\n",
      "26: Encoding Loss -0.5924022793769836, Transition Loss 1.319528341293335, Classifier Loss 0.11528949439525604, Total Loss 11.792855262756348\n",
      "26: Encoding Loss -0.8807418942451477, Transition Loss -6.527359962463379, Classifier Loss 0.07663917541503906, Total Loss 7.662611961364746\n",
      "26: Encoding Loss -0.621943473815918, Transition Loss -3.5132594108581543, Classifier Loss 0.09390310943126678, Total Loss 9.389608383178711\n",
      "26: Encoding Loss -1.3991265296936035, Transition Loss -10.467717170715332, Classifier Loss 0.05980919301509857, Total Loss 5.97882604598999\n",
      "26: Encoding Loss -1.6918609142303467, Transition Loss -13.978591918945312, Classifier Loss 0.09219537675380707, Total Loss 9.216741561889648\n",
      "26: Encoding Loss -1.1746245622634888, Transition Loss -3.3553237915039062, Classifier Loss 0.0900755226612091, Total Loss 9.006880760192871\n",
      "26: Encoding Loss -1.5710468292236328, Transition Loss -7.241366863250732, Classifier Loss 0.04716162011027336, Total Loss 4.714714050292969\n",
      "26: Encoding Loss -1.99036705493927, Transition Loss -7.502458572387695, Classifier Loss 0.06556856632232666, Total Loss 6.555356025695801\n",
      "26: Encoding Loss -0.9663087129592896, Transition Loss -12.804736137390137, Classifier Loss 0.07440038025379181, Total Loss 7.437477111816406\n",
      "26: Encoding Loss -1.2167279720306396, Transition Loss -12.450197219848633, Classifier Loss 0.0823691263794899, Total Loss 8.23442268371582\n",
      "26: Encoding Loss -1.2726901769638062, Transition Loss -0.4020686149597168, Classifier Loss 0.04860340431332588, Total Loss 4.860260009765625\n",
      "26: Encoding Loss -1.978555679321289, Transition Loss -15.983993530273438, Classifier Loss 0.052160535007715225, Total Loss 5.212856769561768\n",
      "26: Encoding Loss -1.6883338689804077, Transition Loss -15.181131362915039, Classifier Loss 0.050312791019678116, Total Loss 5.028243064880371\n",
      "26: Encoding Loss -0.4029492139816284, Transition Loss -0.09610870480537415, Classifier Loss 0.07369958609342575, Total Loss 7.36984920501709\n",
      "26: Encoding Loss -0.5960785746574402, Transition Loss 3.6350958347320557, Classifier Loss 0.06810209155082703, Total Loss 7.537228584289551\n",
      "26: Encoding Loss -0.9834049344062805, Transition Loss -10.296168327331543, Classifier Loss 0.05299428850412369, Total Loss 5.297369480133057\n",
      "26: Encoding Loss -1.2272075414657593, Transition Loss -9.487448692321777, Classifier Loss 0.05906076729297638, Total Loss 5.904179573059082\n",
      "26: Encoding Loss -2.3271167278289795, Transition Loss -11.427762985229492, Classifier Loss 0.06995553523302078, Total Loss 6.993268013000488\n",
      "26: Encoding Loss -0.8881815671920776, Transition Loss -5.150179862976074, Classifier Loss 0.06894238293170929, Total Loss 6.8932085037231445\n",
      "26: Encoding Loss -2.366340160369873, Transition Loss -13.132007598876953, Classifier Loss 0.05257667228579521, Total Loss 5.255040645599365\n",
      "26: Encoding Loss -0.9300442337989807, Transition Loss -7.223955154418945, Classifier Loss 0.08439820259809494, Total Loss 8.438375473022461\n",
      "26: Encoding Loss -0.8599673509597778, Transition Loss -10.384037971496582, Classifier Loss 0.11246003210544586, Total Loss 11.243926048278809\n",
      "26: Encoding Loss -2.541694164276123, Transition Loss -17.950428009033203, Classifier Loss 0.0673280730843544, Total Loss 6.729217052459717\n",
      "26: Encoding Loss -1.7680577039718628, Transition Loss -6.524059295654297, Classifier Loss 0.08334019035100937, Total Loss 8.332714080810547\n",
      "26: Encoding Loss -1.6655826568603516, Transition Loss -8.610807418823242, Classifier Loss 0.12486322969198227, Total Loss 12.484601020812988\n",
      "26: Encoding Loss -1.576254963874817, Transition Loss -14.347869873046875, Classifier Loss 0.0977703109383583, Total Loss 9.774161338806152\n",
      "26: Encoding Loss -2.1013662815093994, Transition Loss -10.660698890686035, Classifier Loss 0.03432277962565422, Total Loss 3.430145740509033\n",
      "26: Encoding Loss -1.0429489612579346, Transition Loss -9.986282348632812, Classifier Loss 0.0715189203619957, Total Loss 7.149894714355469\n",
      "26: Encoding Loss -1.7174559831619263, Transition Loss -3.4697964191436768, Classifier Loss 0.056268639862537384, Total Loss 5.6261701583862305\n",
      "26: Encoding Loss -0.5606285333633423, Transition Loss -6.7610697746276855, Classifier Loss 0.07543954998254776, Total Loss 7.5426025390625\n",
      "26: Encoding Loss -1.2109482288360596, Transition Loss -7.529123306274414, Classifier Loss 0.0666065439581871, Total Loss 6.659148693084717\n",
      "26: Encoding Loss -0.9548075795173645, Transition Loss -3.094540596008301, Classifier Loss 0.05000937357544899, Total Loss 5.00031852722168\n",
      "26: Encoding Loss -0.8731393814086914, Transition Loss -8.756080627441406, Classifier Loss 0.03765163570642471, Total Loss 3.7634124755859375\n",
      "26: Encoding Loss -1.4023445844650269, Transition Loss -10.40632438659668, Classifier Loss 0.08259542286396027, Total Loss 8.257461547851562\n",
      "26: Encoding Loss -0.369544118642807, Transition Loss -10.755112648010254, Classifier Loss 0.07412496209144592, Total Loss 7.410020351409912\n",
      "26: Encoding Loss -1.0723692178726196, Transition Loss 1.0740747451782227, Classifier Loss 0.11031197756528854, Total Loss 11.246012687683105\n",
      "26: Encoding Loss -0.8025857210159302, Transition Loss -6.677127838134766, Classifier Loss 0.04912959039211273, Total Loss 4.911623477935791\n",
      "26: Encoding Loss -0.15551872551441193, Transition Loss -9.70699691772461, Classifier Loss 0.08992380648851395, Total Loss 8.915851593017578\n",
      "26: Encoding Loss -0.4589115083217621, Transition Loss -7.88767671585083, Classifier Loss 0.07210361957550049, Total Loss 7.208776473999023\n",
      "26: Encoding Loss -1.1520904302597046, Transition Loss -7.367184638977051, Classifier Loss 0.06334225833415985, Total Loss 6.332752227783203\n",
      "26: Encoding Loss -1.7228063344955444, Transition Loss -3.378678560256958, Classifier Loss 0.09445938467979431, Total Loss 9.44526195526123\n",
      "26: Encoding Loss -1.8710744380950928, Transition Loss -11.888792037963867, Classifier Loss 0.045711904764175415, Total Loss 4.568812370300293\n",
      "26: Encoding Loss -0.018763558939099312, Transition Loss -0.39529716968536377, Classifier Loss 0.0658782422542572, Total Loss 6.523861885070801\n",
      "26: Encoding Loss -0.43743467330932617, Transition Loss -6.342005729675293, Classifier Loss 0.04395506903529167, Total Loss 4.394217014312744\n",
      "26: Encoding Loss 0.002175279427319765, Transition Loss -5.5582685470581055, Classifier Loss 0.09201102703809738, Total Loss 9.208842277526855\n",
      "26: Encoding Loss -0.786669909954071, Transition Loss -15.1971435546875, Classifier Loss 0.14142829179763794, Total Loss 14.139789581298828\n",
      "26: Encoding Loss -1.7803372144699097, Transition Loss -15.019404411315918, Classifier Loss 0.09600413590669632, Total Loss 9.59740924835205\n",
      "26: Encoding Loss -2.450805425643921, Transition Loss -20.56886100769043, Classifier Loss 0.0550345852971077, Total Loss 5.499344825744629\n",
      "26: Encoding Loss -0.48499903082847595, Transition Loss -10.5255126953125, Classifier Loss 0.1022588387131691, Total Loss 10.223775863647461\n",
      "26: Encoding Loss -1.1204886436462402, Transition Loss -11.55196475982666, Classifier Loss 0.07309325784444809, Total Loss 7.307015419006348\n",
      "26: Encoding Loss -2.3501381874084473, Transition Loss -17.923229217529297, Classifier Loss 0.07216662168502808, Total Loss 7.213077545166016\n",
      "26: Encoding Loss -1.7014483213424683, Transition Loss -5.567020416259766, Classifier Loss 0.09744936972856522, Total Loss 9.743824005126953\n",
      "26: Encoding Loss -0.11617549508810043, Transition Loss -14.575481414794922, Classifier Loss 0.09586058557033539, Total Loss 9.469135284423828\n",
      "26: Encoding Loss -0.0406733937561512, Transition Loss -14.807679176330566, Classifier Loss 0.033838432282209396, Total Loss 3.269566059112549\n",
      "26: Encoding Loss -1.8430763483047485, Transition Loss -18.79244041442871, Classifier Loss 0.06673664599657059, Total Loss 6.669906139373779\n",
      "26: Encoding Loss -1.8331644535064697, Transition Loss -24.520517349243164, Classifier Loss 0.033587586134672165, Total Loss 3.3538546562194824\n",
      "27: Encoding Loss -1.1662983894348145, Transition Loss -3.738600730895996, Classifier Loss 0.040800802409648895, Total Loss 4.07933235168457\n",
      "27: Encoding Loss -1.772576928138733, Transition Loss -4.243917465209961, Classifier Loss 0.11687789112329483, Total Loss 11.68694019317627\n",
      "27: Encoding Loss -1.4434407949447632, Transition Loss 0.7549189329147339, Classifier Loss 0.04742518067359924, Total Loss 4.8935017585754395\n",
      "27: Encoding Loss -1.4593724012374878, Transition Loss -17.569339752197266, Classifier Loss 0.07332664728164673, Total Loss 7.329150676727295\n",
      "27: Encoding Loss -1.1938310861587524, Transition Loss -7.595407485961914, Classifier Loss 0.056424710899591446, Total Loss 5.640952110290527\n",
      "27: Encoding Loss -1.411867618560791, Transition Loss -14.49920654296875, Classifier Loss 0.06181282550096512, Total Loss 6.178382873535156\n",
      "27: Encoding Loss -1.882190465927124, Transition Loss -10.423456192016602, Classifier Loss 0.07466219365596771, Total Loss 7.464134693145752\n",
      "27: Encoding Loss -1.3051937818527222, Transition Loss -7.9168806076049805, Classifier Loss 0.05767039954662323, Total Loss 5.765456199645996\n",
      "27: Encoding Loss -1.293089509010315, Transition Loss -14.447661399841309, Classifier Loss 0.05128031224012375, Total Loss 5.125141620635986\n",
      "27: Encoding Loss -2.1023895740509033, Transition Loss -11.470934867858887, Classifier Loss 0.07793708145618439, Total Loss 7.791414260864258\n",
      "27: Encoding Loss 0.17166399955749512, Transition Loss -7.660486221313477, Classifier Loss 0.06467273831367493, Total Loss 7.779970169067383\n",
      "27: Encoding Loss 1.1709485054016113, Transition Loss -2.8334033489227295, Classifier Loss 0.07602258026599884, Total Loss 16.969280242919922\n",
      "27: Encoding Loss -1.1053142547607422, Transition Loss -7.600552558898926, Classifier Loss 0.05814434215426445, Total Loss 5.81291389465332\n",
      "27: Encoding Loss -0.521255373954773, Transition Loss -7.707064628601074, Classifier Loss 0.07449811697006226, Total Loss 7.448269844055176\n",
      "27: Encoding Loss -1.9129021167755127, Transition Loss -16.16855812072754, Classifier Loss 0.05724344402551651, Total Loss 5.7211103439331055\n",
      "27: Encoding Loss -1.7408175468444824, Transition Loss -20.795799255371094, Classifier Loss 0.07057593017816544, Total Loss 7.053433895111084\n",
      "27: Encoding Loss -0.19764705002307892, Transition Loss -11.069520950317383, Classifier Loss 0.03228560835123062, Total Loss 3.1883184909820557\n",
      "27: Encoding Loss 0.22287359833717346, Transition Loss -7.164414882659912, Classifier Loss 0.0611206591129303, Total Loss 7.870593070983887\n",
      "27: Encoding Loss 0.0603531114757061, Transition Loss -12.241178512573242, Classifier Loss 0.0963830053806305, Total Loss 9.986828804016113\n",
      "27: Encoding Loss 0.8117386102676392, Transition Loss -5.459878444671631, Classifier Loss 0.06151411309838295, Total Loss 12.6442289352417\n",
      "27: Encoding Loss 1.0390608310699463, Transition Loss -15.731155395507812, Classifier Loss 0.08907026052474976, Total Loss 17.216365814208984\n",
      "27: Encoding Loss -0.12627843022346497, Transition Loss -10.475396156311035, Classifier Loss 0.05169901251792908, Total Loss 5.063416004180908\n",
      "27: Encoding Loss 0.308737576007843, Transition Loss -11.369577407836914, Classifier Loss 0.09196431934833527, Total Loss 11.661565780639648\n",
      "27: Encoding Loss -1.0352095365524292, Transition Loss -4.379822731018066, Classifier Loss 0.0912206843495369, Total Loss 9.12119197845459\n",
      "27: Encoding Loss -1.7906501293182373, Transition Loss -12.168267250061035, Classifier Loss 0.03909219428896904, Total Loss 3.9067859649658203\n",
      "27: Encoding Loss -0.8244182467460632, Transition Loss -5.448603630065918, Classifier Loss 0.07269982993602753, Total Loss 7.268893241882324\n",
      "27: Encoding Loss -1.53122878074646, Transition Loss -4.033355712890625, Classifier Loss 0.1049347072839737, Total Loss 10.492664337158203\n",
      "27: Encoding Loss -2.4178128242492676, Transition Loss -19.139095306396484, Classifier Loss 0.08544398099184036, Total Loss 8.540570259094238\n",
      "27: Encoding Loss -1.8526017665863037, Transition Loss -9.992794036865234, Classifier Loss 0.07106184214353561, Total Loss 7.104185581207275\n",
      "27: Encoding Loss -0.8312221169471741, Transition Loss -5.767877578735352, Classifier Loss 0.07792799919843674, Total Loss 7.791646480560303\n",
      "27: Encoding Loss -0.9145622849464417, Transition Loss -11.709867477416992, Classifier Loss 0.08560802042484283, Total Loss 8.558460235595703\n",
      "27: Encoding Loss -0.8206027746200562, Transition Loss -3.6822125911712646, Classifier Loss 0.09383340179920197, Total Loss 9.382603645324707\n",
      "27: Encoding Loss -0.43033596873283386, Transition Loss -11.375600814819336, Classifier Loss 0.07760607451200485, Total Loss 7.758303165435791\n",
      "27: Encoding Loss -0.110826276242733, Transition Loss -6.622030258178711, Classifier Loss 0.17930643260478973, Total Loss 17.810625076293945\n",
      "27: Encoding Loss -1.5606329441070557, Transition Loss -12.957942962646484, Classifier Loss 0.05298347771167755, Total Loss 5.2957563400268555\n",
      "27: Encoding Loss -0.7294067740440369, Transition Loss -7.7917799949646, Classifier Loss 0.04341842979192734, Total Loss 4.340284824371338\n",
      "27: Encoding Loss -1.1412321329116821, Transition Loss -7.506527900695801, Classifier Loss 0.08399006724357605, Total Loss 8.397505760192871\n",
      "27: Encoding Loss -0.6807397603988647, Transition Loss -9.592665672302246, Classifier Loss 0.12474587559700012, Total Loss 12.472668647766113\n",
      "27: Encoding Loss -1.5130149126052856, Transition Loss -10.137057304382324, Classifier Loss 0.07565779983997345, Total Loss 7.5637526512146\n",
      "27: Encoding Loss -1.354364037513733, Transition Loss -11.530342102050781, Classifier Loss 0.07738050818443298, Total Loss 7.735744953155518\n",
      "27: Encoding Loss -1.171734094619751, Transition Loss -8.133853912353516, Classifier Loss 0.043027132749557495, Total Loss 4.30108642578125\n",
      "27: Encoding Loss -1.613932728767395, Transition Loss -16.2047176361084, Classifier Loss 0.09682142734527588, Total Loss 9.678901672363281\n",
      "27: Encoding Loss -1.7452064752578735, Transition Loss -15.693707466125488, Classifier Loss 0.04015500098466873, Total Loss 4.012361526489258\n",
      "27: Encoding Loss -1.4260576963424683, Transition Loss -5.595075607299805, Classifier Loss 0.07554484903812408, Total Loss 7.553365707397461\n",
      "27: Encoding Loss -1.4970605373382568, Transition Loss -16.82870101928711, Classifier Loss 0.05581732094287872, Total Loss 5.578366756439209\n",
      "27: Encoding Loss -0.0701921135187149, Transition Loss -7.482266902923584, Classifier Loss 0.0917370468378067, Total Loss 9.036673545837402\n",
      "27: Encoding Loss 0.012032274156808853, Transition Loss -16.95250129699707, Classifier Loss 0.07637646049261093, Total Loss 7.686994552612305\n",
      "27: Encoding Loss -1.6647448539733887, Transition Loss -6.174532890319824, Classifier Loss 0.040603335946798325, Total Loss 4.059098720550537\n",
      "27: Encoding Loss -1.3577440977096558, Transition Loss -9.02116584777832, Classifier Loss 0.06846191734075546, Total Loss 6.844387531280518\n",
      "27: Encoding Loss -1.1481971740722656, Transition Loss -15.810284614562988, Classifier Loss 0.06025842949748039, Total Loss 6.02268123626709\n",
      "27: Encoding Loss -1.7218748331069946, Transition Loss -18.95511245727539, Classifier Loss 0.052860088646411896, Total Loss 5.282217979431152\n",
      "27: Encoding Loss -0.56028151512146, Transition Loss -11.94312572479248, Classifier Loss 0.07147088646888733, Total Loss 7.144700050354004\n",
      "27: Encoding Loss -1.0719053745269775, Transition Loss -3.0468850135803223, Classifier Loss 0.08505112677812576, Total Loss 8.50450325012207\n",
      "27: Encoding Loss -1.381238579750061, Transition Loss -11.742663383483887, Classifier Loss 0.05648498609662056, Total Loss 5.6461501121521\n",
      "27: Encoding Loss -0.745823860168457, Transition Loss -11.397676467895508, Classifier Loss 0.07860203087329865, Total Loss 7.85792350769043\n",
      "27: Encoding Loss -1.3536571264266968, Transition Loss -4.592877388000488, Classifier Loss 0.09224966168403625, Total Loss 9.224047660827637\n",
      "27: Encoding Loss -0.4406619966030121, Transition Loss -10.72507381439209, Classifier Loss 0.04200402647256851, Total Loss 4.198239326477051\n",
      "27: Encoding Loss -1.6313894987106323, Transition Loss -12.323760986328125, Classifier Loss 0.0703667625784874, Total Loss 7.0342116355896\n",
      "27: Encoding Loss -1.7018646001815796, Transition Loss -18.952604293823242, Classifier Loss 0.05509065091609955, Total Loss 5.505274772644043\n",
      "27: Encoding Loss -1.5612623691558838, Transition Loss -15.823461532592773, Classifier Loss 0.07548277080059052, Total Loss 7.545112133026123\n",
      "27: Encoding Loss -1.0380254983901978, Transition Loss -8.986176490783691, Classifier Loss 0.07094445824623108, Total Loss 7.092648506164551\n",
      "27: Encoding Loss -1.5383650064468384, Transition Loss -7.470635890960693, Classifier Loss 0.11377744376659393, Total Loss 11.376250267028809\n",
      "27: Encoding Loss -1.3663736581802368, Transition Loss -9.523165702819824, Classifier Loss 0.0675056129693985, Total Loss 6.748656749725342\n",
      "27: Encoding Loss -1.0153080224990845, Transition Loss -6.878230571746826, Classifier Loss 0.07922139018774033, Total Loss 7.9207634925842285\n",
      "27: Encoding Loss -1.222986102104187, Transition Loss -11.347604751586914, Classifier Loss 0.07516879588365555, Total Loss 7.5146098136901855\n",
      "27: Encoding Loss -1.643861174583435, Transition Loss -4.512701988220215, Classifier Loss 0.11752805858850479, Total Loss 11.751903533935547\n",
      "27: Encoding Loss -0.28662368655204773, Transition Loss -1.0444769859313965, Classifier Loss 0.06866379082202911, Total Loss 6.861408233642578\n",
      "27: Encoding Loss -1.2791764736175537, Transition Loss -17.258625030517578, Classifier Loss 0.08793918043375015, Total Loss 8.79046630859375\n",
      "27: Encoding Loss -0.1911042481660843, Transition Loss -7.802402973175049, Classifier Loss 0.07234465330839157, Total Loss 7.190097808837891\n",
      "27: Encoding Loss -0.34476378560066223, Transition Loss -8.158265113830566, Classifier Loss 0.13001562654972076, Total Loss 12.999151229858398\n",
      "27: Encoding Loss -1.116020917892456, Transition Loss -7.8501057624816895, Classifier Loss 0.0871610939502716, Total Loss 8.714539527893066\n",
      "27: Encoding Loss -1.0343375205993652, Transition Loss -8.034330368041992, Classifier Loss 0.08177536725997925, Total Loss 8.17593002319336\n",
      "27: Encoding Loss -0.3291904032230377, Transition Loss -4.583308219909668, Classifier Loss 0.10933629423379898, Total Loss 10.931402206420898\n",
      "27: Encoding Loss -1.0962084531784058, Transition Loss -8.035128593444824, Classifier Loss 0.029774850234389305, Total Loss 2.9758780002593994\n",
      "27: Encoding Loss -1.6139475107192993, Transition Loss -6.985511302947998, Classifier Loss 0.11707108467817307, Total Loss 11.705711364746094\n",
      "27: Encoding Loss -1.1586507558822632, Transition Loss -13.39860725402832, Classifier Loss 0.047998473048210144, Total Loss 4.7971673011779785\n",
      "27: Encoding Loss -1.0570327043533325, Transition Loss -5.368608474731445, Classifier Loss 0.09145336598157883, Total Loss 9.144262313842773\n",
      "27: Encoding Loss -2.098741054534912, Transition Loss -11.247406959533691, Classifier Loss 0.07186351716518402, Total Loss 7.1841020584106445\n",
      "27: Encoding Loss -1.2348084449768066, Transition Loss -12.364797592163086, Classifier Loss 0.0374479703605175, Total Loss 3.742324113845825\n",
      "27: Encoding Loss -1.0392218828201294, Transition Loss -9.187508583068848, Classifier Loss 0.1139758974313736, Total Loss 11.395751953125\n",
      "27: Encoding Loss -0.5147656202316284, Transition Loss -10.139522552490234, Classifier Loss 0.06570222973823547, Total Loss 6.568194389343262\n",
      "27: Encoding Loss -1.0949565172195435, Transition Loss -7.768185138702393, Classifier Loss 0.061034880578517914, Total Loss 6.101934432983398\n",
      "27: Encoding Loss -0.7173492312431335, Transition Loss -4.331465244293213, Classifier Loss 0.10351521521806717, Total Loss 10.350655555725098\n",
      "27: Encoding Loss -0.07192366570234299, Transition Loss -10.035310745239258, Classifier Loss 0.09974631667137146, Total Loss 9.836833000183105\n",
      "27: Encoding Loss -2.2563178539276123, Transition Loss -6.869150638580322, Classifier Loss 0.06532564759254456, Total Loss 6.531190872192383\n",
      "27: Encoding Loss -1.3441072702407837, Transition Loss -4.594775676727295, Classifier Loss 0.0731642097234726, Total Loss 7.315502166748047\n",
      "27: Encoding Loss -1.5048068761825562, Transition Loss -14.784196853637695, Classifier Loss 0.0790589302778244, Total Loss 7.902935981750488\n",
      "27: Encoding Loss -1.2816163301467896, Transition Loss -8.593147277832031, Classifier Loss 0.11586131155490875, Total Loss 11.584412574768066\n",
      "27: Encoding Loss -0.8877714276313782, Transition Loss -6.841844081878662, Classifier Loss 0.055724624544382095, Total Loss 5.571094036102295\n",
      "27: Encoding Loss -0.37647387385368347, Transition Loss -12.26008415222168, Classifier Loss 0.06538204848766327, Total Loss 6.535501956939697\n",
      "27: Encoding Loss -2.51544189453125, Transition Loss -20.70676612854004, Classifier Loss 0.06665585190057755, Total Loss 6.661443710327148\n",
      "27: Encoding Loss -0.8001362085342407, Transition Loss -6.3500261306762695, Classifier Loss 0.09592782706022263, Total Loss 9.591512680053711\n",
      "27: Encoding Loss -0.9816259741783142, Transition Loss -6.796220302581787, Classifier Loss 0.08293858915567398, Total Loss 8.292499542236328\n",
      "27: Encoding Loss -0.5863053798675537, Transition Loss -8.890092849731445, Classifier Loss 0.08198950439691544, Total Loss 8.197173118591309\n",
      "27: Encoding Loss -0.5971547365188599, Transition Loss -3.102109909057617, Classifier Loss 0.07917537540197372, Total Loss 7.916917324066162\n",
      "27: Encoding Loss -1.3272284269332886, Transition Loss -12.045506477355957, Classifier Loss 0.08508655428886414, Total Loss 8.506246566772461\n",
      "27: Encoding Loss -1.22123122215271, Transition Loss -15.018898010253906, Classifier Loss 0.08879812806844711, Total Loss 8.876809120178223\n",
      "27: Encoding Loss -1.6969425678253174, Transition Loss -12.079294204711914, Classifier Loss 0.05919549614191055, Total Loss 5.917133808135986\n",
      "27: Encoding Loss -0.9760340452194214, Transition Loss -3.45967960357666, Classifier Loss 0.041106387972831726, Total Loss 4.1099467277526855\n",
      "27: Encoding Loss -1.151296615600586, Transition Loss -6.196375846862793, Classifier Loss 0.11803624033927917, Total Loss 11.802385330200195\n",
      "27: Encoding Loss -1.335064172744751, Transition Loss -13.30201530456543, Classifier Loss 0.07198894023895264, Total Loss 7.196233749389648\n",
      "27: Encoding Loss -0.13866935670375824, Transition Loss 0.9569454789161682, Classifier Loss 0.11280502378940582, Total Loss 11.380072593688965\n",
      "27: Encoding Loss -0.6309419274330139, Transition Loss -6.850856781005859, Classifier Loss 0.07557244598865509, Total Loss 7.555874824523926\n",
      "27: Encoding Loss 0.16858211159706116, Transition Loss -3.759174346923828, Classifier Loss 0.0934041440486908, Total Loss 10.626395225524902\n",
      "27: Encoding Loss -0.8111035823822021, Transition Loss -11.976994514465332, Classifier Loss 0.06127975136041641, Total Loss 6.125579357147217\n",
      "27: Encoding Loss -0.5354081988334656, Transition Loss -15.56237506866455, Classifier Loss 0.09029407054185867, Total Loss 9.026294708251953\n",
      "27: Encoding Loss -0.8887044787406921, Transition Loss -4.746344566345215, Classifier Loss 0.0925394669175148, Total Loss 9.252997398376465\n",
      "27: Encoding Loss -0.9530878067016602, Transition Loss -9.000168800354004, Classifier Loss 0.04838823154568672, Total Loss 4.8370232582092285\n",
      "27: Encoding Loss -1.2054604291915894, Transition Loss -9.083453178405762, Classifier Loss 0.06482964754104614, Total Loss 6.481147766113281\n",
      "27: Encoding Loss -0.6136763691902161, Transition Loss -14.31784439086914, Classifier Loss 0.07376076281070709, Total Loss 7.373212814331055\n",
      "27: Encoding Loss -1.150149941444397, Transition Loss -13.804388046264648, Classifier Loss 0.08456805348396301, Total Loss 8.454044342041016\n",
      "27: Encoding Loss -1.3920029401779175, Transition Loss -2.0528690814971924, Classifier Loss 0.048934563994407654, Total Loss 4.893045902252197\n",
      "27: Encoding Loss -2.249401807785034, Transition Loss -17.539714813232422, Classifier Loss 0.05171127989888191, Total Loss 5.167619705200195\n",
      "27: Encoding Loss -1.1758164167404175, Transition Loss -16.73857307434082, Classifier Loss 0.048060499131679535, Total Loss 4.802701950073242\n",
      "27: Encoding Loss -0.16288970410823822, Transition Loss -1.5395556688308716, Classifier Loss 0.07101361453533173, Total Loss 7.033724308013916\n",
      "27: Encoding Loss -0.16675621271133423, Transition Loss 1.8370362520217896, Classifier Loss 0.0684100091457367, Total Loss 7.144772529602051\n",
      "27: Encoding Loss 0.20243413746356964, Transition Loss -11.96596622467041, Classifier Loss 0.05071604624390602, Total Loss 6.653918743133545\n",
      "27: Encoding Loss -0.6159765720367432, Transition Loss -10.37539291381836, Classifier Loss 0.05361878126859665, Total Loss 5.359802722930908\n",
      "27: Encoding Loss -1.198638677597046, Transition Loss -12.719447135925293, Classifier Loss 0.06864390522241592, Total Loss 6.861846446990967\n",
      "27: Encoding Loss 0.364066481590271, Transition Loss -5.65309476852417, Classifier Loss 0.0718269944190979, Total Loss 10.093704223632812\n",
      "27: Encoding Loss -1.8179293870925903, Transition Loss -14.714786529541016, Classifier Loss 0.05144412815570831, Total Loss 5.141469955444336\n",
      "27: Encoding Loss -1.259575366973877, Transition Loss -9.200511932373047, Classifier Loss 0.082286037504673, Total Loss 8.226763725280762\n",
      "27: Encoding Loss -0.6085612773895264, Transition Loss -11.719503402709961, Classifier Loss 0.10925116389989853, Total Loss 10.922772407531738\n",
      "27: Encoding Loss -1.655873417854309, Transition Loss -19.003015518188477, Classifier Loss 0.07107752561569214, Total Loss 7.103952407836914\n",
      "27: Encoding Loss -1.4231377840042114, Transition Loss -8.395137786865234, Classifier Loss 0.08413722366094589, Total Loss 8.412042617797852\n",
      "27: Encoding Loss -0.6086486577987671, Transition Loss -10.195405960083008, Classifier Loss 0.12509402632713318, Total Loss 12.507363319396973\n",
      "27: Encoding Loss -1.2563211917877197, Transition Loss -15.524331092834473, Classifier Loss 0.09192478656768799, Total Loss 9.189373016357422\n",
      "27: Encoding Loss -1.0566141605377197, Transition Loss -12.258722305297852, Classifier Loss 0.034388329833745956, Total Loss 3.4363813400268555\n",
      "27: Encoding Loss -0.46901193261146545, Transition Loss -11.355189323425293, Classifier Loss 0.07060549408197403, Total Loss 7.058272838592529\n",
      "27: Encoding Loss -0.358406126499176, Transition Loss -5.340496063232422, Classifier Loss 0.054804299026727676, Total Loss 5.478877067565918\n",
      "27: Encoding Loss 0.3651312589645386, Transition Loss -8.497404098510742, Classifier Loss 0.0726715549826622, Total Loss 10.186125755310059\n",
      "27: Encoding Loss -0.48226451873779297, Transition Loss -8.846497535705566, Classifier Loss 0.06733747571706772, Total Loss 6.731975555419922\n",
      "27: Encoding Loss -0.4932404160499573, Transition Loss -4.412266254425049, Classifier Loss 0.053535811603069305, Total Loss 5.352696895599365\n",
      "27: Encoding Loss -0.550163209438324, Transition Loss -10.12175178527832, Classifier Loss 0.039688751101493835, Total Loss 3.966850519180298\n",
      "27: Encoding Loss -0.1021406278014183, Transition Loss -11.774978637695312, Classifier Loss 0.0777173787355423, Total Loss 7.6439290046691895\n",
      "27: Encoding Loss 0.8697513341903687, Transition Loss -11.957215309143066, Classifier Loss 0.08064142614603043, Total Loss 15.01976203918457\n",
      "27: Encoding Loss -0.8629918694496155, Transition Loss -0.8522014617919922, Classifier Loss 0.11050807684659958, Total Loss 11.050637245178223\n",
      "27: Encoding Loss -0.5011721253395081, Transition Loss -8.427349090576172, Classifier Loss 0.048203710466623306, Total Loss 4.8186845779418945\n",
      "27: Encoding Loss 0.2949969470500946, Transition Loss -11.868366241455078, Classifier Loss 0.08488783240318298, Total Loss 10.842635154724121\n",
      "27: Encoding Loss 0.01479327492415905, Transition Loss -7.331664562225342, Classifier Loss 0.07209514826536179, Total Loss 7.2741804122924805\n",
      "27: Encoding Loss -1.1150981187820435, Transition Loss -7.636490821838379, Classifier Loss 0.06559105962514877, Total Loss 6.5575785636901855\n",
      "27: Encoding Loss -1.9274420738220215, Transition Loss -3.620663642883301, Classifier Loss 0.08982925117015839, Total Loss 8.98220157623291\n",
      "27: Encoding Loss -2.1085073947906494, Transition Loss -12.206692695617676, Classifier Loss 0.04261805862188339, Total Loss 4.259364604949951\n",
      "27: Encoding Loss 0.18623857200145721, Transition Loss -0.5280324220657349, Classifier Loss 0.06662853807210922, Total Loss 8.106060981750488\n",
      "27: Encoding Loss -0.6847326755523682, Transition Loss -6.820943355560303, Classifier Loss 0.04385080933570862, Total Loss 4.383716583251953\n",
      "27: Encoding Loss -0.2327529937028885, Transition Loss -6.123929023742676, Classifier Loss 0.08091190457344055, Total Loss 8.071404457092285\n",
      "27: Encoding Loss 0.058400239795446396, Transition Loss -15.202329635620117, Classifier Loss 0.15726740658283234, Total Loss 16.06026840209961\n",
      "27: Encoding Loss -1.2311533689498901, Transition Loss -14.363174438476562, Classifier Loss 0.0922735407948494, Total Loss 9.224481582641602\n",
      "27: Encoding Loss -1.861045002937317, Transition Loss -19.869815826416016, Classifier Loss 0.05496569722890854, Total Loss 5.492595672607422\n",
      "27: Encoding Loss -0.6618717312812805, Transition Loss -9.913732528686523, Classifier Loss 0.10123571008443832, Total Loss 10.121588706970215\n",
      "27: Encoding Loss -0.6626784205436707, Transition Loss -11.102849006652832, Classifier Loss 0.069771908223629, Total Loss 6.97497034072876\n",
      "27: Encoding Loss -1.7689487934112549, Transition Loss -17.45998764038086, Classifier Loss 0.07168415188789368, Total Loss 7.164923191070557\n",
      "27: Encoding Loss -1.4351683855056763, Transition Loss -5.0362677574157715, Classifier Loss 0.09693475067615509, Total Loss 9.69246768951416\n",
      "27: Encoding Loss 0.1966497153043747, Transition Loss -13.998222351074219, Classifier Loss 0.09566788375377655, Total Loss 11.098453521728516\n",
      "27: Encoding Loss -1.508575677871704, Transition Loss -16.54512596130371, Classifier Loss 0.03258231282234192, Total Loss 3.2549221515655518\n",
      "27: Encoding Loss -1.3636270761489868, Transition Loss -19.927448272705078, Classifier Loss 0.06745706498622894, Total Loss 6.741721153259277\n",
      "27: Encoding Loss -2.7107717990875244, Transition Loss -25.85930824279785, Classifier Loss 0.0313897430896759, Total Loss 3.1338024139404297\n",
      "28: Encoding Loss -1.0445917844772339, Transition Loss -4.783741474151611, Classifier Loss 0.040347807109355927, Total Loss 4.0338239669799805\n",
      "28: Encoding Loss -0.9205232262611389, Transition Loss -5.224508285522461, Classifier Loss 0.11749718338251114, Total Loss 11.748673439025879\n",
      "28: Encoding Loss -1.0721511840820312, Transition Loss -0.30577898025512695, Classifier Loss 0.046377360820770264, Total Loss 4.6376752853393555\n",
      "28: Encoding Loss -1.2468383312225342, Transition Loss -18.500333786010742, Classifier Loss 0.0719606950879097, Total Loss 7.19236946105957\n",
      "28: Encoding Loss -1.5019049644470215, Transition Loss -8.48515796661377, Classifier Loss 0.05523647367954254, Total Loss 5.5219502449035645\n",
      "28: Encoding Loss -1.358411431312561, Transition Loss -15.565520286560059, Classifier Loss 0.06103912740945816, Total Loss 6.100799560546875\n",
      "28: Encoding Loss -1.3865591287612915, Transition Loss -11.453598022460938, Classifier Loss 0.07473507523536682, Total Loss 7.471216678619385\n",
      "28: Encoding Loss -1.645923137664795, Transition Loss -9.047996520996094, Classifier Loss 0.05671318247914314, Total Loss 5.669508457183838\n",
      "28: Encoding Loss -0.6759119629859924, Transition Loss -15.51679515838623, Classifier Loss 0.05043807625770569, Total Loss 5.040704250335693\n",
      "28: Encoding Loss -1.840809941291809, Transition Loss -12.492374420166016, Classifier Loss 0.07925424724817276, Total Loss 7.92292594909668\n",
      "28: Encoding Loss 0.4709323048591614, Transition Loss -8.655098915100098, Classifier Loss 0.06405439227819443, Total Loss 10.171162605285645\n",
      "28: Encoding Loss -0.4088677167892456, Transition Loss -3.408263683319092, Classifier Loss 0.0716983750462532, Total Loss 7.169084548950195\n",
      "28: Encoding Loss -1.4782986640930176, Transition Loss -4.83863639831543, Classifier Loss 0.056755926460027695, Total Loss 5.674624919891357\n",
      "28: Encoding Loss -0.5664007663726807, Transition Loss -5.589931011199951, Classifier Loss 0.06557904928922653, Total Loss 6.55678653717041\n",
      "28: Encoding Loss -2.0147194862365723, Transition Loss -12.880372047424316, Classifier Loss 0.053245432674884796, Total Loss 5.321967601776123\n",
      "28: Encoding Loss -2.52390193939209, Transition Loss -17.147878646850586, Classifier Loss 0.07265165448188782, Total Loss 7.261735916137695\n",
      "28: Encoding Loss -1.1495310068130493, Transition Loss -8.55644416809082, Classifier Loss 0.02877952717244625, Total Loss 2.876241445541382\n",
      "28: Encoding Loss -0.3839075267314911, Transition Loss -5.1404900550842285, Classifier Loss 0.056965820491313934, Total Loss 5.695363998413086\n",
      "28: Encoding Loss -0.22377488017082214, Transition Loss -12.302984237670898, Classifier Loss 0.09754624962806702, Total Loss 9.729575157165527\n",
      "28: Encoding Loss -0.6925296783447266, Transition Loss -4.61916446685791, Classifier Loss 0.05938643217086792, Total Loss 5.937719821929932\n",
      "28: Encoding Loss -1.1168938875198364, Transition Loss -14.006107330322266, Classifier Loss 0.08582885563373566, Total Loss 8.580084800720215\n",
      "28: Encoding Loss -1.5142661333084106, Transition Loss -9.958404541015625, Classifier Loss 0.047413259744644165, Total Loss 4.7393341064453125\n",
      "28: Encoding Loss -1.782183289527893, Transition Loss -10.888660430908203, Classifier Loss 0.0813300684094429, Total Loss 8.130828857421875\n",
      "28: Encoding Loss -0.8379880785942078, Transition Loss -2.8929762840270996, Classifier Loss 0.08807011693716049, Total Loss 8.806432723999023\n",
      "28: Encoding Loss -2.0084431171417236, Transition Loss -10.894376754760742, Classifier Loss 0.043640319257974625, Total Loss 4.361853122711182\n",
      "28: Encoding Loss -0.7918708920478821, Transition Loss -4.241464614868164, Classifier Loss 0.07498530298471451, Total Loss 7.497682094573975\n",
      "28: Encoding Loss -1.8134194612503052, Transition Loss -2.4747583866119385, Classifier Loss 0.10411816835403442, Total Loss 10.411321640014648\n",
      "28: Encoding Loss -2.4235498905181885, Transition Loss -17.84795379638672, Classifier Loss 0.08211643993854523, Total Loss 8.208074569702148\n",
      "28: Encoding Loss -2.0470283031463623, Transition Loss -8.71291732788086, Classifier Loss 0.06879531592130661, Total Loss 6.87778902053833\n",
      "28: Encoding Loss -0.8454465270042419, Transition Loss -4.334912300109863, Classifier Loss 0.07531996071338654, Total Loss 7.531129360198975\n",
      "28: Encoding Loss -0.8305343985557556, Transition Loss -10.606382369995117, Classifier Loss 0.08378277719020844, Total Loss 8.3761568069458\n",
      "28: Encoding Loss -1.0228830575942993, Transition Loss -2.341447353363037, Classifier Loss 0.09244739264249802, Total Loss 9.244271278381348\n",
      "28: Encoding Loss -0.5157278776168823, Transition Loss -10.331600189208984, Classifier Loss 0.07699068635702133, Total Loss 7.697001934051514\n",
      "28: Encoding Loss 0.11031661182641983, Transition Loss -5.133127689361572, Classifier Loss 0.18037287890911102, Total Loss 18.799673080444336\n",
      "28: Encoding Loss -2.1809561252593994, Transition Loss -13.650010108947754, Classifier Loss 0.05267750471830368, Total Loss 5.265020370483398\n",
      "28: Encoding Loss -1.241004228591919, Transition Loss -8.7554292678833, Classifier Loss 0.045702483505010605, Total Loss 4.568497180938721\n",
      "28: Encoding Loss -1.209795594215393, Transition Loss -8.30370044708252, Classifier Loss 0.08468176424503326, Total Loss 8.466516494750977\n",
      "28: Encoding Loss -1.078134536743164, Transition Loss -10.455723762512207, Classifier Loss 0.12290339171886444, Total Loss 12.288248062133789\n",
      "28: Encoding Loss -1.7259259223937988, Transition Loss -10.773086547851562, Classifier Loss 0.07731706649065018, Total Loss 7.729551792144775\n",
      "28: Encoding Loss -1.5772435665130615, Transition Loss -11.91989517211914, Classifier Loss 0.07587683200836182, Total Loss 7.585299015045166\n",
      "28: Encoding Loss -1.573473572731018, Transition Loss -8.755072593688965, Classifier Loss 0.04293661192059517, Total Loss 4.291910171508789\n",
      "28: Encoding Loss -1.5162599086761475, Transition Loss -16.66485023498535, Classifier Loss 0.09331659227609634, Total Loss 9.328326225280762\n",
      "28: Encoding Loss -2.046825885772705, Transition Loss -16.02242660522461, Classifier Loss 0.04079100862145424, Total Loss 4.075896739959717\n",
      "28: Encoding Loss -2.0214781761169434, Transition Loss -5.848116874694824, Classifier Loss 0.07292113453149796, Total Loss 7.290943622589111\n",
      "28: Encoding Loss -1.6343562602996826, Transition Loss -17.24423599243164, Classifier Loss 0.05525154992938042, Total Loss 5.5217061042785645\n",
      "28: Encoding Loss -0.13782334327697754, Transition Loss -7.808956623077393, Classifier Loss 0.09153278917074203, Total Loss 9.059027671813965\n",
      "28: Encoding Loss -0.5782294869422913, Transition Loss -17.089391708374023, Classifier Loss 0.07608094811439514, Total Loss 7.604676723480225\n",
      "28: Encoding Loss -2.160766839981079, Transition Loss -6.537847995758057, Classifier Loss 0.0419062003493309, Total Loss 4.18931245803833\n",
      "28: Encoding Loss -1.9225926399230957, Transition Loss -9.300661087036133, Classifier Loss 0.06756197661161423, Total Loss 6.754337310791016\n",
      "28: Encoding Loss -1.4899718761444092, Transition Loss -16.259235382080078, Classifier Loss 0.06090666726231575, Total Loss 6.087414741516113\n",
      "28: Encoding Loss -2.1051363945007324, Transition Loss -19.82669448852539, Classifier Loss 0.05263436585664749, Total Loss 5.259471416473389\n",
      "28: Encoding Loss -0.8624292016029358, Transition Loss -12.370156288146973, Classifier Loss 0.07148122787475586, Total Loss 7.145648956298828\n",
      "28: Encoding Loss -1.6249099969863892, Transition Loss -3.6705198287963867, Classifier Loss 0.08617506921291351, Total Loss 8.616772651672363\n",
      "28: Encoding Loss -1.5761299133300781, Transition Loss -12.167094230651855, Classifier Loss 0.05478798225522041, Total Loss 5.476365089416504\n",
      "28: Encoding Loss -0.6651543974876404, Transition Loss -11.966064453125, Classifier Loss 0.07646068930625916, Total Loss 7.643675804138184\n",
      "28: Encoding Loss -1.7086198329925537, Transition Loss -4.945491790771484, Classifier Loss 0.09187959134578705, Total Loss 9.186969757080078\n",
      "28: Encoding Loss -0.4096978008747101, Transition Loss -11.155123710632324, Classifier Loss 0.04259297251701355, Total Loss 4.256997585296631\n",
      "28: Encoding Loss -2.0927131175994873, Transition Loss -12.726297378540039, Classifier Loss 0.06610632687807083, Total Loss 6.608087539672852\n",
      "28: Encoding Loss -2.2592687606811523, Transition Loss -19.57664680480957, Classifier Loss 0.05287898704409599, Total Loss 5.28398323059082\n",
      "28: Encoding Loss -1.9294296503067017, Transition Loss -16.341238021850586, Classifier Loss 0.07256749272346497, Total Loss 7.253480911254883\n",
      "28: Encoding Loss -1.555732011795044, Transition Loss -9.361105918884277, Classifier Loss 0.06963333487510681, Total Loss 6.961461544036865\n",
      "28: Encoding Loss -2.246244192123413, Transition Loss -7.872071266174316, Classifier Loss 0.11290036141872406, Total Loss 11.288461685180664\n",
      "28: Encoding Loss -1.970160722732544, Transition Loss -9.81599235534668, Classifier Loss 0.06814445555210114, Total Loss 6.8124823570251465\n",
      "28: Encoding Loss -1.34012770652771, Transition Loss -7.271246433258057, Classifier Loss 0.07781606167554855, Total Loss 7.780151844024658\n",
      "28: Encoding Loss -1.7059355974197388, Transition Loss -11.944103240966797, Classifier Loss 0.06722507625818253, Total Loss 6.720118522644043\n",
      "28: Encoding Loss -2.1584718227386475, Transition Loss -4.785550594329834, Classifier Loss 0.11373135447502136, Total Loss 11.372178077697754\n",
      "28: Encoding Loss -0.7040866613388062, Transition Loss -1.4670252799987793, Classifier Loss 0.06752531975507736, Total Loss 6.752238750457764\n",
      "28: Encoding Loss -2.0830698013305664, Transition Loss -17.792875289916992, Classifier Loss 0.08758595585823059, Total Loss 8.755037307739258\n",
      "28: Encoding Loss -0.3685828149318695, Transition Loss -8.174309730529785, Classifier Loss 0.07451999187469482, Total Loss 7.450027942657471\n",
      "28: Encoding Loss -0.7233498096466064, Transition Loss -8.708252906799316, Classifier Loss 0.12839750945568085, Total Loss 12.83800983428955\n",
      "28: Encoding Loss -1.7270312309265137, Transition Loss -8.490303993225098, Classifier Loss 0.08692872524261475, Total Loss 8.691174507141113\n",
      "28: Encoding Loss -1.4962022304534912, Transition Loss -8.55237102508545, Classifier Loss 0.07756505161523819, Total Loss 7.754794597625732\n",
      "28: Encoding Loss -0.8423249125480652, Transition Loss -5.059516429901123, Classifier Loss 0.10682149976491928, Total Loss 10.681138038635254\n",
      "28: Encoding Loss -1.6132315397262573, Transition Loss -8.479083061218262, Classifier Loss 0.03053128905594349, Total Loss 3.0514330863952637\n",
      "28: Encoding Loss -1.988483190536499, Transition Loss -7.468022346496582, Classifier Loss 0.11493339389562607, Total Loss 11.491846084594727\n",
      "28: Encoding Loss -1.5193051099777222, Transition Loss -13.965703010559082, Classifier Loss 0.04791305959224701, Total Loss 4.788512706756592\n",
      "28: Encoding Loss -1.768343448638916, Transition Loss -6.150317192077637, Classifier Loss 0.08934286236763, Total Loss 8.933055877685547\n",
      "28: Encoding Loss -2.6368820667266846, Transition Loss -11.811391830444336, Classifier Loss 0.07042595744132996, Total Loss 7.040233612060547\n",
      "28: Encoding Loss -1.72842276096344, Transition Loss -12.981485366821289, Classifier Loss 0.03456547483801842, Total Loss 3.453951120376587\n",
      "28: Encoding Loss -1.847913146018982, Transition Loss -9.64324951171875, Classifier Loss 0.11440737545490265, Total Loss 11.438809394836426\n",
      "28: Encoding Loss -0.9913451075553894, Transition Loss -10.49892520904541, Classifier Loss 0.06525279581546783, Total Loss 6.523179531097412\n",
      "28: Encoding Loss -1.9706779718399048, Transition Loss -8.343265533447266, Classifier Loss 0.060617756098508835, Total Loss 6.060107231140137\n",
      "28: Encoding Loss -1.3521538972854614, Transition Loss -4.740551948547363, Classifier Loss 0.10296538472175598, Total Loss 10.2955904006958\n",
      "28: Encoding Loss -0.5245135426521301, Transition Loss -10.506519317626953, Classifier Loss 0.09742740541696548, Total Loss 9.740639686584473\n",
      "28: Encoding Loss -2.86837100982666, Transition Loss -7.435011863708496, Classifier Loss 0.06352891027927399, Total Loss 6.351404190063477\n",
      "28: Encoding Loss -1.50750732421875, Transition Loss -5.126470565795898, Classifier Loss 0.07232211530208588, Total Loss 7.231186389923096\n",
      "28: Encoding Loss -1.833677887916565, Transition Loss -15.439712524414062, Classifier Loss 0.07430244237184525, Total Loss 7.427156448364258\n",
      "28: Encoding Loss -1.6450769901275635, Transition Loss -9.09051513671875, Classifier Loss 0.1145559623837471, Total Loss 11.453778266906738\n",
      "28: Encoding Loss -0.8350139260292053, Transition Loss -7.312471389770508, Classifier Loss 0.05663551762700081, Total Loss 5.6620893478393555\n",
      "28: Encoding Loss -0.7326710224151611, Transition Loss -12.720191955566406, Classifier Loss 0.06638537347316742, Total Loss 6.635993480682373\n",
      "28: Encoding Loss -3.3993194103240967, Transition Loss -21.48322296142578, Classifier Loss 0.06288238614797592, Total Loss 6.283941745758057\n",
      "28: Encoding Loss -1.2723225355148315, Transition Loss -6.791131973266602, Classifier Loss 0.09554697573184967, Total Loss 9.553339958190918\n",
      "28: Encoding Loss -1.7189083099365234, Transition Loss -7.490832328796387, Classifier Loss 0.07787542045116425, Total Loss 7.786043643951416\n",
      "28: Encoding Loss -1.006402850151062, Transition Loss -9.54410171508789, Classifier Loss 0.08064157515764236, Total Loss 8.062248229980469\n",
      "28: Encoding Loss -1.0414488315582275, Transition Loss -3.619659662246704, Classifier Loss 0.07615187764167786, Total Loss 7.614463806152344\n",
      "28: Encoding Loss -1.7340019941329956, Transition Loss -12.482006072998047, Classifier Loss 0.08044455200433731, Total Loss 8.041958808898926\n",
      "28: Encoding Loss -1.5165207386016846, Transition Loss -15.675298690795898, Classifier Loss 0.08546602725982666, Total Loss 8.54346752166748\n",
      "28: Encoding Loss -2.184901237487793, Transition Loss -12.752431869506836, Classifier Loss 0.055257536470890045, Total Loss 5.523202896118164\n",
      "28: Encoding Loss -1.3253918886184692, Transition Loss -3.943509101867676, Classifier Loss 0.04040557146072388, Total Loss 4.039768218994141\n",
      "28: Encoding Loss -1.9416544437408447, Transition Loss -6.642742156982422, Classifier Loss 0.11517767608165741, Total Loss 11.516439437866211\n",
      "28: Encoding Loss -2.0274722576141357, Transition Loss -13.530353546142578, Classifier Loss 0.07020001113414764, Total Loss 7.017294883728027\n",
      "28: Encoding Loss -0.6972458958625793, Transition Loss 0.7334351539611816, Classifier Loss 0.11149297654628754, Total Loss 11.295984268188477\n",
      "28: Encoding Loss -0.8663408756256104, Transition Loss -7.4026994705200195, Classifier Loss 0.07441461086273193, Total Loss 7.439980506896973\n",
      "28: Encoding Loss -0.5524883270263672, Transition Loss -4.208207607269287, Classifier Loss 0.09220007061958313, Total Loss 9.219164848327637\n",
      "28: Encoding Loss -1.414254903793335, Transition Loss -11.340295791625977, Classifier Loss 0.05680021643638611, Total Loss 5.677753925323486\n",
      "28: Encoding Loss -1.7463436126708984, Transition Loss -15.050954818725586, Classifier Loss 0.09094907343387604, Total Loss 9.091897964477539\n",
      "28: Encoding Loss -1.3844666481018066, Transition Loss -4.103527545928955, Classifier Loss 0.08603374660015106, Total Loss 8.602553367614746\n",
      "28: Encoding Loss -1.7550792694091797, Transition Loss -8.308984756469727, Classifier Loss 0.04522530362010002, Total Loss 4.52086877822876\n",
      "28: Encoding Loss -2.041870355606079, Transition Loss -8.427417755126953, Classifier Loss 0.06380302459001541, Total Loss 6.378616809844971\n",
      "28: Encoding Loss -0.7462769746780396, Transition Loss -13.868364334106445, Classifier Loss 0.07203705608844757, Total Loss 7.200932025909424\n",
      "28: Encoding Loss -1.2796967029571533, Transition Loss -13.44310474395752, Classifier Loss 0.08136291801929474, Total Loss 8.1336030960083\n",
      "28: Encoding Loss -1.279801845550537, Transition Loss -1.0241780281066895, Classifier Loss 0.04650971665978432, Total Loss 4.650766849517822\n",
      "28: Encoding Loss -2.0555055141448975, Transition Loss -17.247297286987305, Classifier Loss 0.05072145164012909, Total Loss 5.068695545196533\n",
      "28: Encoding Loss -1.7439545392990112, Transition Loss -16.287010192871094, Classifier Loss 0.047543883323669434, Total Loss 4.751131057739258\n",
      "28: Encoding Loss -0.51960688829422, Transition Loss -0.6647855639457703, Classifier Loss 0.07114333659410477, Total Loss 7.114200115203857\n",
      "28: Encoding Loss -0.7239482998847961, Transition Loss 3.1449077129364014, Classifier Loss 0.066978320479393, Total Loss 7.326813697814941\n",
      "28: Encoding Loss -1.234022617340088, Transition Loss -11.348920822143555, Classifier Loss 0.05102882161736488, Total Loss 5.100612640380859\n",
      "28: Encoding Loss -1.3781343698501587, Transition Loss -10.41118049621582, Classifier Loss 0.056989636272192, Total Loss 5.696881294250488\n",
      "28: Encoding Loss -2.564283847808838, Transition Loss -12.526017189025879, Classifier Loss 0.06897854059934616, Total Loss 6.89534854888916\n",
      "28: Encoding Loss -1.165004014968872, Transition Loss -6.022838592529297, Classifier Loss 0.06711749732494354, Total Loss 6.710545063018799\n",
      "28: Encoding Loss -2.5661330223083496, Transition Loss -14.291200637817383, Classifier Loss 0.049606017768383026, Total Loss 4.9577436447143555\n",
      "28: Encoding Loss -1.5115294456481934, Transition Loss -8.318540573120117, Classifier Loss 0.08243235945701599, Total Loss 8.241571426391602\n",
      "28: Encoding Loss -1.1459050178527832, Transition Loss -11.36165714263916, Classifier Loss 0.11123904585838318, Total Loss 11.121631622314453\n",
      "28: Encoding Loss -2.653951406478882, Transition Loss -19.286357879638672, Classifier Loss 0.06525792926549911, Total Loss 6.521935939788818\n",
      "28: Encoding Loss -2.0414156913757324, Transition Loss -7.390789031982422, Classifier Loss 0.07994920015335083, Total Loss 7.993441581726074\n",
      "28: Encoding Loss -1.7281872034072876, Transition Loss -9.479413986206055, Classifier Loss 0.12133529782295227, Total Loss 12.131633758544922\n",
      "28: Encoding Loss -1.7464962005615234, Transition Loss -15.520965576171875, Classifier Loss 0.09502044320106506, Total Loss 9.498940467834473\n",
      "28: Encoding Loss -2.3524699211120605, Transition Loss -11.779241561889648, Classifier Loss 0.03289381042122841, Total Loss 3.287025213241577\n",
      "28: Encoding Loss -1.262135624885559, Transition Loss -10.874608039855957, Classifier Loss 0.07007555663585663, Total Loss 7.005380630493164\n",
      "28: Encoding Loss -2.040804147720337, Transition Loss -4.204049587249756, Classifier Loss 0.052285086363554, Total Loss 5.227667808532715\n",
      "28: Encoding Loss -1.105417251586914, Transition Loss -7.668277263641357, Classifier Loss 0.07228528708219528, Total Loss 7.22699499130249\n",
      "28: Encoding Loss -1.4190784692764282, Transition Loss -8.475786209106445, Classifier Loss 0.06523804366588593, Total Loss 6.522109031677246\n",
      "28: Encoding Loss -1.226585865020752, Transition Loss -3.9127748012542725, Classifier Loss 0.04842572659254074, Total Loss 4.841790199279785\n",
      "28: Encoding Loss -1.146543264389038, Transition Loss -9.680341720581055, Classifier Loss 0.03516403213143349, Total Loss 3.514467239379883\n",
      "28: Encoding Loss -1.5216549634933472, Transition Loss -11.615649223327637, Classifier Loss 0.0775861144065857, Total Loss 7.756288528442383\n",
      "28: Encoding Loss -0.6380923986434937, Transition Loss -11.78536319732666, Classifier Loss 0.07162050902843475, Total Loss 7.159693717956543\n",
      "28: Encoding Loss -1.4074240922927856, Transition Loss 0.5339407920837402, Classifier Loss 0.10754977911710739, Total Loss 10.861766815185547\n",
      "28: Encoding Loss -1.0380908250808716, Transition Loss -7.604074478149414, Classifier Loss 0.04535512626171112, Total Loss 4.533991813659668\n",
      "28: Encoding Loss -0.3344387710094452, Transition Loss -10.747848510742188, Classifier Loss 0.08896715193986893, Total Loss 8.893462181091309\n",
      "28: Encoding Loss -0.46759989857673645, Transition Loss -8.762784004211426, Classifier Loss 0.06782602518796921, Total Loss 6.780844688415527\n",
      "28: Encoding Loss -1.4383544921875, Transition Loss -8.321565628051758, Classifier Loss 0.055312380194664, Total Loss 5.529573917388916\n",
      "28: Encoding Loss -2.1759438514709473, Transition Loss -4.20389461517334, Classifier Loss 0.09226241707801819, Total Loss 9.225400924682617\n",
      "28: Encoding Loss -2.291816473007202, Transition Loss -13.112258911132812, Classifier Loss 0.0449020080268383, Total Loss 4.487578392028809\n",
      "28: Encoding Loss -0.09366384893655777, Transition Loss -0.9616366624832153, Classifier Loss 0.06411633640527725, Total Loss 6.280707359313965\n",
      "28: Encoding Loss -0.917586624622345, Transition Loss -6.885284900665283, Classifier Loss 0.04155440628528595, Total Loss 4.154063701629639\n",
      "28: Encoding Loss -0.46696600317955017, Transition Loss -6.098069190979004, Classifier Loss 0.08915401995182037, Total Loss 8.914176940917969\n",
      "28: Encoding Loss -0.5134567022323608, Transition Loss -15.275430679321289, Classifier Loss 0.1514899730682373, Total Loss 15.145941734313965\n",
      "28: Encoding Loss -2.234257698059082, Transition Loss -15.253265380859375, Classifier Loss 0.08601145446300507, Total Loss 8.598094940185547\n",
      "28: Encoding Loss -2.6685900688171387, Transition Loss -21.006254196166992, Classifier Loss 0.05142740160226822, Total Loss 5.138538837432861\n",
      "28: Encoding Loss -0.8384324312210083, Transition Loss -10.743534088134766, Classifier Loss 0.09827984869480133, Total Loss 9.825836181640625\n",
      "28: Encoding Loss -1.7728787660598755, Transition Loss -11.996459007263184, Classifier Loss 0.06666190922260284, Total Loss 6.663791656494141\n",
      "28: Encoding Loss -2.6655161380767822, Transition Loss -18.452653884887695, Classifier Loss 0.06815015524625778, Total Loss 6.811324596405029\n",
      "28: Encoding Loss -1.9254119396209717, Transition Loss -5.9451165199279785, Classifier Loss 0.09318604320287704, Total Loss 9.317415237426758\n",
      "28: Encoding Loss -0.7965028882026672, Transition Loss -15.05783462524414, Classifier Loss 0.0932176262140274, Total Loss 9.318751335144043\n",
      "28: Encoding Loss -1.3465912342071533, Transition Loss -15.794743537902832, Classifier Loss 0.02609023079276085, Total Loss 2.6058640480041504\n",
      "28: Encoding Loss -1.8028310537338257, Transition Loss -19.104263305664062, Classifier Loss 0.06468702852725983, Total Loss 6.464881896972656\n",
      "28: Encoding Loss -2.101656913757324, Transition Loss -24.877519607543945, Classifier Loss 0.029537424445152283, Total Loss 2.9487669467926025\n",
      "29: Encoding Loss -1.1921721696853638, Transition Loss -3.937417984008789, Classifier Loss 0.040441885590553284, Total Loss 4.04340124130249\n",
      "29: Encoding Loss -1.7347619533538818, Transition Loss -4.56590461730957, Classifier Loss 0.11697393655776978, Total Loss 11.696479797363281\n",
      "29: Encoding Loss -1.5456212759017944, Transition Loss 0.583666980266571, Classifier Loss 0.045578137040138245, Total Loss 4.67454719543457\n",
      "29: Encoding Loss -1.4830242395401, Transition Loss -17.93414878845215, Classifier Loss 0.0707528367638588, Total Loss 7.071696758270264\n",
      "29: Encoding Loss -1.0901710987091064, Transition Loss -7.936269760131836, Classifier Loss 0.05407083034515381, Total Loss 5.405495643615723\n",
      "29: Encoding Loss -1.4334195852279663, Transition Loss -14.708587646484375, Classifier Loss 0.0585126131772995, Total Loss 5.8483195304870605\n",
      "29: Encoding Loss -2.020413398742676, Transition Loss -10.599885940551758, Classifier Loss 0.07195394486188889, Total Loss 7.19327449798584\n",
      "29: Encoding Loss -1.2793619632720947, Transition Loss -8.105472564697266, Classifier Loss 0.054657772183418274, Total Loss 5.464156150817871\n",
      "29: Encoding Loss -1.1049505472183228, Transition Loss -14.75147533416748, Classifier Loss 0.047236502170562744, Total Loss 4.720699787139893\n",
      "29: Encoding Loss -2.090832233428955, Transition Loss -11.788493156433105, Classifier Loss 0.07629624754190445, Total Loss 7.627267360687256\n",
      "29: Encoding Loss 0.07884373515844345, Transition Loss -7.9322190284729, Classifier Loss 0.06186356395483017, Total Loss 6.679769515991211\n",
      "29: Encoding Loss 0.7821032404899597, Transition Loss -2.9814515113830566, Classifier Loss 0.07453882694244385, Total Loss 13.710112571716309\n",
      "29: Encoding Loss -1.265706181526184, Transition Loss -8.20801830291748, Classifier Loss 0.05614180117845535, Total Loss 5.6125383377075195\n",
      "29: Encoding Loss -0.5625712275505066, Transition Loss -8.3922119140625, Classifier Loss 0.07295715063810349, Total Loss 7.294036388397217\n",
      "29: Encoding Loss -2.207362413406372, Transition Loss -16.9278507232666, Classifier Loss 0.05437600612640381, Total Loss 5.434215068817139\n",
      "29: Encoding Loss -1.9573750495910645, Transition Loss -21.492090225219727, Classifier Loss 0.060756731778383255, Total Loss 6.071374893188477\n",
      "29: Encoding Loss -0.2760278582572937, Transition Loss -11.74896240234375, Classifier Loss 0.029531698673963547, Total Loss 2.944443464279175\n",
      "29: Encoding Loss 0.03729686886072159, Transition Loss -7.5522847175598145, Classifier Loss 0.05852791666984558, Total Loss 6.043856620788574\n",
      "29: Encoding Loss -0.25438976287841797, Transition Loss -12.891664505004883, Classifier Loss 0.09234565496444702, Total Loss 9.220831871032715\n",
      "29: Encoding Loss -0.9634994864463806, Transition Loss -6.229625701904297, Classifier Loss 0.05995943397283554, Total Loss 5.994697570800781\n",
      "29: Encoding Loss -1.3780646324157715, Transition Loss -14.643478393554688, Classifier Loss 0.08425609767436981, Total Loss 8.422680854797363\n",
      "29: Encoding Loss -1.6100964546203613, Transition Loss -11.112983703613281, Classifier Loss 0.05047069862484932, Total Loss 5.04484748840332\n",
      "29: Encoding Loss -1.3799967765808105, Transition Loss -12.168497085571289, Classifier Loss 0.08243752270936966, Total Loss 8.241318702697754\n",
      "29: Encoding Loss -1.3857630491256714, Transition Loss -4.444478511810303, Classifier Loss 0.08794252574443817, Total Loss 8.793363571166992\n",
      "29: Encoding Loss -2.134155750274658, Transition Loss -11.793416023254395, Classifier Loss 0.038112130016088486, Total Loss 3.808854341506958\n",
      "29: Encoding Loss -0.8931201100349426, Transition Loss -5.532524108886719, Classifier Loss 0.07302579283714294, Total Loss 7.3014726638793945\n",
      "29: Encoding Loss -1.9400362968444824, Transition Loss -4.336882591247559, Classifier Loss 0.10413233935832977, Total Loss 10.412365913391113\n",
      "29: Encoding Loss -3.1330199241638184, Transition Loss -18.232040405273438, Classifier Loss 0.08243338763713837, Total Loss 8.239691734313965\n",
      "29: Encoding Loss -2.398895502090454, Transition Loss -9.601706504821777, Classifier Loss 0.06708969175815582, Total Loss 6.7070488929748535\n",
      "29: Encoding Loss -0.9397701025009155, Transition Loss -5.718021392822266, Classifier Loss 0.07512497156858444, Total Loss 7.511353492736816\n",
      "29: Encoding Loss -0.9597969651222229, Transition Loss -11.319818496704102, Classifier Loss 0.08232016116380692, Total Loss 8.229752540588379\n",
      "29: Encoding Loss -1.3972053527832031, Transition Loss -3.7151927947998047, Classifier Loss 0.09170050173997879, Total Loss 9.169307708740234\n",
      "29: Encoding Loss -0.6229473352432251, Transition Loss -10.92098617553711, Classifier Loss 0.07512661069631577, Total Loss 7.510476589202881\n",
      "29: Encoding Loss -0.5307440757751465, Transition Loss -6.226572513580322, Classifier Loss 0.17487448453903198, Total Loss 17.486202239990234\n",
      "29: Encoding Loss -2.0820345878601074, Transition Loss -12.49064826965332, Classifier Loss 0.04877805337309837, Total Loss 4.875307083129883\n",
      "29: Encoding Loss -1.2079282999038696, Transition Loss -7.5736002922058105, Classifier Loss 0.04363108426332474, Total Loss 4.361593723297119\n",
      "29: Encoding Loss -1.3479939699172974, Transition Loss -7.051609039306641, Classifier Loss 0.08207669109106064, Total Loss 8.206258773803711\n",
      "29: Encoding Loss -1.0169363021850586, Transition Loss -9.063291549682617, Classifier Loss 0.1157790794968605, Total Loss 11.576094627380371\n",
      "29: Encoding Loss -1.5620173215866089, Transition Loss -9.670737266540527, Classifier Loss 0.07143138349056244, Total Loss 7.141204357147217\n",
      "29: Encoding Loss -1.5861681699752808, Transition Loss -11.072757720947266, Classifier Loss 0.07467213273048401, Total Loss 7.464998722076416\n",
      "29: Encoding Loss -1.4967262744903564, Transition Loss -7.887610912322998, Classifier Loss 0.0413004495203495, Total Loss 4.128467559814453\n",
      "29: Encoding Loss -1.7423503398895264, Transition Loss -15.281532287597656, Classifier Loss 0.09157449007034302, Total Loss 9.15439224243164\n",
      "29: Encoding Loss -2.136431932449341, Transition Loss -14.892498016357422, Classifier Loss 0.03598855435848236, Total Loss 3.595876932144165\n",
      "29: Encoding Loss -1.9866489171981812, Transition Loss -5.616959095001221, Classifier Loss 0.0715404823422432, Total Loss 7.15292501449585\n",
      "29: Encoding Loss -1.7638649940490723, Transition Loss -15.912012100219727, Classifier Loss 0.05225815996527672, Total Loss 5.222633361816406\n",
      "29: Encoding Loss -0.015919717028737068, Transition Loss -7.128801345825195, Classifier Loss 0.0892413854598999, Total Loss 8.86708927154541\n",
      "29: Encoding Loss -0.1939016878604889, Transition Loss -17.448909759521484, Classifier Loss 0.07217130064964294, Total Loss 7.172921180725098\n",
      "29: Encoding Loss -1.8034206628799438, Transition Loss -6.3990020751953125, Classifier Loss 0.0373055636882782, Total Loss 3.729276657104492\n",
      "29: Encoding Loss -2.0699222087860107, Transition Loss -9.450577735900879, Classifier Loss 0.06485649943351746, Total Loss 6.483759880065918\n",
      "29: Encoding Loss -1.4816688299179077, Transition Loss -16.37080955505371, Classifier Loss 0.05805790051817894, Total Loss 5.802515983581543\n",
      "29: Encoding Loss -2.120668888092041, Transition Loss -19.486635208129883, Classifier Loss 0.05779430642724037, Total Loss 5.775533676147461\n",
      "29: Encoding Loss -0.4348735511302948, Transition Loss -12.445564270019531, Classifier Loss 0.07163895666599274, Total Loss 7.161382675170898\n",
      "29: Encoding Loss -1.7521053552627563, Transition Loss -3.1653406620025635, Classifier Loss 0.08921235799789429, Total Loss 8.920602798461914\n",
      "29: Encoding Loss -1.8185335397720337, Transition Loss -12.275554656982422, Classifier Loss 0.05291597545146942, Total Loss 5.28914213180542\n",
      "29: Encoding Loss -0.9141913056373596, Transition Loss -11.969411849975586, Classifier Loss 0.0775427296757698, Total Loss 7.7518792152404785\n",
      "29: Encoding Loss -1.4639877080917358, Transition Loss -5.105981349945068, Classifier Loss 0.09298601746559143, Total Loss 9.29758071899414\n",
      "29: Encoding Loss -0.6461694240570068, Transition Loss -11.339797019958496, Classifier Loss 0.03771567344665527, Total Loss 3.769299268722534\n",
      "29: Encoding Loss -2.2666962146759033, Transition Loss -12.94299602508545, Classifier Loss 0.06589123606681824, Total Loss 6.5865349769592285\n",
      "29: Encoding Loss -2.4869608879089355, Transition Loss -19.657066345214844, Classifier Loss 0.049137018620967865, Total Loss 4.909770488739014\n",
      "29: Encoding Loss -1.9292254447937012, Transition Loss -16.499122619628906, Classifier Loss 0.07467841356992722, Total Loss 7.464541435241699\n",
      "29: Encoding Loss -1.7636550664901733, Transition Loss -9.469256401062012, Classifier Loss 0.06762951612472534, Total Loss 6.7610578536987305\n",
      "29: Encoding Loss -2.1566200256347656, Transition Loss -7.932805061340332, Classifier Loss 0.11333901435136795, Total Loss 11.332314491271973\n",
      "29: Encoding Loss -1.9745509624481201, Transition Loss -10.011434555053711, Classifier Loss 0.06537778675556183, Total Loss 6.535776615142822\n",
      "29: Encoding Loss -1.4098306894302368, Transition Loss -7.290355205535889, Classifier Loss 0.07919631898403168, Total Loss 7.918173789978027\n",
      "29: Encoding Loss -1.8032886981964111, Transition Loss -11.801766395568848, Classifier Loss 0.06963148713111877, Total Loss 6.960788249969482\n",
      "29: Encoding Loss -2.1532745361328125, Transition Loss -4.8235039710998535, Classifier Loss 0.11565106362104416, Total Loss 11.564141273498535\n",
      "29: Encoding Loss -0.453933447599411, Transition Loss -1.3895528316497803, Classifier Loss 0.06539665907621384, Total Loss 6.539377212524414\n",
      "29: Encoding Loss -2.2862837314605713, Transition Loss -18.117712020874023, Classifier Loss 0.08474583178758621, Total Loss 8.470959663391113\n",
      "29: Encoding Loss -0.03868508338928223, Transition Loss -8.426149368286133, Classifier Loss 0.07134785503149033, Total Loss 7.024957656860352\n",
      "29: Encoding Loss -0.8862939476966858, Transition Loss -8.919788360595703, Classifier Loss 0.11640344560146332, Total Loss 11.63856029510498\n",
      "29: Encoding Loss -1.6702053546905518, Transition Loss -8.676934242248535, Classifier Loss 0.08552691340446472, Total Loss 8.550955772399902\n",
      "29: Encoding Loss -1.4694101810455322, Transition Loss -8.824142456054688, Classifier Loss 0.0771418958902359, Total Loss 7.7124247550964355\n",
      "29: Encoding Loss -0.6902447938919067, Transition Loss -5.355888843536377, Classifier Loss 0.1072995588183403, Total Loss 10.72888469696045\n",
      "29: Encoding Loss -1.7179837226867676, Transition Loss -8.914234161376953, Classifier Loss 0.028250455856323242, Total Loss 2.8232626914978027\n",
      "29: Encoding Loss -1.9759193658828735, Transition Loss -7.766425609588623, Classifier Loss 0.115611232817173, Total Loss 11.559569358825684\n",
      "29: Encoding Loss -1.6224812269210815, Transition Loss -14.263023376464844, Classifier Loss 0.045669276267290115, Total Loss 4.564074993133545\n",
      "29: Encoding Loss -1.5263694524765015, Transition Loss -6.120234966278076, Classifier Loss 0.08811017125844955, Total Loss 8.809792518615723\n",
      "29: Encoding Loss -2.7212038040161133, Transition Loss -12.082653045654297, Classifier Loss 0.06831061840057373, Total Loss 6.8286452293396\n",
      "29: Encoding Loss -1.7852561473846436, Transition Loss -13.218729972839355, Classifier Loss 0.03694206476211548, Total Loss 3.6915626525878906\n",
      "29: Encoding Loss -1.6523184776306152, Transition Loss -9.86582088470459, Classifier Loss 0.11385660618543625, Total Loss 11.383687019348145\n",
      "29: Encoding Loss -0.9431850910186768, Transition Loss -10.809781074523926, Classifier Loss 0.064722940325737, Total Loss 6.470131874084473\n",
      "29: Encoding Loss -1.744964838027954, Transition Loss -8.400390625, Classifier Loss 0.05743631720542908, Total Loss 5.741951942443848\n",
      "29: Encoding Loss -1.1609753370285034, Transition Loss -4.886725425720215, Classifier Loss 0.09968171268701553, Total Loss 9.967193603515625\n",
      "29: Encoding Loss -0.430631160736084, Transition Loss -10.708515167236328, Classifier Loss 0.0979544073343277, Total Loss 9.793270111083984\n",
      "29: Encoding Loss -2.7203447818756104, Transition Loss -7.484117031097412, Classifier Loss 0.06216133013367653, Total Loss 6.214636325836182\n",
      "29: Encoding Loss -1.5270262956619263, Transition Loss -5.218218803405762, Classifier Loss 0.07151221483945847, Total Loss 7.150177478790283\n",
      "29: Encoding Loss -1.9871478080749512, Transition Loss -15.452759742736816, Classifier Loss 0.07147945463657379, Total Loss 7.14485502243042\n",
      "29: Encoding Loss -1.5925402641296387, Transition Loss -9.415323257446289, Classifier Loss 0.11194560676813126, Total Loss 11.19267749786377\n",
      "29: Encoding Loss -1.066794991493225, Transition Loss -7.4967546463012695, Classifier Loss 0.05241062119603157, Total Loss 5.23956298828125\n",
      "29: Encoding Loss -0.6601904034614563, Transition Loss -13.1077880859375, Classifier Loss 0.06392259895801544, Total Loss 6.389638423919678\n",
      "29: Encoding Loss -3.4314587116241455, Transition Loss -21.638324737548828, Classifier Loss 0.06018781661987305, Total Loss 6.014453887939453\n",
      "29: Encoding Loss -1.189135193824768, Transition Loss -6.874928951263428, Classifier Loss 0.09416134655475616, Total Loss 9.414759635925293\n",
      "29: Encoding Loss -1.6116788387298584, Transition Loss -7.380686283111572, Classifier Loss 0.0781552642583847, Total Loss 7.814050197601318\n",
      "29: Encoding Loss -1.0273792743682861, Transition Loss -9.398970603942871, Classifier Loss 0.07811129093170166, Total Loss 7.809249401092529\n",
      "29: Encoding Loss -1.0028817653656006, Transition Loss -3.6064462661743164, Classifier Loss 0.07581424713134766, Total Loss 7.580703258514404\n",
      "29: Encoding Loss -1.7444030046463013, Transition Loss -12.785717010498047, Classifier Loss 0.0818069726228714, Total Loss 8.178140640258789\n",
      "29: Encoding Loss -1.4731417894363403, Transition Loss -15.787739753723145, Classifier Loss 0.08891839534044266, Total Loss 8.88868236541748\n",
      "29: Encoding Loss -2.2073378562927246, Transition Loss -12.844552040100098, Classifier Loss 0.05755169317126274, Total Loss 5.75260066986084\n",
      "29: Encoding Loss -1.3426185846328735, Transition Loss -4.0007004737854, Classifier Loss 0.03962940722703934, Total Loss 3.9621405601501465\n",
      "29: Encoding Loss -1.8657382726669312, Transition Loss -6.791691780090332, Classifier Loss 0.11527764052152634, Total Loss 11.526406288146973\n",
      "29: Encoding Loss -1.8468559980392456, Transition Loss -14.01284408569336, Classifier Loss 0.06726181507110596, Total Loss 6.723379135131836\n",
      "29: Encoding Loss -0.5848358273506165, Transition Loss 0.6057695746421814, Classifier Loss 0.11158129572868347, Total Loss 11.27928352355957\n",
      "29: Encoding Loss -0.8569021224975586, Transition Loss -7.782758712768555, Classifier Loss 0.0737600177526474, Total Loss 7.37444543838501\n",
      "29: Encoding Loss -0.36667728424072266, Transition Loss -4.495909690856934, Classifier Loss 0.09025374799966812, Total Loss 9.024114608764648\n",
      "29: Encoding Loss -1.3125572204589844, Transition Loss -11.674599647521973, Classifier Loss 0.057042695581912994, Total Loss 5.701934337615967\n",
      "29: Encoding Loss -1.8651269674301147, Transition Loss -15.377748489379883, Classifier Loss 0.09108859300613403, Total Loss 9.105783462524414\n",
      "29: Encoding Loss -1.1665812730789185, Transition Loss -4.28021240234375, Classifier Loss 0.08996742963790894, Total Loss 8.99588680267334\n",
      "29: Encoding Loss -1.6101210117340088, Transition Loss -8.55980396270752, Classifier Loss 0.045036472380161285, Total Loss 4.5019354820251465\n",
      "29: Encoding Loss -1.8764152526855469, Transition Loss -8.748549461364746, Classifier Loss 0.06349097192287445, Total Loss 6.347347736358643\n",
      "29: Encoding Loss -0.7793546319007874, Transition Loss -13.984026908874512, Classifier Loss 0.06851042062044144, Total Loss 6.848245620727539\n",
      "29: Encoding Loss -1.1233272552490234, Transition Loss -13.561857223510742, Classifier Loss 0.07878770679235458, Total Loss 7.876058578491211\n",
      "29: Encoding Loss -1.0138466358184814, Transition Loss -1.360752820968628, Classifier Loss 0.045298174023628235, Total Loss 4.529545307159424\n",
      "29: Encoding Loss -1.6053550243377686, Transition Loss -17.392427444458008, Classifier Loss 0.048741910606622696, Total Loss 4.870712757110596\n",
      "29: Encoding Loss -1.8695311546325684, Transition Loss -16.570154190063477, Classifier Loss 0.0452914759516716, Total Loss 4.525833606719971\n",
      "29: Encoding Loss -0.5006753206253052, Transition Loss -0.7064965963363647, Classifier Loss 0.06920018792152405, Total Loss 6.919876575469971\n",
      "29: Encoding Loss -0.6027358770370483, Transition Loss 2.721632719039917, Classifier Loss 0.06268385797739029, Total Loss 6.812712669372559\n",
      "29: Encoding Loss -1.1033071279525757, Transition Loss -11.496155738830566, Classifier Loss 0.04950292035937309, Total Loss 4.94799280166626\n",
      "29: Encoding Loss -1.3388278484344482, Transition Loss -10.503180503845215, Classifier Loss 0.05626947060227394, Total Loss 5.624846458435059\n",
      "29: Encoding Loss -2.536703586578369, Transition Loss -12.775453567504883, Classifier Loss 0.06639562547206879, Total Loss 6.637007713317871\n",
      "29: Encoding Loss -0.9750227332115173, Transition Loss -6.040172100067139, Classifier Loss 0.06662408262491226, Total Loss 6.661200523376465\n",
      "29: Encoding Loss -2.4794602394104004, Transition Loss -14.542241096496582, Classifier Loss 0.047735296189785004, Total Loss 4.770621299743652\n",
      "29: Encoding Loss -1.1460602283477783, Transition Loss -8.312910079956055, Classifier Loss 0.08059092611074448, Total Loss 8.057430267333984\n",
      "29: Encoding Loss -0.9770089387893677, Transition Loss -11.681841850280762, Classifier Loss 0.10712327063083649, Total Loss 10.709990501403809\n",
      "29: Encoding Loss -2.6028220653533936, Transition Loss -19.396148681640625, Classifier Loss 0.0643896758556366, Total Loss 6.435088634490967\n",
      "29: Encoding Loss -1.8490900993347168, Transition Loss -7.878077507019043, Classifier Loss 0.08066551387310028, Total Loss 8.06497573852539\n",
      "29: Encoding Loss -1.726791262626648, Transition Loss -9.605180740356445, Classifier Loss 0.11971597373485565, Total Loss 11.969676971435547\n",
      "29: Encoding Loss -1.6030585765838623, Transition Loss -15.435190200805664, Classifier Loss 0.08990294486284256, Total Loss 8.987207412719727\n",
      "29: Encoding Loss -2.2695655822753906, Transition Loss -12.03939151763916, Classifier Loss 0.03194520249962807, Total Loss 3.192112445831299\n",
      "29: Encoding Loss -1.0528035163879395, Transition Loss -11.153022766113281, Classifier Loss 0.06555173546075821, Total Loss 6.552942752838135\n",
      "29: Encoding Loss -2.048429489135742, Transition Loss -4.518635272979736, Classifier Loss 0.05102299153804779, Total Loss 5.101395606994629\n",
      "29: Encoding Loss -0.9042468667030334, Transition Loss -8.051912307739258, Classifier Loss 0.07059694081544876, Total Loss 7.058084011077881\n",
      "29: Encoding Loss -1.306834101676941, Transition Loss -8.742559432983398, Classifier Loss 0.06322477757930756, Total Loss 6.3207292556762695\n",
      "29: Encoding Loss -1.1240031719207764, Transition Loss -4.218323707580566, Classifier Loss 0.04591364040970802, Total Loss 4.59052038192749\n",
      "29: Encoding Loss -1.0735023021697998, Transition Loss -9.858325958251953, Classifier Loss 0.03346475213766098, Total Loss 3.344503402709961\n",
      "29: Encoding Loss -1.3421075344085693, Transition Loss -11.71574592590332, Classifier Loss 0.07968418300151825, Total Loss 7.9660749435424805\n",
      "29: Encoding Loss -0.486478716135025, Transition Loss -11.982620239257812, Classifier Loss 0.07032755017280579, Total Loss 7.030355930328369\n",
      "29: Encoding Loss -1.3688074350357056, Transition Loss 0.0711970329284668, Classifier Loss 0.10510941594839096, Total Loss 10.52518081665039\n",
      "29: Encoding Loss -1.0977733135223389, Transition Loss -8.012237548828125, Classifier Loss 0.04439566284418106, Total Loss 4.437963485717773\n",
      "29: Encoding Loss -0.36028215289115906, Transition Loss -10.978775024414062, Classifier Loss 0.08636226505041122, Total Loss 8.633577346801758\n",
      "29: Encoding Loss -0.34183186292648315, Transition Loss -9.11154842376709, Classifier Loss 0.06505769491195679, Total Loss 6.503085613250732\n",
      "29: Encoding Loss -1.2158610820770264, Transition Loss -8.813944816589355, Classifier Loss 0.05830269679427147, Total Loss 5.828506946563721\n",
      "29: Encoding Loss -2.126957416534424, Transition Loss -4.693216323852539, Classifier Loss 0.09121756255626678, Total Loss 9.120818138122559\n",
      "29: Encoding Loss -2.1087639331817627, Transition Loss -13.57746696472168, Classifier Loss 0.045514918863773346, Total Loss 4.548776149749756\n",
      "29: Encoding Loss -0.031328268349170685, Transition Loss -1.144504189491272, Classifier Loss 0.06263361871242523, Total Loss 6.168638706207275\n",
      "29: Encoding Loss -0.7190446853637695, Transition Loss -7.354982376098633, Classifier Loss 0.04066675528883934, Total Loss 4.065204620361328\n",
      "29: Encoding Loss -0.02739112265408039, Transition Loss -6.508288383483887, Classifier Loss 0.09218915551900864, Total Loss 9.131698608398438\n",
      "29: Encoding Loss -0.8988975882530212, Transition Loss -15.770914077758789, Classifier Loss 0.137308269739151, Total Loss 13.727673530578613\n",
      "29: Encoding Loss -1.956211805343628, Transition Loss -15.676248550415039, Classifier Loss 0.08789298683404922, Total Loss 8.786163330078125\n",
      "29: Encoding Loss -2.478703737258911, Transition Loss -21.41332244873047, Classifier Loss 0.052192989736795425, Total Loss 5.2150163650512695\n",
      "29: Encoding Loss -0.6238166093826294, Transition Loss -11.13728141784668, Classifier Loss 0.1001477837562561, Total Loss 10.012550354003906\n",
      "29: Encoding Loss -1.2578648328781128, Transition Loss -12.310832023620605, Classifier Loss 0.06614591181278229, Total Loss 6.612128734588623\n",
      "29: Encoding Loss -2.456057071685791, Transition Loss -18.758390426635742, Classifier Loss 0.06956547498703003, Total Loss 6.95279598236084\n",
      "29: Encoding Loss -1.8698478937149048, Transition Loss -6.265265464782715, Classifier Loss 0.08964820206165314, Total Loss 8.963566780090332\n",
      "29: Encoding Loss -0.11751554161310196, Transition Loss -15.234537124633789, Classifier Loss 0.09258922189474106, Total Loss 9.143092155456543\n",
      "29: Encoding Loss 0.9233062267303467, Transition Loss -15.369104385375977, Classifier Loss 0.030364301055669785, Total Loss 10.419806480407715\n",
      "29: Encoding Loss 5.783446311950684, Transition Loss -21.335662841796875, Classifier Loss 0.07170546054840088, Total Loss 53.43385314941406\n",
      "29: Encoding Loss 0.20459525287151337, Transition Loss -25.687026977539062, Classifier Loss 0.037034764885902405, Total Loss 5.301743030548096\n",
      "30: Encoding Loss -0.5808743238449097, Transition Loss -2.543933868408203, Classifier Loss 0.03586011379957199, Total Loss 3.5855026245117188\n",
      "30: Encoding Loss -0.6507371664047241, Transition Loss -3.5728366374969482, Classifier Loss 0.11550652980804443, Total Loss 11.549939155578613\n",
      "30: Encoding Loss -0.7809667587280273, Transition Loss 1.4531738758087158, Classifier Loss 0.04157590493559837, Total Loss 4.448225021362305\n",
      "30: Encoding Loss -0.985436201095581, Transition Loss -15.961458206176758, Classifier Loss 0.06608511507511139, Total Loss 6.605319023132324\n",
      "30: Encoding Loss -1.067370891571045, Transition Loss -6.864375114440918, Classifier Loss 0.05376084893941879, Total Loss 5.374711990356445\n",
      "30: Encoding Loss -0.8274498581886292, Transition Loss -12.886429786682129, Classifier Loss 0.05627507343888283, Total Loss 5.624929904937744\n",
      "30: Encoding Loss -0.8519030809402466, Transition Loss -9.084662437438965, Classifier Loss 0.07063303887844086, Total Loss 7.061487197875977\n",
      "30: Encoding Loss -1.0821386575698853, Transition Loss -6.785841941833496, Classifier Loss 0.05532543733716011, Total Loss 5.531186580657959\n",
      "30: Encoding Loss -0.3758973479270935, Transition Loss -13.279380798339844, Classifier Loss 0.04752689599990845, Total Loss 4.749776840209961\n",
      "30: Encoding Loss -1.2452569007873535, Transition Loss -10.667438507080078, Classifier Loss 0.07359052449464798, Total Loss 7.356919288635254\n",
      "30: Encoding Loss -0.034834012389183044, Transition Loss -7.021798133850098, Classifier Loss 0.06092411279678345, Total Loss 5.989627838134766\n",
      "30: Encoding Loss -0.19695721566677094, Transition Loss -3.778162956237793, Classifier Loss 0.07093861699104309, Total Loss 7.054590702056885\n",
      "30: Encoding Loss -0.9549573659896851, Transition Loss -5.027984142303467, Classifier Loss 0.055027227848768234, Total Loss 5.5017170906066895\n",
      "30: Encoding Loss -0.31505346298217773, Transition Loss -5.797346591949463, Classifier Loss 0.06364066153764725, Total Loss 6.3608527183532715\n",
      "30: Encoding Loss -1.5619248151779175, Transition Loss -12.643987655639648, Classifier Loss 0.048963893204927444, Total Loss 4.893860816955566\n",
      "30: Encoding Loss -1.7224743366241455, Transition Loss -16.795574188232422, Classifier Loss 0.06595996767282486, Total Loss 6.592637538909912\n",
      "30: Encoding Loss -0.7995933890342712, Transition Loss -8.560755729675293, Classifier Loss 0.02842235565185547, Total Loss 2.8405234813690186\n",
      "30: Encoding Loss -0.0643119290471077, Transition Loss -5.267446041107178, Classifier Loss 0.05446061119437218, Total Loss 5.311201095581055\n",
      "30: Encoding Loss 0.12988322973251343, Transition Loss -12.187660217285156, Classifier Loss 0.09274496883153915, Total Loss 10.210334777832031\n",
      "30: Encoding Loss -0.33930161595344543, Transition Loss -5.953595161437988, Classifier Loss 0.06116000562906265, Total Loss 6.1138715744018555\n",
      "30: Encoding Loss -1.1249297857284546, Transition Loss -15.945985794067383, Classifier Loss 0.0796264261007309, Total Loss 7.959453582763672\n",
      "30: Encoding Loss -1.1623042821884155, Transition Loss -11.538069725036621, Classifier Loss 0.04536513239145279, Total Loss 4.534205913543701\n",
      "30: Encoding Loss -1.102256178855896, Transition Loss -12.61447525024414, Classifier Loss 0.07923541963100433, Total Loss 7.921019077301025\n",
      "30: Encoding Loss -0.848412275314331, Transition Loss -4.108428955078125, Classifier Loss 0.08266817033290863, Total Loss 8.265995025634766\n",
      "30: Encoding Loss -1.568131923675537, Transition Loss -12.461868286132812, Classifier Loss 0.03756637126207352, Total Loss 3.7541446685791016\n",
      "30: Encoding Loss -0.515717625617981, Transition Loss -5.322231769561768, Classifier Loss 0.07312285155057907, Total Loss 7.311220169067383\n",
      "30: Encoding Loss -1.420901894569397, Transition Loss -3.3884716033935547, Classifier Loss 0.09938964247703552, Total Loss 9.938285827636719\n",
      "30: Encoding Loss -2.2457988262176514, Transition Loss -19.99183464050293, Classifier Loss 0.07931791990995407, Total Loss 7.927793502807617\n",
      "30: Encoding Loss -1.6847116947174072, Transition Loss -10.178045272827148, Classifier Loss 0.0659588873386383, Total Loss 6.593852996826172\n",
      "30: Encoding Loss -0.5233302116394043, Transition Loss -5.454880714416504, Classifier Loss 0.07373496145009995, Total Loss 7.3724045753479\n",
      "30: Encoding Loss -0.7465550899505615, Transition Loss -12.06861400604248, Classifier Loss 0.08012289553880692, Total Loss 8.009876251220703\n",
      "30: Encoding Loss -0.8769989013671875, Transition Loss -3.1916561126708984, Classifier Loss 0.08998043835163116, Total Loss 8.997406005859375\n",
      "30: Encoding Loss -0.31522056460380554, Transition Loss -11.611812591552734, Classifier Loss 0.07309694588184357, Total Loss 7.305329322814941\n",
      "30: Encoding Loss 0.006845248397439718, Transition Loss -6.326892375946045, Classifier Loss 0.17416983842849731, Total Loss 17.444595336914062\n",
      "30: Encoding Loss -1.7177135944366455, Transition Loss -13.13378620147705, Classifier Loss 0.04747755452990532, Total Loss 4.745128631591797\n",
      "30: Encoding Loss -0.916751503944397, Transition Loss -8.049132347106934, Classifier Loss 0.04569533094763756, Total Loss 4.567923069000244\n",
      "30: Encoding Loss -0.9351385831832886, Transition Loss -7.474917411804199, Classifier Loss 0.08121979236602783, Total Loss 8.1204833984375\n",
      "30: Encoding Loss -0.7232828140258789, Transition Loss -9.71457576751709, Classifier Loss 0.11689728498458862, Total Loss 11.687786102294922\n",
      "30: Encoding Loss -1.4429962635040283, Transition Loss -10.215483665466309, Classifier Loss 0.07357723265886307, Total Loss 7.355679988861084\n",
      "30: Encoding Loss -1.25179123878479, Transition Loss -11.569847106933594, Classifier Loss 0.07310827821493149, Total Loss 7.308513641357422\n",
      "30: Encoding Loss -1.0973260402679443, Transition Loss -8.214333534240723, Classifier Loss 0.0397912934422493, Total Loss 3.9774863719940186\n",
      "30: Encoding Loss -1.3784849643707275, Transition Loss -16.349763870239258, Classifier Loss 0.08970558643341064, Total Loss 8.96728801727295\n",
      "30: Encoding Loss -1.6920973062515259, Transition Loss -15.652766227722168, Classifier Loss 0.037506166845560074, Total Loss 3.7474863529205322\n",
      "30: Encoding Loss -1.615341305732727, Transition Loss -5.666275978088379, Classifier Loss 0.0690826028585434, Total Loss 6.9071269035339355\n",
      "30: Encoding Loss -1.5028902292251587, Transition Loss -16.980960845947266, Classifier Loss 0.052670158445835114, Total Loss 5.263619899749756\n",
      "30: Encoding Loss 0.10557102411985397, Transition Loss -7.416950225830078, Classifier Loss 0.0869632288813591, Total Loss 9.41648006439209\n",
      "30: Encoding Loss -0.09854412823915482, Transition Loss -17.09461784362793, Classifier Loss 0.07157185673713684, Total Loss 7.025892734527588\n",
      "30: Encoding Loss -1.2499300241470337, Transition Loss -5.893661975860596, Classifier Loss 0.03704893961548805, Total Loss 3.7037153244018555\n",
      "30: Encoding Loss -1.691139817237854, Transition Loss -8.919546127319336, Classifier Loss 0.06503507494926453, Total Loss 6.501723766326904\n",
      "30: Encoding Loss -1.2719751596450806, Transition Loss -15.894476890563965, Classifier Loss 0.054826829582452774, Total Loss 5.479504108428955\n",
      "30: Encoding Loss -1.7141551971435547, Transition Loss -19.239906311035156, Classifier Loss 0.05541281774640083, Total Loss 5.537433624267578\n",
      "30: Encoding Loss -0.29302647709846497, Transition Loss -12.03524398803711, Classifier Loss 0.06981896609067917, Total Loss 6.97551965713501\n",
      "30: Encoding Loss -1.3424408435821533, Transition Loss -2.727128028869629, Classifier Loss 0.08942411839962006, Total Loss 8.941865921020508\n",
      "30: Encoding Loss -1.6193604469299316, Transition Loss -11.7349271774292, Classifier Loss 0.05258586257696152, Total Loss 5.256239414215088\n",
      "30: Encoding Loss -0.9376453757286072, Transition Loss -11.56650447845459, Classifier Loss 0.07394228875637054, Total Loss 7.391915798187256\n",
      "30: Encoding Loss -1.1511363983154297, Transition Loss -4.547384262084961, Classifier Loss 0.09049825370311737, Total Loss 9.04891586303711\n",
      "30: Encoding Loss -0.5473126173019409, Transition Loss -10.827924728393555, Classifier Loss 0.03790229186415672, Total Loss 3.7880632877349854\n",
      "30: Encoding Loss -1.8646161556243896, Transition Loss -12.428934097290039, Classifier Loss 0.06397528946399689, Total Loss 6.39504337310791\n",
      "30: Encoding Loss -2.023860216140747, Transition Loss -19.314319610595703, Classifier Loss 0.04684940353035927, Total Loss 4.681077480316162\n",
      "30: Encoding Loss -1.522347092628479, Transition Loss -16.101099014282227, Classifier Loss 0.07092715799808502, Total Loss 7.089495658874512\n",
      "30: Encoding Loss -1.5498192310333252, Transition Loss -9.052536010742188, Classifier Loss 0.06669020652770996, Total Loss 6.667210102081299\n",
      "30: Encoding Loss -1.7985934019088745, Transition Loss -7.593990325927734, Classifier Loss 0.11137713491916656, Total Loss 11.136194229125977\n",
      "30: Encoding Loss -1.526662826538086, Transition Loss -9.467992782592773, Classifier Loss 0.06399068981409073, Total Loss 6.397175312042236\n",
      "30: Encoding Loss -1.0844088792800903, Transition Loss -6.883197784423828, Classifier Loss 0.07658720016479492, Total Loss 7.65734338760376\n",
      "30: Encoding Loss -1.4746830463409424, Transition Loss -11.501419067382812, Classifier Loss 0.07139655947685242, Total Loss 7.137355804443359\n",
      "30: Encoding Loss -1.6460238695144653, Transition Loss -4.344235420227051, Classifier Loss 0.11241337656974792, Total Loss 11.240468978881836\n",
      "30: Encoding Loss -0.1838071048259735, Transition Loss -0.9890594482421875, Classifier Loss 0.06431655585765839, Total Loss 6.382894515991211\n",
      "30: Encoding Loss -1.9388219118118286, Transition Loss -17.952974319458008, Classifier Loss 0.08119744062423706, Total Loss 8.116153717041016\n",
      "30: Encoding Loss 0.05166030302643776, Transition Loss -8.011327743530273, Classifier Loss 0.07147359102964401, Total Loss 7.433931827545166\n",
      "30: Encoding Loss -0.5625738501548767, Transition Loss -9.125844955444336, Classifier Loss 0.11785861104726791, Total Loss 11.784035682678223\n",
      "30: Encoding Loss -1.4867403507232666, Transition Loss -8.883055686950684, Classifier Loss 0.08702751994132996, Total Loss 8.70097541809082\n",
      "30: Encoding Loss -1.2217563390731812, Transition Loss -8.949137687683105, Classifier Loss 0.07634127140045166, Total Loss 7.6323370933532715\n",
      "30: Encoding Loss -0.6581737995147705, Transition Loss -5.49293327331543, Classifier Loss 0.10566265881061554, Total Loss 10.565167427062988\n",
      "30: Encoding Loss -1.2660250663757324, Transition Loss -8.979849815368652, Classifier Loss 0.02694002352654934, Total Loss 2.692206382751465\n",
      "30: Encoding Loss -1.7351316213607788, Transition Loss -7.886408805847168, Classifier Loss 0.11385024338960648, Total Loss 11.38344669342041\n",
      "30: Encoding Loss -1.2734707593917847, Transition Loss -14.370935440063477, Classifier Loss 0.04582880064845085, Total Loss 4.580005645751953\n",
      "30: Encoding Loss -1.5334699153900146, Transition Loss -6.550846099853516, Classifier Loss 0.0872863233089447, Total Loss 8.72732162475586\n",
      "30: Encoding Loss -2.345125436782837, Transition Loss -12.217965126037598, Classifier Loss 0.06621558219194412, Total Loss 6.619114398956299\n",
      "30: Encoding Loss -1.570128321647644, Transition Loss -13.409143447875977, Classifier Loss 0.03609539195895195, Total Loss 3.6068575382232666\n",
      "30: Encoding Loss -1.5066449642181396, Transition Loss -10.01885986328125, Classifier Loss 0.11438488960266113, Total Loss 11.436485290527344\n",
      "30: Encoding Loss -0.8377620577812195, Transition Loss -10.84639835357666, Classifier Loss 0.060378298163414, Total Loss 6.035660743713379\n",
      "30: Encoding Loss -1.6374937295913696, Transition Loss -8.646076202392578, Classifier Loss 0.05777904391288757, Total Loss 5.776175498962402\n",
      "30: Encoding Loss -1.113132357597351, Transition Loss -5.102102756500244, Classifier Loss 0.10061614215373993, Total Loss 10.060593605041504\n",
      "30: Encoding Loss -0.4060826599597931, Transition Loss -10.790040969848633, Classifier Loss 0.09500449150800705, Total Loss 9.498211860656738\n",
      "30: Encoding Loss -2.3776803016662598, Transition Loss -7.7174482345581055, Classifier Loss 0.060143955051898956, Total Loss 6.012852191925049\n",
      "30: Encoding Loss -1.368025541305542, Transition Loss -5.486263275146484, Classifier Loss 0.07125164568424225, Total Loss 7.124067306518555\n",
      "30: Encoding Loss -1.6474920511245728, Transition Loss -15.621549606323242, Classifier Loss 0.0721961259841919, Total Loss 7.216488361358643\n",
      "30: Encoding Loss -1.4648361206054688, Transition Loss -9.502968788146973, Classifier Loss 0.11274562776088715, Total Loss 11.272662162780762\n",
      "30: Encoding Loss -0.8059324622154236, Transition Loss -7.562615394592285, Classifier Loss 0.0512755922973156, Total Loss 5.126046657562256\n",
      "30: Encoding Loss -0.8104249238967896, Transition Loss -13.077127456665039, Classifier Loss 0.06464046984910965, Total Loss 6.461431503295898\n",
      "30: Encoding Loss -2.9284279346466064, Transition Loss -21.689186096191406, Classifier Loss 0.05899879336357117, Total Loss 5.895541667938232\n",
      "30: Encoding Loss -1.0325074195861816, Transition Loss -7.0092339515686035, Classifier Loss 0.09184511750936508, Total Loss 9.183110237121582\n",
      "30: Encoding Loss -1.4543153047561646, Transition Loss -7.701604843139648, Classifier Loss 0.07723750174045563, Total Loss 7.722209930419922\n",
      "30: Encoding Loss -0.7486354112625122, Transition Loss -9.695859909057617, Classifier Loss 0.07800712436437607, Total Loss 7.798773288726807\n",
      "30: Encoding Loss -0.9888254404067993, Transition Loss -3.894422769546509, Classifier Loss 0.07455751299858093, Total Loss 7.454972743988037\n",
      "30: Encoding Loss -1.5391708612442017, Transition Loss -12.867716789245605, Classifier Loss 0.07915788888931274, Total Loss 7.913215160369873\n",
      "30: Encoding Loss -1.322493314743042, Transition Loss -15.91916275024414, Classifier Loss 0.08568239957094193, Total Loss 8.565056800842285\n",
      "30: Encoding Loss -1.8102997541427612, Transition Loss -13.010675430297852, Classifier Loss 0.05709927901625633, Total Loss 5.7073259353637695\n",
      "30: Encoding Loss -1.099368691444397, Transition Loss -4.235609531402588, Classifier Loss 0.040064893662929535, Total Loss 4.005641937255859\n",
      "30: Encoding Loss -1.6161054372787476, Transition Loss -7.021670818328857, Classifier Loss 0.11351610720157623, Total Loss 11.35020637512207\n",
      "30: Encoding Loss -1.7106703519821167, Transition Loss -13.957738876342773, Classifier Loss 0.06911090016365051, Total Loss 6.908298492431641\n",
      "30: Encoding Loss -0.49625974893569946, Transition Loss 0.4409979581832886, Classifier Loss 0.10974039137363434, Total Loss 11.062237739562988\n",
      "30: Encoding Loss -0.8945732116699219, Transition Loss -7.76603889465332, Classifier Loss 0.07253792881965637, Total Loss 7.25223970413208\n",
      "30: Encoding Loss -0.40833860635757446, Transition Loss -4.542910099029541, Classifier Loss 0.08874261379241943, Total Loss 8.87328052520752\n",
      "30: Encoding Loss -1.0873427391052246, Transition Loss -11.688898086547852, Classifier Loss 0.056233301758766174, Total Loss 5.620992183685303\n",
      "30: Encoding Loss -1.660402774810791, Transition Loss -15.390192031860352, Classifier Loss 0.08729162067174911, Total Loss 8.726083755493164\n",
      "30: Encoding Loss -1.1141549348831177, Transition Loss -4.405847549438477, Classifier Loss 0.08483627438545227, Total Loss 8.482746124267578\n",
      "30: Encoding Loss -1.4260756969451904, Transition Loss -8.642690658569336, Classifier Loss 0.044556550681591034, Total Loss 4.4539265632629395\n",
      "30: Encoding Loss -1.6295206546783447, Transition Loss -8.739898681640625, Classifier Loss 0.061631470918655396, Total Loss 6.161398887634277\n",
      "30: Encoding Loss -0.6515321135520935, Transition Loss -14.110300064086914, Classifier Loss 0.0695105567574501, Total Loss 6.948233604431152\n",
      "30: Encoding Loss -1.1422088146209717, Transition Loss -13.68418025970459, Classifier Loss 0.07676590979099274, Total Loss 7.673853874206543\n",
      "30: Encoding Loss -1.0276716947555542, Transition Loss -1.385284423828125, Classifier Loss 0.04665010794997215, Total Loss 4.66473388671875\n",
      "30: Encoding Loss -1.6597586870193481, Transition Loss -17.526546478271484, Classifier Loss 0.048196788877248764, Total Loss 4.816173553466797\n",
      "30: Encoding Loss -1.6296225786209106, Transition Loss -16.627073287963867, Classifier Loss 0.044459808617830276, Total Loss 4.442655563354492\n",
      "30: Encoding Loss -0.49548086524009705, Transition Loss -0.9259507656097412, Classifier Loss 0.06858532875776291, Total Loss 6.858346462249756\n",
      "30: Encoding Loss -0.6711400747299194, Transition Loss 2.661181926727295, Classifier Loss 0.06458039581775665, Total Loss 6.990276336669922\n",
      "30: Encoding Loss -0.9968737959861755, Transition Loss -11.652421951293945, Classifier Loss 0.04890307039022446, Total Loss 4.88797664642334\n",
      "30: Encoding Loss -1.1897425651550293, Transition Loss -10.679011344909668, Classifier Loss 0.05213965103030205, Total Loss 5.21182918548584\n",
      "30: Encoding Loss -2.1259310245513916, Transition Loss -12.868644714355469, Classifier Loss 0.06619827449321747, Total Loss 6.617253303527832\n",
      "30: Encoding Loss -0.9152415990829468, Transition Loss -6.253981590270996, Classifier Loss 0.06566377729177475, Total Loss 6.565126895904541\n",
      "30: Encoding Loss -2.2138564586639404, Transition Loss -14.674226760864258, Classifier Loss 0.04901713877916336, Total Loss 4.898778915405273\n",
      "30: Encoding Loss -1.233014464378357, Transition Loss -8.607054710388184, Classifier Loss 0.07972711324691772, Total Loss 7.970990180969238\n",
      "30: Encoding Loss -1.004747748374939, Transition Loss -11.766334533691406, Classifier Loss 0.10796735435724258, Total Loss 10.794382095336914\n",
      "30: Encoding Loss -2.36108660697937, Transition Loss -19.562782287597656, Classifier Loss 0.06010445952415466, Total Loss 6.006533622741699\n",
      "30: Encoding Loss -1.7851887941360474, Transition Loss -7.8384857177734375, Classifier Loss 0.07863789051771164, Total Loss 7.8622212409973145\n",
      "30: Encoding Loss -1.3969669342041016, Transition Loss -9.745110511779785, Classifier Loss 0.11849690228700638, Total Loss 11.84774112701416\n",
      "30: Encoding Loss -1.4627761840820312, Transition Loss -15.691320419311523, Classifier Loss 0.09063027054071426, Total Loss 9.05988883972168\n",
      "30: Encoding Loss -2.117363452911377, Transition Loss -12.138041496276855, Classifier Loss 0.03123985044658184, Total Loss 3.1215574741363525\n",
      "30: Encoding Loss -0.8440030217170715, Transition Loss -11.192310333251953, Classifier Loss 0.06905067712068558, Total Loss 6.902829647064209\n",
      "30: Encoding Loss -1.7061752080917358, Transition Loss -4.553106307983398, Classifier Loss 0.05157202482223511, Total Loss 5.156291961669922\n",
      "30: Encoding Loss -0.8635843992233276, Transition Loss -8.054317474365234, Classifier Loss 0.07119769603013992, Total Loss 7.11815881729126\n",
      "30: Encoding Loss -1.2503790855407715, Transition Loss -8.83253002166748, Classifier Loss 0.061561379581689835, Total Loss 6.15437126159668\n",
      "30: Encoding Loss -0.961715579032898, Transition Loss -4.260458946228027, Classifier Loss 0.04631664976477623, Total Loss 4.630812644958496\n",
      "30: Encoding Loss -0.8819475173950195, Transition Loss -9.959546089172363, Classifier Loss 0.03286789730191231, Total Loss 3.2847976684570312\n",
      "30: Encoding Loss -1.3279502391815186, Transition Loss -11.91217041015625, Classifier Loss 0.07803510874509811, Total Loss 7.801128387451172\n",
      "30: Encoding Loss -0.43360635638237, Transition Loss -12.047539710998535, Classifier Loss 0.06976591050624847, Total Loss 6.974156379699707\n",
      "30: Encoding Loss -1.156004548072815, Transition Loss 0.10105299949645996, Classifier Loss 0.10239753127098083, Total Loss 10.259963035583496\n",
      "30: Encoding Loss -1.0132964849472046, Transition Loss -8.029601097106934, Classifier Loss 0.04420783370733261, Total Loss 4.419177532196045\n",
      "30: Encoding Loss -0.3398236632347107, Transition Loss -11.029200553894043, Classifier Loss 0.08281856775283813, Total Loss 8.278728485107422\n",
      "30: Encoding Loss -0.1512562483549118, Transition Loss -9.17005443572998, Classifier Loss 0.06523004919290543, Total Loss 6.442281246185303\n",
      "30: Encoding Loss -1.1019283533096313, Transition Loss -8.796531677246094, Classifier Loss 0.0565534308552742, Total Loss 5.653583526611328\n",
      "30: Encoding Loss -1.8292008638381958, Transition Loss -4.630124092102051, Classifier Loss 0.08952474594116211, Total Loss 8.95154857635498\n",
      "30: Encoding Loss -2.0569019317626953, Transition Loss -13.613747596740723, Classifier Loss 0.04220280051231384, Total Loss 4.217557430267334\n",
      "30: Encoding Loss 0.09325672686100006, Transition Loss -1.2119183540344238, Classifier Loss 0.061304572969675064, Total Loss 6.7453203201293945\n",
      "30: Encoding Loss -0.46427205204963684, Transition Loss -6.386816024780273, Classifier Loss 0.039538346230983734, Total Loss 3.9525506496429443\n",
      "30: Encoding Loss -0.11307643353939056, Transition Loss -5.663884162902832, Classifier Loss 0.08899211883544922, Total Loss 8.7813138961792\n",
      "30: Encoding Loss 0.4705018699169159, Transition Loss -14.21461296081543, Classifier Loss 0.16348883509635925, Total Loss 20.110048294067383\n",
      "30: Encoding Loss -0.4980812668800354, Transition Loss -16.12357521057129, Classifier Loss 0.08587595075368881, Total Loss 8.584369659423828\n",
      "30: Encoding Loss -0.6780646443367004, Transition Loss -22.33242416381836, Classifier Loss 0.0515989288687706, Total Loss 5.155426502227783\n",
      "30: Encoding Loss 0.012760174460709095, Transition Loss -11.016664505004883, Classifier Loss 0.10002017766237259, Total Loss 10.056037902832031\n",
      "30: Encoding Loss -1.463585615158081, Transition Loss -11.777985572814941, Classifier Loss 0.0640050545334816, Total Loss 6.3981499671936035\n",
      "30: Encoding Loss -2.309532403945923, Transition Loss -17.974685668945312, Classifier Loss 0.0718386247754097, Total Loss 7.180267810821533\n",
      "30: Encoding Loss -1.7761881351470947, Transition Loss -5.910776615142822, Classifier Loss 0.09136530756950378, Total Loss 9.135348320007324\n",
      "30: Encoding Loss -0.6968234777450562, Transition Loss -14.538237571716309, Classifier Loss 0.09358502924442291, Total Loss 9.355595588684082\n",
      "30: Encoding Loss -0.8886770606040955, Transition Loss -15.31287956237793, Classifier Loss 0.02930082380771637, Total Loss 2.9270198345184326\n",
      "30: Encoding Loss -1.6896626949310303, Transition Loss -18.440988540649414, Classifier Loss 0.06256982684135437, Total Loss 6.253294467926025\n",
      "30: Encoding Loss -1.7788050174713135, Transition Loss -23.91300392150879, Classifier Loss 0.029788706451654434, Total Loss 2.974087953567505\n",
      "31: Encoding Loss -0.8970245122909546, Transition Loss -3.9131665229797363, Classifier Loss 0.039857737720012665, Total Loss 3.9849910736083984\n",
      "31: Encoding Loss -1.6243985891342163, Transition Loss -4.661154747009277, Classifier Loss 0.11165563762187958, Total Loss 11.164630889892578\n",
      "31: Encoding Loss -1.3279454708099365, Transition Loss 0.34281325340270996, Classifier Loss 0.04352894797921181, Total Loss 4.421457290649414\n",
      "31: Encoding Loss -1.3563333749771118, Transition Loss -17.42190933227539, Classifier Loss 0.06841854006052017, Total Loss 6.838369846343994\n",
      "31: Encoding Loss -1.1485507488250732, Transition Loss -7.974844455718994, Classifier Loss 0.0530141219496727, Total Loss 5.299817085266113\n",
      "31: Encoding Loss -1.2338334321975708, Transition Loss -14.2086820602417, Classifier Loss 0.05609383434057236, Total Loss 5.606541633605957\n",
      "31: Encoding Loss -1.8467235565185547, Transition Loss -10.314361572265625, Classifier Loss 0.06956569105386734, Total Loss 6.9545063972473145\n",
      "31: Encoding Loss -0.9422599077224731, Transition Loss -7.839982509613037, Classifier Loss 0.05756709352135658, Total Loss 5.755141735076904\n",
      "31: Encoding Loss -1.3209726810455322, Transition Loss -14.325562477111816, Classifier Loss 0.04701186716556549, Total Loss 4.698321342468262\n",
      "31: Encoding Loss -2.0246150493621826, Transition Loss -11.646800994873047, Classifier Loss 0.07458153367042542, Total Loss 7.45582389831543\n",
      "31: Encoding Loss 0.30538398027420044, Transition Loss -7.861616134643555, Classifier Loss 0.061257313936948776, Total Loss 8.564471244812012\n",
      "31: Encoding Loss -0.07769353687763214, Transition Loss -3.698763847351074, Classifier Loss 0.06982911378145218, Total Loss 6.846302509307861\n",
      "31: Encoding Loss -1.2280241250991821, Transition Loss -5.31170654296875, Classifier Loss 0.055308833718299866, Total Loss 5.529820919036865\n",
      "31: Encoding Loss -0.5179794430732727, Transition Loss -6.250329971313477, Classifier Loss 0.060279183089733124, Total Loss 6.026667594909668\n",
      "31: Encoding Loss -1.6861491203308105, Transition Loss -13.618890762329102, Classifier Loss 0.048631701618433, Total Loss 4.860446453094482\n",
      "31: Encoding Loss -2.302741050720215, Transition Loss -18.009231567382812, Classifier Loss 0.0699891746044159, Total Loss 6.9953155517578125\n",
      "31: Encoding Loss -1.2395131587982178, Transition Loss -9.350826263427734, Classifier Loss 0.026158688589930534, Total Loss 2.6139986515045166\n",
      "31: Encoding Loss -0.3443867564201355, Transition Loss -5.760456085205078, Classifier Loss 0.052298031747341156, Total Loss 5.227860927581787\n",
      "31: Encoding Loss -0.0036515125539153814, Transition Loss -13.14040756225586, Classifier Loss 0.09090206772089005, Total Loss 9.07339859008789\n",
      "31: Encoding Loss -0.7570804953575134, Transition Loss -5.563319683074951, Classifier Loss 0.05708668753504753, Total Loss 5.707556247711182\n",
      "31: Encoding Loss -1.3077064752578735, Transition Loss -15.04693603515625, Classifier Loss 0.08174940943717957, Total Loss 8.171931266784668\n",
      "31: Encoding Loss -1.600042700767517, Transition Loss -10.955901145935059, Classifier Loss 0.04566216096282005, Total Loss 4.564024925231934\n",
      "31: Encoding Loss -1.578092336654663, Transition Loss -11.81118392944336, Classifier Loss 0.07577189058065414, Total Loss 7.574826717376709\n",
      "31: Encoding Loss -1.1266876459121704, Transition Loss -3.7173213958740234, Classifier Loss 0.08532121777534485, Total Loss 8.531377792358398\n",
      "31: Encoding Loss -2.047226905822754, Transition Loss -11.76012897491455, Classifier Loss 0.03656788915395737, Total Loss 3.6544368267059326\n",
      "31: Encoding Loss -0.8695560097694397, Transition Loss -5.14667272567749, Classifier Loss 0.07264801859855652, Total Loss 7.263772487640381\n",
      "31: Encoding Loss -1.9743616580963135, Transition Loss -3.4266998767852783, Classifier Loss 0.10007212311029434, Total Loss 10.006526947021484\n",
      "31: Encoding Loss -2.6748135089874268, Transition Loss -18.866769790649414, Classifier Loss 0.07775713503360748, Total Loss 7.771940231323242\n",
      "31: Encoding Loss -2.179147958755493, Transition Loss -9.608699798583984, Classifier Loss 0.06383968144655228, Total Loss 6.382046699523926\n",
      "31: Encoding Loss -0.913194477558136, Transition Loss -5.307568550109863, Classifier Loss 0.0717991292476654, Total Loss 7.17885160446167\n",
      "31: Encoding Loss -0.9312959909439087, Transition Loss -11.532428741455078, Classifier Loss 0.07913179695606232, Total Loss 7.9108734130859375\n",
      "31: Encoding Loss -1.2797554731369019, Transition Loss -3.0861282348632812, Classifier Loss 0.09142763167619705, Total Loss 9.142146110534668\n",
      "31: Encoding Loss -0.5959446430206299, Transition Loss -11.095876693725586, Classifier Loss 0.07174269109964371, Total Loss 7.1720499992370605\n",
      "31: Encoding Loss -0.1908043771982193, Transition Loss -5.86859655380249, Classifier Loss 0.17237433791160583, Total Loss 17.193227767944336\n",
      "31: Encoding Loss -1.7851604223251343, Transition Loss -12.646621704101562, Classifier Loss 0.04680377617478371, Total Loss 4.6778483390808105\n",
      "31: Encoding Loss -0.8996701836585999, Transition Loss -7.382076263427734, Classifier Loss 0.045088957995176315, Total Loss 4.507419586181641\n",
      "31: Encoding Loss -0.9964625239372253, Transition Loss -6.603158473968506, Classifier Loss 0.07766766101121902, Total Loss 7.765445232391357\n",
      "31: Encoding Loss -0.8735732436180115, Transition Loss -8.987483978271484, Classifier Loss 0.12005571275949478, Total Loss 12.00377368927002\n",
      "31: Encoding Loss -1.6288363933563232, Transition Loss -9.61495590209961, Classifier Loss 0.07174711674451828, Total Loss 7.172788619995117\n",
      "31: Encoding Loss -1.4297428131103516, Transition Loss -11.266899108886719, Classifier Loss 0.07270832359790802, Total Loss 7.268579006195068\n",
      "31: Encoding Loss -1.3199481964111328, Transition Loss -7.629648208618164, Classifier Loss 0.03936391323804855, Total Loss 3.9348654747009277\n",
      "31: Encoding Loss -1.6383954286575317, Transition Loss -16.065858840942383, Classifier Loss 0.08689084649085999, Total Loss 8.685872077941895\n",
      "31: Encoding Loss -1.9683339595794678, Transition Loss -15.268251419067383, Classifier Loss 0.03578523173928261, Total Loss 3.575469493865967\n",
      "31: Encoding Loss -1.8095613718032837, Transition Loss -5.307091236114502, Classifier Loss 0.07114829868078232, Total Loss 7.113768577575684\n",
      "31: Encoding Loss -1.7599313259124756, Transition Loss -16.71743392944336, Classifier Loss 0.05052802711725235, Total Loss 5.049458980560303\n",
      "31: Encoding Loss -0.12937316298484802, Transition Loss -6.875319004058838, Classifier Loss 0.08596491813659668, Total Loss 8.493813514709473\n",
      "31: Encoding Loss -0.46898162364959717, Transition Loss -16.874610900878906, Classifier Loss 0.07349753379821777, Total Loss 7.346373081207275\n",
      "31: Encoding Loss -1.3376520872116089, Transition Loss -5.588190078735352, Classifier Loss 0.03726239874958992, Total Loss 3.7251222133636475\n",
      "31: Encoding Loss -1.5939733982086182, Transition Loss -8.592339515686035, Classifier Loss 0.06074986606836319, Total Loss 6.073267936706543\n",
      "31: Encoding Loss -1.0977330207824707, Transition Loss -15.455663681030273, Classifier Loss 0.053762391209602356, Total Loss 5.373147964477539\n",
      "31: Encoding Loss -1.6330145597457886, Transition Loss -18.587100982666016, Classifier Loss 0.053268104791641235, Total Loss 5.323092937469482\n",
      "31: Encoding Loss -0.19030015170574188, Transition Loss -11.56343936920166, Classifier Loss 0.07054020464420319, Total Loss 7.008288383483887\n",
      "31: Encoding Loss -0.8640387654304504, Transition Loss -1.766255259513855, Classifier Loss 0.08338337391614914, Total Loss 8.337984085083008\n",
      "31: Encoding Loss -1.2408944368362427, Transition Loss -11.110542297363281, Classifier Loss 0.0496944934129715, Total Loss 4.967227458953857\n",
      "31: Encoding Loss -0.70293128490448, Transition Loss -10.978241920471191, Classifier Loss 0.0752425566315651, Total Loss 7.522059917449951\n",
      "31: Encoding Loss -1.1929978132247925, Transition Loss -4.012298583984375, Classifier Loss 0.09162681549787521, Total Loss 9.161879539489746\n",
      "31: Encoding Loss -0.6483972072601318, Transition Loss -10.186777114868164, Classifier Loss 0.0360013023018837, Total Loss 3.598093032836914\n",
      "31: Encoding Loss -1.7214967012405396, Transition Loss -11.72215461730957, Classifier Loss 0.059523165225982666, Total Loss 5.949971675872803\n",
      "31: Encoding Loss -1.6339069604873657, Transition Loss -18.562898635864258, Classifier Loss 0.04749656096100807, Total Loss 4.745943546295166\n",
      "31: Encoding Loss -1.4112952947616577, Transition Loss -15.188272476196289, Classifier Loss 0.07015664130449295, Total Loss 7.012626647949219\n",
      "31: Encoding Loss -0.990156352519989, Transition Loss -8.275184631347656, Classifier Loss 0.06568633019924164, Total Loss 6.5669779777526855\n",
      "31: Encoding Loss -1.4321255683898926, Transition Loss -6.792049407958984, Classifier Loss 0.1078483834862709, Total Loss 10.783480644226074\n",
      "31: Encoding Loss -1.3393913507461548, Transition Loss -8.541839599609375, Classifier Loss 0.059380557388067245, Total Loss 5.936347007751465\n",
      "31: Encoding Loss -0.9610804319381714, Transition Loss -6.165640354156494, Classifier Loss 0.07639580965042114, Total Loss 7.638347625732422\n",
      "31: Encoding Loss -1.0435870885849, Transition Loss -10.632719993591309, Classifier Loss 0.06667686998844147, Total Loss 6.665560245513916\n",
      "31: Encoding Loss -1.568536639213562, Transition Loss -3.55959153175354, Classifier Loss 0.11039119958877563, Total Loss 11.038407325744629\n",
      "31: Encoding Loss 0.20145510137081146, Transition Loss -0.20688652992248535, Classifier Loss 0.06495917588472366, Total Loss 8.072100639343262\n",
      "31: Encoding Loss -1.738950252532959, Transition Loss -19.046459197998047, Classifier Loss 0.08541454374790192, Total Loss 8.53764533996582\n",
      "31: Encoding Loss 0.5648738741874695, Transition Loss -10.242791175842285, Classifier Loss 0.07355137914419174, Total Loss 11.872079849243164\n",
      "31: Encoding Loss 1.1989426612854004, Transition Loss -9.030252456665039, Classifier Loss 0.11362564563751221, Total Loss 20.952299118041992\n",
      "31: Encoding Loss -0.9057846665382385, Transition Loss -9.014952659606934, Classifier Loss 0.08194850385189056, Total Loss 8.193046569824219\n",
      "31: Encoding Loss -0.5965578556060791, Transition Loss -9.227938652038574, Classifier Loss 0.07827268540859222, Total Loss 7.825423240661621\n",
      "31: Encoding Loss -0.5916393399238586, Transition Loss -5.94334602355957, Classifier Loss 0.10198216885328293, Total Loss 10.197028160095215\n",
      "31: Encoding Loss -0.0909833163022995, Transition Loss -9.369257926940918, Classifier Loss 0.026338420808315277, Total Loss 2.4998927116394043\n",
      "31: Encoding Loss -0.23029223084449768, Transition Loss -8.253767013549805, Classifier Loss 0.11023250222206116, Total Loss 11.001994132995605\n",
      "31: Encoding Loss -0.2561400830745697, Transition Loss -14.4483003616333, Classifier Loss 0.0437372624874115, Total Loss 4.3601555824279785\n",
      "31: Encoding Loss 0.42270147800445557, Transition Loss -6.980154991149902, Classifier Loss 0.08455796539783478, Total Loss 11.83597183227539\n",
      "31: Encoding Loss -1.2642523050308228, Transition Loss -13.885566711425781, Classifier Loss 0.06313004344701767, Total Loss 6.310227394104004\n",
      "31: Encoding Loss -0.17836850881576538, Transition Loss -15.138090133666992, Classifier Loss 0.041529640555381775, Total Loss 4.096800804138184\n",
      "31: Encoding Loss -0.5405772924423218, Transition Loss -11.674485206604004, Classifier Loss 0.1181202083826065, Total Loss 11.809686660766602\n",
      "31: Encoding Loss -0.012032510712742805, Transition Loss -12.33880615234375, Classifier Loss 0.059566885232925415, Total Loss 5.91070032119751\n",
      "31: Encoding Loss -1.4282158613204956, Transition Loss -9.798405647277832, Classifier Loss 0.0631633996963501, Total Loss 6.314380168914795\n",
      "31: Encoding Loss -1.0263550281524658, Transition Loss -6.319948673248291, Classifier Loss 0.09313072264194489, Total Loss 9.311808586120605\n",
      "31: Encoding Loss -0.30827435851097107, Transition Loss -11.664189338684082, Classifier Loss 0.09287843853235245, Total Loss 9.282981872558594\n",
      "31: Encoding Loss -2.579575777053833, Transition Loss -8.909500122070312, Classifier Loss 0.05961909890174866, Total Loss 5.960127830505371\n",
      "31: Encoding Loss -1.5305410623550415, Transition Loss -6.483620643615723, Classifier Loss 0.06951258331537247, Total Loss 6.9499616622924805\n",
      "31: Encoding Loss -1.5650169849395752, Transition Loss -16.749853134155273, Classifier Loss 0.07505182921886444, Total Loss 7.501832962036133\n",
      "31: Encoding Loss -1.3884928226470947, Transition Loss -10.715339660644531, Classifier Loss 0.10925199836492538, Total Loss 10.923056602478027\n",
      "31: Encoding Loss -0.772969126701355, Transition Loss -8.526576042175293, Classifier Loss 0.05108187347650528, Total Loss 5.106482028961182\n",
      "31: Encoding Loss -0.327934592962265, Transition Loss -14.042956352233887, Classifier Loss 0.06288442760705948, Total Loss 6.284269332885742\n",
      "31: Encoding Loss -2.6523451805114746, Transition Loss -22.934192657470703, Classifier Loss 0.059675801545381546, Total Loss 5.962993621826172\n",
      "31: Encoding Loss -0.9285277724266052, Transition Loss -7.7997636795043945, Classifier Loss 0.09036163985729218, Total Loss 9.0346040725708\n",
      "31: Encoding Loss -0.6968599557876587, Transition Loss -8.574263572692871, Classifier Loss 0.08003498613834381, Total Loss 8.001784324645996\n",
      "31: Encoding Loss -0.4895436465740204, Transition Loss -10.66794204711914, Classifier Loss 0.07612424343824387, Total Loss 7.610289096832275\n",
      "31: Encoding Loss -0.6166731715202332, Transition Loss -4.723484992980957, Classifier Loss 0.07315772026777267, Total Loss 7.3148274421691895\n",
      "31: Encoding Loss -1.3733940124511719, Transition Loss -13.987884521484375, Classifier Loss 0.0806175023317337, Total Loss 8.058953285217285\n",
      "31: Encoding Loss -1.2746492624282837, Transition Loss -16.98108673095703, Classifier Loss 0.0841841772198677, Total Loss 8.415021896362305\n",
      "31: Encoding Loss -1.6459956169128418, Transition Loss -14.153154373168945, Classifier Loss 0.054225143045186996, Total Loss 5.419683933258057\n",
      "31: Encoding Loss -1.1278859376907349, Transition Loss -5.039173603057861, Classifier Loss 0.0393746942281723, Total Loss 3.9364616870880127\n",
      "31: Encoding Loss -1.4302270412445068, Transition Loss -7.860772609710693, Classifier Loss 0.11136193573474884, Total Loss 11.134620666503906\n",
      "31: Encoding Loss -1.2605572938919067, Transition Loss -15.107481002807617, Classifier Loss 0.07123259454965591, Total Loss 7.120237827301025\n",
      "31: Encoding Loss -0.25579166412353516, Transition Loss -0.32984358072280884, Classifier Loss 0.10735710710287094, Total Loss 10.724871635437012\n",
      "31: Encoding Loss -0.7403982281684875, Transition Loss -8.460046768188477, Classifier Loss 0.07238194346427917, Total Loss 7.236502647399902\n",
      "31: Encoding Loss -0.08763177692890167, Transition Loss -5.062925338745117, Classifier Loss 0.0880126953125, Total Loss 8.666755676269531\n",
      "31: Encoding Loss -0.9501593708992004, Transition Loss -12.466272354125977, Classifier Loss 0.055684857070446014, Total Loss 5.56599235534668\n",
      "31: Encoding Loss -1.3863619565963745, Transition Loss -16.25275993347168, Classifier Loss 0.08475731313228607, Total Loss 8.472480773925781\n",
      "31: Encoding Loss -0.7745726704597473, Transition Loss -4.796863555908203, Classifier Loss 0.08360136300325394, Total Loss 8.359176635742188\n",
      "31: Encoding Loss -0.8341456651687622, Transition Loss -9.353592872619629, Classifier Loss 0.04386598616838455, Total Loss 4.384727954864502\n",
      "31: Encoding Loss -1.0748716592788696, Transition Loss -9.422672271728516, Classifier Loss 0.062084220349788666, Total Loss 6.20653772354126\n",
      "31: Encoding Loss 0.03061634674668312, Transition Loss -15.008991241455078, Classifier Loss 0.06878839433193207, Total Loss 7.027758598327637\n",
      "31: Encoding Loss -1.4753347635269165, Transition Loss -13.415010452270508, Classifier Loss 0.08002586662769318, Total Loss 7.999903202056885\n",
      "31: Encoding Loss -1.561820149421692, Transition Loss -1.8561747074127197, Classifier Loss 0.04674918204545975, Total Loss 4.674546718597412\n",
      "31: Encoding Loss -2.092496395111084, Transition Loss -17.16548728942871, Classifier Loss 0.048291511833667755, Total Loss 4.825717926025391\n",
      "31: Encoding Loss -1.646432638168335, Transition Loss -16.438390731811523, Classifier Loss 0.042205631732940674, Total Loss 4.217275142669678\n",
      "31: Encoding Loss -0.762543261051178, Transition Loss -1.3354301452636719, Classifier Loss 0.06384392082691193, Total Loss 6.384125232696533\n",
      "31: Encoding Loss -0.7598934173583984, Transition Loss 1.7591235637664795, Classifier Loss 0.06312908232212067, Total Loss 6.664732933044434\n",
      "31: Encoding Loss -1.0263173580169678, Transition Loss -11.63953685760498, Classifier Loss 0.047981277108192444, Total Loss 4.795799732208252\n",
      "31: Encoding Loss -1.3469971418380737, Transition Loss -10.785255432128906, Classifier Loss 0.0570627823472023, Total Loss 5.704121112823486\n",
      "31: Encoding Loss -2.3469126224517822, Transition Loss -12.912407875061035, Classifier Loss 0.06500143557786942, Total Loss 6.497560977935791\n",
      "31: Encoding Loss -1.4005202054977417, Transition Loss -6.508486747741699, Classifier Loss 0.06525513529777527, Total Loss 6.524211883544922\n",
      "31: Encoding Loss -2.5227503776550293, Transition Loss -14.694141387939453, Classifier Loss 0.04642227292060852, Total Loss 4.639288425445557\n",
      "31: Encoding Loss -1.4079943895339966, Transition Loss -8.736421585083008, Classifier Loss 0.07930576801300049, Total Loss 7.928829669952393\n",
      "31: Encoding Loss -1.220918893814087, Transition Loss -11.958288192749023, Classifier Loss 0.10406558215618134, Total Loss 10.404166221618652\n",
      "31: Encoding Loss -2.5958783626556396, Transition Loss -19.29556655883789, Classifier Loss 0.06067478656768799, Total Loss 6.063619613647461\n",
      "31: Encoding Loss -1.878269910812378, Transition Loss -8.322380065917969, Classifier Loss 0.07621172070503235, Total Loss 7.619507312774658\n",
      "31: Encoding Loss -1.6775479316711426, Transition Loss -9.992077827453613, Classifier Loss 0.11628073453903198, Total Loss 11.626075744628906\n",
      "31: Encoding Loss -1.7455726861953735, Transition Loss -15.547124862670898, Classifier Loss 0.08613283187150955, Total Loss 8.610174179077148\n",
      "31: Encoding Loss -2.0909180641174316, Transition Loss -12.310752868652344, Classifier Loss 0.02963663637638092, Total Loss 2.9612014293670654\n",
      "31: Encoding Loss -0.950921356678009, Transition Loss -11.476754188537598, Classifier Loss 0.06499390304088593, Total Loss 6.497094631195068\n",
      "31: Encoding Loss -1.5460458993911743, Transition Loss -5.078975677490234, Classifier Loss 0.050002630800008774, Total Loss 4.9992475509643555\n",
      "31: Encoding Loss -0.7317287921905518, Transition Loss -8.538848876953125, Classifier Loss 0.06797049194574356, Total Loss 6.795341491699219\n",
      "31: Encoding Loss -1.4691927433013916, Transition Loss -9.173072814941406, Classifier Loss 0.06233546882867813, Total Loss 6.231712341308594\n",
      "31: Encoding Loss -0.8495821356773376, Transition Loss -4.892826557159424, Classifier Loss 0.044385574758052826, Total Loss 4.437579154968262\n",
      "31: Encoding Loss -1.0024467706680298, Transition Loss -10.263439178466797, Classifier Loss 0.033180758357048035, Total Loss 3.316023111343384\n",
      "31: Encoding Loss -1.6085858345031738, Transition Loss -12.096073150634766, Classifier Loss 0.07823511958122253, Total Loss 7.8210930824279785\n",
      "31: Encoding Loss -0.36848118901252747, Transition Loss -12.125992774963379, Classifier Loss 0.06901352107524872, Total Loss 6.898589611053467\n",
      "31: Encoding Loss -0.9340395331382751, Transition Loss -0.8512647151947021, Classifier Loss 0.10204149037599564, Total Loss 10.203978538513184\n",
      "31: Encoding Loss -0.9246752262115479, Transition Loss -8.329061508178711, Classifier Loss 0.0421844944357872, Total Loss 4.2167840003967285\n",
      "31: Encoding Loss -0.307809442281723, Transition Loss -11.215015411376953, Classifier Loss 0.0815889909863472, Total Loss 8.154090881347656\n",
      "31: Encoding Loss -0.41080009937286377, Transition Loss -9.385904312133789, Classifier Loss 0.06439759582281113, Total Loss 6.437816619873047\n",
      "31: Encoding Loss -1.3214361667633057, Transition Loss -9.097227096557617, Classifier Loss 0.0579870231449604, Total Loss 5.796882629394531\n",
      "31: Encoding Loss -1.9506419897079468, Transition Loss -5.188736438751221, Classifier Loss 0.08889992535114288, Total Loss 8.888955116271973\n",
      "31: Encoding Loss -2.191478729248047, Transition Loss -13.723631858825684, Classifier Loss 0.04306206852197647, Total Loss 4.303462028503418\n",
      "31: Encoding Loss -0.05845671519637108, Transition Loss -1.7643897533416748, Classifier Loss 0.06083410233259201, Total Loss 5.952385902404785\n",
      "31: Encoding Loss -0.6659409999847412, Transition Loss -7.763924598693848, Classifier Loss 0.03869858756661415, Total Loss 3.8683059215545654\n",
      "31: Encoding Loss -0.18907703459262848, Transition Loss -6.907575607299805, Classifier Loss 0.09058826416730881, Total Loss 9.013083457946777\n",
      "31: Encoding Loss -0.059842903167009354, Transition Loss -15.467229843139648, Classifier Loss 0.15906895697116852, Total Loss 15.77225399017334\n",
      "31: Encoding Loss -2.115121603012085, Transition Loss -15.391633987426758, Classifier Loss 0.08294538408517838, Total Loss 8.291460037231445\n",
      "31: Encoding Loss -2.587174415588379, Transition Loss -20.951818466186523, Classifier Loss 0.04707668349146843, Total Loss 4.70347785949707\n",
      "31: Encoding Loss -0.8697550296783447, Transition Loss -11.198143005371094, Classifier Loss 0.09769108146429062, Total Loss 9.766868591308594\n",
      "31: Encoding Loss -1.8071815967559814, Transition Loss -12.31573486328125, Classifier Loss 0.062364354729652405, Total Loss 6.233972072601318\n",
      "31: Encoding Loss -2.5096092224121094, Transition Loss -18.729095458984375, Classifier Loss 0.06294871866703033, Total Loss 6.291125774383545\n",
      "31: Encoding Loss -1.8678615093231201, Transition Loss -6.765969276428223, Classifier Loss 0.09352325648069382, Total Loss 9.350972175598145\n",
      "31: Encoding Loss -1.0717109441757202, Transition Loss -15.289998054504395, Classifier Loss 0.09353378415107727, Total Loss 9.350319862365723\n",
      "31: Encoding Loss -1.586742877960205, Transition Loss -16.10621452331543, Classifier Loss 0.02480144426226616, Total Loss 2.4769232273101807\n",
      "31: Encoding Loss -1.8005220890045166, Transition Loss -19.108678817749023, Classifier Loss 0.06191180273890495, Total Loss 6.187358379364014\n",
      "31: Encoding Loss -2.929419994354248, Transition Loss -24.491113662719727, Classifier Loss 0.025428203865885735, Total Loss 2.5379221439361572\n",
      "32: Encoding Loss -1.1653028726577759, Transition Loss -4.722546577453613, Classifier Loss 0.03839191049337387, Total Loss 3.8382463455200195\n",
      "32: Encoding Loss -1.5547455549240112, Transition Loss -5.442639350891113, Classifier Loss 0.11009765416383743, Total Loss 11.00867748260498\n",
      "32: Encoding Loss -1.6357976198196411, Transition Loss -0.7552073001861572, Classifier Loss 0.04169885069131851, Total Loss 4.169734001159668\n",
      "32: Encoding Loss -1.462796926498413, Transition Loss -18.034454345703125, Classifier Loss 0.06545909494161606, Total Loss 6.54230260848999\n",
      "32: Encoding Loss -1.447404146194458, Transition Loss -8.665966033935547, Classifier Loss 0.04942923039197922, Total Loss 4.941189765930176\n",
      "32: Encoding Loss -1.5861225128173828, Transition Loss -14.832196235656738, Classifier Loss 0.05441463738679886, Total Loss 5.438497543334961\n",
      "32: Encoding Loss -1.8875232934951782, Transition Loss -11.099770545959473, Classifier Loss 0.06758858263492584, Total Loss 6.756638050079346\n",
      "32: Encoding Loss -1.5550353527069092, Transition Loss -8.272790908813477, Classifier Loss 0.0539717860519886, Total Loss 5.395524024963379\n",
      "32: Encoding Loss -1.2261106967926025, Transition Loss -14.911476135253906, Classifier Loss 0.045346699655056, Total Loss 4.5316877365112305\n",
      "32: Encoding Loss -2.064438581466675, Transition Loss -12.19941520690918, Classifier Loss 0.07230017334222794, Total Loss 7.227577209472656\n",
      "32: Encoding Loss -0.12336865812540054, Transition Loss -8.693086624145508, Classifier Loss 0.05826672166585922, Total Loss 5.717691898345947\n",
      "32: Encoding Loss -1.0323883295059204, Transition Loss -5.27468729019165, Classifier Loss 0.07024846971035004, Total Loss 7.023792266845703\n",
      "32: Encoding Loss -1.6146279573440552, Transition Loss -7.082785129547119, Classifier Loss 0.05185171216726303, Total Loss 5.1837544441223145\n",
      "32: Encoding Loss -0.9013960957527161, Transition Loss -7.5423712730407715, Classifier Loss 0.062418367713689804, Total Loss 6.240327835083008\n",
      "32: Encoding Loss -2.2678894996643066, Transition Loss -14.740588188171387, Classifier Loss 0.04586039483547211, Total Loss 4.5830912590026855\n",
      "32: Encoding Loss -2.3586132526397705, Transition Loss -18.337112426757812, Classifier Loss 0.0704725980758667, Total Loss 7.04359245300293\n",
      "32: Encoding Loss -1.1848505735397339, Transition Loss -10.635100364685059, Classifier Loss 0.025544973090291023, Total Loss 2.552370309829712\n",
      "32: Encoding Loss -0.4447510838508606, Transition Loss -7.111361980438232, Classifier Loss 0.055373311042785645, Total Loss 5.535893440246582\n",
      "32: Encoding Loss -0.4148630201816559, Transition Loss -13.549995422363281, Classifier Loss 0.09099923074245453, Total Loss 9.09715747833252\n",
      "32: Encoding Loss -0.9420318007469177, Transition Loss -6.5717058181762695, Classifier Loss 0.05856076627969742, Total Loss 5.854762554168701\n",
      "32: Encoding Loss -1.2310709953308105, Transition Loss -15.371967315673828, Classifier Loss 0.08309350162744522, Total Loss 8.306275367736816\n",
      "32: Encoding Loss -1.7099484205245972, Transition Loss -11.72776985168457, Classifier Loss 0.04333227127790451, Total Loss 4.330881595611572\n",
      "32: Encoding Loss -1.461309552192688, Transition Loss -12.705116271972656, Classifier Loss 0.07418962568044662, Total Loss 7.416421413421631\n",
      "32: Encoding Loss -1.1727874279022217, Transition Loss -4.676909923553467, Classifier Loss 0.08574484288692474, Total Loss 8.573548316955566\n",
      "32: Encoding Loss -2.0039117336273193, Transition Loss -12.36628246307373, Classifier Loss 0.037638433277606964, Total Loss 3.7613699436187744\n",
      "32: Encoding Loss -0.8819025754928589, Transition Loss -5.971809387207031, Classifier Loss 0.06990030407905579, Total Loss 6.98883581161499\n",
      "32: Encoding Loss -1.9600247144699097, Transition Loss -4.794728755950928, Classifier Loss 0.10098825395107269, Total Loss 10.09786605834961\n",
      "32: Encoding Loss -2.7549946308135986, Transition Loss -19.139873504638672, Classifier Loss 0.07796134799718857, Total Loss 7.792306900024414\n",
      "32: Encoding Loss -2.160141944885254, Transition Loss -10.07671070098877, Classifier Loss 0.06320249289274216, Total Loss 6.318233966827393\n",
      "32: Encoding Loss -0.9775242209434509, Transition Loss -6.225757122039795, Classifier Loss 0.07121382653713226, Total Loss 7.120137691497803\n",
      "32: Encoding Loss -1.0538333654403687, Transition Loss -11.990154266357422, Classifier Loss 0.0783601701259613, Total Loss 7.833619117736816\n",
      "32: Encoding Loss -1.3239480257034302, Transition Loss -3.9493892192840576, Classifier Loss 0.08775444328784943, Total Loss 8.774654388427734\n",
      "32: Encoding Loss -0.719954252243042, Transition Loss -11.51657485961914, Classifier Loss 0.0696573331952095, Total Loss 6.963430404663086\n",
      "32: Encoding Loss -0.3039605915546417, Transition Loss -6.5815277099609375, Classifier Loss 0.17133361101150513, Total Loss 17.129165649414062\n",
      "32: Encoding Loss -1.8807222843170166, Transition Loss -13.208267211914062, Classifier Loss 0.04800143837928772, Total Loss 4.797502040863037\n",
      "32: Encoding Loss -1.1309497356414795, Transition Loss -8.108840942382812, Classifier Loss 0.043094001710414886, Total Loss 4.307778358459473\n",
      "32: Encoding Loss -1.185041069984436, Transition Loss -7.469125747680664, Classifier Loss 0.0768609270453453, Total Loss 7.684598922729492\n",
      "32: Encoding Loss -0.9225374460220337, Transition Loss -9.573352813720703, Classifier Loss 0.11791138350963593, Total Loss 11.789223670959473\n",
      "32: Encoding Loss -1.6570649147033691, Transition Loss -10.495726585388184, Classifier Loss 0.06819162517786026, Total Loss 6.817063331604004\n",
      "32: Encoding Loss -1.4758808612823486, Transition Loss -11.863751411437988, Classifier Loss 0.07010850310325623, Total Loss 7.008477687835693\n",
      "32: Encoding Loss -1.3359123468399048, Transition Loss -8.609683990478516, Classifier Loss 0.03817011043429375, Total Loss 3.815289258956909\n",
      "32: Encoding Loss -1.7530397176742554, Transition Loss -16.21563720703125, Classifier Loss 0.08646989613771439, Total Loss 8.643746376037598\n",
      "32: Encoding Loss -1.967870831489563, Transition Loss -15.865288734436035, Classifier Loss 0.03273312374949455, Total Loss 3.270139217376709\n",
      "32: Encoding Loss -1.8952124118804932, Transition Loss -6.366826057434082, Classifier Loss 0.06725432723760605, Total Loss 6.7241597175598145\n",
      "32: Encoding Loss -1.763170599937439, Transition Loss -16.83021354675293, Classifier Loss 0.04844873026013374, Total Loss 4.8415069580078125\n",
      "32: Encoding Loss -0.10680952668190002, Transition Loss -7.833005905151367, Classifier Loss 0.0843309611082077, Total Loss 8.309561729431152\n",
      "32: Encoding Loss -0.532566249370575, Transition Loss -17.160736083984375, Classifier Loss 0.06826549023389816, Total Loss 6.823116302490234\n",
      "32: Encoding Loss -1.6856473684310913, Transition Loss -6.5972514152526855, Classifier Loss 0.03540495038032532, Total Loss 3.539175510406494\n",
      "32: Encoding Loss -1.751747488975525, Transition Loss -9.48360824584961, Classifier Loss 0.06249652057886124, Total Loss 6.24775505065918\n",
      "32: Encoding Loss -1.342058777809143, Transition Loss -15.780364990234375, Classifier Loss 0.05291876941919327, Total Loss 5.288720607757568\n",
      "32: Encoding Loss -1.8959288597106934, Transition Loss -18.400094985961914, Classifier Loss 0.052336640655994415, Total Loss 5.229983806610107\n",
      "32: Encoding Loss -0.4392111003398895, Transition Loss -12.172578811645508, Classifier Loss 0.06753706187009811, Total Loss 6.751251697540283\n",
      "32: Encoding Loss -1.3230868577957153, Transition Loss -3.3157975673675537, Classifier Loss 0.08480535447597504, Total Loss 8.479872703552246\n",
      "32: Encoding Loss -1.394489049911499, Transition Loss -12.018702507019043, Classifier Loss 0.048612579703330994, Total Loss 4.858854293823242\n",
      "32: Encoding Loss -0.9343593716621399, Transition Loss -11.62219524383545, Classifier Loss 0.07459064573049545, Total Loss 7.456739902496338\n",
      "32: Encoding Loss -1.6107438802719116, Transition Loss -5.436485290527344, Classifier Loss 0.090491384267807, Total Loss 9.048050880432129\n",
      "32: Encoding Loss -0.5595340132713318, Transition Loss -11.04241943359375, Classifier Loss 0.0359218455851078, Total Loss 3.5899760723114014\n",
      "32: Encoding Loss -1.7867580652236938, Transition Loss -12.633722305297852, Classifier Loss 0.05856773629784584, Total Loss 5.854247093200684\n",
      "32: Encoding Loss -2.0582010746002197, Transition Loss -18.607955932617188, Classifier Loss 0.04564743489027023, Total Loss 4.56102180480957\n",
      "32: Encoding Loss -1.6215252876281738, Transition Loss -15.709707260131836, Classifier Loss 0.06925828754901886, Total Loss 6.92268705368042\n",
      "32: Encoding Loss -1.2998168468475342, Transition Loss -9.249394416809082, Classifier Loss 0.06479563564062119, Total Loss 6.4777140617370605\n",
      "32: Encoding Loss -1.827340006828308, Transition Loss -7.897424221038818, Classifier Loss 0.10380177944898605, Total Loss 10.3785982131958\n",
      "32: Encoding Loss -1.7095096111297607, Transition Loss -9.849966049194336, Classifier Loss 0.060376763343811035, Total Loss 6.035706520080566\n",
      "32: Encoding Loss -1.04838228225708, Transition Loss -7.247258186340332, Classifier Loss 0.07649050652980804, Total Loss 7.647601127624512\n",
      "32: Encoding Loss -1.364190697669983, Transition Loss -11.291417121887207, Classifier Loss 0.06844090670347214, Total Loss 6.841832160949707\n",
      "32: Encoding Loss -1.9120491743087769, Transition Loss -4.969538688659668, Classifier Loss 0.10998590290546417, Total Loss 10.997596740722656\n",
      "32: Encoding Loss -0.4410437345504761, Transition Loss -1.6998193264007568, Classifier Loss 0.06312987208366394, Total Loss 6.312629222869873\n",
      "32: Encoding Loss -1.542423963546753, Transition Loss -17.342567443847656, Classifier Loss 0.08019216358661652, Total Loss 8.015748023986816\n",
      "32: Encoding Loss -0.3038277328014374, Transition Loss -8.397867202758789, Classifier Loss 0.0706653818488121, Total Loss 7.061967372894287\n",
      "32: Encoding Loss -0.6927015781402588, Transition Loss -8.56739616394043, Classifier Loss 0.12068494409322739, Total Loss 12.066781044006348\n",
      "32: Encoding Loss -1.4083044528961182, Transition Loss -8.308978080749512, Classifier Loss 0.08154296875, Total Loss 8.152634620666504\n",
      "32: Encoding Loss -1.2751412391662598, Transition Loss -8.518204689025879, Classifier Loss 0.0756755992770195, Total Loss 7.565855979919434\n",
      "32: Encoding Loss -0.6326017379760742, Transition Loss -5.349985599517822, Classifier Loss 0.10170629620552063, Total Loss 10.169559478759766\n",
      "32: Encoding Loss -1.330668330192566, Transition Loss -8.763324737548828, Classifier Loss 0.025541162118315697, Total Loss 2.552363634109497\n",
      "32: Encoding Loss -1.8821228742599487, Transition Loss -7.59497594833374, Classifier Loss 0.10716146975755692, Total Loss 10.714628219604492\n",
      "32: Encoding Loss -1.3586307764053345, Transition Loss -13.710051536560059, Classifier Loss 0.04193578287959099, Total Loss 4.190836429595947\n",
      "32: Encoding Loss -1.240830898284912, Transition Loss -5.907941818237305, Classifier Loss 0.08325785398483276, Total Loss 8.324604034423828\n",
      "32: Encoding Loss -2.4077603816986084, Transition Loss -11.69522762298584, Classifier Loss 0.06268583983182907, Total Loss 6.266244888305664\n",
      "32: Encoding Loss -1.550244688987732, Transition Loss -12.597508430480957, Classifier Loss 0.03146769851446152, Total Loss 3.1442501544952393\n",
      "32: Encoding Loss -1.3449702262878418, Transition Loss -9.530616760253906, Classifier Loss 0.10897214710712433, Total Loss 10.895308494567871\n",
      "32: Encoding Loss -0.7933828234672546, Transition Loss -10.45450210571289, Classifier Loss 0.05794326588511467, Total Loss 5.792235851287842\n",
      "32: Encoding Loss -1.412420630455017, Transition Loss -8.076517105102539, Classifier Loss 0.054326027631759644, Total Loss 5.430987358093262\n",
      "32: Encoding Loss -0.9270405173301697, Transition Loss -4.912837982177734, Classifier Loss 0.09617560356855392, Total Loss 9.616578102111816\n",
      "32: Encoding Loss -0.3096821904182434, Transition Loss -10.383451461791992, Classifier Loss 0.0915314331650734, Total Loss 9.148642539978027\n",
      "32: Encoding Loss -2.2747905254364014, Transition Loss -7.150263786315918, Classifier Loss 0.058903057128190994, Total Loss 5.888875484466553\n",
      "32: Encoding Loss -1.511602520942688, Transition Loss -5.143268585205078, Classifier Loss 0.07120063900947571, Total Loss 7.119035243988037\n",
      "32: Encoding Loss -1.633349061012268, Transition Loss -14.561809539794922, Classifier Loss 0.07014186680316925, Total Loss 7.011274337768555\n",
      "32: Encoding Loss -1.3153103590011597, Transition Loss -9.073559761047363, Classifier Loss 0.11118551343679428, Total Loss 11.11673641204834\n",
      "32: Encoding Loss -0.955589234828949, Transition Loss -7.381237506866455, Classifier Loss 0.0527668334543705, Total Loss 5.275207042694092\n",
      "32: Encoding Loss -0.596462607383728, Transition Loss -12.659724235534668, Classifier Loss 0.05983572080731392, Total Loss 5.981040000915527\n",
      "32: Encoding Loss -2.9326062202453613, Transition Loss -20.335235595703125, Classifier Loss 0.05604984238743782, Total Loss 5.600917339324951\n",
      "32: Encoding Loss -0.9057150483131409, Transition Loss -6.7137956619262695, Classifier Loss 0.08897082507610321, Total Loss 8.895739555358887\n",
      "32: Encoding Loss -1.1893402338027954, Transition Loss -6.999435901641846, Classifier Loss 0.07576259225606918, Total Loss 7.574859142303467\n",
      "32: Encoding Loss -0.6793181896209717, Transition Loss -8.96501350402832, Classifier Loss 0.07320712506771088, Total Loss 7.318919658660889\n",
      "32: Encoding Loss -0.902817964553833, Transition Loss -3.6996307373046875, Classifier Loss 0.07491901516914368, Total Loss 7.491161346435547\n",
      "32: Encoding Loss -1.4635968208312988, Transition Loss -12.38433837890625, Classifier Loss 0.07827603816986084, Total Loss 7.825127124786377\n",
      "32: Encoding Loss -1.321873426437378, Transition Loss -14.987075805664062, Classifier Loss 0.08460339903831482, Total Loss 8.457342147827148\n",
      "32: Encoding Loss -1.734220027923584, Transition Loss -12.31456470489502, Classifier Loss 0.05498511716723442, Total Loss 5.496048927307129\n",
      "32: Encoding Loss -1.2061666250228882, Transition Loss -4.092583656311035, Classifier Loss 0.03815409541130066, Total Loss 3.8145911693573\n",
      "32: Encoding Loss -1.5386080741882324, Transition Loss -6.754326343536377, Classifier Loss 0.11107282340526581, Total Loss 11.105932235717773\n",
      "32: Encoding Loss -1.601312279701233, Transition Loss -13.648683547973633, Classifier Loss 0.06646915525197983, Total Loss 6.644185543060303\n",
      "32: Encoding Loss -0.4079238176345825, Transition Loss 0.11213284730911255, Classifier Loss 0.10777316987514496, Total Loss 10.799670219421387\n",
      "32: Encoding Loss -0.8306503295898438, Transition Loss -7.704838752746582, Classifier Loss 0.0712350383400917, Total Loss 7.121962547302246\n",
      "32: Encoding Loss -0.007494575809687376, Transition Loss -4.660467147827148, Classifier Loss 0.08604821562767029, Total Loss 8.575701713562012\n",
      "32: Encoding Loss -1.1333539485931396, Transition Loss -12.942766189575195, Classifier Loss 0.05319461598992348, Total Loss 5.316873073577881\n",
      "32: Encoding Loss -1.2970023155212402, Transition Loss -16.75436782836914, Classifier Loss 0.08581812679767609, Total Loss 8.578461647033691\n",
      "32: Encoding Loss -1.1018728017807007, Transition Loss -5.394852638244629, Classifier Loss 0.08348171412944794, Total Loss 8.347092628479004\n",
      "32: Encoding Loss -1.1916686296463013, Transition Loss -9.963724136352539, Classifier Loss 0.045630279928445816, Total Loss 4.56103515625\n",
      "32: Encoding Loss -1.6343971490859985, Transition Loss -10.019645690917969, Classifier Loss 0.05896148830652237, Total Loss 5.894144535064697\n",
      "32: Encoding Loss -0.9540141224861145, Transition Loss -15.373778343200684, Classifier Loss 0.06539099663496017, Total Loss 6.536025047302246\n",
      "32: Encoding Loss -1.3414943218231201, Transition Loss -14.874547004699707, Classifier Loss 0.07694044709205627, Total Loss 7.691069602966309\n",
      "32: Encoding Loss -1.5622690916061401, Transition Loss -2.5359904766082764, Classifier Loss 0.04359041154384613, Total Loss 4.35853385925293\n",
      "32: Encoding Loss -2.1931700706481934, Transition Loss -18.90046501159668, Classifier Loss 0.045248690992593765, Total Loss 4.52108907699585\n",
      "32: Encoding Loss -1.5327930450439453, Transition Loss -17.999876022338867, Classifier Loss 0.04236355796456337, Total Loss 4.232755661010742\n",
      "32: Encoding Loss -0.6113811731338501, Transition Loss -1.741990566253662, Classifier Loss 0.06479839980602264, Total Loss 6.479491233825684\n",
      "32: Encoding Loss -0.5448436141014099, Transition Loss 1.7090997695922852, Classifier Loss 0.06247423589229584, Total Loss 6.589243412017822\n",
      "32: Encoding Loss -0.8144913911819458, Transition Loss -12.851341247558594, Classifier Loss 0.04553680494427681, Total Loss 4.55111026763916\n",
      "32: Encoding Loss -1.0022746324539185, Transition Loss -11.890359878540039, Classifier Loss 0.0518806166946888, Total Loss 5.185683727264404\n",
      "32: Encoding Loss -2.321148633956909, Transition Loss -14.345593452453613, Classifier Loss 0.061919860541820526, Total Loss 6.189116954803467\n",
      "32: Encoding Loss -1.28336763381958, Transition Loss -7.404226303100586, Classifier Loss 0.06332308053970337, Total Loss 6.330826759338379\n",
      "32: Encoding Loss -2.4251341819763184, Transition Loss -16.147247314453125, Classifier Loss 0.0448555126786232, Total Loss 4.482321739196777\n",
      "32: Encoding Loss -1.1206790208816528, Transition Loss -9.757377624511719, Classifier Loss 0.0795341283082962, Total Loss 7.951461315155029\n",
      "32: Encoding Loss -1.1580426692962646, Transition Loss -13.109296798706055, Classifier Loss 0.10044582188129425, Total Loss 10.041960716247559\n",
      "32: Encoding Loss -2.478494167327881, Transition Loss -21.060394287109375, Classifier Loss 0.06175515800714493, Total Loss 6.171303749084473\n",
      "32: Encoding Loss -1.8324042558670044, Transition Loss -9.269908905029297, Classifier Loss 0.07623790204524994, Total Loss 7.621936321258545\n",
      "32: Encoding Loss -1.5294344425201416, Transition Loss -10.942535400390625, Classifier Loss 0.11464773118495941, Total Loss 11.462584495544434\n",
      "32: Encoding Loss -1.7364345788955688, Transition Loss -16.858596801757812, Classifier Loss 0.08801418542861938, Total Loss 8.798046112060547\n",
      "32: Encoding Loss -1.7966841459274292, Transition Loss -13.459129333496094, Classifier Loss 0.029522953554987907, Total Loss 2.94960355758667\n",
      "32: Encoding Loss -0.6098687648773193, Transition Loss -12.502487182617188, Classifier Loss 0.06519430130720139, Total Loss 6.516929626464844\n",
      "32: Encoding Loss -0.9934056997299194, Transition Loss -5.776581764221191, Classifier Loss 0.04908984526991844, Total Loss 4.907829284667969\n",
      "32: Encoding Loss -0.14774280786514282, Transition Loss -9.39303207397461, Classifier Loss 0.06919082999229431, Total Loss 6.834727764129639\n",
      "32: Encoding Loss -0.4985526502132416, Transition Loss -10.480809211730957, Classifier Loss 0.05977776646614075, Total Loss 5.975679397583008\n",
      "32: Encoding Loss 0.46020931005477905, Transition Loss -5.626305103302002, Classifier Loss 0.0422995388507843, Total Loss 7.910495758056641\n",
      "32: Encoding Loss 2.0030159950256348, Transition Loss -9.04844856262207, Classifier Loss 0.040581461042165756, Total Loss 20.080463409423828\n",
      "32: Encoding Loss -0.49621519446372986, Transition Loss -14.233625411987305, Classifier Loss 0.07655714452266693, Total Loss 7.652866363525391\n",
      "32: Encoding Loss 0.5458881855010986, Transition Loss -14.02680492401123, Classifier Loss 0.06871040910482407, Total Loss 11.235340118408203\n",
      "32: Encoding Loss -1.1956441402435303, Transition Loss -0.5032994747161865, Classifier Loss 0.0991426408290863, Total Loss 9.914162635803223\n",
      "32: Encoding Loss -0.9080601930618286, Transition Loss -9.158438682556152, Classifier Loss 0.04042374715209007, Total Loss 4.040543079376221\n",
      "32: Encoding Loss -0.42821618914604187, Transition Loss -12.483171463012695, Classifier Loss 0.07739627361297607, Total Loss 7.737098693847656\n",
      "32: Encoding Loss -0.027128273621201515, Transition Loss -10.28059196472168, Classifier Loss 0.0631929263472557, Total Loss 6.231926441192627\n",
      "32: Encoding Loss -1.1210871934890747, Transition Loss -9.607304573059082, Classifier Loss 0.055457353591918945, Total Loss 5.543813705444336\n",
      "32: Encoding Loss -2.0934152603149414, Transition Loss -5.293627738952637, Classifier Loss 0.08485709130764008, Total Loss 8.484650611877441\n",
      "32: Encoding Loss -2.0581181049346924, Transition Loss -14.530220031738281, Classifier Loss 0.04022158682346344, Total Loss 4.019252777099609\n",
      "32: Encoding Loss 0.012762965634465218, Transition Loss -1.6503822803497314, Classifier Loss 0.06069357693195343, Total Loss 6.125264644622803\n",
      "32: Encoding Loss -0.7727116942405701, Transition Loss -7.420497894287109, Classifier Loss 0.03838740289211273, Total Loss 3.8372561931610107\n",
      "32: Encoding Loss -0.18245267868041992, Transition Loss -6.76133918762207, Classifier Loss 0.08397150039672852, Total Loss 8.346117973327637\n",
      "32: Encoding Loss 0.3090079724788666, Transition Loss -15.144906997680664, Classifier Loss 0.15304265916347504, Total Loss 17.770828247070312\n",
      "32: Encoding Loss -0.5229268074035645, Transition Loss -16.233928680419922, Classifier Loss 0.08614137768745422, Total Loss 8.61089038848877\n",
      "32: Encoding Loss -0.852179229259491, Transition Loss -22.267179489135742, Classifier Loss 0.049312036484479904, Total Loss 4.926750183105469\n",
      "32: Encoding Loss -0.12172137945890427, Transition Loss -11.445343017578125, Classifier Loss 0.10417039692401886, Total Loss 10.305920600891113\n",
      "32: Encoding Loss 0.5827754735946655, Transition Loss -12.971282005310059, Classifier Loss 0.06441514939069748, Total Loss 11.101125717163086\n",
      "32: Encoding Loss -2.335728406906128, Transition Loss -19.216110229492188, Classifier Loss 0.06389334797859192, Total Loss 6.385491371154785\n",
      "32: Encoding Loss -1.573185682296753, Transition Loss -7.534278869628906, Classifier Loss 0.09094875305891037, Total Loss 9.093368530273438\n",
      "32: Encoding Loss -0.8785520195960999, Transition Loss -16.06010627746582, Classifier Loss 0.08796753734350204, Total Loss 8.79354190826416\n",
      "32: Encoding Loss -1.4290542602539062, Transition Loss -16.61382484436035, Classifier Loss 0.026351936161518097, Total Loss 2.631870746612549\n",
      "32: Encoding Loss -1.3620524406433105, Transition Loss -19.637826919555664, Classifier Loss 0.06260072439908981, Total Loss 6.2561445236206055\n",
      "32: Encoding Loss -2.077188491821289, Transition Loss -25.328453063964844, Classifier Loss 0.02526300586760044, Total Loss 2.5212349891662598\n",
      "33: Encoding Loss -0.9202079176902771, Transition Loss -5.6420087814331055, Classifier Loss 0.037945568561553955, Total Loss 3.793428421020508\n",
      "33: Encoding Loss -1.115568995475769, Transition Loss -6.068085670471191, Classifier Loss 0.10884710401296616, Total Loss 10.883496284484863\n",
      "33: Encoding Loss -1.3310962915420532, Transition Loss -1.4754149913787842, Classifier Loss 0.04116835817694664, Total Loss 4.116540431976318\n",
      "33: Encoding Loss -1.148148775100708, Transition Loss -18.456890106201172, Classifier Loss 0.0652620941400528, Total Loss 6.522518157958984\n",
      "33: Encoding Loss -1.3001658916473389, Transition Loss -9.201976776123047, Classifier Loss 0.05071910470724106, Total Loss 5.070069789886475\n",
      "33: Encoding Loss -1.4594895839691162, Transition Loss -15.595596313476562, Classifier Loss 0.05441771820187569, Total Loss 5.438652992248535\n",
      "33: Encoding Loss -1.7311142683029175, Transition Loss -11.692782402038574, Classifier Loss 0.06823776662349701, Total Loss 6.821438312530518\n",
      "33: Encoding Loss -1.6121137142181396, Transition Loss -9.299195289611816, Classifier Loss 0.053606342524290085, Total Loss 5.358774662017822\n",
      "33: Encoding Loss -0.7115564346313477, Transition Loss -15.550657272338867, Classifier Loss 0.045258767902851105, Total Loss 4.522767066955566\n",
      "33: Encoding Loss -1.859588861465454, Transition Loss -12.781184196472168, Classifier Loss 0.07229911535978317, Total Loss 7.227355003356934\n",
      "33: Encoding Loss 0.418905645608902, Transition Loss -9.174789428710938, Classifier Loss 0.05844612047076225, Total Loss 9.193975448608398\n",
      "33: Encoding Loss -0.2606746256351471, Transition Loss -3.8370018005371094, Classifier Loss 0.0671449527144432, Total Loss 6.704196929931641\n",
      "33: Encoding Loss -1.2032042741775513, Transition Loss -5.498513698577881, Classifier Loss 0.053235411643981934, Total Loss 5.322441577911377\n",
      "33: Encoding Loss -0.47117912769317627, Transition Loss -6.417903900146484, Classifier Loss 0.06098312512040138, Total Loss 6.097023963928223\n",
      "33: Encoding Loss -1.7352006435394287, Transition Loss -13.871235847473145, Classifier Loss 0.04493395984172821, Total Loss 4.490621566772461\n",
      "33: Encoding Loss -2.3528761863708496, Transition Loss -18.185178756713867, Classifier Loss 0.07006962597370148, Total Loss 7.003325939178467\n",
      "33: Encoding Loss -1.021865725517273, Transition Loss -9.515271186828613, Classifier Loss 0.026094386354088783, Total Loss 2.6075356006622314\n",
      "33: Encoding Loss -0.2603757977485657, Transition Loss -5.78433895111084, Classifier Loss 0.049503009766340256, Total Loss 4.939540863037109\n",
      "33: Encoding Loss -0.1205676719546318, Transition Loss -13.234172821044922, Classifier Loss 0.08749113976955414, Total Loss 8.636537551879883\n",
      "33: Encoding Loss -0.33311447501182556, Transition Loss -5.081659317016602, Classifier Loss 0.05668194219470024, Total Loss 5.666025638580322\n",
      "33: Encoding Loss -0.8392526507377625, Transition Loss -15.124064445495605, Classifier Loss 0.08328183740377426, Total Loss 8.325159072875977\n",
      "33: Encoding Loss -1.2484095096588135, Transition Loss -10.874711990356445, Classifier Loss 0.044840507209300995, Total Loss 4.481875896453857\n",
      "33: Encoding Loss -1.4495311975479126, Transition Loss -11.626524925231934, Classifier Loss 0.07340160757303238, Total Loss 7.337835311889648\n",
      "33: Encoding Loss -0.5045420527458191, Transition Loss -3.233579158782959, Classifier Loss 0.08348961919546127, Total Loss 8.34831428527832\n",
      "33: Encoding Loss -1.7711505889892578, Transition Loss -11.655010223388672, Classifier Loss 0.03666757419705391, Total Loss 3.664426326751709\n",
      "33: Encoding Loss -0.3896067440509796, Transition Loss -4.859736442565918, Classifier Loss 0.07151520252227783, Total Loss 7.150396347045898\n",
      "33: Encoding Loss -1.5791741609573364, Transition Loss -2.9380154609680176, Classifier Loss 0.09945599734783173, Total Loss 9.945012092590332\n",
      "33: Encoding Loss -2.149819850921631, Transition Loss -19.13861083984375, Classifier Loss 0.07762272655963898, Total Loss 7.7584452629089355\n",
      "33: Encoding Loss -1.652800440788269, Transition Loss -9.395776748657227, Classifier Loss 0.06271456927061081, Total Loss 6.269577503204346\n",
      "33: Encoding Loss -0.5863415598869324, Transition Loss -4.918980598449707, Classifier Loss 0.0686526671051979, Total Loss 6.864283084869385\n",
      "33: Encoding Loss -0.6437479257583618, Transition Loss -11.568243980407715, Classifier Loss 0.07648445665836334, Total Loss 7.646131992340088\n",
      "33: Encoding Loss -0.8345382213592529, Transition Loss -2.6541616916656494, Classifier Loss 0.08689133077859879, Total Loss 8.68860149383545\n",
      "33: Encoding Loss -0.3690299987792969, Transition Loss -11.251855850219727, Classifier Loss 0.07140298187732697, Total Loss 7.1377177238464355\n",
      "33: Encoding Loss 0.25825440883636475, Transition Loss -5.656376838684082, Classifier Loss 0.17581923305988312, Total Loss 19.636695861816406\n",
      "33: Encoding Loss -2.0748417377471924, Transition Loss -13.942959785461426, Classifier Loss 0.04492293298244476, Total Loss 4.489504814147949\n",
      "33: Encoding Loss -1.1382310390472412, Transition Loss -9.103096961975098, Classifier Loss 0.04708169773221016, Total Loss 4.7063493728637695\n",
      "33: Encoding Loss -1.2746044397354126, Transition Loss -8.511266708374023, Classifier Loss 0.0797606110572815, Total Loss 7.974358558654785\n",
      "33: Encoding Loss -1.0281709432601929, Transition Loss -10.63076400756836, Classifier Loss 0.11371689289808273, Total Loss 11.369563102722168\n",
      "33: Encoding Loss -1.5904428958892822, Transition Loss -11.252639770507812, Classifier Loss 0.07060755044221878, Total Loss 7.058504581451416\n",
      "33: Encoding Loss -1.5734719038009644, Transition Loss -12.242976188659668, Classifier Loss 0.0689881443977356, Total Loss 6.896365642547607\n",
      "33: Encoding Loss -1.3331389427185059, Transition Loss -9.194040298461914, Classifier Loss 0.03848391771316528, Total Loss 3.846552848815918\n",
      "33: Encoding Loss -1.639992117881775, Transition Loss -16.87813949584961, Classifier Loss 0.0855817049741745, Total Loss 8.554794311523438\n",
      "33: Encoding Loss -1.9811553955078125, Transition Loss -16.243072509765625, Classifier Loss 0.03520987182855606, Total Loss 3.5177385807037354\n",
      "33: Encoding Loss -1.9395067691802979, Transition Loss -6.5399932861328125, Classifier Loss 0.06785286217927933, Total Loss 6.783978462219238\n",
      "33: Encoding Loss -1.615746259689331, Transition Loss -17.52334213256836, Classifier Loss 0.05031199753284454, Total Loss 5.027695178985596\n",
      "33: Encoding Loss -0.07529192417860031, Transition Loss -8.184416770935059, Classifier Loss 0.08363716304302216, Total Loss 8.226102828979492\n",
      "33: Encoding Loss -0.4081352949142456, Transition Loss -17.486637115478516, Classifier Loss 0.06942074000835419, Total Loss 6.938503742218018\n",
      "33: Encoding Loss -2.0320205688476562, Transition Loss -6.887752532958984, Classifier Loss 0.03743140399456024, Total Loss 3.74176287651062\n",
      "33: Encoding Loss -2.114882469177246, Transition Loss -9.740053176879883, Classifier Loss 0.060924332588911057, Total Loss 6.090485572814941\n",
      "33: Encoding Loss -1.5783330202102661, Transition Loss -16.390419006347656, Classifier Loss 0.054080210626125336, Total Loss 5.40474271774292\n",
      "33: Encoding Loss -2.142045021057129, Transition Loss -19.781225204467773, Classifier Loss 0.05086642876267433, Total Loss 5.082686424255371\n",
      "33: Encoding Loss -0.8182623982429504, Transition Loss -12.765296936035156, Classifier Loss 0.06601321697235107, Total Loss 6.598768711090088\n",
      "33: Encoding Loss -1.6456358432769775, Transition Loss -4.025139808654785, Classifier Loss 0.08081155270338058, Total Loss 8.080349922180176\n",
      "33: Encoding Loss -1.6550394296646118, Transition Loss -12.416296005249023, Classifier Loss 0.04963158816099167, Total Loss 4.960675239562988\n",
      "33: Encoding Loss -0.6475204229354858, Transition Loss -12.239534378051758, Classifier Loss 0.07495201379060745, Total Loss 7.492753505706787\n",
      "33: Encoding Loss -1.6627004146575928, Transition Loss -5.508849143981934, Classifier Loss 0.08734183758497238, Total Loss 8.733081817626953\n",
      "33: Encoding Loss -0.5684486031532288, Transition Loss -11.454137802124023, Classifier Loss 0.03605308383703232, Total Loss 3.603017568588257\n",
      "33: Encoding Loss -2.11340594291687, Transition Loss -13.032051086425781, Classifier Loss 0.060467056930065155, Total Loss 6.0440993309021\n",
      "33: Encoding Loss -2.3752903938293457, Transition Loss -19.410037994384766, Classifier Loss 0.04746853932738304, Total Loss 4.742971897125244\n",
      "33: Encoding Loss -1.886852741241455, Transition Loss -16.391876220703125, Classifier Loss 0.0686129778623581, Total Loss 6.8580193519592285\n",
      "33: Encoding Loss -1.5913814306259155, Transition Loss -9.606376647949219, Classifier Loss 0.06430160999298096, Total Loss 6.428239822387695\n",
      "33: Encoding Loss -2.213798761367798, Transition Loss -8.241497993469238, Classifier Loss 0.10449438542127609, Total Loss 10.447790145874023\n",
      "33: Encoding Loss -2.108733654022217, Transition Loss -10.092475891113281, Classifier Loss 0.05893240123987198, Total Loss 5.891221523284912\n",
      "33: Encoding Loss -1.2568418979644775, Transition Loss -7.506305694580078, Classifier Loss 0.07254351675510406, Total Loss 7.252850532531738\n",
      "33: Encoding Loss -1.6592520475387573, Transition Loss -11.959662437438965, Classifier Loss 0.06661421060562134, Total Loss 6.659029006958008\n",
      "33: Encoding Loss -2.147761344909668, Transition Loss -5.083591938018799, Classifier Loss 0.10721078515052795, Total Loss 10.720062255859375\n",
      "33: Encoding Loss -0.684841513633728, Transition Loss -1.8955941200256348, Classifier Loss 0.062056221067905426, Total Loss 6.205243110656738\n",
      "33: Encoding Loss -2.2783167362213135, Transition Loss -18.013229370117188, Classifier Loss 0.08360244333744049, Total Loss 8.35664176940918\n",
      "33: Encoding Loss -0.20244045555591583, Transition Loss -8.715727806091309, Classifier Loss 0.0684918761253357, Total Loss 6.812682628631592\n",
      "33: Encoding Loss -0.9258963465690613, Transition Loss -8.971049308776855, Classifier Loss 0.11375206708908081, Total Loss 11.3734130859375\n",
      "33: Encoding Loss -1.8063162565231323, Transition Loss -8.736356735229492, Classifier Loss 0.08028038591146469, Total Loss 8.026291847229004\n",
      "33: Encoding Loss -1.4002610445022583, Transition Loss -8.84548568725586, Classifier Loss 0.07433529943227768, Total Loss 7.431760787963867\n",
      "33: Encoding Loss -0.7606712579727173, Transition Loss -5.569819450378418, Classifier Loss 0.10117676109075546, Total Loss 10.116561889648438\n",
      "33: Encoding Loss -1.6223269701004028, Transition Loss -8.943475723266602, Classifier Loss 0.026003889739513397, Total Loss 2.598600387573242\n",
      "33: Encoding Loss -1.9402166604995728, Transition Loss -7.861679553985596, Classifier Loss 0.1037524864077568, Total Loss 10.373676300048828\n",
      "33: Encoding Loss -1.5042656660079956, Transition Loss -14.189655303955078, Classifier Loss 0.04111675173044205, Total Loss 4.108837127685547\n",
      "33: Encoding Loss -1.593055009841919, Transition Loss -6.5877685546875, Classifier Loss 0.08286019414663315, Total Loss 8.284701347351074\n",
      "33: Encoding Loss -2.6823322772979736, Transition Loss -12.22321605682373, Classifier Loss 0.06310692429542542, Total Loss 6.3082475662231445\n",
      "33: Encoding Loss -1.780087947845459, Transition Loss -13.270421981811523, Classifier Loss 0.03354903310537338, Total Loss 3.3522491455078125\n",
      "33: Encoding Loss -1.8988757133483887, Transition Loss -9.863618850708008, Classifier Loss 0.10844400525093079, Total Loss 10.842427253723145\n",
      "33: Encoding Loss -1.052929162979126, Transition Loss -10.859501838684082, Classifier Loss 0.05760766938328743, Total Loss 5.758594989776611\n",
      "33: Encoding Loss -2.062800407409668, Transition Loss -8.71780014038086, Classifier Loss 0.05438702926039696, Total Loss 5.436959266662598\n",
      "33: Encoding Loss -1.309363842010498, Transition Loss -5.285355567932129, Classifier Loss 0.0987907201051712, Total Loss 9.878015518188477\n",
      "33: Encoding Loss -0.5457004308700562, Transition Loss -10.871668815612793, Classifier Loss 0.09037655591964722, Total Loss 9.035481452941895\n",
      "33: Encoding Loss -2.55487322807312, Transition Loss -7.88371467590332, Classifier Loss 0.060261696577072144, Total Loss 6.024592876434326\n",
      "33: Encoding Loss -1.3289453983306885, Transition Loss -5.76591682434082, Classifier Loss 0.06729553639888763, Total Loss 6.728400707244873\n",
      "33: Encoding Loss -1.7457292079925537, Transition Loss -15.510891914367676, Classifier Loss 0.06891635805368423, Total Loss 6.888533592224121\n",
      "33: Encoding Loss -1.4685699939727783, Transition Loss -9.574368476867676, Classifier Loss 0.106491819024086, Total Loss 10.64726734161377\n",
      "33: Encoding Loss -0.6963001489639282, Transition Loss -7.77348518371582, Classifier Loss 0.051062922924757004, Total Loss 5.104737758636475\n",
      "33: Encoding Loss -1.0029218196868896, Transition Loss -13.03772258758545, Classifier Loss 0.05886796489357948, Total Loss 5.884189128875732\n",
      "33: Encoding Loss -3.5186643600463867, Transition Loss -21.340578079223633, Classifier Loss 0.05818827822804451, Total Loss 5.814559459686279\n",
      "33: Encoding Loss -1.1521626710891724, Transition Loss -7.10995626449585, Classifier Loss 0.08910296857357025, Total Loss 8.90887451171875\n",
      "33: Encoding Loss -1.7727959156036377, Transition Loss -7.78399658203125, Classifier Loss 0.07574748247861862, Total Loss 7.573191165924072\n",
      "33: Encoding Loss -0.920545756816864, Transition Loss -9.71950912475586, Classifier Loss 0.07395009696483612, Total Loss 7.393065452575684\n",
      "33: Encoding Loss -1.1987974643707275, Transition Loss -4.092894554138184, Classifier Loss 0.07476667314767838, Total Loss 7.47584867477417\n",
      "33: Encoding Loss -1.864768624305725, Transition Loss -12.724355697631836, Classifier Loss 0.078823983669281, Total Loss 7.87985372543335\n",
      "33: Encoding Loss -1.2768123149871826, Transition Loss -15.718099594116211, Classifier Loss 0.07762333750724792, Total Loss 7.759190082550049\n",
      "33: Encoding Loss -2.022383451461792, Transition Loss -12.93840217590332, Classifier Loss 0.05501483008265495, Total Loss 5.498895168304443\n",
      "33: Encoding Loss -1.29288649559021, Transition Loss -4.431858062744141, Classifier Loss 0.03764045983552933, Total Loss 3.7631595134735107\n",
      "33: Encoding Loss -2.1517982482910156, Transition Loss -7.204566955566406, Classifier Loss 0.11074948310852051, Total Loss 11.073507308959961\n",
      "33: Encoding Loss -2.138913154602051, Transition Loss -13.745368957519531, Classifier Loss 0.0656777173280716, Total Loss 6.565022945404053\n",
      "33: Encoding Loss -0.8399683237075806, Transition Loss 0.18020868301391602, Classifier Loss 0.10732588171958923, Total Loss 10.76862907409668\n",
      "33: Encoding Loss -0.7915217876434326, Transition Loss -7.856170177459717, Classifier Loss 0.0718812495470047, Total Loss 7.186553955078125\n",
      "33: Encoding Loss -0.5779315829277039, Transition Loss -4.6024627685546875, Classifier Loss 0.08470609039068222, Total Loss 8.469688415527344\n",
      "33: Encoding Loss -1.3048250675201416, Transition Loss -11.499464988708496, Classifier Loss 0.05277236923575401, Total Loss 5.274937152862549\n",
      "33: Encoding Loss -1.7554646730422974, Transition Loss -15.120487213134766, Classifier Loss 0.08440271019935608, Total Loss 8.437247276306152\n",
      "33: Encoding Loss -1.1663140058517456, Transition Loss -4.579529762268066, Classifier Loss 0.08480386435985565, Total Loss 8.479471206665039\n",
      "33: Encoding Loss -1.6284064054489136, Transition Loss -8.613200187683105, Classifier Loss 0.04342491924762726, Total Loss 4.340769290924072\n",
      "33: Encoding Loss -1.9659477472305298, Transition Loss -8.632596015930176, Classifier Loss 0.06017794460058212, Total Loss 6.016067981719971\n",
      "33: Encoding Loss -0.945041835308075, Transition Loss -13.783512115478516, Classifier Loss 0.06311728805303574, Total Loss 6.308972358703613\n",
      "33: Encoding Loss -1.3479853868484497, Transition Loss -13.439663887023926, Classifier Loss 0.07655970752239227, Total Loss 7.653282642364502\n",
      "33: Encoding Loss -1.2842631340026855, Transition Loss -1.5055506229400635, Classifier Loss 0.04422489181160927, Total Loss 4.4221882820129395\n",
      "33: Encoding Loss -2.016820192337036, Transition Loss -17.129291534423828, Classifier Loss 0.04453948885202408, Total Loss 4.4505228996276855\n",
      "33: Encoding Loss -1.8553589582443237, Transition Loss -16.202356338500977, Classifier Loss 0.040625981986522675, Total Loss 4.059357643127441\n",
      "33: Encoding Loss -0.5471280813217163, Transition Loss -1.0769604444503784, Classifier Loss 0.06436047703027725, Total Loss 6.4358320236206055\n",
      "33: Encoding Loss -0.7382757067680359, Transition Loss 2.2238216400146484, Classifier Loss 0.061145082116127014, Total Loss 6.559272289276123\n",
      "33: Encoding Loss -1.261742353439331, Transition Loss -11.459532737731934, Classifier Loss 0.04574671387672424, Total Loss 4.5723795890808105\n",
      "33: Encoding Loss -1.4149022102355957, Transition Loss -10.516036033630371, Classifier Loss 0.050777483731508255, Total Loss 5.0756449699401855\n",
      "33: Encoding Loss -2.6235036849975586, Transition Loss -12.532102584838867, Classifier Loss 0.06205224245786667, Total Loss 6.2027177810668945\n",
      "33: Encoding Loss -1.285298466682434, Transition Loss -6.268645286560059, Classifier Loss 0.06351792067289352, Total Loss 6.35053825378418\n",
      "33: Encoding Loss -2.653231620788574, Transition Loss -14.412492752075195, Classifier Loss 0.04496479034423828, Total Loss 4.49359655380249\n",
      "33: Encoding Loss -1.534209132194519, Transition Loss -8.53394889831543, Classifier Loss 0.07802930474281311, Total Loss 7.8012237548828125\n",
      "33: Encoding Loss -1.208158016204834, Transition Loss -11.680794715881348, Classifier Loss 0.10128748416900635, Total Loss 10.126411437988281\n",
      "33: Encoding Loss -2.668740749359131, Transition Loss -19.118144989013672, Classifier Loss 0.05896258354187012, Total Loss 5.892434597015381\n",
      "33: Encoding Loss -2.0359601974487305, Transition Loss -7.854215145111084, Classifier Loss 0.07483185082674026, Total Loss 7.481614589691162\n",
      "33: Encoding Loss -1.7162349224090576, Transition Loss -9.631312370300293, Classifier Loss 0.11320406198501587, Total Loss 11.318479537963867\n",
      "33: Encoding Loss -1.9533756971359253, Transition Loss -15.359749794006348, Classifier Loss 0.0884111300110817, Total Loss 8.838041305541992\n",
      "33: Encoding Loss -2.2038090229034424, Transition Loss -12.016623497009277, Classifier Loss 0.029216384515166283, Total Loss 2.9192352294921875\n",
      "33: Encoding Loss -1.1927050352096558, Transition Loss -11.06902027130127, Classifier Loss 0.06359939277172089, Total Loss 6.357725143432617\n",
      "33: Encoding Loss -1.8715916872024536, Transition Loss -4.667613983154297, Classifier Loss 0.04810091480612755, Total Loss 4.809157848358154\n",
      "33: Encoding Loss -0.9190277457237244, Transition Loss -8.004822731018066, Classifier Loss 0.06612329185009003, Total Loss 6.6107282638549805\n",
      "33: Encoding Loss -1.4017632007598877, Transition Loss -8.831421852111816, Classifier Loss 0.06001003459095955, Total Loss 5.999237060546875\n",
      "33: Encoding Loss -1.126866102218628, Transition Loss -4.3674139976501465, Classifier Loss 0.04153485596179962, Total Loss 4.152612209320068\n",
      "33: Encoding Loss -1.2017016410827637, Transition Loss -9.849851608276367, Classifier Loss 0.031826894730329514, Total Loss 3.1807193756103516\n",
      "33: Encoding Loss -1.595895528793335, Transition Loss -11.863842964172363, Classifier Loss 0.07516321539878845, Total Loss 7.513948917388916\n",
      "33: Encoding Loss -0.7201770544052124, Transition Loss -11.869058609008789, Classifier Loss 0.06535038352012634, Total Loss 6.532664775848389\n",
      "33: Encoding Loss -1.332968831062317, Transition Loss -0.18307948112487793, Classifier Loss 0.09950033575296402, Total Loss 9.949996948242188\n",
      "33: Encoding Loss -0.8711259961128235, Transition Loss -7.87285041809082, Classifier Loss 0.04022828862071037, Total Loss 4.021254539489746\n",
      "33: Encoding Loss -0.28032779693603516, Transition Loss -10.699824333190918, Classifier Loss 0.08080798387527466, Total Loss 8.072985649108887\n",
      "33: Encoding Loss -0.31698569655418396, Transition Loss -9.084178924560547, Classifier Loss 0.06052678823471069, Total Loss 6.048928260803223\n",
      "33: Encoding Loss -1.4054433107376099, Transition Loss -8.599360466003418, Classifier Loss 0.05861056596040726, Total Loss 5.859336853027344\n",
      "33: Encoding Loss -2.1640613079071045, Transition Loss -4.51862907409668, Classifier Loss 0.08609287440776825, Total Loss 8.608383178710938\n",
      "33: Encoding Loss -2.06241774559021, Transition Loss -13.370777130126953, Classifier Loss 0.042334459722042084, Total Loss 4.230772018432617\n",
      "33: Encoding Loss -0.1908693164587021, Transition Loss -1.2149505615234375, Classifier Loss 0.05909951031208038, Total Loss 5.86672306060791\n",
      "33: Encoding Loss -0.8197498917579651, Transition Loss -7.093530654907227, Classifier Loss 0.03709404915571213, Total Loss 3.707986354827881\n",
      "33: Encoding Loss -0.20809592306613922, Transition Loss -6.400054931640625, Classifier Loss 0.08610211312770844, Total Loss 8.57776927947998\n",
      "33: Encoding Loss -0.10680779069662094, Transition Loss -14.685977935791016, Classifier Loss 0.15537285804748535, Total Loss 15.41238021850586\n",
      "33: Encoding Loss -2.4114019870758057, Transition Loss -14.449195861816406, Classifier Loss 0.0809251144528389, Total Loss 8.089621543884277\n",
      "33: Encoding Loss -2.4489266872406006, Transition Loss -19.6361141204834, Classifier Loss 0.04457201808691025, Total Loss 4.453274726867676\n",
      "33: Encoding Loss -0.761792004108429, Transition Loss -10.215030670166016, Classifier Loss 0.09119456261396408, Total Loss 9.117413520812988\n",
      "33: Encoding Loss -1.9558278322219849, Transition Loss -11.41845703125, Classifier Loss 0.05884266644716263, Total Loss 5.881983280181885\n",
      "33: Encoding Loss -2.627332925796509, Transition Loss -17.355146408081055, Classifier Loss 0.06153368204832077, Total Loss 6.14989709854126\n",
      "33: Encoding Loss -1.9109327793121338, Transition Loss -5.994521617889404, Classifier Loss 0.08981026709079742, Total Loss 8.979827880859375\n",
      "33: Encoding Loss -0.831457257270813, Transition Loss -14.181122779846191, Classifier Loss 0.0877280980348587, Total Loss 8.769973754882812\n",
      "33: Encoding Loss -1.708499789237976, Transition Loss -14.931022644042969, Classifier Loss 0.02625884860754013, Total Loss 2.622898578643799\n",
      "33: Encoding Loss -1.7145901918411255, Transition Loss -17.866249084472656, Classifier Loss 0.05929943174123764, Total Loss 5.926369667053223\n",
      "33: Encoding Loss -2.6681549549102783, Transition Loss -23.261350631713867, Classifier Loss 0.02519456297159195, Total Loss 2.5148041248321533\n",
      "34: Encoding Loss -1.0967744588851929, Transition Loss -4.161496639251709, Classifier Loss 0.03666316717863083, Total Loss 3.6654844284057617\n",
      "34: Encoding Loss -1.5155673027038574, Transition Loss -4.8699727058410645, Classifier Loss 0.10586607456207275, Total Loss 10.585633277893066\n",
      "34: Encoding Loss -1.5443016290664673, Transition Loss -0.1265217363834381, Classifier Loss 0.04015093296766281, Total Loss 4.015068054199219\n",
      "34: Encoding Loss -1.4488693475723267, Transition Loss -16.866987228393555, Classifier Loss 0.06047528609633446, Total Loss 6.044155120849609\n",
      "34: Encoding Loss -1.0949244499206543, Transition Loss -7.933929920196533, Classifier Loss 0.04898334667086601, Total Loss 4.896747589111328\n",
      "34: Encoding Loss -1.5401222705841064, Transition Loss -13.802448272705078, Classifier Loss 0.05197914317250252, Total Loss 5.195153713226318\n",
      "34: Encoding Loss -2.04313588142395, Transition Loss -9.957712173461914, Classifier Loss 0.06550659984350204, Total Loss 6.548668384552002\n",
      "34: Encoding Loss -1.4712668657302856, Transition Loss -7.634477138519287, Classifier Loss 0.050918228924274445, Total Loss 5.090296268463135\n",
      "34: Encoding Loss -0.9051135778427124, Transition Loss -13.912710189819336, Classifier Loss 0.043618351221084595, Total Loss 4.359052658081055\n",
      "34: Encoding Loss -1.9987727403640747, Transition Loss -11.34037971496582, Classifier Loss 0.07059630751609802, Total Loss 7.0573625564575195\n",
      "34: Encoding Loss -0.2864347994327545, Transition Loss -7.768770217895508, Classifier Loss 0.05670378729701042, Total Loss 5.664037227630615\n",
      "34: Encoding Loss -0.9176374673843384, Transition Loss -4.441898345947266, Classifier Loss 0.06831569969654083, Total Loss 6.830681800842285\n",
      "34: Encoding Loss -1.4222099781036377, Transition Loss -6.089676380157471, Classifier Loss 0.05044970288872719, Total Loss 5.043752670288086\n",
      "34: Encoding Loss -0.5962608456611633, Transition Loss -6.729488372802734, Classifier Loss 0.06170471012592316, Total Loss 6.169125080108643\n",
      "34: Encoding Loss -2.3314013481140137, Transition Loss -13.322677612304688, Classifier Loss 0.04265523701906204, Total Loss 4.262859344482422\n",
      "34: Encoding Loss -2.4126811027526855, Transition Loss -17.053756713867188, Classifier Loss 0.06756109744310379, Total Loss 6.75269889831543\n",
      "34: Encoding Loss -0.9061837792396545, Transition Loss -9.524402618408203, Classifier Loss 0.023926852270960808, Total Loss 2.390780210494995\n",
      "34: Encoding Loss -0.3762788772583008, Transition Loss -6.173884868621826, Classifier Loss 0.050279222428798676, Total Loss 5.026434421539307\n",
      "34: Encoding Loss -0.27363988757133484, Transition Loss -12.596508979797363, Classifier Loss 0.08833763748407364, Total Loss 8.824445724487305\n",
      "34: Encoding Loss -0.8088290095329285, Transition Loss -5.5419464111328125, Classifier Loss 0.05432753264904022, Total Loss 5.431644916534424\n",
      "34: Encoding Loss -0.93328857421875, Transition Loss -14.123031616210938, Classifier Loss 0.08141672611236572, Total Loss 8.138847351074219\n",
      "34: Encoding Loss -1.4681789875030518, Transition Loss -10.545838356018066, Classifier Loss 0.0442427322268486, Total Loss 4.422163963317871\n",
      "34: Encoding Loss -1.2623616456985474, Transition Loss -11.330595970153809, Classifier Loss 0.07156015187501907, Total Loss 7.153749465942383\n",
      "34: Encoding Loss -1.078676700592041, Transition Loss -3.9312191009521484, Classifier Loss 0.080473393201828, Total Loss 8.046553611755371\n",
      "34: Encoding Loss -1.9695690870285034, Transition Loss -11.082552909851074, Classifier Loss 0.03477734699845314, Total Loss 3.475518226623535\n",
      "34: Encoding Loss -0.6144261956214905, Transition Loss -5.08194637298584, Classifier Loss 0.0680115818977356, Total Loss 6.800141334533691\n",
      "34: Encoding Loss -1.7098814249038696, Transition Loss -3.764568567276001, Classifier Loss 0.09855234622955322, Total Loss 9.85448169708252\n",
      "34: Encoding Loss -2.989506244659424, Transition Loss -17.45138168334961, Classifier Loss 0.07521796971559525, Total Loss 7.518306732177734\n",
      "34: Encoding Loss -1.8920031785964966, Transition Loss -9.146276473999023, Classifier Loss 0.0590178444981575, Total Loss 5.8999552726745605\n",
      "34: Encoding Loss -0.7041788697242737, Transition Loss -5.302692413330078, Classifier Loss 0.06879246979951859, Total Loss 6.8781867027282715\n",
      "34: Encoding Loss -0.9684750437736511, Transition Loss -10.890810012817383, Classifier Loss 0.074897401034832, Total Loss 7.4875617027282715\n",
      "34: Encoding Loss -1.218685269355774, Transition Loss -3.386624813079834, Classifier Loss 0.0850745216012001, Total Loss 8.50677490234375\n",
      "34: Encoding Loss -0.668552041053772, Transition Loss -10.557703018188477, Classifier Loss 0.06998512148857117, Total Loss 6.996400833129883\n",
      "34: Encoding Loss -0.2257523238658905, Transition Loss -5.813344478607178, Classifier Loss 0.17017273604869843, Total Loss 16.99445915222168\n",
      "34: Encoding Loss -1.6599875688552856, Transition Loss -11.75763988494873, Classifier Loss 0.04368776082992554, Total Loss 4.366424560546875\n",
      "34: Encoding Loss -0.6654677987098694, Transition Loss -7.1087188720703125, Classifier Loss 0.04288959130644798, Total Loss 4.287537097930908\n",
      "34: Encoding Loss -1.1034023761749268, Transition Loss -6.456327438354492, Classifier Loss 0.07284878194332123, Total Loss 7.2835869789123535\n",
      "34: Encoding Loss -0.7679625749588013, Transition Loss -8.444597244262695, Classifier Loss 0.1143367737531662, Total Loss 11.431988716125488\n",
      "34: Encoding Loss -1.4807560443878174, Transition Loss -9.19088363647461, Classifier Loss 0.06634515523910522, Total Loss 6.6326775550842285\n",
      "34: Encoding Loss -1.2421923875808716, Transition Loss -10.395622253417969, Classifier Loss 0.0697048231959343, Total Loss 6.968403339385986\n",
      "34: Encoding Loss -1.00294029712677, Transition Loss -7.494399547576904, Classifier Loss 0.03699181228876114, Total Loss 3.6976823806762695\n",
      "34: Encoding Loss -1.7030006647109985, Transition Loss -14.57275104522705, Classifier Loss 0.0823957696557045, Total Loss 8.236662864685059\n",
      "34: Encoding Loss -1.75100839138031, Transition Loss -13.99108600616455, Classifier Loss 0.032954879105091095, Total Loss 3.292689561843872\n",
      "34: Encoding Loss -1.6074155569076538, Transition Loss -5.334113121032715, Classifier Loss 0.06634925305843353, Total Loss 6.633858680725098\n",
      "34: Encoding Loss -1.4888420104980469, Transition Loss -15.204917907714844, Classifier Loss 0.04642648249864578, Total Loss 4.6396074295043945\n",
      "34: Encoding Loss 0.17731255292892456, Transition Loss -6.589359283447266, Classifier Loss 0.08137024194002151, Total Loss 9.50015640258789\n",
      "34: Encoding Loss 1.8069636821746826, Transition Loss -19.291833877563477, Classifier Loss 0.06633783876895905, Total Loss 21.085636138916016\n",
      "34: Encoding Loss 1.0039763450622559, Transition Loss -5.76292610168457, Classifier Loss 0.03960804268717766, Total Loss 11.991461753845215\n",
      "34: Encoding Loss -1.084343671798706, Transition Loss -11.908565521240234, Classifier Loss 0.061331264674663544, Total Loss 6.130744457244873\n",
      "34: Encoding Loss -0.6858513951301575, Transition Loss -18.288110733032227, Classifier Loss 0.052753996104002, Total Loss 5.27174186706543\n",
      "34: Encoding Loss -1.3099571466445923, Transition Loss -21.421659469604492, Classifier Loss 0.04829015955328941, Total Loss 4.824731349945068\n",
      "34: Encoding Loss -0.3106856942176819, Transition Loss -14.777542114257812, Classifier Loss 0.06948566436767578, Total Loss 6.94326114654541\n",
      "34: Encoding Loss -0.8076668381690979, Transition Loss -6.446633815765381, Classifier Loss 0.08403297513723373, Total Loss 8.402008056640625\n",
      "34: Encoding Loss -0.5176190137863159, Transition Loss -14.540578842163086, Classifier Loss 0.04962875321507454, Total Loss 4.959966659545898\n",
      "34: Encoding Loss 0.1445525735616684, Transition Loss -14.028581619262695, Classifier Loss 0.07555653899908066, Total Loss 8.623514175415039\n",
      "34: Encoding Loss -0.9919295907020569, Transition Loss -4.771901607513428, Classifier Loss 0.08755084872245789, Total Loss 8.754130363464355\n",
      "34: Encoding Loss -0.45198294520378113, Transition Loss -10.917953491210938, Classifier Loss 0.03192707896232605, Total Loss 3.1905131340026855\n",
      "34: Encoding Loss -1.4983736276626587, Transition Loss -12.727396965026855, Classifier Loss 0.061836980283260345, Total Loss 6.181152820587158\n",
      "34: Encoding Loss -1.4500654935836792, Transition Loss -19.15509796142578, Classifier Loss 0.04181664437055588, Total Loss 4.177833557128906\n",
      "34: Encoding Loss -1.1439913511276245, Transition Loss -16.084331512451172, Classifier Loss 0.06705117225646973, Total Loss 6.701900482177734\n",
      "34: Encoding Loss -0.9301712512969971, Transition Loss -9.243341445922852, Classifier Loss 0.06369736790657043, Total Loss 6.3678879737854\n",
      "34: Encoding Loss -1.2946795225143433, Transition Loss -7.780786037445068, Classifier Loss 0.10423284769058228, Total Loss 10.421728134155273\n",
      "34: Encoding Loss -1.1510440111160278, Transition Loss -9.817973136901855, Classifier Loss 0.05904750898480415, Total Loss 5.902787208557129\n",
      "34: Encoding Loss -0.987163782119751, Transition Loss -7.047114372253418, Classifier Loss 0.07260023802518845, Total Loss 7.2586140632629395\n",
      "34: Encoding Loss -0.8567495346069336, Transition Loss -11.354801177978516, Classifier Loss 0.06571323424577713, Total Loss 6.569052219390869\n",
      "34: Encoding Loss -1.4707236289978027, Transition Loss -4.435952186584473, Classifier Loss 0.1080547571182251, Total Loss 10.804588317871094\n",
      "34: Encoding Loss 0.1631006896495819, Transition Loss -0.8854198455810547, Classifier Loss 0.06015007570385933, Total Loss 7.252511501312256\n",
      "34: Encoding Loss -2.1592519283294678, Transition Loss -19.07683563232422, Classifier Loss 0.08561490476131439, Total Loss 8.557674407958984\n",
      "34: Encoding Loss -0.14651063084602356, Transition Loss -10.133337020874023, Classifier Loss 0.06731051206588745, Total Loss 6.645284175872803\n",
      "34: Encoding Loss -0.39774128794670105, Transition Loss -10.625079154968262, Classifier Loss 0.11106600612401962, Total Loss 11.104365348815918\n",
      "34: Encoding Loss -1.8245816230773926, Transition Loss -10.231569290161133, Classifier Loss 0.08010341227054596, Total Loss 8.008295059204102\n",
      "34: Encoding Loss -1.207787036895752, Transition Loss -10.419899940490723, Classifier Loss 0.07377529889345169, Total Loss 7.375446319580078\n",
      "34: Encoding Loss -0.8328501582145691, Transition Loss -6.96761417388916, Classifier Loss 0.10097352415323257, Total Loss 10.095958709716797\n",
      "34: Encoding Loss -0.673867404460907, Transition Loss -10.68993854522705, Classifier Loss 0.025284668430685997, Total Loss 2.5263290405273438\n",
      "34: Encoding Loss -1.6426328420639038, Transition Loss -9.215880393981934, Classifier Loss 0.10419412702322006, Total Loss 10.417569160461426\n",
      "34: Encoding Loss -1.2123490571975708, Transition Loss -15.49808120727539, Classifier Loss 0.04243636131286621, Total Loss 4.240536689758301\n",
      "34: Encoding Loss -1.5291465520858765, Transition Loss -8.219213485717773, Classifier Loss 0.08264626562595367, Total Loss 8.262982368469238\n",
      "34: Encoding Loss -2.1460988521575928, Transition Loss -13.725539207458496, Classifier Loss 0.06210516393184662, Total Loss 6.207771301269531\n",
      "34: Encoding Loss -1.327203392982483, Transition Loss -14.702927589416504, Classifier Loss 0.03263097256422043, Total Loss 3.2601566314697266\n",
      "34: Encoding Loss -1.6122602224349976, Transition Loss -11.359355926513672, Classifier Loss 0.1060352772474289, Total Loss 10.601256370544434\n",
      "34: Encoding Loss -0.6936063170433044, Transition Loss -12.20240592956543, Classifier Loss 0.05656673014163971, Total Loss 5.654232501983643\n",
      "34: Encoding Loss -2.0645368099212646, Transition Loss -9.874651908874512, Classifier Loss 0.05326520651578903, Total Loss 5.324545383453369\n",
      "34: Encoding Loss -1.1219819784164429, Transition Loss -6.815520286560059, Classifier Loss 0.09632264077663422, Total Loss 9.630901336669922\n",
      "34: Encoding Loss -0.5755560398101807, Transition Loss -11.760880470275879, Classifier Loss 0.09042540192604065, Total Loss 9.040188789367676\n",
      "34: Encoding Loss -1.9877785444259644, Transition Loss -9.408276557922363, Classifier Loss 0.05708238109946251, Total Loss 5.706356525421143\n",
      "34: Encoding Loss -0.8956748843193054, Transition Loss -7.125262260437012, Classifier Loss 0.06711386144161224, Total Loss 6.7099609375\n",
      "34: Encoding Loss -1.6159077882766724, Transition Loss -16.608219146728516, Classifier Loss 0.06899441033601761, Total Loss 6.896119594573975\n",
      "34: Encoding Loss -1.1740580797195435, Transition Loss -11.13123893737793, Classifier Loss 0.10941757261753082, Total Loss 10.939531326293945\n",
      "34: Encoding Loss 0.1332114040851593, Transition Loss -9.100635528564453, Classifier Loss 0.051091380417346954, Total Loss 6.075592994689941\n",
      "34: Encoding Loss 1.3447470664978027, Transition Loss -11.389878273010254, Classifier Loss 0.05805007368326187, Total Loss 16.560707092285156\n",
      "34: Encoding Loss -2.734278917312622, Transition Loss -21.737010955810547, Classifier Loss 0.05584750697016716, Total Loss 5.5804033279418945\n",
      "34: Encoding Loss -0.6193482279777527, Transition Loss -7.227468490600586, Classifier Loss 0.09172996133565903, Total Loss 9.171550750732422\n",
      "34: Encoding Loss -1.5742449760437012, Transition Loss -7.859724044799805, Classifier Loss 0.07304994761943817, Total Loss 7.303422451019287\n",
      "34: Encoding Loss -0.4450749158859253, Transition Loss -9.977523803710938, Classifier Loss 0.07529325783252716, Total Loss 7.527315139770508\n",
      "34: Encoding Loss -0.9529422521591187, Transition Loss -4.3866448402404785, Classifier Loss 0.07803831994533539, Total Loss 7.80295467376709\n",
      "34: Encoding Loss -1.4061129093170166, Transition Loss -13.364039421081543, Classifier Loss 0.08078072220087051, Total Loss 8.075399398803711\n",
      "34: Encoding Loss -0.9620870351791382, Transition Loss -16.235851287841797, Classifier Loss 0.08083248883485794, Total Loss 8.080001831054688\n",
      "34: Encoding Loss -1.038201093673706, Transition Loss -13.344466209411621, Classifier Loss 0.06013920158147812, Total Loss 6.011251449584961\n",
      "34: Encoding Loss -0.44529417157173157, Transition Loss -4.7577362060546875, Classifier Loss 0.035730041563510895, Total Loss 3.572037696838379\n",
      "34: Encoding Loss -1.2008119821548462, Transition Loss -7.695755958557129, Classifier Loss 0.10766435414552689, Total Loss 10.764896392822266\n",
      "34: Encoding Loss -1.8821074962615967, Transition Loss -14.823820114135742, Classifier Loss 0.06438891589641571, Total Loss 6.43592643737793\n",
      "34: Encoding Loss -0.4454044997692108, Transition Loss -0.38475310802459717, Classifier Loss 0.11158338189125061, Total Loss 11.158246040344238\n",
      "34: Encoding Loss -0.705055832862854, Transition Loss -8.33112621307373, Classifier Loss 0.07309088855981827, Total Loss 7.307422637939453\n",
      "34: Encoding Loss -0.8144181370735168, Transition Loss -4.999215602874756, Classifier Loss 0.08271101117134094, Total Loss 8.270101547241211\n",
      "34: Encoding Loss -1.0448319911956787, Transition Loss -12.183086395263672, Classifier Loss 0.051722947508096695, Total Loss 5.169857978820801\n",
      "34: Encoding Loss -1.1519834995269775, Transition Loss -15.888492584228516, Classifier Loss 0.08552086353302002, Total Loss 8.548908233642578\n",
      "34: Encoding Loss -1.180679202079773, Transition Loss -4.915918350219727, Classifier Loss 0.08358117938041687, Total Loss 8.357134819030762\n",
      "34: Encoding Loss -1.2062487602233887, Transition Loss -9.191981315612793, Classifier Loss 0.044404350221157074, Total Loss 4.438596725463867\n",
      "34: Encoding Loss -1.5339064598083496, Transition Loss -9.271318435668945, Classifier Loss 0.05744714289903641, Total Loss 5.742859840393066\n",
      "34: Encoding Loss -0.9772739410400391, Transition Loss -14.601195335388184, Classifier Loss 0.06227915734052658, Total Loss 6.2249956130981445\n",
      "34: Encoding Loss -1.3539971113204956, Transition Loss -14.184331893920898, Classifier Loss 0.07535476982593536, Total Loss 7.53264045715332\n",
      "34: Encoding Loss -1.4275460243225098, Transition Loss -2.1495723724365234, Classifier Loss 0.04256347939372063, Total Loss 4.255918025970459\n",
      "34: Encoding Loss -2.0601134300231934, Transition Loss -18.043418884277344, Classifier Loss 0.04296359047293663, Total Loss 4.292750358581543\n",
      "34: Encoding Loss -1.5312488079071045, Transition Loss -17.22559356689453, Classifier Loss 0.03962240368127823, Total Loss 3.9587953090667725\n",
      "34: Encoding Loss -0.5580518245697021, Transition Loss -1.58560311794281, Classifier Loss 0.06274592131376266, Total Loss 6.274274826049805\n",
      "34: Encoding Loss -0.6394885182380676, Transition Loss 1.6452208757400513, Classifier Loss 0.06008603796362877, Total Loss 6.337647914886475\n",
      "34: Encoding Loss -0.7051648497581482, Transition Loss -12.230840682983398, Classifier Loss 0.04472973570227623, Total Loss 4.470527172088623\n",
      "34: Encoding Loss -0.9737405180931091, Transition Loss -11.410922050476074, Classifier Loss 0.050185371190309525, Total Loss 5.016254901885986\n",
      "34: Encoding Loss -2.1090128421783447, Transition Loss -13.466348648071289, Classifier Loss 0.059327833354473114, Total Loss 5.930089950561523\n",
      "34: Encoding Loss -1.1247916221618652, Transition Loss -6.944199085235596, Classifier Loss 0.06261210888624191, Total Loss 6.259821891784668\n",
      "34: Encoding Loss -2.3390321731567383, Transition Loss -15.454394340515137, Classifier Loss 0.04474450275301933, Total Loss 4.4713592529296875\n",
      "34: Encoding Loss -1.0643717050552368, Transition Loss -9.19158935546875, Classifier Loss 0.07727205008268356, Total Loss 7.725366592407227\n",
      "34: Encoding Loss -1.2867881059646606, Transition Loss -12.515423774719238, Classifier Loss 0.10217707604169846, Total Loss 10.215204238891602\n",
      "34: Encoding Loss -2.4608747959136963, Transition Loss -20.049091339111328, Classifier Loss 0.05828090384602547, Total Loss 5.824080467224121\n",
      "34: Encoding Loss -1.6647108793258667, Transition Loss -8.744845390319824, Classifier Loss 0.07301291078329086, Total Loss 7.29954195022583\n",
      "34: Encoding Loss -1.368740439414978, Transition Loss -10.448295593261719, Classifier Loss 0.11132105439901352, Total Loss 11.130016326904297\n",
      "34: Encoding Loss -1.568899393081665, Transition Loss -16.239843368530273, Classifier Loss 0.08571968972682953, Total Loss 8.568720817565918\n",
      "34: Encoding Loss -1.720677137374878, Transition Loss -12.870800971984863, Classifier Loss 0.02880679816007614, Total Loss 2.878105640411377\n",
      "34: Encoding Loss -0.594839870929718, Transition Loss -11.967338562011719, Classifier Loss 0.06653200089931488, Total Loss 6.650806903839111\n",
      "34: Encoding Loss -1.009595513343811, Transition Loss -5.32568883895874, Classifier Loss 0.04745563119649887, Total Loss 4.744497776031494\n",
      "34: Encoding Loss -0.06586126238107681, Transition Loss -8.903763771057129, Classifier Loss 0.06550541520118713, Total Loss 6.414365291595459\n",
      "34: Encoding Loss -1.3719431161880493, Transition Loss -9.59946346282959, Classifier Loss 0.058222219347953796, Total Loss 5.8203020095825195\n",
      "34: Encoding Loss -0.48659834265708923, Transition Loss -5.161318778991699, Classifier Loss 0.040779683738946915, Total Loss 4.076933860778809\n",
      "34: Encoding Loss -0.7526336908340454, Transition Loss -10.75666618347168, Classifier Loss 0.031092647463083267, Total Loss 3.1071135997772217\n",
      "34: Encoding Loss -1.4040225744247437, Transition Loss -12.734502792358398, Classifier Loss 0.07325242459774017, Total Loss 7.322695732116699\n",
      "34: Encoding Loss -0.32998886704444885, Transition Loss -12.687291145324707, Classifier Loss 0.06419702619314194, Total Loss 6.415888786315918\n",
      "34: Encoding Loss -0.6476466655731201, Transition Loss -1.0083727836608887, Classifier Loss 0.09859231114387512, Total Loss 9.859029769897461\n",
      "34: Encoding Loss -0.5546501278877258, Transition Loss -8.747989654541016, Classifier Loss 0.039849527180194855, Total Loss 3.983203172683716\n",
      "34: Encoding Loss 0.13421230018138885, Transition Loss -11.740188598632812, Classifier Loss 0.08021972328424454, Total Loss 8.996928215026855\n",
      "34: Encoding Loss 0.5436915755271912, Transition Loss -8.320590019226074, Classifier Loss 0.06070148944854736, Total Loss 10.418017387390137\n",
      "34: Encoding Loss -1.1765013933181763, Transition Loss -9.202512741088867, Classifier Loss 0.056586381047964096, Total Loss 5.656797409057617\n",
      "34: Encoding Loss -1.8761801719665527, Transition Loss -4.899735450744629, Classifier Loss 0.08323508501052856, Total Loss 8.322527885437012\n",
      "34: Encoding Loss -2.222301483154297, Transition Loss -14.097223281860352, Classifier Loss 0.04237600043416023, Total Loss 4.234780311584473\n",
      "34: Encoding Loss -0.028963414952158928, Transition Loss -1.3524386882781982, Classifier Loss 0.05907246470451355, Total Loss 5.817525863647461\n",
      "34: Encoding Loss -0.8072153329849243, Transition Loss -7.6348557472229, Classifier Loss 0.03646223992109299, Total Loss 3.6446969509124756\n",
      "34: Encoding Loss -0.42258527874946594, Transition Loss -6.906320571899414, Classifier Loss 0.08351379632949829, Total Loss 8.349958419799805\n",
      "34: Encoding Loss -0.45457732677459717, Transition Loss -15.866188049316406, Classifier Loss 0.15225136280059814, Total Loss 15.221953392028809\n",
      "34: Encoding Loss -2.071890354156494, Transition Loss -15.69326400756836, Classifier Loss 0.07816073298454285, Total Loss 7.812934875488281\n",
      "34: Encoding Loss -2.656611919403076, Transition Loss -21.415508270263672, Classifier Loss 0.04553685337305069, Total Loss 4.549402236938477\n",
      "34: Encoding Loss -0.9244480729103088, Transition Loss -11.239910125732422, Classifier Loss 0.09400035440921783, Total Loss 9.397788047790527\n",
      "34: Encoding Loss -1.7993956804275513, Transition Loss -12.520482063293457, Classifier Loss 0.05845963954925537, Total Loss 5.8434600830078125\n",
      "34: Encoding Loss -2.445539712905884, Transition Loss -19.15468978881836, Classifier Loss 0.06003587320446968, Total Loss 5.999756336212158\n",
      "34: Encoding Loss -1.898128628730774, Transition Loss -6.620965480804443, Classifier Loss 0.08740182220935822, Total Loss 8.73885726928711\n",
      "34: Encoding Loss -1.02474844455719, Transition Loss -15.553706169128418, Classifier Loss 0.08654508739709854, Total Loss 8.651397705078125\n",
      "34: Encoding Loss -1.652807593345642, Transition Loss -16.389135360717773, Classifier Loss 0.02643933892250061, Total Loss 2.640655994415283\n",
      "34: Encoding Loss -1.859106183052063, Transition Loss -19.5500545501709, Classifier Loss 0.05844327062368393, Total Loss 5.84041690826416\n",
      "34: Encoding Loss -3.017040252685547, Transition Loss -25.4255313873291, Classifier Loss 0.025373339653015137, Total Loss 2.5322489738464355\n",
      "35: Encoding Loss -1.1470555067062378, Transition Loss -4.579183101654053, Classifier Loss 0.03659782186150551, Total Loss 3.6588664054870605\n",
      "35: Encoding Loss -1.5591799020767212, Transition Loss -5.307924270629883, Classifier Loss 0.10673977434635162, Total Loss 10.672916412353516\n",
      "35: Encoding Loss -1.6457570791244507, Transition Loss -0.15847080945968628, Classifier Loss 0.039292216300964355, Total Loss 3.929189920425415\n",
      "35: Encoding Loss -1.6258600950241089, Transition Loss -18.58782386779785, Classifier Loss 0.062407199293375015, Total Loss 6.237002372741699\n",
      "35: Encoding Loss -1.6061590909957886, Transition Loss -8.687154769897461, Classifier Loss 0.04969590902328491, Total Loss 4.967853546142578\n",
      "35: Encoding Loss -1.5208518505096436, Transition Loss -15.168729782104492, Classifier Loss 0.0515199676156044, Total Loss 5.14896297454834\n",
      "35: Encoding Loss -1.773267149925232, Transition Loss -11.03639030456543, Classifier Loss 0.06333613395690918, Total Loss 6.331406116485596\n",
      "35: Encoding Loss -1.4815729856491089, Transition Loss -8.375459671020508, Classifier Loss 0.052727118134498596, Total Loss 5.271036624908447\n",
      "35: Encoding Loss -1.29245924949646, Transition Loss -15.331504821777344, Classifier Loss 0.04205508530139923, Total Loss 4.202442646026611\n",
      "35: Encoding Loss -2.1366147994995117, Transition Loss -12.596236228942871, Classifier Loss 0.06937337666749954, Total Loss 6.934818744659424\n",
      "35: Encoding Loss -0.2995833158493042, Transition Loss -8.746147155761719, Classifier Loss 0.05520547926425934, Total Loss 5.515519142150879\n",
      "35: Encoding Loss -0.8102448582649231, Transition Loss -4.987411022186279, Classifier Loss 0.06741821765899658, Total Loss 6.740824222564697\n",
      "35: Encoding Loss -1.7586321830749512, Transition Loss -6.888595104217529, Classifier Loss 0.04999331384897232, Total Loss 4.99795389175415\n",
      "35: Encoding Loss -0.9380179643630981, Transition Loss -7.536648273468018, Classifier Loss 0.0586560033261776, Total Loss 5.864092826843262\n",
      "35: Encoding Loss -2.1941869258880615, Transition Loss -14.989825248718262, Classifier Loss 0.042497873306274414, Total Loss 4.246789455413818\n",
      "35: Encoding Loss -2.3904051780700684, Transition Loss -18.957408905029297, Classifier Loss 0.06759566813707352, Total Loss 6.755775451660156\n",
      "35: Encoding Loss -1.2946321964263916, Transition Loss -10.677247047424316, Classifier Loss 0.023530807346105576, Total Loss 2.350945234298706\n",
      "35: Encoding Loss -0.6140533089637756, Transition Loss -6.912417411804199, Classifier Loss 0.05010931193828583, Total Loss 5.009548664093018\n",
      "35: Encoding Loss -0.35039132833480835, Transition Loss -14.028878211975098, Classifier Loss 0.088320791721344, Total Loss 8.828630447387695\n",
      "35: Encoding Loss -0.9325058460235596, Transition Loss -6.360595703125, Classifier Loss 0.055222973227500916, Total Loss 5.52102518081665\n",
      "35: Encoding Loss -1.2449965476989746, Transition Loss -15.928194046020508, Classifier Loss 0.08070670813322067, Total Loss 8.067485809326172\n",
      "35: Encoding Loss -1.7513188123703003, Transition Loss -11.942070007324219, Classifier Loss 0.040698446333408356, Total Loss 4.067456245422363\n",
      "35: Encoding Loss -1.5900667905807495, Transition Loss -12.856117248535156, Classifier Loss 0.06992866843938828, Total Loss 6.990295886993408\n",
      "35: Encoding Loss -1.1903880834579468, Transition Loss -4.511806488037109, Classifier Loss 0.08085495233535767, Total Loss 8.084592819213867\n",
      "35: Encoding Loss -2.0951521396636963, Transition Loss -12.594216346740723, Classifier Loss 0.03578954190015793, Total Loss 3.5764353275299072\n",
      "35: Encoding Loss -0.8943433165550232, Transition Loss -5.947389602661133, Classifier Loss 0.06990543007850647, Total Loss 6.989353179931641\n",
      "35: Encoding Loss -2.020341157913208, Transition Loss -4.4284162521362305, Classifier Loss 0.09501884877681732, Total Loss 9.500998497009277\n",
      "35: Encoding Loss -2.630657434463501, Transition Loss -19.7452392578125, Classifier Loss 0.07339216768741608, Total Loss 7.335267543792725\n",
      "35: Encoding Loss -2.174204111099243, Transition Loss -10.244026184082031, Classifier Loss 0.06026511266827583, Total Loss 6.0244622230529785\n",
      "35: Encoding Loss -1.1316097974777222, Transition Loss -6.01842737197876, Classifier Loss 0.06721869111061096, Total Loss 6.720665454864502\n",
      "35: Encoding Loss -1.0160977840423584, Transition Loss -12.313347816467285, Classifier Loss 0.0734090805053711, Total Loss 7.33844518661499\n",
      "35: Encoding Loss -1.1415106058120728, Transition Loss -3.786442756652832, Classifier Loss 0.08497060835361481, Total Loss 8.49630355834961\n",
      "35: Encoding Loss -0.8016141653060913, Transition Loss -11.928680419921875, Classifier Loss 0.0666712149977684, Total Loss 6.664735794067383\n",
      "35: Encoding Loss -0.25221776962280273, Transition Loss -6.626832008361816, Classifier Loss 0.17039521038532257, Total Loss 17.02642822265625\n",
      "35: Encoding Loss -1.9277846813201904, Transition Loss -13.571667671203613, Classifier Loss 0.0449926033616066, Total Loss 4.496546268463135\n",
      "35: Encoding Loss -1.0471196174621582, Transition Loss -8.251337051391602, Classifier Loss 0.04202691465616226, Total Loss 4.201041221618652\n",
      "35: Encoding Loss -1.2373892068862915, Transition Loss -7.478166580200195, Classifier Loss 0.07216373085975647, Total Loss 7.214877128601074\n",
      "35: Encoding Loss -0.9433906078338623, Transition Loss -9.893684387207031, Classifier Loss 0.11897927522659302, Total Loss 11.89594841003418\n",
      "35: Encoding Loss -1.7248866558074951, Transition Loss -10.672122955322266, Classifier Loss 0.06731340289115906, Total Loss 6.729206085205078\n",
      "35: Encoding Loss -1.5131008625030518, Transition Loss -12.09904670715332, Classifier Loss 0.06812090426683426, Total Loss 6.809670448303223\n",
      "35: Encoding Loss -1.382725477218628, Transition Loss -8.633161544799805, Classifier Loss 0.03578362241387367, Total Loss 3.5766355991363525\n",
      "35: Encoding Loss -1.851379632949829, Transition Loss -16.8669376373291, Classifier Loss 0.0818805992603302, Total Loss 8.184686660766602\n",
      "35: Encoding Loss -2.1297714710235596, Transition Loss -16.275634765625, Classifier Loss 0.03263377398252487, Total Loss 3.260122299194336\n",
      "35: Encoding Loss -1.9971660375595093, Transition Loss -6.317549228668213, Classifier Loss 0.06688781082630157, Total Loss 6.6875176429748535\n",
      "35: Encoding Loss -1.9215534925460815, Transition Loss -17.592647552490234, Classifier Loss 0.04714566469192505, Total Loss 4.711047649383545\n",
      "35: Encoding Loss -0.304338663816452, Transition Loss -7.855119228363037, Classifier Loss 0.08072906732559204, Total Loss 8.068488121032715\n",
      "35: Encoding Loss -0.6728559136390686, Transition Loss -18.023534774780273, Classifier Loss 0.06509681046009064, Total Loss 6.506076335906982\n",
      "35: Encoding Loss -1.6655644178390503, Transition Loss -6.608394622802734, Classifier Loss 0.034340955317020416, Total Loss 3.4327738285064697\n",
      "35: Encoding Loss -1.8103843927383423, Transition Loss -9.65501880645752, Classifier Loss 0.05698718875646591, Total Loss 5.6967878341674805\n",
      "35: Encoding Loss -1.485317349433899, Transition Loss -16.49638557434082, Classifier Loss 0.050029680132865906, Total Loss 4.999668598175049\n",
      "35: Encoding Loss -2.011883020401001, Transition Loss -19.621973037719727, Classifier Loss 0.04886779561638832, Total Loss 4.882855415344238\n",
      "35: Encoding Loss -0.5570395588874817, Transition Loss -12.811392784118652, Classifier Loss 0.06609389930963516, Total Loss 6.606827735900879\n",
      "35: Encoding Loss -1.3190745115280151, Transition Loss -3.1608872413635254, Classifier Loss 0.08030769228935242, Total Loss 8.030137062072754\n",
      "35: Encoding Loss -1.5953730344772339, Transition Loss -12.484357833862305, Classifier Loss 0.04606389254331589, Total Loss 4.6038923263549805\n",
      "35: Encoding Loss -0.9754201769828796, Transition Loss -12.207244873046875, Classifier Loss 0.07095581293106079, Total Loss 7.0931396484375\n",
      "35: Encoding Loss -1.6028268337249756, Transition Loss -5.329912185668945, Classifier Loss 0.08723289519548416, Total Loss 8.722223281860352\n",
      "35: Encoding Loss -0.6719191670417786, Transition Loss -11.4095458984375, Classifier Loss 0.03384336084127426, Total Loss 3.382054090499878\n",
      "35: Encoding Loss -2.020195960998535, Transition Loss -13.155571937561035, Classifier Loss 0.05902988836169243, Total Loss 5.900357723236084\n",
      "35: Encoding Loss -2.206289529800415, Transition Loss -19.679628372192383, Classifier Loss 0.0427430234849453, Total Loss 4.270366668701172\n",
      "35: Encoding Loss -1.6595733165740967, Transition Loss -16.521648406982422, Classifier Loss 0.0669083371758461, Total Loss 6.68752908706665\n",
      "35: Encoding Loss -1.5087295770645142, Transition Loss -9.573066711425781, Classifier Loss 0.062368541955947876, Total Loss 6.2349395751953125\n",
      "35: Encoding Loss -1.8917447328567505, Transition Loss -8.188619613647461, Classifier Loss 0.10369735956192017, Total Loss 10.368098258972168\n",
      "35: Encoding Loss -1.693116545677185, Transition Loss -10.115379333496094, Classifier Loss 0.055497925728559494, Total Loss 5.547769546508789\n",
      "35: Encoding Loss -1.3000681400299072, Transition Loss -7.480377197265625, Classifier Loss 0.07065149396657944, Total Loss 7.063653469085693\n",
      "35: Encoding Loss -1.4789620637893677, Transition Loss -11.880637168884277, Classifier Loss 0.06349995732307434, Total Loss 6.347619533538818\n",
      "35: Encoding Loss -1.9240951538085938, Transition Loss -4.935544013977051, Classifier Loss 0.10573366284370422, Total Loss 10.572379112243652\n",
      "35: Encoding Loss -0.5848747491836548, Transition Loss -1.4856019020080566, Classifier Loss 0.06139872223138809, Total Loss 6.139575004577637\n",
      "35: Encoding Loss -1.8794360160827637, Transition Loss -18.43816566467285, Classifier Loss 0.07926598191261292, Total Loss 7.922910213470459\n",
      "35: Encoding Loss -0.24684852361679077, Transition Loss -8.736002922058105, Classifier Loss 0.06819423288106918, Total Loss 6.804278373718262\n",
      "35: Encoding Loss -0.8676163554191589, Transition Loss -8.914921760559082, Classifier Loss 0.11134825646877289, Total Loss 11.133042335510254\n",
      "35: Encoding Loss -1.4992210865020752, Transition Loss -8.601736068725586, Classifier Loss 0.0766913890838623, Total Loss 7.667418479919434\n",
      "35: Encoding Loss -1.317700743675232, Transition Loss -8.808990478515625, Classifier Loss 0.0738103911280632, Total Loss 7.379277229309082\n",
      "35: Encoding Loss -0.6724079251289368, Transition Loss -5.4533371925354, Classifier Loss 0.09744807332754135, Total Loss 9.7437162399292\n",
      "35: Encoding Loss -1.4607361555099487, Transition Loss -8.96203327178955, Classifier Loss 0.023196008056402206, Total Loss 2.3178083896636963\n",
      "35: Encoding Loss -1.8149771690368652, Transition Loss -7.9418182373046875, Classifier Loss 0.10032201558351517, Total Loss 10.03061294555664\n",
      "35: Encoding Loss -1.581322431564331, Transition Loss -14.550474166870117, Classifier Loss 0.03923597186803818, Total Loss 3.920686960220337\n",
      "35: Encoding Loss -1.0634883642196655, Transition Loss -6.097568988800049, Classifier Loss 0.07924845814704895, Total Loss 7.923625946044922\n",
      "35: Encoding Loss -2.663479804992676, Transition Loss -12.305298805236816, Classifier Loss 0.0607660636305809, Total Loss 6.074145317077637\n",
      "35: Encoding Loss -1.6573500633239746, Transition Loss -13.376423835754395, Classifier Loss 0.02895147167146206, Total Loss 2.8924717903137207\n",
      "35: Encoding Loss -1.362471342086792, Transition Loss -9.889171600341797, Classifier Loss 0.1055392324924469, Total Loss 10.551945686340332\n",
      "35: Encoding Loss -0.8823992609977722, Transition Loss -10.952381134033203, Classifier Loss 0.05517680570483208, Total Loss 5.5154900550842285\n",
      "35: Encoding Loss -1.4681284427642822, Transition Loss -8.555586814880371, Classifier Loss 0.05044145509600639, Total Loss 5.0424346923828125\n",
      "35: Encoding Loss -1.0342029333114624, Transition Loss -5.029443740844727, Classifier Loss 0.09984062612056732, Total Loss 9.98305606842041\n",
      "35: Encoding Loss -0.27104461193084717, Transition Loss -11.14065170288086, Classifier Loss 0.0866534635424614, Total Loss 8.65583324432373\n",
      "35: Encoding Loss -2.2888975143432617, Transition Loss -7.470090866088867, Classifier Loss 0.05677293986082077, Total Loss 5.67579984664917\n",
      "35: Encoding Loss -1.4146065711975098, Transition Loss -5.428117752075195, Classifier Loss 0.06852401047945023, Total Loss 6.851315498352051\n",
      "35: Encoding Loss -1.8762446641921997, Transition Loss -15.660605430603027, Classifier Loss 0.06540840864181519, Total Loss 6.537708759307861\n",
      "35: Encoding Loss -1.2654892206192017, Transition Loss -9.504205703735352, Classifier Loss 0.10835251957178116, Total Loss 10.833351135253906\n",
      "35: Encoding Loss -1.2092697620391846, Transition Loss -7.719974040985107, Classifier Loss 0.049532700330019, Total Loss 4.951725959777832\n",
      "35: Encoding Loss -0.8556252717971802, Transition Loss -13.44444465637207, Classifier Loss 0.056374430656433105, Total Loss 5.634754180908203\n",
      "35: Encoding Loss -3.0972464084625244, Transition Loss -21.7901668548584, Classifier Loss 0.05419464781880379, Total Loss 5.415106773376465\n",
      "35: Encoding Loss -1.0573210716247559, Transition Loss -7.060016632080078, Classifier Loss 0.08677183836698532, Total Loss 8.675771713256836\n",
      "35: Encoding Loss -1.3605247735977173, Transition Loss -7.410708427429199, Classifier Loss 0.07590015232563019, Total Loss 7.588533401489258\n",
      "35: Encoding Loss -0.8911868333816528, Transition Loss -9.482446670532227, Classifier Loss 0.06971552222967148, Total Loss 6.969655990600586\n",
      "35: Encoding Loss -1.177685260772705, Transition Loss -3.7842631340026855, Classifier Loss 0.07411675155162811, Total Loss 7.410918235778809\n",
      "35: Encoding Loss -1.6743823289871216, Transition Loss -13.013397216796875, Classifier Loss 0.07422725856304169, Total Loss 7.420123100280762\n",
      "35: Encoding Loss -1.4801996946334839, Transition Loss -16.002513885498047, Classifier Loss 0.07711997628211975, Total Loss 7.708796977996826\n",
      "35: Encoding Loss -1.8572649955749512, Transition Loss -12.924722671508789, Classifier Loss 0.05230256915092468, Total Loss 5.227672100067139\n",
      "35: Encoding Loss -1.084344506263733, Transition Loss -4.081543445587158, Classifier Loss 0.03721511363983154, Total Loss 3.7206950187683105\n",
      "35: Encoding Loss -1.6182805299758911, Transition Loss -7.005856990814209, Classifier Loss 0.10764577239751816, Total Loss 10.763175964355469\n",
      "35: Encoding Loss -1.8402607440948486, Transition Loss -14.307398796081543, Classifier Loss 0.06389184296131134, Total Loss 6.386322975158691\n",
      "35: Encoding Loss -0.5853774547576904, Transition Loss 0.5065559148788452, Classifier Loss 0.10618685185909271, Total Loss 10.719995498657227\n",
      "35: Encoding Loss -1.04215669631958, Transition Loss -8.10173225402832, Classifier Loss 0.07002720981836319, Total Loss 7.001100540161133\n",
      "35: Encoding Loss -0.42128679156303406, Transition Loss -4.758792400360107, Classifier Loss 0.08283239603042603, Total Loss 8.282244682312012\n",
      "35: Encoding Loss -1.1608179807662964, Transition Loss -11.995495796203613, Classifier Loss 0.0513843335211277, Total Loss 5.1360344886779785\n",
      "35: Encoding Loss -1.7697957754135132, Transition Loss -15.607934951782227, Classifier Loss 0.08145980536937714, Total Loss 8.14285945892334\n",
      "35: Encoding Loss -1.328585147857666, Transition Loss -4.649781703948975, Classifier Loss 0.08124928921461105, Total Loss 8.123998641967773\n",
      "35: Encoding Loss -1.1353970766067505, Transition Loss -8.725564002990723, Classifier Loss 0.04246643930673599, Total Loss 4.244898796081543\n",
      "35: Encoding Loss -1.5972784757614136, Transition Loss -8.980874061584473, Classifier Loss 0.05746643990278244, Total Loss 5.744847774505615\n",
      "35: Encoding Loss -0.9185957312583923, Transition Loss -14.319212913513184, Classifier Loss 0.06112859398126602, Total Loss 6.109995365142822\n",
      "35: Encoding Loss -1.288500428199768, Transition Loss -13.935403823852539, Classifier Loss 0.0702877938747406, Total Loss 7.025992393493652\n",
      "35: Encoding Loss -1.1185190677642822, Transition Loss -1.6997921466827393, Classifier Loss 0.04362665116786957, Total Loss 4.362325191497803\n",
      "35: Encoding Loss -1.5159929990768433, Transition Loss -17.795822143554688, Classifier Loss 0.04263924062252045, Total Loss 4.260365009307861\n",
      "35: Encoding Loss -1.8111621141433716, Transition Loss -17.014482498168945, Classifier Loss 0.03877122700214386, Total Loss 3.8737196922302246\n",
      "35: Encoding Loss -0.7229498624801636, Transition Loss -1.0177021026611328, Classifier Loss 0.06287021189928055, Total Loss 6.28681755065918\n",
      "35: Encoding Loss -0.594849169254303, Transition Loss 2.2361068725585938, Classifier Loss 0.060475874692201614, Total Loss 6.494808673858643\n",
      "35: Encoding Loss -0.8348742127418518, Transition Loss -11.783870697021484, Classifier Loss 0.04408197104930878, Total Loss 4.405839920043945\n",
      "35: Encoding Loss -1.2713189125061035, Transition Loss -10.92800521850586, Classifier Loss 0.05082322657108307, Total Loss 5.080136775970459\n",
      "35: Encoding Loss -2.179887056350708, Transition Loss -12.94924545288086, Classifier Loss 0.059624504297971725, Total Loss 5.959860801696777\n",
      "35: Encoding Loss -1.0178823471069336, Transition Loss -6.3921661376953125, Classifier Loss 0.062109872698783875, Total Loss 6.2097086906433105\n",
      "35: Encoding Loss -2.3425402641296387, Transition Loss -15.020554542541504, Classifier Loss 0.043948523700237274, Total Loss 4.391848087310791\n",
      "35: Encoding Loss -0.9318826794624329, Transition Loss -8.591449737548828, Classifier Loss 0.0773298442363739, Total Loss 7.731266021728516\n",
      "35: Encoding Loss -1.1758553981781006, Transition Loss -12.21249771118164, Classifier Loss 0.09750358015298843, Total Loss 9.747915267944336\n",
      "35: Encoding Loss -2.41801118850708, Transition Loss -19.6925106048584, Classifier Loss 0.05461691692471504, Total Loss 5.4577531814575195\n",
      "35: Encoding Loss -1.865510106086731, Transition Loss -8.218554496765137, Classifier Loss 0.07283584028482437, Total Loss 7.281940460205078\n",
      "35: Encoding Loss -1.6272395849227905, Transition Loss -9.961161613464355, Classifier Loss 0.10901415348052979, Total Loss 10.899422645568848\n",
      "35: Encoding Loss -1.439673662185669, Transition Loss -15.686792373657227, Classifier Loss 0.0846722275018692, Total Loss 8.464085578918457\n",
      "35: Encoding Loss -1.9895554780960083, Transition Loss -12.2926607131958, Classifier Loss 0.028745876625180244, Total Loss 2.872129201889038\n",
      "35: Encoding Loss -0.6688382029533386, Transition Loss -11.517694473266602, Classifier Loss 0.0609629787504673, Total Loss 6.093994140625\n",
      "35: Encoding Loss -1.7221447229385376, Transition Loss -4.85529899597168, Classifier Loss 0.0470542311668396, Total Loss 4.7044525146484375\n",
      "35: Encoding Loss -0.7737429738044739, Transition Loss -8.451238632202148, Classifier Loss 0.06754355132579803, Total Loss 6.752664566040039\n",
      "35: Encoding Loss -1.431483507156372, Transition Loss -9.137080192565918, Classifier Loss 0.058060646057128906, Total Loss 5.804237365722656\n",
      "35: Encoding Loss -0.8269320726394653, Transition Loss -4.592234134674072, Classifier Loss 0.04053002595901489, Total Loss 4.052083969116211\n",
      "35: Encoding Loss -0.9755973219871521, Transition Loss -10.214485168457031, Classifier Loss 0.031060315668582916, Total Loss 3.1039886474609375\n",
      "35: Encoding Loss -1.2315770387649536, Transition Loss -12.216554641723633, Classifier Loss 0.07661337405443192, Total Loss 7.658894062042236\n",
      "35: Encoding Loss -0.3039860427379608, Transition Loss -12.308807373046875, Classifier Loss 0.0631801038980484, Total Loss 6.3126702308654785\n",
      "35: Encoding Loss -1.0975699424743652, Transition Loss -0.25161147117614746, Classifier Loss 0.0953279510140419, Total Loss 9.532744407653809\n",
      "35: Encoding Loss -0.9167643785476685, Transition Loss -8.233051300048828, Classifier Loss 0.03919120132923126, Total Loss 3.917473554611206\n",
      "35: Encoding Loss -0.47762492299079895, Transition Loss -10.969051361083984, Classifier Loss 0.07858015596866608, Total Loss 7.855818271636963\n",
      "35: Encoding Loss -0.35145169496536255, Transition Loss -9.633429527282715, Classifier Loss 0.05788774788379669, Total Loss 5.786228656768799\n",
      "35: Encoding Loss -0.9016432166099548, Transition Loss -8.991863250732422, Classifier Loss 0.057984255254268646, Total Loss 5.796627521514893\n",
      "35: Encoding Loss -1.9117431640625, Transition Loss -4.699037551879883, Classifier Loss 0.0840422585606575, Total Loss 8.403286933898926\n",
      "35: Encoding Loss -1.9428006410598755, Transition Loss -13.93277645111084, Classifier Loss 0.04133452847599983, Total Loss 4.130666255950928\n",
      "35: Encoding Loss -0.000911862647626549, Transition Loss -1.1016714572906494, Classifier Loss 0.057584814727306366, Total Loss 5.754640102386475\n",
      "35: Encoding Loss -0.6926714777946472, Transition Loss -7.56136417388916, Classifier Loss 0.035157352685928345, Total Loss 3.5142228603363037\n",
      "35: Encoding Loss -0.11698027700185776, Transition Loss -6.840371608734131, Classifier Loss 0.08546150475740433, Total Loss 8.431507110595703\n",
      "35: Encoding Loss 0.7549800872802734, Transition Loss -15.622977256774902, Classifier Loss 0.15825094282627106, Total Loss 21.8618106842041\n",
      "35: Encoding Loss 2.2223002910614014, Transition Loss -16.036401748657227, Classifier Loss 0.09117766469717026, Total Loss 26.892959594726562\n",
      "35: Encoding Loss -1.5492782592773438, Transition Loss -21.887142181396484, Classifier Loss 0.04838475584983826, Total Loss 4.8340983390808105\n",
      "35: Encoding Loss 0.3833167850971222, Transition Loss -12.451478958129883, Classifier Loss 0.0950905978679657, Total Loss 12.57291030883789\n",
      "35: Encoding Loss -1.0036861896514893, Transition Loss -12.964693069458008, Classifier Loss 0.05791100859642029, Total Loss 5.78850793838501\n",
      "35: Encoding Loss -1.7116014957427979, Transition Loss -19.7235164642334, Classifier Loss 0.06123927980661392, Total Loss 6.119983196258545\n",
      "35: Encoding Loss -1.4459376335144043, Transition Loss -6.984209060668945, Classifier Loss 0.0840071439743042, Total Loss 8.399317741394043\n",
      "35: Encoding Loss -0.3247852027416229, Transition Loss -15.95123291015625, Classifier Loss 0.08795088529586792, Total Loss 8.790388107299805\n",
      "35: Encoding Loss -0.5741702318191528, Transition Loss -16.8850154876709, Classifier Loss 0.025337735190987587, Total Loss 2.5303964614868164\n",
      "35: Encoding Loss -1.2385677099227905, Transition Loss -20.00665283203125, Classifier Loss 0.05724044516682625, Total Loss 5.720043182373047\n",
      "35: Encoding Loss -1.4108221530914307, Transition Loss -26.123838424682617, Classifier Loss 0.02439991384744644, Total Loss 2.4347667694091797\n",
      "36: Encoding Loss -0.45388907194137573, Transition Loss -4.721975803375244, Classifier Loss 0.03778624162077904, Total Loss 3.7776694297790527\n",
      "36: Encoding Loss -1.3865000009536743, Transition Loss -5.687005996704102, Classifier Loss 0.10333935171365738, Total Loss 10.332797050476074\n",
      "36: Encoding Loss -0.9977518320083618, Transition Loss -0.31804701685905457, Classifier Loss 0.039659835398197174, Total Loss 3.9659199714660645\n",
      "36: Encoding Loss -0.9184328317642212, Transition Loss -19.074939727783203, Classifier Loss 0.06096247211098671, Total Loss 6.092432022094727\n",
      "36: Encoding Loss -0.762339174747467, Transition Loss -9.108920097351074, Classifier Loss 0.048050664365291595, Total Loss 4.803244590759277\n",
      "36: Encoding Loss -0.6481360197067261, Transition Loss -15.540822982788086, Classifier Loss 0.0514599084854126, Total Loss 5.142882823944092\n",
      "36: Encoding Loss -1.363794207572937, Transition Loss -11.35366153717041, Classifier Loss 0.06378964334726334, Total Loss 6.3766937255859375\n",
      "36: Encoding Loss -0.3856278657913208, Transition Loss -8.549102783203125, Classifier Loss 0.0537315309047699, Total Loss 5.371265888214111\n",
      "36: Encoding Loss -0.869782030582428, Transition Loss -15.75167465209961, Classifier Loss 0.042732156813144684, Total Loss 4.2700653076171875\n",
      "36: Encoding Loss -1.512338399887085, Transition Loss -12.900161743164062, Classifier Loss 0.06864058971405029, Total Loss 6.861478805541992\n",
      "36: Encoding Loss 0.30163347721099854, Transition Loss -9.088866233825684, Classifier Loss 0.053332068026065826, Total Loss 7.74137020111084\n",
      "36: Encoding Loss -0.06427054852247238, Transition Loss -4.161297798156738, Classifier Loss 0.06405951082706451, Total Loss 6.271329402923584\n",
      "36: Encoding Loss -1.33492910861969, Transition Loss -6.324808120727539, Classifier Loss 0.050404489040374756, Total Loss 5.039184093475342\n",
      "36: Encoding Loss -0.6050392389297485, Transition Loss -7.181832790374756, Classifier Loss 0.0588061586022377, Total Loss 5.87917947769165\n",
      "36: Encoding Loss -1.7307038307189941, Transition Loss -14.253223419189453, Classifier Loss 0.04243319109082222, Total Loss 4.240468502044678\n",
      "36: Encoding Loss -2.0925028324127197, Transition Loss -18.283111572265625, Classifier Loss 0.0712856873869896, Total Loss 7.124912261962891\n",
      "36: Encoding Loss -1.0951552391052246, Transition Loss -10.244755744934082, Classifier Loss 0.02318204939365387, Total Loss 2.3161559104919434\n",
      "36: Encoding Loss -0.43198826909065247, Transition Loss -6.593223571777344, Classifier Loss 0.04803244769573212, Total Loss 4.801898956298828\n",
      "36: Encoding Loss -0.044871736317873, Transition Loss -13.528707504272461, Classifier Loss 0.08458573371171951, Total Loss 8.33854866027832\n",
      "36: Encoding Loss -0.7909119129180908, Transition Loss -6.10223388671875, Classifier Loss 0.05267591401934624, Total Loss 5.266371250152588\n",
      "36: Encoding Loss -1.0338948965072632, Transition Loss -15.619638442993164, Classifier Loss 0.07735206931829453, Total Loss 7.732083320617676\n",
      "36: Encoding Loss -1.5309607982635498, Transition Loss -11.695873260498047, Classifier Loss 0.04441026598215103, Total Loss 4.438687324523926\n",
      "36: Encoding Loss -1.5560506582260132, Transition Loss -12.452868461608887, Classifier Loss 0.06856187433004379, Total Loss 6.853696823120117\n",
      "36: Encoding Loss -0.9309719800949097, Transition Loss -4.215730667114258, Classifier Loss 0.08100173622369766, Total Loss 8.09933090209961\n",
      "36: Encoding Loss -1.933335542678833, Transition Loss -12.275125503540039, Classifier Loss 0.03447813540697098, Total Loss 3.4453585147857666\n",
      "36: Encoding Loss -0.6057992577552795, Transition Loss -5.7320427894592285, Classifier Loss 0.06967435777187347, Total Loss 6.966289520263672\n",
      "36: Encoding Loss -1.8661506175994873, Transition Loss -4.227940082550049, Classifier Loss 0.09468162059783936, Total Loss 9.467316627502441\n",
      "36: Encoding Loss -2.318406581878662, Transition Loss -19.37181854248047, Classifier Loss 0.07429985702037811, Total Loss 7.426111221313477\n",
      "36: Encoding Loss -1.8932316303253174, Transition Loss -9.979560852050781, Classifier Loss 0.05805003270506859, Total Loss 5.803007125854492\n",
      "36: Encoding Loss -0.8760564923286438, Transition Loss -5.856487274169922, Classifier Loss 0.06678898632526398, Total Loss 6.677727699279785\n",
      "36: Encoding Loss -0.8589358925819397, Transition Loss -12.125057220458984, Classifier Loss 0.07279136031866074, Total Loss 7.2767109870910645\n",
      "36: Encoding Loss -0.9881227016448975, Transition Loss -3.583327293395996, Classifier Loss 0.08156491816043854, Total Loss 8.155776023864746\n",
      "36: Encoding Loss -0.7349166870117188, Transition Loss -11.78773307800293, Classifier Loss 0.06628737598657608, Total Loss 6.62637996673584\n",
      "36: Encoding Loss -0.07369402796030045, Transition Loss -6.380332946777344, Classifier Loss 0.16937537491321564, Total Loss 16.800323486328125\n",
      "36: Encoding Loss -1.6634528636932373, Transition Loss -13.49948501586914, Classifier Loss 0.04239127039909363, Total Loss 4.236427307128906\n",
      "36: Encoding Loss -0.7705919742584229, Transition Loss -8.175867080688477, Classifier Loss 0.04292314127087593, Total Loss 4.290678977966309\n",
      "36: Encoding Loss -1.0945194959640503, Transition Loss -7.364250183105469, Classifier Loss 0.07132547348737717, Total Loss 7.13107442855835\n",
      "36: Encoding Loss -0.8521583676338196, Transition Loss -9.657709121704102, Classifier Loss 0.11526758968830109, Total Loss 11.52482795715332\n",
      "36: Encoding Loss -1.5805237293243408, Transition Loss -10.558984756469727, Classifier Loss 0.06510162353515625, Total Loss 6.508050441741943\n",
      "36: Encoding Loss -1.3659588098526, Transition Loss -12.003592491149902, Classifier Loss 0.0680943951010704, Total Loss 6.8070387840271\n",
      "36: Encoding Loss -1.1440495252609253, Transition Loss -8.436628341674805, Classifier Loss 0.03542880713939667, Total Loss 3.5411934852600098\n",
      "36: Encoding Loss -1.750542163848877, Transition Loss -16.727096557617188, Classifier Loss 0.0789070799946785, Total Loss 7.887362480163574\n",
      "36: Encoding Loss -1.8280603885650635, Transition Loss -16.068294525146484, Classifier Loss 0.03247595205903053, Total Loss 3.2443816661834717\n",
      "36: Encoding Loss -1.7439607381820679, Transition Loss -6.295594692230225, Classifier Loss 0.06519301235675812, Total Loss 6.518042087554932\n",
      "36: Encoding Loss -1.732179045677185, Transition Loss -17.52524185180664, Classifier Loss 0.04606271907687187, Total Loss 4.602766513824463\n",
      "36: Encoding Loss -0.2729843854904175, Transition Loss -7.678857803344727, Classifier Loss 0.07879820466041565, Total Loss 7.871365547180176\n",
      "36: Encoding Loss -0.395704448223114, Transition Loss -18.039833068847656, Classifier Loss 0.06268169730901718, Total Loss 6.264441967010498\n",
      "36: Encoding Loss -1.4054230451583862, Transition Loss -6.523467063903809, Classifier Loss 0.03338270261883736, Total Loss 3.336965560913086\n",
      "36: Encoding Loss -1.7597248554229736, Transition Loss -9.66716194152832, Classifier Loss 0.056227125227451324, Total Loss 5.620779037475586\n",
      "36: Encoding Loss -1.2904489040374756, Transition Loss -16.372018814086914, Classifier Loss 0.0507248193025589, Total Loss 5.069207668304443\n",
      "36: Encoding Loss -1.8428230285644531, Transition Loss -19.461380004882812, Classifier Loss 0.05025313049554825, Total Loss 5.021420478820801\n",
      "36: Encoding Loss -0.3005291223526001, Transition Loss -12.678384780883789, Classifier Loss 0.06701172888278961, Total Loss 6.6954474449157715\n",
      "36: Encoding Loss -1.169650673866272, Transition Loss -2.971466302871704, Classifier Loss 0.08338499069213867, Total Loss 8.337904930114746\n",
      "36: Encoding Loss -1.4490540027618408, Transition Loss -12.280475616455078, Classifier Loss 0.04590975493192673, Total Loss 4.588519096374512\n",
      "36: Encoding Loss -0.9506322145462036, Transition Loss -12.0147123336792, Classifier Loss 0.0713157057762146, Total Loss 7.1291680335998535\n",
      "36: Encoding Loss -1.3091408014297485, Transition Loss -5.26479959487915, Classifier Loss 0.08398865163326263, Total Loss 8.397811889648438\n",
      "36: Encoding Loss -0.7786855101585388, Transition Loss -11.237709999084473, Classifier Loss 0.03217483311891556, Total Loss 3.215235710144043\n",
      "36: Encoding Loss -1.9099252223968506, Transition Loss -12.95883560180664, Classifier Loss 0.05538090690970421, Total Loss 5.535499095916748\n",
      "36: Encoding Loss -2.003265619277954, Transition Loss -19.387035369873047, Classifier Loss 0.04146513342857361, Total Loss 4.142635822296143\n",
      "36: Encoding Loss -1.4790982007980347, Transition Loss -16.244319915771484, Classifier Loss 0.06535070389509201, Total Loss 6.5318217277526855\n",
      "36: Encoding Loss -1.3800870180130005, Transition Loss -9.392057418823242, Classifier Loss 0.06113079935312271, Total Loss 6.111201763153076\n",
      "36: Encoding Loss -1.74311101436615, Transition Loss -7.987310886383057, Classifier Loss 0.10175015777349472, Total Loss 10.173418045043945\n",
      "36: Encoding Loss -1.5788809061050415, Transition Loss -9.814306259155273, Classifier Loss 0.05390769988298416, Total Loss 5.38880729675293\n",
      "36: Encoding Loss -1.1620177030563354, Transition Loss -7.25738525390625, Classifier Loss 0.07066517323255539, Total Loss 7.065065860748291\n",
      "36: Encoding Loss -1.3553904294967651, Transition Loss -11.593987464904785, Classifier Loss 0.06624935567378998, Total Loss 6.622616767883301\n",
      "36: Encoding Loss -1.7072789669036865, Transition Loss -4.677248001098633, Classifier Loss 0.1030886098742485, Total Loss 10.3079252243042\n",
      "36: Encoding Loss -0.2651328444480896, Transition Loss -1.294919729232788, Classifier Loss 0.060121722519397736, Total Loss 6.003410339355469\n",
      "36: Encoding Loss -1.5726096630096436, Transition Loss -18.29407501220703, Classifier Loss 0.08013111352920532, Total Loss 8.009451866149902\n",
      "36: Encoding Loss -0.1757071316242218, Transition Loss -8.567336082458496, Classifier Loss 0.067280113697052, Total Loss 6.670840740203857\n",
      "36: Encoding Loss -0.8127652406692505, Transition Loss -8.452147483825684, Classifier Loss 0.11338280141353607, Total Loss 11.336588859558105\n",
      "36: Encoding Loss -1.1587880849838257, Transition Loss -8.189483642578125, Classifier Loss 0.07512801140546799, Total Loss 7.511163234710693\n",
      "36: Encoding Loss -1.1045632362365723, Transition Loss -8.399077415466309, Classifier Loss 0.07221078127622604, Total Loss 7.219398021697998\n",
      "36: Encoding Loss -0.5843288898468018, Transition Loss -5.189123153686523, Classifier Loss 0.09638452529907227, Total Loss 9.637414932250977\n",
      "36: Encoding Loss -1.3794782161712646, Transition Loss -8.543848037719727, Classifier Loss 0.023222163319587708, Total Loss 2.320507526397705\n",
      "36: Encoding Loss -1.6272449493408203, Transition Loss -7.656339645385742, Classifier Loss 0.10110239684581757, Total Loss 10.108708381652832\n",
      "36: Encoding Loss -1.332065224647522, Transition Loss -14.166584968566895, Classifier Loss 0.0380985289812088, Total Loss 3.8070194721221924\n",
      "36: Encoding Loss -0.7672372460365295, Transition Loss -5.782959938049316, Classifier Loss 0.07804416865110397, Total Loss 7.80325984954834\n",
      "36: Encoding Loss -2.319068670272827, Transition Loss -12.004599571228027, Classifier Loss 0.06016883999109268, Total Loss 6.0144829750061035\n",
      "36: Encoding Loss -1.5482197999954224, Transition Loss -12.949773788452148, Classifier Loss 0.02820119448006153, Total Loss 2.8175294399261475\n",
      "36: Encoding Loss -1.07779860496521, Transition Loss -9.495073318481445, Classifier Loss 0.10339465737342834, Total Loss 10.337567329406738\n",
      "36: Encoding Loss -0.7375799417495728, Transition Loss -10.696916580200195, Classifier Loss 0.05434897914528847, Total Loss 5.432758331298828\n",
      "36: Encoding Loss -1.1933536529541016, Transition Loss -8.331979751586914, Classifier Loss 0.050678931176662445, Total Loss 5.066226482391357\n",
      "36: Encoding Loss -0.8027055263519287, Transition Loss -4.811757564544678, Classifier Loss 0.0951296016573906, Total Loss 9.511998176574707\n",
      "36: Encoding Loss -0.2184886336326599, Transition Loss -10.985937118530273, Classifier Loss 0.08526265621185303, Total Loss 8.49881362915039\n",
      "36: Encoding Loss -1.9218966960906982, Transition Loss -7.090200901031494, Classifier Loss 0.05501805990934372, Total Loss 5.500387668609619\n",
      "36: Encoding Loss -1.0943140983581543, Transition Loss -5.162167072296143, Classifier Loss 0.06761819869279861, Total Loss 6.760787487030029\n",
      "36: Encoding Loss -1.5648510456085205, Transition Loss -15.16051197052002, Classifier Loss 0.0658576488494873, Total Loss 6.582732677459717\n",
      "36: Encoding Loss -1.1014164686203003, Transition Loss -8.9466552734375, Classifier Loss 0.10551762580871582, Total Loss 10.549973487854004\n",
      "36: Encoding Loss -1.1603515148162842, Transition Loss -7.371721267700195, Classifier Loss 0.052206046879291534, Total Loss 5.219130516052246\n",
      "36: Encoding Loss -0.6449235677719116, Transition Loss -13.170119285583496, Classifier Loss 0.05521699786186218, Total Loss 5.519065856933594\n",
      "36: Encoding Loss -2.655853748321533, Transition Loss -21.213336944580078, Classifier Loss 0.054368749260902405, Total Loss 5.432631969451904\n",
      "36: Encoding Loss -0.699977457523346, Transition Loss -6.723915100097656, Classifier Loss 0.08707448840141296, Total Loss 8.706104278564453\n",
      "36: Encoding Loss -1.1027418375015259, Transition Loss -7.008119106292725, Classifier Loss 0.07425998151302338, Total Loss 7.424596786499023\n",
      "36: Encoding Loss -0.6906436681747437, Transition Loss -9.056840896606445, Classifier Loss 0.06869334727525711, Total Loss 6.867523193359375\n",
      "36: Encoding Loss -0.8914501667022705, Transition Loss -3.457585334777832, Classifier Loss 0.07347425073385239, Total Loss 7.346733570098877\n",
      "36: Encoding Loss -1.2871657609939575, Transition Loss -12.617439270019531, Classifier Loss 0.07488115131855011, Total Loss 7.485591888427734\n",
      "36: Encoding Loss -1.222274899482727, Transition Loss -15.528165817260742, Classifier Loss 0.07587414979934692, Total Loss 7.584309101104736\n",
      "36: Encoding Loss -1.5959312915802002, Transition Loss -12.300789833068848, Classifier Loss 0.05195210129022598, Total Loss 5.192749977111816\n",
      "36: Encoding Loss -0.8876828551292419, Transition Loss -3.7039904594421387, Classifier Loss 0.037185996770858765, Total Loss 3.7178590297698975\n",
      "36: Encoding Loss -1.4011255502700806, Transition Loss -6.655583381652832, Classifier Loss 0.10427595674991608, Total Loss 10.426264762878418\n",
      "36: Encoding Loss -1.596274971961975, Transition Loss -13.872308731079102, Classifier Loss 0.0642477497458458, Total Loss 6.422000885009766\n",
      "36: Encoding Loss -0.42742112278938293, Transition Loss 0.8247589468955994, Classifier Loss 0.1057056188583374, Total Loss 10.735481262207031\n",
      "36: Encoding Loss -0.6918578743934631, Transition Loss -7.879368305206299, Classifier Loss 0.07049670815467834, Total Loss 7.048094749450684\n",
      "36: Encoding Loss -0.24408912658691406, Transition Loss -4.5124993324279785, Classifier Loss 0.08072012662887573, Total Loss 8.056805610656738\n",
      "36: Encoding Loss -0.8468847274780273, Transition Loss -11.597810745239258, Classifier Loss 0.04975603520870209, Total Loss 4.9732842445373535\n",
      "36: Encoding Loss -1.6077046394348145, Transition Loss -15.160317420959473, Classifier Loss 0.08307560533285141, Total Loss 8.304529190063477\n",
      "36: Encoding Loss -0.8098178505897522, Transition Loss -4.412594795227051, Classifier Loss 0.08087724447250366, Total Loss 8.08684253692627\n",
      "36: Encoding Loss -0.8072332739830017, Transition Loss -8.380840301513672, Classifier Loss 0.0424530953168869, Total Loss 4.243633270263672\n",
      "36: Encoding Loss -1.277750015258789, Transition Loss -8.5813627243042, Classifier Loss 0.05647215247154236, Total Loss 5.645499229431152\n",
      "36: Encoding Loss -0.6395227909088135, Transition Loss -13.760099411010742, Classifier Loss 0.05983436107635498, Total Loss 5.980684280395508\n",
      "36: Encoding Loss -0.8370054364204407, Transition Loss -13.563671112060547, Classifier Loss 0.0705927237868309, Total Loss 7.0565595626831055\n",
      "36: Encoding Loss -0.6867355108261108, Transition Loss -1.450413703918457, Classifier Loss 0.04387761652469635, Total Loss 4.387471675872803\n",
      "36: Encoding Loss -0.9726435542106628, Transition Loss -17.160808563232422, Classifier Loss 0.04179975017905235, Total Loss 4.17654275894165\n",
      "36: Encoding Loss -1.4333847761154175, Transition Loss -16.505069732666016, Classifier Loss 0.03681208938360214, Total Loss 3.677907943725586\n",
      "36: Encoding Loss -0.4540756344795227, Transition Loss -0.7068493962287903, Classifier Loss 0.061091359704732895, Total Loss 6.10898494720459\n",
      "36: Encoding Loss -0.2407284528017044, Transition Loss 2.2320642471313477, Classifier Loss 0.06115855276584625, Total Loss 6.546792507171631\n",
      "36: Encoding Loss -0.48296669125556946, Transition Loss -11.169819831848145, Classifier Loss 0.043315205723047256, Total Loss 4.329283714294434\n",
      "36: Encoding Loss -0.9587412476539612, Transition Loss -10.330991744995117, Classifier Loss 0.04874947667121887, Total Loss 4.8728814125061035\n",
      "36: Encoding Loss -1.834488034248352, Transition Loss -12.2362642288208, Classifier Loss 0.05895411595702171, Total Loss 5.8929643630981445\n",
      "36: Encoding Loss -0.5887971520423889, Transition Loss -5.838349342346191, Classifier Loss 0.06178504228591919, Total Loss 6.1773362159729\n",
      "36: Encoding Loss -1.895962119102478, Transition Loss -14.359903335571289, Classifier Loss 0.04268617928028107, Total Loss 4.265746116638184\n",
      "36: Encoding Loss -0.4467000961303711, Transition Loss -8.024900436401367, Classifier Loss 0.07529636472463608, Total Loss 7.528017044067383\n",
      "36: Encoding Loss -0.7805073857307434, Transition Loss -11.860627174377441, Classifier Loss 0.10314884781837463, Total Loss 10.31251335144043\n",
      "36: Encoding Loss -2.0198874473571777, Transition Loss -18.992738723754883, Classifier Loss 0.05565329268574715, Total Loss 5.561530590057373\n",
      "36: Encoding Loss -1.3934290409088135, Transition Loss -7.858622074127197, Classifier Loss 0.0724295824766159, Total Loss 7.241386413574219\n",
      "36: Encoding Loss -1.3904626369476318, Transition Loss -9.482892036437988, Classifier Loss 0.10727578401565552, Total Loss 10.72568130493164\n",
      "36: Encoding Loss -1.1404331922531128, Transition Loss -15.0957670211792, Classifier Loss 0.08456675708293915, Total Loss 8.453656196594238\n",
      "36: Encoding Loss -1.593870759010315, Transition Loss -11.730978012084961, Classifier Loss 0.02812628448009491, Total Loss 2.8102822303771973\n",
      "36: Encoding Loss -0.4228222668170929, Transition Loss -11.035337448120117, Classifier Loss 0.061496712267398834, Total Loss 6.14742374420166\n",
      "36: Encoding Loss -1.482784390449524, Transition Loss -4.537195205688477, Classifier Loss 0.045973729342222214, Total Loss 4.596465587615967\n",
      "36: Encoding Loss -0.6629894971847534, Transition Loss -8.058940887451172, Classifier Loss 0.06435177475214005, Total Loss 6.433565616607666\n",
      "36: Encoding Loss -1.0587960481643677, Transition Loss -8.756856918334961, Classifier Loss 0.05771429091691971, Total Loss 5.769677639007568\n",
      "36: Encoding Loss -0.7373809218406677, Transition Loss -4.241848945617676, Classifier Loss 0.038438666611909866, Total Loss 3.8430182933807373\n",
      "36: Encoding Loss -0.822580099105835, Transition Loss -9.738451957702637, Classifier Loss 0.029439561069011688, Total Loss 2.9420084953308105\n",
      "36: Encoding Loss -0.7998043298721313, Transition Loss -11.781231880187988, Classifier Loss 0.07234875857830048, Total Loss 7.232519626617432\n",
      "36: Encoding Loss -0.11158549785614014, Transition Loss -11.884737014770508, Classifier Loss 0.06270956993103027, Total Loss 6.150529384613037\n",
      "36: Encoding Loss -0.8715518712997437, Transition Loss 0.1803286075592041, Classifier Loss 0.09366657584905624, Total Loss 9.40272331237793\n",
      "36: Encoding Loss -0.7828556299209595, Transition Loss -7.879182815551758, Classifier Loss 0.03929876163601875, Total Loss 3.928300142288208\n",
      "36: Encoding Loss -0.4189775288105011, Transition Loss -10.379137992858887, Classifier Loss 0.0775953158736229, Total Loss 7.75740909576416\n",
      "36: Encoding Loss -0.027785878628492355, Transition Loss -9.295110702514648, Classifier Loss 0.05683165416121483, Total Loss 5.594490051269531\n",
      "36: Encoding Loss -0.7020559906959534, Transition Loss -8.906606674194336, Classifier Loss 0.053028274327516556, Total Loss 5.301045894622803\n",
      "36: Encoding Loss -1.6568665504455566, Transition Loss -4.608180999755859, Classifier Loss 0.08308766037225723, Total Loss 8.307845115661621\n",
      "36: Encoding Loss -1.72191321849823, Transition Loss -13.915447235107422, Classifier Loss 0.03843662887811661, Total Loss 3.8408799171447754\n",
      "36: Encoding Loss 0.04187052324414253, Transition Loss -1.0026954412460327, Classifier Loss 0.056190647184848785, Total Loss 5.840705394744873\n",
      "36: Encoding Loss -0.6966907978057861, Transition Loss -8.265753746032715, Classifier Loss 0.03540536016225815, Total Loss 3.5388827323913574\n",
      "36: Encoding Loss 0.10800742357969284, Transition Loss -7.701086044311523, Classifier Loss 0.08508146554231644, Total Loss 9.249650001525879\n",
      "36: Encoding Loss 0.8314290642738342, Transition Loss -15.208982467651367, Classifier Loss 0.1165807768702507, Total Loss 18.306467056274414\n",
      "36: Encoding Loss -1.7815020084381104, Transition Loss -18.947397232055664, Classifier Loss 0.07669418305158615, Total Loss 7.665628910064697\n",
      "36: Encoding Loss -2.15494966506958, Transition Loss -25.508317947387695, Classifier Loss 0.0448978990316391, Total Loss 4.4846882820129395\n",
      "36: Encoding Loss 0.22774173319339752, Transition Loss -14.216133117675781, Classifier Loss 0.09192019701004028, Total Loss 10.990375518798828\n",
      "36: Encoding Loss 0.10360438376665115, Transition Loss -10.055646896362305, Classifier Loss 0.055876389145851135, Total Loss 6.290061950683594\n",
      "36: Encoding Loss -1.9764612913131714, Transition Loss -22.667354583740234, Classifier Loss 0.06039213016629219, Total Loss 6.034679889678955\n",
      "36: Encoding Loss -1.1682637929916382, Transition Loss -9.392568588256836, Classifier Loss 0.08770103752613068, Total Loss 8.768224716186523\n",
      "36: Encoding Loss -0.8114675283432007, Transition Loss -19.197628021240234, Classifier Loss 0.08744657039642334, Total Loss 8.74081802368164\n",
      "36: Encoding Loss -1.4287230968475342, Transition Loss -19.892295837402344, Classifier Loss 0.024240300059318542, Total Loss 2.4200515747070312\n",
      "36: Encoding Loss -0.8194873332977295, Transition Loss -23.349123001098633, Classifier Loss 0.058774176985025406, Total Loss 5.872747898101807\n",
      "36: Encoding Loss -2.125265598297119, Transition Loss -30.137710571289062, Classifier Loss 0.024577276781201363, Total Loss 2.451700210571289\n",
      "37: Encoding Loss -0.7583261728286743, Transition Loss -7.537474155426025, Classifier Loss 0.03553440421819687, Total Loss 3.5519328117370605\n",
      "37: Encoding Loss -0.680371105670929, Transition Loss -7.853839874267578, Classifier Loss 0.10367029160261154, Total Loss 10.365458488464355\n",
      "37: Encoding Loss -1.0368163585662842, Transition Loss -2.5739264488220215, Classifier Loss 0.038225673139095306, Total Loss 3.822052478790283\n",
      "37: Encoding Loss -0.9702045321464539, Transition Loss -21.95870018005371, Classifier Loss 0.05926993489265442, Total Loss 5.922601699829102\n",
      "37: Encoding Loss -1.3658969402313232, Transition Loss -11.294604301452637, Classifier Loss 0.047938767820596695, Total Loss 4.7916178703308105\n",
      "37: Encoding Loss -1.1020756959915161, Transition Loss -18.941709518432617, Classifier Loss 0.0500376857817173, Total Loss 4.9999799728393555\n",
      "37: Encoding Loss -1.2503083944320679, Transition Loss -14.241074562072754, Classifier Loss 0.06307291984558105, Total Loss 6.304443836212158\n",
      "37: Encoding Loss -1.3717116117477417, Transition Loss -11.730157852172852, Classifier Loss 0.0511137880384922, Total Loss 5.10903263092041\n",
      "37: Encoding Loss -0.08679032325744629, Transition Loss -18.655847549438477, Classifier Loss 0.04170277342200279, Total Loss 4.03273344039917\n",
      "37: Encoding Loss -1.379794716835022, Transition Loss -15.741125106811523, Classifier Loss 0.06726392358541489, Total Loss 6.7232441902160645\n",
      "37: Encoding Loss 0.8388136625289917, Transition Loss -11.480962753295898, Classifier Loss 0.05421298369765282, Total Loss 12.129511833190918\n",
      "37: Encoding Loss -0.22534914314746857, Transition Loss -3.7962605953216553, Classifier Loss 0.06233333423733711, Total Loss 6.210735321044922\n",
      "37: Encoding Loss -1.164802074432373, Transition Loss -5.246352672576904, Classifier Loss 0.0506385862827301, Total Loss 5.062809467315674\n",
      "37: Encoding Loss -0.3458954691886902, Transition Loss -6.347222328186035, Classifier Loss 0.05759609490633011, Total Loss 5.757590293884277\n",
      "37: Encoding Loss -1.5089401006698608, Transition Loss -12.999734878540039, Classifier Loss 0.04155174270272255, Total Loss 4.15257453918457\n",
      "37: Encoding Loss -2.3255155086517334, Transition Loss -16.998428344726562, Classifier Loss 0.06671113520860672, Total Loss 6.667713642120361\n",
      "37: Encoding Loss -0.9948264956474304, Transition Loss -9.192605018615723, Classifier Loss 0.024089626967906952, Total Loss 2.4071242809295654\n",
      "37: Encoding Loss -0.1535288244485855, Transition Loss -5.679743766784668, Classifier Loss 0.04583923891186714, Total Loss 4.506199836730957\n",
      "37: Encoding Loss -0.22903145849704742, Transition Loss -12.27365779876709, Classifier Loss 0.08297769725322723, Total Loss 8.27515697479248\n",
      "37: Encoding Loss 0.08537066727876663, Transition Loss -4.6007537841796875, Classifier Loss 0.05343763530254364, Total Loss 5.891514301300049\n",
      "37: Encoding Loss -0.03634602203965187, Transition Loss -17.527023315429688, Classifier Loss 0.07875524461269379, Total Loss 7.767886161804199\n",
      "37: Encoding Loss -1.829211950302124, Transition Loss -13.174580574035645, Classifier Loss 0.04063867777585983, Total Loss 4.061232566833496\n",
      "37: Encoding Loss -1.6551495790481567, Transition Loss -14.09364128112793, Classifier Loss 0.0695720836520195, Total Loss 6.954389572143555\n",
      "37: Encoding Loss -1.2778244018554688, Transition Loss -5.817051887512207, Classifier Loss 0.07956276834011078, Total Loss 7.955113410949707\n",
      "37: Encoding Loss -2.3779776096343994, Transition Loss -13.803627014160156, Classifier Loss 0.031679119914770126, Total Loss 3.165151357650757\n",
      "37: Encoding Loss -0.9914185404777527, Transition Loss -6.921776294708252, Classifier Loss 0.06805417686700821, Total Loss 6.804033279418945\n",
      "37: Encoding Loss -2.1393587589263916, Transition Loss -5.495416164398193, Classifier Loss 0.09408833086490631, Total Loss 9.407734870910645\n",
      "37: Encoding Loss -2.993044853210449, Transition Loss -21.272207260131836, Classifier Loss 0.07181447744369507, Total Loss 7.177193641662598\n",
      "37: Encoding Loss -2.1526219844818115, Transition Loss -11.582756042480469, Classifier Loss 0.058184485882520676, Total Loss 5.816132068634033\n",
      "37: Encoding Loss -1.2764816284179688, Transition Loss -7.111496448516846, Classifier Loss 0.06731714308261871, Total Loss 6.730291843414307\n",
      "37: Encoding Loss -1.271246075630188, Transition Loss -13.461427688598633, Classifier Loss 0.07117319107055664, Total Loss 7.114626884460449\n",
      "37: Encoding Loss -1.0816510915756226, Transition Loss -4.862128734588623, Classifier Loss 0.08148697018623352, Total Loss 8.147724151611328\n",
      "37: Encoding Loss -0.8442382216453552, Transition Loss -13.019076347351074, Classifier Loss 0.06615984439849854, Total Loss 6.613380432128906\n",
      "37: Encoding Loss -0.2554578185081482, Transition Loss -7.905322551727295, Classifier Loss 0.16821503639221191, Total Loss 16.809057235717773\n",
      "37: Encoding Loss -1.997489333152771, Transition Loss -14.768741607666016, Classifier Loss 0.042346905916929245, Total Loss 4.23173713684082\n",
      "37: Encoding Loss -0.9641616344451904, Transition Loss -9.529048919677734, Classifier Loss 0.04209315776824951, Total Loss 4.207409858703613\n",
      "37: Encoding Loss -1.6599067449569702, Transition Loss -8.813183784484863, Classifier Loss 0.0722355917096138, Total Loss 7.22179651260376\n",
      "37: Encoding Loss -1.084205150604248, Transition Loss -11.141561508178711, Classifier Loss 0.11601796001195908, Total Loss 11.599567413330078\n",
      "37: Encoding Loss -1.8927654027938843, Transition Loss -12.207191467285156, Classifier Loss 0.06433846056461334, Total Loss 6.4314045906066895\n",
      "37: Encoding Loss -1.6725386381149292, Transition Loss -13.335226058959961, Classifier Loss 0.06572612375020981, Total Loss 6.569945335388184\n",
      "37: Encoding Loss -1.40150785446167, Transition Loss -9.865824699401855, Classifier Loss 0.034979164600372314, Total Loss 3.495943307876587\n",
      "37: Encoding Loss -2.214550018310547, Transition Loss -18.368261337280273, Classifier Loss 0.08036722242832184, Total Loss 8.033048629760742\n",
      "37: Encoding Loss -2.09981369972229, Transition Loss -17.6212100982666, Classifier Loss 0.03183206170797348, Total Loss 3.1796817779541016\n",
      "37: Encoding Loss -2.002169132232666, Transition Loss -7.407345294952393, Classifier Loss 0.061128340661525726, Total Loss 6.111352443695068\n",
      "37: Encoding Loss -1.8249962329864502, Transition Loss -19.012897491455078, Classifier Loss 0.04618949443101883, Total Loss 4.615146636962891\n",
      "37: Encoding Loss -0.3786954879760742, Transition Loss -9.021099090576172, Classifier Loss 0.0779842957854271, Total Loss 7.796393871307373\n",
      "37: Encoding Loss -0.13847336173057556, Transition Loss -19.29827308654785, Classifier Loss 0.06326395273208618, Total Loss 6.230515003204346\n",
      "37: Encoding Loss -1.5240066051483154, Transition Loss -7.694611549377441, Classifier Loss 0.03319936245679855, Total Loss 3.318397283554077\n",
      "37: Encoding Loss -2.147887706756592, Transition Loss -11.08279037475586, Classifier Loss 0.0554431676864624, Total Loss 5.542100429534912\n",
      "37: Encoding Loss -1.4224151372909546, Transition Loss -18.04861831665039, Classifier Loss 0.04972226172685623, Total Loss 4.968616485595703\n",
      "37: Encoding Loss -1.711480975151062, Transition Loss -21.522775650024414, Classifier Loss 0.048749037086963654, Total Loss 4.870599269866943\n",
      "37: Encoding Loss -0.3600926697254181, Transition Loss -14.222922325134277, Classifier Loss 0.06583722680807114, Total Loss 6.580420970916748\n",
      "37: Encoding Loss -1.636398196220398, Transition Loss -4.685728073120117, Classifier Loss 0.08156921714544296, Total Loss 8.155983924865723\n",
      "37: Encoding Loss -1.7807329893112183, Transition Loss -13.696681022644043, Classifier Loss 0.04496970400214195, Total Loss 4.4942307472229\n",
      "37: Encoding Loss -0.8460368514060974, Transition Loss -13.569877624511719, Classifier Loss 0.07273845374584198, Total Loss 7.2711310386657715\n",
      "37: Encoding Loss -1.2851879596710205, Transition Loss -6.398468971252441, Classifier Loss 0.08627969026565552, Total Loss 8.626688957214355\n",
      "37: Encoding Loss -0.4672302305698395, Transition Loss -12.733687400817871, Classifier Loss 0.03313601762056351, Total Loss 3.311049461364746\n",
      "37: Encoding Loss -2.0823843479156494, Transition Loss -14.565949440002441, Classifier Loss 0.055054496973752975, Total Loss 5.502536773681641\n",
      "37: Encoding Loss -2.353687047958374, Transition Loss -21.26202392578125, Classifier Loss 0.04065759479999542, Total Loss 4.061507225036621\n",
      "37: Encoding Loss -1.4952210187911987, Transition Loss -18.254547119140625, Classifier Loss 0.06432203203439713, Total Loss 6.428552150726318\n",
      "37: Encoding Loss -2.0206220149993896, Transition Loss -10.807037353515625, Classifier Loss 0.060924701392650604, Total Loss 6.090308666229248\n",
      "37: Encoding Loss -2.1330859661102295, Transition Loss -9.426647186279297, Classifier Loss 0.1013360321521759, Total Loss 10.131717681884766\n",
      "37: Encoding Loss -1.8758831024169922, Transition Loss -11.352495193481445, Classifier Loss 0.053591638803482056, Total Loss 5.356893062591553\n",
      "37: Encoding Loss -1.2046445608139038, Transition Loss -8.473005294799805, Classifier Loss 0.07080596685409546, Total Loss 7.078902244567871\n",
      "37: Encoding Loss -1.6268442869186401, Transition Loss -13.214120864868164, Classifier Loss 0.06481575965881348, Total Loss 6.478933334350586\n",
      "37: Encoding Loss -1.9540117979049683, Transition Loss -6.030033111572266, Classifier Loss 0.102904312312603, Total Loss 10.289224624633789\n",
      "37: Encoding Loss -0.4331029951572418, Transition Loss -2.6754207611083984, Classifier Loss 0.05837083235383034, Total Loss 5.836522579193115\n",
      "37: Encoding Loss -2.4611878395080566, Transition Loss -20.09627914428711, Classifier Loss 0.08210966736078262, Total Loss 8.206948280334473\n",
      "37: Encoding Loss -0.017752474173903465, Transition Loss -10.183327674865723, Classifier Loss 0.06731565296649933, Total Loss 6.668524742126465\n",
      "37: Encoding Loss -0.8985467553138733, Transition Loss -9.988693237304688, Classifier Loss 0.11106561869382858, Total Loss 11.10456371307373\n",
      "37: Encoding Loss -1.932255506515503, Transition Loss -9.719189643859863, Classifier Loss 0.07641372829675674, Total Loss 7.639428615570068\n",
      "37: Encoding Loss -1.4829292297363281, Transition Loss -9.778179168701172, Classifier Loss 0.07116589695215225, Total Loss 7.114634037017822\n",
      "37: Encoding Loss -0.995545506477356, Transition Loss -6.487666606903076, Classifier Loss 0.097660593688488, Total Loss 9.764760971069336\n",
      "37: Encoding Loss -1.6256707906723022, Transition Loss -9.995390892028809, Classifier Loss 0.024458112195134163, Total Loss 2.443812131881714\n",
      "37: Encoding Loss -2.0342395305633545, Transition Loss -8.906867980957031, Classifier Loss 0.09836457669734955, Total Loss 9.834675788879395\n",
      "37: Encoding Loss -1.5827833414077759, Transition Loss -15.351924896240234, Classifier Loss 0.03793370723724365, Total Loss 3.7903003692626953\n",
      "37: Encoding Loss -1.6271387338638306, Transition Loss -7.588467597961426, Classifier Loss 0.07810147851705551, Total Loss 7.808629989624023\n",
      "37: Encoding Loss -2.65815806388855, Transition Loss -13.383150100708008, Classifier Loss 0.06031543016433716, Total Loss 6.028866291046143\n",
      "37: Encoding Loss -1.9623832702636719, Transition Loss -14.642738342285156, Classifier Loss 0.02998894639313221, Total Loss 2.9959661960601807\n",
      "37: Encoding Loss -1.898255705833435, Transition Loss -10.953418731689453, Classifier Loss 0.10224202275276184, Total Loss 10.22201156616211\n",
      "37: Encoding Loss -1.0459755659103394, Transition Loss -11.956561088562012, Classifier Loss 0.05487643927335739, Total Loss 5.485252380371094\n",
      "37: Encoding Loss -1.9741450548171997, Transition Loss -9.697742462158203, Classifier Loss 0.05002103000879288, Total Loss 5.0001630783081055\n",
      "37: Encoding Loss -1.4510540962219238, Transition Loss -6.082898139953613, Classifier Loss 0.09446612745523453, Total Loss 9.445395469665527\n",
      "37: Encoding Loss -0.5943981409072876, Transition Loss -11.758246421813965, Classifier Loss 0.08458150923252106, Total Loss 8.455799102783203\n",
      "37: Encoding Loss -2.635551691055298, Transition Loss -8.737662315368652, Classifier Loss 0.05588853731751442, Total Loss 5.587106227874756\n",
      "37: Encoding Loss -1.4785079956054688, Transition Loss -6.648123741149902, Classifier Loss 0.06915880739688873, Total Loss 6.914551258087158\n",
      "37: Encoding Loss -1.9860981702804565, Transition Loss -16.725473403930664, Classifier Loss 0.06741974502801895, Total Loss 6.738629341125488\n",
      "37: Encoding Loss -1.5353268384933472, Transition Loss -10.684324264526367, Classifier Loss 0.10585896670818329, Total Loss 10.583759307861328\n",
      "37: Encoding Loss -1.0054080486297607, Transition Loss -8.63542366027832, Classifier Loss 0.0499902218580246, Total Loss 4.997294902801514\n",
      "37: Encoding Loss -1.2199699878692627, Transition Loss -14.281436920166016, Classifier Loss 0.054815299808979034, Total Loss 5.478673934936523\n",
      "37: Encoding Loss -3.392314910888672, Transition Loss -22.945655822753906, Classifier Loss 0.05232522636651993, Total Loss 5.227933406829834\n",
      "37: Encoding Loss -1.3268815279006958, Transition Loss -7.821354866027832, Classifier Loss 0.08566753566265106, Total Loss 8.565189361572266\n",
      "37: Encoding Loss -1.9271975755691528, Transition Loss -8.795205116271973, Classifier Loss 0.07271134853363037, Total Loss 7.269375801086426\n",
      "37: Encoding Loss -1.0409072637557983, Transition Loss -10.630901336669922, Classifier Loss 0.068821981549263, Total Loss 6.880072116851807\n",
      "37: Encoding Loss -1.4057056903839111, Transition Loss -4.803645133972168, Classifier Loss 0.07082440704107285, Total Loss 7.081480026245117\n",
      "37: Encoding Loss -1.8445160388946533, Transition Loss -13.940256118774414, Classifier Loss 0.07424265891313553, Total Loss 7.421477794647217\n",
      "37: Encoding Loss -1.5199681520462036, Transition Loss -17.081632614135742, Classifier Loss 0.07480240613222122, Total Loss 7.4768242835998535\n",
      "37: Encoding Loss -2.08951735496521, Transition Loss -14.101310729980469, Classifier Loss 0.053793106228113174, Total Loss 5.376490116119385\n",
      "37: Encoding Loss -1.2619705200195312, Transition Loss -5.322849273681641, Classifier Loss 0.03585086762905121, Total Loss 3.584022283554077\n",
      "37: Encoding Loss -2.0278942584991455, Transition Loss -8.09765625, Classifier Loss 0.10360722988843918, Total Loss 10.359103202819824\n",
      "37: Encoding Loss -2.265552282333374, Transition Loss -15.075576782226562, Classifier Loss 0.061880387365818024, Total Loss 6.185023784637451\n",
      "37: Encoding Loss -0.943601667881012, Transition Loss -0.373762845993042, Classifier Loss 0.10402493178844452, Total Loss 10.402419090270996\n",
      "37: Encoding Loss -1.1663005352020264, Transition Loss -8.733744621276855, Classifier Loss 0.06891948729753494, Total Loss 6.890202045440674\n",
      "37: Encoding Loss -0.7487441897392273, Transition Loss -5.1117353439331055, Classifier Loss 0.08009707927703857, Total Loss 8.008686065673828\n",
      "37: Encoding Loss -1.4303597211837769, Transition Loss -12.320405960083008, Classifier Loss 0.049705155193805695, Total Loss 4.968051433563232\n",
      "37: Encoding Loss -1.9261492490768433, Transition Loss -16.373741149902344, Classifier Loss 0.08020614087581635, Total Loss 8.017338752746582\n",
      "37: Encoding Loss -1.482941746711731, Transition Loss -5.241097450256348, Classifier Loss 0.07705941796302795, Total Loss 7.704893589019775\n",
      "37: Encoding Loss -1.6423434019088745, Transition Loss -9.737314224243164, Classifier Loss 0.04222569987177849, Total Loss 4.220622539520264\n",
      "37: Encoding Loss -1.93815279006958, Transition Loss -9.622782707214355, Classifier Loss 0.05489005148410797, Total Loss 5.4870805740356445\n",
      "37: Encoding Loss -1.1151915788650513, Transition Loss -14.869869232177734, Classifier Loss 0.058092765510082245, Total Loss 5.806302547454834\n",
      "37: Encoding Loss -1.5177513360977173, Transition Loss -14.48729133605957, Classifier Loss 0.07207168638706207, Total Loss 7.20427131652832\n",
      "37: Encoding Loss -1.431647539138794, Transition Loss -2.0861635208129883, Classifier Loss 0.04229307547211647, Total Loss 4.228890419006348\n",
      "37: Encoding Loss -1.9581506252288818, Transition Loss -18.494190216064453, Classifier Loss 0.04160725325345993, Total Loss 4.157026290893555\n",
      "37: Encoding Loss -1.8210861682891846, Transition Loss -17.666471481323242, Classifier Loss 0.037211477756500244, Total Loss 3.7176144123077393\n",
      "37: Encoding Loss -0.7608298659324646, Transition Loss -1.6095038652420044, Classifier Loss 0.060484085232019424, Total Loss 6.048086643218994\n",
      "37: Encoding Loss -0.8514169454574585, Transition Loss 1.6588022708892822, Classifier Loss 0.05969680845737457, Total Loss 6.301441192626953\n",
      "37: Encoding Loss -1.089476466178894, Transition Loss -12.561741828918457, Classifier Loss 0.04257718101143837, Total Loss 4.2552056312561035\n",
      "37: Encoding Loss -1.38169264793396, Transition Loss -11.571531295776367, Classifier Loss 0.04812455177307129, Total Loss 4.810141086578369\n",
      "37: Encoding Loss -2.477797508239746, Transition Loss -13.780927658081055, Classifier Loss 0.05896409600973129, Total Loss 5.893653392791748\n",
      "37: Encoding Loss -1.3047935962677002, Transition Loss -7.073495864868164, Classifier Loss 0.06061454117298126, Total Loss 6.060039520263672\n",
      "37: Encoding Loss -2.7000319957733154, Transition Loss -15.781242370605469, Classifier Loss 0.042413339018821716, Total Loss 4.23817777633667\n",
      "37: Encoding Loss -1.3809654712677002, Transition Loss -9.593607902526855, Classifier Loss 0.07416746020317078, Total Loss 7.414827346801758\n",
      "37: Encoding Loss -1.3514339923858643, Transition Loss -12.841052055358887, Classifier Loss 0.09739689528942108, Total Loss 9.73712158203125\n",
      "37: Encoding Loss -2.764254093170166, Transition Loss -20.522884368896484, Classifier Loss 0.05632011219859123, Total Loss 5.627906799316406\n",
      "37: Encoding Loss -2.0810155868530273, Transition Loss -8.992949485778809, Classifier Loss 0.07010902464389801, Total Loss 7.009103775024414\n",
      "37: Encoding Loss -1.8264338970184326, Transition Loss -10.48709487915039, Classifier Loss 0.10585741698741913, Total Loss 10.58364486694336\n",
      "37: Encoding Loss -1.8908737897872925, Transition Loss -16.58049964904785, Classifier Loss 0.08433488756418228, Total Loss 8.43017292022705\n",
      "37: Encoding Loss -2.1515743732452393, Transition Loss -13.236896514892578, Classifier Loss 0.0279034823179245, Total Loss 2.787700891494751\n",
      "37: Encoding Loss -1.0996109247207642, Transition Loss -11.965557098388672, Classifier Loss 0.06373615562915802, Total Loss 6.371222496032715\n",
      "37: Encoding Loss -1.8973387479782104, Transition Loss -5.3837785720825195, Classifier Loss 0.04558451473712921, Total Loss 4.557374954223633\n",
      "37: Encoding Loss -1.0021790266036987, Transition Loss -8.962160110473633, Classifier Loss 0.06499988585710526, Total Loss 6.498196125030518\n",
      "37: Encoding Loss -1.5870486497879028, Transition Loss -9.767095565795898, Classifier Loss 0.05744295194745064, Total Loss 5.7423415184021\n",
      "37: Encoding Loss -1.24142324924469, Transition Loss -5.158920764923096, Classifier Loss 0.03720690310001373, Total Loss 3.719658374786377\n",
      "37: Encoding Loss -1.1281906366348267, Transition Loss -10.775625228881836, Classifier Loss 0.029039757326245308, Total Loss 2.901820659637451\n",
      "37: Encoding Loss -1.5252761840820312, Transition Loss -13.023777961730957, Classifier Loss 0.07230072468519211, Total Loss 7.2274675369262695\n",
      "37: Encoding Loss -0.6512471437454224, Transition Loss -12.9434814453125, Classifier Loss 0.06121552363038063, Total Loss 6.118963718414307\n",
      "37: Encoding Loss -1.3370611667633057, Transition Loss -0.7398011684417725, Classifier Loss 0.09251105785369873, Total Loss 9.250958442687988\n",
      "37: Encoding Loss -1.0502711534500122, Transition Loss -8.943431854248047, Classifier Loss 0.037839021533727646, Total Loss 3.782113552093506\n",
      "37: Encoding Loss -0.5360082387924194, Transition Loss -11.732735633850098, Classifier Loss 0.07671304047107697, Total Loss 7.668957710266113\n",
      "37: Encoding Loss -0.5252944231033325, Transition Loss -10.028818130493164, Classifier Loss 0.05599554628133774, Total Loss 5.597548484802246\n",
      "37: Encoding Loss -1.28880774974823, Transition Loss -9.720836639404297, Classifier Loss 0.052132975310087204, Total Loss 5.211353302001953\n",
      "37: Encoding Loss -2.233344316482544, Transition Loss -5.474687099456787, Classifier Loss 0.08298709988594055, Total Loss 8.297615051269531\n",
      "37: Encoding Loss -2.255962610244751, Transition Loss -14.720024108886719, Classifier Loss 0.03969055414199829, Total Loss 3.966111421585083\n",
      "37: Encoding Loss -0.3332323729991913, Transition Loss -1.7379560470581055, Classifier Loss 0.05547795072197914, Total Loss 5.546299934387207\n",
      "37: Encoding Loss -0.8136375546455383, Transition Loss -8.10810661315918, Classifier Loss 0.03432817384600639, Total Loss 3.4311957359313965\n",
      "37: Encoding Loss -0.25552812218666077, Transition Loss -7.511988639831543, Classifier Loss 0.08388558775186539, Total Loss 8.376211166381836\n",
      "37: Encoding Loss -0.1819642037153244, Transition Loss -16.407299041748047, Classifier Loss 0.14657215774059296, Total Loss 14.603848457336426\n",
      "37: Encoding Loss -2.431894540786743, Transition Loss -16.532230377197266, Classifier Loss 0.07554960250854492, Total Loss 7.551653861999512\n",
      "37: Encoding Loss -2.511542320251465, Transition Loss -22.276927947998047, Classifier Loss 0.0416216142475605, Total Loss 4.157705783843994\n",
      "37: Encoding Loss -0.9293748736381531, Transition Loss -11.833489418029785, Classifier Loss 0.09206805378198624, Total Loss 9.204438209533691\n",
      "37: Encoding Loss -2.022739887237549, Transition Loss -13.359247207641602, Classifier Loss 0.05416497588157654, Total Loss 5.413825988769531\n",
      "37: Encoding Loss -2.467961549758911, Transition Loss -19.82552146911621, Classifier Loss 0.05613783374428749, Total Loss 5.609818458557129\n",
      "37: Encoding Loss -1.8913912773132324, Transition Loss -7.367484092712402, Classifier Loss 0.08847068250179291, Total Loss 8.84559440612793\n",
      "37: Encoding Loss -1.0782418251037598, Transition Loss -16.270626068115234, Classifier Loss 0.08555768430233002, Total Loss 8.55251407623291\n",
      "37: Encoding Loss -1.788398027420044, Transition Loss -17.170236587524414, Classifier Loss 0.02401847019791603, Total Loss 2.3984131813049316\n",
      "37: Encoding Loss -1.6793133020401, Transition Loss -20.320068359375, Classifier Loss 0.05696597695350647, Total Loss 5.692533493041992\n",
      "37: Encoding Loss -2.778927803039551, Transition Loss -26.383623123168945, Classifier Loss 0.020594019442796707, Total Loss 2.0541253089904785\n",
      "38: Encoding Loss -1.2614235877990723, Transition Loss -5.432568550109863, Classifier Loss 0.034631941467523575, Total Loss 3.4621076583862305\n",
      "38: Encoding Loss -1.6660170555114746, Transition Loss -6.187617301940918, Classifier Loss 0.10181473940610886, Total Loss 10.180235862731934\n",
      "38: Encoding Loss -1.774774432182312, Transition Loss -0.9591236114501953, Classifier Loss 0.03778086230158806, Total Loss 3.7778942584991455\n",
      "38: Encoding Loss -1.5427911281585693, Transition Loss -19.209022521972656, Classifier Loss 0.0596943199634552, Total Loss 5.965590000152588\n",
      "38: Encoding Loss -1.3291250467300415, Transition Loss -9.478446960449219, Classifier Loss 0.04619339853525162, Total Loss 4.617444038391113\n",
      "38: Encoding Loss -1.4998918771743774, Transition Loss -15.871983528137207, Classifier Loss 0.04888594150543213, Total Loss 4.885419845581055\n",
      "38: Encoding Loss -1.9704886674880981, Transition Loss -11.643645286560059, Classifier Loss 0.06010826304554939, Total Loss 6.00849723815918\n",
      "38: Encoding Loss -1.4644240140914917, Transition Loss -9.062393188476562, Classifier Loss 0.04880157485604286, Total Loss 4.878345012664795\n",
      "38: Encoding Loss -0.7406327724456787, Transition Loss -16.002885818481445, Classifier Loss 0.039465390145778656, Total Loss 3.943338394165039\n",
      "38: Encoding Loss -2.0152981281280518, Transition Loss -13.204376220703125, Classifier Loss 0.06611145287752151, Total Loss 6.608504772186279\n",
      "38: Encoding Loss -0.3776094317436218, Transition Loss -9.2921142578125, Classifier Loss 0.05352902412414551, Total Loss 5.350803375244141\n",
      "38: Encoding Loss -0.8338397145271301, Transition Loss -5.541956901550293, Classifier Loss 0.06347610056400299, Total Loss 6.34650182723999\n",
      "38: Encoding Loss -1.5818331241607666, Transition Loss -7.460406303405762, Classifier Loss 0.0468997023999691, Total Loss 4.688477993011475\n",
      "38: Encoding Loss -0.7569102048873901, Transition Loss -8.206746101379395, Classifier Loss 0.05830460786819458, Total Loss 5.828819274902344\n",
      "38: Encoding Loss -2.215667247772217, Transition Loss -15.534985542297363, Classifier Loss 0.038269151002168655, Total Loss 3.823807954788208\n",
      "38: Encoding Loss -2.423555612564087, Transition Loss -19.61648178100586, Classifier Loss 0.06200272589921951, Total Loss 6.196349143981934\n",
      "38: Encoding Loss -1.1726460456848145, Transition Loss -11.487439155578613, Classifier Loss 0.02214653417468071, Total Loss 2.2123560905456543\n",
      "38: Encoding Loss -0.5044800639152527, Transition Loss -7.5197601318359375, Classifier Loss 0.04640232399106026, Total Loss 4.63872766494751\n",
      "38: Encoding Loss -0.4478512108325958, Transition Loss -14.708563804626465, Classifier Loss 0.08337653428316116, Total Loss 8.334697723388672\n",
      "38: Encoding Loss -0.8606604933738708, Transition Loss -6.943875312805176, Classifier Loss 0.05241624638438225, Total Loss 5.240236282348633\n",
      "38: Encoding Loss -1.0096360445022583, Transition Loss -16.475553512573242, Classifier Loss 0.07751010358333588, Total Loss 7.747715473175049\n",
      "38: Encoding Loss -1.7466827630996704, Transition Loss -12.609649658203125, Classifier Loss 0.03805551305413246, Total Loss 3.8030292987823486\n",
      "38: Encoding Loss -1.5197468996047974, Transition Loss -13.443953514099121, Classifier Loss 0.06554622948169708, Total Loss 6.551934242248535\n",
      "38: Encoding Loss -1.228184461593628, Transition Loss -5.232210159301758, Classifier Loss 0.0775614082813263, Total Loss 7.755094051361084\n",
      "38: Encoding Loss -2.0617241859436035, Transition Loss -13.24958610534668, Classifier Loss 0.03184987232089043, Total Loss 3.1823372840881348\n",
      "38: Encoding Loss -0.7032025456428528, Transition Loss -6.506814002990723, Classifier Loss 0.06517861783504486, Total Loss 6.5165605545043945\n",
      "38: Encoding Loss -2.048558235168457, Transition Loss -5.104997634887695, Classifier Loss 0.09370876848697662, Total Loss 9.369855880737305\n",
      "38: Encoding Loss -2.9383037090301514, Transition Loss -20.421140670776367, Classifier Loss 0.07148682326078415, Total Loss 7.144598007202148\n",
      "38: Encoding Loss -2.01045298576355, Transition Loss -10.995393753051758, Classifier Loss 0.05709889158606529, Total Loss 5.7076897621154785\n",
      "38: Encoding Loss -0.8977687954902649, Transition Loss -6.786799430847168, Classifier Loss 0.06439575552940369, Total Loss 6.438218116760254\n",
      "38: Encoding Loss -1.2211954593658447, Transition Loss -12.975883483886719, Classifier Loss 0.07006099075078964, Total Loss 7.003504276275635\n",
      "38: Encoding Loss -1.315675973892212, Transition Loss -4.411138534545898, Classifier Loss 0.07992978394031525, Total Loss 7.992096424102783\n",
      "38: Encoding Loss -0.8567098379135132, Transition Loss -12.454769134521484, Classifier Loss 0.06541245430707932, Total Loss 6.538754463195801\n",
      "38: Encoding Loss -0.10608845204114914, Transition Loss -7.153698921203613, Classifier Loss 0.17007458209991455, Total Loss 16.883499145507812\n",
      "38: Encoding Loss -1.7527083158493042, Transition Loss -14.177154541015625, Classifier Loss 0.0416446216404438, Total Loss 4.161626815795898\n",
      "38: Encoding Loss -0.7535209655761719, Transition Loss -8.797929763793945, Classifier Loss 0.041424527764320374, Total Loss 4.140693187713623\n",
      "38: Encoding Loss -1.1873666048049927, Transition Loss -8.139313697814941, Classifier Loss 0.07058022916316986, Total Loss 7.056395053863525\n",
      "38: Encoding Loss -0.9382396936416626, Transition Loss -10.314558029174805, Classifier Loss 0.1157996878027916, Total Loss 11.577905654907227\n",
      "38: Encoding Loss -1.6754051446914673, Transition Loss -11.528826713562012, Classifier Loss 0.06316367536783218, Total Loss 6.314061641693115\n",
      "38: Encoding Loss -1.4109681844711304, Transition Loss -12.859426498413086, Classifier Loss 0.06432498246431351, Total Loss 6.42992639541626\n",
      "38: Encoding Loss -1.106408953666687, Transition Loss -9.33830451965332, Classifier Loss 0.03391558676958084, Total Loss 3.38969087600708\n",
      "38: Encoding Loss -1.8921221494674683, Transition Loss -17.592491149902344, Classifier Loss 0.07677169144153595, Total Loss 7.673650741577148\n",
      "38: Encoding Loss -1.804817795753479, Transition Loss -16.91433334350586, Classifier Loss 0.03145962208509445, Total Loss 3.1425793170928955\n",
      "38: Encoding Loss -1.808794617652893, Transition Loss -6.985671043395996, Classifier Loss 0.06187150999903679, Total Loss 6.18575382232666\n",
      "38: Encoding Loss -1.640357255935669, Transition Loss -18.249408721923828, Classifier Loss 0.04479939863085747, Total Loss 4.476290225982666\n",
      "38: Encoding Loss -0.1051720604300499, Transition Loss -8.32604694366455, Classifier Loss 0.07632225751876831, Total Loss 7.50732946395874\n",
      "38: Encoding Loss -0.5143119096755981, Transition Loss -18.647489547729492, Classifier Loss 0.06091241538524628, Total Loss 6.0875115394592285\n",
      "38: Encoding Loss -1.3760950565338135, Transition Loss -7.115985870361328, Classifier Loss 0.03244265168905258, Total Loss 3.242841958999634\n",
      "38: Encoding Loss -1.8761601448059082, Transition Loss -10.27759838104248, Classifier Loss 0.05517818406224251, Total Loss 5.515762805938721\n",
      "38: Encoding Loss -1.1948559284210205, Transition Loss -16.881696701049805, Classifier Loss 0.04855688288807869, Total Loss 4.852311611175537\n",
      "38: Encoding Loss -1.5693265199661255, Transition Loss -19.902542114257812, Classifier Loss 0.048975490033626556, Total Loss 4.893568515777588\n",
      "38: Encoding Loss -0.44263771176338196, Transition Loss -13.231088638305664, Classifier Loss 0.06373833119869232, Total Loss 6.371169567108154\n",
      "38: Encoding Loss -1.1544474363327026, Transition Loss -3.7778563499450684, Classifier Loss 0.08221065998077393, Total Loss 8.22031021118164\n",
      "38: Encoding Loss -1.0696731805801392, Transition Loss -13.010320663452148, Classifier Loss 0.04204285517334938, Total Loss 4.201683521270752\n",
      "38: Encoding Loss -0.8591413497924805, Transition Loss -12.73811149597168, Classifier Loss 0.069229856133461, Total Loss 6.920437812805176\n",
      "38: Encoding Loss -1.3996764421463013, Transition Loss -5.935556888580322, Classifier Loss 0.08501100540161133, Total Loss 8.499913215637207\n",
      "38: Encoding Loss -0.6131590008735657, Transition Loss -11.924004554748535, Classifier Loss 0.030978243798017502, Total Loss 3.0954394340515137\n",
      "38: Encoding Loss -1.827134609222412, Transition Loss -13.722280502319336, Classifier Loss 0.053692273795604706, Total Loss 5.366482734680176\n",
      "38: Encoding Loss -1.9398258924484253, Transition Loss -20.09084701538086, Classifier Loss 0.03883284702897072, Total Loss 3.8792667388916016\n",
      "38: Encoding Loss -1.4068714380264282, Transition Loss -16.937061309814453, Classifier Loss 0.06291057914495468, Total Loss 6.287670612335205\n",
      "38: Encoding Loss -1.1211408376693726, Transition Loss -9.944961547851562, Classifier Loss 0.05969957262277603, Total Loss 5.967968463897705\n",
      "38: Encoding Loss -1.7882472276687622, Transition Loss -8.512475967407227, Classifier Loss 0.09690731018781662, Total Loss 9.6890287399292\n",
      "38: Encoding Loss -1.8259254693984985, Transition Loss -10.441291809082031, Classifier Loss 0.050565823912620544, Total Loss 5.054494380950928\n",
      "38: Encoding Loss -0.8844106793403625, Transition Loss -7.820389270782471, Classifier Loss 0.07119310647249222, Total Loss 7.117746829986572\n",
      "38: Encoding Loss -1.021366834640503, Transition Loss -12.243178367614746, Classifier Loss 0.06431960314512253, Total Loss 6.429511547088623\n",
      "38: Encoding Loss -1.7863011360168457, Transition Loss -5.398017883300781, Classifier Loss 0.10064472258090973, Total Loss 10.063392639160156\n",
      "38: Encoding Loss -0.1916143298149109, Transition Loss -1.9936035871505737, Classifier Loss 0.060562603175640106, Total Loss 6.0134406089782715\n",
      "38: Encoding Loss -1.0831815004348755, Transition Loss -18.986717224121094, Classifier Loss 0.08131295442581177, Total Loss 8.127497673034668\n",
      "38: Encoding Loss -0.09193763136863708, Transition Loss -9.155115127563477, Classifier Loss 0.06665907800197601, Total Loss 6.532459259033203\n",
      "38: Encoding Loss -0.3503873944282532, Transition Loss -9.037184715270996, Classifier Loss 0.10684831440448761, Total Loss 10.682381629943848\n",
      "38: Encoding Loss -0.95805823802948, Transition Loss -8.852909088134766, Classifier Loss 0.0734175592660904, Total Loss 7.339985370635986\n",
      "38: Encoding Loss -0.6385247707366943, Transition Loss -8.96998119354248, Classifier Loss 0.07292510569095612, Total Loss 7.290716648101807\n",
      "38: Encoding Loss -0.34252455830574036, Transition Loss -5.702508926391602, Classifier Loss 0.09273194521665573, Total Loss 9.271212577819824\n",
      "38: Encoding Loss -1.1804072856903076, Transition Loss -9.107892990112305, Classifier Loss 0.02288787066936493, Total Loss 2.2869656085968018\n",
      "38: Encoding Loss -1.8029354810714722, Transition Loss -8.190278053283691, Classifier Loss 0.0943845883011818, Total Loss 9.436820030212402\n",
      "38: Encoding Loss -0.8616277575492859, Transition Loss -14.756234169006348, Classifier Loss 0.03516930341720581, Total Loss 3.513979196548462\n",
      "38: Encoding Loss -0.5492879152297974, Transition Loss -6.245854377746582, Classifier Loss 0.07513251900672913, Total Loss 7.512002468109131\n",
      "38: Encoding Loss -2.06233811378479, Transition Loss -12.664424896240234, Classifier Loss 0.058647990226745605, Total Loss 5.8622660636901855\n",
      "38: Encoding Loss -1.12135648727417, Transition Loss -13.670217514038086, Classifier Loss 0.02719598449766636, Total Loss 2.716864585876465\n",
      "38: Encoding Loss -0.9362823367118835, Transition Loss -9.974124908447266, Classifier Loss 0.09945923089981079, Total Loss 9.943927764892578\n",
      "38: Encoding Loss -0.3807002305984497, Transition Loss -11.196404457092285, Classifier Loss 0.053350456058979034, Total Loss 5.332592487335205\n",
      "38: Encoding Loss -0.8169699311256409, Transition Loss -8.657612800598145, Classifier Loss 0.04682675004005432, Total Loss 4.680943489074707\n",
      "38: Encoding Loss -0.5498903393745422, Transition Loss -5.0455451011657715, Classifier Loss 0.09519586712121964, Total Loss 9.518577575683594\n",
      "38: Encoding Loss 0.038914673030376434, Transition Loss -11.4782133102417, Classifier Loss 0.08131076395511627, Total Loss 8.331578254699707\n",
      "38: Encoding Loss -0.7402908205986023, Transition Loss -10.79185962677002, Classifier Loss 0.05238282307982445, Total Loss 5.236124038696289\n",
      "38: Encoding Loss 0.09947772324085236, Transition Loss -8.43879222869873, Classifier Loss 0.06331418454647064, Total Loss 6.9982829093933105\n",
      "38: Encoding Loss 3.972830295562744, Transition Loss -14.549566268920898, Classifier Loss 0.06781134754419327, Total Loss 38.56086730957031\n",
      "38: Encoding Loss -0.4493163526058197, Transition Loss -9.635831832885742, Classifier Loss 0.1064000129699707, Total Loss 10.6380615234375\n",
      "38: Encoding Loss -0.0629955306649208, Transition Loss -8.060474395751953, Classifier Loss 0.052692197263240814, Total Loss 5.134378910064697\n",
      "38: Encoding Loss -0.9969985485076904, Transition Loss -13.203845977783203, Classifier Loss 0.05423043668270111, Total Loss 5.420403003692627\n",
      "38: Encoding Loss -2.5926270484924316, Transition Loss -20.7445125579834, Classifier Loss 0.051398251205682755, Total Loss 5.135676383972168\n",
      "38: Encoding Loss -0.5559424757957458, Transition Loss -7.318843841552734, Classifier Loss 0.08589472621679306, Total Loss 8.588008880615234\n",
      "38: Encoding Loss -1.34299898147583, Transition Loss -7.77451753616333, Classifier Loss 0.07095932960510254, Total Loss 7.0943779945373535\n",
      "38: Encoding Loss -0.23626111447811127, Transition Loss -9.729745864868164, Classifier Loss 0.06733388453722, Total Loss 6.714293003082275\n",
      "38: Encoding Loss -0.8977116942405701, Transition Loss -4.6186652183532715, Classifier Loss 0.07078101485967636, Total Loss 7.077178001403809\n",
      "38: Encoding Loss -1.2195019721984863, Transition Loss -12.866862297058105, Classifier Loss 0.07350138574838638, Total Loss 7.347565174102783\n",
      "38: Encoding Loss -0.7743197083473206, Transition Loss -15.472947120666504, Classifier Loss 0.07481508702039719, Total Loss 7.478414058685303\n",
      "38: Encoding Loss -0.7406362891197205, Transition Loss -12.66302490234375, Classifier Loss 0.05444416403770447, Total Loss 5.4418840408325195\n",
      "38: Encoding Loss -0.29700741171836853, Transition Loss -4.7788801193237305, Classifier Loss 0.03503118455410004, Total Loss 3.4986255168914795\n",
      "38: Encoding Loss -1.0409810543060303, Transition Loss -7.518443584442139, Classifier Loss 0.1066557914018631, Total Loss 10.664074897766113\n",
      "38: Encoding Loss -1.7869704961776733, Transition Loss -14.025718688964844, Classifier Loss 0.0613371841609478, Total Loss 6.130913257598877\n",
      "38: Encoding Loss -0.3647788166999817, Transition Loss -0.6988029479980469, Classifier Loss 0.10908608883619308, Total Loss 10.908082008361816\n",
      "38: Encoding Loss -0.5890087485313416, Transition Loss -8.153326988220215, Classifier Loss 0.07130817323923111, Total Loss 7.129186630249023\n",
      "38: Encoding Loss -0.43616825342178345, Transition Loss -4.956136226654053, Classifier Loss 0.07753504812717438, Total Loss 7.752490997314453\n",
      "38: Encoding Loss -0.6382442116737366, Transition Loss -11.566930770874023, Classifier Loss 0.04980356618762016, Total Loss 4.978043079376221\n",
      "38: Encoding Loss -1.0689858198165894, Transition Loss -14.8743896484375, Classifier Loss 0.07927325367927551, Total Loss 7.924350261688232\n",
      "38: Encoding Loss -0.8472310304641724, Transition Loss -5.030757904052734, Classifier Loss 0.07912081480026245, Total Loss 7.911075592041016\n",
      "38: Encoding Loss -0.9820780158042908, Transition Loss -8.80447006225586, Classifier Loss 0.04435502737760544, Total Loss 4.433741569519043\n",
      "38: Encoding Loss -1.1901503801345825, Transition Loss -8.848885536193848, Classifier Loss 0.05481612682342529, Total Loss 5.4798431396484375\n",
      "38: Encoding Loss -0.7709316611289978, Transition Loss -13.554610252380371, Classifier Loss 0.05665712058544159, Total Loss 5.66300106048584\n",
      "38: Encoding Loss -0.9870348572731018, Transition Loss -13.318452835083008, Classifier Loss 0.07061751186847687, Total Loss 7.059087753295898\n",
      "38: Encoding Loss -1.1209895610809326, Transition Loss -2.388073682785034, Classifier Loss 0.04196270555257797, Total Loss 4.1957926750183105\n",
      "38: Encoding Loss -1.639589786529541, Transition Loss -16.775165557861328, Classifier Loss 0.040362559258937836, Total Loss 4.032900810241699\n",
      "38: Encoding Loss -1.346276044845581, Transition Loss -16.222267150878906, Classifier Loss 0.03716879338026047, Total Loss 3.713634967803955\n",
      "38: Encoding Loss -0.50140380859375, Transition Loss -1.7465710639953613, Classifier Loss 0.05890859663486481, Total Loss 5.890509128570557\n",
      "38: Encoding Loss -0.37433061003685, Transition Loss 0.886788010597229, Classifier Loss 0.05834558978676796, Total Loss 6.0116448402404785\n",
      "38: Encoding Loss -0.48794999718666077, Transition Loss -11.370000839233398, Classifier Loss 0.041651979088783264, Total Loss 4.162921905517578\n",
      "38: Encoding Loss -0.7351961135864258, Transition Loss -10.671272277832031, Classifier Loss 0.04910631477832794, Total Loss 4.908497333526611\n",
      "38: Encoding Loss -1.7095965147018433, Transition Loss -12.416423797607422, Classifier Loss 0.05567409470677376, Total Loss 5.5649261474609375\n",
      "38: Encoding Loss -0.8630247712135315, Transition Loss -6.514981269836426, Classifier Loss 0.06008283421397209, Total Loss 6.0069804191589355\n",
      "38: Encoding Loss -2.0938801765441895, Transition Loss -14.403825759887695, Classifier Loss 0.040842678397893906, Total Loss 4.081387042999268\n",
      "38: Encoding Loss -0.7572214007377625, Transition Loss -8.628087997436523, Classifier Loss 0.07381604611873627, Total Loss 7.379878997802734\n",
      "38: Encoding Loss -1.0694786310195923, Transition Loss -11.9227933883667, Classifier Loss 0.09752720594406128, Total Loss 9.750336647033691\n",
      "38: Encoding Loss -2.074946165084839, Transition Loss -18.523754119873047, Classifier Loss 0.054673708975315094, Total Loss 5.4636664390563965\n",
      "38: Encoding Loss -1.519690752029419, Transition Loss -8.380084991455078, Classifier Loss 0.06996620446443558, Total Loss 6.9949445724487305\n",
      "38: Encoding Loss -1.0787409543991089, Transition Loss -9.729606628417969, Classifier Loss 0.1041133925318718, Total Loss 10.409393310546875\n",
      "38: Encoding Loss -1.3250011205673218, Transition Loss -14.928567886352539, Classifier Loss 0.08298100531101227, Total Loss 8.295114517211914\n",
      "38: Encoding Loss -1.4437181949615479, Transition Loss -11.825348854064941, Classifier Loss 0.026998765766620636, Total Loss 2.6975114345550537\n",
      "38: Encoding Loss -0.2756555378437042, Transition Loss -11.08457088470459, Classifier Loss 0.05912867560982704, Total Loss 5.904210090637207\n",
      "38: Encoding Loss -0.7900697588920593, Transition Loss -5.0362420082092285, Classifier Loss 0.04422257840633392, Total Loss 4.421250820159912\n",
      "38: Encoding Loss -0.06231005862355232, Transition Loss -8.263405799865723, Classifier Loss 0.060732096433639526, Total Loss 5.938657760620117\n",
      "38: Encoding Loss -1.1575648784637451, Transition Loss -9.076231002807617, Classifier Loss 0.05567629262804985, Total Loss 5.565814018249512\n",
      "38: Encoding Loss -0.41099613904953003, Transition Loss -4.894280910491943, Classifier Loss 0.037015460431575775, Total Loss 3.7005019187927246\n",
      "38: Encoding Loss -0.5423958897590637, Transition Loss -9.964353561401367, Classifier Loss 0.02813958190381527, Total Loss 2.811964988708496\n",
      "38: Encoding Loss -1.1467018127441406, Transition Loss -11.957830429077148, Classifier Loss 0.0702928975224495, Total Loss 7.026898384094238\n",
      "38: Encoding Loss -0.1717667430639267, Transition Loss -11.907591819763184, Classifier Loss 0.06023754924535751, Total Loss 5.962384223937988\n",
      "38: Encoding Loss -0.4494788944721222, Transition Loss -1.1672409772872925, Classifier Loss 0.09022092819213867, Total Loss 9.021846771240234\n",
      "38: Encoding Loss -0.3719675838947296, Transition Loss -8.20445728302002, Classifier Loss 0.037511639297008514, Total Loss 3.7492263317108154\n",
      "38: Encoding Loss 0.23233167827129364, Transition Loss -10.482439041137695, Classifier Loss 0.07563624531030655, Total Loss 9.401444435119629\n",
      "38: Encoding Loss 0.23801150918006897, Transition Loss -10.274147987365723, Classifier Loss 0.05575687810778618, Total Loss 7.46124792098999\n",
      "38: Encoding Loss -0.9631151556968689, Transition Loss -9.588921546936035, Classifier Loss 0.05156329274177551, Total Loss 5.154411315917969\n",
      "38: Encoding Loss -1.734907865524292, Transition Loss -5.3294219970703125, Classifier Loss 0.08024957776069641, Total Loss 8.02389144897461\n",
      "38: Encoding Loss -2.0941014289855957, Transition Loss -14.432304382324219, Classifier Loss 0.037786100059747696, Total Loss 3.775723457336426\n",
      "38: Encoding Loss -0.07736162841320038, Transition Loss -1.6090114116668701, Classifier Loss 0.05530603975057602, Total Loss 5.394386291503906\n",
      "38: Encoding Loss -0.5122253894805908, Transition Loss -8.0860595703125, Classifier Loss 0.03311896324157715, Total Loss 3.3102784156799316\n",
      "38: Encoding Loss -0.2029787003993988, Transition Loss -7.397648334503174, Classifier Loss 0.08340422064065933, Total Loss 8.304534912109375\n",
      "38: Encoding Loss 0.09348038583993912, Transition Loss -15.784585952758789, Classifier Loss 0.1498996615409851, Total Loss 15.603821754455566\n",
      "38: Encoding Loss -1.1667366027832031, Transition Loss -16.62445640563965, Classifier Loss 0.08074484765529633, Total Loss 8.071160316467285\n",
      "38: Encoding Loss -1.578233242034912, Transition Loss -22.425317764282227, Classifier Loss 0.046549782156944275, Total Loss 4.650493144989014\n",
      "38: Encoding Loss -0.6461939215660095, Transition Loss -11.951614379882812, Classifier Loss 0.09785155951976776, Total Loss 9.782766342163086\n",
      "38: Encoding Loss -0.7129336595535278, Transition Loss -13.364117622375488, Classifier Loss 0.05702093988656998, Total Loss 5.699421405792236\n",
      "38: Encoding Loss -1.6544729471206665, Transition Loss -19.936296463012695, Classifier Loss 0.05916358530521393, Total Loss 5.9123711585998535\n",
      "38: Encoding Loss -1.3320887088775635, Transition Loss -7.3589396476745605, Classifier Loss 0.08029545843601227, Total Loss 8.028074264526367\n",
      "38: Encoding Loss 0.02197512611746788, Transition Loss -16.104511260986328, Classifier Loss 0.08611510694026947, Total Loss 8.711479187011719\n",
      "38: Encoding Loss -1.5829333066940308, Transition Loss -17.75194549560547, Classifier Loss 0.02435288205742836, Total Loss 2.4317378997802734\n",
      "38: Encoding Loss -1.7279231548309326, Transition Loss -20.854032516479492, Classifier Loss 0.058875106275081635, Total Loss 5.883339881896973\n",
      "38: Encoding Loss -2.6678290367126465, Transition Loss -27.037757873535156, Classifier Loss 0.022798703983426094, Total Loss 2.274462938308716\n",
      "39: Encoding Loss -1.1793687343597412, Transition Loss -6.147693634033203, Classifier Loss 0.034254126250743866, Total Loss 3.4241831302642822\n",
      "39: Encoding Loss -1.5295923948287964, Transition Loss -6.8252458572387695, Classifier Loss 0.10131477564573288, Total Loss 10.130112648010254\n",
      "39: Encoding Loss -1.5159430503845215, Transition Loss -1.67288339138031, Classifier Loss 0.03763343393802643, Total Loss 3.7630088329315186\n",
      "39: Encoding Loss -1.50590181350708, Transition Loss -19.767791748046875, Classifier Loss 0.058865200728178024, Total Loss 5.882566452026367\n",
      "39: Encoding Loss -1.6038497686386108, Transition Loss -10.074600219726562, Classifier Loss 0.04770878329873085, Total Loss 4.768863201141357\n",
      "39: Encoding Loss -1.6942347288131714, Transition Loss -16.57645606994629, Classifier Loss 0.04884316399693489, Total Loss 4.881000995635986\n",
      "39: Encoding Loss -1.8682866096496582, Transition Loss -12.359664916992188, Classifier Loss 0.060534894466400146, Total Loss 6.051017761230469\n",
      "39: Encoding Loss -1.562726616859436, Transition Loss -9.767807960510254, Classifier Loss 0.04919052869081497, Total Loss 4.917099475860596\n",
      "39: Encoding Loss -1.2924047708511353, Transition Loss -16.676183700561523, Classifier Loss 0.03973936289548874, Total Loss 3.9706010818481445\n",
      "39: Encoding Loss -2.2385778427124023, Transition Loss -13.859382629394531, Classifier Loss 0.06407469511032104, Total Loss 6.404697895050049\n",
      "39: Encoding Loss 0.010509181767702103, Transition Loss -9.963911056518555, Classifier Loss 0.052960947155952454, Total Loss 5.339657306671143\n",
      "39: Encoding Loss -0.5746564865112305, Transition Loss -4.904461860656738, Classifier Loss 0.061268389225006104, Total Loss 6.125857830047607\n",
      "39: Encoding Loss -1.7934004068374634, Transition Loss -6.819384574890137, Classifier Loss 0.04773326218128204, Total Loss 4.771962642669678\n",
      "39: Encoding Loss -0.9288679361343384, Transition Loss -7.7538957595825195, Classifier Loss 0.05753053352236748, Total Loss 5.751502513885498\n",
      "39: Encoding Loss -2.0700478553771973, Transition Loss -14.897455215454102, Classifier Loss 0.0394248366355896, Total Loss 3.9395041465759277\n",
      "39: Encoding Loss -2.397702932357788, Transition Loss -19.048583984375, Classifier Loss 0.06534048169851303, Total Loss 6.530238151550293\n",
      "39: Encoding Loss -1.264654278755188, Transition Loss -10.904114723205566, Classifier Loss 0.022356970235705376, Total Loss 2.233516216278076\n",
      "39: Encoding Loss -0.5572745203971863, Transition Loss -7.022595405578613, Classifier Loss 0.04547353461384773, Total Loss 4.5459489822387695\n",
      "39: Encoding Loss -0.44573915004730225, Transition Loss -14.259897232055664, Classifier Loss 0.08117896318435669, Total Loss 8.115029335021973\n",
      "39: Encoding Loss -0.8263962268829346, Transition Loss -6.3504133224487305, Classifier Loss 0.05123567581176758, Total Loss 5.122297286987305\n",
      "39: Encoding Loss -1.2660776376724243, Transition Loss -16.113595962524414, Classifier Loss 0.07578375190496445, Total Loss 7.575152397155762\n",
      "39: Encoding Loss -1.7190849781036377, Transition Loss -12.158954620361328, Classifier Loss 0.03964574635028839, Total Loss 3.9621427059173584\n",
      "39: Encoding Loss -1.7118700742721558, Transition Loss -12.844828605651855, Classifier Loss 0.06707731634378433, Total Loss 6.705162525177002\n",
      "39: Encoding Loss -1.1152411699295044, Transition Loss -4.577926158905029, Classifier Loss 0.07937952131032944, Total Loss 7.937036514282227\n",
      "39: Encoding Loss -2.1632721424102783, Transition Loss -12.713385581970215, Classifier Loss 0.03336825966835022, Total Loss 3.3342831134796143\n",
      "39: Encoding Loss -0.8192203640937805, Transition Loss -6.117177963256836, Classifier Loss 0.0685417577624321, Total Loss 6.852952003479004\n",
      "39: Encoding Loss -2.2137513160705566, Transition Loss -4.5519232749938965, Classifier Loss 0.09260118007659912, Total Loss 9.259207725524902\n",
      "39: Encoding Loss -2.592156410217285, Transition Loss -19.966995239257812, Classifier Loss 0.07002636045217514, Total Loss 6.998642444610596\n",
      "39: Encoding Loss -2.141244411468506, Transition Loss -10.455044746398926, Classifier Loss 0.057317912578582764, Total Loss 5.729700565338135\n",
      "39: Encoding Loss -1.0421311855316162, Transition Loss -6.216143608093262, Classifier Loss 0.06322881579399109, Total Loss 6.321638584136963\n",
      "39: Encoding Loss -1.123213291168213, Transition Loss -12.620157241821289, Classifier Loss 0.0687083899974823, Total Loss 6.86831521987915\n",
      "39: Encoding Loss -1.1654222011566162, Transition Loss -3.9581174850463867, Classifier Loss 0.07918465882539749, Total Loss 7.917674541473389\n",
      "39: Encoding Loss -0.895455539226532, Transition Loss -12.24850845336914, Classifier Loss 0.06416931003332138, Total Loss 6.4144816398620605\n",
      "39: Encoding Loss -0.24524018168449402, Transition Loss -6.728226184844971, Classifier Loss 0.1686401218175888, Total Loss 16.848745346069336\n",
      "39: Encoding Loss -1.9944865703582764, Transition Loss -13.925537109375, Classifier Loss 0.04123986139893532, Total Loss 4.121201038360596\n",
      "39: Encoding Loss -1.0444788932800293, Transition Loss -8.614237785339355, Classifier Loss 0.039192914962768555, Total Loss 3.9175686836242676\n",
      "39: Encoding Loss -1.254151701927185, Transition Loss -7.684171676635742, Classifier Loss 0.06730975210666656, Total Loss 6.729438304901123\n",
      "39: Encoding Loss -1.036780595779419, Transition Loss -10.053504943847656, Classifier Loss 0.11590756475925446, Total Loss 11.588746070861816\n",
      "39: Encoding Loss -1.859085202217102, Transition Loss -11.123692512512207, Classifier Loss 0.0613095797598362, Total Loss 6.128733158111572\n",
      "39: Encoding Loss -1.4841595888137817, Transition Loss -12.56612491607666, Classifier Loss 0.06320769339799881, Total Loss 6.31825590133667\n",
      "39: Encoding Loss -1.3250868320465088, Transition Loss -8.911036491394043, Classifier Loss 0.03341931477189064, Total Loss 3.340149402618408\n",
      "39: Encoding Loss -1.8719303607940674, Transition Loss -17.3704776763916, Classifier Loss 0.074847511947155, Total Loss 7.481276988983154\n",
      "39: Encoding Loss -2.0834403038024902, Transition Loss -16.623546600341797, Classifier Loss 0.02982194349169731, Total Loss 2.978869676589966\n",
      "39: Encoding Loss -2.0731256008148193, Transition Loss -6.765023231506348, Classifier Loss 0.06262341886758804, Total Loss 6.260989189147949\n",
      "39: Encoding Loss -1.8886334896087646, Transition Loss -18.10965919494629, Classifier Loss 0.04405469819903374, Total Loss 4.401847839355469\n",
      "39: Encoding Loss -0.4516596496105194, Transition Loss -8.11062240600586, Classifier Loss 0.07533182203769684, Total Loss 7.531548500061035\n",
      "39: Encoding Loss -0.6538671255111694, Transition Loss -18.694473266601562, Classifier Loss 0.060602519661188126, Total Loss 6.05651330947876\n",
      "39: Encoding Loss -1.5724107027053833, Transition Loss -6.884688854217529, Classifier Loss 0.03223585709929466, Total Loss 3.2222087383270264\n",
      "39: Encoding Loss -1.9340819120407104, Transition Loss -10.153697967529297, Classifier Loss 0.053673937916755676, Total Loss 5.365363121032715\n",
      "39: Encoding Loss -1.4758098125457764, Transition Loss -16.88160514831543, Classifier Loss 0.048119474202394485, Total Loss 4.808570861816406\n",
      "39: Encoding Loss -2.0426554679870605, Transition Loss -20.06774139404297, Classifier Loss 0.04647953435778618, Total Loss 4.643939971923828\n",
      "39: Encoding Loss -0.4925350844860077, Transition Loss -13.19412612915039, Classifier Loss 0.06477748602628708, Total Loss 6.4751081466674805\n",
      "39: Encoding Loss -1.3226512670516968, Transition Loss -3.41092586517334, Classifier Loss 0.08165254443883896, Total Loss 8.164572715759277\n",
      "39: Encoding Loss -1.7306245565414429, Transition Loss -12.747748374938965, Classifier Loss 0.04234599322080612, Total Loss 4.232049465179443\n",
      "39: Encoding Loss -1.0404568910598755, Transition Loss -12.622228622436523, Classifier Loss 0.07154273241758347, Total Loss 7.1517486572265625\n",
      "39: Encoding Loss -1.6017283201217651, Transition Loss -5.6801838874816895, Classifier Loss 0.08269134908914566, Total Loss 8.267998695373535\n",
      "39: Encoding Loss -0.9102089405059814, Transition Loss -11.745798110961914, Classifier Loss 0.03133268654346466, Total Loss 3.1309194564819336\n",
      "39: Encoding Loss -2.100147008895874, Transition Loss -13.515298843383789, Classifier Loss 0.053097955882549286, Total Loss 5.307092189788818\n",
      "39: Encoding Loss -2.2024993896484375, Transition Loss -19.963027954101562, Classifier Loss 0.039762385189533234, Total Loss 3.972245931625366\n",
      "39: Encoding Loss -1.561931848526001, Transition Loss -16.892637252807617, Classifier Loss 0.062388814985752106, Total Loss 6.235503196716309\n",
      "39: Encoding Loss -1.7274316549301147, Transition Loss -9.782809257507324, Classifier Loss 0.05895587056875229, Total Loss 5.893630504608154\n",
      "39: Encoding Loss -1.9792561531066895, Transition Loss -8.40133285522461, Classifier Loss 0.09446924924850464, Total Loss 9.445244789123535\n",
      "39: Encoding Loss -1.7407373189926147, Transition Loss -10.211240768432617, Classifier Loss 0.050808750092983246, Total Loss 5.078832626342773\n",
      "39: Encoding Loss -1.3156458139419556, Transition Loss -7.615575313568115, Classifier Loss 0.06954873353242874, Total Loss 6.95335054397583\n",
      "39: Encoding Loss -1.4743988513946533, Transition Loss -11.961472511291504, Classifier Loss 0.06307460367679596, Total Loss 6.305068016052246\n",
      "39: Encoding Loss -1.9652791023254395, Transition Loss -5.018120765686035, Classifier Loss 0.09936659038066864, Total Loss 9.93565559387207\n",
      "39: Encoding Loss -0.48847824335098267, Transition Loss -1.6357290744781494, Classifier Loss 0.0583648607134819, Total Loss 5.836156845092773\n",
      "39: Encoding Loss -1.6456737518310547, Transition Loss -18.92487335205078, Classifier Loss 0.07467009872198105, Total Loss 7.4632248878479\n",
      "39: Encoding Loss -0.2876083254814148, Transition Loss -9.132290840148926, Classifier Loss 0.06687158346176147, Total Loss 6.680700302124023\n",
      "39: Encoding Loss -0.9427438974380493, Transition Loss -9.096137046813965, Classifier Loss 0.10721522569656372, Total Loss 10.71970272064209\n",
      "39: Encoding Loss -1.599549412727356, Transition Loss -8.850555419921875, Classifier Loss 0.07069126516580582, Total Loss 7.067356586456299\n",
      "39: Encoding Loss -1.407771348953247, Transition Loss -9.053525924682617, Classifier Loss 0.0695551186800003, Total Loss 6.953701496124268\n",
      "39: Encoding Loss -0.8826842904090881, Transition Loss -5.823447227478027, Classifier Loss 0.09121235460042953, Total Loss 9.120071411132812\n",
      "39: Encoding Loss -1.5219906568527222, Transition Loss -9.201208114624023, Classifier Loss 0.022997239604592323, Total Loss 2.2978835105895996\n",
      "39: Encoding Loss -1.9819070100784302, Transition Loss -8.327082633972168, Classifier Loss 0.09495402872562408, Total Loss 9.493738174438477\n",
      "39: Encoding Loss -1.5287702083587646, Transition Loss -14.897557258605957, Classifier Loss 0.035603661090135574, Total Loss 3.557386636734009\n",
      "39: Encoding Loss -1.1839488744735718, Transition Loss -6.55452299118042, Classifier Loss 0.07510089874267578, Total Loss 7.508779048919678\n",
      "39: Encoding Loss -2.5238702297210693, Transition Loss -12.747075080871582, Classifier Loss 0.057044848799705505, Total Loss 5.701935291290283\n",
      "39: Encoding Loss -1.6269558668136597, Transition Loss -13.829550743103027, Classifier Loss 0.02804453857243061, Total Loss 2.8016879558563232\n",
      "39: Encoding Loss -1.4355671405792236, Transition Loss -10.17227840423584, Classifier Loss 0.10087208449840546, Total Loss 10.085174560546875\n",
      "39: Encoding Loss -0.859054446220398, Transition Loss -11.395503997802734, Classifier Loss 0.05369272083044052, Total Loss 5.366992950439453\n",
      "39: Encoding Loss -1.470073938369751, Transition Loss -9.03421401977539, Classifier Loss 0.04726385325193405, Total Loss 4.724578380584717\n",
      "39: Encoding Loss -1.1437126398086548, Transition Loss -5.42851448059082, Classifier Loss 0.09425264596939087, Total Loss 9.424179077148438\n",
      "39: Encoding Loss -0.4221474230289459, Transition Loss -11.605680465698242, Classifier Loss 0.08148069679737091, Total Loss 8.145707130432129\n",
      "39: Encoding Loss -2.313727378845215, Transition Loss -7.940492630004883, Classifier Loss 0.05330982059240341, Total Loss 5.329394340515137\n",
      "39: Encoding Loss -1.452680230140686, Transition Loss -5.934891700744629, Classifier Loss 0.06643486768007278, Total Loss 6.642300128936768\n",
      "39: Encoding Loss -1.901672601699829, Transition Loss -16.10880470275879, Classifier Loss 0.06277588754892349, Total Loss 6.274366855621338\n",
      "39: Encoding Loss -1.4283894300460815, Transition Loss -9.809575080871582, Classifier Loss 0.10433239489793777, Total Loss 10.431278228759766\n",
      "39: Encoding Loss -1.2913347482681274, Transition Loss -8.11628532409668, Classifier Loss 0.04884486645460129, Total Loss 4.882863521575928\n",
      "39: Encoding Loss -0.9374366998672485, Transition Loss -13.99516773223877, Classifier Loss 0.056059468537569046, Total Loss 5.603147983551025\n",
      "39: Encoding Loss -3.0552573204040527, Transition Loss -22.269380569458008, Classifier Loss 0.052422843873500824, Total Loss 5.237830638885498\n",
      "39: Encoding Loss -1.0486767292022705, Transition Loss -7.361038684844971, Classifier Loss 0.0816662460565567, Total Loss 8.165152549743652\n",
      "39: Encoding Loss -1.4114869832992554, Transition Loss -7.868461608886719, Classifier Loss 0.07007042318582535, Total Loss 7.005468845367432\n",
      "39: Encoding Loss -0.9165099263191223, Transition Loss -9.885635375976562, Classifier Loss 0.06566288322210312, Total Loss 6.564311504364014\n",
      "39: Encoding Loss -1.2943156957626343, Transition Loss -4.154046058654785, Classifier Loss 0.0702626183629036, Total Loss 7.025431156158447\n",
      "39: Encoding Loss -1.609173059463501, Transition Loss -13.454936981201172, Classifier Loss 0.07186179608106613, Total Loss 7.183488845825195\n",
      "39: Encoding Loss -1.5499247312545776, Transition Loss -16.535539627075195, Classifier Loss 0.07455501705408096, Total Loss 7.4521942138671875\n",
      "39: Encoding Loss -1.915588617324829, Transition Loss -13.239777565002441, Classifier Loss 0.04893648251891136, Total Loss 4.891000270843506\n",
      "39: Encoding Loss -1.0862257480621338, Transition Loss -4.458495616912842, Classifier Loss 0.03533761203289032, Total Loss 3.532869577407837\n",
      "39: Encoding Loss -1.6160318851470947, Transition Loss -7.414931297302246, Classifier Loss 0.09972557425498962, Total Loss 9.971074104309082\n",
      "39: Encoding Loss -2.005884885787964, Transition Loss -14.708191871643066, Classifier Loss 0.0619046576321125, Total Loss 6.187524318695068\n",
      "39: Encoding Loss -0.7469120621681213, Transition Loss 0.20808732509613037, Classifier Loss 0.102959044277668, Total Loss 10.33752155303955\n",
      "39: Encoding Loss -1.0700528621673584, Transition Loss -8.710549354553223, Classifier Loss 0.06835533678531647, Total Loss 6.833791732788086\n",
      "39: Encoding Loss -0.5216330289840698, Transition Loss -5.13856840133667, Classifier Loss 0.07811720669269562, Total Loss 7.81069278717041\n",
      "39: Encoding Loss -1.2251149415969849, Transition Loss -12.386458396911621, Classifier Loss 0.048662539571523666, Total Loss 4.863776683807373\n",
      "39: Encoding Loss -1.9616656303405762, Transition Loss -16.167890548706055, Classifier Loss 0.077975794672966, Total Loss 7.794345855712891\n",
      "39: Encoding Loss -1.262338399887085, Transition Loss -5.200222969055176, Classifier Loss 0.07497134804725647, Total Loss 7.496094703674316\n",
      "39: Encoding Loss -1.2744587659835815, Transition Loss -9.473257064819336, Classifier Loss 0.041352417320013046, Total Loss 4.133347034454346\n",
      "39: Encoding Loss -1.6796778440475464, Transition Loss -9.380937576293945, Classifier Loss 0.053494423627853394, Total Loss 5.3475661277771\n",
      "39: Encoding Loss -0.9555192589759827, Transition Loss -14.655522346496582, Classifier Loss 0.05885987728834152, Total Loss 5.883056640625\n",
      "39: Encoding Loss -1.2855494022369385, Transition Loss -14.354646682739258, Classifier Loss 0.06781171262264252, Total Loss 6.7783002853393555\n",
      "39: Encoding Loss -1.0778084993362427, Transition Loss -1.9853200912475586, Classifier Loss 0.041439905762672424, Total Loss 4.1435933113098145\n",
      "39: Encoding Loss -1.3326443433761597, Transition Loss -18.177398681640625, Classifier Loss 0.039507776498794556, Total Loss 3.9471421241760254\n",
      "39: Encoding Loss -1.8134024143218994, Transition Loss -17.468318939208984, Classifier Loss 0.03557790443301201, Total Loss 3.5542969703674316\n",
      "39: Encoding Loss -0.7788220047950745, Transition Loss -1.2859795093536377, Classifier Loss 0.058376044034957886, Total Loss 5.837347507476807\n",
      "39: Encoding Loss -0.5135089755058289, Transition Loss 1.7494724988937378, Classifier Loss 0.058499228209257126, Total Loss 6.199816703796387\n",
      "39: Encoding Loss -0.7312654256820679, Transition Loss -12.178960800170898, Classifier Loss 0.04090070724487305, Total Loss 4.087635040283203\n",
      "39: Encoding Loss -1.2393079996109009, Transition Loss -11.203712463378906, Classifier Loss 0.04866400361061096, Total Loss 4.86415958404541\n",
      "39: Encoding Loss -2.203347682952881, Transition Loss -13.293415069580078, Classifier Loss 0.05610417202115059, Total Loss 5.607758522033691\n",
      "39: Encoding Loss -1.099733591079712, Transition Loss -6.687952995300293, Classifier Loss 0.05995187908411026, Total Loss 5.993850231170654\n",
      "39: Encoding Loss -2.4262566566467285, Transition Loss -15.472112655639648, Classifier Loss 0.04107779264450073, Total Loss 4.104685306549072\n",
      "39: Encoding Loss -0.8964775204658508, Transition Loss -9.081543922424316, Classifier Loss 0.07210603356361389, Total Loss 7.208786964416504\n",
      "39: Encoding Loss -1.2212066650390625, Transition Loss -12.801129341125488, Classifier Loss 0.0950077697634697, Total Loss 9.49821662902832\n",
      "39: Encoding Loss -2.443953037261963, Transition Loss -20.170896530151367, Classifier Loss 0.0530080646276474, Total Loss 5.296772480010986\n",
      "39: Encoding Loss -1.8502869606018066, Transition Loss -8.767577171325684, Classifier Loss 0.06783957779407501, Total Loss 6.782204627990723\n",
      "39: Encoding Loss -1.7967816591262817, Transition Loss -10.266216278076172, Classifier Loss 0.10221019387245178, Total Loss 10.218966484069824\n",
      "39: Encoding Loss -1.5671442747116089, Transition Loss -16.16330909729004, Classifier Loss 0.08309268951416016, Total Loss 8.306035995483398\n",
      "39: Encoding Loss -1.9069530963897705, Transition Loss -12.815994262695312, Classifier Loss 0.02654951810836792, Total Loss 2.652388572692871\n",
      "39: Encoding Loss -0.7867048382759094, Transition Loss -11.817237854003906, Classifier Loss 0.06284618377685547, Total Loss 6.282254695892334\n",
      "39: Encoding Loss -1.7144039869308472, Transition Loss -5.233681678771973, Classifier Loss 0.04390481486916542, Total Loss 4.389434814453125\n",
      "39: Encoding Loss -0.857395589351654, Transition Loss -8.803489685058594, Classifier Loss 0.05583140254020691, Total Loss 5.5813798904418945\n",
      "39: Encoding Loss -1.4938163757324219, Transition Loss -9.574067115783691, Classifier Loss 0.0557284951210022, Total Loss 5.570934295654297\n",
      "39: Encoding Loss -1.0356004238128662, Transition Loss -4.890564441680908, Classifier Loss 0.036389291286468506, Total Loss 3.637950897216797\n",
      "39: Encoding Loss -1.1021747589111328, Transition Loss -10.48387622833252, Classifier Loss 0.028591353446245193, Total Loss 2.8570384979248047\n",
      "39: Encoding Loss -1.1770273447036743, Transition Loss -12.708518981933594, Classifier Loss 0.07167695462703705, Total Loss 7.165153980255127\n",
      "39: Encoding Loss -0.28030237555503845, Transition Loss -12.758406639099121, Classifier Loss 0.061318375170230865, Total Loss 6.12360954284668\n",
      "39: Encoding Loss -1.2059357166290283, Transition Loss -0.5646536350250244, Classifier Loss 0.09113092720508575, Total Loss 9.112979888916016\n",
      "39: Encoding Loss -1.122706651687622, Transition Loss -8.73952579498291, Classifier Loss 0.03773629665374756, Total Loss 3.7718818187713623\n",
      "39: Encoding Loss -0.6864351034164429, Transition Loss -11.30398178100586, Classifier Loss 0.07527430355548859, Total Loss 7.525169849395752\n",
      "39: Encoding Loss -0.36292794346809387, Transition Loss -10.08082103729248, Classifier Loss 0.053912531584501266, Total Loss 5.388824462890625\n",
      "39: Encoding Loss -0.7903296947479248, Transition Loss -9.465670585632324, Classifier Loss 0.05158038064837456, Total Loss 5.156145095825195\n",
      "39: Encoding Loss -1.972427248954773, Transition Loss -5.131809711456299, Classifier Loss 0.08019401133060455, Total Loss 8.018375396728516\n",
      "39: Encoding Loss -2.165156126022339, Transition Loss -14.579753875732422, Classifier Loss 0.03796323388814926, Total Loss 3.793407440185547\n",
      "39: Encoding Loss -0.11157839745283127, Transition Loss -1.4065852165222168, Classifier Loss 0.053749118000268936, Total Loss 5.2565741539001465\n",
      "39: Encoding Loss -0.6941646933555603, Transition Loss -7.871302604675293, Classifier Loss 0.03322141245007515, Total Loss 3.3205668926239014\n",
      "39: Encoding Loss -0.1603444665670395, Transition Loss -7.153665542602539, Classifier Loss 0.07993394881486893, Total Loss 7.922159194946289\n",
      "39: Encoding Loss -0.1882125735282898, Transition Loss -15.797540664672852, Classifier Loss 0.15030263364315033, Total Loss 14.98206901550293\n",
      "39: Encoding Loss -1.895778775215149, Transition Loss -16.240253448486328, Classifier Loss 0.07224162667989731, Total Loss 7.220914363861084\n",
      "39: Encoding Loss -1.9080486297607422, Transition Loss -21.853487014770508, Classifier Loss 0.038988836109638214, Total Loss 3.894512891769409\n",
      "39: Encoding Loss -1.0138014554977417, Transition Loss -11.43874740600586, Classifier Loss 0.08943018317222595, Total Loss 8.940730094909668\n",
      "39: Encoding Loss -1.5333505868911743, Transition Loss -12.897411346435547, Classifier Loss 0.05236661434173584, Total Loss 5.234081745147705\n",
      "39: Encoding Loss -1.895021915435791, Transition Loss -19.42460823059082, Classifier Loss 0.053784362971782684, Total Loss 5.374551296234131\n",
      "39: Encoding Loss -1.3878034353256226, Transition Loss -6.920094013214111, Classifier Loss 0.08230289816856384, Total Loss 8.22890567779541\n",
      "39: Encoding Loss -0.7226384282112122, Transition Loss -15.695899963378906, Classifier Loss 0.08314532041549683, Total Loss 8.311392784118652\n",
      "39: Encoding Loss -1.1219538450241089, Transition Loss -16.78833770751953, Classifier Loss 0.020915426313877106, Total Loss 2.0881850719451904\n",
      "39: Encoding Loss -1.327763319015503, Transition Loss -19.798063278198242, Classifier Loss 0.055073607712984085, Total Loss 5.503401279449463\n",
      "39: Encoding Loss -2.337157964706421, Transition Loss -25.843486785888672, Classifier Loss 0.019627898931503296, Total Loss 1.9576212167739868\n",
      "40: Encoding Loss -0.7013571858406067, Transition Loss -4.847220420837402, Classifier Loss 0.034162990748882294, Total Loss 3.415329694747925\n",
      "40: Encoding Loss -1.5695816278457642, Transition Loss -5.992456912994385, Classifier Loss 0.1012517511844635, Total Loss 10.123976707458496\n",
      "40: Encoding Loss -1.4138877391815186, Transition Loss -0.5388978123664856, Classifier Loss 0.03719310462474823, Total Loss 3.719202756881714\n",
      "40: Encoding Loss -1.1409882307052612, Transition Loss -18.88309669494629, Classifier Loss 0.05485645681619644, Total Loss 5.481869220733643\n",
      "40: Encoding Loss -0.9634857177734375, Transition Loss -9.38970947265625, Classifier Loss 0.04651559889316559, Total Loss 4.64968204498291\n",
      "40: Encoding Loss -1.1286789178848267, Transition Loss -15.262520790100098, Classifier Loss 0.04717235267162323, Total Loss 4.714182376861572\n",
      "40: Encoding Loss -1.5074387788772583, Transition Loss -11.074853897094727, Classifier Loss 0.05770706385374069, Total Loss 5.768491268157959\n",
      "40: Encoding Loss -0.7063199281692505, Transition Loss -8.305639266967773, Classifier Loss 0.04916839301586151, Total Loss 4.915177822113037\n",
      "40: Encoding Loss -0.669430136680603, Transition Loss -15.595273971557617, Classifier Loss 0.03862382844090462, Total Loss 3.8592638969421387\n",
      "40: Encoding Loss -1.7513657808303833, Transition Loss -12.783113479614258, Classifier Loss 0.062475357204675674, Total Loss 6.244978904724121\n",
      "40: Encoding Loss -0.24494893848896027, Transition Loss -8.926986694335938, Classifier Loss 0.051019418984651566, Total Loss 5.086140155792236\n",
      "40: Encoding Loss -0.15244077146053314, Transition Loss -4.83730936050415, Classifier Loss 0.06204191967844963, Total Loss 6.1255364418029785\n",
      "40: Encoding Loss -0.43564900755882263, Transition Loss -6.3023881912231445, Classifier Loss 0.04560910910367966, Total Loss 4.559627532958984\n",
      "40: Encoding Loss -0.0988369882106781, Transition Loss -7.3569231033325195, Classifier Loss 0.05462582781910896, Total Loss 5.333425045013428\n",
      "40: Encoding Loss -0.07086529582738876, Transition Loss -13.735042572021484, Classifier Loss 0.035788316279649734, Total Loss 3.440437078475952\n",
      "40: Encoding Loss -1.2268842458724976, Transition Loss -17.896852493286133, Classifier Loss 0.06684071570634842, Total Loss 6.680492401123047\n",
      "40: Encoding Loss -0.5422067642211914, Transition Loss -10.315563201904297, Classifier Loss 0.02144172415137291, Total Loss 2.1421091556549072\n",
      "40: Encoding Loss -0.10337316244840622, Transition Loss -6.713862895965576, Classifier Loss 0.04341062903404236, Total Loss 4.215150833129883\n",
      "40: Encoding Loss 0.36014503240585327, Transition Loss -13.137840270996094, Classifier Loss 0.08103938400745392, Total Loss 10.982015609741211\n",
      "40: Encoding Loss -0.47905293107032776, Transition Loss -8.519174575805664, Classifier Loss 0.054479826241731644, Total Loss 5.44627571105957\n",
      "40: Encoding Loss -0.17333711683750153, Transition Loss -17.070030212402344, Classifier Loss 0.0705464705824852, Total Loss 6.993664264678955\n",
      "40: Encoding Loss -0.5435423254966736, Transition Loss -13.582046508789062, Classifier Loss 0.040054455399513245, Total Loss 4.0027289390563965\n",
      "40: Encoding Loss 0.32933470606803894, Transition Loss -14.500307083129883, Classifier Loss 0.06169108301401138, Total Loss 8.799581527709961\n",
      "40: Encoding Loss 1.1404919624328613, Transition Loss -3.274477958679199, Classifier Loss 0.07527021318674088, Total Loss 16.65030288696289\n",
      "40: Encoding Loss -1.3545979261398315, Transition Loss -13.541661262512207, Classifier Loss 0.03144656494259834, Total Loss 3.1419479846954346\n",
      "40: Encoding Loss -0.2811196744441986, Transition Loss -7.175605773925781, Classifier Loss 0.06419264525175095, Total Loss 6.41227912902832\n",
      "40: Encoding Loss -1.3316371440887451, Transition Loss -5.908469200134277, Classifier Loss 0.09001520276069641, Total Loss 9.000338554382324\n",
      "40: Encoding Loss -2.1387853622436523, Transition Loss -20.28580093383789, Classifier Loss 0.069949209690094, Total Loss 6.990863800048828\n",
      "40: Encoding Loss -0.9345611333847046, Transition Loss -11.597895622253418, Classifier Loss 0.055700045078992844, Total Loss 5.567684650421143\n",
      "40: Encoding Loss -0.6145967841148376, Transition Loss -7.422379016876221, Classifier Loss 0.0644579827785492, Total Loss 6.444314002990723\n",
      "40: Encoding Loss -0.798637330532074, Transition Loss -13.183029174804688, Classifier Loss 0.06759464740753174, Total Loss 6.756828308105469\n",
      "40: Encoding Loss -0.15449757874011993, Transition Loss -5.447308540344238, Classifier Loss 0.0783480703830719, Total Loss 7.7581048011779785\n",
      "40: Encoding Loss 0.15984505414962769, Transition Loss -12.761658668518066, Classifier Loss 0.06188536435365677, Total Loss 7.394449710845947\n",
      "40: Encoding Loss -0.06699037551879883, Transition Loss -6.980842590332031, Classifier Loss 0.16805489361286163, Total Loss 16.669328689575195\n",
      "40: Encoding Loss -1.566620945930481, Transition Loss -14.053476333618164, Classifier Loss 0.04263690486550331, Total Loss 4.260879993438721\n",
      "40: Encoding Loss -0.7143459320068359, Transition Loss -8.87440013885498, Classifier Loss 0.043482713401317596, Total Loss 4.34649658203125\n",
      "40: Encoding Loss -0.4276871979236603, Transition Loss -8.114156723022461, Classifier Loss 0.07176075875759125, Total Loss 7.1744208335876465\n",
      "40: Encoding Loss -0.8872312903404236, Transition Loss -10.392298698425293, Classifier Loss 0.1128920465707779, Total Loss 11.287126541137695\n",
      "40: Encoding Loss -1.4402893781661987, Transition Loss -11.335643768310547, Classifier Loss 0.06402863562107086, Total Loss 6.4005961418151855\n",
      "40: Encoding Loss -0.7848790884017944, Transition Loss -12.632683753967285, Classifier Loss 0.06119576096534729, Total Loss 6.117049217224121\n",
      "40: Encoding Loss -0.8032556176185608, Transition Loss -9.145048141479492, Classifier Loss 0.03295071795582771, Total Loss 3.293242931365967\n",
      "40: Encoding Loss -1.1439214944839478, Transition Loss -17.536174774169922, Classifier Loss 0.07344789057970047, Total Loss 7.341281890869141\n",
      "40: Encoding Loss -1.5085371732711792, Transition Loss -16.681331634521484, Classifier Loss 0.031704533845186234, Total Loss 3.167117118835449\n",
      "40: Encoding Loss -1.779752492904663, Transition Loss -6.672096252441406, Classifier Loss 0.05645594745874405, Total Loss 5.644260406494141\n",
      "40: Encoding Loss -1.334862470626831, Transition Loss -18.34440040588379, Classifier Loss 0.043366871774196625, Total Loss 4.3330183029174805\n",
      "40: Encoding Loss 0.16865882277488708, Transition Loss -8.176300048828125, Classifier Loss 0.07449513673782349, Total Loss 8.735296249389648\n",
      "40: Encoding Loss 1.021970272064209, Transition Loss -18.38782501220703, Classifier Loss 0.06095973029732704, Total Loss 14.268057823181152\n",
      "40: Encoding Loss -0.912981390953064, Transition Loss -6.924122333526611, Classifier Loss 0.03479006513953209, Total Loss 3.4776217937469482\n",
      "40: Encoding Loss -0.07630590349435806, Transition Loss -9.840378761291504, Classifier Loss 0.05718119442462921, Total Loss 5.580196380615234\n",
      "40: Encoding Loss 0.09664347022771835, Transition Loss -16.309396743774414, Classifier Loss 0.05100560188293457, Total Loss 5.741397380828857\n",
      "40: Encoding Loss -1.6819186210632324, Transition Loss -21.1280574798584, Classifier Loss 0.04999615252017975, Total Loss 4.995389461517334\n",
      "40: Encoding Loss -0.5167041420936584, Transition Loss -14.140392303466797, Classifier Loss 0.06561503559350967, Total Loss 6.5586748123168945\n",
      "40: Encoding Loss -1.4040100574493408, Transition Loss -4.985691070556641, Classifier Loss 0.08388619869947433, Total Loss 8.387621879577637\n",
      "40: Encoding Loss -1.5289576053619385, Transition Loss -13.708136558532715, Classifier Loss 0.043155185878276825, Total Loss 4.312776565551758\n",
      "40: Encoding Loss -0.8555699586868286, Transition Loss -13.530084609985352, Classifier Loss 0.0690608099102974, Total Loss 6.903375148773193\n",
      "40: Encoding Loss -1.2891286611557007, Transition Loss -6.794651508331299, Classifier Loss 0.08209475874900818, Total Loss 8.20811653137207\n",
      "40: Encoding Loss -0.20890527963638306, Transition Loss -12.59414291381836, Classifier Loss 0.030725983902812004, Total Loss 3.039409875869751\n",
      "40: Encoding Loss -1.5219225883483887, Transition Loss -14.607980728149414, Classifier Loss 0.05424882099032402, Total Loss 5.421960353851318\n",
      "40: Encoding Loss -2.1167449951171875, Transition Loss -20.932260513305664, Classifier Loss 0.03837485611438751, Total Loss 3.833299160003662\n",
      "40: Encoding Loss -1.160844326019287, Transition Loss -18.01774024963379, Classifier Loss 0.06250863522291183, Total Loss 6.247260093688965\n",
      "40: Encoding Loss -1.766432523727417, Transition Loss -10.97990894317627, Classifier Loss 0.058708854019641876, Total Loss 5.86868953704834\n",
      "40: Encoding Loss -1.9966223239898682, Transition Loss -9.683090209960938, Classifier Loss 0.09663724154233932, Total Loss 9.661787033081055\n",
      "40: Encoding Loss -1.5330389738082886, Transition Loss -11.53864574432373, Classifier Loss 0.05242208018898964, Total Loss 5.2399001121521\n",
      "40: Encoding Loss -0.7131374478340149, Transition Loss -8.809449195861816, Classifier Loss 0.06810614466667175, Total Loss 6.808852672576904\n",
      "40: Encoding Loss -1.2682234048843384, Transition Loss -13.289885520935059, Classifier Loss 0.06135338172316551, Total Loss 6.132680416107178\n",
      "40: Encoding Loss -1.6785863637924194, Transition Loss -6.34397029876709, Classifier Loss 0.10081899911165237, Total Loss 10.080631256103516\n",
      "40: Encoding Loss -0.3999707102775574, Transition Loss -3.113140821456909, Classifier Loss 0.056375063955783844, Total Loss 5.636782169342041\n",
      "40: Encoding Loss -2.245382785797119, Transition Loss -19.750995635986328, Classifier Loss 0.07510453462600708, Total Loss 7.506503105163574\n",
      "40: Encoding Loss -0.08509165793657303, Transition Loss -10.278230667114258, Classifier Loss 0.06606437265872955, Total Loss 6.469999313354492\n",
      "40: Encoding Loss -0.7630617022514343, Transition Loss -10.438828468322754, Classifier Loss 0.11025364696979523, Total Loss 11.023277282714844\n",
      "40: Encoding Loss -1.6919728517532349, Transition Loss -10.17484188079834, Classifier Loss 0.07103046774864197, Total Loss 7.101011753082275\n",
      "40: Encoding Loss -1.0860674381256104, Transition Loss -10.282926559448242, Classifier Loss 0.07042321562767029, Total Loss 7.040265083312988\n",
      "40: Encoding Loss -1.0759799480438232, Transition Loss -6.980240345001221, Classifier Loss 0.09054616093635559, Total Loss 9.05321979522705\n",
      "40: Encoding Loss -0.9256433844566345, Transition Loss -10.45103931427002, Classifier Loss 0.023284003138542175, Total Loss 2.326310157775879\n",
      "40: Encoding Loss -1.427585244178772, Transition Loss -9.34626579284668, Classifier Loss 0.09399570524692535, Total Loss 9.397701263427734\n",
      "40: Encoding Loss -1.140657663345337, Transition Loss -15.823653221130371, Classifier Loss 0.0355665497481823, Total Loss 3.553490161895752\n",
      "40: Encoding Loss -0.9833300709724426, Transition Loss -8.109962463378906, Classifier Loss 0.07394105195999146, Total Loss 7.392482757568359\n",
      "40: Encoding Loss -2.2280988693237305, Transition Loss -13.942585945129395, Classifier Loss 0.05619550123810768, Total Loss 5.616761684417725\n",
      "40: Encoding Loss -1.4047298431396484, Transition Loss -15.104961395263672, Classifier Loss 0.028232641518115997, Total Loss 2.8202431201934814\n",
      "40: Encoding Loss -1.3996444940567017, Transition Loss -11.391426086425781, Classifier Loss 0.09808818995952606, Total Loss 9.806540489196777\n",
      "40: Encoding Loss -0.6258583664894104, Transition Loss -12.471097946166992, Classifier Loss 0.052302971482276917, Total Loss 5.227802753448486\n",
      "40: Encoding Loss -1.7319971323013306, Transition Loss -10.239450454711914, Classifier Loss 0.046206921339035034, Total Loss 4.6186442375183105\n",
      "40: Encoding Loss -1.0848926305770874, Transition Loss -6.770328521728516, Classifier Loss 0.09186404943466187, Total Loss 9.185050964355469\n",
      "40: Encoding Loss -0.5189586877822876, Transition Loss -12.31214714050293, Classifier Loss 0.0813894122838974, Total Loss 8.13647747039795\n",
      "40: Encoding Loss -1.694871425628662, Transition Loss -9.447759628295898, Classifier Loss 0.0529034361243248, Total Loss 5.288454055786133\n",
      "40: Encoding Loss -0.7621229290962219, Transition Loss -7.308294296264648, Classifier Loss 0.06514737010002136, Total Loss 6.513275623321533\n",
      "40: Encoding Loss -1.6127244234085083, Transition Loss -17.22368621826172, Classifier Loss 0.06449998170137405, Total Loss 6.446553707122803\n",
      "40: Encoding Loss -0.9292871356010437, Transition Loss -11.238799095153809, Classifier Loss 0.10200349241495132, Total Loss 10.198101043701172\n",
      "40: Encoding Loss -0.2569306492805481, Transition Loss -9.266246795654297, Classifier Loss 0.04929462820291519, Total Loss 4.9171366691589355\n",
      "40: Encoding Loss -1.185302972793579, Transition Loss -14.829130172729492, Classifier Loss 0.054112307727336884, Total Loss 5.408264636993408\n",
      "40: Encoding Loss -3.0389130115509033, Transition Loss -23.457862854003906, Classifier Loss 0.05055275559425354, Total Loss 5.050583839416504\n",
      "40: Encoding Loss -0.8456323742866516, Transition Loss -8.515780448913574, Classifier Loss 0.08407172560691833, Total Loss 8.405468940734863\n",
      "40: Encoding Loss -1.659892201423645, Transition Loss -9.544322967529297, Classifier Loss 0.07397578656673431, Total Loss 7.395669937133789\n",
      "40: Encoding Loss -0.2584553062915802, Transition Loss -11.429569244384766, Classifier Loss 0.06453585624694824, Total Loss 6.441219329833984\n",
      "40: Encoding Loss -1.0829070806503296, Transition Loss -5.72443962097168, Classifier Loss 0.07039938122034073, Total Loss 7.038793087005615\n",
      "40: Encoding Loss -1.4297237396240234, Transition Loss -14.635217666625977, Classifier Loss 0.07115763425827026, Total Loss 7.112836837768555\n",
      "40: Encoding Loss -0.8516525030136108, Transition Loss -17.737598419189453, Classifier Loss 0.07180028408765793, Total Loss 7.176480770111084\n",
      "40: Encoding Loss -0.9978488087654114, Transition Loss -14.893587112426758, Classifier Loss 0.051172468811273575, Total Loss 5.1142683029174805\n",
      "40: Encoding Loss -0.4845379889011383, Transition Loss -6.104555130004883, Classifier Loss 0.033497486263513565, Total Loss 3.348525285720825\n",
      "40: Encoding Loss -1.408478021621704, Transition Loss -8.918893814086914, Classifier Loss 0.10166867077350616, Total Loss 10.165083885192871\n",
      "40: Encoding Loss -2.199050188064575, Transition Loss -15.639129638671875, Classifier Loss 0.05853112414479256, Total Loss 5.849984169006348\n",
      "40: Encoding Loss -0.6821776628494263, Transition Loss -1.2138395309448242, Classifier Loss 0.10371941328048706, Total Loss 10.371698379516602\n",
      "40: Encoding Loss -0.9098350405693054, Transition Loss -9.536335945129395, Classifier Loss 0.0688105896115303, Total Loss 6.879151821136475\n",
      "40: Encoding Loss -0.7877416014671326, Transition Loss -5.799222469329834, Classifier Loss 0.07637457549571991, Total Loss 7.636297702789307\n",
      "40: Encoding Loss -1.0603886842727661, Transition Loss -12.928109169006348, Classifier Loss 0.04813636094331741, Total Loss 4.811050891876221\n",
      "40: Encoding Loss -1.3878532648086548, Transition Loss -16.87367057800293, Classifier Loss 0.07844647765159607, Total Loss 7.841273307800293\n",
      "40: Encoding Loss -1.2011092901229858, Transition Loss -6.016456604003906, Classifier Loss 0.07583394646644592, Total Loss 7.582191467285156\n",
      "40: Encoding Loss -1.3208465576171875, Transition Loss -10.474836349487305, Classifier Loss 0.04264890030026436, Total Loss 4.262795448303223\n",
      "40: Encoding Loss -1.5918660163879395, Transition Loss -10.236955642700195, Classifier Loss 0.05181333050131798, Total Loss 5.179285526275635\n",
      "40: Encoding Loss -0.9940204620361328, Transition Loss -15.515804290771484, Classifier Loss 0.05624663084745407, Total Loss 5.6215596199035645\n",
      "40: Encoding Loss -1.3928186893463135, Transition Loss -15.106158256530762, Classifier Loss 0.06857145577669144, Total Loss 6.854124546051025\n",
      "40: Encoding Loss -1.540176510810852, Transition Loss -2.823195457458496, Classifier Loss 0.03968216851353645, Total Loss 3.9676523208618164\n",
      "40: Encoding Loss -2.1219661235809326, Transition Loss -18.992084503173828, Classifier Loss 0.03966551274061203, Total Loss 3.9627528190612793\n",
      "40: Encoding Loss -1.6418238878250122, Transition Loss -18.09772300720215, Classifier Loss 0.03477806597948074, Total Loss 3.474187135696411\n",
      "40: Encoding Loss -0.6194995641708374, Transition Loss -2.3759028911590576, Classifier Loss 0.05731210112571716, Total Loss 5.730734825134277\n",
      "40: Encoding Loss -0.61826092004776, Transition Loss 0.8602554798126221, Classifier Loss 0.05833995342254639, Total Loss 6.006046295166016\n",
      "40: Encoding Loss -0.7127846479415894, Transition Loss -13.137360572814941, Classifier Loss 0.039681483060121536, Total Loss 3.9655208587646484\n",
      "40: Encoding Loss -0.9867782592773438, Transition Loss -12.193735122680664, Classifier Loss 0.047456126660108566, Total Loss 4.743174076080322\n",
      "40: Encoding Loss -2.168104887008667, Transition Loss -14.261953353881836, Classifier Loss 0.05494341626763344, Total Loss 5.491489410400391\n",
      "40: Encoding Loss -1.2785855531692505, Transition Loss -7.825193405151367, Classifier Loss 0.05957242101430893, Total Loss 5.955677032470703\n",
      "40: Encoding Loss -2.4917144775390625, Transition Loss -16.37458038330078, Classifier Loss 0.040592990815639496, Total Loss 4.056024074554443\n",
      "40: Encoding Loss -1.131300687789917, Transition Loss -10.291906356811523, Classifier Loss 0.06973706930875778, Total Loss 6.971648216247559\n",
      "40: Encoding Loss -1.4327870607376099, Transition Loss -13.40476131439209, Classifier Loss 0.09702178090810776, Total Loss 9.69949722290039\n",
      "40: Encoding Loss -2.485957145690918, Transition Loss -21.089405059814453, Classifier Loss 0.05395293980836868, Total Loss 5.39107608795166\n",
      "40: Encoding Loss -1.8752659559249878, Transition Loss -9.583263397216797, Classifier Loss 0.06772694736719131, Total Loss 6.770777702331543\n",
      "40: Encoding Loss -1.3929312229156494, Transition Loss -11.139398574829102, Classifier Loss 0.10175485908985138, Total Loss 10.173257827758789\n",
      "40: Encoding Loss -1.657165765762329, Transition Loss -17.2143497467041, Classifier Loss 0.07818317413330078, Total Loss 7.814874649047852\n",
      "40: Encoding Loss -1.643317461013794, Transition Loss -13.809704780578613, Classifier Loss 0.026635386049747467, Total Loss 2.6607768535614014\n",
      "40: Encoding Loss -0.6534057259559631, Transition Loss -12.569686889648438, Classifier Loss 0.05814788490533829, Total Loss 5.81227445602417\n",
      "40: Encoding Loss -1.0448167324066162, Transition Loss -6.0826897621154785, Classifier Loss 0.044952522963285446, Total Loss 4.494035720825195\n",
      "40: Encoding Loss -0.2443946748971939, Transition Loss -9.505858421325684, Classifier Loss 0.06292108446359634, Total Loss 6.276005744934082\n",
      "40: Encoding Loss -1.345770239830017, Transition Loss -10.398082733154297, Classifier Loss 0.05471450462937355, Total Loss 5.4693708419799805\n",
      "40: Encoding Loss -0.42304089665412903, Transition Loss -5.82849645614624, Classifier Loss 0.03520645946264267, Total Loss 3.5194408893585205\n",
      "40: Encoding Loss -0.5754792094230652, Transition Loss -11.360435485839844, Classifier Loss 0.02777859941124916, Total Loss 2.775587797164917\n",
      "40: Encoding Loss -1.2961400747299194, Transition Loss -13.671667098999023, Classifier Loss 0.07011431455612183, Total Loss 7.008697509765625\n",
      "40: Encoding Loss -0.187239870429039, Transition Loss -13.450766563415527, Classifier Loss 0.058567725121974945, Total Loss 5.808282375335693\n",
      "40: Encoding Loss -0.33256903290748596, Transition Loss -1.5135633945465088, Classifier Loss 0.08908651769161224, Total Loss 8.90717601776123\n",
      "40: Encoding Loss -0.1566302329301834, Transition Loss -9.60948371887207, Classifier Loss 0.03509886935353279, Total Loss 3.43448805809021\n",
      "40: Encoding Loss 1.2888368368148804, Transition Loss -12.65836238861084, Classifier Loss 0.0760592371225357, Total Loss 17.914087295532227\n",
      "40: Encoding Loss 1.1605807542800903, Transition Loss -8.116294860839844, Classifier Loss 0.053718451410532, Total Loss 14.654868125915527\n",
      "40: Encoding Loss -0.7490609884262085, Transition Loss -9.405372619628906, Classifier Loss 0.05104777589440346, Total Loss 5.102896690368652\n",
      "40: Encoding Loss -1.6875081062316895, Transition Loss -5.114336967468262, Classifier Loss 0.07629340142011642, Total Loss 7.628317356109619\n",
      "40: Encoding Loss -1.9652199745178223, Transition Loss -14.362231254577637, Classifier Loss 0.0387689545750618, Total Loss 3.874022960662842\n",
      "40: Encoding Loss 0.084975466132164, Transition Loss -1.574785590171814, Classifier Loss 0.05412109196186066, Total Loss 5.957180023193359\n",
      "40: Encoding Loss -0.906033456325531, Transition Loss -7.884304046630859, Classifier Loss 0.03258208930492401, Total Loss 3.256632089614868\n",
      "40: Encoding Loss -0.5725477933883667, Transition Loss -7.396976470947266, Classifier Loss 0.0805649533867836, Total Loss 8.05501651763916\n",
      "40: Encoding Loss -0.5394299030303955, Transition Loss -16.3991756439209, Classifier Loss 0.1446259617805481, Total Loss 14.45931625366211\n",
      "40: Encoding Loss -2.194584369659424, Transition Loss -16.40380859375, Classifier Loss 0.07185424119234085, Total Loss 7.182143688201904\n",
      "40: Encoding Loss -2.573469638824463, Transition Loss -22.22368049621582, Classifier Loss 0.03989017382264137, Total Loss 3.984572649002075\n",
      "40: Encoding Loss -1.0351730585098267, Transition Loss -11.559664726257324, Classifier Loss 0.09319271892309189, Total Loss 9.316960334777832\n",
      "40: Encoding Loss -1.8778533935546875, Transition Loss -13.17101764678955, Classifier Loss 0.051814816892147064, Total Loss 5.178847789764404\n",
      "40: Encoding Loss -2.4873809814453125, Transition Loss -19.710189819335938, Classifier Loss 0.0557304210960865, Total Loss 5.5690999031066895\n",
      "40: Encoding Loss -1.8110146522521973, Transition Loss -6.958442687988281, Classifier Loss 0.08106353878974915, Total Loss 8.104962348937988\n",
      "40: Encoding Loss -1.0757606029510498, Transition Loss -16.072492599487305, Classifier Loss 0.08327708393335342, Total Loss 8.324493408203125\n",
      "40: Encoding Loss -1.695854902267456, Transition Loss -16.969139099121094, Classifier Loss 0.022465182468295097, Total Loss 2.24312424659729\n",
      "40: Encoding Loss -1.824753999710083, Transition Loss -20.324663162231445, Classifier Loss 0.054109808057546616, Total Loss 5.406915664672852\n",
      "40: Encoding Loss -2.7193140983581543, Transition Loss -26.636566162109375, Classifier Loss 0.020627779886126518, Total Loss 2.057450771331787\n",
      "41: Encoding Loss -1.2805076837539673, Transition Loss -5.076406478881836, Classifier Loss 0.0333620086312294, Total Loss 3.3351857662200928\n",
      "41: Encoding Loss -1.7741023302078247, Transition Loss -5.7343621253967285, Classifier Loss 0.09631218016147614, Total Loss 9.630070686340332\n",
      "41: Encoding Loss -1.6995035409927368, Transition Loss -0.12143418192863464, Classifier Loss 0.03632137179374695, Total Loss 3.632112979888916\n",
      "41: Encoding Loss -1.630340337753296, Transition Loss -19.19540786743164, Classifier Loss 0.057287875562906265, Total Loss 5.724948406219482\n",
      "41: Encoding Loss -1.6226173639297485, Transition Loss -9.007536888122559, Classifier Loss 0.04511567950248718, Total Loss 4.509766578674316\n",
      "41: Encoding Loss -1.3654457330703735, Transition Loss -15.684366226196289, Classifier Loss 0.04672454297542572, Total Loss 4.669317245483398\n",
      "41: Encoding Loss -1.8712811470031738, Transition Loss -11.148521423339844, Classifier Loss 0.05759930983185768, Total Loss 5.757701396942139\n",
      "41: Encoding Loss -1.3916980028152466, Transition Loss -8.886006355285645, Classifier Loss 0.0481087751686573, Total Loss 4.809100151062012\n",
      "41: Encoding Loss -1.2661538124084473, Transition Loss -15.840719223022461, Classifier Loss 0.036577947437763214, Total Loss 3.6546266078948975\n",
      "41: Encoding Loss -2.2426095008850098, Transition Loss -13.110623359680176, Classifier Loss 0.06173330545425415, Total Loss 6.170708179473877\n",
      "41: Encoding Loss -0.4306856393814087, Transition Loss -8.913674354553223, Classifier Loss 0.049503739923238754, Total Loss 4.9485626220703125\n",
      "41: Encoding Loss -0.7956106066703796, Transition Loss -5.318245887756348, Classifier Loss 0.06134585291147232, Total Loss 6.133521556854248\n",
      "41: Encoding Loss -1.8520171642303467, Transition Loss -7.138429641723633, Classifier Loss 0.04536420851945877, Total Loss 4.5349931716918945\n",
      "41: Encoding Loss -1.062670350074768, Transition Loss -7.990922927856445, Classifier Loss 0.054972246289253235, Total Loss 5.495626449584961\n",
      "41: Encoding Loss -2.1399221420288086, Transition Loss -15.260047912597656, Classifier Loss 0.03608640655875206, Total Loss 3.605588674545288\n",
      "41: Encoding Loss -2.324420213699341, Transition Loss -19.614566802978516, Classifier Loss 0.06268434226512909, Total Loss 6.2645111083984375\n",
      "41: Encoding Loss -1.31748366355896, Transition Loss -11.066810607910156, Classifier Loss 0.02162967063486576, Total Loss 2.1607534885406494\n",
      "41: Encoding Loss -0.6153694987297058, Transition Loss -7.122347354888916, Classifier Loss 0.044733572751283646, Total Loss 4.471932888031006\n",
      "41: Encoding Loss -0.6433945894241333, Transition Loss -14.75291633605957, Classifier Loss 0.07988132536411285, Total Loss 7.98518180847168\n",
      "41: Encoding Loss -0.8480238318443298, Transition Loss -6.500203609466553, Classifier Loss 0.05107257142663002, Total Loss 5.105957508087158\n",
      "41: Encoding Loss -1.2959903478622437, Transition Loss -16.511104583740234, Classifier Loss 0.07411808520555496, Total Loss 7.408506393432617\n",
      "41: Encoding Loss -1.7001146078109741, Transition Loss -12.35334587097168, Classifier Loss 0.0392557755112648, Total Loss 3.9231069087982178\n",
      "41: Encoding Loss -1.6024807691574097, Transition Loss -13.042449951171875, Classifier Loss 0.06262094527482986, Total Loss 6.259486198425293\n",
      "41: Encoding Loss -1.2463713884353638, Transition Loss -4.856781005859375, Classifier Loss 0.07485884428024292, Total Loss 7.484913349151611\n",
      "41: Encoding Loss -2.058337450027466, Transition Loss -12.928629875183105, Classifier Loss 0.033509261906147, Total Loss 3.3483405113220215\n",
      "41: Encoding Loss -0.8172436356544495, Transition Loss -6.23587703704834, Classifier Loss 0.06316947937011719, Total Loss 6.315700531005859\n",
      "41: Encoding Loss -2.1965551376342773, Transition Loss -4.421453952789307, Classifier Loss 0.0901748463511467, Total Loss 9.016600608825684\n",
      "41: Encoding Loss -2.6506597995758057, Transition Loss -20.21387481689453, Classifier Loss 0.06758653372526169, Total Loss 6.754610538482666\n",
      "41: Encoding Loss -2.0144498348236084, Transition Loss -10.721100807189941, Classifier Loss 0.0559418722987175, Total Loss 5.592042922973633\n",
      "41: Encoding Loss -1.0108734369277954, Transition Loss -6.290661811828613, Classifier Loss 0.06142355501651764, Total Loss 6.141097545623779\n",
      "41: Encoding Loss -1.2681083679199219, Transition Loss -12.816727638244629, Classifier Loss 0.06660370528697968, Total Loss 6.657806873321533\n",
      "41: Encoding Loss -1.274940848350525, Transition Loss -4.125368595123291, Classifier Loss 0.07703255861997604, Total Loss 7.702430725097656\n",
      "41: Encoding Loss -0.8847232460975647, Transition Loss -12.445297241210938, Classifier Loss 0.06081383302807808, Total Loss 6.078894138336182\n",
      "41: Encoding Loss -0.3237837851047516, Transition Loss -6.95859432220459, Classifier Loss 0.16434209048748016, Total Loss 16.431257247924805\n",
      "41: Encoding Loss -1.9709413051605225, Transition Loss -14.210149765014648, Classifier Loss 0.04075964167714119, Total Loss 4.073122024536133\n",
      "41: Encoding Loss -1.0514470338821411, Transition Loss -8.929408073425293, Classifier Loss 0.03970976918935776, Total Loss 3.969190835952759\n",
      "41: Encoding Loss -1.226493000984192, Transition Loss -8.043478965759277, Classifier Loss 0.06642726808786392, Total Loss 6.641118049621582\n",
      "41: Encoding Loss -1.097180724143982, Transition Loss -10.55700969696045, Classifier Loss 0.10989660769701004, Total Loss 10.987549781799316\n",
      "41: Encoding Loss -1.8315868377685547, Transition Loss -11.403188705444336, Classifier Loss 0.059428948909044266, Total Loss 5.940614223480225\n",
      "41: Encoding Loss -1.3868721723556519, Transition Loss -12.704290390014648, Classifier Loss 0.06131480634212494, Total Loss 6.128939628601074\n",
      "41: Encoding Loss -1.2433578968048096, Transition Loss -9.21509075164795, Classifier Loss 0.0321444533765316, Total Loss 3.212602376937866\n",
      "41: Encoding Loss -1.8757680654525757, Transition Loss -17.631389617919922, Classifier Loss 0.07427199929952621, Total Loss 7.423673629760742\n",
      "41: Encoding Loss -2.0800347328186035, Transition Loss -16.794384002685547, Classifier Loss 0.02969258278608322, Total Loss 2.9658994674682617\n",
      "41: Encoding Loss -2.1433868408203125, Transition Loss -6.7581787109375, Classifier Loss 0.05663633346557617, Total Loss 5.662281513214111\n",
      "41: Encoding Loss -1.8159680366516113, Transition Loss -18.51510238647461, Classifier Loss 0.043028246611356735, Total Loss 4.299121379852295\n",
      "41: Encoding Loss -0.3367690443992615, Transition Loss -8.290395736694336, Classifier Loss 0.0730522871017456, Total Loss 7.302549839019775\n",
      "41: Encoding Loss -0.8737888336181641, Transition Loss -18.689281463623047, Classifier Loss 0.058893218636512756, Total Loss 5.885583877563477\n",
      "41: Encoding Loss -1.6780188083648682, Transition Loss -6.970812797546387, Classifier Loss 0.031658392399549484, Total Loss 3.164444923400879\n",
      "41: Encoding Loss -1.9667404890060425, Transition Loss -10.03880500793457, Classifier Loss 0.052075017243623734, Total Loss 5.205493927001953\n",
      "41: Encoding Loss -1.5039844512939453, Transition Loss -16.9556884765625, Classifier Loss 0.046084947884082794, Total Loss 4.605103492736816\n",
      "41: Encoding Loss -2.002608299255371, Transition Loss -20.452861785888672, Classifier Loss 0.0467495359480381, Total Loss 4.670862674713135\n",
      "41: Encoding Loss -0.7217652797698975, Transition Loss -13.31796646118164, Classifier Loss 0.06265081465244293, Total Loss 6.262417793273926\n",
      "41: Encoding Loss -1.2247885465621948, Transition Loss -3.6090688705444336, Classifier Loss 0.07781222462654114, Total Loss 7.780500411987305\n",
      "41: Encoding Loss -1.5300854444503784, Transition Loss -13.000081062316895, Classifier Loss 0.04059942811727524, Total Loss 4.057342529296875\n",
      "41: Encoding Loss -1.0647919178009033, Transition Loss -12.84677505493164, Classifier Loss 0.07186231762170792, Total Loss 7.183662414550781\n",
      "41: Encoding Loss -1.6069755554199219, Transition Loss -5.819509506225586, Classifier Loss 0.0809035673737526, Total Loss 8.089193344116211\n",
      "41: Encoding Loss -0.7434816360473633, Transition Loss -11.882455825805664, Classifier Loss 0.029660047963261604, Total Loss 2.9636282920837402\n",
      "41: Encoding Loss -1.9730801582336426, Transition Loss -13.655279159545898, Classifier Loss 0.052976153790950775, Total Loss 5.29488468170166\n",
      "41: Encoding Loss -2.1804039478302, Transition Loss -20.357051849365234, Classifier Loss 0.03973545879125595, Total Loss 3.9694743156433105\n",
      "41: Encoding Loss -1.6238065958023071, Transition Loss -16.98691749572754, Classifier Loss 0.059753455221652985, Total Loss 5.971948146820068\n",
      "41: Encoding Loss -1.5909979343414307, Transition Loss -9.90274429321289, Classifier Loss 0.05777282267808914, Total Loss 5.775301456451416\n",
      "41: Encoding Loss -1.9159106016159058, Transition Loss -8.483760833740234, Classifier Loss 0.09236755967140198, Total Loss 9.23505973815918\n",
      "41: Encoding Loss -1.8235363960266113, Transition Loss -10.273892402648926, Classifier Loss 0.04989536106586456, Total Loss 4.987481594085693\n",
      "41: Encoding Loss -1.1838182210922241, Transition Loss -7.803069114685059, Classifier Loss 0.06852298974990845, Total Loss 6.850738048553467\n",
      "41: Encoding Loss -1.3544658422470093, Transition Loss -12.353536605834961, Classifier Loss 0.06278086453676224, Total Loss 6.27561616897583\n",
      "41: Encoding Loss -1.9728754758834839, Transition Loss -5.127802848815918, Classifier Loss 0.0964636504650116, Total Loss 9.645339965820312\n",
      "41: Encoding Loss -0.5651530623435974, Transition Loss -1.7374598979949951, Classifier Loss 0.05666474997997284, Total Loss 5.6661272048950195\n",
      "41: Encoding Loss -1.741405963897705, Transition Loss -19.05963897705078, Classifier Loss 0.07812942564487457, Total Loss 7.809130668640137\n",
      "41: Encoding Loss -0.3660617172718048, Transition Loss -9.14394474029541, Classifier Loss 0.06493522971868515, Total Loss 6.491325855255127\n",
      "41: Encoding Loss -0.9367788434028625, Transition Loss -9.292037010192871, Classifier Loss 0.10476621985435486, Total Loss 10.474762916564941\n",
      "41: Encoding Loss -1.6545275449752808, Transition Loss -9.135737419128418, Classifier Loss 0.07122104614973068, Total Loss 7.120277404785156\n",
      "41: Encoding Loss -1.3249610662460327, Transition Loss -9.174857139587402, Classifier Loss 0.07046680152416229, Total Loss 7.044845104217529\n",
      "41: Encoding Loss -0.9096242189407349, Transition Loss -5.824126243591309, Classifier Loss 0.08902858197689056, Total Loss 8.901693344116211\n",
      "41: Encoding Loss -1.4365408420562744, Transition Loss -9.120725631713867, Classifier Loss 0.022520655766129494, Total Loss 2.25024151802063\n",
      "41: Encoding Loss -1.9754159450531006, Transition Loss -8.316219329833984, Classifier Loss 0.09411110728979111, Total Loss 9.40944766998291\n",
      "41: Encoding Loss -1.4945429563522339, Transition Loss -15.009685516357422, Classifier Loss 0.03433151915669441, Total Loss 3.430150032043457\n",
      "41: Encoding Loss -1.141054630279541, Transition Loss -6.575098991394043, Classifier Loss 0.07254117727279663, Total Loss 7.252802848815918\n",
      "41: Encoding Loss -2.5336813926696777, Transition Loss -12.772582054138184, Classifier Loss 0.05513850226998329, Total Loss 5.511295795440674\n",
      "41: Encoding Loss -1.5745396614074707, Transition Loss -13.89206314086914, Classifier Loss 0.0271008238196373, Total Loss 2.707303762435913\n",
      "41: Encoding Loss -1.478253960609436, Transition Loss -10.137548446655273, Classifier Loss 0.10239506512880325, Total Loss 10.237479209899902\n",
      "41: Encoding Loss -0.9045733213424683, Transition Loss -11.379680633544922, Classifier Loss 0.0503213107585907, Total Loss 5.029855251312256\n",
      "41: Encoding Loss -1.4986573457717896, Transition Loss -9.065651893615723, Classifier Loss 0.04540059342980385, Total Loss 4.5382466316223145\n",
      "41: Encoding Loss -1.06515634059906, Transition Loss -5.374420642852783, Classifier Loss 0.08972369879484177, Total Loss 8.971295356750488\n",
      "41: Encoding Loss -0.5106920003890991, Transition Loss -11.726305961608887, Classifier Loss 0.07853005081415176, Total Loss 7.850659370422363\n",
      "41: Encoding Loss -2.2952587604522705, Transition Loss -7.959109306335449, Classifier Loss 0.05328124761581421, Total Loss 5.326533317565918\n",
      "41: Encoding Loss -1.3819750547409058, Transition Loss -5.958480358123779, Classifier Loss 0.06839367747306824, Total Loss 6.838176250457764\n",
      "41: Encoding Loss -1.9452036619186401, Transition Loss -16.37274169921875, Classifier Loss 0.05923149734735489, Total Loss 5.919875144958496\n",
      "41: Encoding Loss -1.298132300376892, Transition Loss -9.786643981933594, Classifier Loss 0.1010683923959732, Total Loss 10.10488224029541\n",
      "41: Encoding Loss -1.128939151763916, Transition Loss -8.294548988342285, Classifier Loss 0.04912180453538895, Total Loss 4.910521507263184\n",
      "41: Encoding Loss -1.009881854057312, Transition Loss -14.118197441101074, Classifier Loss 0.05151715874671936, Total Loss 5.148891925811768\n",
      "41: Encoding Loss -3.079010486602783, Transition Loss -22.647390365600586, Classifier Loss 0.04865207523107529, Total Loss 4.860678195953369\n",
      "41: Encoding Loss -1.0300390720367432, Transition Loss -7.56956672668457, Classifier Loss 0.07900556176900864, Total Loss 7.899042129516602\n",
      "41: Encoding Loss -1.3942630290985107, Transition Loss -8.115694046020508, Classifier Loss 0.06815031170845032, Total Loss 6.813407897949219\n",
      "41: Encoding Loss -0.8169386386871338, Transition Loss -10.136515617370605, Classifier Loss 0.06234419718384743, Total Loss 6.232392311096191\n",
      "41: Encoding Loss -1.318901777267456, Transition Loss -4.269582748413086, Classifier Loss 0.06974606215953827, Total Loss 6.973752021789551\n",
      "41: Encoding Loss -1.6974483728408813, Transition Loss -13.509479522705078, Classifier Loss 0.06987430900335312, Total Loss 6.984729290008545\n",
      "41: Encoding Loss -1.4509875774383545, Transition Loss -16.833417892456055, Classifier Loss 0.07204604893922806, Total Loss 7.201238632202148\n",
      "41: Encoding Loss -1.8086638450622559, Transition Loss -13.572096824645996, Classifier Loss 0.04903366416692734, Total Loss 4.900651931762695\n",
      "41: Encoding Loss -1.0807768106460571, Transition Loss -4.646847724914551, Classifier Loss 0.03390316665172577, Total Loss 3.389387369155884\n",
      "41: Encoding Loss -1.7183293104171753, Transition Loss -7.499846458435059, Classifier Loss 0.09797537326812744, Total Loss 9.796036720275879\n",
      "41: Encoding Loss -2.0772640705108643, Transition Loss -14.478671073913574, Classifier Loss 0.05767792463302612, Total Loss 5.764896869659424\n",
      "41: Encoding Loss -0.7434889078140259, Transition Loss 0.4890782833099365, Classifier Loss 0.10076845437288284, Total Loss 10.174660682678223\n",
      "41: Encoding Loss -1.111445665359497, Transition Loss -8.68537425994873, Classifier Loss 0.06681343913078308, Total Loss 6.679606914520264\n",
      "41: Encoding Loss -0.34378567337989807, Transition Loss -5.146803379058838, Classifier Loss 0.07588820159435272, Total Loss 7.586984157562256\n",
      "41: Encoding Loss -1.0763661861419678, Transition Loss -12.423125267028809, Classifier Loss 0.04756969213485718, Total Loss 4.7544846534729\n",
      "41: Encoding Loss -1.8207001686096191, Transition Loss -16.168386459350586, Classifier Loss 0.07732539623975754, Total Loss 7.729305744171143\n",
      "41: Encoding Loss -1.2281088829040527, Transition Loss -5.236528396606445, Classifier Loss 0.0749673992395401, Total Loss 7.495692729949951\n",
      "41: Encoding Loss -1.2902523279190063, Transition Loss -9.370046615600586, Classifier Loss 0.040456485003232956, Total Loss 4.043774604797363\n",
      "41: Encoding Loss -1.5697057247161865, Transition Loss -9.3045654296875, Classifier Loss 0.0517631359398365, Total Loss 5.174452304840088\n",
      "41: Encoding Loss -0.9659989476203918, Transition Loss -14.770964622497559, Classifier Loss 0.055943813174963, Total Loss 5.591427326202393\n",
      "41: Encoding Loss -1.2593343257904053, Transition Loss -14.438192367553711, Classifier Loss 0.06514415144920349, Total Loss 6.5115275382995605\n",
      "41: Encoding Loss -1.0152767896652222, Transition Loss -1.731367588043213, Classifier Loss 0.039454929530620575, Total Loss 3.9451467990875244\n",
      "41: Encoding Loss -1.321908950805664, Transition Loss -18.254940032958984, Classifier Loss 0.038155846297740936, Total Loss 3.811933755874634\n",
      "41: Encoding Loss -1.8339619636535645, Transition Loss -17.46883773803711, Classifier Loss 0.03468584641814232, Total Loss 3.465090751647949\n",
      "41: Encoding Loss -0.7462011575698853, Transition Loss -1.1655733585357666, Classifier Loss 0.05675039067864418, Total Loss 5.674806118011475\n",
      "41: Encoding Loss -0.5286753177642822, Transition Loss 2.1617372035980225, Classifier Loss 0.05833856016397476, Total Loss 6.266202926635742\n",
      "41: Encoding Loss -0.6808011531829834, Transition Loss -12.106670379638672, Classifier Loss 0.03940223902463913, Total Loss 3.93780255317688\n",
      "41: Encoding Loss -1.1687475442886353, Transition Loss -11.242740631103516, Classifier Loss 0.04950598254799843, Total Loss 4.948349475860596\n",
      "41: Encoding Loss -2.110290050506592, Transition Loss -13.19786262512207, Classifier Loss 0.05503106117248535, Total Loss 5.500466346740723\n",
      "41: Encoding Loss -1.0447111129760742, Transition Loss -6.708026885986328, Classifier Loss 0.05811250954866409, Total Loss 5.809909343719482\n",
      "41: Encoding Loss -2.465970277786255, Transition Loss -15.448102951049805, Classifier Loss 0.040155038237571716, Total Loss 4.012414455413818\n",
      "41: Encoding Loss -0.978896975517273, Transition Loss -9.06558609008789, Classifier Loss 0.07154501974582672, Total Loss 7.152688980102539\n",
      "41: Encoding Loss -1.27695894241333, Transition Loss -12.739005088806152, Classifier Loss 0.09085271507501602, Total Loss 9.082723617553711\n",
      "41: Encoding Loss -2.4230146408081055, Transition Loss -20.35466957092285, Classifier Loss 0.05159249156713486, Total Loss 5.155178546905518\n",
      "41: Encoding Loss -1.8776545524597168, Transition Loss -8.516609191894531, Classifier Loss 0.06650033593177795, Total Loss 6.648330211639404\n",
      "41: Encoding Loss -1.6620217561721802, Transition Loss -10.291699409484863, Classifier Loss 0.09894592314958572, Total Loss 9.892534255981445\n",
      "41: Encoding Loss -1.5313502550125122, Transition Loss -16.360103607177734, Classifier Loss 0.07939702272415161, Total Loss 7.936429977416992\n",
      "41: Encoding Loss -1.9826380014419556, Transition Loss -12.767982482910156, Classifier Loss 0.027003426104784012, Total Loss 2.697788953781128\n",
      "41: Encoding Loss -0.6929805278778076, Transition Loss -11.783574104309082, Classifier Loss 0.060829371213912964, Total Loss 6.080580711364746\n",
      "41: Encoding Loss -1.654954195022583, Transition Loss -5.079148292541504, Classifier Loss 0.043193425983190536, Total Loss 4.318326950073242\n",
      "41: Encoding Loss -0.8625151515007019, Transition Loss -8.681049346923828, Classifier Loss 0.061375752091407776, Total Loss 6.135838985443115\n",
      "41: Encoding Loss -1.5050652027130127, Transition Loss -9.538419723510742, Classifier Loss 0.054535042494535446, Total Loss 5.451596260070801\n",
      "41: Encoding Loss -0.9346616864204407, Transition Loss -4.779743671417236, Classifier Loss 0.034580670297145844, Total Loss 3.45711088180542\n",
      "41: Encoding Loss -1.0394965410232544, Transition Loss -10.461797714233398, Classifier Loss 0.028591280803084373, Total Loss 2.8570356369018555\n",
      "41: Encoding Loss -1.1165269613265991, Transition Loss -12.781357765197754, Classifier Loss 0.0715966671705246, Total Loss 7.157110214233398\n",
      "41: Encoding Loss -0.2720392346382141, Transition Loss -12.7758207321167, Classifier Loss 0.05815761536359787, Total Loss 5.806110858917236\n",
      "41: Encoding Loss -1.171098232269287, Transition Loss -0.20591974258422852, Classifier Loss 0.08623509109020233, Total Loss 8.623468399047852\n",
      "41: Encoding Loss -1.11418879032135, Transition Loss -8.64561653137207, Classifier Loss 0.034680649638175964, Total Loss 3.4663360118865967\n",
      "41: Encoding Loss -0.5878167748451233, Transition Loss -11.307853698730469, Classifier Loss 0.0747988298535347, Total Loss 7.477621555328369\n",
      "41: Encoding Loss -0.3542003929615021, Transition Loss -10.15814208984375, Classifier Loss 0.05179869383573532, Total Loss 5.177274703979492\n",
      "41: Encoding Loss -0.8178449869155884, Transition Loss -9.334725379943848, Classifier Loss 0.05075812712311745, Total Loss 5.073945999145508\n",
      "41: Encoding Loss -2.017733335494995, Transition Loss -4.931558132171631, Classifier Loss 0.07809598743915558, Total Loss 7.808612823486328\n",
      "41: Encoding Loss -1.9831812381744385, Transition Loss -14.595385551452637, Classifier Loss 0.03605636581778526, Total Loss 3.602717638015747\n",
      "41: Encoding Loss -0.13681043684482574, Transition Loss -1.334862470626831, Classifier Loss 0.052394457161426544, Total Loss 5.145447254180908\n",
      "41: Encoding Loss -0.691239595413208, Transition Loss -7.885124683380127, Classifier Loss 0.03164580836892128, Total Loss 3.16300368309021\n",
      "41: Encoding Loss -0.1823735535144806, Transition Loss -7.23351526260376, Classifier Loss 0.08342327177524567, Total Loss 8.29113483428955\n",
      "41: Encoding Loss -0.09471285343170166, Transition Loss -16.755582809448242, Classifier Loss 0.14741213619709015, Total Loss 14.607699394226074\n",
      "41: Encoding Loss -2.080129623413086, Transition Loss -17.575511932373047, Classifier Loss 0.07055455446243286, Total Loss 7.051939964294434\n",
      "41: Encoding Loss -1.9733233451843262, Transition Loss -23.67819595336914, Classifier Loss 0.03870219364762306, Total Loss 3.8654837608337402\n",
      "41: Encoding Loss -0.8386334180831909, Transition Loss -12.346015930175781, Classifier Loss 0.0892462432384491, Total Loss 8.922155380249023\n",
      "41: Encoding Loss -1.5212976932525635, Transition Loss -13.916191101074219, Classifier Loss 0.050308048725128174, Total Loss 5.028021335601807\n",
      "41: Encoding Loss -1.9563086032867432, Transition Loss -20.761232376098633, Classifier Loss 0.05274874344468117, Total Loss 5.270721912384033\n",
      "41: Encoding Loss -1.3849045038223267, Transition Loss -7.330018997192383, Classifier Loss 0.08251437544822693, Total Loss 8.249971389770508\n",
      "41: Encoding Loss -0.6644718050956726, Transition Loss -16.960172653198242, Classifier Loss 0.08131126314401627, Total Loss 8.127734184265137\n",
      "41: Encoding Loss -1.137283444404602, Transition Loss -18.060483932495117, Classifier Loss 0.02114102430641651, Total Loss 2.110490322113037\n",
      "41: Encoding Loss -1.3178493976593018, Transition Loss -21.484392166137695, Classifier Loss 0.05531194806098938, Total Loss 5.52689790725708\n",
      "41: Encoding Loss -2.163212299346924, Transition Loss -28.058778762817383, Classifier Loss 0.01820676028728485, Total Loss 1.8150643110275269\n",
      "42: Encoding Loss -0.681583046913147, Transition Loss -5.317802429199219, Classifier Loss 0.03257874399423599, Total Loss 3.2568109035491943\n",
      "42: Encoding Loss -1.516963243484497, Transition Loss -6.118033409118652, Classifier Loss 0.09600161015987396, Total Loss 9.598937034606934\n",
      "42: Encoding Loss -1.1177769899368286, Transition Loss -0.03156927227973938, Classifier Loss 0.03594048321247101, Total Loss 3.5940420627593994\n",
      "42: Encoding Loss -1.152919054031372, Transition Loss -20.261838912963867, Classifier Loss 0.05534188076853752, Total Loss 5.5301361083984375\n",
      "42: Encoding Loss -0.9772055149078369, Transition Loss -9.74573040008545, Classifier Loss 0.04496860131621361, Total Loss 4.494910717010498\n",
      "42: Encoding Loss -1.0927411317825317, Transition Loss -16.52498435974121, Classifier Loss 0.046106502413749695, Total Loss 4.607345104217529\n",
      "42: Encoding Loss -1.522472858428955, Transition Loss -11.587472915649414, Classifier Loss 0.05647806078195572, Total Loss 5.645488739013672\n",
      "42: Encoding Loss -0.7704231142997742, Transition Loss -9.221925735473633, Classifier Loss 0.04699286073446274, Total Loss 4.697441577911377\n",
      "42: Encoding Loss -0.7306299805641174, Transition Loss -16.558855056762695, Classifier Loss 0.03613632544875145, Total Loss 3.610320568084717\n",
      "42: Encoding Loss -1.700870156288147, Transition Loss -13.664026260375977, Classifier Loss 0.060995884239673615, Total Loss 6.096855640411377\n",
      "42: Encoding Loss -0.21114222705364227, Transition Loss -9.247834205627441, Classifier Loss 0.04978705942630768, Total Loss 4.947519302368164\n",
      "42: Encoding Loss -0.3350600600242615, Transition Loss -5.523122787475586, Classifier Loss 0.06194942072033882, Total Loss 6.192756175994873\n",
      "42: Encoding Loss -0.9414214491844177, Transition Loss -7.796536445617676, Classifier Loss 0.04434571787714958, Total Loss 4.43301248550415\n",
      "42: Encoding Loss -0.6216408610343933, Transition Loss -8.668987274169922, Classifier Loss 0.054927386343479156, Total Loss 5.491004943847656\n",
      "42: Encoding Loss -1.2783405780792236, Transition Loss -16.39787483215332, Classifier Loss 0.03384110704064369, Total Loss 3.380831003189087\n",
      "42: Encoding Loss -1.789566159248352, Transition Loss -21.133054733276367, Classifier Loss 0.06414326280355453, Total Loss 6.410099506378174\n",
      "42: Encoding Loss -0.9291197061538696, Transition Loss -12.083526611328125, Classifier Loss 0.020507417619228363, Total Loss 2.0483250617980957\n",
      "42: Encoding Loss -0.3531227707862854, Transition Loss -7.84584903717041, Classifier Loss 0.04367406666278839, Total Loss 4.365253448486328\n",
      "42: Encoding Loss -0.4024428129196167, Transition Loss -15.875136375427246, Classifier Loss 0.08005955815315247, Total Loss 8.002689361572266\n",
      "42: Encoding Loss -0.25067928433418274, Transition Loss -6.96624755859375, Classifier Loss 0.050160493701696396, Total Loss 5.002439975738525\n",
      "42: Encoding Loss -0.670739471912384, Transition Loss -18.118690490722656, Classifier Loss 0.07379592955112457, Total Loss 7.375968933105469\n",
      "42: Encoding Loss -1.09831702709198, Transition Loss -13.497617721557617, Classifier Loss 0.03478815779089928, Total Loss 3.476116180419922\n",
      "42: Encoding Loss -0.951836347579956, Transition Loss -14.326250076293945, Classifier Loss 0.06175621598958969, Total Loss 6.172756195068359\n",
      "42: Encoding Loss -0.5700407028198242, Transition Loss -5.173487663269043, Classifier Loss 0.0726492777466774, Total Loss 7.263893127441406\n",
      "42: Encoding Loss -1.1463032960891724, Transition Loss -14.350025177001953, Classifier Loss 0.031093811616301537, Total Loss 3.106511116027832\n",
      "42: Encoding Loss 0.06112396717071533, Transition Loss -6.824024200439453, Classifier Loss 0.06014575436711311, Total Loss 6.369920253753662\n",
      "42: Encoding Loss -1.4969245195388794, Transition Loss -6.072592735290527, Classifier Loss 0.08913286030292511, Total Loss 8.912071228027344\n",
      "42: Encoding Loss -2.475888729095459, Transition Loss -19.89145851135254, Classifier Loss 0.06642501801252365, Total Loss 6.638523578643799\n",
      "42: Encoding Loss -1.3043477535247803, Transition Loss -11.48146915435791, Classifier Loss 0.05378793179988861, Total Loss 5.3764967918396\n",
      "42: Encoding Loss -0.7306748628616333, Transition Loss -7.520454406738281, Classifier Loss 0.06277033686637878, Total Loss 6.275529861450195\n",
      "42: Encoding Loss -0.859007716178894, Transition Loss -13.017660140991211, Classifier Loss 0.06600386649370193, Total Loss 6.597783088684082\n",
      "42: Encoding Loss -0.41149580478668213, Transition Loss -5.54140567779541, Classifier Loss 0.0770333856344223, Total Loss 7.702166557312012\n",
      "42: Encoding Loss -0.36856117844581604, Transition Loss -12.479841232299805, Classifier Loss 0.06216339021921158, Total Loss 6.213507175445557\n",
      "42: Encoding Loss 0.3022206127643585, Transition Loss -7.83064603805542, Classifier Loss 0.15917688608169556, Total Loss 18.330854415893555\n",
      "42: Encoding Loss 2.19820499420166, Transition Loss -13.430081367492676, Classifier Loss 0.04620693251490593, Total Loss 22.20364761352539\n",
      "42: Encoding Loss 0.4777451753616333, Transition Loss -11.243764877319336, Classifier Loss 0.03726496174931526, Total Loss 7.546205520629883\n",
      "42: Encoding Loss -0.15932823717594147, Transition Loss -7.610108852386475, Classifier Loss 0.07151976972818375, Total Loss 7.079651355743408\n",
      "42: Encoding Loss -0.3350718915462494, Transition Loss -9.741316795349121, Classifier Loss 0.11186189949512482, Total Loss 11.183160781860352\n",
      "42: Encoding Loss -0.7742488980293274, Transition Loss -10.83880615234375, Classifier Loss 0.058210425078868866, Total Loss 5.818874835968018\n",
      "42: Encoding Loss -0.4404298663139343, Transition Loss -12.14112663269043, Classifier Loss 0.05790141597390175, Total Loss 5.787694931030273\n",
      "42: Encoding Loss -0.4660026431083679, Transition Loss -8.573904991149902, Classifier Loss 0.03202865645289421, Total Loss 3.2011449337005615\n",
      "42: Encoding Loss -0.4157858192920685, Transition Loss -16.814619064331055, Classifier Loss 0.07432931661605835, Total Loss 7.429515361785889\n",
      "42: Encoding Loss -0.8871440887451172, Transition Loss -16.032794952392578, Classifier Loss 0.029263462871313095, Total Loss 2.923139810562134\n",
      "42: Encoding Loss -1.4258809089660645, Transition Loss -6.347103118896484, Classifier Loss 0.05709012597799301, Total Loss 5.707743167877197\n",
      "42: Encoding Loss -0.8732916116714478, Transition Loss -17.381547927856445, Classifier Loss 0.041908908635377884, Total Loss 4.187414646148682\n",
      "42: Encoding Loss 0.47744303941726685, Transition Loss -7.894087791442871, Classifier Loss 0.07239128649234772, Total Loss 11.057089805603027\n",
      "42: Encoding Loss -0.09438738971948624, Transition Loss -18.656993865966797, Classifier Loss 0.05925331637263298, Total Loss 5.791257381439209\n",
      "42: Encoding Loss -1.3643646240234375, Transition Loss -7.181404113769531, Classifier Loss 0.02938898093998432, Total Loss 2.9374618530273438\n",
      "42: Encoding Loss -2.026149034500122, Transition Loss -10.40643310546875, Classifier Loss 0.050975535064935684, Total Loss 5.09547233581543\n",
      "42: Encoding Loss -1.4322054386138916, Transition Loss -17.064056396484375, Classifier Loss 0.04348856955766678, Total Loss 4.345444202423096\n",
      "42: Encoding Loss -2.0233004093170166, Transition Loss -20.303258895874023, Classifier Loss 0.04878726601600647, Total Loss 4.8746657371521\n",
      "42: Encoding Loss -0.18385368585586548, Transition Loss -13.556538581848145, Classifier Loss 0.06502503901720047, Total Loss 6.451267242431641\n",
      "42: Encoding Loss -1.4217808246612549, Transition Loss -3.8475029468536377, Classifier Loss 0.08026880025863647, Total Loss 8.026110649108887\n",
      "42: Encoding Loss -1.5937613248825073, Transition Loss -12.919614791870117, Classifier Loss 0.041178081184625626, Total Loss 4.115224361419678\n",
      "42: Encoding Loss -0.7554053068161011, Transition Loss -12.798572540283203, Classifier Loss 0.06825713068246841, Total Loss 6.823153495788574\n",
      "42: Encoding Loss -1.1059458255767822, Transition Loss -6.067697525024414, Classifier Loss 0.08105114847421646, Total Loss 8.103901863098145\n",
      "42: Encoding Loss -0.5424911379814148, Transition Loss -11.906350135803223, Classifier Loss 0.028732560575008392, Total Loss 2.8708744049072266\n",
      "42: Encoding Loss -1.8468918800354004, Transition Loss -13.743855476379395, Classifier Loss 0.05076185613870621, Total Loss 5.073436737060547\n",
      "42: Encoding Loss -2.1878552436828613, Transition Loss -20.058958053588867, Classifier Loss 0.036061182618141174, Total Loss 3.6021063327789307\n",
      "42: Encoding Loss -1.500236988067627, Transition Loss -17.12640953063965, Classifier Loss 0.061870839446783066, Total Loss 6.183658599853516\n",
      "42: Encoding Loss -1.9673677682876587, Transition Loss -10.170073509216309, Classifier Loss 0.0573800727725029, Total Loss 5.735972881317139\n",
      "42: Encoding Loss -1.9177836179733276, Transition Loss -8.896467208862305, Classifier Loss 0.09794414043426514, Total Loss 9.792634963989258\n",
      "42: Encoding Loss -1.3726028203964233, Transition Loss -10.696450233459473, Classifier Loss 0.04843274503946304, Total Loss 4.841135501861572\n",
      "42: Encoding Loss -1.0753607749938965, Transition Loss -8.045145988464355, Classifier Loss 0.06805504113435745, Total Loss 6.803895473480225\n",
      "42: Encoding Loss -1.4445968866348267, Transition Loss -12.328272819519043, Classifier Loss 0.06335330754518509, Total Loss 6.332865238189697\n",
      "42: Encoding Loss -1.7572696208953857, Transition Loss -5.577915191650391, Classifier Loss 0.09943912923336029, Total Loss 9.94279670715332\n",
      "42: Encoding Loss -0.5297830104827881, Transition Loss -2.294067859649658, Classifier Loss 0.05453327298164368, Total Loss 5.4528679847717285\n",
      "42: Encoding Loss -2.0924789905548096, Transition Loss -19.10411834716797, Classifier Loss 0.07111069560050964, Total Loss 7.107248783111572\n",
      "42: Encoding Loss -0.22228951752185822, Transition Loss -9.648086547851562, Classifier Loss 0.06467481702566147, Total Loss 6.442235469818115\n",
      "42: Encoding Loss -1.032813549041748, Transition Loss -9.635763168334961, Classifier Loss 0.10355627536773682, Total Loss 10.353699684143066\n",
      "42: Encoding Loss -1.5546743869781494, Transition Loss -9.338196754455566, Classifier Loss 0.06947188079357147, Total Loss 6.945320129394531\n",
      "42: Encoding Loss -1.1365197896957397, Transition Loss -9.476935386657715, Classifier Loss 0.06949087232351303, Total Loss 6.9471917152404785\n",
      "42: Encoding Loss -0.9457342624664307, Transition Loss -6.251087665557861, Classifier Loss 0.08831677585840225, Total Loss 8.830427169799805\n",
      "42: Encoding Loss -1.1597905158996582, Transition Loss -9.709150314331055, Classifier Loss 0.02137199603021145, Total Loss 2.1352577209472656\n",
      "42: Encoding Loss -1.4686983823776245, Transition Loss -8.73289680480957, Classifier Loss 0.09042029082775116, Total Loss 9.040283203125\n",
      "42: Encoding Loss -1.395361065864563, Transition Loss -15.220009803771973, Classifier Loss 0.03392423316836357, Total Loss 3.3893795013427734\n",
      "42: Encoding Loss -0.9365323185920715, Transition Loss -7.202203750610352, Classifier Loss 0.07154180109500885, Total Loss 7.152739524841309\n",
      "42: Encoding Loss -2.29548978805542, Transition Loss -13.165311813354492, Classifier Loss 0.05634697899222374, Total Loss 5.6320648193359375\n",
      "42: Encoding Loss -1.7349036931991577, Transition Loss -14.258539199829102, Classifier Loss 0.02428010292351246, Total Loss 2.4251585006713867\n",
      "42: Encoding Loss -1.2938342094421387, Transition Loss -10.667373657226562, Classifier Loss 0.09607186168432236, Total Loss 9.605052947998047\n",
      "42: Encoding Loss -0.6321596503257751, Transition Loss -11.852621078491211, Classifier Loss 0.04993563890457153, Total Loss 4.9911932945251465\n",
      "42: Encoding Loss -1.548236608505249, Transition Loss -9.587907791137695, Classifier Loss 0.04429322108626366, Total Loss 4.427404880523682\n",
      "42: Encoding Loss -1.0297865867614746, Transition Loss -6.111061096191406, Classifier Loss 0.09085346013307571, Total Loss 9.084123611450195\n",
      "42: Encoding Loss -0.10958556830883026, Transition Loss -11.829675674438477, Classifier Loss 0.07780193537473679, Total Loss 7.658097743988037\n",
      "42: Encoding Loss -1.7480318546295166, Transition Loss -8.432408332824707, Classifier Loss 0.05230574309825897, Total Loss 5.228887557983398\n",
      "42: Encoding Loss -0.9773103594779968, Transition Loss -6.456584930419922, Classifier Loss 0.06502509862184525, Total Loss 6.501218795776367\n",
      "42: Encoding Loss -1.6184769868850708, Transition Loss -16.363834381103516, Classifier Loss 0.06084740161895752, Total Loss 6.081467628479004\n",
      "42: Encoding Loss -0.8799489736557007, Transition Loss -10.233582496643066, Classifier Loss 0.10429096221923828, Total Loss 10.42704963684082\n",
      "42: Encoding Loss -0.7801162600517273, Transition Loss -8.573925971984863, Classifier Loss 0.0482356958091259, Total Loss 4.821855068206787\n",
      "42: Encoding Loss -1.15073561668396, Transition Loss -14.182015419006348, Classifier Loss 0.051072631031274796, Total Loss 5.104426860809326\n",
      "42: Encoding Loss -2.9649908542633057, Transition Loss -22.34665298461914, Classifier Loss 0.04830630123615265, Total Loss 4.826160907745361\n",
      "42: Encoding Loss -0.8754375576972961, Transition Loss -7.684624195098877, Classifier Loss 0.08010608702898026, Total Loss 8.009071350097656\n",
      "42: Encoding Loss -1.6729060411453247, Transition Loss -8.46863079071045, Classifier Loss 0.06921742111444473, Total Loss 6.920048236846924\n",
      "42: Encoding Loss -0.6623634099960327, Transition Loss -10.43911361694336, Classifier Loss 0.06236397475004196, Total Loss 6.234309673309326\n",
      "42: Encoding Loss -1.1732046604156494, Transition Loss -4.7625532150268555, Classifier Loss 0.07022993266582489, Total Loss 7.022040367126465\n",
      "42: Encoding Loss -1.495453119277954, Transition Loss -13.734983444213867, Classifier Loss 0.07069963961839676, Total Loss 7.067216873168945\n",
      "42: Encoding Loss -1.2803874015808105, Transition Loss -16.79131317138672, Classifier Loss 0.07274029403924942, Total Loss 7.2706708908081055\n",
      "42: Encoding Loss -1.4358155727386475, Transition Loss -13.775507926940918, Classifier Loss 0.049887582659721375, Total Loss 4.9860029220581055\n",
      "42: Encoding Loss -0.783430814743042, Transition Loss -5.278787612915039, Classifier Loss 0.03395090252161026, Total Loss 3.3940346240997314\n",
      "42: Encoding Loss -1.4325612783432007, Transition Loss -7.977469444274902, Classifier Loss 0.09999621659517288, Total Loss 9.998025894165039\n",
      "42: Encoding Loss -1.8826582431793213, Transition Loss -14.941728591918945, Classifier Loss 0.05589954927563667, Total Loss 5.586966514587402\n",
      "42: Encoding Loss -0.7805144786834717, Transition Loss -0.5111376643180847, Classifier Loss 0.10140178352594376, Total Loss 10.140076637268066\n",
      "42: Encoding Loss -0.996350884437561, Transition Loss -8.802460670471191, Classifier Loss 0.06809797883033752, Total Loss 6.808037281036377\n",
      "42: Encoding Loss -0.9094023108482361, Transition Loss -5.316856384277344, Classifier Loss 0.07402636855840683, Total Loss 7.401573657989502\n",
      "42: Encoding Loss -1.213060975074768, Transition Loss -12.391797065734863, Classifier Loss 0.046448323875665665, Total Loss 4.642354488372803\n",
      "42: Encoding Loss -1.6381672620773315, Transition Loss -16.19553565979004, Classifier Loss 0.07652916759252548, Total Loss 7.649677753448486\n",
      "42: Encoding Loss -1.3562595844268799, Transition Loss -5.408803462982178, Classifier Loss 0.07418902963399887, Total Loss 7.417820930480957\n",
      "42: Encoding Loss -1.0275928974151611, Transition Loss -9.639256477355957, Classifier Loss 0.04196092486381531, Total Loss 4.194164752960205\n",
      "42: Encoding Loss -1.4808309078216553, Transition Loss -9.516317367553711, Classifier Loss 0.05078946799039841, Total Loss 5.077043533325195\n",
      "42: Encoding Loss -1.1902354955673218, Transition Loss -14.800193786621094, Classifier Loss 0.05386577919125557, Total Loss 5.383617877960205\n",
      "42: Encoding Loss -1.4743436574935913, Transition Loss -14.367809295654297, Classifier Loss 0.0663120299577713, Total Loss 6.628329753875732\n",
      "42: Encoding Loss -1.453583836555481, Transition Loss -2.324690818786621, Classifier Loss 0.039365071803331375, Total Loss 3.936042308807373\n",
      "42: Encoding Loss -1.6834585666656494, Transition Loss -18.165420532226562, Classifier Loss 0.03821825981140137, Total Loss 3.818192958831787\n",
      "42: Encoding Loss -1.5073277950286865, Transition Loss -17.473194122314453, Classifier Loss 0.03350042924284935, Total Loss 3.346548080444336\n",
      "42: Encoding Loss -0.856315553188324, Transition Loss -1.7030434608459473, Classifier Loss 0.05552633851766586, Total Loss 5.552293300628662\n",
      "42: Encoding Loss -0.548958420753479, Transition Loss 1.2743988037109375, Classifier Loss 0.05517059564590454, Total Loss 5.771939277648926\n",
      "42: Encoding Loss -0.6795655488967896, Transition Loss -12.26004409790039, Classifier Loss 0.03811262175440788, Total Loss 3.808810234069824\n",
      "42: Encoding Loss -1.2229726314544678, Transition Loss -11.470202445983887, Classifier Loss 0.046273455023765564, Total Loss 4.625051498413086\n",
      "42: Encoding Loss -2.2534632682800293, Transition Loss -13.500934600830078, Classifier Loss 0.05387651547789574, Total Loss 5.384951114654541\n",
      "42: Encoding Loss -1.4250946044921875, Transition Loss -7.068991661071777, Classifier Loss 0.057291846722364426, Total Loss 5.727770805358887\n",
      "42: Encoding Loss -2.3451027870178223, Transition Loss -15.577603340148926, Classifier Loss 0.039067186415195465, Total Loss 3.9036033153533936\n",
      "42: Encoding Loss -1.0396740436553955, Transition Loss -9.325850486755371, Classifier Loss 0.07008595019578934, Total Loss 7.006729602813721\n",
      "42: Encoding Loss -1.3897631168365479, Transition Loss -12.908761978149414, Classifier Loss 0.09223633259534836, Total Loss 9.221051216125488\n",
      "42: Encoding Loss -2.52361798286438, Transition Loss -20.263111114501953, Classifier Loss 0.04967981204390526, Total Loss 4.963928699493408\n",
      "42: Encoding Loss -1.7539616823196411, Transition Loss -9.045089721679688, Classifier Loss 0.06498321145772934, Total Loss 6.496511936187744\n",
      "42: Encoding Loss -1.686561107635498, Transition Loss -10.628265380859375, Classifier Loss 0.09783033281564713, Total Loss 9.78090763092041\n",
      "42: Encoding Loss -1.5613937377929688, Transition Loss -16.35026741027832, Classifier Loss 0.0793880745768547, Total Loss 7.935537338256836\n",
      "42: Encoding Loss -1.6779518127441406, Transition Loss -13.098567962646484, Classifier Loss 0.026191776618361473, Total Loss 2.6165578365325928\n",
      "42: Encoding Loss -0.5318130850791931, Transition Loss -11.96851921081543, Classifier Loss 0.05964403972029686, Total Loss 5.962009906768799\n",
      "42: Encoding Loss -1.193516492843628, Transition Loss -5.531623840332031, Classifier Loss 0.04198657348752022, Total Loss 4.197551250457764\n",
      "42: Encoding Loss -0.32632145285606384, Transition Loss -9.065093040466309, Classifier Loss 0.06298565864562988, Total Loss 6.295315265655518\n",
      "42: Encoding Loss -1.2802081108093262, Transition Loss -9.804102897644043, Classifier Loss 0.05417286604642868, Total Loss 5.41532564163208\n",
      "42: Encoding Loss -0.5589994788169861, Transition Loss -5.4052534103393555, Classifier Loss 0.03398628532886505, Total Loss 3.397547483444214\n",
      "42: Encoding Loss -0.7669009566307068, Transition Loss -10.759650230407715, Classifier Loss 0.027135737240314484, Total Loss 2.7114217281341553\n",
      "42: Encoding Loss -1.1983869075775146, Transition Loss -12.95131778717041, Classifier Loss 0.06919904798269272, Total Loss 6.917314529418945\n",
      "42: Encoding Loss 0.09109596908092499, Transition Loss -12.805719375610352, Classifier Loss 0.057260267436504364, Total Loss 6.320210933685303\n",
      "42: Encoding Loss -1.476720929145813, Transition Loss -1.7678532600402832, Classifier Loss 0.08874563127756119, Total Loss 8.8742094039917\n",
      "42: Encoding Loss -1.290360927581787, Transition Loss -9.978104591369629, Classifier Loss 0.039136406034231186, Total Loss 3.91164493560791\n",
      "42: Encoding Loss -0.7366074323654175, Transition Loss -12.984639167785645, Classifier Loss 0.06816022098064423, Total Loss 6.813425064086914\n",
      "42: Encoding Loss -0.32447555661201477, Transition Loss -10.803365707397461, Classifier Loss 0.051779866218566895, Total Loss 5.174300193786621\n",
      "42: Encoding Loss -1.4113883972167969, Transition Loss -10.585201263427734, Classifier Loss 0.04662824049592018, Total Loss 4.660706996917725\n",
      "42: Encoding Loss -2.628129243850708, Transition Loss -6.35880184173584, Classifier Loss 0.07956672459840775, Total Loss 7.9554009437561035\n",
      "42: Encoding Loss -2.4668469429016113, Transition Loss -15.529090881347656, Classifier Loss 0.038878895342350006, Total Loss 3.8847837448120117\n",
      "42: Encoding Loss -0.15442949533462524, Transition Loss -2.6585025787353516, Classifier Loss 0.052173350006341934, Total Loss 5.141122817993164\n",
      "42: Encoding Loss -1.3147549629211426, Transition Loss -9.116905212402344, Classifier Loss 0.03296789154410362, Total Loss 3.2949657440185547\n",
      "42: Encoding Loss -0.7592713236808777, Transition Loss -8.704225540161133, Classifier Loss 0.07483780384063721, Total Loss 7.482039451599121\n",
      "42: Encoding Loss -0.792704701423645, Transition Loss -17.961509704589844, Classifier Loss 0.13071469962596893, Total Loss 13.067877769470215\n",
      "42: Encoding Loss -2.492173910140991, Transition Loss -17.609333038330078, Classifier Loss 0.0760016143321991, Total Loss 7.596639633178711\n",
      "42: Encoding Loss -2.7352328300476074, Transition Loss -23.632423400878906, Classifier Loss 0.041081398725509644, Total Loss 4.1034135818481445\n",
      "42: Encoding Loss -1.0225656032562256, Transition Loss -12.747518539428711, Classifier Loss 0.08598175644874573, Total Loss 8.595626831054688\n",
      "42: Encoding Loss -2.0388495922088623, Transition Loss -14.38255500793457, Classifier Loss 0.04986647143959999, Total Loss 4.983770847320557\n",
      "42: Encoding Loss -2.8405442237854004, Transition Loss -21.03964614868164, Classifier Loss 0.05248790234327316, Total Loss 5.244582176208496\n",
      "42: Encoding Loss -2.1183416843414307, Transition Loss -8.171144485473633, Classifier Loss 0.07878675311803818, Total Loss 7.877041339874268\n",
      "42: Encoding Loss -0.890730619430542, Transition Loss -17.46886444091797, Classifier Loss 0.08116228878498077, Total Loss 8.112735748291016\n",
      "42: Encoding Loss -1.5608364343643188, Transition Loss -18.281719207763672, Classifier Loss 0.023888301104307175, Total Loss 2.385173797607422\n",
      "42: Encoding Loss -2.054485321044922, Transition Loss -21.608076095581055, Classifier Loss 0.05295521393418312, Total Loss 5.291199684143066\n",
      "42: Encoding Loss -1.9324184656143188, Transition Loss -28.118593215942383, Classifier Loss 0.018012717366218567, Total Loss 1.7956479787826538\n",
      "43: Encoding Loss -1.2202141284942627, Transition Loss -6.107292175292969, Classifier Loss 0.03297891467809677, Total Loss 3.2966699600219727\n",
      "43: Encoding Loss -1.9193426370620728, Transition Loss -6.643339157104492, Classifier Loss 0.09298931062221527, Total Loss 9.297602653503418\n",
      "43: Encoding Loss -1.6206412315368652, Transition Loss -1.1538342237472534, Classifier Loss 0.03532929718494415, Total Loss 3.5326988697052\n",
      "43: Encoding Loss -1.6059383153915405, Transition Loss -20.325525283813477, Classifier Loss 0.056573983281850815, Total Loss 5.6533331871032715\n",
      "43: Encoding Loss -1.189369559288025, Transition Loss -9.892595291137695, Classifier Loss 0.04417872428894043, Total Loss 4.415894031524658\n",
      "43: Encoding Loss -1.6650032997131348, Transition Loss -16.947772979736328, Classifier Loss 0.047245170921087265, Total Loss 4.721127510070801\n",
      "43: Encoding Loss -2.3997693061828613, Transition Loss -12.241157531738281, Classifier Loss 0.0565168596804142, Total Loss 5.649238109588623\n",
      "43: Encoding Loss -1.3594642877578735, Transition Loss -9.843832015991211, Classifier Loss 0.047622837126255035, Total Loss 4.76031494140625\n",
      "43: Encoding Loss -1.237102746963501, Transition Loss -16.736976623535156, Classifier Loss 0.03611450269818306, Total Loss 3.608102798461914\n",
      "43: Encoding Loss -2.3036794662475586, Transition Loss -13.847379684448242, Classifier Loss 0.05856088548898697, Total Loss 5.85331916809082\n",
      "43: Encoding Loss -0.07041899114847183, Transition Loss -9.777469635009766, Classifier Loss 0.04861004650592804, Total Loss 4.723474502563477\n",
      "43: Encoding Loss -1.2659121751785278, Transition Loss -5.970620632171631, Classifier Loss 0.05928324535489082, Total Loss 5.927130699157715\n",
      "43: Encoding Loss -1.7526557445526123, Transition Loss -7.902650356292725, Classifier Loss 0.04401088505983353, Total Loss 4.399507999420166\n",
      "43: Encoding Loss -0.9408742785453796, Transition Loss -8.610018730163574, Classifier Loss 0.054770298302173615, Total Loss 5.475307941436768\n",
      "43: Encoding Loss -2.436633825302124, Transition Loss -16.336627960205078, Classifier Loss 0.035780854523181915, Total Loss 3.5748181343078613\n",
      "43: Encoding Loss -2.5767481327056885, Transition Loss -20.743526458740234, Classifier Loss 0.0658537745475769, Total Loss 6.581229209899902\n",
      "43: Encoding Loss -0.9440562725067139, Transition Loss -12.057201385498047, Classifier Loss 0.021670205518603325, Total Loss 2.16460919380188\n",
      "43: Encoding Loss -0.665213406085968, Transition Loss -7.780117511749268, Classifier Loss 0.04099429026246071, Total Loss 4.097873210906982\n",
      "43: Encoding Loss -0.7249759435653687, Transition Loss -15.640310287475586, Classifier Loss 0.07876082509756088, Total Loss 7.872954368591309\n",
      "43: Encoding Loss -0.6372188329696655, Transition Loss -7.330573558807373, Classifier Loss 0.050379641354084015, Total Loss 5.036498069763184\n",
      "43: Encoding Loss -1.5505287647247314, Transition Loss -17.494680404663086, Classifier Loss 0.0736415684223175, Total Loss 7.360657691955566\n",
      "43: Encoding Loss -1.611106038093567, Transition Loss -13.34549331665039, Classifier Loss 0.035864245146512985, Total Loss 3.5837554931640625\n",
      "43: Encoding Loss -1.4986461400985718, Transition Loss -14.23843002319336, Classifier Loss 0.06090853363275528, Total Loss 6.088005542755127\n",
      "43: Encoding Loss -1.4540506601333618, Transition Loss -5.662300109863281, Classifier Loss 0.07234181463718414, Total Loss 7.233048915863037\n",
      "43: Encoding Loss -2.0062685012817383, Transition Loss -13.953630447387695, Classifier Loss 0.03075440227985382, Total Loss 3.0726494789123535\n",
      "43: Encoding Loss -0.7234219908714294, Transition Loss -7.005819320678711, Classifier Loss 0.0635535940527916, Total Loss 6.353958606719971\n",
      "43: Encoding Loss -2.083195209503174, Transition Loss -5.321422100067139, Classifier Loss 0.08730731159448624, Total Loss 8.729666709899902\n",
      "43: Encoding Loss -3.103482484817505, Transition Loss -21.664257049560547, Classifier Loss 0.0651555210351944, Total Loss 6.511219024658203\n",
      "43: Encoding Loss -2.1007015705108643, Transition Loss -11.741446495056152, Classifier Loss 0.0523470863699913, Total Loss 5.232360363006592\n",
      "43: Encoding Loss -0.8979451060295105, Transition Loss -7.306130409240723, Classifier Loss 0.0597715750336647, Total Loss 5.975696563720703\n",
      "43: Encoding Loss -1.2203844785690308, Transition Loss -13.821562767028809, Classifier Loss 0.06348571181297302, Total Loss 6.345807075500488\n",
      "43: Encoding Loss -1.5593708753585815, Transition Loss -4.937512397766113, Classifier Loss 0.07520610094070435, Total Loss 7.519622325897217\n",
      "43: Encoding Loss -0.9011483788490295, Transition Loss -13.218657493591309, Classifier Loss 0.05920988321304321, Total Loss 5.918344497680664\n",
      "43: Encoding Loss -0.48946884274482727, Transition Loss -7.839651107788086, Classifier Loss 0.1582299917936325, Total Loss 15.821429252624512\n",
      "43: Encoding Loss -2.088118076324463, Transition Loss -15.205272674560547, Classifier Loss 0.03897908329963684, Total Loss 3.894867181777954\n",
      "43: Encoding Loss -1.0035195350646973, Transition Loss -9.954516410827637, Classifier Loss 0.0404534712433815, Total Loss 4.043356418609619\n",
      "43: Encoding Loss -1.02959144115448, Transition Loss -9.255515098571777, Classifier Loss 0.06413591653108597, Total Loss 6.411740779876709\n",
      "43: Encoding Loss -1.2042163610458374, Transition Loss -11.580911636352539, Classifier Loss 0.10292374342679977, Total Loss 10.290058135986328\n",
      "43: Encoding Loss -1.689694881439209, Transition Loss -12.803531646728516, Classifier Loss 0.0586232990026474, Total Loss 5.859769344329834\n",
      "43: Encoding Loss -0.9802811741828918, Transition Loss -13.827454566955566, Classifier Loss 0.059083323925733566, Total Loss 5.905566692352295\n",
      "43: Encoding Loss -1.0569348335266113, Transition Loss -10.427261352539062, Classifier Loss 0.031118877232074738, Total Loss 3.10980224609375\n",
      "43: Encoding Loss -1.610819697380066, Transition Loss -18.85879135131836, Classifier Loss 0.07314097881317139, Total Loss 7.310326099395752\n",
      "43: Encoding Loss -1.772117018699646, Transition Loss -18.00714683532715, Classifier Loss 0.029432564973831177, Total Loss 2.939655065536499\n",
      "43: Encoding Loss -2.104212999343872, Transition Loss -7.512791633605957, Classifier Loss 0.054553329944610596, Total Loss 5.453830242156982\n",
      "43: Encoding Loss -1.5529472827911377, Transition Loss -19.422515869140625, Classifier Loss 0.04143565148115158, Total Loss 4.139680862426758\n",
      "43: Encoding Loss 0.08727695792913437, Transition Loss -9.221110343933105, Classifier Loss 0.07014629244804382, Total Loss 7.577365875244141\n",
      "43: Encoding Loss 1.3322030305862427, Transition Loss -16.735172271728516, Classifier Loss 0.057546041905879974, Total Loss 16.40888023376465\n",
      "43: Encoding Loss 0.2814098298549652, Transition Loss -6.420546531677246, Classifier Loss 0.03425413742661476, Total Loss 5.669902324676514\n",
      "43: Encoding Loss -1.228913426399231, Transition Loss -12.1431884765625, Classifier Loss 0.05258845537900925, Total Loss 5.2564167976379395\n",
      "43: Encoding Loss -0.7462023496627808, Transition Loss -18.1976375579834, Classifier Loss 0.04791988432407379, Total Loss 4.78834867477417\n",
      "43: Encoding Loss -1.3340309858322144, Transition Loss -21.27218246459961, Classifier Loss 0.04387819021940231, Total Loss 4.383564472198486\n",
      "43: Encoding Loss -0.5785395503044128, Transition Loss -14.915698051452637, Classifier Loss 0.06530654430389404, Total Loss 6.5276713371276855\n",
      "43: Encoding Loss -0.9055662751197815, Transition Loss -6.60204553604126, Classifier Loss 0.07993189245462418, Total Loss 7.99186897277832\n",
      "43: Encoding Loss -0.6042290329933167, Transition Loss -14.512165069580078, Classifier Loss 0.04199722781777382, Total Loss 4.196820259094238\n",
      "43: Encoding Loss 0.06199495494365692, Transition Loss -14.138710021972656, Classifier Loss 0.07133258134126663, Total Loss 7.493649005889893\n",
      "43: Encoding Loss -1.4659919738769531, Transition Loss -5.7161712646484375, Classifier Loss 0.08016040921211243, Total Loss 8.014897346496582\n",
      "43: Encoding Loss -0.6250975728034973, Transition Loss -11.634439468383789, Classifier Loss 0.026909366250038147, Total Loss 2.6886096000671387\n",
      "43: Encoding Loss -1.860771656036377, Transition Loss -13.655969619750977, Classifier Loss 0.05232618749141693, Total Loss 5.22988748550415\n",
      "43: Encoding Loss -1.7738139629364014, Transition Loss -19.963985443115234, Classifier Loss 0.035146210342645645, Total Loss 3.5106282234191895\n",
      "43: Encoding Loss -1.3700509071350098, Transition Loss -17.058115005493164, Classifier Loss 0.06007513403892517, Total Loss 6.004101753234863\n",
      "43: Encoding Loss -1.2957470417022705, Transition Loss -9.928041458129883, Classifier Loss 0.05637889355421066, Total Loss 5.635903835296631\n",
      "43: Encoding Loss -1.6015808582305908, Transition Loss -8.621172904968262, Classifier Loss 0.09113116562366486, Total Loss 9.1113920211792\n",
      "43: Encoding Loss -1.506841778755188, Transition Loss -10.541893005371094, Classifier Loss 0.04922734946012497, Total Loss 4.920626163482666\n",
      "43: Encoding Loss -1.275829792022705, Transition Loss -7.740453720092773, Classifier Loss 0.06556982547044754, Total Loss 6.555434226989746\n",
      "43: Encoding Loss -1.1103711128234863, Transition Loss -12.00644588470459, Classifier Loss 0.061251200735569, Total Loss 6.122718811035156\n",
      "43: Encoding Loss -1.8619234561920166, Transition Loss -5.177430629730225, Classifier Loss 0.09677210450172424, Total Loss 9.676175117492676\n",
      "43: Encoding Loss -0.2971096336841583, Transition Loss -1.7038871049880981, Classifier Loss 0.05315602570772171, Total Loss 5.311735153198242\n",
      "43: Encoding Loss -1.0954381227493286, Transition Loss -19.29327392578125, Classifier Loss 0.07776804268360138, Total Loss 7.772945880889893\n",
      "43: Encoding Loss -0.2242402732372284, Transition Loss -9.538936614990234, Classifier Loss 0.06368203461170197, Total Loss 6.343929767608643\n",
      "43: Encoding Loss -0.594720184803009, Transition Loss -9.337167739868164, Classifier Loss 0.10740435868501663, Total Loss 10.738568305969238\n",
      "43: Encoding Loss -1.1106126308441162, Transition Loss -8.960776329040527, Classifier Loss 0.0680619403719902, Total Loss 6.804401874542236\n",
      "43: Encoding Loss -1.0499310493469238, Transition Loss -9.202709197998047, Classifier Loss 0.06902444362640381, Total Loss 6.900603771209717\n",
      "43: Encoding Loss -0.492024302482605, Transition Loss -5.903075218200684, Classifier Loss 0.08608710765838623, Total Loss 8.607528686523438\n",
      "43: Encoding Loss -1.1584774255752563, Transition Loss -9.609749794006348, Classifier Loss 0.020963750779628754, Total Loss 2.0944530963897705\n",
      "43: Encoding Loss -1.9064141511917114, Transition Loss -8.607925415039062, Classifier Loss 0.087444007396698, Total Loss 8.742679595947266\n",
      "43: Encoding Loss -1.3830591440200806, Transition Loss -15.287456512451172, Classifier Loss 0.033599112182855606, Total Loss 3.356853723526001\n",
      "43: Encoding Loss -0.6764323115348816, Transition Loss -6.7731428146362305, Classifier Loss 0.0706380307674408, Total Loss 7.062448501586914\n",
      "43: Encoding Loss -2.2758567333221436, Transition Loss -13.151611328125, Classifier Loss 0.05478588119149208, Total Loss 5.475957870483398\n",
      "43: Encoding Loss -1.1280158758163452, Transition Loss -14.264622688293457, Classifier Loss 0.02460862323641777, Total Loss 2.4580094814300537\n",
      "43: Encoding Loss -1.0862305164337158, Transition Loss -10.673877716064453, Classifier Loss 0.09836220741271973, Total Loss 9.834086418151855\n",
      "43: Encoding Loss -0.6709354519844055, Transition Loss -11.909010887145996, Classifier Loss 0.048720583319664, Total Loss 4.86967658996582\n",
      "43: Encoding Loss -1.021365761756897, Transition Loss -9.394026756286621, Classifier Loss 0.042843371629714966, Total Loss 4.282458305358887\n",
      "43: Encoding Loss -0.981471598148346, Transition Loss -5.992175102233887, Classifier Loss 0.09212009608745575, Total Loss 9.210810661315918\n",
      "43: Encoding Loss -0.3028310537338257, Transition Loss -11.82392692565918, Classifier Loss 0.07501140981912613, Total Loss 7.495797634124756\n",
      "43: Encoding Loss -2.2658767700195312, Transition Loss -8.201057434082031, Classifier Loss 0.05183834582567215, Total Loss 5.182194232940674\n",
      "43: Encoding Loss -1.4264872074127197, Transition Loss -6.211086273193359, Classifier Loss 0.06456207484006882, Total Loss 6.454965114593506\n",
      "43: Encoding Loss -1.7967588901519775, Transition Loss -16.440359115600586, Classifier Loss 0.05916530266404152, Total Loss 5.913241863250732\n",
      "43: Encoding Loss -1.0865859985351562, Transition Loss -10.297294616699219, Classifier Loss 0.0974830687046051, Total Loss 9.746248245239258\n",
      "43: Encoding Loss -1.3889743089675903, Transition Loss -8.392803192138672, Classifier Loss 0.047791942954063416, Total Loss 4.777515888214111\n",
      "43: Encoding Loss -0.605181872844696, Transition Loss -14.362497329711914, Classifier Loss 0.05097360908985138, Total Loss 5.094488620758057\n",
      "43: Encoding Loss -2.515991449356079, Transition Loss -22.622446060180664, Classifier Loss 0.0464298278093338, Total Loss 4.638458251953125\n",
      "43: Encoding Loss -0.89903324842453, Transition Loss -7.483532905578613, Classifier Loss 0.07756761461496353, Total Loss 7.755264759063721\n",
      "43: Encoding Loss -0.7238107323646545, Transition Loss -8.144363403320312, Classifier Loss 0.06942735612392426, Total Loss 6.941106796264648\n",
      "43: Encoding Loss -0.7033191919326782, Transition Loss -10.248360633850098, Classifier Loss 0.05982852727174759, Total Loss 5.980803489685059\n",
      "43: Encoding Loss -1.1129491329193115, Transition Loss -4.494399070739746, Classifier Loss 0.06883552670478821, Total Loss 6.882653713226318\n",
      "43: Encoding Loss -1.4464610815048218, Transition Loss -13.93919849395752, Classifier Loss 0.06770174950361252, Total Loss 6.7673869132995605\n",
      "43: Encoding Loss -1.4178638458251953, Transition Loss -16.88264274597168, Classifier Loss 0.06795639544725418, Total Loss 6.792263031005859\n",
      "43: Encoding Loss -1.6100555658340454, Transition Loss -13.7337646484375, Classifier Loss 0.049190204590559006, Total Loss 4.916274070739746\n",
      "43: Encoding Loss -0.944374680519104, Transition Loss -4.876128196716309, Classifier Loss 0.03228944540023804, Total Loss 3.2279694080352783\n",
      "43: Encoding Loss -1.2045021057128906, Transition Loss -7.8317975997924805, Classifier Loss 0.09570088982582092, Total Loss 9.568523406982422\n",
      "43: Encoding Loss -1.6460407972335815, Transition Loss -15.422721862792969, Classifier Loss 0.05748072266578674, Total Loss 5.744987487792969\n",
      "43: Encoding Loss -0.4706111550331116, Transition Loss -0.2836400270462036, Classifier Loss 0.09900981187820435, Total Loss 9.900919914245605\n",
      "43: Encoding Loss -1.0874271392822266, Transition Loss -8.938766479492188, Classifier Loss 0.06680795550346375, Total Loss 6.6790080070495605\n",
      "43: Encoding Loss -0.18875284492969513, Transition Loss -5.257328987121582, Classifier Loss 0.07251470535993576, Total Loss 7.205806255340576\n",
      "43: Encoding Loss -0.7492585778236389, Transition Loss -12.55221939086914, Classifier Loss 0.04636364430189133, Total Loss 4.633853912353516\n",
      "43: Encoding Loss -1.5218360424041748, Transition Loss -16.458282470703125, Classifier Loss 0.07790154218673706, Total Loss 7.786862850189209\n",
      "43: Encoding Loss -0.7377461194992065, Transition Loss -5.208958625793457, Classifier Loss 0.07323359698057175, Total Loss 7.322317600250244\n",
      "43: Encoding Loss -0.3424292206764221, Transition Loss -9.559062957763672, Classifier Loss 0.040451452136039734, Total Loss 4.042388916015625\n",
      "43: Encoding Loss -0.94944828748703, Transition Loss -9.591599464416504, Classifier Loss 0.050007909536361694, Total Loss 4.998872756958008\n",
      "43: Encoding Loss -0.29893654584884644, Transition Loss -15.047179222106934, Classifier Loss 0.05270323157310486, Total Loss 5.263971328735352\n",
      "43: Encoding Loss -0.6436498761177063, Transition Loss -14.617815971374512, Classifier Loss 0.064315065741539, Total Loss 6.428583145141602\n",
      "43: Encoding Loss -0.30471071600914, Transition Loss -2.165060520172119, Classifier Loss 0.03927692025899887, Total Loss 3.924442768096924\n",
      "43: Encoding Loss -0.3089257478713989, Transition Loss -18.66145133972168, Classifier Loss 0.03717421740293503, Total Loss 3.711210012435913\n",
      "43: Encoding Loss -1.5179227590560913, Transition Loss -17.964885711669922, Classifier Loss 0.03214293718338013, Total Loss 3.210700750350952\n",
      "43: Encoding Loss -0.32698509097099304, Transition Loss -1.331397294998169, Classifier Loss 0.05502162501215935, Total Loss 5.500488758087158\n",
      "43: Encoding Loss -0.19984674453735352, Transition Loss 1.496848464012146, Classifier Loss 0.0561031736433506, Total Loss 5.87318229675293\n",
      "43: Encoding Loss 0.36163273453712463, Transition Loss -12.35385513305664, Classifier Loss 0.03846196085214615, Total Loss 6.736354827880859\n",
      "43: Encoding Loss 0.2385905683040619, Transition Loss -13.50672435760498, Classifier Loss 0.04636998847126961, Total Loss 6.52676248550415\n",
      "43: Encoding Loss -0.42703646421432495, Transition Loss -12.562692642211914, Classifier Loss 0.05540056526660919, Total Loss 5.537510871887207\n",
      "43: Encoding Loss 0.6237688660621643, Transition Loss -6.094468116760254, Classifier Loss 0.057983748614788055, Total Loss 10.787306785583496\n",
      "43: Encoding Loss -2.13025164604187, Transition Loss -16.75420379638672, Classifier Loss 0.039311181753873825, Total Loss 3.927767515182495\n",
      "43: Encoding Loss -0.981781542301178, Transition Loss -10.814180374145508, Classifier Loss 0.06650993227958679, Total Loss 6.648830413818359\n",
      "43: Encoding Loss -1.221123218536377, Transition Loss -13.855426788330078, Classifier Loss 0.08611268550157547, Total Loss 8.608497619628906\n",
      "43: Encoding Loss -2.2486135959625244, Transition Loss -21.10661506652832, Classifier Loss 0.05270170047879219, Total Loss 5.26594877243042\n",
      "43: Encoding Loss -1.5046041011810303, Transition Loss -10.422229766845703, Classifier Loss 0.0666382759809494, Total Loss 6.6617431640625\n",
      "43: Encoding Loss -0.9953565001487732, Transition Loss -11.643790245056152, Classifier Loss 0.09562662988901138, Total Loss 9.560334205627441\n",
      "43: Encoding Loss -1.4434973001480103, Transition Loss -17.376323699951172, Classifier Loss 0.07666484266519547, Total Loss 7.663009166717529\n",
      "43: Encoding Loss -1.2687277793884277, Transition Loss -14.368948936462402, Classifier Loss 0.02559940330684185, Total Loss 2.5570664405822754\n",
      "43: Encoding Loss -0.2892035245895386, Transition Loss -12.97125244140625, Classifier Loss 0.053995195776224136, Total Loss 5.3924970626831055\n",
      "43: Encoding Loss -0.4350258708000183, Transition Loss -6.783662796020508, Classifier Loss 0.04209843650460243, Total Loss 4.208463191986084\n",
      "43: Encoding Loss 0.28651857376098633, Transition Loss -10.228103637695312, Classifier Loss 0.060236454010009766, Total Loss 8.308972358703613\n",
      "43: Encoding Loss -0.8543853163719177, Transition Loss -10.42199993133545, Classifier Loss 0.05724962428212166, Total Loss 5.7228779792785645\n",
      "43: Encoding Loss -0.8961495161056519, Transition Loss -6.176605224609375, Classifier Loss 0.03743431717157364, Total Loss 3.742196559906006\n",
      "43: Encoding Loss -0.8902810215950012, Transition Loss -11.355098724365234, Classifier Loss 0.03258958086371422, Total Loss 3.2566871643066406\n",
      "43: Encoding Loss -0.22326096892356873, Transition Loss -13.525677680969238, Classifier Loss 0.06087472289800644, Total Loss 6.061927795410156\n",
      "43: Encoding Loss 0.23909015953540802, Transition Loss -13.235708236694336, Classifier Loss 0.05751332268118858, Total Loss 7.6453328132629395\n",
      "43: Encoding Loss -1.4031916856765747, Transition Loss -2.087609052658081, Classifier Loss 0.08482489734888077, Total Loss 8.482071876525879\n",
      "43: Encoding Loss -0.9773546457290649, Transition Loss -10.09421157836914, Classifier Loss 0.03363436087965965, Total Loss 3.361417055130005\n",
      "43: Encoding Loss -0.414184033870697, Transition Loss -12.99437427520752, Classifier Loss 0.06773994863033295, Total Loss 6.771338939666748\n",
      "43: Encoding Loss -0.5502265691757202, Transition Loss -10.900834083557129, Classifier Loss 0.051302388310432434, Total Loss 5.128058910369873\n",
      "43: Encoding Loss -1.6560183763504028, Transition Loss -10.837718963623047, Classifier Loss 0.04839303344488144, Total Loss 4.8371357917785645\n",
      "43: Encoding Loss -2.445730686187744, Transition Loss -6.737536907196045, Classifier Loss 0.07356388866901398, Total Loss 7.35504150390625\n",
      "43: Encoding Loss -2.4997055530548096, Transition Loss -15.746248245239258, Classifier Loss 0.034408748149871826, Total Loss 3.43772554397583\n",
      "43: Encoding Loss -0.32544928789138794, Transition Loss -3.1406760215759277, Classifier Loss 0.05098670721054077, Total Loss 5.096564292907715\n",
      "43: Encoding Loss -1.1060079336166382, Transition Loss -9.321500778198242, Classifier Loss 0.03196409344673157, Total Loss 3.194545030593872\n",
      "43: Encoding Loss -0.5906253457069397, Transition Loss -8.94885540008545, Classifier Loss 0.08035161346197128, Total Loss 8.033370971679688\n",
      "43: Encoding Loss -0.19489847123622894, Transition Loss -17.466920852661133, Classifier Loss 0.13743048906326294, Total Loss 13.699563980102539\n",
      "43: Encoding Loss -2.498640537261963, Transition Loss -17.15749740600586, Classifier Loss 0.07012341171503067, Total Loss 7.008909702301025\n",
      "43: Encoding Loss -2.716447114944458, Transition Loss -22.984220504760742, Classifier Loss 0.03944588080048561, Total Loss 3.9399912357330322\n",
      "43: Encoding Loss -0.7525658011436462, Transition Loss -12.679061889648438, Classifier Loss 0.09122534096240997, Total Loss 9.11999797821045\n",
      "43: Encoding Loss -2.1715950965881348, Transition Loss -14.342702865600586, Classifier Loss 0.048402536660432816, Total Loss 4.837385177612305\n",
      "43: Encoding Loss -2.7014405727386475, Transition Loss -20.75039291381836, Classifier Loss 0.05144023150205612, Total Loss 5.139873027801514\n",
      "43: Encoding Loss -2.062623977661133, Transition Loss -8.59685230255127, Classifier Loss 0.08175630867481232, Total Loss 8.173911094665527\n",
      "43: Encoding Loss -1.1465694904327393, Transition Loss -17.320880889892578, Classifier Loss 0.07788991183042526, Total Loss 7.785526752471924\n",
      "43: Encoding Loss -1.854576587677002, Transition Loss -18.072206497192383, Classifier Loss 0.02102169580757618, Total Loss 2.098555088043213\n",
      "43: Encoding Loss -1.6655083894729614, Transition Loss -21.232894897460938, Classifier Loss 0.05524832755327225, Total Loss 5.520586013793945\n",
      "43: Encoding Loss -2.2019340991973877, Transition Loss -27.528642654418945, Classifier Loss 0.017198661342263222, Total Loss 1.7143604755401611\n",
      "44: Encoding Loss -1.3837188482284546, Transition Loss -6.643892288208008, Classifier Loss 0.031677138060331345, Total Loss 3.1663851737976074\n",
      "44: Encoding Loss -1.6668145656585693, Transition Loss -7.294201850891113, Classifier Loss 0.09224581718444824, Total Loss 9.223122596740723\n",
      "44: Encoding Loss -1.7863270044326782, Transition Loss -2.2974720001220703, Classifier Loss 0.03456493094563484, Total Loss 3.456033706665039\n",
      "44: Encoding Loss -1.4801167249679565, Transition Loss -20.13743019104004, Classifier Loss 0.054250311106443405, Total Loss 5.421003818511963\n",
      "44: Encoding Loss -1.6254292726516724, Transition Loss -10.421852111816406, Classifier Loss 0.043935924768447876, Total Loss 4.391508102416992\n",
      "44: Encoding Loss -1.7985650300979614, Transition Loss -17.040075302124023, Classifier Loss 0.04488207772374153, Total Loss 4.484799861907959\n",
      "44: Encoding Loss -2.2473676204681396, Transition Loss -12.778301239013672, Classifier Loss 0.05501403659582138, Total Loss 5.498847961425781\n",
      "44: Encoding Loss -1.683120846748352, Transition Loss -10.30750846862793, Classifier Loss 0.046541836112737656, Total Loss 4.6521220207214355\n",
      "44: Encoding Loss -0.9822921752929688, Transition Loss -16.963298797607422, Classifier Loss 0.03524191305041313, Total Loss 3.520798683166504\n",
      "44: Encoding Loss -2.1938321590423584, Transition Loss -14.1788969039917, Classifier Loss 0.05807303637266159, Total Loss 5.804467678070068\n",
      "44: Encoding Loss 0.06842069327831268, Transition Loss -10.412817001342773, Classifier Loss 0.0475839339196682, Total Loss 5.168519973754883\n",
      "44: Encoding Loss -0.14206047356128693, Transition Loss -4.089620590209961, Classifier Loss 0.05916785076260567, Total Loss 5.8276448249816895\n",
      "44: Encoding Loss -1.2642903327941895, Transition Loss -5.594467639923096, Classifier Loss 0.0441877618432045, Total Loss 4.417657375335693\n",
      "44: Encoding Loss -0.35345250368118286, Transition Loss -6.814548969268799, Classifier Loss 0.05160173773765564, Total Loss 5.158233642578125\n",
      "44: Encoding Loss -1.5294666290283203, Transition Loss -13.62804889678955, Classifier Loss 0.03573039174079895, Total Loss 3.5703134536743164\n",
      "44: Encoding Loss -2.261975049972534, Transition Loss -17.811140060424805, Classifier Loss 0.0716027095913887, Total Loss 7.156708717346191\n",
      "44: Encoding Loss -1.1714398860931396, Transition Loss -9.816632270812988, Classifier Loss 0.020332099869847298, Total Loss 2.0312466621398926\n",
      "44: Encoding Loss -0.4945501685142517, Transition Loss -6.106040954589844, Classifier Loss 0.03856810927391052, Total Loss 3.855588436126709\n",
      "44: Encoding Loss -0.2861824333667755, Transition Loss -13.302508354187012, Classifier Loss 0.07516610622406006, Total Loss 7.509128570556641\n",
      "44: Encoding Loss -0.5471576452255249, Transition Loss -5.217581748962402, Classifier Loss 0.04640674963593483, Total Loss 4.639631748199463\n",
      "44: Encoding Loss -0.8536228537559509, Transition Loss -15.21275520324707, Classifier Loss 0.07598510384559631, Total Loss 7.595467567443848\n",
      "44: Encoding Loss -1.4472479820251465, Transition Loss -11.23577880859375, Classifier Loss 0.03441961109638214, Total Loss 3.439713954925537\n",
      "44: Encoding Loss -1.4628745317459106, Transition Loss -11.68879508972168, Classifier Loss 0.0615357905626297, Total Loss 6.151241302490234\n",
      "44: Encoding Loss -0.7246614098548889, Transition Loss -3.5013375282287598, Classifier Loss 0.07589337229728699, Total Loss 7.588636875152588\n",
      "44: Encoding Loss -2.076934337615967, Transition Loss -11.783225059509277, Classifier Loss 0.030515437945723534, Total Loss 3.049187183380127\n",
      "44: Encoding Loss -0.5140308737754822, Transition Loss -5.331212997436523, Classifier Loss 0.07137840241193771, Total Loss 7.136773586273193\n",
      "44: Encoding Loss -2.019904375076294, Transition Loss -3.5893421173095703, Classifier Loss 0.08911668509244919, Total Loss 8.910950660705566\n",
      "44: Encoding Loss -2.3227157592773438, Transition Loss -18.93269920349121, Classifier Loss 0.06572800874710083, Total Loss 6.569014072418213\n",
      "44: Encoding Loss -1.6762547492980957, Transition Loss -9.378825187683105, Classifier Loss 0.052326589822769165, Total Loss 5.230782985687256\n",
      "44: Encoding Loss -0.6989783644676208, Transition Loss -5.252052307128906, Classifier Loss 0.059105999767780304, Total Loss 5.909549713134766\n",
      "44: Encoding Loss -0.9912556409835815, Transition Loss -11.862300872802734, Classifier Loss 0.06256178021430969, Total Loss 6.253805637359619\n",
      "44: Encoding Loss -1.0015891790390015, Transition Loss -3.094794988632202, Classifier Loss 0.07288999110460281, Total Loss 7.288380146026611\n",
      "44: Encoding Loss -0.7621343731880188, Transition Loss -11.536338806152344, Classifier Loss 0.06047753244638443, Total Loss 6.045445919036865\n",
      "44: Encoding Loss 0.2875291705131531, Transition Loss -5.821380138397217, Classifier Loss 0.16820979118347168, Total Loss 19.115407943725586\n",
      "44: Encoding Loss -2.2603728771209717, Transition Loss -15.344995498657227, Classifier Loss 0.038066547363996506, Total Loss 3.8035857677459717\n",
      "44: Encoding Loss -1.2056667804718018, Transition Loss -10.458683013916016, Classifier Loss 0.04330811649560928, Total Loss 4.328719615936279\n",
      "44: Encoding Loss -1.426429033279419, Transition Loss -9.659430503845215, Classifier Loss 0.06607462465763092, Total Loss 6.605530738830566\n",
      "44: Encoding Loss -1.2981343269348145, Transition Loss -11.839964866638184, Classifier Loss 0.10501489788293839, Total Loss 10.49912166595459\n",
      "44: Encoding Loss -1.969523310661316, Transition Loss -12.982948303222656, Classifier Loss 0.05640260875225067, Total Loss 5.637664318084717\n",
      "44: Encoding Loss -1.4093455076217651, Transition Loss -13.760310173034668, Classifier Loss 0.058079592883586884, Total Loss 5.805207252502441\n",
      "44: Encoding Loss -1.372429370880127, Transition Loss -10.646210670471191, Classifier Loss 0.03159865364432335, Total Loss 3.15773606300354\n",
      "44: Encoding Loss -1.8456904888153076, Transition Loss -18.559921264648438, Classifier Loss 0.07023712992668152, Total Loss 7.02000093460083\n",
      "44: Encoding Loss -2.3083505630493164, Transition Loss -17.802764892578125, Classifier Loss 0.03246504068374634, Total Loss 3.242943525314331\n",
      "44: Encoding Loss -2.4507837295532227, Transition Loss -7.915914535522461, Classifier Loss 0.05403105914592743, Total Loss 5.401522636413574\n",
      "44: Encoding Loss -1.70741605758667, Transition Loss -19.09954071044922, Classifier Loss 0.04189877584576607, Total Loss 4.1860575675964355\n",
      "44: Encoding Loss -0.17587457597255707, Transition Loss -9.537814140319824, Classifier Loss 0.07017850875854492, Total Loss 6.960634231567383\n",
      "44: Encoding Loss -1.0994888544082642, Transition Loss -19.44475555419922, Classifier Loss 0.05830749496817589, Total Loss 5.826860427856445\n",
      "44: Encoding Loss -2.2931783199310303, Transition Loss -8.248605728149414, Classifier Loss 0.031111711636185646, Total Loss 3.1095216274261475\n",
      "44: Encoding Loss -2.141662836074829, Transition Loss -11.346367835998535, Classifier Loss 0.04972422868013382, Total Loss 4.97015380859375\n",
      "44: Encoding Loss -1.6683006286621094, Transition Loss -18.042802810668945, Classifier Loss 0.046929433941841125, Total Loss 4.689334869384766\n",
      "44: Encoding Loss -2.2813427448272705, Transition Loss -21.58603286743164, Classifier Loss 0.04427440091967583, Total Loss 4.423122882843018\n",
      "44: Encoding Loss -1.253199815750122, Transition Loss -14.51616096496582, Classifier Loss 0.061894264072179794, Total Loss 6.186522960662842\n",
      "44: Encoding Loss -1.5921484231948853, Transition Loss -5.3576459884643555, Classifier Loss 0.07789766043424606, Total Loss 7.788694381713867\n",
      "44: Encoding Loss -1.6208992004394531, Transition Loss -13.936110496520996, Classifier Loss 0.03927445039153099, Total Loss 3.9246578216552734\n",
      "44: Encoding Loss -1.05846107006073, Transition Loss -13.775738716125488, Classifier Loss 0.06835758686065674, Total Loss 6.833003520965576\n",
      "44: Encoding Loss -1.9585282802581787, Transition Loss -6.965584754943848, Classifier Loss 0.07779747992753983, Total Loss 7.778354644775391\n",
      "44: Encoding Loss -0.7216633558273315, Transition Loss -12.806291580200195, Classifier Loss 0.02912365272641182, Total Loss 2.909803867340088\n",
      "44: Encoding Loss -2.2372026443481445, Transition Loss -14.675350189208984, Classifier Loss 0.0484687015414238, Total Loss 4.843935012817383\n",
      "44: Encoding Loss -2.5400912761688232, Transition Loss -20.953977584838867, Classifier Loss 0.03767302259802818, Total Loss 3.7631115913391113\n",
      "44: Encoding Loss -2.016090154647827, Transition Loss -18.0898494720459, Classifier Loss 0.05785808339715004, Total Loss 5.782190799713135\n",
      "44: Encoding Loss -1.6855288743972778, Transition Loss -10.953714370727539, Classifier Loss 0.05706562101840973, Total Loss 5.704371452331543\n",
      "44: Encoding Loss -2.3322770595550537, Transition Loss -9.7618989944458, Classifier Loss 0.08899801969528198, Total Loss 8.897850036621094\n",
      "44: Encoding Loss -2.4055984020233154, Transition Loss -11.527749061584473, Classifier Loss 0.048448238521814346, Total Loss 4.842518329620361\n",
      "44: Encoding Loss -1.136144757270813, Transition Loss -8.793391227722168, Classifier Loss 0.06426842510700226, Total Loss 6.425084114074707\n",
      "44: Encoding Loss -1.6069674491882324, Transition Loss -13.252740859985352, Classifier Loss 0.05830708518624306, Total Loss 5.828057765960693\n",
      "44: Encoding Loss -2.3606505393981934, Transition Loss -6.356000900268555, Classifier Loss 0.09381638467311859, Total Loss 9.380367279052734\n",
      "44: Encoding Loss -0.9850839376449585, Transition Loss -3.113588333129883, Classifier Loss 0.05365321785211563, Total Loss 5.364698886871338\n",
      "44: Encoding Loss -2.314854621887207, Transition Loss -19.729671478271484, Classifier Loss 0.06719163060188293, Total Loss 6.715217113494873\n",
      "44: Encoding Loss -0.4998392164707184, Transition Loss -10.333649635314941, Classifier Loss 0.06514297425746918, Total Loss 6.512229919433594\n",
      "44: Encoding Loss -0.9233725666999817, Transition Loss -10.431901931762695, Classifier Loss 0.09988515824079514, Total Loss 9.986429214477539\n",
      "44: Encoding Loss -2.1845574378967285, Transition Loss -10.117756843566895, Classifier Loss 0.0656968429684639, Total Loss 6.567660808563232\n",
      "44: Encoding Loss -1.546377182006836, Transition Loss -10.221367835998535, Classifier Loss 0.06622497737407684, Total Loss 6.620453357696533\n",
      "44: Encoding Loss -1.0186086893081665, Transition Loss -6.971108436584473, Classifier Loss 0.08554356545209885, Total Loss 8.552962303161621\n",
      "44: Encoding Loss -1.6410479545593262, Transition Loss -10.500415802001953, Classifier Loss 0.021574145182967186, Total Loss 2.1553144454956055\n",
      "44: Encoding Loss -2.325408458709717, Transition Loss -9.393291473388672, Classifier Loss 0.0877678394317627, Total Loss 8.77490520477295\n",
      "44: Encoding Loss -1.582197666168213, Transition Loss -15.777494430541992, Classifier Loss 0.0330154225230217, Total Loss 3.298386812210083\n",
      "44: Encoding Loss -1.861085295677185, Transition Loss -8.210363388061523, Classifier Loss 0.06980359554290771, Total Loss 6.97871732711792\n",
      "44: Encoding Loss -2.759934425354004, Transition Loss -13.926773071289062, Classifier Loss 0.05487360060214996, Total Loss 5.484574794769287\n",
      "44: Encoding Loss -1.881429672241211, Transition Loss -15.025138854980469, Classifier Loss 0.026180949062108994, Total Loss 2.6150898933410645\n",
      "44: Encoding Loss -2.1936533451080322, Transition Loss -11.379876136779785, Classifier Loss 0.0944715142250061, Total Loss 9.44487476348877\n",
      "44: Encoding Loss -1.2568168640136719, Transition Loss -12.501873016357422, Classifier Loss 0.048432931303977966, Total Loss 4.840792655944824\n",
      "44: Encoding Loss -2.208562135696411, Transition Loss -10.216790199279785, Classifier Loss 0.04304547235369682, Total Loss 4.302504062652588\n",
      "44: Encoding Loss -1.5361608266830444, Transition Loss -6.874650955200195, Classifier Loss 0.09199831634759903, Total Loss 9.198456764221191\n",
      "44: Encoding Loss -0.8863935470581055, Transition Loss -12.235692024230957, Classifier Loss 0.07534658908843994, Total Loss 7.532211780548096\n",
      "44: Encoding Loss -2.8358724117279053, Transition Loss -9.42313003540039, Classifier Loss 0.050697725266218185, Total Loss 5.067888259887695\n",
      "44: Encoding Loss -1.480942964553833, Transition Loss -7.312187194824219, Classifier Loss 0.06664078682661057, Total Loss 6.66261625289917\n",
      "44: Encoding Loss -2.110032081604004, Transition Loss -17.182544708251953, Classifier Loss 0.05973818898200989, Total Loss 5.970382213592529\n",
      "44: Encoding Loss -1.6092242002487183, Transition Loss -11.173096656799316, Classifier Loss 0.10070165991783142, Total Loss 10.067931175231934\n",
      "44: Encoding Loss -0.7672547698020935, Transition Loss -9.188339233398438, Classifier Loss 0.04784078896045685, Total Loss 4.782241344451904\n",
      "44: Encoding Loss -1.3023384809494019, Transition Loss -14.67182731628418, Classifier Loss 0.05190934240818024, Total Loss 5.187999725341797\n",
      "44: Encoding Loss -3.7015044689178467, Transition Loss -23.140676498413086, Classifier Loss 0.04641727730631828, Total Loss 4.637099742889404\n",
      "44: Encoding Loss -1.3976513147354126, Transition Loss -8.243550300598145, Classifier Loss 0.07840720564126968, Total Loss 7.839071750640869\n",
      "44: Encoding Loss -1.8645570278167725, Transition Loss -9.313069343566895, Classifier Loss 0.06886130571365356, Total Loss 6.884267807006836\n",
      "44: Encoding Loss -0.9080647230148315, Transition Loss -11.24077033996582, Classifier Loss 0.06060852110385895, Total Loss 6.058603763580322\n",
      "44: Encoding Loss -1.4699130058288574, Transition Loss -5.463380813598633, Classifier Loss 0.06745151430368423, Total Loss 6.744058609008789\n",
      "44: Encoding Loss -2.0823922157287598, Transition Loss -14.401741027832031, Classifier Loss 0.06742025911808014, Total Loss 6.739145278930664\n",
      "44: Encoding Loss -1.4460469484329224, Transition Loss -17.43977928161621, Classifier Loss 0.06821075081825256, Total Loss 6.817586898803711\n",
      "44: Encoding Loss -1.9968180656433105, Transition Loss -14.637306213378906, Classifier Loss 0.04895151033997536, Total Loss 4.892223834991455\n",
      "44: Encoding Loss -1.3124136924743652, Transition Loss -5.898978233337402, Classifier Loss 0.03243739902973175, Total Loss 3.2425601482391357\n",
      "44: Encoding Loss -2.388280153274536, Transition Loss -8.650814056396484, Classifier Loss 0.09567420184612274, Total Loss 9.565690040588379\n",
      "44: Encoding Loss -2.6324031352996826, Transition Loss -15.416913032531738, Classifier Loss 0.05642835050821304, Total Loss 5.63975191116333\n",
      "44: Encoding Loss -1.0924824476242065, Transition Loss -1.039052963256836, Classifier Loss 0.09820490330457687, Total Loss 9.820281982421875\n",
      "44: Encoding Loss -1.0495704412460327, Transition Loss -9.40339183807373, Classifier Loss 0.0657612606883049, Total Loss 6.574245452880859\n",
      "44: Encoding Loss -0.6212462782859802, Transition Loss -5.656079292297363, Classifier Loss 0.07188013941049576, Total Loss 7.186882972717285\n",
      "44: Encoding Loss -1.3064531087875366, Transition Loss -12.673511505126953, Classifier Loss 0.04569970816373825, Total Loss 4.5674357414245605\n",
      "44: Encoding Loss -1.995727300643921, Transition Loss -16.675077438354492, Classifier Loss 0.07520370930433273, Total Loss 7.517035961151123\n",
      "44: Encoding Loss -1.4601266384124756, Transition Loss -5.850430965423584, Classifier Loss 0.07273392379283905, Total Loss 7.27222204208374\n",
      "44: Encoding Loss -1.7326852083206177, Transition Loss -10.21037483215332, Classifier Loss 0.04067389294505119, Total Loss 4.065347194671631\n",
      "44: Encoding Loss -2.019482374191284, Transition Loss -9.947344779968262, Classifier Loss 0.04840672016143799, Total Loss 4.838682651519775\n",
      "44: Encoding Loss -1.0420936346054077, Transition Loss -15.280117988586426, Classifier Loss 0.05221749097108841, Total Loss 5.218693256378174\n",
      "44: Encoding Loss -1.5111721754074097, Transition Loss -14.823444366455078, Classifier Loss 0.06444002687931061, Total Loss 6.441038131713867\n",
      "44: Encoding Loss -1.2945698499679565, Transition Loss -2.639570474624634, Classifier Loss 0.03727201744914055, Total Loss 3.7266738414764404\n",
      "44: Encoding Loss -2.0442330837249756, Transition Loss -18.716022491455078, Classifier Loss 0.03690902143716812, Total Loss 3.6871590614318848\n",
      "44: Encoding Loss -2.172227621078491, Transition Loss -17.865680694580078, Classifier Loss 0.03253127261996269, Total Loss 3.24955415725708\n",
      "44: Encoding Loss -0.8368597030639648, Transition Loss -2.120271682739258, Classifier Loss 0.05337471142411232, Total Loss 5.337047100067139\n",
      "44: Encoding Loss -0.7883477807044983, Transition Loss 0.9018039703369141, Classifier Loss 0.05400368571281433, Total Loss 5.5807294845581055\n",
      "44: Encoding Loss -1.297164797782898, Transition Loss -12.896434783935547, Classifier Loss 0.03760535269975662, Total Loss 3.757956027984619\n",
      "44: Encoding Loss -1.4367804527282715, Transition Loss -11.977444648742676, Classifier Loss 0.045652780681848526, Total Loss 4.562882423400879\n",
      "44: Encoding Loss -2.5826375484466553, Transition Loss -14.09083080291748, Classifier Loss 0.0518413707613945, Total Loss 5.181318759918213\n",
      "44: Encoding Loss -1.4520888328552246, Transition Loss -7.645094394683838, Classifier Loss 0.056613970547914505, Total Loss 5.659867763519287\n",
      "44: Encoding Loss -2.9365720748901367, Transition Loss -16.228641510009766, Classifier Loss 0.03951768949627876, Total Loss 3.9485230445861816\n",
      "44: Encoding Loss -1.679573893547058, Transition Loss -10.008630752563477, Classifier Loss 0.0668216273188591, Total Loss 6.680160999298096\n",
      "44: Encoding Loss -1.629931092262268, Transition Loss -13.240729331970215, Classifier Loss 0.08972562849521637, Total Loss 8.969914436340332\n",
      "44: Encoding Loss -2.8517422676086426, Transition Loss -20.898283004760742, Classifier Loss 0.05103619396686554, Total Loss 5.099440097808838\n",
      "44: Encoding Loss -2.430105686187744, Transition Loss -9.521047592163086, Classifier Loss 0.06426713615655899, Total Loss 6.424809455871582\n",
      "44: Encoding Loss -1.8316619396209717, Transition Loss -11.094388961791992, Classifier Loss 0.09427468478679657, Total Loss 9.425249099731445\n",
      "44: Encoding Loss -2.171132802963257, Transition Loss -17.051753997802734, Classifier Loss 0.07395973801612854, Total Loss 7.392563343048096\n",
      "44: Encoding Loss -2.237919569015503, Transition Loss -13.81686019897461, Classifier Loss 0.025864936411380768, Total Loss 2.583730459213257\n",
      "44: Encoding Loss -1.2583709955215454, Transition Loss -12.423696517944336, Classifier Loss 0.05834249407052994, Total Loss 5.8317646980285645\n",
      "44: Encoding Loss -2.0800232887268066, Transition Loss -5.973324775695801, Classifier Loss 0.0417691245675087, Total Loss 4.175717830657959\n",
      "44: Encoding Loss -1.2805685997009277, Transition Loss -9.502460479736328, Classifier Loss 0.060272663831710815, Total Loss 6.025365829467773\n",
      "44: Encoding Loss -1.891831874847412, Transition Loss -10.332088470458984, Classifier Loss 0.05285388231277466, Total Loss 5.283321380615234\n",
      "44: Encoding Loss -1.2778174877166748, Transition Loss -5.898161888122559, Classifier Loss 0.03212027624249458, Total Loss 3.210847854614258\n",
      "44: Encoding Loss -1.3910454511642456, Transition Loss -11.312530517578125, Classifier Loss 0.02715042233467102, Total Loss 2.7127795219421387\n",
      "44: Encoding Loss -1.4989351034164429, Transition Loss -13.654298782348633, Classifier Loss 0.0663151815533638, Total Loss 6.628787517547607\n",
      "44: Encoding Loss -0.9539690017700195, Transition Loss -13.340381622314453, Classifier Loss 0.0529361292719841, Total Loss 5.290945053100586\n",
      "44: Encoding Loss -1.5113146305084229, Transition Loss -1.5321640968322754, Classifier Loss 0.0830942839384079, Total Loss 8.309122085571289\n",
      "44: Encoding Loss -1.1136398315429688, Transition Loss -9.527579307556152, Classifier Loss 0.03363994508981705, Total Loss 3.362089157104492\n",
      "44: Encoding Loss -0.47027766704559326, Transition Loss -12.324633598327637, Classifier Loss 0.0711606815457344, Total Loss 7.113598823547363\n",
      "44: Encoding Loss -0.3852491080760956, Transition Loss -10.644680976867676, Classifier Loss 0.04901137202978134, Total Loss 4.898828029632568\n",
      "44: Encoding Loss -1.4809566736221313, Transition Loss -10.2431640625, Classifier Loss 0.04703199490904808, Total Loss 4.701150894165039\n",
      "44: Encoding Loss -2.524937152862549, Transition Loss -6.08719539642334, Classifier Loss 0.07437895983457565, Total Loss 7.436678409576416\n",
      "44: Encoding Loss -2.3964009284973145, Transition Loss -15.237785339355469, Classifier Loss 0.035349443554878235, Total Loss 3.5318968296051025\n",
      "44: Encoding Loss -0.3904891908168793, Transition Loss -2.5537571907043457, Classifier Loss 0.05012679100036621, Total Loss 5.012021064758301\n",
      "44: Encoding Loss -1.188964605331421, Transition Loss -8.714400291442871, Classifier Loss 0.03041769750416279, Total Loss 3.040026903152466\n",
      "44: Encoding Loss -0.49448996782302856, Transition Loss -8.226917266845703, Classifier Loss 0.08561975508928299, Total Loss 8.560328483581543\n",
      "44: Encoding Loss -0.39875322580337524, Transition Loss -16.946666717529297, Classifier Loss 0.14496466517448425, Total Loss 14.49297046661377\n",
      "44: Encoding Loss -2.741055488586426, Transition Loss -16.798477172851562, Classifier Loss 0.06840742379426956, Total Loss 6.837382793426514\n",
      "44: Encoding Loss -2.871189594268799, Transition Loss -22.53611183166504, Classifier Loss 0.03708900138735771, Total Loss 3.704392910003662\n",
      "44: Encoding Loss -1.0460493564605713, Transition Loss -12.26431655883789, Classifier Loss 0.09097696095705032, Total Loss 9.095243453979492\n",
      "44: Encoding Loss -2.166036367416382, Transition Loss -13.816624641418457, Classifier Loss 0.04794437065720558, Total Loss 4.79167366027832\n",
      "44: Encoding Loss -2.98348069190979, Transition Loss -20.151887893676758, Classifier Loss 0.05022148787975311, Total Loss 5.018118381500244\n",
      "44: Encoding Loss -2.263967275619507, Transition Loss -8.00700855255127, Classifier Loss 0.07883143424987793, Total Loss 7.881542205810547\n",
      "44: Encoding Loss -0.9361010789871216, Transition Loss -16.680683135986328, Classifier Loss 0.07946635782718658, Total Loss 7.943299770355225\n",
      "44: Encoding Loss -1.761570692062378, Transition Loss -17.52812957763672, Classifier Loss 0.02211127243936062, Total Loss 2.2076215744018555\n",
      "44: Encoding Loss -1.9883118867874146, Transition Loss -20.66165542602539, Classifier Loss 0.053724586963653564, Total Loss 5.368326187133789\n",
      "44: Encoding Loss -2.224303960800171, Transition Loss -26.718263626098633, Classifier Loss 0.016294749453663826, Total Loss 1.6241313219070435\n",
      "45: Encoding Loss -1.2524992227554321, Transition Loss -5.993237018585205, Classifier Loss 0.03166462853550911, Total Loss 3.165264368057251\n",
      "45: Encoding Loss -1.965248465538025, Transition Loss -6.7598185539245605, Classifier Loss 0.09135235100984573, Total Loss 9.133882522583008\n",
      "45: Encoding Loss -1.8020172119140625, Transition Loss -1.5654683113098145, Classifier Loss 0.03446689993143082, Total Loss 3.4463770389556885\n",
      "45: Encoding Loss -1.6074284315109253, Transition Loss -19.55890655517578, Classifier Loss 0.05273551866412163, Total Loss 5.26963996887207\n",
      "45: Encoding Loss -1.3631913661956787, Transition Loss -9.956401824951172, Classifier Loss 0.04264527186751366, Total Loss 4.26253604888916\n",
      "45: Encoding Loss -1.905161738395691, Transition Loss -16.318096160888672, Classifier Loss 0.04479190707206726, Total Loss 4.475927352905273\n",
      "45: Encoding Loss -2.541208505630493, Transition Loss -12.045561790466309, Classifier Loss 0.05434979498386383, Total Loss 5.432570457458496\n",
      "45: Encoding Loss -1.5281074047088623, Transition Loss -9.61213493347168, Classifier Loss 0.04579172283411026, Total Loss 4.577249526977539\n",
      "45: Encoding Loss -1.4405596256256104, Transition Loss -16.299957275390625, Classifier Loss 0.034161776304244995, Total Loss 3.4129178524017334\n",
      "45: Encoding Loss -2.48616886138916, Transition Loss -13.528190612792969, Classifier Loss 0.058108579367399216, Total Loss 5.808152198791504\n",
      "45: Encoding Loss -0.20771199464797974, Transition Loss -9.735382080078125, Classifier Loss 0.04886709153652191, Total Loss 4.853363990783691\n",
      "45: Encoding Loss -1.342348575592041, Transition Loss -6.332848072052002, Classifier Loss 0.05884884297847748, Total Loss 5.883617877960205\n",
      "45: Encoding Loss -1.7890750169754028, Transition Loss -8.311875343322754, Classifier Loss 0.04236164689064026, Total Loss 4.23450231552124\n",
      "45: Encoding Loss -0.9126089811325073, Transition Loss -8.92431926727295, Classifier Loss 0.053235974162817, Total Loss 5.321812629699707\n",
      "45: Encoding Loss -2.501140832901001, Transition Loss -16.176761627197266, Classifier Loss 0.03361877053976059, Total Loss 3.3586416244506836\n",
      "45: Encoding Loss -2.5631942749023438, Transition Loss -20.242929458618164, Classifier Loss 0.058783359825611115, Total Loss 5.874287128448486\n",
      "45: Encoding Loss -1.0909157991409302, Transition Loss -12.159608840942383, Classifier Loss 0.019686151295900345, Total Loss 1.9661833047866821\n",
      "45: Encoding Loss -0.700658917427063, Transition Loss -8.110930442810059, Classifier Loss 0.04169197008013725, Total Loss 4.167574882507324\n",
      "45: Encoding Loss -0.7393755316734314, Transition Loss -15.337169647216797, Classifier Loss 0.0765276625752449, Total Loss 7.649698734283447\n",
      "45: Encoding Loss -0.7417709231376648, Transition Loss -7.6692938804626465, Classifier Loss 0.0477294921875, Total Loss 4.7714152336120605\n",
      "45: Encoding Loss -1.4214556217193604, Transition Loss -17.16323471069336, Classifier Loss 0.07028187811374664, Total Loss 7.024755001068115\n",
      "45: Encoding Loss -1.6949005126953125, Transition Loss -13.311084747314453, Classifier Loss 0.03394973650574684, Total Loss 3.3923115730285645\n",
      "45: Encoding Loss -1.1677616834640503, Transition Loss -14.17863941192627, Classifier Loss 0.058799341320991516, Total Loss 5.877098560333252\n",
      "45: Encoding Loss -1.5376718044281006, Transition Loss -6.002025604248047, Classifier Loss 0.07319287210702896, Total Loss 7.318087100982666\n",
      "45: Encoding Loss -2.0379626750946045, Transition Loss -13.81982707977295, Classifier Loss 0.03029829077422619, Total Loss 3.0270650386810303\n",
      "45: Encoding Loss -0.6706222295761108, Transition Loss -7.237697601318359, Classifier Loss 0.05944066494703293, Total Loss 5.9426188468933105\n",
      "45: Encoding Loss -2.354123830795288, Transition Loss -5.8658905029296875, Classifier Loss 0.08706721663475037, Total Loss 8.705548286437988\n",
      "45: Encoding Loss -3.3236374855041504, Transition Loss -20.94064712524414, Classifier Loss 0.0640304759144783, Total Loss 6.39885950088501\n",
      "45: Encoding Loss -2.1118686199188232, Transition Loss -11.589284896850586, Classifier Loss 0.05023551359772682, Total Loss 5.021233558654785\n",
      "45: Encoding Loss -0.6575803756713867, Transition Loss -7.515325546264648, Classifier Loss 0.05750182271003723, Total Loss 5.748679161071777\n",
      "45: Encoding Loss -1.1775678396224976, Transition Loss -13.596717834472656, Classifier Loss 0.06177438795566559, Total Loss 6.174719333648682\n",
      "45: Encoding Loss -1.6956559419631958, Transition Loss -5.258783340454102, Classifier Loss 0.0734785944223404, Total Loss 7.346807479858398\n",
      "45: Encoding Loss -0.7885507345199585, Transition Loss -13.043985366821289, Classifier Loss 0.058390866965055466, Total Loss 5.836477756500244\n",
      "45: Encoding Loss -0.7074632048606873, Transition Loss -7.976210594177246, Classifier Loss 0.15479856729507446, Total Loss 15.47826099395752\n",
      "45: Encoding Loss -1.9089092016220093, Transition Loss -14.90406322479248, Classifier Loss 0.037830401211977005, Total Loss 3.780059337615967\n",
      "45: Encoding Loss -0.997290849685669, Transition Loss -9.89210033416748, Classifier Loss 0.03735349699854851, Total Loss 3.7333712577819824\n",
      "45: Encoding Loss -0.9809826612472534, Transition Loss -9.219409942626953, Classifier Loss 0.06378073245286942, Total Loss 6.376229286193848\n",
      "45: Encoding Loss -1.1280947923660278, Transition Loss -11.287263870239258, Classifier Loss 0.10640761256217957, Total Loss 10.638504028320312\n",
      "45: Encoding Loss -1.7282370328903198, Transition Loss -12.603350639343262, Classifier Loss 0.05605058744549751, Total Loss 5.602538108825684\n",
      "45: Encoding Loss -1.04575777053833, Transition Loss -13.622739791870117, Classifier Loss 0.05835115164518356, Total Loss 5.832390308380127\n",
      "45: Encoding Loss -0.8647077083587646, Transition Loss -10.41887092590332, Classifier Loss 0.030719030648469925, Total Loss 3.069819211959839\n",
      "45: Encoding Loss -1.5771931409835815, Transition Loss -18.26204490661621, Classifier Loss 0.06997823715209961, Total Loss 6.994171142578125\n",
      "45: Encoding Loss -1.9366145133972168, Transition Loss -17.564807891845703, Classifier Loss 0.027180328965187073, Total Loss 2.71451997756958\n",
      "45: Encoding Loss -2.194166660308838, Transition Loss -7.761178493499756, Classifier Loss 0.05075669288635254, Total Loss 5.074117183685303\n",
      "45: Encoding Loss -1.4362986087799072, Transition Loss -18.788341522216797, Classifier Loss 0.03946293517947197, Total Loss 3.942535877227783\n",
      "45: Encoding Loss 0.1919015347957611, Transition Loss -9.267342567443848, Classifier Loss 0.06815516948699951, Total Loss 8.306670188903809\n",
      "45: Encoding Loss 1.4344398975372314, Transition Loss -17.87363052368164, Classifier Loss 0.0560503825545311, Total Loss 17.076982498168945\n",
      "45: Encoding Loss 0.7546082735061646, Transition Loss -6.6395463943481445, Classifier Loss 0.03191441670060158, Total Loss 9.226980209350586\n",
      "45: Encoding Loss -1.2739428281784058, Transition Loss -11.331293106079102, Classifier Loss 0.049992602318525314, Total Loss 4.9969940185546875\n",
      "45: Encoding Loss -0.8147435784339905, Transition Loss -17.025951385498047, Classifier Loss 0.04530605301260948, Total Loss 4.527200222015381\n",
      "45: Encoding Loss -1.3602194786071777, Transition Loss -19.86505126953125, Classifier Loss 0.04388893395662308, Total Loss 4.384920597076416\n",
      "45: Encoding Loss -0.6113675236701965, Transition Loss -13.942684173583984, Classifier Loss 0.06505005061626434, Total Loss 6.502216339111328\n",
      "45: Encoding Loss -0.8846594095230103, Transition Loss -5.876590728759766, Classifier Loss 0.07960540801286697, Total Loss 7.959365367889404\n",
      "45: Encoding Loss -0.6850542426109314, Transition Loss -13.5499267578125, Classifier Loss 0.03962218016386032, Total Loss 3.959507942199707\n",
      "45: Encoding Loss -0.07480762898921967, Transition Loss -13.227384567260742, Classifier Loss 0.06489163637161255, Total Loss 6.35054349899292\n",
      "45: Encoding Loss -1.218121886253357, Transition Loss -7.560194969177246, Classifier Loss 0.07831636071205139, Total Loss 7.8301239013671875\n",
      "45: Encoding Loss 0.6119606494903564, Transition Loss -12.396154403686523, Classifier Loss 0.030834071338176727, Total Loss 7.976613521575928\n",
      "45: Encoding Loss -1.5987178087234497, Transition Loss -13.812625885009766, Classifier Loss 0.052622996270656586, Total Loss 5.259537220001221\n",
      "45: Encoding Loss -1.338978886604309, Transition Loss -20.10320281982422, Classifier Loss 0.03418881818652153, Total Loss 3.4148612022399902\n",
      "45: Encoding Loss -1.008872151374817, Transition Loss -17.19178581237793, Classifier Loss 0.056011755019426346, Total Loss 5.597736835479736\n",
      "45: Encoding Loss -0.823586642742157, Transition Loss -10.005624771118164, Classifier Loss 0.055540747940540314, Total Loss 5.5520734786987305\n",
      "45: Encoding Loss -1.286845326423645, Transition Loss -8.709830284118652, Classifier Loss 0.09369475394487381, Total Loss 9.367733001708984\n",
      "45: Encoding Loss -1.2235084772109985, Transition Loss -10.594015121459961, Classifier Loss 0.04774916172027588, Total Loss 4.772797584533691\n",
      "45: Encoding Loss -1.056714653968811, Transition Loss -7.825381755828857, Classifier Loss 0.06466426700353622, Total Loss 6.464861869812012\n",
      "45: Encoding Loss -0.7451773285865784, Transition Loss -12.009013175964355, Classifier Loss 0.06015234813094139, Total Loss 6.012833118438721\n",
      "45: Encoding Loss -1.4740865230560303, Transition Loss -5.1591596603393555, Classifier Loss 0.09467720985412598, Total Loss 9.466689109802246\n",
      "45: Encoding Loss 0.010620624758303165, Transition Loss -1.6187286376953125, Classifier Loss 0.05130395293235779, Total Loss 5.1761474609375\n",
      "45: Encoding Loss -1.8724032640457153, Transition Loss -19.449581146240234, Classifier Loss 0.0710664913058281, Total Loss 7.102758884429932\n",
      "45: Encoding Loss -0.7960265874862671, Transition Loss -10.286945343017578, Classifier Loss 0.06356698274612427, Total Loss 6.354640483856201\n",
      "45: Encoding Loss -0.9850111603736877, Transition Loss -10.22691822052002, Classifier Loss 0.09888338297605515, Total Loss 9.886292457580566\n",
      "45: Encoding Loss -1.8933608531951904, Transition Loss -9.82848834991455, Classifier Loss 0.06740149110555649, Total Loss 6.738183498382568\n",
      "45: Encoding Loss -1.5688756704330444, Transition Loss -10.037413597106934, Classifier Loss 0.06710174679756165, Total Loss 6.70816707611084\n",
      "45: Encoding Loss -0.9557582139968872, Transition Loss -6.841616630554199, Classifier Loss 0.08493559062480927, Total Loss 8.49219036102295\n",
      "45: Encoding Loss -1.4107251167297363, Transition Loss -10.394676208496094, Classifier Loss 0.020476749166846275, Total Loss 2.04559588432312\n",
      "45: Encoding Loss -2.0845420360565186, Transition Loss -9.288393020629883, Classifier Loss 0.0874607264995575, Total Loss 8.74421501159668\n",
      "45: Encoding Loss -1.6387182474136353, Transition Loss -15.59610652923584, Classifier Loss 0.03346392884850502, Total Loss 3.343273639678955\n",
      "45: Encoding Loss -1.573543667793274, Transition Loss -7.8565568923950195, Classifier Loss 0.06804855912923813, Total Loss 6.803284645080566\n",
      "45: Encoding Loss -2.5477869510650635, Transition Loss -13.721305847167969, Classifier Loss 0.05218840390443802, Total Loss 5.2160964012146\n",
      "45: Encoding Loss -1.8153599500656128, Transition Loss -14.783635139465332, Classifier Loss 0.025059163570404053, Total Loss 2.502959728240967\n",
      "45: Encoding Loss -1.7582173347473145, Transition Loss -11.277839660644531, Classifier Loss 0.09009502083063126, Total Loss 9.007246971130371\n",
      "45: Encoding Loss -1.085028886795044, Transition Loss -12.416646003723145, Classifier Loss 0.04782542586326599, Total Loss 4.780059337615967\n",
      "45: Encoding Loss -1.8884724378585815, Transition Loss -10.01977252960205, Classifier Loss 0.041840266436338425, Total Loss 4.182022571563721\n",
      "45: Encoding Loss -1.4517693519592285, Transition Loss -6.835893630981445, Classifier Loss 0.09019620716571808, Total Loss 9.018253326416016\n",
      "45: Encoding Loss -0.752437949180603, Transition Loss -12.1094331741333, Classifier Loss 0.07373349368572235, Total Loss 7.370927333831787\n",
      "45: Encoding Loss -2.6570699214935303, Transition Loss -9.133368492126465, Classifier Loss 0.04867462068796158, Total Loss 4.865635395050049\n",
      "45: Encoding Loss -1.505659580230713, Transition Loss -7.1151018142700195, Classifier Loss 0.06527449935674667, Total Loss 6.526027202606201\n",
      "45: Encoding Loss -2.1516854763031006, Transition Loss -16.690622329711914, Classifier Loss 0.055750325322151184, Total Loss 5.571694374084473\n",
      "45: Encoding Loss -1.6679695844650269, Transition Loss -10.997465133666992, Classifier Loss 0.09793462604284286, Total Loss 9.791263580322266\n",
      "45: Encoding Loss -1.0617419481277466, Transition Loss -9.050590515136719, Classifier Loss 0.04712706059217453, Total Loss 4.710896015167236\n",
      "45: Encoding Loss -1.1594488620758057, Transition Loss -14.56308364868164, Classifier Loss 0.049824781715869904, Total Loss 4.979565620422363\n",
      "45: Encoding Loss -3.3107829093933105, Transition Loss -22.57086944580078, Classifier Loss 0.046285487711429596, Total Loss 4.624034404754639\n",
      "45: Encoding Loss -1.2543597221374512, Transition Loss -8.072563171386719, Classifier Loss 0.07689554989337921, Total Loss 7.68794059753418\n",
      "45: Encoding Loss -1.4727121591567993, Transition Loss -8.938410758972168, Classifier Loss 0.07001104950904846, Total Loss 6.999317169189453\n",
      "45: Encoding Loss -0.8782224655151367, Transition Loss -10.918743133544922, Classifier Loss 0.05880415067076683, Total Loss 5.878231048583984\n",
      "45: Encoding Loss -1.4638540744781494, Transition Loss -5.4006428718566895, Classifier Loss 0.06694923341274261, Total Loss 6.693843364715576\n",
      "45: Encoding Loss -1.9530644416809082, Transition Loss -14.298494338989258, Classifier Loss 0.06722459197044373, Total Loss 6.719599723815918\n",
      "45: Encoding Loss -1.635749340057373, Transition Loss -17.08246612548828, Classifier Loss 0.06732679158449173, Total Loss 6.729262828826904\n",
      "45: Encoding Loss -1.8791290521621704, Transition Loss -14.231412887573242, Classifier Loss 0.05059674009680748, Total Loss 5.056827545166016\n",
      "45: Encoding Loss -1.197723150253296, Transition Loss -5.7569122314453125, Classifier Loss 0.03138233348727226, Total Loss 3.137082099914551\n",
      "45: Encoding Loss -1.908718466758728, Transition Loss -8.535499572753906, Classifier Loss 0.09415038675069809, Total Loss 9.413331985473633\n",
      "45: Encoding Loss -2.2579505443573, Transition Loss -15.457749366760254, Classifier Loss 0.05716975778341293, Total Loss 5.713884353637695\n",
      "45: Encoding Loss -0.9022420048713684, Transition Loss -1.2019741535186768, Classifier Loss 0.09843064844608307, Total Loss 9.842824935913086\n",
      "45: Encoding Loss -1.2789642810821533, Transition Loss -9.29129409790039, Classifier Loss 0.06650858372449875, Total Loss 6.64900016784668\n",
      "45: Encoding Loss -0.6402742862701416, Transition Loss -5.704498291015625, Classifier Loss 0.07066936790943146, Total Loss 7.0657958984375\n",
      "45: Encoding Loss -1.252436876296997, Transition Loss -12.640695571899414, Classifier Loss 0.0453965924680233, Total Loss 4.537130832672119\n",
      "45: Encoding Loss -1.934203028678894, Transition Loss -16.476924896240234, Classifier Loss 0.0772833600640297, Total Loss 7.725040435791016\n",
      "45: Encoding Loss -1.3548215627670288, Transition Loss -5.823287010192871, Classifier Loss 0.0693141296505928, Total Loss 6.930248737335205\n",
      "45: Encoding Loss -1.4456878900527954, Transition Loss -10.134455680847168, Classifier Loss 0.040965769439935684, Total Loss 4.094550132751465\n",
      "45: Encoding Loss -1.574151635169983, Transition Loss -9.860671997070312, Classifier Loss 0.047997284680604935, Total Loss 4.797756195068359\n",
      "45: Encoding Loss -0.9329021573066711, Transition Loss -15.042625427246094, Classifier Loss 0.052519507706165314, Total Loss 5.2489423751831055\n",
      "45: Encoding Loss -1.460696816444397, Transition Loss -14.57158374786377, Classifier Loss 0.06333886831998825, Total Loss 6.330972194671631\n",
      "45: Encoding Loss -1.2228314876556396, Transition Loss -2.9050238132476807, Classifier Loss 0.03744220361113548, Total Loss 3.7436392307281494\n",
      "45: Encoding Loss -1.7039048671722412, Transition Loss -18.489917755126953, Classifier Loss 0.03673458844423294, Total Loss 3.6697609424591064\n",
      "45: Encoding Loss -2.1120409965515137, Transition Loss -17.792787551879883, Classifier Loss 0.031299445778131485, Total Loss 3.1263859272003174\n",
      "45: Encoding Loss -0.9509552717208862, Transition Loss -2.201707363128662, Classifier Loss 0.05292815342545509, Total Loss 5.292375087738037\n",
      "45: Encoding Loss -0.7280111312866211, Transition Loss 0.4805107116699219, Classifier Loss 0.05367504805326462, Total Loss 5.463606834411621\n",
      "45: Encoding Loss -0.8886435031890869, Transition Loss -12.683478355407715, Classifier Loss 0.036930400878190994, Total Loss 3.6905033588409424\n",
      "45: Encoding Loss -1.4826682806015015, Transition Loss -11.843291282653809, Classifier Loss 0.04448621720075607, Total Loss 4.446253299713135\n",
      "45: Encoding Loss -2.2577688694000244, Transition Loss -13.914541244506836, Classifier Loss 0.05272653326392174, Total Loss 5.269870281219482\n",
      "45: Encoding Loss -1.3384941816329956, Transition Loss -7.495880126953125, Classifier Loss 0.05529608950018883, Total Loss 5.528109550476074\n",
      "45: Encoding Loss -2.6163697242736816, Transition Loss -16.067047119140625, Classifier Loss 0.03748287260532379, Total Loss 3.7450737953186035\n",
      "45: Encoding Loss -1.4084925651550293, Transition Loss -9.754997253417969, Classifier Loss 0.06552846729755402, Total Loss 6.550895690917969\n",
      "45: Encoding Loss -1.412382960319519, Transition Loss -13.271310806274414, Classifier Loss 0.08564580231904984, Total Loss 8.561925888061523\n",
      "45: Encoding Loss -2.5994999408721924, Transition Loss -20.41738510131836, Classifier Loss 0.05013186112046242, Total Loss 5.0091023445129395\n",
      "45: Encoding Loss -2.1505625247955322, Transition Loss -9.620670318603516, Classifier Loss 0.06400006264448166, Total Loss 6.398082256317139\n",
      "45: Encoding Loss -1.7834042310714722, Transition Loss -10.969438552856445, Classifier Loss 0.09329497069120407, Total Loss 9.327303886413574\n",
      "45: Encoding Loss -1.7084391117095947, Transition Loss -16.58830451965332, Classifier Loss 0.07145411521196365, Total Loss 7.142093658447266\n",
      "45: Encoding Loss -2.2457122802734375, Transition Loss -13.511711120605469, Classifier Loss 0.025947431102395058, Total Loss 2.592040777206421\n",
      "45: Encoding Loss -0.9158903360366821, Transition Loss -12.294990539550781, Classifier Loss 0.05633534863591194, Total Loss 5.631075859069824\n",
      "45: Encoding Loss -1.9124048948287964, Transition Loss -5.959869384765625, Classifier Loss 0.04111042991280556, Total Loss 4.109850883483887\n",
      "45: Encoding Loss -1.2240408658981323, Transition Loss -9.518162727355957, Classifier Loss 0.06142018362879753, Total Loss 6.140114784240723\n",
      "45: Encoding Loss -1.7693434953689575, Transition Loss -10.2108154296875, Classifier Loss 0.05287779122591019, Total Loss 5.285737037658691\n",
      "45: Encoding Loss -1.100974202156067, Transition Loss -5.933406829833984, Classifier Loss 0.03228302299976349, Total Loss 3.2271156311035156\n",
      "45: Encoding Loss -1.2958062887191772, Transition Loss -11.14350414276123, Classifier Loss 0.026916146278381348, Total Loss 2.6893858909606934\n",
      "45: Encoding Loss -1.4427984952926636, Transition Loss -13.387380599975586, Classifier Loss 0.06680727005004883, Total Loss 6.678049564361572\n",
      "45: Encoding Loss -0.42858654260635376, Transition Loss -13.102927207946777, Classifier Loss 0.05417310446500778, Total Loss 5.414658546447754\n",
      "45: Encoding Loss -1.37872314453125, Transition Loss -1.7914550304412842, Classifier Loss 0.08302310854196548, Total Loss 8.301952362060547\n",
      "45: Encoding Loss -1.3645356893539429, Transition Loss -9.546026229858398, Classifier Loss 0.0324806272983551, Total Loss 3.2461533546447754\n",
      "45: Encoding Loss -0.7487993836402893, Transition Loss -12.164019584655762, Classifier Loss 0.07022515684366226, Total Loss 7.020082950592041\n",
      "45: Encoding Loss -0.38095301389694214, Transition Loss -10.60425853729248, Classifier Loss 0.048435136675834656, Total Loss 4.841180324554443\n",
      "45: Encoding Loss -1.127394437789917, Transition Loss -10.269986152648926, Classifier Loss 0.04871831461787224, Total Loss 4.869777202606201\n",
      "45: Encoding Loss -2.270128011703491, Transition Loss -6.180031776428223, Classifier Loss 0.07368973642587662, Total Loss 7.367737770080566\n",
      "45: Encoding Loss -2.366989850997925, Transition Loss -15.100092887878418, Classifier Loss 0.034642674028873444, Total Loss 3.461247444152832\n",
      "45: Encoding Loss -0.23233161866664886, Transition Loss -2.551820755004883, Classifier Loss 0.0496978834271431, Total Loss 4.950541019439697\n",
      "45: Encoding Loss -1.0790901184082031, Transition Loss -8.893318176269531, Classifier Loss 0.03024498000741005, Total Loss 3.022719383239746\n",
      "45: Encoding Loss -0.6549429297447205, Transition Loss -8.221268653869629, Classifier Loss 0.07634575664997101, Total Loss 7.632931709289551\n",
      "45: Encoding Loss -0.7468900680541992, Transition Loss -16.663965225219727, Classifier Loss 0.13774390518665314, Total Loss 13.77105712890625\n",
      "45: Encoding Loss -2.19648814201355, Transition Loss -16.475208282470703, Classifier Loss 0.06834970414638519, Total Loss 6.8316755294799805\n",
      "45: Encoding Loss -2.457756996154785, Transition Loss -21.9719295501709, Classifier Loss 0.037175584584474564, Total Loss 3.7131640911102295\n",
      "45: Encoding Loss -1.0607103109359741, Transition Loss -12.105319023132324, Classifier Loss 0.08511554449796677, Total Loss 8.509133338928223\n",
      "45: Encoding Loss -1.7998499870300293, Transition Loss -13.55640983581543, Classifier Loss 0.04693055525422096, Total Loss 4.690344333648682\n",
      "45: Encoding Loss -2.4594106674194336, Transition Loss -19.8002872467041, Classifier Loss 0.050334759056568146, Total Loss 5.029515743255615\n",
      "45: Encoding Loss -1.923980712890625, Transition Loss -7.959766387939453, Classifier Loss 0.07673042267560959, Total Loss 7.671450138092041\n",
      "45: Encoding Loss -0.9272053837776184, Transition Loss -16.18748664855957, Classifier Loss 0.07679521292448044, Total Loss 7.676283359527588\n",
      "45: Encoding Loss -1.286596417427063, Transition Loss -17.11544418334961, Classifier Loss 0.020946217700839043, Total Loss 2.091198682785034\n",
      "45: Encoding Loss -1.7408894300460815, Transition Loss -20.03756332397461, Classifier Loss 0.05293874070048332, Total Loss 5.289866924285889\n",
      "45: Encoding Loss -1.9348584413528442, Transition Loss -25.799827575683594, Classifier Loss 0.015486492775380611, Total Loss 1.5434893369674683\n",
      "46: Encoding Loss -0.9601542353630066, Transition Loss -5.803535461425781, Classifier Loss 0.030869366601109505, Total Loss 3.0857760906219482\n",
      "46: Encoding Loss -1.8733057975769043, Transition Loss -6.858511924743652, Classifier Loss 0.08881302177906036, Total Loss 8.87993049621582\n",
      "46: Encoding Loss -1.5793834924697876, Transition Loss -1.9234693050384521, Classifier Loss 0.034220609813928604, Total Loss 3.4216761589050293\n",
      "46: Encoding Loss -1.4732095003128052, Transition Loss -19.159719467163086, Classifier Loss 0.051605962216854095, Total Loss 5.156764507293701\n",
      "46: Encoding Loss -1.3310414552688599, Transition Loss -10.030359268188477, Classifier Loss 0.04376869276165962, Total Loss 4.374863147735596\n",
      "46: Encoding Loss -1.6053783893585205, Transition Loss -15.857983589172363, Classifier Loss 0.043769922107458115, Total Loss 4.373820781707764\n",
      "46: Encoding Loss -2.097982168197632, Transition Loss -11.99228286743164, Classifier Loss 0.05268494412302971, Total Loss 5.266096115112305\n",
      "46: Encoding Loss -1.1407206058502197, Transition Loss -9.199079513549805, Classifier Loss 0.044282954186201096, Total Loss 4.426455974578857\n",
      "46: Encoding Loss -1.454295039176941, Transition Loss -16.010522842407227, Classifier Loss 0.03428182005882263, Total Loss 3.4249799251556396\n",
      "46: Encoding Loss -2.3120980262756348, Transition Loss -13.325681686401367, Classifier Loss 0.057371512055397034, Total Loss 5.734486103057861\n",
      "46: Encoding Loss -0.03995385020971298, Transition Loss -9.835515975952148, Classifier Loss 0.0477832593023777, Total Loss 4.66616678237915\n",
      "46: Encoding Loss -0.7655194401741028, Transition Loss -5.608448505401611, Classifier Loss 0.057823535054922104, Total Loss 5.781231880187988\n",
      "46: Encoding Loss -1.6672359704971313, Transition Loss -7.793453216552734, Classifier Loss 0.04183964058756828, Total Loss 4.182405471801758\n",
      "46: Encoding Loss -1.104463815689087, Transition Loss -8.513349533081055, Classifier Loss 0.05090545117855072, Total Loss 5.088842391967773\n",
      "46: Encoding Loss -1.9507511854171753, Transition Loss -15.548202514648438, Classifier Loss 0.03359617665410042, Total Loss 3.3565080165863037\n",
      "46: Encoding Loss -2.3984262943267822, Transition Loss -19.36550521850586, Classifier Loss 0.06182866916060448, Total Loss 6.178994178771973\n",
      "46: Encoding Loss -1.3482691049575806, Transition Loss -11.749013900756836, Classifier Loss 0.019611448049545288, Total Loss 1.9587949514389038\n",
      "46: Encoding Loss -0.7440956830978394, Transition Loss -7.972944736480713, Classifier Loss 0.04017714783549309, Total Loss 4.016120433807373\n",
      "46: Encoding Loss -0.7277438640594482, Transition Loss -14.692825317382812, Classifier Loss 0.07531403750181198, Total Loss 7.5284647941589355\n",
      "46: Encoding Loss -0.714923620223999, Transition Loss -7.210107803344727, Classifier Loss 0.04756203293800354, Total Loss 4.754761219024658\n",
      "46: Encoding Loss -1.2957561016082764, Transition Loss -16.655790328979492, Classifier Loss 0.07136448472738266, Total Loss 7.133117198944092\n",
      "46: Encoding Loss -1.666091799736023, Transition Loss -12.963800430297852, Classifier Loss 0.033881328999996185, Total Loss 3.385540008544922\n",
      "46: Encoding Loss -1.3368407487869263, Transition Loss -13.716300964355469, Classifier Loss 0.05784435570240021, Total Loss 5.7816925048828125\n",
      "46: Encoding Loss -1.2842460870742798, Transition Loss -5.422598838806152, Classifier Loss 0.07231618463993073, Total Loss 7.230534076690674\n",
      "46: Encoding Loss -1.9615968465805054, Transition Loss -13.411251068115234, Classifier Loss 0.03269490599632263, Total Loss 3.26680850982666\n",
      "46: Encoding Loss -0.6202905774116516, Transition Loss -6.968025207519531, Classifier Loss 0.05844083055853844, Total Loss 5.842689037322998\n",
      "46: Encoding Loss -2.3326642513275146, Transition Loss -5.637064456939697, Classifier Loss 0.08546967804431915, Total Loss 8.5458402633667\n",
      "46: Encoding Loss -2.6934590339660645, Transition Loss -20.285938262939453, Classifier Loss 0.06312999874353409, Total Loss 6.3089423179626465\n",
      "46: Encoding Loss -1.9326914548873901, Transition Loss -10.865710258483887, Classifier Loss 0.048972535878419876, Total Loss 4.89508056640625\n",
      "46: Encoding Loss -0.8300322890281677, Transition Loss -7.072039604187012, Classifier Loss 0.057876888662576675, Total Loss 5.786274433135986\n",
      "46: Encoding Loss -1.2225018739700317, Transition Loss -13.260246276855469, Classifier Loss 0.060878071933984756, Total Loss 6.085155010223389\n",
      "46: Encoding Loss -1.4516029357910156, Transition Loss -4.752042770385742, Classifier Loss 0.07180355489253998, Total Loss 7.179405212402344\n",
      "46: Encoding Loss -0.7711167931556702, Transition Loss -12.686656951904297, Classifier Loss 0.059023912996053696, Total Loss 5.8998541831970215\n",
      "46: Encoding Loss -0.4141000807285309, Transition Loss -7.347805500030518, Classifier Loss 0.1562945693731308, Total Loss 15.6279296875\n",
      "46: Encoding Loss -1.7638219594955444, Transition Loss -14.53622817993164, Classifier Loss 0.038316838443279266, Total Loss 3.8287765979766846\n",
      "46: Encoding Loss -0.9737546443939209, Transition Loss -9.337542533874512, Classifier Loss 0.0401858352124691, Total Loss 4.016716003417969\n",
      "46: Encoding Loss -0.9167222380638123, Transition Loss -8.545063018798828, Classifier Loss 0.06046662479639053, Total Loss 6.044953346252441\n",
      "46: Encoding Loss -1.0584379434585571, Transition Loss -10.690020561218262, Classifier Loss 0.10502124577760696, Total Loss 10.49998664855957\n",
      "46: Encoding Loss -1.7923781871795654, Transition Loss -12.119443893432617, Classifier Loss 0.05483276769518852, Total Loss 5.480853080749512\n",
      "46: Encoding Loss -1.0207377672195435, Transition Loss -13.34618091583252, Classifier Loss 0.056913480162620544, Total Loss 5.688678741455078\n",
      "46: Encoding Loss -0.9912957549095154, Transition Loss -9.909078598022461, Classifier Loss 0.029524195939302444, Total Loss 2.9504377841949463\n",
      "46: Encoding Loss -1.510068416595459, Transition Loss -17.77030372619629, Classifier Loss 0.06530586630105972, Total Loss 6.527032852172852\n",
      "46: Encoding Loss -1.9182586669921875, Transition Loss -17.277782440185547, Classifier Loss 0.02851342223584652, Total Loss 2.847886562347412\n",
      "46: Encoding Loss -2.150261640548706, Transition Loss -7.643075466156006, Classifier Loss 0.05258560553193092, Total Loss 5.2570319175720215\n",
      "46: Encoding Loss -1.5113060474395752, Transition Loss -18.420202255249023, Classifier Loss 0.03913813456892967, Total Loss 3.9101293087005615\n",
      "46: Encoding Loss -0.11202581971883774, Transition Loss -9.018481254577637, Classifier Loss 0.0672362893819809, Total Loss 6.604151725769043\n",
      "46: Encoding Loss -1.309322476387024, Transition Loss -19.10018539428711, Classifier Loss 0.054619207978248596, Total Loss 5.45810079574585\n",
      "46: Encoding Loss -1.731405258178711, Transition Loss -7.753113746643066, Classifier Loss 0.02927185408771038, Total Loss 2.9256346225738525\n",
      "46: Encoding Loss -1.6021958589553833, Transition Loss -10.864724159240723, Classifier Loss 0.04759678244590759, Total Loss 4.757505416870117\n",
      "46: Encoding Loss -1.2179473638534546, Transition Loss -17.244564056396484, Classifier Loss 0.04308687523007393, Total Loss 4.305238723754883\n",
      "46: Encoding Loss -1.9035829305648804, Transition Loss -20.07736587524414, Classifier Loss 0.04431096464395523, Total Loss 4.427081108093262\n",
      "46: Encoding Loss -0.7354165315628052, Transition Loss -13.85501766204834, Classifier Loss 0.06139010563492775, Total Loss 6.136239528656006\n",
      "46: Encoding Loss -0.7798276543617249, Transition Loss -4.2251176834106445, Classifier Loss 0.07749182730913162, Total Loss 7.748337745666504\n",
      "46: Encoding Loss -0.9693803191184998, Transition Loss -13.517983436584473, Classifier Loss 0.03667899966239929, Total Loss 3.665196418762207\n",
      "46: Encoding Loss -0.8799759149551392, Transition Loss -13.16085433959961, Classifier Loss 0.06461098790168762, Total Loss 6.458466529846191\n",
      "46: Encoding Loss -1.6145583391189575, Transition Loss -6.69003963470459, Classifier Loss 0.07873844355344772, Total Loss 7.872506141662598\n",
      "46: Encoding Loss -0.3662245571613312, Transition Loss -12.159595489501953, Classifier Loss 0.02778782695531845, Total Loss 2.775984525680542\n",
      "46: Encoding Loss -1.6185410022735596, Transition Loss -14.17303466796875, Classifier Loss 0.0467526838183403, Total Loss 4.672433376312256\n",
      "46: Encoding Loss -1.8440546989440918, Transition Loss -20.121992111206055, Classifier Loss 0.03471898287534714, Total Loss 3.4678738117218018\n",
      "46: Encoding Loss -1.441203236579895, Transition Loss -17.102378845214844, Classifier Loss 0.05668697506189346, Total Loss 5.665277004241943\n",
      "46: Encoding Loss -0.9783996343612671, Transition Loss -10.39852237701416, Classifier Loss 0.05545111000537872, Total Loss 5.543031692504883\n",
      "46: Encoding Loss -1.7530046701431274, Transition Loss -9.107033729553223, Classifier Loss 0.08395543694496155, Total Loss 8.393722534179688\n",
      "46: Encoding Loss -1.7314918041229248, Transition Loss -11.001773834228516, Classifier Loss 0.04431389272212982, Total Loss 4.429189205169678\n",
      "46: Encoding Loss -0.7883169054985046, Transition Loss -8.476417541503906, Classifier Loss 0.0649963840842247, Total Loss 6.49794340133667\n",
      "46: Encoding Loss -0.9541533589363098, Transition Loss -12.476069450378418, Classifier Loss 0.05744265019893646, Total Loss 5.741769790649414\n",
      "46: Encoding Loss -1.8728623390197754, Transition Loss -5.9625067710876465, Classifier Loss 0.09311395138502121, Total Loss 9.310202598571777\n",
      "46: Encoding Loss -0.35887470841407776, Transition Loss -2.4806618690490723, Classifier Loss 0.053559765219688416, Total Loss 5.355003833770752\n",
      "46: Encoding Loss -0.8922588229179382, Transition Loss -19.060293197631836, Classifier Loss 0.06838622689247131, Total Loss 6.834810733795166\n",
      "46: Encoding Loss -0.35773828625679016, Transition Loss -9.768603324890137, Classifier Loss 0.06262180209159851, Total Loss 6.259729862213135\n",
      "46: Encoding Loss -0.579741895198822, Transition Loss -9.738990783691406, Classifier Loss 0.10218902677297592, Total Loss 10.216955184936523\n",
      "46: Encoding Loss -1.3003016710281372, Transition Loss -9.434429168701172, Classifier Loss 0.06568107008934021, Total Loss 6.566220283508301\n",
      "46: Encoding Loss -1.0584391355514526, Transition Loss -9.668954849243164, Classifier Loss 0.06630599498748779, Total Loss 6.628665924072266\n",
      "46: Encoding Loss -0.42362847924232483, Transition Loss -6.425124168395996, Classifier Loss 0.08313141763210297, Total Loss 8.311819076538086\n",
      "46: Encoding Loss -1.0454362630844116, Transition Loss -9.942790985107422, Classifier Loss 0.020371733233332634, Total Loss 2.035184621810913\n",
      "46: Encoding Loss -1.9902749061584473, Transition Loss -8.873807907104492, Classifier Loss 0.08788705617189407, Total Loss 8.786931037902832\n",
      "46: Encoding Loss -1.1928515434265137, Transition Loss -15.201403617858887, Classifier Loss 0.031190453097224236, Total Loss 3.1160049438476562\n",
      "46: Encoding Loss -0.9680787324905396, Transition Loss -7.04959774017334, Classifier Loss 0.06712380796670914, Total Loss 6.710970878601074\n",
      "46: Encoding Loss -2.209547519683838, Transition Loss -13.284893035888672, Classifier Loss 0.051427796483039856, Total Loss 5.140122890472412\n",
      "46: Encoding Loss -1.1345964670181274, Transition Loss -14.050863265991211, Classifier Loss 0.024302847683429718, Total Loss 2.4274744987487793\n",
      "46: Encoding Loss -1.2688237428665161, Transition Loss -10.666553497314453, Classifier Loss 0.09556806832551956, Total Loss 9.554673194885254\n",
      "46: Encoding Loss -0.7643734216690063, Transition Loss -11.939054489135742, Classifier Loss 0.04606842249631882, Total Loss 4.604454040527344\n",
      "46: Encoding Loss -1.2436145544052124, Transition Loss -9.336665153503418, Classifier Loss 0.04062366485595703, Total Loss 4.06049919128418\n",
      "46: Encoding Loss -0.9920192360877991, Transition Loss -6.270965576171875, Classifier Loss 0.08693154901266098, Total Loss 8.691901206970215\n",
      "46: Encoding Loss -0.5125715136528015, Transition Loss -11.957869529724121, Classifier Loss 0.07227051258087158, Total Loss 7.224659442901611\n",
      "46: Encoding Loss -2.140577793121338, Transition Loss -8.491830825805664, Classifier Loss 0.04880101606249809, Total Loss 4.878403186798096\n",
      "46: Encoding Loss -1.25858736038208, Transition Loss -6.516334056854248, Classifier Loss 0.064236119389534, Total Loss 6.422308921813965\n",
      "46: Encoding Loss -1.7111256122589111, Transition Loss -16.279319763183594, Classifier Loss 0.05784474313259125, Total Loss 5.781218528747559\n",
      "46: Encoding Loss -1.172824501991272, Transition Loss -10.300223350524902, Classifier Loss 0.0986441895365715, Total Loss 9.862359046936035\n",
      "46: Encoding Loss -0.8974732160568237, Transition Loss -8.796117782592773, Classifier Loss 0.04651164636015892, Total Loss 4.649405479431152\n",
      "46: Encoding Loss -0.602350115776062, Transition Loss -14.319795608520508, Classifier Loss 0.05175862833857536, Total Loss 5.172998905181885\n",
      "46: Encoding Loss -2.7827365398406982, Transition Loss -22.13239860534668, Classifier Loss 0.043806467205286026, Total Loss 4.376220226287842\n",
      "46: Encoding Loss -0.7684385180473328, Transition Loss -7.790119171142578, Classifier Loss 0.0763726457953453, Total Loss 7.635706901550293\n",
      "46: Encoding Loss -0.5674358010292053, Transition Loss -8.254267692565918, Classifier Loss 0.06792177259922028, Total Loss 6.790526390075684\n",
      "46: Encoding Loss -0.5013291239738464, Transition Loss -10.290847778320312, Classifier Loss 0.05774290859699249, Total Loss 5.7722320556640625\n",
      "46: Encoding Loss -1.0873054265975952, Transition Loss -4.837890148162842, Classifier Loss 0.06633466482162476, Total Loss 6.6324992179870605\n",
      "46: Encoding Loss -1.4310905933380127, Transition Loss -13.910449028015137, Classifier Loss 0.06650817394256592, Total Loss 6.648035526275635\n",
      "46: Encoding Loss -1.185767412185669, Transition Loss -16.630321502685547, Classifier Loss 0.06645480543375015, Total Loss 6.642154693603516\n",
      "46: Encoding Loss -1.2695070505142212, Transition Loss -13.654190063476562, Classifier Loss 0.047665778547525406, Total Loss 4.7638468742370605\n",
      "46: Encoding Loss -0.9479841589927673, Transition Loss -5.17141056060791, Classifier Loss 0.032904040068387985, Total Loss 3.289369821548462\n",
      "46: Encoding Loss -1.3469865322113037, Transition Loss -7.8974504470825195, Classifier Loss 0.09300556033849716, Total Loss 9.29897689819336\n",
      "46: Encoding Loss -1.547326683998108, Transition Loss -15.02708625793457, Classifier Loss 0.05425913631916046, Total Loss 5.422908306121826\n",
      "46: Encoding Loss -0.36456477642059326, Transition Loss -0.6285747289657593, Classifier Loss 0.09683959186077118, Total Loss 9.683444023132324\n",
      "46: Encoding Loss -0.5452962517738342, Transition Loss -8.877516746520996, Classifier Loss 0.06499268114566803, Total Loss 6.49749231338501\n",
      "46: Encoding Loss 0.45645496249198914, Transition Loss -5.598686218261719, Classifier Loss 0.07058954238891602, Total Loss 10.709465026855469\n",
      "46: Encoding Loss 1.8314260244369507, Transition Loss -13.855949401855469, Classifier Loss 0.04593078792095184, Total Loss 19.241716384887695\n",
      "46: Encoding Loss 0.21206483244895935, Transition Loss -14.274961471557617, Classifier Loss 0.06839829683303833, Total Loss 8.504693031311035\n",
      "46: Encoding Loss -1.1157022714614868, Transition Loss -4.916306018829346, Classifier Loss 0.07238036394119263, Total Loss 7.237052917480469\n",
      "46: Encoding Loss -1.0581051111221313, Transition Loss -8.790311813354492, Classifier Loss 0.04565248638391495, Total Loss 4.563490390777588\n",
      "46: Encoding Loss -1.4804736375808716, Transition Loss -8.614956855773926, Classifier Loss 0.04727945476770401, Total Loss 4.726222515106201\n",
      "46: Encoding Loss -0.9875154495239258, Transition Loss -13.28445816040039, Classifier Loss 0.0502094030380249, Total Loss 5.018283367156982\n",
      "46: Encoding Loss -1.438181757926941, Transition Loss -13.052239418029785, Classifier Loss 0.06742645055055618, Total Loss 6.740034580230713\n",
      "46: Encoding Loss -1.5011154413223267, Transition Loss -2.2342920303344727, Classifier Loss 0.036427274346351624, Total Loss 3.6422805786132812\n",
      "46: Encoding Loss -2.144106864929199, Transition Loss -16.495128631591797, Classifier Loss 0.036076031625270844, Total Loss 3.604304075241089\n",
      "46: Encoding Loss -1.444119930267334, Transition Loss -15.911576271057129, Classifier Loss 0.031472429633140564, Total Loss 3.1440606117248535\n",
      "46: Encoding Loss -0.594169557094574, Transition Loss -1.8039637804031372, Classifier Loss 0.050095632672309875, Total Loss 5.009202480316162\n",
      "46: Encoding Loss -0.5930968523025513, Transition Loss 0.7127599716186523, Classifier Loss 0.05157381668686867, Total Loss 5.299933433532715\n",
      "46: Encoding Loss -0.6924663186073303, Transition Loss -11.272655487060547, Classifier Loss 0.035689450800418854, Total Loss 3.566690683364868\n",
      "46: Encoding Loss -0.9775975942611694, Transition Loss -10.562549591064453, Classifier Loss 0.04573025181889534, Total Loss 4.570912837982178\n",
      "46: Encoding Loss -1.9981231689453125, Transition Loss -12.360962867736816, Classifier Loss 0.04934895038604736, Total Loss 4.932422637939453\n",
      "46: Encoding Loss -1.2668722867965698, Transition Loss -6.651821613311768, Classifier Loss 0.054169971495866776, Total Loss 5.415666580200195\n",
      "46: Encoding Loss -2.3805291652679443, Transition Loss -14.366661071777344, Classifier Loss 0.03665522485971451, Total Loss 3.662649154663086\n",
      "46: Encoding Loss -1.0655962228775024, Transition Loss -8.84487533569336, Classifier Loss 0.06706132739782333, Total Loss 6.704363822937012\n",
      "46: Encoding Loss -1.4868247509002686, Transition Loss -11.95682144165039, Classifier Loss 0.08795607089996338, Total Loss 8.793214797973633\n",
      "46: Encoding Loss -2.557588815689087, Transition Loss -18.51614761352539, Classifier Loss 0.04806043207645416, Total Loss 4.802340030670166\n",
      "46: Encoding Loss -1.742914080619812, Transition Loss -8.5103759765625, Classifier Loss 0.06237857788801193, Total Loss 6.2361555099487305\n",
      "46: Encoding Loss -1.488809585571289, Transition Loss -9.805351257324219, Classifier Loss 0.09114202111959457, Total Loss 9.112241744995117\n",
      "46: Encoding Loss -1.7958036661148071, Transition Loss -15.148346900939941, Classifier Loss 0.07465602457523346, Total Loss 7.4625725746154785\n",
      "46: Encoding Loss -1.592409610748291, Transition Loss -12.166240692138672, Classifier Loss 0.025089949369430542, Total Loss 2.506561756134033\n",
      "46: Encoding Loss -0.807569146156311, Transition Loss -10.982614517211914, Classifier Loss 0.05640720576047897, Total Loss 5.638524055480957\n",
      "46: Encoding Loss -1.0345029830932617, Transition Loss -5.164190292358398, Classifier Loss 0.039358802139759064, Total Loss 3.934847354888916\n",
      "46: Encoding Loss -0.4845770597457886, Transition Loss -8.442153930664062, Classifier Loss 0.061574362218379974, Total Loss 6.155745506286621\n",
      "46: Encoding Loss -1.3766156435012817, Transition Loss -9.054986000061035, Classifier Loss 0.05053297057747841, Total Loss 5.051486015319824\n",
      "46: Encoding Loss -0.549065887928009, Transition Loss -5.14885950088501, Classifier Loss 0.031452059745788574, Total Loss 3.1441760063171387\n",
      "46: Encoding Loss -0.6706487536430359, Transition Loss -9.956232070922852, Classifier Loss 0.025403805077075958, Total Loss 2.538389205932617\n",
      "46: Encoding Loss -1.4346593618392944, Transition Loss -12.071578025817871, Classifier Loss 0.06554291397333145, Total Loss 6.551877021789551\n",
      "46: Encoding Loss -0.3696342706680298, Transition Loss -11.751352310180664, Classifier Loss 0.05353058874607086, Total Loss 5.3503851890563965\n",
      "46: Encoding Loss -0.6812551617622375, Transition Loss -1.3902781009674072, Classifier Loss 0.0811295434832573, Total Loss 8.112675666809082\n",
      "46: Encoding Loss -0.5680760741233826, Transition Loss -8.566160202026367, Classifier Loss 0.03208572044968605, Total Loss 3.2068588733673096\n",
      "46: Encoding Loss 0.16450397670269012, Transition Loss -10.878782272338867, Classifier Loss 0.06757102906703949, Total Loss 8.005183219909668\n",
      "46: Encoding Loss 0.3817770779132843, Transition Loss -9.585229873657227, Classifier Loss 0.047196440398693085, Total Loss 7.771738052368164\n",
      "46: Encoding Loss -1.2435343265533447, Transition Loss -9.775127410888672, Classifier Loss 0.04740432649850845, Total Loss 4.73847770690918\n",
      "46: Encoding Loss -2.176292896270752, Transition Loss -5.6814446449279785, Classifier Loss 0.07044029980897903, Total Loss 7.042893886566162\n",
      "46: Encoding Loss -2.528656244277954, Transition Loss -14.603556632995605, Classifier Loss 0.033247578889131546, Total Loss 3.3218371868133545\n",
      "46: Encoding Loss -0.3933299481868744, Transition Loss -2.076153516769409, Classifier Loss 0.05006634443998337, Total Loss 5.006086826324463\n",
      "46: Encoding Loss -0.9304538369178772, Transition Loss -8.351297378540039, Classifier Loss 0.029354587197303772, Total Loss 2.933788299560547\n",
      "46: Encoding Loss -0.4171009659767151, Transition Loss -7.807061195373535, Classifier Loss 0.07759633660316467, Total Loss 7.758021354675293\n",
      "46: Encoding Loss -0.5806666612625122, Transition Loss -16.17923355102539, Classifier Loss 0.13792367279529572, Total Loss 13.789131164550781\n",
      "46: Encoding Loss -2.2850821018218994, Transition Loss -16.05479621887207, Classifier Loss 0.06482241302728653, Total Loss 6.479030132293701\n",
      "46: Encoding Loss -2.6695828437805176, Transition Loss -21.46654510498047, Classifier Loss 0.0366208516061306, Total Loss 3.65779185295105\n",
      "46: Encoding Loss -1.183066725730896, Transition Loss -11.610212326049805, Classifier Loss 0.08934447914361954, Total Loss 8.93212604522705\n",
      "46: Encoding Loss -2.1274378299713135, Transition Loss -13.25493335723877, Classifier Loss 0.04568593576550484, Total Loss 4.565942287445068\n",
      "46: Encoding Loss -2.6133692264556885, Transition Loss -19.393354415893555, Classifier Loss 0.04888354241847992, Total Loss 4.8844757080078125\n",
      "46: Encoding Loss -1.9928994178771973, Transition Loss -7.546455383300781, Classifier Loss 0.07801195979118347, Total Loss 7.799686908721924\n",
      "46: Encoding Loss -1.3578566312789917, Transition Loss -15.795171737670898, Classifier Loss 0.07724316418170929, Total Loss 7.721157550811768\n",
      "46: Encoding Loss -1.8844962120056152, Transition Loss -16.709213256835938, Classifier Loss 0.02159639075398445, Total Loss 2.156297206878662\n",
      "46: Encoding Loss -1.8625414371490479, Transition Loss -19.680152893066406, Classifier Loss 0.05368112772703171, Total Loss 5.3641767501831055\n",
      "46: Encoding Loss -3.1091716289520264, Transition Loss -25.665632247924805, Classifier Loss 0.015374129638075829, Total Loss 1.5322798490524292\n",
      "47: Encoding Loss -1.3442217111587524, Transition Loss -5.491743087768555, Classifier Loss 0.030524184927344322, Total Loss 3.0513200759887695\n",
      "47: Encoding Loss -1.7467364072799683, Transition Loss -6.477568626403809, Classifier Loss 0.0896238386631012, Total Loss 8.961089134216309\n",
      "47: Encoding Loss -1.90106999874115, Transition Loss -1.5003585815429688, Classifier Loss 0.03368029743432999, Total Loss 3.367729663848877\n",
      "47: Encoding Loss -1.7422047853469849, Transition Loss -18.81673812866211, Classifier Loss 0.051503464579582214, Total Loss 5.146583080291748\n",
      "47: Encoding Loss -1.7783619165420532, Transition Loss -9.496142387390137, Classifier Loss 0.044513456523418427, Total Loss 4.449446201324463\n",
      "47: Encoding Loss -1.5004135370254517, Transition Loss -15.505024909973145, Classifier Loss 0.04262378811836243, Total Loss 4.259277820587158\n",
      "47: Encoding Loss -2.0037760734558105, Transition Loss -11.571043968200684, Classifier Loss 0.05164346098899841, Total Loss 5.162032127380371\n",
      "47: Encoding Loss -1.505372166633606, Transition Loss -8.847990036010742, Classifier Loss 0.04413991793990135, Total Loss 4.412222385406494\n",
      "47: Encoding Loss -1.2334895133972168, Transition Loss -15.738870620727539, Classifier Loss 0.033548347651958466, Total Loss 3.351686954498291\n",
      "47: Encoding Loss -2.3121869564056396, Transition Loss -13.085443496704102, Classifier Loss 0.05634234473109245, Total Loss 5.631617546081543\n",
      "47: Encoding Loss -0.5862454175949097, Transition Loss -9.446991920471191, Classifier Loss 0.04662293195724487, Total Loss 4.660404205322266\n",
      "47: Encoding Loss -0.9176086187362671, Transition Loss -5.719061374664307, Classifier Loss 0.05727842077612877, Total Loss 5.72669792175293\n",
      "47: Encoding Loss -1.9646090269088745, Transition Loss -7.684700965881348, Classifier Loss 0.04107873886823654, Total Loss 4.106337070465088\n",
      "47: Encoding Loss -1.1420862674713135, Transition Loss -8.440844535827637, Classifier Loss 0.05131144821643829, Total Loss 5.129456996917725\n",
      "47: Encoding Loss -2.259697914123535, Transition Loss -15.347546577453613, Classifier Loss 0.031922418624162674, Total Loss 3.1891725063323975\n",
      "47: Encoding Loss -2.463240623474121, Transition Loss -19.231769561767578, Classifier Loss 0.06372105330228806, Total Loss 6.368258953094482\n",
      "47: Encoding Loss -1.3802368640899658, Transition Loss -11.600041389465332, Classifier Loss 0.01950640231370926, Total Loss 1.9483201503753662\n",
      "47: Encoding Loss -0.7732232213020325, Transition Loss -7.769991874694824, Classifier Loss 0.03869345784187317, Total Loss 3.8677916526794434\n",
      "47: Encoding Loss -0.5975554585456848, Transition Loss -14.596333503723145, Classifier Loss 0.07486866414546967, Total Loss 7.483947277069092\n",
      "47: Encoding Loss -0.9961259961128235, Transition Loss -7.11287784576416, Classifier Loss 0.0465749055147171, Total Loss 4.656068325042725\n",
      "47: Encoding Loss -1.4379550218582153, Transition Loss -16.352758407592773, Classifier Loss 0.07048475742340088, Total Loss 7.045205116271973\n",
      "47: Encoding Loss -1.8279132843017578, Transition Loss -12.758623123168945, Classifier Loss 0.03693240135908127, Total Loss 3.690688371658325\n",
      "47: Encoding Loss -1.6341725587844849, Transition Loss -13.425975799560547, Classifier Loss 0.05663742125034332, Total Loss 5.661056995391846\n",
      "47: Encoding Loss -1.2393248081207275, Transition Loss -5.3915534019470215, Classifier Loss 0.07486648112535477, Total Loss 7.485569953918457\n",
      "47: Encoding Loss -2.3132197856903076, Transition Loss -13.168362617492676, Classifier Loss 0.030631447210907936, Total Loss 3.0605111122131348\n",
      "47: Encoding Loss -0.9338204860687256, Transition Loss -6.887652397155762, Classifier Loss 0.06244157999753952, Total Loss 6.2427802085876465\n",
      "47: Encoding Loss -2.5192503929138184, Transition Loss -5.523919105529785, Classifier Loss 0.0839524045586586, Total Loss 8.394136428833008\n",
      "47: Encoding Loss -2.760772943496704, Transition Loss -20.076936721801758, Classifier Loss 0.0625307708978653, Total Loss 6.249061584472656\n",
      "47: Encoding Loss -2.143812656402588, Transition Loss -10.758820533752441, Classifier Loss 0.049739547073841095, Total Loss 4.971802711486816\n",
      "47: Encoding Loss -1.1944966316223145, Transition Loss -6.9030022621154785, Classifier Loss 0.05541389435529709, Total Loss 5.540009021759033\n",
      "47: Encoding Loss -1.2892518043518066, Transition Loss -13.001742362976074, Classifier Loss 0.06044355779886246, Total Loss 6.041755676269531\n",
      "47: Encoding Loss -1.3495256900787354, Transition Loss -4.7516279220581055, Classifier Loss 0.07095980644226074, Total Loss 7.095030307769775\n",
      "47: Encoding Loss -1.0255045890808105, Transition Loss -12.556899070739746, Classifier Loss 0.05949344113469124, Total Loss 5.946832656860352\n",
      "47: Encoding Loss -0.47623756527900696, Transition Loss -7.262789249420166, Classifier Loss 0.15772953629493713, Total Loss 15.77149772644043\n",
      "47: Encoding Loss -2.1029555797576904, Transition Loss -14.202049255371094, Classifier Loss 0.03753647208213806, Total Loss 3.7508068084716797\n",
      "47: Encoding Loss -1.0978718996047974, Transition Loss -9.207097053527832, Classifier Loss 0.03983328491449356, Total Loss 3.981487274169922\n",
      "47: Encoding Loss -1.4495817422866821, Transition Loss -8.274860382080078, Classifier Loss 0.05771063640713692, Total Loss 5.769408702850342\n",
      "47: Encoding Loss -1.2395111322402954, Transition Loss -10.459147453308105, Classifier Loss 0.10281430184841156, Total Loss 10.279338836669922\n",
      "47: Encoding Loss -2.05025315284729, Transition Loss -11.884512901306152, Classifier Loss 0.05278177186846733, Total Loss 5.2758002281188965\n",
      "47: Encoding Loss -1.4530216455459595, Transition Loss -13.04784870147705, Classifier Loss 0.056077197194099426, Total Loss 5.605110168457031\n",
      "47: Encoding Loss -1.3446910381317139, Transition Loss -9.639986038208008, Classifier Loss 0.029227089136838913, Total Loss 2.920780897140503\n",
      "47: Encoding Loss -2.032925844192505, Transition Loss -17.54075813293457, Classifier Loss 0.06757277995347977, Total Loss 6.753769874572754\n",
      "47: Encoding Loss -2.1767077445983887, Transition Loss -16.924755096435547, Classifier Loss 0.028018714860081673, Total Loss 2.7984864711761475\n",
      "47: Encoding Loss -2.1768369674682617, Transition Loss -7.48956298828125, Classifier Loss 0.05208582431077957, Total Loss 5.207084655761719\n",
      "47: Encoding Loss -1.9174166917800903, Transition Loss -18.113536834716797, Classifier Loss 0.03934114798903465, Total Loss 3.9304919242858887\n",
      "47: Encoding Loss -0.5561734437942505, Transition Loss -8.877913475036621, Classifier Loss 0.06603530049324036, Total Loss 6.601754188537598\n",
      "47: Encoding Loss -0.9063252806663513, Transition Loss -18.843223571777344, Classifier Loss 0.05367767810821533, Total Loss 5.363999366760254\n",
      "47: Encoding Loss -1.7912014722824097, Transition Loss -7.504382133483887, Classifier Loss 0.027526726946234703, Total Loss 2.751171827316284\n",
      "47: Encoding Loss -2.219658136367798, Transition Loss -10.701974868774414, Classifier Loss 0.047073934227228165, Total Loss 4.7052531242370605\n",
      "47: Encoding Loss -1.5859706401824951, Transition Loss -17.036901473999023, Classifier Loss 0.04332274571061134, Total Loss 4.328866958618164\n",
      "47: Encoding Loss -2.205333709716797, Transition Loss -19.94755744934082, Classifier Loss 0.044632911682128906, Total Loss 4.459301471710205\n",
      "47: Encoding Loss -0.7490355968475342, Transition Loss -13.654520034790039, Classifier Loss 0.06161314621567726, Total Loss 6.158583641052246\n",
      "47: Encoding Loss -1.4162205457687378, Transition Loss -4.121847152709961, Classifier Loss 0.0763963907957077, Total Loss 7.638814449310303\n",
      "47: Encoding Loss -1.6391642093658447, Transition Loss -13.082106590270996, Classifier Loss 0.036267347633838654, Total Loss 3.6241183280944824\n",
      "47: Encoding Loss -1.0798373222351074, Transition Loss -12.88644790649414, Classifier Loss 0.06660360097885132, Total Loss 6.657783031463623\n",
      "47: Encoding Loss -1.772729516029358, Transition Loss -6.4155592918396, Classifier Loss 0.07702968269586563, Total Loss 7.701684951782227\n",
      "47: Encoding Loss -0.9517272114753723, Transition Loss -11.951616287231445, Classifier Loss 0.027413442730903625, Total Loss 2.7389538288116455\n",
      "47: Encoding Loss -2.191441297531128, Transition Loss -13.821812629699707, Classifier Loss 0.046253230422735214, Total Loss 4.62255859375\n",
      "47: Encoding Loss -2.3140370845794678, Transition Loss -19.761974334716797, Classifier Loss 0.03285253047943115, Total Loss 3.2813005447387695\n",
      "47: Encoding Loss -1.7112927436828613, Transition Loss -16.928348541259766, Classifier Loss 0.0554712638258934, Total Loss 5.543740749359131\n",
      "47: Encoding Loss -1.871090292930603, Transition Loss -10.145206451416016, Classifier Loss 0.054879989475011826, Total Loss 5.4859700202941895\n",
      "47: Encoding Loss -2.1623075008392334, Transition Loss -8.945467948913574, Classifier Loss 0.08273904025554657, Total Loss 8.272114753723145\n",
      "47: Encoding Loss -1.9333120584487915, Transition Loss -10.67923355102539, Classifier Loss 0.04413484409451485, Total Loss 4.411348819732666\n",
      "47: Encoding Loss -1.230679988861084, Transition Loss -8.129509925842285, Classifier Loss 0.06261847168207169, Total Loss 6.260221004486084\n",
      "47: Encoding Loss -1.4976626634597778, Transition Loss -12.150582313537598, Classifier Loss 0.05806264281272888, Total Loss 5.803834438323975\n",
      "47: Encoding Loss -2.064375400543213, Transition Loss -5.736108303070068, Classifier Loss 0.0897686779499054, Total Loss 8.975720405578613\n",
      "47: Encoding Loss -0.7479114532470703, Transition Loss -2.542886257171631, Classifier Loss 0.051532991230487823, Total Loss 5.152790546417236\n",
      "47: Encoding Loss -1.8592795133590698, Transition Loss -18.877079010009766, Classifier Loss 0.07202368974685669, Total Loss 7.1985931396484375\n",
      "47: Encoding Loss -0.5920124053955078, Transition Loss -9.716300010681152, Classifier Loss 0.06217378377914429, Total Loss 6.21543550491333\n",
      "47: Encoding Loss -1.1017518043518066, Transition Loss -9.6579008102417, Classifier Loss 0.09640620648860931, Total Loss 9.638689041137695\n",
      "47: Encoding Loss -1.8141040802001953, Transition Loss -9.389228820800781, Classifier Loss 0.06534819304943085, Total Loss 6.5329413414001465\n",
      "47: Encoding Loss -1.4618873596191406, Transition Loss -9.519011497497559, Classifier Loss 0.06537042558193207, Total Loss 6.5351386070251465\n",
      "47: Encoding Loss -0.9136762022972107, Transition Loss -6.482668399810791, Classifier Loss 0.08179452270269394, Total Loss 8.178154945373535\n",
      "47: Encoding Loss -1.5521160364151, Transition Loss -9.842781066894531, Classifier Loss 0.01943553052842617, Total Loss 1.941584587097168\n",
      "47: Encoding Loss -2.0894925594329834, Transition Loss -8.866416931152344, Classifier Loss 0.08644331246614456, Total Loss 8.642558097839355\n",
      "47: Encoding Loss -1.6164863109588623, Transition Loss -15.033475875854492, Classifier Loss 0.031077036634087563, Total Loss 3.1046969890594482\n",
      "47: Encoding Loss -1.3631861209869385, Transition Loss -7.201554775238037, Classifier Loss 0.06717409193515778, Total Loss 6.715968608856201\n",
      "47: Encoding Loss -2.6075942516326904, Transition Loss -13.080235481262207, Classifier Loss 0.05148589611053467, Total Loss 5.1459736824035645\n",
      "47: Encoding Loss -1.804727554321289, Transition Loss -14.041193008422852, Classifier Loss 0.023139841854572296, Total Loss 2.31117582321167\n",
      "47: Encoding Loss -1.6102581024169922, Transition Loss -10.632550239562988, Classifier Loss 0.09232453256845474, Total Loss 9.230326652526855\n",
      "47: Encoding Loss -1.0752798318862915, Transition Loss -11.815783500671387, Classifier Loss 0.044441141188144684, Total Loss 4.441751003265381\n",
      "47: Encoding Loss -1.6687859296798706, Transition Loss -9.470368385314941, Classifier Loss 0.04054205119609833, Total Loss 4.052310943603516\n",
      "47: Encoding Loss -1.3771573305130005, Transition Loss -6.183636665344238, Classifier Loss 0.08863462507724762, Total Loss 8.862225532531738\n",
      "47: Encoding Loss -0.6553173065185547, Transition Loss -11.765810012817383, Classifier Loss 0.07176652550697327, Total Loss 7.174299240112305\n",
      "47: Encoding Loss -2.440354347229004, Transition Loss -8.437238693237305, Classifier Loss 0.046805039048194885, Total Loss 4.678816318511963\n",
      "47: Encoding Loss -1.3989495038986206, Transition Loss -6.564788818359375, Classifier Loss 0.06386458873748779, Total Loss 6.385146141052246\n",
      "47: Encoding Loss -2.062135934829712, Transition Loss -16.138399124145508, Classifier Loss 0.055613141506910324, Total Loss 5.558086395263672\n",
      "47: Encoding Loss -1.521932601928711, Transition Loss -10.366119384765625, Classifier Loss 0.09886318445205688, Total Loss 9.884244918823242\n",
      "47: Encoding Loss -1.2218204736709595, Transition Loss -8.650869369506836, Classifier Loss 0.0447465255856514, Total Loss 4.4729228019714355\n",
      "47: Encoding Loss -1.211516261100769, Transition Loss -14.18122386932373, Classifier Loss 0.047914814203977585, Total Loss 4.788645267486572\n",
      "47: Encoding Loss -3.261655330657959, Transition Loss -21.97987174987793, Classifier Loss 0.04332952946424484, Total Loss 4.328557014465332\n",
      "47: Encoding Loss -1.1188448667526245, Transition Loss -7.697656631469727, Classifier Loss 0.07396822422742844, Total Loss 7.395282745361328\n",
      "47: Encoding Loss -1.6023014783859253, Transition Loss -8.419575691223145, Classifier Loss 0.06752748042345047, Total Loss 6.751064300537109\n",
      "47: Encoding Loss -0.9785717725753784, Transition Loss -10.287617683410645, Classifier Loss 0.05625143647193909, Total Loss 5.623085975646973\n",
      "47: Encoding Loss -1.4710280895233154, Transition Loss -4.90202522277832, Classifier Loss 0.06666676700115204, Total Loss 6.665696144104004\n",
      "47: Encoding Loss -1.8362332582473755, Transition Loss -13.774158477783203, Classifier Loss 0.06591654568910599, Total Loss 6.588900089263916\n",
      "47: Encoding Loss -1.593530297279358, Transition Loss -16.584945678710938, Classifier Loss 0.06514561921358109, Total Loss 6.511245250701904\n",
      "47: Encoding Loss -1.8517063856124878, Transition Loss -13.652420043945312, Classifier Loss 0.047467056661844254, Total Loss 4.7439751625061035\n",
      "47: Encoding Loss -1.1849478483200073, Transition Loss -5.318270206451416, Classifier Loss 0.03119029477238655, Total Loss 3.1179659366607666\n",
      "47: Encoding Loss -1.8845268487930298, Transition Loss -8.02180290222168, Classifier Loss 0.09235633909702301, Total Loss 9.234029769897461\n",
      "47: Encoding Loss -2.1816391944885254, Transition Loss -14.928163528442383, Classifier Loss 0.05462443083524704, Total Loss 5.4594573974609375\n",
      "47: Encoding Loss -0.9546222686767578, Transition Loss -0.7139255404472351, Classifier Loss 0.09620113670825958, Total Loss 9.619970321655273\n",
      "47: Encoding Loss -1.081911325454712, Transition Loss -8.885127067565918, Classifier Loss 0.06546004116535187, Total Loss 6.544227123260498\n",
      "47: Encoding Loss -0.6555385589599609, Transition Loss -5.475426197052002, Classifier Loss 0.06890635192394257, Total Loss 6.88953971862793\n",
      "47: Encoding Loss -1.2203072309494019, Transition Loss -12.288309097290039, Classifier Loss 0.043491803109645844, Total Loss 4.346722602844238\n",
      "47: Encoding Loss -2.116145133972168, Transition Loss -15.973756790161133, Classifier Loss 0.07325880229473114, Total Loss 7.322685241699219\n",
      "47: Encoding Loss -1.2733899354934692, Transition Loss -5.534899711608887, Classifier Loss 0.06865795701742172, Total Loss 6.864688396453857\n",
      "47: Encoding Loss -1.0313725471496582, Transition Loss -9.538028717041016, Classifier Loss 0.0400766059756279, Total Loss 4.0057525634765625\n",
      "47: Encoding Loss -1.497205138206482, Transition Loss -9.429374694824219, Classifier Loss 0.046057164669036865, Total Loss 4.603830814361572\n",
      "47: Encoding Loss -1.0361053943634033, Transition Loss -14.50165843963623, Classifier Loss 0.04849397763609886, Total Loss 4.846497535705566\n",
      "47: Encoding Loss -1.3400156497955322, Transition Loss -14.112556457519531, Classifier Loss 0.06198748201131821, Total Loss 6.195925712585449\n",
      "47: Encoding Loss -1.076272964477539, Transition Loss -2.57639217376709, Classifier Loss 0.036132827401161194, Total Loss 3.612767457962036\n",
      "47: Encoding Loss -1.3787493705749512, Transition Loss -17.79581069946289, Classifier Loss 0.034873105585575104, Total Loss 3.4837515354156494\n",
      "47: Encoding Loss -1.9760929346084595, Transition Loss -17.272192001342773, Classifier Loss 0.030619047582149506, Total Loss 3.058450222015381\n",
      "47: Encoding Loss -0.9070139527320862, Transition Loss -1.7502963542938232, Classifier Loss 0.050810690969228745, Total Loss 5.080718994140625\n",
      "47: Encoding Loss -0.6541173458099365, Transition Loss 0.753997266292572, Classifier Loss 0.05423491448163986, Total Loss 5.574290752410889\n",
      "47: Encoding Loss -0.8109506368637085, Transition Loss -12.055689811706543, Classifier Loss 0.03491484373807907, Total Loss 3.4890732765197754\n",
      "47: Encoding Loss -1.3378287553787231, Transition Loss -11.28111457824707, Classifier Loss 0.045853860676288605, Total Loss 4.5831298828125\n",
      "47: Encoding Loss -2.2570457458496094, Transition Loss -13.379515647888184, Classifier Loss 0.04993588849902153, Total Loss 4.990912914276123\n",
      "47: Encoding Loss -1.292850136756897, Transition Loss -7.039797782897949, Classifier Loss 0.05502656474709511, Total Loss 5.501248359680176\n",
      "47: Encoding Loss -2.585283041000366, Transition Loss -15.471535682678223, Classifier Loss 0.036203764379024506, Total Loss 3.6172821521759033\n",
      "47: Encoding Loss -1.0727684497833252, Transition Loss -9.176694869995117, Classifier Loss 0.06561792641878128, Total Loss 6.559957504272461\n",
      "47: Encoding Loss -1.4901152849197388, Transition Loss -12.836813926696777, Classifier Loss 0.08376175165176392, Total Loss 8.373607635498047\n",
      "47: Encoding Loss -2.605097532272339, Transition Loss -19.807819366455078, Classifier Loss 0.04801010340452194, Total Loss 4.797048568725586\n",
      "47: Encoding Loss -2.019582986831665, Transition Loss -9.170586585998535, Classifier Loss 0.0630606859922409, Total Loss 6.304234504699707\n",
      "47: Encoding Loss -1.8851479291915894, Transition Loss -10.496115684509277, Classifier Loss 0.08945953845977783, Total Loss 8.943854331970215\n",
      "47: Encoding Loss -1.7975375652313232, Transition Loss -15.93008041381836, Classifier Loss 0.07223597913980484, Total Loss 7.220411777496338\n",
      "47: Encoding Loss -1.835395097732544, Transition Loss -12.827163696289062, Classifier Loss 0.024971049278974533, Total Loss 2.494539499282837\n",
      "47: Encoding Loss -0.7956330180168152, Transition Loss -11.80296802520752, Classifier Loss 0.05386968329548836, Total Loss 5.384607315063477\n",
      "47: Encoding Loss -1.7036497592926025, Transition Loss -5.534252166748047, Classifier Loss 0.04014592990279198, Total Loss 4.013486385345459\n",
      "47: Encoding Loss -0.9873257875442505, Transition Loss -9.109491348266602, Classifier Loss 0.05852816253900528, Total Loss 5.850994110107422\n",
      "47: Encoding Loss -1.5960794687271118, Transition Loss -9.752721786499023, Classifier Loss 0.05133146047592163, Total Loss 5.131195545196533\n",
      "47: Encoding Loss -0.9579200744628906, Transition Loss -5.447465896606445, Classifier Loss 0.02978084422647953, Total Loss 2.976994752883911\n",
      "47: Encoding Loss -1.1647346019744873, Transition Loss -10.587390899658203, Classifier Loss 0.02619110606610775, Total Loss 2.6169931888580322\n",
      "47: Encoding Loss -1.2467600107192993, Transition Loss -12.761075973510742, Classifier Loss 0.06979266554117203, Total Loss 6.976714611053467\n",
      "47: Encoding Loss -0.338728129863739, Transition Loss -12.644608497619629, Classifier Loss 0.05227581411600113, Total Loss 5.224095821380615\n",
      "47: Encoding Loss -1.265440583229065, Transition Loss -1.4752559661865234, Classifier Loss 0.08089465647935867, Total Loss 8.089171409606934\n",
      "47: Encoding Loss -1.1469115018844604, Transition Loss -9.128471374511719, Classifier Loss 0.03381139039993286, Total Loss 3.3793132305145264\n",
      "47: Encoding Loss -0.7915830016136169, Transition Loss -11.510369300842285, Classifier Loss 0.06816229969263077, Total Loss 6.81392765045166\n",
      "47: Encoding Loss -0.40164652466773987, Transition Loss -10.319483757019043, Classifier Loss 0.046799369156360626, Total Loss 4.677778244018555\n",
      "47: Encoding Loss -0.9314478635787964, Transition Loss -9.836699485778809, Classifier Loss 0.04581253603100777, Total Loss 4.579286098480225\n",
      "47: Encoding Loss -2.16575288772583, Transition Loss -5.826079368591309, Classifier Loss 0.0712122991681099, Total Loss 7.120064735412598\n",
      "47: Encoding Loss -2.2402074337005615, Transition Loss -14.627327919006348, Classifier Loss 0.033906713128089905, Total Loss 3.3877458572387695\n",
      "47: Encoding Loss -0.20699039101600647, Transition Loss -2.165125608444214, Classifier Loss 0.04831333085894585, Total Loss 4.799055576324463\n",
      "47: Encoding Loss -0.8479258418083191, Transition Loss -8.352896690368652, Classifier Loss 0.02884165197610855, Total Loss 2.8824946880340576\n",
      "47: Encoding Loss -0.11542919278144836, Transition Loss -7.655600070953369, Classifier Loss 0.08082275092601776, Total Loss 7.966063022613525\n",
      "47: Encoding Loss -0.2363811433315277, Transition Loss -15.508781433105469, Classifier Loss 0.14362788200378418, Total Loss 14.342584609985352\n",
      "47: Encoding Loss -2.1357815265655518, Transition Loss -15.589731216430664, Classifier Loss 0.06486472487449646, Total Loss 6.483354568481445\n",
      "47: Encoding Loss -2.0275654792785645, Transition Loss -20.69333267211914, Classifier Loss 0.035974372178316116, Total Loss 3.5932984352111816\n",
      "47: Encoding Loss -1.0108000040054321, Transition Loss -11.27778434753418, Classifier Loss 0.08549834042787552, Total Loss 8.547578811645508\n",
      "47: Encoding Loss -1.8170247077941895, Transition Loss -12.65477466583252, Classifier Loss 0.04519685357809067, Total Loss 4.517154216766357\n",
      "47: Encoding Loss -2.086179733276367, Transition Loss -18.53639030456543, Classifier Loss 0.047631751745939255, Total Loss 4.759467601776123\n",
      "47: Encoding Loss -1.5685114860534668, Transition Loss -7.278859615325928, Classifier Loss 0.0770459696650505, Total Loss 7.703141212463379\n",
      "47: Encoding Loss -0.920457124710083, Transition Loss -15.073577880859375, Classifier Loss 0.07516156136989594, Total Loss 7.513141632080078\n",
      "47: Encoding Loss -1.266952633857727, Transition Loss -16.09940528869629, Classifier Loss 0.019204188138246536, Total Loss 1.9171990156173706\n",
      "47: Encoding Loss -1.3708521127700806, Transition Loss -18.798358917236328, Classifier Loss 0.052528031170368195, Total Loss 5.2490434646606445\n",
      "47: Encoding Loss -2.35198712348938, Transition Loss -24.217330932617188, Classifier Loss 0.015767285600304604, Total Loss 1.571885108947754\n",
      "48: Encoding Loss -0.6800480484962463, Transition Loss -5.226187705993652, Classifier Loss 0.030057165771722794, Total Loss 3.004671335220337\n",
      "48: Encoding Loss -1.6320754289627075, Transition Loss -6.420546531677246, Classifier Loss 0.08647428452968597, Total Loss 8.64614486694336\n",
      "48: Encoding Loss -1.5076435804367065, Transition Loss -1.6288974285125732, Classifier Loss 0.03315946087241173, Total Loss 3.3156204223632812\n",
      "48: Encoding Loss -1.2758129835128784, Transition Loss -18.043073654174805, Classifier Loss 0.049068570137023926, Total Loss 4.903248310089111\n",
      "48: Encoding Loss -1.1755095720291138, Transition Loss -9.479483604431152, Classifier Loss 0.042292311787605286, Total Loss 4.227335453033447\n",
      "48: Encoding Loss -1.3361835479736328, Transition Loss -14.681868553161621, Classifier Loss 0.042281024158000946, Total Loss 4.225165843963623\n",
      "48: Encoding Loss -1.7645357847213745, Transition Loss -11.072733879089355, Classifier Loss 0.05145757272839546, Total Loss 5.143542766571045\n",
      "48: Encoding Loss -0.8722395300865173, Transition Loss -8.242223739624023, Classifier Loss 0.04391419515013695, Total Loss 4.389770984649658\n",
      "48: Encoding Loss -0.8462493419647217, Transition Loss -14.972071647644043, Classifier Loss 0.03316159173846245, Total Loss 3.3131649494171143\n",
      "48: Encoding Loss -1.8801581859588623, Transition Loss -12.40589427947998, Classifier Loss 0.05445423722267151, Total Loss 5.4429426193237305\n",
      "48: Encoding Loss -0.35913679003715515, Transition Loss -9.066425323486328, Classifier Loss 0.04761630296707153, Total Loss 4.759344100952148\n",
      "48: Encoding Loss -0.46032607555389404, Transition Loss -5.20429801940918, Classifier Loss 0.057729050517082214, Total Loss 5.771856307983398\n",
      "48: Encoding Loss -1.2031556367874146, Transition Loss -7.447648048400879, Classifier Loss 0.040324755012989044, Total Loss 4.0309858322143555\n",
      "48: Encoding Loss -0.7101537585258484, Transition Loss -8.155593872070312, Classifier Loss 0.05095815658569336, Total Loss 5.094184398651123\n",
      "48: Encoding Loss -1.5293625593185425, Transition Loss -14.510311126708984, Classifier Loss 0.03092445619404316, Total Loss 3.089543581008911\n",
      "48: Encoding Loss -1.907084345817566, Transition Loss -17.991159439086914, Classifier Loss 0.059605516493320465, Total Loss 5.956953525543213\n",
      "48: Encoding Loss -1.074940800666809, Transition Loss -11.139084815979004, Classifier Loss 0.01860085502266884, Total Loss 1.8578577041625977\n",
      "48: Encoding Loss -0.5841104984283447, Transition Loss -7.686649322509766, Classifier Loss 0.03994353488087654, Total Loss 3.992816209793091\n",
      "48: Encoding Loss -0.3452672064304352, Transition Loss -13.6893892288208, Classifier Loss 0.07381026446819305, Total Loss 7.377521514892578\n",
      "48: Encoding Loss -0.5174852013587952, Transition Loss -6.74841833114624, Classifier Loss 0.04708584025502205, Total Loss 4.707233905792236\n",
      "48: Encoding Loss -0.8176249861717224, Transition Loss -15.488462448120117, Classifier Loss 0.07088039070367813, Total Loss 7.084941387176514\n",
      "48: Encoding Loss -1.4779918193817139, Transition Loss -12.097966194152832, Classifier Loss 0.03594609349966049, Total Loss 3.5921895503997803\n",
      "48: Encoding Loss -1.1757606267929077, Transition Loss -12.795198440551758, Classifier Loss 0.055392876267433167, Total Loss 5.536728382110596\n",
      "48: Encoding Loss -0.7893542647361755, Transition Loss -4.985443115234375, Classifier Loss 0.06949112564325333, Total Loss 6.948115348815918\n",
      "48: Encoding Loss -1.6053767204284668, Transition Loss -12.56412124633789, Classifier Loss 0.02971484512090683, Total Loss 2.9689714908599854\n",
      "48: Encoding Loss -0.14959992468357086, Transition Loss -6.490528106689453, Classifier Loss 0.061271585524082184, Total Loss 6.045283794403076\n",
      "48: Encoding Loss -1.8909599781036377, Transition Loss -5.072887420654297, Classifier Loss 0.08280374854803085, Total Loss 8.279359817504883\n",
      "48: Encoding Loss -1.8962584733963013, Transition Loss -17.76117706298828, Classifier Loss 0.06040281057357788, Total Loss 6.036728858947754\n",
      "48: Encoding Loss -1.1427518129348755, Transition Loss -9.431965827941895, Classifier Loss 0.05031149089336395, Total Loss 5.029262542724609\n",
      "48: Encoding Loss -0.3842611014842987, Transition Loss -6.234735488891602, Classifier Loss 0.05545535683631897, Total Loss 5.544101715087891\n",
      "48: Encoding Loss -0.646797239780426, Transition Loss -11.766361236572266, Classifier Loss 0.05935592204332352, Total Loss 5.933238983154297\n",
      "48: Encoding Loss -1.0791058540344238, Transition Loss -4.098310470581055, Classifier Loss 0.07082775235176086, Total Loss 7.081955432891846\n",
      "48: Encoding Loss -0.4800640642642975, Transition Loss -11.255393028259277, Classifier Loss 0.05678001791238785, Total Loss 5.675747871398926\n",
      "48: Encoding Loss 0.11112210154533386, Transition Loss -6.152860641479492, Classifier Loss 0.1558103859424591, Total Loss 16.350339889526367\n",
      "48: Encoding Loss -0.15384884178638458, Transition Loss -20.19480323791504, Classifier Loss 0.042681314051151276, Total Loss 4.187827110290527\n",
      "48: Encoding Loss 2.5589354038238525, Transition Loss -16.06987953186035, Classifier Loss 0.03569517657160759, Total Loss 24.03778648376465\n",
      "48: Encoding Loss 2.0214860439300537, Transition Loss -6.259343147277832, Classifier Loss 0.06826949864625931, Total Loss 22.997587203979492\n",
      "48: Encoding Loss 0.30247530341148376, Transition Loss -11.4465970993042, Classifier Loss 0.10793062299489975, Total Loss 13.207564353942871\n",
      "48: Encoding Loss -1.6260932683944702, Transition Loss -11.7671537399292, Classifier Loss 0.05638895183801651, Total Loss 5.636541366577148\n",
      "48: Encoding Loss -1.128852367401123, Transition Loss -12.927093505859375, Classifier Loss 0.05492563545703888, Total Loss 5.489978313446045\n",
      "48: Encoding Loss -0.9952175617218018, Transition Loss -9.519519805908203, Classifier Loss 0.028898900374770164, Total Loss 2.887985944747925\n",
      "48: Encoding Loss -1.4685255289077759, Transition Loss -17.33188819885254, Classifier Loss 0.06643857061862946, Total Loss 6.640390396118164\n",
      "48: Encoding Loss -1.6401114463806152, Transition Loss -16.76572608947754, Classifier Loss 0.027111267670989037, Total Loss 2.7077736854553223\n",
      "48: Encoding Loss -1.898735523223877, Transition Loss -7.396033763885498, Classifier Loss 0.05324596166610718, Total Loss 5.323117256164551\n",
      "48: Encoding Loss -1.5258898735046387, Transition Loss -17.920757293701172, Classifier Loss 0.038647912442684174, Total Loss 3.8612070083618164\n",
      "48: Encoding Loss -0.19786427915096283, Transition Loss -8.83797550201416, Classifier Loss 0.06490187346935272, Total Loss 6.450543403625488\n",
      "48: Encoding Loss -1.0735772848129272, Transition Loss -18.697895050048828, Classifier Loss 0.053030431270599365, Total Loss 5.299304008483887\n",
      "48: Encoding Loss -1.5901710987091064, Transition Loss -7.556303977966309, Classifier Loss 0.028603902086615562, Total Loss 2.8588788509368896\n",
      "48: Encoding Loss -1.7990739345550537, Transition Loss -10.71471118927002, Classifier Loss 0.04712538421154022, Total Loss 4.710395336151123\n",
      "48: Encoding Loss -1.2483675479888916, Transition Loss -17.022655487060547, Classifier Loss 0.04200128838419914, Total Loss 4.1967244148254395\n",
      "48: Encoding Loss -1.8426604270935059, Transition Loss -20.003080368041992, Classifier Loss 0.042824093252420425, Total Loss 4.278408527374268\n",
      "48: Encoding Loss -0.6282592415809631, Transition Loss -13.724861145019531, Classifier Loss 0.0610751174390316, Total Loss 6.104766368865967\n",
      "48: Encoding Loss -0.7598121166229248, Transition Loss -4.2550482749938965, Classifier Loss 0.07830698788166046, Total Loss 7.829847812652588\n",
      "48: Encoding Loss -1.1377073526382446, Transition Loss -13.092926025390625, Classifier Loss 0.03651271015405655, Total Loss 3.6486525535583496\n",
      "48: Encoding Loss -0.7304394841194153, Transition Loss -12.850898742675781, Classifier Loss 0.06834195554256439, Total Loss 6.831625461578369\n",
      "48: Encoding Loss -1.5074495077133179, Transition Loss -6.444547653198242, Classifier Loss 0.075804203748703, Total Loss 7.579131603240967\n",
      "48: Encoding Loss -0.7150894403457642, Transition Loss -11.886082649230957, Classifier Loss 0.027714205905795097, Total Loss 2.769043207168579\n",
      "48: Encoding Loss -1.7955455780029297, Transition Loss -13.85590934753418, Classifier Loss 0.047747015953063965, Total Loss 4.77193021774292\n",
      "48: Encoding Loss -1.7693337202072144, Transition Loss -19.693626403808594, Classifier Loss 0.03320790454745293, Total Loss 3.316851854324341\n",
      "48: Encoding Loss -1.3980127573013306, Transition Loss -16.957992553710938, Classifier Loss 0.053136926144361496, Total Loss 5.310300827026367\n",
      "48: Encoding Loss -1.162761926651001, Transition Loss -10.23608112335205, Classifier Loss 0.05437042564153671, Total Loss 5.434995651245117\n",
      "48: Encoding Loss -1.6527655124664307, Transition Loss -9.048225402832031, Classifier Loss 0.08446142077445984, Total Loss 8.444332122802734\n",
      "48: Encoding Loss -1.6799870729446411, Transition Loss -10.844836235046387, Classifier Loss 0.04476655274629593, Total Loss 4.474486351013184\n",
      "48: Encoding Loss -0.9905902743339539, Transition Loss -8.213143348693848, Classifier Loss 0.06203263998031616, Total Loss 6.2016215324401855\n",
      "48: Encoding Loss -1.0457792282104492, Transition Loss -12.21162223815918, Classifier Loss 0.057815633714199066, Total Loss 5.779120922088623\n",
      "48: Encoding Loss -1.7849833965301514, Transition Loss -5.743179798126221, Classifier Loss 0.08907409012317657, Total Loss 8.90626049041748\n",
      "48: Encoding Loss -0.4649061858654022, Transition Loss -2.417236804962158, Classifier Loss 0.051389023661613464, Total Loss 5.1384124755859375\n",
      "48: Encoding Loss -1.2998095750808716, Transition Loss -18.790454864501953, Classifier Loss 0.0703415721654892, Total Loss 7.030399322509766\n",
      "48: Encoding Loss -0.34386226534843445, Transition Loss -9.71603012084961, Classifier Loss 0.0622137114405632, Total Loss 6.218624114990234\n",
      "48: Encoding Loss -0.6467324495315552, Transition Loss -9.647871971130371, Classifier Loss 0.09724585711956024, Total Loss 9.72265625\n",
      "48: Encoding Loss -1.3731762170791626, Transition Loss -9.284637451171875, Classifier Loss 0.06377537548542023, Total Loss 6.375680923461914\n",
      "48: Encoding Loss -1.1030336618423462, Transition Loss -9.467220306396484, Classifier Loss 0.06475578248500824, Total Loss 6.473684787750244\n",
      "48: Encoding Loss -0.5040159225463867, Transition Loss -6.268631458282471, Classifier Loss 0.08010454475879669, Total Loss 8.009199142456055\n",
      "48: Encoding Loss -1.2488642930984497, Transition Loss -9.874692916870117, Classifier Loss 0.0189876276999712, Total Loss 1.8967878818511963\n",
      "48: Encoding Loss -1.8864954710006714, Transition Loss -8.811562538146973, Classifier Loss 0.08720914274454117, Total Loss 8.719151496887207\n",
      "48: Encoding Loss -1.2187167406082153, Transition Loss -15.00610637664795, Classifier Loss 0.030823782086372375, Total Loss 3.0793769359588623\n",
      "48: Encoding Loss -1.0245391130447388, Transition Loss -7.177515029907227, Classifier Loss 0.06530380994081497, Total Loss 6.5289459228515625\n",
      "48: Encoding Loss -2.2551004886627197, Transition Loss -13.133842468261719, Classifier Loss 0.05075596272945404, Total Loss 5.072969436645508\n",
      "48: Encoding Loss -1.393954873085022, Transition Loss -14.05273723602295, Classifier Loss 0.024493709206581116, Total Loss 2.4465603828430176\n",
      "48: Encoding Loss -1.4331161975860596, Transition Loss -10.709659576416016, Classifier Loss 0.09210722148418427, Total Loss 9.208580017089844\n",
      "48: Encoding Loss -0.9034033417701721, Transition Loss -11.927810668945312, Classifier Loss 0.04591047763824463, Total Loss 4.588662147521973\n",
      "48: Encoding Loss -1.407275915145874, Transition Loss -9.426304817199707, Classifier Loss 0.03968245908617973, Total Loss 3.9663608074188232\n",
      "48: Encoding Loss -1.1361883878707886, Transition Loss -6.41018533706665, Classifier Loss 0.0843089297413826, Total Loss 8.429611206054688\n",
      "48: Encoding Loss -0.44646868109703064, Transition Loss -11.724224090576172, Classifier Loss 0.0693020150065422, Total Loss 6.927842617034912\n",
      "48: Encoding Loss -2.2140374183654785, Transition Loss -8.578422546386719, Classifier Loss 0.04703722894191742, Total Loss 4.702007293701172\n",
      "48: Encoding Loss -1.1704151630401611, Transition Loss -6.590869903564453, Classifier Loss 0.06587085872888565, Total Loss 6.58576774597168\n",
      "48: Encoding Loss -1.741060495376587, Transition Loss -16.139636993408203, Classifier Loss 0.05387283116579056, Total Loss 5.3840556144714355\n",
      "48: Encoding Loss -1.1996691226959229, Transition Loss -10.375767707824707, Classifier Loss 0.09417906403541565, Total Loss 9.415831565856934\n",
      "48: Encoding Loss -1.0518081188201904, Transition Loss -8.583821296691895, Classifier Loss 0.04647693783044815, Total Loss 4.645977020263672\n",
      "48: Encoding Loss -0.8600119352340698, Transition Loss -13.999574661254883, Classifier Loss 0.04641631618142128, Total Loss 4.638831615447998\n",
      "48: Encoding Loss -2.7443888187408447, Transition Loss -21.843050003051758, Classifier Loss 0.04311143979430199, Total Loss 4.306775093078613\n",
      "48: Encoding Loss -0.8814522624015808, Transition Loss -7.626743316650391, Classifier Loss 0.07433688640594482, Total Loss 7.432163238525391\n",
      "48: Encoding Loss -1.0427087545394897, Transition Loss -8.354696273803711, Classifier Loss 0.06704265624284744, Total Loss 6.702594757080078\n",
      "48: Encoding Loss -0.6752955913543701, Transition Loss -10.348432540893555, Classifier Loss 0.05549133941531181, Total Loss 5.547064304351807\n",
      "48: Encoding Loss -1.1515692472457886, Transition Loss -4.8949737548828125, Classifier Loss 0.06487293541431427, Total Loss 6.48631477355957\n",
      "48: Encoding Loss -1.5501447916030884, Transition Loss -13.726604461669922, Classifier Loss 0.06474219262599945, Total Loss 6.4714741706848145\n",
      "48: Encoding Loss -1.295237421989441, Transition Loss -16.414892196655273, Classifier Loss 0.06450435519218445, Total Loss 6.447152614593506\n",
      "48: Encoding Loss -1.4915329217910767, Transition Loss -13.503829956054688, Classifier Loss 0.04699142649769783, Total Loss 4.696441650390625\n",
      "48: Encoding Loss -0.9614359736442566, Transition Loss -5.094209671020508, Classifier Loss 0.03031303733587265, Total Loss 3.030284881591797\n",
      "48: Encoding Loss -1.5882843732833862, Transition Loss -7.873880386352539, Classifier Loss 0.09042875468730927, Total Loss 9.041300773620605\n",
      "48: Encoding Loss -1.8413406610488892, Transition Loss -14.899298667907715, Classifier Loss 0.053603485226631165, Total Loss 5.3573689460754395\n",
      "48: Encoding Loss -0.6640176177024841, Transition Loss -0.7692240476608276, Classifier Loss 0.09382912516593933, Total Loss 9.382759094238281\n",
      "48: Encoding Loss -0.8918447494506836, Transition Loss -8.834283828735352, Classifier Loss 0.06531617790460587, Total Loss 6.529850959777832\n",
      "48: Encoding Loss -0.2952222228050232, Transition Loss -5.422802925109863, Classifier Loss 0.06738177686929703, Total Loss 6.733367919921875\n",
      "48: Encoding Loss -0.8422176241874695, Transition Loss -12.312087059020996, Classifier Loss 0.04379243403673172, Total Loss 4.376780986785889\n",
      "48: Encoding Loss -1.8320443630218506, Transition Loss -16.028589248657227, Classifier Loss 0.0623805969953537, Total Loss 6.234853744506836\n",
      "48: Encoding Loss -0.9299086928367615, Transition Loss -5.527946472167969, Classifier Loss 0.06754137575626373, Total Loss 6.7530317306518555\n",
      "48: Encoding Loss -0.6727473139762878, Transition Loss -9.468864440917969, Classifier Loss 0.03962647542357445, Total Loss 3.9607536792755127\n",
      "48: Encoding Loss -1.1594538688659668, Transition Loss -9.41130256652832, Classifier Loss 0.04590612277388573, Total Loss 4.588730335235596\n",
      "48: Encoding Loss -0.6655585765838623, Transition Loss -14.580698013305664, Classifier Loss 0.04899018630385399, Total Loss 4.896102428436279\n",
      "48: Encoding Loss -1.0059125423431396, Transition Loss -14.171363830566406, Classifier Loss 0.06004688888788223, Total Loss 6.001854419708252\n",
      "48: Encoding Loss -0.6975913643836975, Transition Loss -2.611387252807617, Classifier Loss 0.03673556447029114, Total Loss 3.6730339527130127\n",
      "48: Encoding Loss -1.0258382558822632, Transition Loss -17.954971313476562, Classifier Loss 0.034892335534095764, Total Loss 3.485642433166504\n",
      "48: Encoding Loss -1.7423768043518066, Transition Loss -17.364646911621094, Classifier Loss 0.029865164309740067, Total Loss 2.9830434322357178\n",
      "48: Encoding Loss -0.6368674039840698, Transition Loss -1.8050646781921387, Classifier Loss 0.05060071870684624, Total Loss 5.05971097946167\n",
      "48: Encoding Loss -0.35236379504203796, Transition Loss 0.7406628131866455, Classifier Loss 0.05241267383098602, Total Loss 5.388799667358398\n",
      "48: Encoding Loss -0.6843901872634888, Transition Loss -12.23208999633789, Classifier Loss 0.034408409148454666, Total Loss 3.438394546508789\n",
      "48: Encoding Loss -1.0926536321640015, Transition Loss -11.490074157714844, Classifier Loss 0.044670864939689636, Total Loss 4.464788436889648\n",
      "48: Encoding Loss -1.8607312440872192, Transition Loss -13.474810600280762, Classifier Loss 0.05023924261331558, Total Loss 5.021229267120361\n",
      "48: Encoding Loss -0.917367160320282, Transition Loss -7.141772270202637, Classifier Loss 0.053744420409202576, Total Loss 5.373013973236084\n",
      "48: Encoding Loss -2.2033851146698, Transition Loss -15.65145206451416, Classifier Loss 0.03659173101186752, Total Loss 3.6560428142547607\n",
      "48: Encoding Loss -0.7663072943687439, Transition Loss -9.204475402832031, Classifier Loss 0.06424105912446976, Total Loss 6.42226505279541\n",
      "48: Encoding Loss -1.1314913034439087, Transition Loss -12.856531143188477, Classifier Loss 0.08590807020664215, Total Loss 8.588235855102539\n",
      "48: Encoding Loss -2.1047580242156982, Transition Loss -19.968955993652344, Classifier Loss 0.04893503710627556, Total Loss 4.889509677886963\n",
      "48: Encoding Loss -1.733445405960083, Transition Loss -9.218780517578125, Classifier Loss 0.06076745316386223, Total Loss 6.074901580810547\n",
      "48: Encoding Loss -1.5439540147781372, Transition Loss -10.712937355041504, Classifier Loss 0.08786047250032425, Total Loss 8.783904075622559\n",
      "48: Encoding Loss -1.465542197227478, Transition Loss -16.21088218688965, Classifier Loss 0.07008137553930283, Total Loss 7.0048956871032715\n",
      "48: Encoding Loss -1.7834886312484741, Transition Loss -12.986959457397461, Classifier Loss 0.02539427950978279, Total Loss 2.5368306636810303\n",
      "48: Encoding Loss -0.5768703818321228, Transition Loss -12.022651672363281, Classifier Loss 0.053710099309682846, Total Loss 5.368605136871338\n",
      "48: Encoding Loss -1.6160026788711548, Transition Loss -5.687383651733398, Classifier Loss 0.039509691298007965, Total Loss 3.94983172416687\n",
      "48: Encoding Loss -0.8700879216194153, Transition Loss -9.223549842834473, Classifier Loss 0.053702984005212784, Total Loss 5.368453502655029\n",
      "48: Encoding Loss -1.3520424365997314, Transition Loss -9.897697448730469, Classifier Loss 0.05040895938873291, Total Loss 5.03891658782959\n",
      "48: Encoding Loss -0.9727199077606201, Transition Loss -5.623047828674316, Classifier Loss 0.030070282518863678, Total Loss 3.005903720855713\n",
      "48: Encoding Loss -1.0277796983718872, Transition Loss -10.835200309753418, Classifier Loss 0.026364052668213844, Total Loss 2.6342382431030273\n",
      "48: Encoding Loss -1.124013066291809, Transition Loss -12.982288360595703, Classifier Loss 0.06599122285842896, Total Loss 6.5965256690979\n",
      "48: Encoding Loss -0.2890162765979767, Transition Loss -12.811735153198242, Classifier Loss 0.0517776682972908, Total Loss 5.170753002166748\n",
      "48: Encoding Loss -1.1716281175613403, Transition Loss -1.585284948348999, Classifier Loss 0.0792454406619072, Total Loss 7.924226760864258\n",
      "48: Encoding Loss -1.0832265615463257, Transition Loss -9.205833435058594, Classifier Loss 0.03273019567131996, Total Loss 3.2711784839630127\n",
      "48: Encoding Loss -0.7350675463676453, Transition Loss -11.767596244812012, Classifier Loss 0.06879188120365143, Total Loss 6.876834392547607\n",
      "48: Encoding Loss -0.2209005057811737, Transition Loss -10.390207290649414, Classifier Loss 0.04579480364918709, Total Loss 4.553390979766846\n",
      "48: Encoding Loss -0.6781752109527588, Transition Loss -9.909812927246094, Classifier Loss 0.045636411756277084, Total Loss 4.561659336090088\n",
      "48: Encoding Loss -1.9289432764053345, Transition Loss -5.864855766296387, Classifier Loss 0.0706610456109047, Total Loss 7.064931392669678\n",
      "48: Encoding Loss -1.8195583820343018, Transition Loss -14.651988983154297, Classifier Loss 0.03229966387152672, Total Loss 3.2270359992980957\n",
      "48: Encoding Loss 0.07794532179832458, Transition Loss -2.3367857933044434, Classifier Loss 0.047200631350278854, Total Loss 5.207311630249023\n",
      "48: Encoding Loss -0.5442215800285339, Transition Loss -8.608421325683594, Classifier Loss 0.02825026772916317, Total Loss 2.8233048915863037\n",
      "48: Encoding Loss 0.38770997524261475, Transition Loss -8.13996410369873, Classifier Loss 0.08348885923624039, Total Loss 11.448773384094238\n",
      "48: Encoding Loss -0.2833978235721588, Transition Loss -16.425390243530273, Classifier Loss 0.11023525148630142, Total Loss 11.01502799987793\n",
      "48: Encoding Loss -0.3258613646030426, Transition Loss -15.70418930053711, Classifier Loss 0.07452242821455002, Total Loss 7.447642803192139\n",
      "48: Encoding Loss -0.2185034602880478, Transition Loss -21.06159210205078, Classifier Loss 0.04231622815132141, Total Loss 4.202163219451904\n",
      "48: Encoding Loss 0.46739843487739563, Transition Loss -11.124677658081055, Classifier Loss 0.09570594131946564, Total Loss 13.307551383972168\n",
      "48: Encoding Loss -2.155109167098999, Transition Loss -13.962732315063477, Classifier Loss 0.04076089337468147, Total Loss 4.073297023773193\n",
      "48: Encoding Loss -2.5244500637054443, Transition Loss -20.19594383239746, Classifier Loss 0.047718070447444916, Total Loss 4.767767906188965\n",
      "48: Encoding Loss -1.8187248706817627, Transition Loss -8.272167205810547, Classifier Loss 0.08066418766975403, Total Loss 8.064764022827148\n",
      "48: Encoding Loss -1.3504406213760376, Transition Loss -16.868518829345703, Classifier Loss 0.07635214924812317, Total Loss 7.63184118270874\n",
      "48: Encoding Loss -1.7703253030776978, Transition Loss -17.69491958618164, Classifier Loss 0.022163214161992073, Total Loss 2.212782382965088\n",
      "48: Encoding Loss -1.5403087139129639, Transition Loss -20.780078887939453, Classifier Loss 0.05442730337381363, Total Loss 5.438574314117432\n",
      "48: Encoding Loss -2.5777742862701416, Transition Loss -26.761093139648438, Classifier Loss 0.015178022906184196, Total Loss 1.5124499797821045\n",
      "49: Encoding Loss -1.1462550163269043, Transition Loss -6.445847511291504, Classifier Loss 0.028093328699469566, Total Loss 2.8080437183380127\n",
      "49: Encoding Loss -1.57892906665802, Transition Loss -7.200311660766602, Classifier Loss 0.08843142539262772, Total Loss 8.841702461242676\n",
      "49: Encoding Loss -1.5045497417449951, Transition Loss -2.2899580001831055, Classifier Loss 0.031904980540275574, Total Loss 3.190040111541748\n",
      "49: Encoding Loss -1.45969820022583, Transition Loss -19.77741050720215, Classifier Loss 0.05038544163107872, Total Loss 5.034588813781738\n",
      "49: Encoding Loss -1.672729253768921, Transition Loss -10.359341621398926, Classifier Loss 0.043757762759923935, Total Loss 4.373704433441162\n",
      "49: Encoding Loss -1.801993727684021, Transition Loss -16.7165470123291, Classifier Loss 0.04158906266093254, Total Loss 4.1555633544921875\n",
      "49: Encoding Loss -2.0488264560699463, Transition Loss -12.619352340698242, Classifier Loss 0.05085934326052666, Total Loss 5.083410263061523\n",
      "49: Encoding Loss -1.5819315910339355, Transition Loss -10.089055061340332, Classifier Loss 0.04314761981368065, Total Loss 4.312744140625\n",
      "49: Encoding Loss -1.1223868131637573, Transition Loss -16.7067813873291, Classifier Loss 0.03221989423036575, Total Loss 3.2186479568481445\n",
      "49: Encoding Loss -2.273322105407715, Transition Loss -13.946475982666016, Classifier Loss 0.053933173418045044, Total Loss 5.390527725219727\n",
      "49: Encoding Loss 0.10236259549856186, Transition Loss -10.290643692016602, Classifier Loss 0.045954663306474686, Total Loss 5.287012100219727\n",
      "49: Encoding Loss 0.3131018877029419, Transition Loss -3.716278314590454, Classifier Loss 0.05649818852543831, Total Loss 8.15170955657959\n",
      "49: Encoding Loss -2.223489284515381, Transition Loss -8.995877265930176, Classifier Loss 0.040357448160648346, Total Loss 4.033945560455322\n",
      "49: Encoding Loss -1.2269295454025269, Transition Loss -9.403228759765625, Classifier Loss 0.05397102236747742, Total Loss 5.395221710205078\n",
      "49: Encoding Loss -2.8980817794799805, Transition Loss -16.73328971862793, Classifier Loss 0.03298885375261307, Total Loss 3.2955386638641357\n",
      "49: Encoding Loss -2.0451173782348633, Transition Loss -20.84890365600586, Classifier Loss 0.05673076957464218, Total Loss 5.668907165527344\n",
      "49: Encoding Loss -0.8049284815788269, Transition Loss -12.606990814208984, Classifier Loss 0.01982252672314644, Total Loss 1.9797313213348389\n",
      "49: Encoding Loss -0.5716801285743713, Transition Loss -8.467304229736328, Classifier Loss 0.03626153990626335, Total Loss 3.624460458755493\n",
      "49: Encoding Loss -0.2533758282661438, Transition Loss -15.826998710632324, Classifier Loss 0.07414299249649048, Total Loss 7.3996968269348145\n",
      "49: Encoding Loss -0.9897374510765076, Transition Loss -8.40610408782959, Classifier Loss 0.04791423678398132, Total Loss 4.789742469787598\n",
      "49: Encoding Loss -1.3475289344787598, Transition Loss -17.667652130126953, Classifier Loss 0.06691455841064453, Total Loss 6.687922477722168\n",
      "49: Encoding Loss -1.6288695335388184, Transition Loss -13.87623405456543, Classifier Loss 0.03442488610744476, Total Loss 3.439713478088379\n",
      "49: Encoding Loss -1.3153873682022095, Transition Loss -14.812691688537598, Classifier Loss 0.05686648190021515, Total Loss 5.683685779571533\n",
      "49: Encoding Loss -1.5064373016357422, Transition Loss -6.8647966384887695, Classifier Loss 0.06912082433700562, Total Loss 6.910709857940674\n",
      "49: Encoding Loss -2.263852834701538, Transition Loss -14.269953727722168, Classifier Loss 0.02864804118871689, Total Loss 2.861949920654297\n",
      "49: Encoding Loss -0.8655338287353516, Transition Loss -7.837213039398193, Classifier Loss 0.057665884494781494, Total Loss 5.765020847320557\n",
      "49: Encoding Loss -2.543064832687378, Transition Loss -6.553177356719971, Classifier Loss 0.08316342532634735, Total Loss 8.315032005310059\n",
      "49: Encoding Loss -3.4110991954803467, Transition Loss -21.392166137695312, Classifier Loss 0.059373997151851654, Total Loss 5.933121204376221\n",
      "49: Encoding Loss -2.1738853454589844, Transition Loss -12.082383155822754, Classifier Loss 0.04729345440864563, Total Loss 4.7269287109375\n",
      "49: Encoding Loss -1.195071816444397, Transition Loss -7.99790620803833, Classifier Loss 0.055851828306913376, Total Loss 5.583582878112793\n",
      "49: Encoding Loss -1.3163379430770874, Transition Loss -13.962166786193848, Classifier Loss 0.05864277482032776, Total Loss 5.861485004425049\n",
      "49: Encoding Loss -1.489605188369751, Transition Loss -5.991365432739258, Classifier Loss 0.06996525079011917, Total Loss 6.995326995849609\n",
      "49: Encoding Loss -0.8188139796257019, Transition Loss -13.364461898803711, Classifier Loss 0.059929195791482925, Total Loss 5.990246772766113\n",
      "49: Encoding Loss -0.786835789680481, Transition Loss -8.76143741607666, Classifier Loss 0.14949928224086761, Total Loss 14.948176383972168\n",
      "49: Encoding Loss -2.3322513103485107, Transition Loss -14.864346504211426, Classifier Loss 0.03737999126315117, Total Loss 3.7350263595581055\n",
      "49: Encoding Loss -1.1388808488845825, Transition Loss -10.169028282165527, Classifier Loss 0.03691466152667999, Total Loss 3.689432382583618\n",
      "49: Encoding Loss -1.6122738122940063, Transition Loss -9.42066478729248, Classifier Loss 0.059220172464847565, Total Loss 5.920133113861084\n",
      "49: Encoding Loss -1.2758331298828125, Transition Loss -11.581841468811035, Classifier Loss 0.10312877595424652, Total Loss 10.310561180114746\n",
      "49: Encoding Loss -1.8821355104446411, Transition Loss -12.867368698120117, Classifier Loss 0.05283719673752785, Total Loss 5.281146049499512\n",
      "49: Encoding Loss -1.472351312637329, Transition Loss -13.600475311279297, Classifier Loss 0.0557132251560688, Total Loss 5.568602561950684\n",
      "49: Encoding Loss -1.1544660329818726, Transition Loss -10.582298278808594, Classifier Loss 0.029350807890295982, Total Loss 2.932964324951172\n",
      "49: Encoding Loss -1.9282033443450928, Transition Loss -18.222131729125977, Classifier Loss 0.06500312685966492, Total Loss 6.496668338775635\n",
      "49: Encoding Loss -2.0859336853027344, Transition Loss -17.592742919921875, Classifier Loss 0.027292244136333466, Total Loss 2.725705862045288\n",
      "49: Encoding Loss -2.2713351249694824, Transition Loss -8.059823989868164, Classifier Loss 0.05138545110821724, Total Loss 5.136932849884033\n",
      "49: Encoding Loss -1.5697762966156006, Transition Loss -18.798194885253906, Classifier Loss 0.03838931769132614, Total Loss 3.835172176361084\n",
      "49: Encoding Loss -0.19086650013923645, Transition Loss -9.682576179504395, Classifier Loss 0.06410417705774307, Total Loss 6.3654937744140625\n",
      "49: Encoding Loss -0.5793216228485107, Transition Loss -19.0806884765625, Classifier Loss 0.05336683243513107, Total Loss 5.33286714553833\n",
      "49: Encoding Loss -2.1585683822631836, Transition Loss -8.404560089111328, Classifier Loss 0.02742389775812626, Total Loss 2.740708827972412\n",
      "49: Encoding Loss -2.3575329780578613, Transition Loss -11.383371353149414, Classifier Loss 0.046993568539619446, Total Loss 4.697080135345459\n",
      "49: Encoding Loss -1.4685438871383667, Transition Loss -17.80246925354004, Classifier Loss 0.041622094810009, Total Loss 4.15864896774292\n",
      "49: Encoding Loss -2.034923791885376, Transition Loss -21.17671775817871, Classifier Loss 0.04506916180253029, Total Loss 4.502680778503418\n",
      "49: Encoding Loss -1.0076673030853271, Transition Loss -14.410510063171387, Classifier Loss 0.06231575459241867, Total Loss 6.22869348526001\n",
      "49: Encoding Loss -1.6000834703445435, Transition Loss -5.693332672119141, Classifier Loss 0.0758371353149414, Total Loss 7.582574844360352\n",
      "49: Encoding Loss -1.486335277557373, Transition Loss -13.888202667236328, Classifier Loss 0.035522256046533585, Total Loss 3.549448013305664\n",
      "49: Encoding Loss -0.7205964922904968, Transition Loss -13.689117431640625, Classifier Loss 0.06504704803228378, Total Loss 6.501966953277588\n",
      "49: Encoding Loss -1.836858868598938, Transition Loss -7.285799980163574, Classifier Loss 0.0745895579457283, Total Loss 7.457498550415039\n",
      "49: Encoding Loss -0.34426194429397583, Transition Loss -12.769816398620605, Classifier Loss 0.02721378579735756, Total Loss 2.718031167984009\n",
      "49: Encoding Loss -1.9364391565322876, Transition Loss -14.679494857788086, Classifier Loss 0.04506555572152138, Total Loss 4.50361967086792\n",
      "49: Encoding Loss -2.395382881164551, Transition Loss -20.63555335998535, Classifier Loss 0.033150963485240936, Total Loss 3.310969352722168\n",
      "49: Encoding Loss -1.6967343091964722, Transition Loss -17.925289154052734, Classifier Loss 0.053615931421518326, Total Loss 5.35800838470459\n",
      "49: Encoding Loss -1.8304835557937622, Transition Loss -11.055265426635742, Classifier Loss 0.053605757653713226, Total Loss 5.358364582061768\n",
      "49: Encoding Loss -2.3213114738464355, Transition Loss -9.9035005569458, Classifier Loss 0.09074753522872925, Total Loss 9.072772979736328\n",
      "49: Encoding Loss -2.2332727909088135, Transition Loss -11.679096221923828, Classifier Loss 0.043413031846284866, Total Loss 4.338967323303223\n",
      "49: Encoding Loss -0.8462833166122437, Transition Loss -9.086078643798828, Classifier Loss 0.06233830004930496, Total Loss 6.232012748718262\n",
      "49: Encoding Loss -1.459712266921997, Transition Loss -13.31627368927002, Classifier Loss 0.057153698056936264, Total Loss 5.712706565856934\n",
      "49: Encoding Loss -2.227287769317627, Transition Loss -6.802054405212402, Classifier Loss 0.08841586858034134, Total Loss 8.840227127075195\n",
      "49: Encoding Loss -0.9726305603981018, Transition Loss -3.6815185546875, Classifier Loss 0.05054473876953125, Total Loss 5.053737640380859\n",
      "49: Encoding Loss -2.2922160625457764, Transition Loss -19.425931930541992, Classifier Loss 0.06657576560974121, Total Loss 6.653691291809082\n",
      "49: Encoding Loss -0.29563596844673157, Transition Loss -10.576883316040039, Classifier Loss 0.06047751009464264, Total Loss 6.041954517364502\n",
      "49: Encoding Loss -0.761660635471344, Transition Loss -10.699853897094727, Classifier Loss 0.08965755999088287, Total Loss 8.963616371154785\n",
      "49: Encoding Loss -2.133423328399658, Transition Loss -10.442934036254883, Classifier Loss 0.06501447409391403, Total Loss 6.499358654022217\n",
      "49: Encoding Loss -1.4123624563217163, Transition Loss -10.45712947845459, Classifier Loss 0.06486955285072327, Total Loss 6.484863758087158\n",
      "49: Encoding Loss -1.1271910667419434, Transition Loss -7.228533744812012, Classifier Loss 0.07968360930681229, Total Loss 7.966915130615234\n",
      "49: Encoding Loss -1.3425636291503906, Transition Loss -10.729063034057617, Classifier Loss 0.018999645486474037, Total Loss 1.8978188037872314\n",
      "49: Encoding Loss -2.090409994125366, Transition Loss -9.613767623901367, Classifier Loss 0.08360951393842697, Total Loss 8.359028816223145\n",
      "49: Encoding Loss -1.3039335012435913, Transition Loss -15.675132751464844, Classifier Loss 0.03102477639913559, Total Loss 3.0993425846099854\n",
      "49: Encoding Loss -1.6655770540237427, Transition Loss -8.543457984924316, Classifier Loss 0.06466858834028244, Total Loss 6.465150356292725\n",
      "49: Encoding Loss -2.4981319904327393, Transition Loss -13.918632507324219, Classifier Loss 0.04930606111884117, Total Loss 4.927822113037109\n",
      "49: Encoding Loss -1.6476600170135498, Transition Loss -14.969910621643066, Classifier Loss 0.02272780053317547, Total Loss 2.2697858810424805\n",
      "49: Encoding Loss -2.1444177627563477, Transition Loss -11.552332878112793, Classifier Loss 0.08962138742208481, Total Loss 8.95982837677002\n",
      "49: Encoding Loss -1.0484551191329956, Transition Loss -12.559980392456055, Classifier Loss 0.043573539704084396, Total Loss 4.354842185974121\n",
      "49: Encoding Loss -2.229774236679077, Transition Loss -10.389452934265137, Classifier Loss 0.03876090049743652, Total Loss 3.8740122318267822\n",
      "49: Encoding Loss -1.5517534017562866, Transition Loss -7.24656867980957, Classifier Loss 0.08562960475683212, Total Loss 8.561511039733887\n",
      "49: Encoding Loss -0.7923690676689148, Transition Loss -12.286416053771973, Classifier Loss 0.07074427604675293, Total Loss 7.071970462799072\n",
      "49: Encoding Loss -2.4611146450042725, Transition Loss -9.704626083374023, Classifier Loss 0.04601889103651047, Total Loss 4.599948406219482\n",
      "49: Encoding Loss -0.9456037282943726, Transition Loss -7.617575645446777, Classifier Loss 0.062493134289979935, Total Loss 6.2477898597717285\n",
      "49: Encoding Loss -1.7951902151107788, Transition Loss -17.033613204956055, Classifier Loss 0.05529962480068207, Total Loss 5.526556015014648\n",
      "49: Encoding Loss -1.4077366590499878, Transition Loss -11.3253812789917, Classifier Loss 0.09605905413627625, Total Loss 9.60364055633545\n",
      "49: Encoding Loss -0.27403199672698975, Transition Loss -9.397873878479004, Classifier Loss 0.0451255738735199, Total Loss 4.5039496421813965\n",
      "49: Encoding Loss -1.2979004383087158, Transition Loss -14.562793731689453, Classifier Loss 0.04770873859524727, Total Loss 4.767961502075195\n",
      "49: Encoding Loss -3.5151281356811523, Transition Loss -22.745912551879883, Classifier Loss 0.04318205267190933, Total Loss 4.313656330108643\n",
      "49: Encoding Loss -1.2012102603912354, Transition Loss -8.519758224487305, Classifier Loss 0.0740089938044548, Total Loss 7.399195671081543\n",
      "49: Encoding Loss -1.6802505254745483, Transition Loss -9.582995414733887, Classifier Loss 0.06386607885360718, Total Loss 6.3846917152404785\n",
      "49: Encoding Loss -0.49135926365852356, Transition Loss -11.4580078125, Classifier Loss 0.05454116687178612, Total Loss 5.4518232345581055\n",
      "49: Encoding Loss -1.4226224422454834, Transition Loss -5.934682846069336, Classifier Loss 0.06473054736852646, Total Loss 6.47186803817749\n",
      "49: Encoding Loss -1.8979761600494385, Transition Loss -14.438163757324219, Classifier Loss 0.06539279222488403, Total Loss 6.536391258239746\n",
      "49: Encoding Loss -1.1557104587554932, Transition Loss -17.31216049194336, Classifier Loss 0.06303677707910538, Total Loss 6.300215244293213\n",
      "49: Encoding Loss -1.559532642364502, Transition Loss -14.753181457519531, Classifier Loss 0.04717019200325012, Total Loss 4.714068412780762\n",
      "49: Encoding Loss -0.9778793454170227, Transition Loss -6.343259334564209, Classifier Loss 0.029469097033143044, Total Loss 2.945641040802002\n",
      "49: Encoding Loss -2.2627828121185303, Transition Loss -8.893022537231445, Classifier Loss 0.09264441579580307, Total Loss 9.262662887573242\n",
      "49: Encoding Loss -2.645498037338257, Transition Loss -15.320316314697266, Classifier Loss 0.05388061702251434, Total Loss 5.384997367858887\n",
      "49: Encoding Loss -1.1363303661346436, Transition Loss -1.6979743242263794, Classifier Loss 0.09542248398065567, Total Loss 9.541909217834473\n",
      "49: Encoding Loss -0.9508296847343445, Transition Loss -9.521546363830566, Classifier Loss 0.06439585238695145, Total Loss 6.437680721282959\n",
      "49: Encoding Loss -0.694817841053009, Transition Loss -5.964729309082031, Classifier Loss 0.06688839942216873, Total Loss 6.687646865844727\n",
      "49: Encoding Loss -1.2294169664382935, Transition Loss -12.677931785583496, Classifier Loss 0.042915742844343185, Total Loss 4.28903865814209\n",
      "49: Encoding Loss -1.8946669101715088, Transition Loss -16.499958038330078, Classifier Loss 0.06888216733932495, Total Loss 6.88491678237915\n",
      "49: Encoding Loss -1.3020433187484741, Transition Loss -6.211431980133057, Classifier Loss 0.06535015255212784, Total Loss 6.533772945404053\n",
      "49: Encoding Loss -1.6793766021728516, Transition Loss -10.288378715515137, Classifier Loss 0.04060826450586319, Total Loss 4.0587687492370605\n",
      "49: Encoding Loss -1.8728209733963013, Transition Loss -10.057291030883789, Classifier Loss 0.04454811289906502, Total Loss 4.4527997970581055\n",
      "49: Encoding Loss -0.9373480081558228, Transition Loss -15.111184120178223, Classifier Loss 0.0476280078291893, Total Loss 4.7597784996032715\n",
      "49: Encoding Loss -1.5265934467315674, Transition Loss -14.644831657409668, Classifier Loss 0.06114262342453003, Total Loss 6.111333847045898\n",
      "49: Encoding Loss -1.40786612033844, Transition Loss -3.128948450088501, Classifier Loss 0.0351046584546566, Total Loss 3.5098400115966797\n",
      "49: Encoding Loss -2.1915316581726074, Transition Loss -18.33197784423828, Classifier Loss 0.03417104110121727, Total Loss 3.413437604904175\n",
      "49: Encoding Loss -2.1362688541412354, Transition Loss -17.56792449951172, Classifier Loss 0.029289882630109787, Total Loss 2.9254746437072754\n",
      "49: Encoding Loss -0.8741179704666138, Transition Loss -2.573021650314331, Classifier Loss 0.04967796802520752, Total Loss 4.967282295227051\n",
      "49: Encoding Loss -0.7710992097854614, Transition Loss 0.257243275642395, Classifier Loss 0.05168301612138748, Total Loss 5.21975040435791\n",
      "49: Encoding Loss -1.247071385383606, Transition Loss -12.8130464553833, Classifier Loss 0.0338502936065197, Total Loss 3.3824667930603027\n",
      "49: Encoding Loss -1.385755181312561, Transition Loss -12.066112518310547, Classifier Loss 0.043341830372810364, Total Loss 4.331769943237305\n",
      "49: Encoding Loss -2.6210134029388428, Transition Loss -14.06161880493164, Classifier Loss 0.04992971196770668, Total Loss 4.990159034729004\n",
      "49: Encoding Loss -1.6602180004119873, Transition Loss -7.921061992645264, Classifier Loss 0.05304728448390961, Total Loss 5.303144454956055\n",
      "49: Encoding Loss -2.8688628673553467, Transition Loss -16.07689666748047, Classifier Loss 0.03565312922000885, Total Loss 3.5620975494384766\n",
      "49: Encoding Loss -1.744896411895752, Transition Loss -10.156331062316895, Classifier Loss 0.06292517483234406, Total Loss 6.2904863357543945\n",
      "49: Encoding Loss -1.657636284828186, Transition Loss -13.153023719787598, Classifier Loss 0.0787021592259407, Total Loss 7.867585182189941\n",
      "49: Encoding Loss -2.869366407394409, Transition Loss -20.47414779663086, Classifier Loss 0.04502971097826958, Total Loss 4.498876571655273\n",
      "49: Encoding Loss -2.511392831802368, Transition Loss -9.65022087097168, Classifier Loss 0.062213234603405, Total Loss 6.219393253326416\n",
      "49: Encoding Loss -1.7094497680664062, Transition Loss -11.151166915893555, Classifier Loss 0.08644677698612213, Total Loss 8.642447471618652\n",
      "49: Encoding Loss -2.232351779937744, Transition Loss -16.840656280517578, Classifier Loss 0.06953839957714081, Total Loss 6.950471878051758\n",
      "49: Encoding Loss -1.9783459901809692, Transition Loss -13.692120552062988, Classifier Loss 0.024630943313241005, Total Loss 2.460355758666992\n",
      "49: Encoding Loss -1.0913031101226807, Transition Loss -12.403620719909668, Classifier Loss 0.05219176411628723, Total Loss 5.216695785522461\n",
      "49: Encoding Loss -1.7749930620193481, Transition Loss -6.299160003662109, Classifier Loss 0.038521479815244675, Total Loss 3.850888252258301\n",
      "49: Encoding Loss -0.9320790767669678, Transition Loss -9.665066719055176, Classifier Loss 0.05599641799926758, Total Loss 5.597708702087402\n",
      "49: Encoding Loss -1.7223249673843384, Transition Loss -10.502098083496094, Classifier Loss 0.0492854081094265, Total Loss 4.926440238952637\n",
      "49: Encoding Loss -1.245247721672058, Transition Loss -6.249731540679932, Classifier Loss 0.030169840902090073, Total Loss 3.0157339572906494\n",
      "49: Encoding Loss -1.1650246381759644, Transition Loss -11.335147857666016, Classifier Loss 0.02478952892124653, Total Loss 2.4766857624053955\n",
      "49: Encoding Loss -1.5861289501190186, Transition Loss -13.60931396484375, Classifier Loss 0.06760843098163605, Total Loss 6.758121490478516\n",
      "49: Encoding Loss -0.750144898891449, Transition Loss -13.205790519714355, Classifier Loss 0.051381710916757584, Total Loss 5.1355299949646\n",
      "49: Encoding Loss -1.3038500547409058, Transition Loss -2.0846099853515625, Classifier Loss 0.07919856905937195, Total Loss 7.919440269470215\n",
      "49: Encoding Loss -1.085391640663147, Transition Loss -9.646565437316895, Classifier Loss 0.0313180536031723, Total Loss 3.129876136779785\n",
      "49: Encoding Loss -0.24151299893856049, Transition Loss -12.340767860412598, Classifier Loss 0.06842432171106339, Total Loss 6.82476806640625\n",
      "49: Encoding Loss -0.13247540593147278, Transition Loss -10.798133850097656, Classifier Loss 0.045031629502773285, Total Loss 4.402837753295898\n",
      "49: Encoding Loss -1.3732625246047974, Transition Loss -10.632680892944336, Classifier Loss 0.04682096838951111, Total Loss 4.6799702644348145\n",
      "49: Encoding Loss -2.063802719116211, Transition Loss -6.895892143249512, Classifier Loss 0.07013696432113647, Total Loss 7.012317657470703\n",
      "49: Encoding Loss -1.825335144996643, Transition Loss -15.315845489501953, Classifier Loss 0.033244527876377106, Total Loss 3.321389675140381\n",
      "49: Encoding Loss 0.16918137669563293, Transition Loss -3.5745227336883545, Classifier Loss 0.04650987684726715, Total Loss 5.942357540130615\n",
      "49: Encoding Loss 0.7293700575828552, Transition Loss -6.27899694442749, Classifier Loss 0.027344852685928345, Total Loss 8.56818962097168\n",
      "49: Encoding Loss 1.5435287952423096, Transition Loss -9.379195213317871, Classifier Loss 0.07183898240327835, Total Loss 19.530254364013672\n",
      "49: Encoding Loss 0.602732241153717, Transition Loss -16.66749382019043, Classifier Loss 0.10684878379106522, Total Loss 15.503403663635254\n",
      "49: Encoding Loss -2.446605682373047, Transition Loss -16.141447067260742, Classifier Loss 0.061570800840854645, Total Loss 6.1538519859313965\n",
      "49: Encoding Loss -2.8524422645568848, Transition Loss -21.646453857421875, Classifier Loss 0.03478645160794258, Total Loss 3.474315881729126\n",
      "49: Encoding Loss -1.2048161029815674, Transition Loss -11.782608032226562, Classifier Loss 0.09002096951007843, Total Loss 8.999740600585938\n",
      "49: Encoding Loss -2.142613649368286, Transition Loss -13.490260124206543, Classifier Loss 0.04290834069252014, Total Loss 4.288136005401611\n",
      "49: Encoding Loss -2.866516590118408, Transition Loss -19.55677604675293, Classifier Loss 0.04818350821733475, Total Loss 4.814439296722412\n",
      "49: Encoding Loss -2.176270008087158, Transition Loss -7.644813537597656, Classifier Loss 0.07528197020292282, Total Loss 7.526668071746826\n",
      "49: Encoding Loss -1.2982983589172363, Transition Loss -16.227306365966797, Classifier Loss 0.07593168318271637, Total Loss 7.589922904968262\n",
      "49: Encoding Loss -1.7481869459152222, Transition Loss -17.00714683532715, Classifier Loss 0.02104911394417286, Total Loss 2.1015098094940186\n",
      "49: Encoding Loss -1.9509378671646118, Transition Loss -20.177684783935547, Classifier Loss 0.052287254482507706, Total Loss 5.224689960479736\n",
      "49: Encoding Loss -2.234548330307007, Transition Loss -26.158401489257812, Classifier Loss 0.014944183640182018, Total Loss 1.4891866445541382\n",
      "50: Encoding Loss -1.3344546556472778, Transition Loss -5.927959442138672, Classifier Loss 0.029542267322540283, Total Loss 2.9530410766601562\n",
      "50: Encoding Loss -1.9255958795547485, Transition Loss -6.623867034912109, Classifier Loss 0.0866076797246933, Total Loss 8.659442901611328\n",
      "50: Encoding Loss -1.934877872467041, Transition Loss -1.633805274963379, Classifier Loss 0.03174908459186554, Total Loss 3.174581527709961\n",
      "50: Encoding Loss -1.7478176355361938, Transition Loss -19.222862243652344, Classifier Loss 0.05092417821288109, Total Loss 5.088572978973389\n",
      "50: Encoding Loss -1.7702988386154175, Transition Loss -9.670764923095703, Classifier Loss 0.041329652070999146, Total Loss 4.131031036376953\n",
      "50: Encoding Loss -1.6467089653015137, Transition Loss -16.09723472595215, Classifier Loss 0.0414331816136837, Total Loss 4.140098571777344\n",
      "50: Encoding Loss -2.423600435256958, Transition Loss -11.95706558227539, Classifier Loss 0.05001701042056084, Total Loss 4.999309539794922\n",
      "50: Encoding Loss -1.5138065814971924, Transition Loss -9.59896469116211, Classifier Loss 0.04338442534208298, Total Loss 4.336522579193115\n",
      "50: Encoding Loss -1.6724587678909302, Transition Loss -16.117429733276367, Classifier Loss 0.031086280941963196, Total Loss 3.1054046154022217\n",
      "50: Encoding Loss -2.557692050933838, Transition Loss -13.451644897460938, Classifier Loss 0.05442150682210922, Total Loss 5.439460277557373\n",
      "50: Encoding Loss -0.22274774312973022, Transition Loss -9.612990379333496, Classifier Loss 0.04528056085109711, Total Loss 4.503043174743652\n",
      "50: Encoding Loss -1.4489284753799438, Transition Loss -6.454536437988281, Classifier Loss 0.054056089371442795, Total Loss 5.404318332672119\n",
      "50: Encoding Loss -2.102785348892212, Transition Loss -8.098785400390625, Classifier Loss 0.03995642066001892, Total Loss 3.9940223693847656\n",
      "50: Encoding Loss -1.452393651008606, Transition Loss -8.748594284057617, Classifier Loss 0.04909713193774223, Total Loss 4.907963752746582\n",
      "50: Encoding Loss -2.5077662467956543, Transition Loss -15.890235900878906, Classifier Loss 0.031252421438694, Total Loss 3.1220641136169434\n",
      "50: Encoding Loss -2.387648582458496, Transition Loss -20.00070571899414, Classifier Loss 0.05910049006342888, Total Loss 5.906048774719238\n",
      "50: Encoding Loss -1.2612528800964355, Transition Loss -11.892557144165039, Classifier Loss 0.01879683881998062, Total Loss 1.8773053884506226\n",
      "50: Encoding Loss -0.749862015247345, Transition Loss -7.915406227111816, Classifier Loss 0.03658464550971985, Total Loss 3.656881332397461\n",
      "50: Encoding Loss -0.7162366509437561, Transition Loss -15.216622352600098, Classifier Loss 0.07222738116979599, Total Loss 7.219695091247559\n",
      "50: Encoding Loss -0.8303239345550537, Transition Loss -7.5812225341796875, Classifier Loss 0.04561110585927963, Total Loss 4.55959415435791\n",
      "50: Encoding Loss -1.6687092781066895, Transition Loss -16.994314193725586, Classifier Loss 0.0692981407046318, Total Loss 6.926414966583252\n",
      "50: Encoding Loss -1.6994348764419556, Transition Loss -13.101064682006836, Classifier Loss 0.03099604696035385, Total Loss 3.096984386444092\n",
      "50: Encoding Loss -1.5146758556365967, Transition Loss -13.882230758666992, Classifier Loss 0.054216235876083374, Total Loss 5.41884708404541\n",
      "50: Encoding Loss -1.5204353332519531, Transition Loss -6.01790714263916, Classifier Loss 0.06794624030590057, Total Loss 6.793420314788818\n",
      "50: Encoding Loss -2.1631507873535156, Transition Loss -13.641969680786133, Classifier Loss 0.029816074296832085, Total Loss 2.978878974914551\n",
      "50: Encoding Loss -0.9151574373245239, Transition Loss -7.257855415344238, Classifier Loss 0.05775023251771927, Total Loss 5.773571968078613\n",
      "50: Encoding Loss -2.5561273097991943, Transition Loss -5.767542839050293, Classifier Loss 0.08248873054981232, Total Loss 8.247718811035156\n",
      "50: Encoding Loss -3.0577948093414307, Transition Loss -20.6051082611084, Classifier Loss 0.05909714475274086, Total Loss 5.9055938720703125\n",
      "50: Encoding Loss -2.249830722808838, Transition Loss -11.329750061035156, Classifier Loss 0.046658001840114594, Total Loss 4.663534164428711\n",
      "50: Encoding Loss -1.0024093389511108, Transition Loss -7.323718547821045, Classifier Loss 0.054029811173677444, Total Loss 5.401516437530518\n",
      "50: Encoding Loss -1.1956015825271606, Transition Loss -13.456952095031738, Classifier Loss 0.05751849338412285, Total Loss 5.749157905578613\n",
      "50: Encoding Loss -1.6297396421432495, Transition Loss -5.237780570983887, Classifier Loss 0.0679117739200592, Total Loss 6.790129661560059\n",
      "50: Encoding Loss -0.7940813899040222, Transition Loss -12.821603775024414, Classifier Loss 0.0549500547349453, Total Loss 5.492441177368164\n",
      "50: Encoding Loss -0.9164592027664185, Transition Loss -7.869575023651123, Classifier Loss 0.15277822315692902, Total Loss 15.276248931884766\n",
      "50: Encoding Loss -2.2066192626953125, Transition Loss -14.479415893554688, Classifier Loss 0.03552624210715294, Total Loss 3.5497283935546875\n",
      "50: Encoding Loss -1.1705803871154785, Transition Loss -9.686238288879395, Classifier Loss 0.038487374782562256, Total Loss 3.8468003273010254\n",
      "50: Encoding Loss -1.2575981616973877, Transition Loss -8.747291564941406, Classifier Loss 0.05838847905397415, Total Loss 5.837098598480225\n",
      "50: Encoding Loss -1.2915881872177124, Transition Loss -11.066160202026367, Classifier Loss 0.10134540498256683, Total Loss 10.13232707977295\n",
      "50: Encoding Loss -1.9625413417816162, Transition Loss -12.266112327575684, Classifier Loss 0.04901355132460594, Total Loss 4.89890193939209\n",
      "50: Encoding Loss -1.1669872999191284, Transition Loss -13.247011184692383, Classifier Loss 0.054640766233205795, Total Loss 5.461427211761475\n",
      "50: Encoding Loss -1.2002978324890137, Transition Loss -10.056446075439453, Classifier Loss 0.027859074994921684, Total Loss 2.7838962078094482\n",
      "50: Encoding Loss -1.6629595756530762, Transition Loss -17.896255493164062, Classifier Loss 0.0650814101099968, Total Loss 6.504561901092529\n",
      "50: Encoding Loss -2.1011712551116943, Transition Loss -17.189546585083008, Classifier Loss 0.027161795645952225, Total Loss 2.7127416133880615\n",
      "50: Encoding Loss -2.205413579940796, Transition Loss -7.661495208740234, Classifier Loss 0.04952331632375717, Total Loss 4.950799465179443\n",
      "50: Encoding Loss -1.5631998777389526, Transition Loss -18.442007064819336, Classifier Loss 0.03802318498492241, Total Loss 3.7986302375793457\n",
      "50: Encoding Loss -0.2504948377609253, Transition Loss -9.245347023010254, Classifier Loss 0.0638488382101059, Total Loss 6.370763778686523\n",
      "50: Encoding Loss -1.2745531797409058, Transition Loss -18.865097045898438, Classifier Loss 0.052602291107177734, Total Loss 5.256455898284912\n",
      "50: Encoding Loss -2.1209709644317627, Transition Loss -7.867236137390137, Classifier Loss 0.02706827037036419, Total Loss 2.7052533626556396\n",
      "50: Encoding Loss -1.9986761808395386, Transition Loss -10.85031509399414, Classifier Loss 0.045441385358572006, Total Loss 4.54196834564209\n",
      "50: Encoding Loss -1.5666135549545288, Transition Loss -17.33509635925293, Classifier Loss 0.04171636328101158, Total Loss 4.168169021606445\n",
      "50: Encoding Loss -2.3113350868225098, Transition Loss -20.600387573242188, Classifier Loss 0.04306114464998245, Total Loss 4.301994800567627\n",
      "50: Encoding Loss -1.1082172393798828, Transition Loss -13.919583320617676, Classifier Loss 0.06135401129722595, Total Loss 6.132617473602295\n",
      "50: Encoding Loss -1.2888758182525635, Transition Loss -4.766255855560303, Classifier Loss 0.07755110412836075, Total Loss 7.754157066345215\n",
      "50: Encoding Loss -1.453570008277893, Transition Loss -13.409632682800293, Classifier Loss 0.03451037034392357, Total Loss 3.448354959487915\n",
      "50: Encoding Loss -0.9455834031105042, Transition Loss -13.268195152282715, Classifier Loss 0.06420552730560303, Total Loss 6.417899131774902\n",
      "50: Encoding Loss -1.7931668758392334, Transition Loss -6.707840442657471, Classifier Loss 0.07657254487276077, Total Loss 7.655913352966309\n",
      "50: Encoding Loss -0.6150618195533752, Transition Loss -12.187249183654785, Classifier Loss 0.026653870940208435, Total Loss 2.662949800491333\n",
      "50: Encoding Loss -1.9524599313735962, Transition Loss -14.100709915161133, Classifier Loss 0.04513714462518692, Total Loss 4.510894298553467\n",
      "50: Encoding Loss -2.236828088760376, Transition Loss -20.261798858642578, Classifier Loss 0.03238958865404129, Total Loss 3.2349064350128174\n",
      "50: Encoding Loss -1.9079097509384155, Transition Loss -17.371187210083008, Classifier Loss 0.05313119664788246, Total Loss 5.309645652770996\n",
      "50: Encoding Loss -1.6186788082122803, Transition Loss -10.476374626159668, Classifier Loss 0.05319679528474808, Total Loss 5.31758451461792\n",
      "50: Encoding Loss -2.2378900051116943, Transition Loss -9.365165710449219, Classifier Loss 0.0834437757730484, Total Loss 8.342504501342773\n",
      "50: Encoding Loss -2.113030195236206, Transition Loss -10.977060317993164, Classifier Loss 0.04192568361759186, Total Loss 4.190372943878174\n",
      "50: Encoding Loss -0.9922662973403931, Transition Loss -8.527149200439453, Classifier Loss 0.06161869689822197, Total Loss 6.160163879394531\n",
      "50: Encoding Loss -1.4527440071105957, Transition Loss -12.681221008300781, Classifier Loss 0.05749216303229332, Total Loss 5.746679782867432\n",
      "50: Encoding Loss -2.22627329826355, Transition Loss -6.144103050231934, Classifier Loss 0.08698272705078125, Total Loss 8.697043418884277\n",
      "50: Encoding Loss -0.9219487905502319, Transition Loss -2.9826765060424805, Classifier Loss 0.04926858842372894, Total Loss 4.926262378692627\n",
      "50: Encoding Loss -1.7153513431549072, Transition Loss -19.09186363220215, Classifier Loss 0.07302460819482803, Total Loss 7.298642158508301\n",
      "50: Encoding Loss -0.4926527738571167, Transition Loss -10.027737617492676, Classifier Loss 0.061209432780742645, Total Loss 6.118936061859131\n",
      "50: Encoding Loss -0.8921306729316711, Transition Loss -10.007864952087402, Classifier Loss 0.09878760576248169, Total Loss 9.876758575439453\n",
      "50: Encoding Loss -1.9575401544570923, Transition Loss -9.770709991455078, Classifier Loss 0.06221917271614075, Total Loss 6.219963073730469\n",
      "50: Encoding Loss -1.4782230854034424, Transition Loss -9.834785461425781, Classifier Loss 0.06370753794908524, Total Loss 6.368786811828613\n",
      "50: Encoding Loss -0.8922561407089233, Transition Loss -6.60640811920166, Classifier Loss 0.07855991274118423, Total Loss 7.85467004776001\n",
      "50: Encoding Loss -1.329742670059204, Transition Loss -9.973259925842285, Classifier Loss 0.017901120707392693, Total Loss 1.7881174087524414\n",
      "50: Encoding Loss -2.2601003646850586, Transition Loss -9.040689468383789, Classifier Loss 0.08118396997451782, Total Loss 8.116588592529297\n",
      "50: Encoding Loss -1.403544545173645, Transition Loss -15.287975311279297, Classifier Loss 0.029992684721946716, Total Loss 2.996211051940918\n",
      "50: Encoding Loss -1.6471483707427979, Transition Loss -7.739292621612549, Classifier Loss 0.06299611181020737, Total Loss 6.298063278198242\n",
      "50: Encoding Loss -2.4384825229644775, Transition Loss -13.296306610107422, Classifier Loss 0.04931244999170303, Total Loss 4.928585529327393\n",
      "50: Encoding Loss -1.51077139377594, Transition Loss -14.348224639892578, Classifier Loss 0.021588679403066635, Total Loss 2.1559982299804688\n",
      "50: Encoding Loss -1.8804981708526611, Transition Loss -10.989248275756836, Classifier Loss 0.09456600248813629, Total Loss 9.454401969909668\n",
      "50: Encoding Loss -0.9658697843551636, Transition Loss -12.021610260009766, Classifier Loss 0.04226193577051163, Total Loss 4.223789215087891\n",
      "50: Encoding Loss -1.797412395477295, Transition Loss -9.801572799682617, Classifier Loss 0.03731691464781761, Total Loss 3.7297310829162598\n",
      "50: Encoding Loss -1.466946005821228, Transition Loss -6.560596466064453, Classifier Loss 0.08782316744327545, Total Loss 8.781004905700684\n",
      "50: Encoding Loss -0.7368470430374146, Transition Loss -11.989500045776367, Classifier Loss 0.06962449848651886, Total Loss 6.960052013397217\n",
      "50: Encoding Loss -2.6401610374450684, Transition Loss -8.869020462036133, Classifier Loss 0.046077050268650055, Total Loss 4.605931282043457\n",
      "50: Encoding Loss -1.3927985429763794, Transition Loss -6.935379981994629, Classifier Loss 0.0646982192993164, Total Loss 6.468434810638428\n",
      "50: Encoding Loss -2.002676486968994, Transition Loss -16.542585372924805, Classifier Loss 0.05185486376285553, Total Loss 5.182178020477295\n",
      "50: Encoding Loss -1.7612582445144653, Transition Loss -10.587225914001465, Classifier Loss 0.09433070570230484, Total Loss 9.430953025817871\n",
      "50: Encoding Loss -0.8526744246482849, Transition Loss -8.868184089660645, Classifier Loss 0.04438469186425209, Total Loss 4.436695575714111\n",
      "50: Encoding Loss -1.1201369762420654, Transition Loss -14.220651626586914, Classifier Loss 0.046600423753261566, Total Loss 4.657197952270508\n",
      "50: Encoding Loss -3.3109686374664307, Transition Loss -22.301589965820312, Classifier Loss 0.04318683594465256, Total Loss 4.314223289489746\n",
      "50: Encoding Loss -1.1676775217056274, Transition Loss -7.959979057312012, Classifier Loss 0.07433971762657166, Total Loss 7.432379722595215\n",
      "50: Encoding Loss -1.2289376258850098, Transition Loss -8.832298278808594, Classifier Loss 0.0634157583117485, Total Loss 6.339808940887451\n",
      "50: Encoding Loss -0.7893821001052856, Transition Loss -10.767655372619629, Classifier Loss 0.053627826273441315, Total Loss 5.360629081726074\n",
      "50: Encoding Loss -1.3955388069152832, Transition Loss -5.216204643249512, Classifier Loss 0.06459984183311462, Total Loss 6.4589409828186035\n",
      "50: Encoding Loss -1.8818910121917725, Transition Loss -13.939544677734375, Classifier Loss 0.06390052288770676, Total Loss 6.387264251708984\n",
      "50: Encoding Loss -1.480995535850525, Transition Loss -16.90494728088379, Classifier Loss 0.06425623595714569, Total Loss 6.422242641448975\n",
      "50: Encoding Loss -1.7183308601379395, Transition Loss -14.005266189575195, Classifier Loss 0.04638484865427017, Total Loss 4.635684013366699\n",
      "50: Encoding Loss -1.055100440979004, Transition Loss -5.585093975067139, Classifier Loss 0.02967938967049122, Total Loss 2.9668219089508057\n",
      "50: Encoding Loss -1.9352363348007202, Transition Loss -8.109167098999023, Classifier Loss 0.09069515764713287, Total Loss 9.067893981933594\n",
      "50: Encoding Loss -2.3489134311676025, Transition Loss -14.827205657958984, Classifier Loss 0.052141912281513214, Total Loss 5.211225986480713\n",
      "50: Encoding Loss -0.9339602589607239, Transition Loss -0.9100985527038574, Classifier Loss 0.09481455385684967, Total Loss 9.481273651123047\n",
      "50: Encoding Loss -1.1921215057373047, Transition Loss -9.037245750427246, Classifier Loss 0.06467586010694504, Total Loss 6.465778827667236\n",
      "50: Encoding Loss -0.47716110944747925, Transition Loss -5.647149562835693, Classifier Loss 0.06588198244571686, Total Loss 6.58706521987915\n",
      "50: Encoding Loss -1.0967121124267578, Transition Loss -12.45994758605957, Classifier Loss 0.042720310389995575, Total Loss 4.269538879394531\n",
      "50: Encoding Loss -1.9072402715682983, Transition Loss -16.198938369750977, Classifier Loss 0.06521911174058914, Total Loss 6.51867151260376\n",
      "50: Encoding Loss -1.140629529953003, Transition Loss -5.790274143218994, Classifier Loss 0.06540821492671967, Total Loss 6.539663314819336\n",
      "50: Encoding Loss -1.2141796350479126, Transition Loss -9.765213012695312, Classifier Loss 0.04022547975182533, Total Loss 4.020595073699951\n",
      "50: Encoding Loss -1.431362271308899, Transition Loss -9.584915161132812, Classifier Loss 0.044785093516111374, Total Loss 4.476592540740967\n",
      "50: Encoding Loss -0.6845195889472961, Transition Loss -14.820590019226074, Classifier Loss 0.04765063896775246, Total Loss 4.762099742889404\n",
      "50: Encoding Loss -1.280794620513916, Transition Loss -14.350480079650879, Classifier Loss 0.05955546349287033, Total Loss 5.952676296234131\n",
      "50: Encoding Loss -0.9992228150367737, Transition Loss -2.6550228595733643, Classifier Loss 0.035241879522800446, Total Loss 3.5236570835113525\n",
      "50: Encoding Loss -1.503731369972229, Transition Loss -18.178783416748047, Classifier Loss 0.03369056060910225, Total Loss 3.365420341491699\n",
      "50: Encoding Loss -2.165457248687744, Transition Loss -17.447240829467773, Classifier Loss 0.028593625873327255, Total Loss 2.8558731079101562\n",
      "50: Encoding Loss -0.8530536890029907, Transition Loss -2.0694286823272705, Classifier Loss 0.04874717816710472, Total Loss 4.874303817749023\n",
      "50: Encoding Loss -0.5811322927474976, Transition Loss 0.8254505395889282, Classifier Loss 0.050365298986434937, Total Loss 5.201620101928711\n",
      "50: Encoding Loss -0.92154461145401, Transition Loss -12.401432991027832, Classifier Loss 0.03289957717061043, Total Loss 3.287477493286133\n",
      "50: Encoding Loss -1.3811615705490112, Transition Loss -11.659814834594727, Classifier Loss 0.045110054314136505, Total Loss 4.508673667907715\n",
      "50: Encoding Loss -2.257481813430786, Transition Loss -13.705617904663086, Classifier Loss 0.04839978367090225, Total Loss 4.8372368812561035\n",
      "50: Encoding Loss -1.328368902206421, Transition Loss -7.459646224975586, Classifier Loss 0.05289829522371292, Total Loss 5.288337707519531\n",
      "50: Encoding Loss -2.632758140563965, Transition Loss -15.730308532714844, Classifier Loss 0.03431231528520584, Total Loss 3.4280853271484375\n",
      "50: Encoding Loss -1.3823007345199585, Transition Loss -9.699575424194336, Classifier Loss 0.06337636709213257, Total Loss 6.335697174072266\n",
      "50: Encoding Loss -1.395110845565796, Transition Loss -12.910707473754883, Classifier Loss 0.07963819056749344, Total Loss 7.961236953735352\n",
      "50: Encoding Loss -2.4707157611846924, Transition Loss -20.255964279174805, Classifier Loss 0.046554893255233765, Total Loss 4.651438236236572\n",
      "50: Encoding Loss -2.2138824462890625, Transition Loss -9.294095039367676, Classifier Loss 0.059964679181575775, Total Loss 5.994609355926514\n",
      "50: Encoding Loss -1.6828455924987793, Transition Loss -10.850757598876953, Classifier Loss 0.08486505597829819, Total Loss 8.484334945678711\n",
      "50: Encoding Loss -1.8208540678024292, Transition Loss -16.492149353027344, Classifier Loss 0.06910844147205353, Total Loss 6.907546043395996\n",
      "50: Encoding Loss -2.0341036319732666, Transition Loss -13.240694999694824, Classifier Loss 0.024568092077970505, Total Loss 2.4541611671447754\n",
      "50: Encoding Loss -0.9642497301101685, Transition Loss -12.051589012145996, Classifier Loss 0.051006607711315155, Total Loss 5.098250389099121\n",
      "50: Encoding Loss -1.8520715236663818, Transition Loss -5.803095817565918, Classifier Loss 0.03890545666217804, Total Loss 3.8893849849700928\n",
      "50: Encoding Loss -1.141916036605835, Transition Loss -9.280914306640625, Classifier Loss 0.05416973680257797, Total Loss 5.415117263793945\n",
      "50: Encoding Loss -1.7152321338653564, Transition Loss -10.009815216064453, Classifier Loss 0.04814717918634415, Total Loss 4.812716007232666\n",
      "50: Encoding Loss -1.1221060752868652, Transition Loss -5.733866214752197, Classifier Loss 0.028783908113837242, Total Loss 2.877243995666504\n",
      "50: Encoding Loss -1.305774450302124, Transition Loss -10.874258041381836, Classifier Loss 0.025208838284015656, Total Loss 2.5187089443206787\n",
      "50: Encoding Loss -1.474395513534546, Transition Loss -13.185812950134277, Classifier Loss 0.06720886379480362, Total Loss 6.7182488441467285\n",
      "50: Encoding Loss -0.49374324083328247, Transition Loss -12.865436553955078, Classifier Loss 0.04936978965997696, Total Loss 4.934404373168945\n",
      "50: Encoding Loss -1.4169765710830688, Transition Loss -1.4605352878570557, Classifier Loss 0.07676275074481964, Total Loss 7.67598295211792\n",
      "50: Encoding Loss -1.344920039176941, Transition Loss -9.267189979553223, Classifier Loss 0.030112098902463913, Total Loss 3.0093564987182617\n",
      "50: Encoding Loss -0.6597562432289124, Transition Loss -11.874000549316406, Classifier Loss 0.06665569543838501, Total Loss 6.66319465637207\n",
      "50: Encoding Loss -0.301266610622406, Transition Loss -10.410640716552734, Classifier Loss 0.043367259204387665, Total Loss 4.331522464752197\n",
      "50: Encoding Loss -1.1229735612869263, Transition Loss -9.912208557128906, Classifier Loss 0.04754656180739403, Total Loss 4.752674102783203\n",
      "50: Encoding Loss -2.296241521835327, Transition Loss -5.9666056632995605, Classifier Loss 0.06855975836515427, Total Loss 6.8547821044921875\n",
      "50: Encoding Loss -2.352367639541626, Transition Loss -14.83096694946289, Classifier Loss 0.03010937012732029, Total Loss 3.0079708099365234\n",
      "50: Encoding Loss -0.20438127219676971, Transition Loss -2.473222017288208, Classifier Loss 0.045837584882974625, Total Loss 4.549767971038818\n",
      "50: Encoding Loss -1.2092667818069458, Transition Loss -8.596288681030273, Classifier Loss 0.027535220608115196, Total Loss 2.751802921295166\n",
      "50: Encoding Loss -0.7086151838302612, Transition Loss -8.058926582336426, Classifier Loss 0.07390538603067398, Total Loss 7.388926982879639\n",
      "50: Encoding Loss -0.9020864963531494, Transition Loss -16.536434173583984, Classifier Loss 0.13239534199237823, Total Loss 13.236227035522461\n",
      "50: Encoding Loss -2.150775671005249, Transition Loss -16.37449836730957, Classifier Loss 0.06404151767492294, Total Loss 6.400876998901367\n",
      "50: Encoding Loss -2.253020763397217, Transition Loss -21.845754623413086, Classifier Loss 0.03658751770853996, Total Loss 3.6543824672698975\n",
      "50: Encoding Loss -0.9556115865707397, Transition Loss -11.99802303314209, Classifier Loss 0.0857740193605423, Total Loss 8.575002670288086\n",
      "50: Encoding Loss -1.7855616807937622, Transition Loss -13.49325942993164, Classifier Loss 0.043349046260118484, Total Loss 4.3322062492370605\n",
      "50: Encoding Loss -2.5181803703308105, Transition Loss -19.484479904174805, Classifier Loss 0.046233318746089935, Total Loss 4.6194353103637695\n",
      "50: Encoding Loss -1.8628729581832886, Transition Loss -7.746375560760498, Classifier Loss 0.07434674352407455, Total Loss 7.433125019073486\n",
      "50: Encoding Loss -0.8215800523757935, Transition Loss -16.154809951782227, Classifier Loss 0.07439393550157547, Total Loss 7.43616247177124\n",
      "50: Encoding Loss -1.0930836200714111, Transition Loss -17.040767669677734, Classifier Loss 0.021427562460303307, Total Loss 2.139348030090332\n",
      "50: Encoding Loss -1.566681146621704, Transition Loss -20.053544998168945, Classifier Loss 0.05219763517379761, Total Loss 5.215753078460693\n",
      "50: Encoding Loss -1.2842144966125488, Transition Loss -25.73686981201172, Classifier Loss 0.012747146189212799, Total Loss 1.2695672512054443\n",
      "51: Encoding Loss -0.7899038791656494, Transition Loss -5.925783157348633, Classifier Loss 0.029287345707416534, Total Loss 2.927549362182617\n",
      "51: Encoding Loss -1.8225280046463013, Transition Loss -6.7584967613220215, Classifier Loss 0.08449333161115646, Total Loss 8.447981834411621\n",
      "51: Encoding Loss -1.3591355085372925, Transition Loss -1.6963053941726685, Classifier Loss 0.03259610757231712, Total Loss 3.2592713832855225\n",
      "51: Encoding Loss -1.2382351160049438, Transition Loss -19.02785301208496, Classifier Loss 0.048480529338121414, Total Loss 4.844247341156006\n",
      "51: Encoding Loss -1.1388003826141357, Transition Loss -9.855374336242676, Classifier Loss 0.0398804247379303, Total Loss 3.9860713481903076\n",
      "51: Encoding Loss -1.4985466003417969, Transition Loss -15.818441390991211, Classifier Loss 0.04144325107336044, Total Loss 4.1411614418029785\n",
      "51: Encoding Loss -2.2866196632385254, Transition Loss -11.764900207519531, Classifier Loss 0.04898221418261528, Total Loss 4.895868301391602\n",
      "51: Encoding Loss -1.0468249320983887, Transition Loss -9.316656112670898, Classifier Loss 0.041944097727537155, Total Loss 4.192546367645264\n",
      "51: Encoding Loss -1.3868314027786255, Transition Loss -15.849747657775879, Classifier Loss 0.030897216871380806, Total Loss 3.0865516662597656\n",
      "51: Encoding Loss -2.200812339782715, Transition Loss -13.175585746765137, Classifier Loss 0.05206666886806488, Total Loss 5.204031944274902\n",
      "51: Encoding Loss 0.050642792135477066, Transition Loss -9.496044158935547, Classifier Loss 0.043897032737731934, Total Loss 4.668859958648682\n",
      "51: Encoding Loss 0.6693775057792664, Transition Loss -3.763359546661377, Classifier Loss 0.06212291121482849, Total Loss 11.566558837890625\n",
      "51: Encoding Loss -1.4380719661712646, Transition Loss -8.447139739990234, Classifier Loss 0.03955195099115372, Total Loss 3.953505754470825\n",
      "51: Encoding Loss -0.8838605880737305, Transition Loss -8.883702278137207, Classifier Loss 0.056375861167907715, Total Loss 5.635809421539307\n",
      "51: Encoding Loss -2.2920806407928467, Transition Loss -15.648506164550781, Classifier Loss 0.03175624459981918, Total Loss 3.172494649887085\n",
      "51: Encoding Loss -2.000241994857788, Transition Loss -19.46194839477539, Classifier Loss 0.054083485156297684, Total Loss 5.40445613861084\n",
      "51: Encoding Loss -0.2630316913127899, Transition Loss -11.86985969543457, Classifier Loss 0.019223470240831375, Total Loss 1.9109978675842285\n",
      "51: Encoding Loss -0.3238013982772827, Transition Loss -7.997466564178467, Classifier Loss 0.03589542582631111, Total Loss 3.5863840579986572\n",
      "51: Encoding Loss -0.305123895406723, Transition Loss -14.756495475769043, Classifier Loss 0.07331723719835281, Total Loss 7.325991153717041\n",
      "51: Encoding Loss -0.12246724218130112, Transition Loss -7.753522872924805, Classifier Loss 0.04801541566848755, Total Loss 4.691877365112305\n",
      "51: Encoding Loss -1.6097662448883057, Transition Loss -15.953451156616211, Classifier Loss 0.06366904824972153, Total Loss 6.363714218139648\n",
      "51: Encoding Loss -0.6389116644859314, Transition Loss -12.458045959472656, Classifier Loss 0.03322269767522812, Total Loss 3.3197779655456543\n",
      "51: Encoding Loss -0.4454277455806732, Transition Loss -13.204499244689941, Classifier Loss 0.05509832128882408, Total Loss 5.507176399230957\n",
      "51: Encoding Loss -1.2357889413833618, Transition Loss -6.035299301147461, Classifier Loss 0.06448680907487869, Total Loss 6.447474002838135\n",
      "51: Encoding Loss -1.3577897548675537, Transition Loss -12.779722213745117, Classifier Loss 0.027490416541695595, Total Loss 2.746485710144043\n",
      "51: Encoding Loss -0.12414305657148361, Transition Loss -7.019896030426025, Classifier Loss 0.05373286083340645, Total Loss 5.26539421081543\n",
      "51: Encoding Loss -1.830488920211792, Transition Loss -5.588383674621582, Classifier Loss 0.07949772477149963, Total Loss 7.948654651641846\n",
      "51: Encoding Loss -2.5117225646972656, Transition Loss -18.001094818115234, Classifier Loss 0.055800553411245346, Total Loss 5.576455116271973\n",
      "51: Encoding Loss -1.5177152156829834, Transition Loss -10.120251655578613, Classifier Loss 0.04632000997662544, Total Loss 4.629976749420166\n",
      "51: Encoding Loss 0.09940797835588455, Transition Loss -6.7166032791137695, Classifier Loss 0.05530524626374245, Total Loss 6.197129726409912\n",
      "51: Encoding Loss -0.23553986847400665, Transition Loss -15.271434783935547, Classifier Loss 0.05799179524183273, Total Loss 5.778692722320557\n",
      "51: Encoding Loss 0.6496212482452393, Transition Loss -6.286990165710449, Classifier Loss 0.06421104818582535, Total Loss 11.616817474365234\n",
      "51: Encoding Loss 1.318528652191162, Transition Loss -10.659452438354492, Classifier Loss 0.05592580512166023, Total Loss 16.1386775970459\n",
      "51: Encoding Loss -0.11214092373847961, Transition Loss -8.242782592773438, Classifier Loss 0.16307282447814941, Total Loss 16.188058853149414\n",
      "51: Encoding Loss -1.429571509361267, Transition Loss -14.994304656982422, Classifier Loss 0.03636379912495613, Total Loss 3.6333811283111572\n",
      "51: Encoding Loss -0.2642406225204468, Transition Loss -10.01948356628418, Classifier Loss 0.036867547780275345, Total Loss 3.6760499477386475\n",
      "51: Encoding Loss -1.1668356657028198, Transition Loss -9.174853324890137, Classifier Loss 0.057208068668842316, Total Loss 5.718972206115723\n",
      "51: Encoding Loss -0.6753285527229309, Transition Loss -11.444931983947754, Classifier Loss 0.10465635359287262, Total Loss 10.463346481323242\n",
      "51: Encoding Loss -1.1098792552947998, Transition Loss -12.836651802062988, Classifier Loss 0.0523073785007, Total Loss 5.228170394897461\n",
      "51: Encoding Loss -0.8015812039375305, Transition Loss -13.781320571899414, Classifier Loss 0.056429728865623474, Total Loss 5.640216827392578\n",
      "51: Encoding Loss -0.5873015522956848, Transition Loss -10.474583625793457, Classifier Loss 0.027386799454689026, Total Loss 2.7365849018096924\n",
      "51: Encoding Loss -1.436845302581787, Transition Loss -18.607187271118164, Classifier Loss 0.0600697360932827, Total Loss 6.0032525062561035\n",
      "51: Encoding Loss -1.3147947788238525, Transition Loss -17.912519454956055, Classifier Loss 0.02638840302824974, Total Loss 2.6352577209472656\n",
      "51: Encoding Loss -1.4482051134109497, Transition Loss -8.218413352966309, Classifier Loss 0.0545961931347847, Total Loss 5.4579758644104\n",
      "51: Encoding Loss -1.1954257488250732, Transition Loss -19.232410430908203, Classifier Loss 0.036813534796237946, Total Loss 3.677507162094116\n",
      "51: Encoding Loss 0.17514953017234802, Transition Loss -9.768675804138184, Classifier Loss 0.060923196375370026, Total Loss 7.435612201690674\n",
      "51: Encoding Loss -0.7141574621200562, Transition Loss -18.13923454284668, Classifier Loss 0.047570958733558655, Total Loss 4.753468036651611\n",
      "51: Encoding Loss -1.4140323400497437, Transition Loss -6.454856872558594, Classifier Loss 0.025173289701342583, Total Loss 2.516037940979004\n",
      "51: Encoding Loss -2.194824457168579, Transition Loss -9.770231246948242, Classifier Loss 0.04309266805648804, Total Loss 4.307312965393066\n",
      "51: Encoding Loss -1.4350250959396362, Transition Loss -16.331634521484375, Classifier Loss 0.038425642997026443, Total Loss 3.8392980098724365\n",
      "51: Encoding Loss -1.927936315536499, Transition Loss -19.517826080322266, Classifier Loss 0.04477100819349289, Total Loss 4.4731974601745605\n",
      "51: Encoding Loss -0.46762439608573914, Transition Loss -12.809045791625977, Classifier Loss 0.06342296302318573, Total Loss 6.339728832244873\n",
      "51: Encoding Loss -1.1099441051483154, Transition Loss -3.046499252319336, Classifier Loss 0.07941751927137375, Total Loss 7.941142559051514\n",
      "51: Encoding Loss -1.5784422159194946, Transition Loss -12.143928527832031, Classifier Loss 0.03453215956687927, Total Loss 3.450787305831909\n",
      "51: Encoding Loss -0.9394781589508057, Transition Loss -12.167293548583984, Classifier Loss 0.06691088527441025, Total Loss 6.688655376434326\n",
      "51: Encoding Loss -1.550692081451416, Transition Loss -5.472482681274414, Classifier Loss 0.07630416750907898, Total Loss 7.629322528839111\n",
      "51: Encoding Loss -1.1258864402770996, Transition Loss -11.093994140625, Classifier Loss 0.024929221719503403, Total Loss 2.4907033443450928\n",
      "51: Encoding Loss -2.2846038341522217, Transition Loss -12.972579956054688, Classifier Loss 0.04364916309714317, Total Loss 4.362321853637695\n",
      "51: Encoding Loss -2.0996737480163574, Transition Loss -19.23520278930664, Classifier Loss 0.029380038380622864, Total Loss 2.934156656265259\n",
      "51: Encoding Loss -1.7307453155517578, Transition Loss -16.352609634399414, Classifier Loss 0.051601655781269073, Total Loss 5.156895160675049\n",
      "51: Encoding Loss -1.5227105617523193, Transition Loss -9.322408676147461, Classifier Loss 0.05154778063297272, Total Loss 5.152913570404053\n",
      "51: Encoding Loss -2.0089681148529053, Transition Loss -8.1643705368042, Classifier Loss 0.08894933760166168, Total Loss 8.893301010131836\n",
      "51: Encoding Loss -1.9284244775772095, Transition Loss -9.676019668579102, Classifier Loss 0.04204857721924782, Total Loss 4.202922821044922\n",
      "51: Encoding Loss -1.3403249979019165, Transition Loss -7.29173469543457, Classifier Loss 0.060950569808483124, Total Loss 6.09359884262085\n",
      "51: Encoding Loss -1.3177375793457031, Transition Loss -11.449610710144043, Classifier Loss 0.05571475252509117, Total Loss 5.569185256958008\n",
      "51: Encoding Loss -1.9320127964019775, Transition Loss -4.714405536651611, Classifier Loss 0.08734935522079468, Total Loss 8.733992576599121\n",
      "51: Encoding Loss -0.42211776971817017, Transition Loss -1.4590181112289429, Classifier Loss 0.04751872643828392, Total Loss 4.751539707183838\n",
      "51: Encoding Loss -1.705554723739624, Transition Loss -18.38367462158203, Classifier Loss 0.0718255564570427, Total Loss 7.1788787841796875\n",
      "51: Encoding Loss -0.2532508969306946, Transition Loss -8.996451377868652, Classifier Loss 0.057975828647613525, Total Loss 5.784311294555664\n",
      "51: Encoding Loss -0.9318889379501343, Transition Loss -8.730446815490723, Classifier Loss 0.10201609134674072, Total Loss 10.199862480163574\n",
      "51: Encoding Loss -1.5157339572906494, Transition Loss -8.548945426940918, Classifier Loss 0.061142753809690475, Total Loss 6.112565517425537\n",
      "51: Encoding Loss -1.2251911163330078, Transition Loss -8.618112564086914, Classifier Loss 0.06493556499481201, Total Loss 6.491832733154297\n",
      "51: Encoding Loss -0.604655921459198, Transition Loss -5.551642417907715, Classifier Loss 0.07679744809865952, Total Loss 7.678634166717529\n",
      "51: Encoding Loss -1.7274428606033325, Transition Loss -8.756169319152832, Classifier Loss 0.01806977577507496, Total Loss 1.805226445198059\n",
      "51: Encoding Loss -1.9938205480575562, Transition Loss -8.11789321899414, Classifier Loss 0.07987275719642639, Total Loss 7.985651969909668\n",
      "51: Encoding Loss -1.4483851194381714, Transition Loss -14.422918319702148, Classifier Loss 0.029920388013124466, Total Loss 2.989154100418091\n",
      "51: Encoding Loss -1.1195776462554932, Transition Loss -6.4655561447143555, Classifier Loss 0.06376592069864273, Total Loss 6.375298976898193\n",
      "51: Encoding Loss -2.5202279090881348, Transition Loss -12.27957820892334, Classifier Loss 0.04862632974982262, Total Loss 4.860177040100098\n",
      "51: Encoding Loss -1.7907634973526, Transition Loss -13.417885780334473, Classifier Loss 0.021555829793214798, Total Loss 2.1528992652893066\n",
      "51: Encoding Loss -1.65507173538208, Transition Loss -9.974252700805664, Classifier Loss 0.09527912735939026, Total Loss 9.525918006896973\n",
      "51: Encoding Loss -1.16060209274292, Transition Loss -11.152447700500488, Classifier Loss 0.04154954105615616, Total Loss 4.15272331237793\n",
      "51: Encoding Loss -1.675423264503479, Transition Loss -9.074974060058594, Classifier Loss 0.03740803152322769, Total Loss 3.738988161087036\n",
      "51: Encoding Loss -1.2992388010025024, Transition Loss -5.540587425231934, Classifier Loss 0.0847698226571083, Total Loss 8.475873947143555\n",
      "51: Encoding Loss -0.550995945930481, Transition Loss -11.388134002685547, Classifier Loss 0.06749647110700607, Total Loss 6.747369289398193\n",
      "51: Encoding Loss -2.5281240940093994, Transition Loss -7.672461986541748, Classifier Loss 0.04512304440140724, Total Loss 4.510769844055176\n",
      "51: Encoding Loss -1.3859862089157104, Transition Loss -5.857520580291748, Classifier Loss 0.06289279460906982, Total Loss 6.288107872009277\n",
      "51: Encoding Loss -1.9285329580307007, Transition Loss -15.68635082244873, Classifier Loss 0.04960988461971283, Total Loss 4.957851409912109\n",
      "51: Encoding Loss -1.438788652420044, Transition Loss -9.512141227722168, Classifier Loss 0.0998210459947586, Total Loss 9.980201721191406\n",
      "51: Encoding Loss -1.3873099088668823, Transition Loss -7.826464653015137, Classifier Loss 0.04536346346139908, Total Loss 4.534780979156494\n",
      "51: Encoding Loss -1.1142175197601318, Transition Loss -13.595569610595703, Classifier Loss 0.0434703566133976, Total Loss 4.3443169593811035\n",
      "51: Encoding Loss -3.081986904144287, Transition Loss -21.5379581451416, Classifier Loss 0.039541468024253845, Total Loss 3.9498393535614014\n",
      "51: Encoding Loss -1.0424867868423462, Transition Loss -7.0664448738098145, Classifier Loss 0.07107820361852646, Total Loss 7.106407165527344\n",
      "51: Encoding Loss -1.2712310552597046, Transition Loss -7.81695556640625, Classifier Loss 0.06341134011745453, Total Loss 6.33957052230835\n",
      "51: Encoding Loss -0.989031195640564, Transition Loss -9.722439765930176, Classifier Loss 0.05196136236190796, Total Loss 5.194191932678223\n",
      "51: Encoding Loss -1.3351235389709473, Transition Loss -4.17313814163208, Classifier Loss 0.06294567883014679, Total Loss 6.293733596801758\n",
      "51: Encoding Loss -1.6878302097320557, Transition Loss -13.101021766662598, Classifier Loss 0.06275227665901184, Total Loss 6.272607326507568\n",
      "51: Encoding Loss -1.508520245552063, Transition Loss -16.086090087890625, Classifier Loss 0.06246932968497276, Total Loss 6.243715763092041\n",
      "51: Encoding Loss -1.9287556409835815, Transition Loss -12.880191802978516, Classifier Loss 0.04718451201915741, Total Loss 4.715875148773193\n",
      "51: Encoding Loss -1.1679195165634155, Transition Loss -4.419549465179443, Classifier Loss 0.030048657208681107, Total Loss 3.003981828689575\n",
      "51: Encoding Loss -1.9516029357910156, Transition Loss -7.1415605545043945, Classifier Loss 0.0919182151556015, Total Loss 9.19039249420166\n",
      "51: Encoding Loss -2.119889497756958, Transition Loss -14.154863357543945, Classifier Loss 0.05164070799946785, Total Loss 5.1612396240234375\n",
      "51: Encoding Loss -0.8573475480079651, Transition Loss 0.15220725536346436, Classifier Loss 0.09478326141834259, Total Loss 9.508767127990723\n",
      "51: Encoding Loss -1.0135432481765747, Transition Loss -8.478626251220703, Classifier Loss 0.06587337702512741, Total Loss 6.585641860961914\n",
      "51: Encoding Loss -0.5496540069580078, Transition Loss -4.8995795249938965, Classifier Loss 0.06382297724485397, Total Loss 6.381317615509033\n",
      "51: Encoding Loss -1.2368022203445435, Transition Loss -11.894858360290527, Classifier Loss 0.041398026049137115, Total Loss 4.137423515319824\n",
      "51: Encoding Loss -2.179063320159912, Transition Loss -15.684196472167969, Classifier Loss 0.07088949531316757, Total Loss 7.085813045501709\n",
      "51: Encoding Loss -1.1146552562713623, Transition Loss -5.1158857345581055, Classifier Loss 0.06503573060035706, Total Loss 6.502549648284912\n",
      "51: Encoding Loss -1.0106178522109985, Transition Loss -9.159599304199219, Classifier Loss 0.04019678384065628, Total Loss 4.017846584320068\n",
      "51: Encoding Loss -1.5556671619415283, Transition Loss -8.968152046203613, Classifier Loss 0.043697573244571686, Total Loss 4.3679633140563965\n",
      "51: Encoding Loss -0.9709672331809998, Transition Loss -14.199329376220703, Classifier Loss 0.04541759192943573, Total Loss 4.538918972015381\n",
      "51: Encoding Loss -1.323045015335083, Transition Loss -13.971855163574219, Classifier Loss 0.05932356417179108, Total Loss 5.929562091827393\n",
      "51: Encoding Loss -0.9076769948005676, Transition Loss -1.884583592414856, Classifier Loss 0.034188415855169296, Total Loss 3.4184646606445312\n",
      "51: Encoding Loss -1.2795352935791016, Transition Loss -17.67569923400879, Classifier Loss 0.03271334245800972, Total Loss 3.267799139022827\n",
      "51: Encoding Loss -1.8750663995742798, Transition Loss -17.06385612487793, Classifier Loss 0.02967243269085884, Total Loss 2.9638304710388184\n",
      "51: Encoding Loss -0.7781651020050049, Transition Loss -1.359832763671875, Classifier Loss 0.04785612225532532, Total Loss 4.785340309143066\n",
      "51: Encoding Loss -0.6720273494720459, Transition Loss 1.4137372970581055, Classifier Loss 0.04907532036304474, Total Loss 5.190279483795166\n",
      "51: Encoding Loss -1.007847785949707, Transition Loss -11.882006645202637, Classifier Loss 0.03209903836250305, Total Loss 3.2075273990631104\n",
      "51: Encoding Loss -1.3874661922454834, Transition Loss -11.109106063842773, Classifier Loss 0.04421411454677582, Total Loss 4.419189929962158\n",
      "51: Encoding Loss -2.2957704067230225, Transition Loss -13.102932929992676, Classifier Loss 0.048373281955718994, Total Loss 4.834707260131836\n",
      "51: Encoding Loss -1.2410922050476074, Transition Loss -6.762280464172363, Classifier Loss 0.052506156265735626, Total Loss 5.249263286590576\n",
      "51: Encoding Loss -2.5270071029663086, Transition Loss -15.260196685791016, Classifier Loss 0.034624774008989334, Total Loss 3.459425449371338\n",
      "51: Encoding Loss -1.160199761390686, Transition Loss -9.145343780517578, Classifier Loss 0.0624222569167614, Total Loss 6.240396499633789\n",
      "51: Encoding Loss -1.420933485031128, Transition Loss -12.696294784545898, Classifier Loss 0.08174888789653778, Total Loss 8.172348976135254\n",
      "51: Encoding Loss -2.499730110168457, Transition Loss -19.933866500854492, Classifier Loss 0.045406125485897064, Total Loss 4.536625862121582\n",
      "51: Encoding Loss -2.0355398654937744, Transition Loss -8.844992637634277, Classifier Loss 0.05802113190293312, Total Loss 5.800343990325928\n",
      "51: Encoding Loss -1.8853691816329956, Transition Loss -10.333876609802246, Classifier Loss 0.08322072774171829, Total Loss 8.320006370544434\n",
      "51: Encoding Loss -1.8532315492630005, Transition Loss -16.17605209350586, Classifier Loss 0.0673789381980896, Total Loss 6.734658718109131\n",
      "51: Encoding Loss -1.9435302019119263, Transition Loss -12.909157752990723, Classifier Loss 0.02471749112010002, Total Loss 2.4691672325134277\n",
      "51: Encoding Loss -1.032839298248291, Transition Loss -11.701478958129883, Classifier Loss 0.053784944117069244, Total Loss 5.376153945922852\n",
      "51: Encoding Loss -1.968487024307251, Transition Loss -5.288029670715332, Classifier Loss 0.03737642243504524, Total Loss 3.7365846633911133\n",
      "51: Encoding Loss -1.2732701301574707, Transition Loss -8.9047269821167, Classifier Loss 0.05305038392543793, Total Loss 5.303257465362549\n",
      "51: Encoding Loss -1.5920543670654297, Transition Loss -9.637322425842285, Classifier Loss 0.0488443486392498, Total Loss 4.88250732421875\n",
      "51: Encoding Loss -1.2873746156692505, Transition Loss -5.256368160247803, Classifier Loss 0.028660202398896217, Total Loss 2.86496901512146\n",
      "51: Encoding Loss -1.3700006008148193, Transition Loss -10.540043830871582, Classifier Loss 0.02465256117284298, Total Loss 2.4631481170654297\n",
      "51: Encoding Loss -1.2340781688690186, Transition Loss -12.860246658325195, Classifier Loss 0.06363332271575928, Total Loss 6.36076021194458\n",
      "51: Encoding Loss -0.5943889617919922, Transition Loss -12.57954216003418, Classifier Loss 0.04831186681985855, Total Loss 4.828670978546143\n",
      "51: Encoding Loss -1.439928650856018, Transition Loss -0.8883504867553711, Classifier Loss 0.07281990349292755, Total Loss 7.28181266784668\n",
      "51: Encoding Loss -1.3520092964172363, Transition Loss -8.99123764038086, Classifier Loss 0.030620675534009933, Total Loss 3.060269355773926\n",
      "51: Encoding Loss -0.9812516570091248, Transition Loss -11.491146087646484, Classifier Loss 0.06571871042251587, Total Loss 6.569572448730469\n",
      "51: Encoding Loss -0.4290892779827118, Transition Loss -10.176990509033203, Classifier Loss 0.04298941418528557, Total Loss 4.296875476837158\n",
      "51: Encoding Loss -1.1090877056121826, Transition Loss -9.616243362426758, Classifier Loss 0.04349920153617859, Total Loss 4.347997188568115\n",
      "51: Encoding Loss -2.3708767890930176, Transition Loss -5.478347301483154, Classifier Loss 0.06706379354000092, Total Loss 6.7052836418151855\n",
      "51: Encoding Loss -2.2100212574005127, Transition Loss -14.663595199584961, Classifier Loss 0.02972732111811638, Total Loss 2.969799280166626\n",
      "51: Encoding Loss -0.2696395516395569, Transition Loss -1.928924560546875, Classifier Loss 0.04493122547864914, Total Loss 4.4851765632629395\n",
      "51: Encoding Loss -1.1202845573425293, Transition Loss -8.183880805969238, Classifier Loss 0.027401028200984, Total Loss 2.7384660243988037\n",
      "51: Encoding Loss -0.3605611026287079, Transition Loss -7.630947589874268, Classifier Loss 0.07283531874418259, Total Loss 7.281556129455566\n",
      "51: Encoding Loss -0.7558425664901733, Transition Loss -16.18639373779297, Classifier Loss 0.1331261843442917, Total Loss 13.309380531311035\n",
      "51: Encoding Loss -2.3070247173309326, Transition Loss -16.224565505981445, Classifier Loss 0.06367459893226624, Total Loss 6.364214897155762\n",
      "51: Encoding Loss -2.3868839740753174, Transition Loss -21.702163696289062, Classifier Loss 0.035876642912626266, Total Loss 3.5833239555358887\n",
      "51: Encoding Loss -1.2873427867889404, Transition Loss -11.632879257202148, Classifier Loss 0.08899649977684021, Total Loss 8.897322654724121\n",
      "51: Encoding Loss -2.0329511165618896, Transition Loss -13.298830032348633, Classifier Loss 0.04171932861208916, Total Loss 4.1692728996276855\n",
      "51: Encoding Loss -2.560650110244751, Transition Loss -19.392925262451172, Classifier Loss 0.046600475907325745, Total Loss 4.6561689376831055\n",
      "51: Encoding Loss -1.9838236570358276, Transition Loss -7.371421813964844, Classifier Loss 0.07331530749797821, Total Loss 7.330056190490723\n",
      "51: Encoding Loss -1.029718041419983, Transition Loss -15.798783302307129, Classifier Loss 0.07270976155996323, Total Loss 7.267816543579102\n",
      "51: Encoding Loss -1.479300618171692, Transition Loss -16.793285369873047, Classifier Loss 0.02007259614765644, Total Loss 2.0039010047912598\n",
      "51: Encoding Loss -1.73931884765625, Transition Loss -19.860069274902344, Classifier Loss 0.051213521510362625, Total Loss 5.117380142211914\n",
      "51: Encoding Loss -2.1514463424682617, Transition Loss -25.81698989868164, Classifier Loss 0.012377448379993439, Total Loss 1.232581377029419\n",
      "52: Encoding Loss -0.8757154941558838, Transition Loss -5.328421115875244, Classifier Loss 0.028888778761029243, Total Loss 2.8878121376037598\n",
      "52: Encoding Loss -1.9500923156738281, Transition Loss -6.324387073516846, Classifier Loss 0.08513975143432617, Total Loss 8.512710571289062\n",
      "52: Encoding Loss -1.6423814296722412, Transition Loss -0.9908619523048401, Classifier Loss 0.032165393233299255, Total Loss 3.216341257095337\n",
      "52: Encoding Loss -1.4677335023880005, Transition Loss -18.869094848632812, Classifier Loss 0.04890206456184387, Total Loss 4.886432647705078\n",
      "52: Encoding Loss -1.22469961643219, Transition Loss -9.511578559875488, Classifier Loss 0.04039754346013069, Total Loss 4.0378522872924805\n",
      "52: Encoding Loss -1.3348040580749512, Transition Loss -15.463441848754883, Classifier Loss 0.04083491861820221, Total Loss 4.080399036407471\n",
      "52: Encoding Loss -2.2996063232421875, Transition Loss -11.31955623626709, Classifier Loss 0.0476311594247818, Total Loss 4.760851860046387\n",
      "52: Encoding Loss -1.0520641803741455, Transition Loss -8.847875595092773, Classifier Loss 0.042327336966991425, Total Loss 4.230964183807373\n",
      "52: Encoding Loss -1.310846209526062, Transition Loss -15.638294219970703, Classifier Loss 0.030026569962501526, Total Loss 2.9995293617248535\n",
      "52: Encoding Loss -2.22048282623291, Transition Loss -13.006571769714355, Classifier Loss 0.05104200914502144, Total Loss 5.10159969329834\n",
      "52: Encoding Loss -0.4526553750038147, Transition Loss -9.199786186218262, Classifier Loss 0.04364548623561859, Total Loss 4.362697601318359\n",
      "52: Encoding Loss -0.9719807505607605, Transition Loss -5.581103801727295, Classifier Loss 0.05429680272936821, Total Loss 5.428564071655273\n",
      "52: Encoding Loss -1.4848185777664185, Transition Loss -7.529989242553711, Classifier Loss 0.03842150419950485, Total Loss 3.840644359588623\n",
      "52: Encoding Loss -0.8837934136390686, Transition Loss -8.392219543457031, Classifier Loss 0.05003007873892784, Total Loss 5.00132942199707\n",
      "52: Encoding Loss -1.972602367401123, Transition Loss -15.194122314453125, Classifier Loss 0.029597453773021698, Total Loss 2.9567065238952637\n",
      "52: Encoding Loss -2.2250447273254395, Transition Loss -19.193675994873047, Classifier Loss 0.05671769008040428, Total Loss 5.667930603027344\n",
      "52: Encoding Loss -0.9418590068817139, Transition Loss -11.424381256103516, Classifier Loss 0.017154734581708908, Total Loss 1.7131885290145874\n",
      "52: Encoding Loss -0.6427568793296814, Transition Loss -7.642484664916992, Classifier Loss 0.03471726179122925, Total Loss 3.4701976776123047\n",
      "52: Encoding Loss -0.61952143907547, Transition Loss -14.608715057373047, Classifier Loss 0.07132242619991302, Total Loss 7.129321098327637\n",
      "52: Encoding Loss -0.5131711363792419, Transition Loss -6.960573196411133, Classifier Loss 0.04303920641541481, Total Loss 4.302528381347656\n",
      "52: Encoding Loss -1.432700753211975, Transition Loss -16.46500587463379, Classifier Loss 0.06790363788604736, Total Loss 6.7870707511901855\n",
      "52: Encoding Loss -1.4727047681808472, Transition Loss -12.548641204833984, Classifier Loss 0.03138141334056854, Total Loss 3.135631561279297\n",
      "52: Encoding Loss -1.3589411973953247, Transition Loss -13.198948860168457, Classifier Loss 0.05208522081375122, Total Loss 5.2058820724487305\n",
      "52: Encoding Loss -1.0966616868972778, Transition Loss -5.196895122528076, Classifier Loss 0.06814814358949661, Total Loss 6.813775062561035\n",
      "52: Encoding Loss -1.8245967626571655, Transition Loss -13.079416275024414, Classifier Loss 0.027781497687101364, Total Loss 2.77553391456604\n",
      "52: Encoding Loss -0.3636208176612854, Transition Loss -6.675970554351807, Classifier Loss 0.06026960909366608, Total Loss 6.025223255157471\n",
      "52: Encoding Loss -2.236093759536743, Transition Loss -5.161396026611328, Classifier Loss 0.07979027926921844, Total Loss 7.9779953956604\n",
      "52: Encoding Loss -2.5567681789398193, Transition Loss -19.855478286743164, Classifier Loss 0.05715201050043106, Total Loss 5.7112298011779785\n",
      "52: Encoding Loss -1.6674911975860596, Transition Loss -10.650264739990234, Classifier Loss 0.045838482677936554, Total Loss 4.581718444824219\n",
      "52: Encoding Loss -0.6771713495254517, Transition Loss -6.7854905128479, Classifier Loss 0.053462717682123184, Total Loss 5.34491491317749\n",
      "52: Encoding Loss -1.0346661806106567, Transition Loss -13.005946159362793, Classifier Loss 0.055048465728759766, Total Loss 5.5022454261779785\n",
      "52: Encoding Loss -1.4957400560379028, Transition Loss -4.541665554046631, Classifier Loss 0.06743863224983215, Total Loss 6.742954730987549\n",
      "52: Encoding Loss -0.8125651478767395, Transition Loss -12.370323181152344, Classifier Loss 0.05561058968305588, Total Loss 5.558585166931152\n",
      "52: Encoding Loss -0.45750123262405396, Transition Loss -7.03092622756958, Classifier Loss 0.14640793204307556, Total Loss 14.639378547668457\n",
      "52: Encoding Loss -1.7532099485397339, Transition Loss -14.230917930603027, Classifier Loss 0.03575233370065689, Total Loss 3.572387218475342\n",
      "52: Encoding Loss -0.7675549387931824, Transition Loss -9.174969673156738, Classifier Loss 0.03526860103011131, Total Loss 3.525024890899658\n",
      "52: Encoding Loss -0.6866929531097412, Transition Loss -8.27681827545166, Classifier Loss 0.054904986172914505, Total Loss 5.488842964172363\n",
      "52: Encoding Loss -0.9876114130020142, Transition Loss -10.45967960357666, Classifier Loss 0.09260087460279465, Total Loss 9.257994651794434\n",
      "52: Encoding Loss -1.550893783569336, Transition Loss -11.777005195617676, Classifier Loss 0.04482650384306908, Total Loss 4.480294704437256\n",
      "52: Encoding Loss -0.7222213745117188, Transition Loss -12.987289428710938, Classifier Loss 0.052641477435827255, Total Loss 5.261550426483154\n",
      "52: Encoding Loss -0.6611570715904236, Transition Loss -9.535041809082031, Classifier Loss 0.028276199474930763, Total Loss 2.8257129192352295\n",
      "52: Encoding Loss -1.3646794557571411, Transition Loss -17.44498634338379, Classifier Loss 0.06371921300888062, Total Loss 6.368432521820068\n",
      "52: Encoding Loss -1.612687349319458, Transition Loss -16.749000549316406, Classifier Loss 0.026737352833151817, Total Loss 2.6703855991363525\n",
      "52: Encoding Loss -1.8880596160888672, Transition Loss -7.259800910949707, Classifier Loss 0.05135582014918327, Total Loss 5.134130001068115\n",
      "52: Encoding Loss -1.3269466161727905, Transition Loss -18.06923484802246, Classifier Loss 0.03588813915848732, Total Loss 3.585200071334839\n",
      "52: Encoding Loss -0.013113868422806263, Transition Loss -8.620802879333496, Classifier Loss 0.06014537066221237, Total Loss 5.965829849243164\n",
      "52: Encoding Loss -0.4611045718193054, Transition Loss -18.89397430419922, Classifier Loss 0.05010624974966049, Total Loss 5.006838798522949\n",
      "52: Encoding Loss -1.7428314685821533, Transition Loss -7.431305408477783, Classifier Loss 0.024884836748242378, Total Loss 2.486997365951538\n",
      "52: Encoding Loss -2.4305648803710938, Transition Loss -10.542280197143555, Classifier Loss 0.042566828429698944, Total Loss 4.254574298858643\n",
      "52: Encoding Loss -1.65804922580719, Transition Loss -17.02904510498047, Classifier Loss 0.03872441500425339, Total Loss 3.8690357208251953\n",
      "52: Encoding Loss -2.161447048187256, Transition Loss -20.129545211791992, Classifier Loss 0.04763799533247948, Total Loss 4.7597737312316895\n",
      "52: Encoding Loss -0.5249123573303223, Transition Loss -13.566486358642578, Classifier Loss 0.06255029886960983, Total Loss 6.252315998077393\n",
      "52: Encoding Loss -1.4360930919647217, Transition Loss -3.8104660511016846, Classifier Loss 0.07651597261428833, Total Loss 7.650835037231445\n",
      "52: Encoding Loss -1.5458396673202515, Transition Loss -13.081894874572754, Classifier Loss 0.03237004578113556, Total Loss 3.2343881130218506\n",
      "52: Encoding Loss -0.8672130703926086, Transition Loss -12.928308486938477, Classifier Loss 0.06548455357551575, Total Loss 6.54586935043335\n",
      "52: Encoding Loss -1.693597674369812, Transition Loss -6.366778373718262, Classifier Loss 0.07506335526704788, Total Loss 7.505062580108643\n",
      "52: Encoding Loss -0.9711642861366272, Transition Loss -11.889450073242188, Classifier Loss 0.025344926863908768, Total Loss 2.5321147441864014\n",
      "52: Encoding Loss -2.19499135017395, Transition Loss -13.84333610534668, Classifier Loss 0.04216102510690689, Total Loss 4.213334083557129\n",
      "52: Encoding Loss -2.4053902626037598, Transition Loss -20.024396896362305, Classifier Loss 0.029855169355869293, Total Loss 2.9815120697021484\n",
      "52: Encoding Loss -1.7935503721237183, Transition Loss -16.8985595703125, Classifier Loss 0.0531008280813694, Total Loss 5.306703090667725\n",
      "52: Encoding Loss -1.7824195623397827, Transition Loss -10.098346710205078, Classifier Loss 0.05098504200577736, Total Loss 5.096484184265137\n",
      "52: Encoding Loss -2.3883185386657715, Transition Loss -8.845247268676758, Classifier Loss 0.08224862813949585, Total Loss 8.22309398651123\n",
      "52: Encoding Loss -2.1424989700317383, Transition Loss -10.522481918334961, Classifier Loss 0.040452081710100174, Total Loss 4.0431036949157715\n",
      "52: Encoding Loss -1.0033284425735474, Transition Loss -8.220999717712402, Classifier Loss 0.061851490288972855, Total Loss 6.183505058288574\n",
      "52: Encoding Loss -1.4909805059432983, Transition Loss -12.328536987304688, Classifier Loss 0.0559881217777729, Total Loss 5.596346378326416\n",
      "52: Encoding Loss -2.183227062225342, Transition Loss -5.6952409744262695, Classifier Loss 0.08696234226226807, Total Loss 8.69509506225586\n",
      "52: Encoding Loss -0.7008693814277649, Transition Loss -2.407028913497925, Classifier Loss 0.04860609024763107, Total Loss 4.8601274490356445\n",
      "52: Encoding Loss -1.8486896753311157, Transition Loss -19.026031494140625, Classifier Loss 0.06870991736650467, Total Loss 6.867186546325684\n",
      "52: Encoding Loss -0.39520713686943054, Transition Loss -9.717927932739258, Classifier Loss 0.060205068439245224, Total Loss 6.0184407234191895\n",
      "52: Encoding Loss -1.1769206523895264, Transition Loss -9.621844291687012, Classifier Loss 0.09199123084545135, Total Loss 9.197198867797852\n",
      "52: Encoding Loss -1.8807330131530762, Transition Loss -9.493178367614746, Classifier Loss 0.05913642421364784, Total Loss 5.911743640899658\n",
      "52: Encoding Loss -1.4034088850021362, Transition Loss -9.583304405212402, Classifier Loss 0.06477652490139008, Total Loss 6.475735664367676\n",
      "52: Encoding Loss -0.9095780253410339, Transition Loss -6.3329854011535645, Classifier Loss 0.07603523135185242, Total Loss 7.602256774902344\n",
      "52: Encoding Loss -1.6309951543807983, Transition Loss -9.652242660522461, Classifier Loss 0.01713215559720993, Total Loss 1.71128511428833\n",
      "52: Encoding Loss -2.208050489425659, Transition Loss -8.773112297058105, Classifier Loss 0.08523759990930557, Total Loss 8.522005081176758\n",
      "52: Encoding Loss -1.4676165580749512, Transition Loss -15.065309524536133, Classifier Loss 0.029054932296276093, Total Loss 2.902480125427246\n",
      "52: Encoding Loss -1.3062381744384766, Transition Loss -7.024432182312012, Classifier Loss 0.061310820281505585, Total Loss 6.1296772956848145\n",
      "52: Encoding Loss -2.6319549083709717, Transition Loss -12.987691879272461, Classifier Loss 0.04780702292919159, Total Loss 4.778104782104492\n",
      "52: Encoding Loss -1.7293167114257812, Transition Loss -13.839693069458008, Classifier Loss 0.01863892190158367, Total Loss 1.8611242771148682\n",
      "52: Encoding Loss -1.73783540725708, Transition Loss -10.478229522705078, Classifier Loss 0.0873338058590889, Total Loss 8.731285095214844\n",
      "52: Encoding Loss -0.9349939823150635, Transition Loss -11.75244426727295, Classifier Loss 0.04137296974658966, Total Loss 4.134946823120117\n",
      "52: Encoding Loss -1.7644556760787964, Transition Loss -9.444265365600586, Classifier Loss 0.03642803430557251, Total Loss 3.6409146785736084\n",
      "52: Encoding Loss -1.3586387634277344, Transition Loss -6.0993170738220215, Classifier Loss 0.08254273235797882, Total Loss 8.253053665161133\n",
      "52: Encoding Loss -0.6201750040054321, Transition Loss -11.965751647949219, Classifier Loss 0.06771399825811386, Total Loss 6.769006729125977\n",
      "52: Encoding Loss -2.367234468460083, Transition Loss -8.328798294067383, Classifier Loss 0.04553980752825737, Total Loss 4.5523152351379395\n",
      "52: Encoding Loss -1.3598624467849731, Transition Loss -6.438965797424316, Classifier Loss 0.06292659044265747, Total Loss 6.291370868682861\n",
      "52: Encoding Loss -2.1003482341766357, Transition Loss -16.201688766479492, Classifier Loss 0.05233784765005112, Total Loss 5.230544567108154\n",
      "52: Encoding Loss -1.450563907623291, Transition Loss -10.015528678894043, Classifier Loss 0.09703207015991211, Total Loss 9.701204299926758\n",
      "52: Encoding Loss -1.1580628156661987, Transition Loss -8.59979248046875, Classifier Loss 0.04430488124489784, Total Loss 4.428768157958984\n",
      "52: Encoding Loss -1.166323184967041, Transition Loss -14.109801292419434, Classifier Loss 0.046598855406045914, Total Loss 4.6570634841918945\n",
      "52: Encoding Loss -3.4708924293518066, Transition Loss -22.014205932617188, Classifier Loss 0.04033975675702095, Total Loss 4.0295729637146\n",
      "52: Encoding Loss -0.9970390200614929, Transition Loss -7.710783004760742, Classifier Loss 0.07137145102024078, Total Loss 7.135602951049805\n",
      "52: Encoding Loss -1.4265635013580322, Transition Loss -8.17566967010498, Classifier Loss 0.062323205173015594, Total Loss 6.230685234069824\n",
      "52: Encoding Loss -0.8971823453903198, Transition Loss -10.190462112426758, Classifier Loss 0.05248020961880684, Total Loss 5.245983123779297\n",
      "52: Encoding Loss -1.4412955045700073, Transition Loss -4.663700103759766, Classifier Loss 0.062145113945007324, Total Loss 6.213578701019287\n",
      "52: Encoding Loss -1.7465296983718872, Transition Loss -13.517481803894043, Classifier Loss 0.061646394431591034, Total Loss 6.161935806274414\n",
      "52: Encoding Loss -1.5879689455032349, Transition Loss -16.497900009155273, Classifier Loss 0.05984341353178024, Total Loss 5.981041431427002\n",
      "52: Encoding Loss -1.7699791193008423, Transition Loss -13.402584075927734, Classifier Loss 0.04540877416729927, Total Loss 4.538197040557861\n",
      "52: Encoding Loss -1.1691046953201294, Transition Loss -4.906513690948486, Classifier Loss 0.03031173348426819, Total Loss 3.0301918983459473\n",
      "52: Encoding Loss -2.0224220752716064, Transition Loss -7.5465192794799805, Classifier Loss 0.08743537217378616, Total Loss 8.742027282714844\n",
      "52: Encoding Loss -2.2608025074005127, Transition Loss -14.316361427307129, Classifier Loss 0.05239521712064743, Total Loss 5.236658096313477\n",
      "52: Encoding Loss -0.8784641623497009, Transition Loss -0.07015520334243774, Classifier Loss 0.09378217160701752, Total Loss 9.378202438354492\n",
      "52: Encoding Loss -0.9099971652030945, Transition Loss -8.565811157226562, Classifier Loss 0.06418578326702118, Total Loss 6.41686487197876\n",
      "52: Encoding Loss -0.2672404646873474, Transition Loss -5.274468898773193, Classifier Loss 0.06386493146419525, Total Loss 6.377388000488281\n",
      "52: Encoding Loss -0.9073838591575623, Transition Loss -12.134286880493164, Classifier Loss 0.04145606979727745, Total Loss 4.1431803703308105\n",
      "52: Encoding Loss -2.0402414798736572, Transition Loss -15.607684135437012, Classifier Loss 0.0625830590724945, Total Loss 6.255184650421143\n",
      "52: Encoding Loss -0.8230388760566711, Transition Loss -5.333113193511963, Classifier Loss 0.06318281590938568, Total Loss 6.3172149658203125\n",
      "52: Encoding Loss -0.38627687096595764, Transition Loss -8.979450225830078, Classifier Loss 0.03940420597791672, Total Loss 3.9384512901306152\n",
      "52: Encoding Loss -1.0359481573104858, Transition Loss -8.862177848815918, Classifier Loss 0.04363333433866501, Total Loss 4.361560821533203\n",
      "52: Encoding Loss -0.661148190498352, Transition Loss -14.051736831665039, Classifier Loss 0.044714417308568954, Total Loss 4.468631267547607\n",
      "52: Encoding Loss -0.967739462852478, Transition Loss -13.793058395385742, Classifier Loss 0.058061856776475906, Total Loss 5.803427219390869\n",
      "52: Encoding Loss -0.4488055109977722, Transition Loss -1.9445313215255737, Classifier Loss 0.034536316990852356, Total Loss 3.4532299041748047\n",
      "52: Encoding Loss -0.7279137372970581, Transition Loss -17.385221481323242, Classifier Loss 0.03223424032330513, Total Loss 3.21994686126709\n",
      "52: Encoding Loss -1.9076282978057861, Transition Loss -16.853145599365234, Classifier Loss 0.027744224295020103, Total Loss 2.7710518836975098\n",
      "52: Encoding Loss -0.6496424078941345, Transition Loss -1.1521506309509277, Classifier Loss 0.046518724411726, Total Loss 4.651642322540283\n",
      "52: Encoding Loss -0.1626387983560562, Transition Loss 1.4573652744293213, Classifier Loss 0.051843658089637756, Total Loss 5.408267498016357\n",
      "52: Encoding Loss -0.11167516559362411, Transition Loss -11.117599487304688, Classifier Loss 0.03198014572262764, Total Loss 3.07781720161438\n",
      "52: Encoding Loss 0.1326766461133957, Transition Loss -9.465059280395508, Classifier Loss 0.045141786336898804, Total Loss 5.475738048553467\n",
      "52: Encoding Loss 2.6618096828460693, Transition Loss -17.224590301513672, Classifier Loss 0.04712817072868347, Total Loss 26.003849029541016\n",
      "52: Encoding Loss 1.7962230443954468, Transition Loss -6.6468682289123535, Classifier Loss 0.05790231004357338, Total Loss 20.1586856842041\n",
      "52: Encoding Loss -2.284636974334717, Transition Loss -14.20828914642334, Classifier Loss 0.03499411791563034, Total Loss 3.496570110321045\n",
      "52: Encoding Loss -1.0502305030822754, Transition Loss -8.719161987304688, Classifier Loss 0.0617666020989418, Total Loss 6.1749162673950195\n",
      "52: Encoding Loss -1.2186863422393799, Transition Loss -12.060052871704102, Classifier Loss 0.07590299844741821, Total Loss 7.587887763977051\n",
      "52: Encoding Loss -2.289304494857788, Transition Loss -18.096160888671875, Classifier Loss 0.049027230590581894, Total Loss 4.89910364151001\n",
      "52: Encoding Loss -1.7549933195114136, Transition Loss -8.803854942321777, Classifier Loss 0.05732036381959915, Total Loss 5.730275630950928\n",
      "52: Encoding Loss -1.385969638824463, Transition Loss -10.031808853149414, Classifier Loss 0.08026358485221863, Total Loss 8.024352073669434\n",
      "52: Encoding Loss -1.5944671630859375, Transition Loss -14.839912414550781, Classifier Loss 0.06377702206373215, Total Loss 6.374734401702881\n",
      "52: Encoding Loss -1.4886209964752197, Transition Loss -12.099326133728027, Classifier Loss 0.02414868026971817, Total Loss 2.4124481678009033\n",
      "52: Encoding Loss -0.382605642080307, Transition Loss -11.265294075012207, Classifier Loss 0.04633728787302971, Total Loss 4.631276607513428\n",
      "52: Encoding Loss -0.9410727620124817, Transition Loss -5.645988464355469, Classifier Loss 0.03671170026063919, Total Loss 3.6700408458709717\n",
      "52: Encoding Loss -0.5193876624107361, Transition Loss -8.840861320495605, Classifier Loss 0.05120532214641571, Total Loss 5.118763446807861\n",
      "52: Encoding Loss -1.2120115756988525, Transition Loss -9.294116973876953, Classifier Loss 0.047342654317617416, Total Loss 4.7324066162109375\n",
      "52: Encoding Loss -0.6457854509353638, Transition Loss -5.5729265213012695, Classifier Loss 0.02792512997984886, Total Loss 2.791398286819458\n",
      "52: Encoding Loss -0.9291883707046509, Transition Loss -10.080131530761719, Classifier Loss 0.024693764746189117, Total Loss 2.467360496520996\n",
      "52: Encoding Loss -1.1014487743377686, Transition Loss -12.025979042053223, Classifier Loss 0.06338540464639664, Total Loss 6.336135387420654\n",
      "52: Encoding Loss 0.05572308599948883, Transition Loss -11.70975112915039, Classifier Loss 0.04843326658010483, Total Loss 5.158078193664551\n",
      "52: Encoding Loss -1.1972389221191406, Transition Loss -2.464651107788086, Classifier Loss 0.07575570046901703, Total Loss 7.575077056884766\n",
      "52: Encoding Loss -1.2522673606872559, Transition Loss -10.69175910949707, Classifier Loss 0.03102552518248558, Total Loss 3.1004140377044678\n",
      "52: Encoding Loss -0.7694692015647888, Transition Loss -13.577771186828613, Classifier Loss 0.062467873096466064, Total Loss 6.2440714836120605\n",
      "52: Encoding Loss -0.29845407605171204, Transition Loss -11.682977676391602, Classifier Loss 0.04325180873274803, Total Loss 4.319454193115234\n",
      "52: Encoding Loss -1.2232742309570312, Transition Loss -11.423667907714844, Classifier Loss 0.043448396027088165, Total Loss 4.342555046081543\n",
      "52: Encoding Loss -2.3371052742004395, Transition Loss -7.231385707855225, Classifier Loss 0.0660480484366417, Total Loss 6.603358745574951\n",
      "52: Encoding Loss -2.3485515117645264, Transition Loss -16.51508140563965, Classifier Loss 0.030224869027733803, Total Loss 3.019183874130249\n",
      "52: Encoding Loss -0.08569984883069992, Transition Loss -3.4519944190979004, Classifier Loss 0.0448312908411026, Total Loss 4.3482513427734375\n",
      "52: Encoding Loss -1.173190712928772, Transition Loss -9.978341102600098, Classifier Loss 0.027405191212892532, Total Loss 2.738523483276367\n",
      "52: Encoding Loss -0.3340156674385071, Transition Loss -9.510348320007324, Classifier Loss 0.06868444383144379, Total Loss 6.865423679351807\n",
      "52: Encoding Loss -0.6177716851234436, Transition Loss -18.419845581054688, Classifier Loss 0.1300673931837082, Total Loss 13.003055572509766\n",
      "52: Encoding Loss -2.4634246826171875, Transition Loss -17.971332550048828, Classifier Loss 0.0633540153503418, Total Loss 6.3318071365356445\n",
      "52: Encoding Loss -2.475270986557007, Transition Loss -24.028976440429688, Classifier Loss 0.03398214653134346, Total Loss 3.39340877532959\n",
      "52: Encoding Loss -1.1629749536514282, Transition Loss -13.385085105895996, Classifier Loss 0.08251455426216125, Total Loss 8.248778343200684\n",
      "52: Encoding Loss -2.1007657051086426, Transition Loss -15.014495849609375, Classifier Loss 0.04194154962897301, Total Loss 4.1911516189575195\n",
      "52: Encoding Loss -2.6090476512908936, Transition Loss -21.60426902770996, Classifier Loss 0.04437799006700516, Total Loss 4.433478355407715\n",
      "52: Encoding Loss -1.8443303108215332, Transition Loss -8.925840377807617, Classifier Loss 0.07289683818817139, Total Loss 7.287898540496826\n",
      "52: Encoding Loss -1.1278640031814575, Transition Loss -18.01690673828125, Classifier Loss 0.07160309702157974, Total Loss 7.1567063331604\n",
      "52: Encoding Loss -1.652122974395752, Transition Loss -18.927093505859375, Classifier Loss 0.02102898433804512, Total Loss 2.0991129875183105\n",
      "52: Encoding Loss -1.7199980020523071, Transition Loss -22.120521545410156, Classifier Loss 0.0507989376783371, Total Loss 5.075469493865967\n",
      "52: Encoding Loss -2.4215874671936035, Transition Loss -28.440956115722656, Classifier Loss 0.013211550191044807, Total Loss 1.3154668807983398\n",
      "53: Encoding Loss -0.873570442199707, Transition Loss -6.984354019165039, Classifier Loss 0.028227005153894424, Total Loss 2.821303606033325\n",
      "53: Encoding Loss -1.6695270538330078, Transition Loss -7.737062931060791, Classifier Loss 0.08107762783765793, Total Loss 8.10621452331543\n",
      "53: Encoding Loss -1.5698473453521729, Transition Loss -2.530470609664917, Classifier Loss 0.03125469386577606, Total Loss 3.1249632835388184\n",
      "53: Encoding Loss -1.5365574359893799, Transition Loss -21.07293701171875, Classifier Loss 0.050488077104091644, Total Loss 5.04459285736084\n",
      "53: Encoding Loss -1.267618179321289, Transition Loss -11.137320518493652, Classifier Loss 0.04010675102472305, Total Loss 4.008447647094727\n",
      "53: Encoding Loss -1.5877494812011719, Transition Loss -17.72478485107422, Classifier Loss 0.041231583803892136, Total Loss 4.1196136474609375\n",
      "53: Encoding Loss -2.3097941875457764, Transition Loss -13.334744453430176, Classifier Loss 0.046376001089811325, Total Loss 4.634932994842529\n",
      "53: Encoding Loss -1.2863025665283203, Transition Loss -10.705809593200684, Classifier Loss 0.04072950407862663, Total Loss 4.070809364318848\n",
      "53: Encoding Loss -1.1799204349517822, Transition Loss -17.643510818481445, Classifier Loss 0.029866768047213554, Total Loss 2.9831480979919434\n",
      "53: Encoding Loss -2.2188689708709717, Transition Loss -14.76408576965332, Classifier Loss 0.049631617963314056, Total Loss 4.960208892822266\n",
      "53: Encoding Loss -0.30024826526641846, Transition Loss -10.890596389770508, Classifier Loss 0.04303917661309242, Total Loss 4.298523426055908\n",
      "53: Encoding Loss -1.1422532796859741, Transition Loss -7.208380699157715, Classifier Loss 0.05319994315505028, Total Loss 5.3185529708862305\n",
      "53: Encoding Loss -1.5019367933273315, Transition Loss -9.267977714538574, Classifier Loss 0.0381217896938324, Total Loss 3.8103253841400146\n",
      "53: Encoding Loss -0.8674795031547546, Transition Loss -9.91876220703125, Classifier Loss 0.05053051933646202, Total Loss 5.051068305969238\n",
      "53: Encoding Loss -2.1526312828063965, Transition Loss -17.557096481323242, Classifier Loss 0.02912934496998787, Total Loss 2.9094231128692627\n",
      "53: Encoding Loss -2.148554563522339, Transition Loss -21.695268630981445, Classifier Loss 0.05504896119236946, Total Loss 5.500556945800781\n",
      "53: Encoding Loss -0.8769607543945312, Transition Loss -13.362643241882324, Classifier Loss 0.017939800396561623, Total Loss 1.7913074493408203\n",
      "53: Encoding Loss -0.5422939658164978, Transition Loss -9.080095291137695, Classifier Loss 0.03322093188762665, Total Loss 3.320276975631714\n",
      "53: Encoding Loss -0.5771555304527283, Transition Loss -16.586484909057617, Classifier Loss 0.0708097368478775, Total Loss 7.077656269073486\n",
      "53: Encoding Loss -0.6640856862068176, Transition Loss -8.674781799316406, Classifier Loss 0.04413503780961037, Total Loss 4.411768913269043\n",
      "53: Encoding Loss -1.3687914609909058, Transition Loss -18.537757873535156, Classifier Loss 0.06726497411727905, Total Loss 6.722790241241455\n",
      "53: Encoding Loss -1.60563325881958, Transition Loss -14.497912406921387, Classifier Loss 0.03141222149133682, Total Loss 3.138322591781616\n",
      "53: Encoding Loss -1.4641149044036865, Transition Loss -15.36872386932373, Classifier Loss 0.05066791921854019, Total Loss 5.063718318939209\n",
      "53: Encoding Loss -1.1644082069396973, Transition Loss -6.819894790649414, Classifier Loss 0.06702118366956711, Total Loss 6.700754642486572\n",
      "53: Encoding Loss -1.845893383026123, Transition Loss -15.141469955444336, Classifier Loss 0.027753692120313644, Total Loss 2.772340774536133\n",
      "53: Encoding Loss -0.35958290100097656, Transition Loss -8.19951057434082, Classifier Loss 0.054563384503126144, Total Loss 5.454233646392822\n",
      "53: Encoding Loss -2.2752950191497803, Transition Loss -6.774839401245117, Classifier Loss 0.07928411662578583, Total Loss 7.927056312561035\n",
      "53: Encoding Loss -2.8194010257720947, Transition Loss -22.538227081298828, Classifier Loss 0.05570106580853462, Total Loss 5.565598964691162\n",
      "53: Encoding Loss -1.7103370428085327, Transition Loss -12.5169038772583, Classifier Loss 0.044831711798906326, Total Loss 4.480667591094971\n",
      "53: Encoding Loss -0.853752076625824, Transition Loss -8.371686935424805, Classifier Loss 0.052527591586112976, Total Loss 5.251084804534912\n",
      "53: Encoding Loss -1.0610980987548828, Transition Loss -14.83725357055664, Classifier Loss 0.053892832249403, Total Loss 5.386315822601318\n",
      "53: Encoding Loss -1.4384254217147827, Transition Loss -5.936140060424805, Classifier Loss 0.06600506603717804, Total Loss 6.5993194580078125\n",
      "53: Encoding Loss -0.9011488556861877, Transition Loss -14.09241008758545, Classifier Loss 0.053607698529958725, Total Loss 5.3579511642456055\n",
      "53: Encoding Loss -0.6637506484985352, Transition Loss -8.797907829284668, Classifier Loss 0.1463417112827301, Total Loss 14.63241195678711\n",
      "53: Encoding Loss -1.8964630365371704, Transition Loss -16.01581382751465, Classifier Loss 0.0359969288110733, Total Loss 3.596489667892456\n",
      "53: Encoding Loss -0.7451788187026978, Transition Loss -10.77061653137207, Classifier Loss 0.037080731242895126, Total Loss 3.705919027328491\n",
      "53: Encoding Loss -0.9723769426345825, Transition Loss -9.94215202331543, Classifier Loss 0.05370016396045685, Total Loss 5.368028163909912\n",
      "53: Encoding Loss -1.0899282693862915, Transition Loss -12.188202857971191, Classifier Loss 0.0967680960893631, Total Loss 9.674371719360352\n",
      "53: Encoding Loss -1.6182606220245361, Transition Loss -13.702383041381836, Classifier Loss 0.04830031841993332, Total Loss 4.827291488647461\n",
      "53: Encoding Loss -0.9447563290596008, Transition Loss -14.717824935913086, Classifier Loss 0.05272817611694336, Total Loss 5.269874095916748\n",
      "53: Encoding Loss -0.7142205834388733, Transition Loss -11.296422958374023, Classifier Loss 0.026311643421649933, Total Loss 2.6289050579071045\n",
      "53: Encoding Loss -1.6828532218933105, Transition Loss -19.603700637817383, Classifier Loss 0.05941159278154373, Total Loss 5.937238693237305\n",
      "53: Encoding Loss -1.7796251773834229, Transition Loss -19.009105682373047, Classifier Loss 0.026915477588772774, Total Loss 2.687746047973633\n",
      "53: Encoding Loss -2.0836236476898193, Transition Loss -8.687505722045898, Classifier Loss 0.04734024778008461, Total Loss 4.732287406921387\n",
      "53: Encoding Loss -1.6104882955551147, Transition Loss -20.21903419494629, Classifier Loss 0.03642198070883751, Total Loss 3.6381542682647705\n",
      "53: Encoding Loss -0.1465371996164322, Transition Loss -10.326981544494629, Classifier Loss 0.059675224125385284, Total Loss 5.881743907928467\n",
      "53: Encoding Loss -0.8857039213180542, Transition Loss -21.269184112548828, Classifier Loss 0.04943784698843956, Total Loss 4.939530849456787\n",
      "53: Encoding Loss -1.7515922784805298, Transition Loss -9.299603462219238, Classifier Loss 0.026145702227950096, Total Loss 2.6127102375030518\n",
      "53: Encoding Loss -1.9410946369171143, Transition Loss -12.598928451538086, Classifier Loss 0.04388537257909775, Total Loss 4.386017799377441\n",
      "53: Encoding Loss -1.425337553024292, Transition Loss -19.598310470581055, Classifier Loss 0.04002115875482559, Total Loss 3.9981961250305176\n",
      "53: Encoding Loss -2.07452392578125, Transition Loss -22.99563980102539, Classifier Loss 0.042609114199876785, Total Loss 4.256312370300293\n",
      "53: Encoding Loss -0.6082314848899841, Transition Loss -15.939138412475586, Classifier Loss 0.0608639195561409, Total Loss 6.08320426940918\n",
      "53: Encoding Loss -0.9570913314819336, Transition Loss -5.848667144775391, Classifier Loss 0.0762859359383583, Total Loss 7.6274237632751465\n",
      "53: Encoding Loss -0.9053350687026978, Transition Loss -15.474119186401367, Classifier Loss 0.031887516379356384, Total Loss 3.185656785964966\n",
      "53: Encoding Loss -0.6130611896514893, Transition Loss -15.130608558654785, Classifier Loss 0.0655471682548523, Total Loss 6.551691055297852\n",
      "53: Encoding Loss -1.5319724082946777, Transition Loss -8.002716064453125, Classifier Loss 0.07434742152690887, Total Loss 7.433141231536865\n",
      "53: Encoding Loss -0.6727019548416138, Transition Loss -14.014123916625977, Classifier Loss 0.02569398656487465, Total Loss 2.5665957927703857\n",
      "53: Encoding Loss -1.6507830619812012, Transition Loss -16.26075553894043, Classifier Loss 0.042748354375362396, Total Loss 4.271583557128906\n",
      "53: Encoding Loss -2.035719633102417, Transition Loss -22.720796585083008, Classifier Loss 0.030966339632868767, Total Loss 3.0920896530151367\n",
      "53: Encoding Loss -1.6525864601135254, Transition Loss -19.608938217163086, Classifier Loss 0.050307925790548325, Total Loss 5.0268707275390625\n",
      "53: Encoding Loss -1.2807563543319702, Transition Loss -12.14008617401123, Classifier Loss 0.05137893557548523, Total Loss 5.135465621948242\n",
      "53: Encoding Loss -1.9401112794876099, Transition Loss -10.748367309570312, Classifier Loss 0.08286573737859726, Total Loss 8.284423828125\n",
      "53: Encoding Loss -1.9344156980514526, Transition Loss -12.742195129394531, Classifier Loss 0.04028382524847984, Total Loss 4.025834083557129\n",
      "53: Encoding Loss -0.5960758328437805, Transition Loss -9.948299407958984, Classifier Loss 0.059679485857486725, Total Loss 5.965958595275879\n",
      "53: Encoding Loss -1.038696050643921, Transition Loss -14.47294807434082, Classifier Loss 0.05432472005486488, Total Loss 5.429577827453613\n",
      "53: Encoding Loss -1.8793460130691528, Transition Loss -7.369804859161377, Classifier Loss 0.08392957597970963, Total Loss 8.391483306884766\n",
      "53: Encoding Loss -0.525055468082428, Transition Loss -3.8010947704315186, Classifier Loss 0.048159245401620865, Total Loss 4.815164089202881\n",
      "53: Encoding Loss -1.5094430446624756, Transition Loss -21.46531867980957, Classifier Loss 0.07088139653205872, Total Loss 7.083846569061279\n",
      "53: Encoding Loss -0.18040870130062103, Transition Loss -11.602293014526367, Classifier Loss 0.06018257141113281, Total Loss 5.96454381942749\n",
      "53: Encoding Loss -0.8738889098167419, Transition Loss -11.840682029724121, Classifier Loss 0.09318649768829346, Total Loss 9.31628131866455\n",
      "53: Encoding Loss -1.5672696828842163, Transition Loss -11.59335708618164, Classifier Loss 0.05881781131029129, Total Loss 5.879462242126465\n",
      "53: Encoding Loss -1.0485069751739502, Transition Loss -11.71458625793457, Classifier Loss 0.06342409551143646, Total Loss 6.340066909790039\n",
      "53: Encoding Loss -0.6644846200942993, Transition Loss -8.01301383972168, Classifier Loss 0.07387903332710266, Total Loss 7.386300563812256\n",
      "53: Encoding Loss -1.2449530363082886, Transition Loss -12.090922355651855, Classifier Loss 0.016740689054131508, Total Loss 1.671650767326355\n",
      "53: Encoding Loss -1.8251140117645264, Transition Loss -10.778854370117188, Classifier Loss 0.07581764459609985, Total Loss 7.57960844039917\n",
      "53: Encoding Loss -1.1337902545928955, Transition Loss -17.773956298828125, Classifier Loss 0.028882576152682304, Total Loss 2.884702682495117\n",
      "53: Encoding Loss -1.1281157732009888, Transition Loss -9.35085678100586, Classifier Loss 0.059960462152957916, Total Loss 5.994175910949707\n",
      "53: Encoding Loss -2.2359933853149414, Transition Loss -15.71389389038086, Classifier Loss 0.04677005857229233, Total Loss 4.673862934112549\n",
      "53: Encoding Loss -1.3584167957305908, Transition Loss -16.839214324951172, Classifier Loss 0.02066153660416603, Total Loss 2.0627858638763428\n",
      "53: Encoding Loss -1.5913054943084717, Transition Loss -13.048441886901855, Classifier Loss 0.0833602249622345, Total Loss 8.333413124084473\n",
      "53: Encoding Loss -0.8925438523292542, Transition Loss -14.153910636901855, Classifier Loss 0.039675869047641754, Total Loss 3.9647562503814697\n",
      "53: Encoding Loss -1.6353391408920288, Transition Loss -11.554035186767578, Classifier Loss 0.035742469131946564, Total Loss 3.5719361305236816\n",
      "53: Encoding Loss -1.0545259714126587, Transition Loss -8.145566940307617, Classifier Loss 0.08037250488996506, Total Loss 8.035621643066406\n",
      "53: Encoding Loss -0.2917167544364929, Transition Loss -13.83041763305664, Classifier Loss 0.06676146388053894, Total Loss 6.6692585945129395\n",
      "53: Encoding Loss -2.0231385231018066, Transition Loss -10.678369522094727, Classifier Loss 0.0439087375998497, Total Loss 4.38873815536499\n",
      "53: Encoding Loss -1.1772139072418213, Transition Loss -8.379734992980957, Classifier Loss 0.0624091662466526, Total Loss 6.239240646362305\n",
      "53: Encoding Loss -1.7592800855636597, Transition Loss -19.183015823364258, Classifier Loss 0.04899546131491661, Total Loss 4.89570951461792\n",
      "53: Encoding Loss -1.1765092611312866, Transition Loss -12.757001876831055, Classifier Loss 0.09253714978694916, Total Loss 9.251163482666016\n",
      "53: Encoding Loss -0.6628453731536865, Transition Loss -10.530418395996094, Classifier Loss 0.045182134956121445, Total Loss 4.516107082366943\n",
      "53: Encoding Loss -1.0605151653289795, Transition Loss -16.51740264892578, Classifier Loss 0.044660694897174835, Total Loss 4.462766170501709\n",
      "53: Encoding Loss -3.1567742824554443, Transition Loss -25.63536834716797, Classifier Loss 0.040453992784023285, Total Loss 4.040272235870361\n",
      "53: Encoding Loss -0.7386661767959595, Transition Loss -9.508922576904297, Classifier Loss 0.06803085654973984, Total Loss 6.801184177398682\n",
      "53: Encoding Loss -1.0058090686798096, Transition Loss -10.532593727111816, Classifier Loss 0.06621392071247101, Total Loss 6.619285583496094\n",
      "53: Encoding Loss -0.4356333017349243, Transition Loss -12.55610466003418, Classifier Loss 0.05020265281200409, Total Loss 5.017731189727783\n",
      "53: Encoding Loss -1.1016238927841187, Transition Loss -6.418234348297119, Classifier Loss 0.06258396059274673, Total Loss 6.257112503051758\n",
      "53: Encoding Loss -1.5223273038864136, Transition Loss -16.30590057373047, Classifier Loss 0.06208883970975876, Total Loss 6.205622673034668\n",
      "53: Encoding Loss -1.1659855842590332, Transition Loss -19.460418701171875, Classifier Loss 0.060754887759685516, Total Loss 6.071596622467041\n",
      "53: Encoding Loss -1.3558744192123413, Transition Loss -16.418079376220703, Classifier Loss 0.0459449477493763, Total Loss 4.591211318969727\n",
      "53: Encoding Loss -0.9925162196159363, Transition Loss -6.7766547203063965, Classifier Loss 0.029246017336845398, Total Loss 2.923246383666992\n",
      "53: Encoding Loss -1.8713511228561401, Transition Loss -9.75167179107666, Classifier Loss 0.08797881752252579, Total Loss 8.795931816101074\n",
      "53: Encoding Loss -2.109715223312378, Transition Loss -17.40096664428711, Classifier Loss 0.05029958486557007, Total Loss 5.0264787673950195\n",
      "53: Encoding Loss -0.7286306619644165, Transition Loss -1.6591835021972656, Classifier Loss 0.09316721558570862, Total Loss 9.316390037536621\n",
      "53: Encoding Loss -0.7431514263153076, Transition Loss -10.77940559387207, Classifier Loss 0.06388619542121887, Total Loss 6.386463642120361\n",
      "53: Encoding Loss -0.1010884940624237, Transition Loss -6.965447425842285, Classifier Loss 0.06257285922765732, Total Loss 6.129705429077148\n",
      "53: Encoding Loss -0.508234977722168, Transition Loss -15.101515769958496, Classifier Loss 0.04089640453457832, Total Loss 4.0866193771362305\n",
      "53: Encoding Loss -1.7873502969741821, Transition Loss -19.38093376159668, Classifier Loss 0.06463740020990372, Total Loss 6.459863662719727\n",
      "53: Encoding Loss -0.6422946453094482, Transition Loss -7.421071529388428, Classifier Loss 0.06191672012209892, Total Loss 6.190187454223633\n",
      "53: Encoding Loss -0.4744030237197876, Transition Loss -12.105981826782227, Classifier Loss 0.03964775800704956, Total Loss 3.962350606918335\n",
      "53: Encoding Loss -0.9168310761451721, Transition Loss -11.873003005981445, Classifier Loss 0.04182961955666542, Total Loss 4.180587291717529\n",
      "53: Encoding Loss -0.5049861073493958, Transition Loss -17.852191925048828, Classifier Loss 0.04430362582206726, Total Loss 4.426791191101074\n",
      "53: Encoding Loss -0.8869196772575378, Transition Loss -17.277084350585938, Classifier Loss 0.05679252743721008, Total Loss 5.675796985626221\n",
      "53: Encoding Loss -0.4690192639827728, Transition Loss -3.9222095012664795, Classifier Loss 0.03371497243642807, Total Loss 3.3707075119018555\n",
      "53: Encoding Loss -0.9147323966026306, Transition Loss -21.78291130065918, Classifier Loss 0.03207799047231674, Total Loss 3.203442335128784\n",
      "53: Encoding Loss -1.9547897577285767, Transition Loss -20.810901641845703, Classifier Loss 0.027586933225393295, Total Loss 2.754531145095825\n",
      "53: Encoding Loss -0.6183255910873413, Transition Loss -2.9735753536224365, Classifier Loss 0.045903004705905914, Total Loss 4.589705944061279\n",
      "53: Encoding Loss 0.040364012122154236, Transition Loss 0.32951951026916504, Classifier Loss 0.05081488564610481, Total Loss 5.359468460083008\n",
      "53: Encoding Loss -0.9616798162460327, Transition Loss -11.750036239624023, Classifier Loss 0.030262058600783348, Total Loss 3.023855686187744\n",
      "53: Encoding Loss -1.2657687664031982, Transition Loss -11.161273956298828, Classifier Loss 0.04480468109250069, Total Loss 4.478236198425293\n",
      "53: Encoding Loss -2.6295313835144043, Transition Loss -13.096855163574219, Classifier Loss 0.04723265394568443, Total Loss 4.720645904541016\n",
      "53: Encoding Loss -1.5841529369354248, Transition Loss -7.081899642944336, Classifier Loss 0.050121381878852844, Total Loss 5.0107221603393555\n",
      "53: Encoding Loss -2.745408535003662, Transition Loss -15.171562194824219, Classifier Loss 0.03380848094820976, Total Loss 3.3778138160705566\n",
      "53: Encoding Loss -1.2183892726898193, Transition Loss -9.25823974609375, Classifier Loss 0.06286486238241196, Total Loss 6.284634590148926\n",
      "53: Encoding Loss -1.605533242225647, Transition Loss -12.737685203552246, Classifier Loss 0.074886254966259, Total Loss 7.486077785491943\n",
      "53: Encoding Loss -2.860337972640991, Transition Loss -19.387935638427734, Classifier Loss 0.04314591735601425, Total Loss 4.310714244842529\n",
      "53: Encoding Loss -2.169084072113037, Transition Loss -9.219927787780762, Classifier Loss 0.05599261075258255, Total Loss 5.59741735458374\n",
      "53: Encoding Loss -1.7780067920684814, Transition Loss -10.51339054107666, Classifier Loss 0.07870302349328995, Total Loss 7.868199348449707\n",
      "53: Encoding Loss -2.0926945209503174, Transition Loss -15.789826393127441, Classifier Loss 0.06355627626180649, Total Loss 6.352469444274902\n",
      "53: Encoding Loss -1.723082184791565, Transition Loss -12.836701393127441, Classifier Loss 0.02301378920674324, Total Loss 2.298811674118042\n",
      "53: Encoding Loss -0.804216742515564, Transition Loss -11.805268287658691, Classifier Loss 0.047503285109996796, Total Loss 4.747967720031738\n",
      "53: Encoding Loss -1.4471112489700317, Transition Loss -5.819297790527344, Classifier Loss 0.036539170891046524, Total Loss 3.6527531147003174\n",
      "53: Encoding Loss -0.5293248295783997, Transition Loss -9.185009002685547, Classifier Loss 0.051559872925281525, Total Loss 5.154150009155273\n",
      "53: Encoding Loss -1.4957457780838013, Transition Loss -9.811500549316406, Classifier Loss 0.04636360704898834, Total Loss 4.634398460388184\n",
      "53: Encoding Loss -0.7884654998779297, Transition Loss -5.70029354095459, Classifier Loss 0.026616130024194717, Total Loss 2.660472869873047\n",
      "53: Encoding Loss -1.0834174156188965, Transition Loss -10.596263885498047, Classifier Loss 0.023771679028868675, Total Loss 2.3750486373901367\n",
      "53: Encoding Loss -1.274211049079895, Transition Loss -12.735254287719727, Classifier Loss 0.06834595650434494, Total Loss 6.832048416137695\n",
      "53: Encoding Loss -0.4131859242916107, Transition Loss -12.468039512634277, Classifier Loss 0.0479426346719265, Total Loss 4.791710376739502\n",
      "53: Encoding Loss -0.9782827496528625, Transition Loss -1.8760838508605957, Classifier Loss 0.07237058132886887, Total Loss 7.236682891845703\n",
      "53: Encoding Loss -1.0082885026931763, Transition Loss -9.231254577636719, Classifier Loss 0.029854390770196915, Total Loss 2.9835927486419678\n",
      "53: Encoding Loss -0.48613041639328003, Transition Loss -11.513306617736816, Classifier Loss 0.06568307429552078, Total Loss 6.566002368927002\n",
      "53: Encoding Loss -0.3043546974658966, Transition Loss -10.2825345993042, Classifier Loss 0.04141968861222267, Total Loss 4.137065887451172\n",
      "53: Encoding Loss -1.1760889291763306, Transition Loss -9.871068000793457, Classifier Loss 0.04498631879687309, Total Loss 4.496657848358154\n",
      "53: Encoding Loss -2.2292442321777344, Transition Loss -6.118767261505127, Classifier Loss 0.06549272686243057, Total Loss 6.548048973083496\n",
      "53: Encoding Loss -2.157017469406128, Transition Loss -14.544535636901855, Classifier Loss 0.028934288769960403, Total Loss 2.890519857406616\n",
      "53: Encoding Loss -0.14292216300964355, Transition Loss -2.5880091190338135, Classifier Loss 0.043429963290691376, Total Loss 4.255044460296631\n",
      "53: Encoding Loss -0.7608826160430908, Transition Loss -8.567797660827637, Classifier Loss 0.02615993469953537, Total Loss 2.6142799854278564\n",
      "53: Encoding Loss 0.2898915410041809, Transition Loss -7.990225791931152, Classifier Loss 0.07769536226987839, Total Loss 10.082727432250977\n",
      "53: Encoding Loss -0.1982300579547882, Transition Loss -18.1224365234375, Classifier Loss 0.10278595983982086, Total Loss 10.237350463867188\n",
      "53: Encoding Loss 0.14514468610286713, Transition Loss -17.23502540588379, Classifier Loss 0.0711326003074646, Total Loss 8.185826301574707\n",
      "53: Encoding Loss -1.2848106622695923, Transition Loss -23.424158096313477, Classifier Loss 0.03459449112415314, Total Loss 3.4547641277313232\n",
      "53: Encoding Loss 0.6553393602371216, Transition Loss -13.458379745483398, Classifier Loss 0.08007606863975525, Total Loss 13.24763011932373\n",
      "53: Encoding Loss -1.162989854812622, Transition Loss -12.45574951171875, Classifier Loss 0.04040196165442467, Total Loss 4.037704944610596\n",
      "53: Encoding Loss -1.6400902271270752, Transition Loss -18.5058650970459, Classifier Loss 0.04337162524461746, Total Loss 4.333461284637451\n",
      "53: Encoding Loss -1.2312142848968506, Transition Loss -6.59898042678833, Classifier Loss 0.07073034346103668, Total Loss 7.071714401245117\n",
      "53: Encoding Loss -0.059973251074552536, Transition Loss -14.735823631286621, Classifier Loss 0.07368335872888565, Total Loss 7.233762741088867\n",
      "53: Encoding Loss -0.7867193818092346, Transition Loss -16.094715118408203, Classifier Loss 0.020256562158465385, Total Loss 2.022437334060669\n",
      "53: Encoding Loss -1.257161021232605, Transition Loss -19.061111450195312, Classifier Loss 0.049482107162475586, Total Loss 4.944398403167725\n",
      "53: Encoding Loss -1.5249886512756348, Transition Loss -24.954557418823242, Classifier Loss 0.01272218395024538, Total Loss 1.2672274112701416\n",
      "54: Encoding Loss -0.3719889521598816, Transition Loss -4.64586067199707, Classifier Loss 0.02773907780647278, Total Loss 2.772681951522827\n",
      "54: Encoding Loss -1.670089602470398, Transition Loss -5.9068403244018555, Classifier Loss 0.08106529712677002, Total Loss 8.105347633361816\n",
      "54: Encoding Loss -0.9976372122764587, Transition Loss -0.6259899735450745, Classifier Loss 0.030066046863794327, Total Loss 3.006479501724243\n",
      "54: Encoding Loss -1.1988736391067505, Transition Loss -18.36747169494629, Classifier Loss 0.04978525638580322, Total Loss 4.974852085113525\n",
      "54: Encoding Loss -0.8302769064903259, Transition Loss -9.003854751586914, Classifier Loss 0.0403805747628212, Total Loss 4.036256790161133\n",
      "54: Encoding Loss -0.6438294053077698, Transition Loss -14.814715385437012, Classifier Loss 0.03993407264351845, Total Loss 3.9904444217681885\n",
      "54: Encoding Loss -1.6183605194091797, Transition Loss -10.857884407043457, Classifier Loss 0.046817924827337265, Total Loss 4.679620742797852\n",
      "54: Encoding Loss -0.21790963411331177, Transition Loss -8.086853981018066, Classifier Loss 0.04051054269075394, Total Loss 4.023876667022705\n",
      "54: Encoding Loss -0.7441093921661377, Transition Loss -14.991800308227539, Classifier Loss 0.028865627944469452, Total Loss 2.8835644721984863\n",
      "54: Encoding Loss -1.4495718479156494, Transition Loss -12.393632888793945, Classifier Loss 0.0470350906252861, Total Loss 4.701030254364014\n",
      "54: Encoding Loss -0.039848096668720245, Transition Loss -8.787500381469727, Classifier Loss 0.0416659340262413, Total Loss 4.054811477661133\n",
      "54: Encoding Loss -0.35166609287261963, Transition Loss -4.881988525390625, Classifier Loss 0.052600253373384476, Total Loss 5.258434295654297\n",
      "54: Encoding Loss -1.0887973308563232, Transition Loss -6.872475624084473, Classifier Loss 0.03760620206594467, Total Loss 3.7592456340789795\n",
      "54: Encoding Loss -0.5495132207870483, Transition Loss -7.919504642486572, Classifier Loss 0.046423543244600296, Total Loss 4.640770435333252\n",
      "54: Encoding Loss -1.3610516786575317, Transition Loss -14.664469718933105, Classifier Loss 0.028704088181257248, Total Loss 2.867475986480713\n",
      "54: Encoding Loss -1.9138990640640259, Transition Loss -18.531330108642578, Classifier Loss 0.05871807038784027, Total Loss 5.868100643157959\n",
      "54: Encoding Loss -0.7395901083946228, Transition Loss -10.900800704956055, Classifier Loss 0.017484303563833237, Total Loss 1.7462501525878906\n",
      "54: Encoding Loss -0.5645972490310669, Transition Loss -7.190029621124268, Classifier Loss 0.03151214122772217, Total Loss 3.1497762203216553\n",
      "54: Encoding Loss -0.48453161120414734, Transition Loss -14.028450965881348, Classifier Loss 0.06850484758615494, Total Loss 6.847676753997803\n",
      "54: Encoding Loss 0.129006490111351, Transition Loss -6.346866607666016, Classifier Loss 0.04470142722129822, Total Loss 5.399253845214844\n",
      "54: Encoding Loss 0.6583594679832458, Transition Loss -17.820171356201172, Classifier Loss 0.0711091160774231, Total Loss 12.374222755432129\n",
      "54: Encoding Loss -0.2595066726207733, Transition Loss -12.261855125427246, Classifier Loss 0.03360645845532417, Total Loss 3.348376512527466\n",
      "54: Encoding Loss -0.3683556914329529, Transition Loss -12.731115341186523, Classifier Loss 0.05181710422039032, Total Loss 5.1788249015808105\n",
      "54: Encoding Loss -0.33788108825683594, Transition Loss -4.792337894439697, Classifier Loss 0.05765111744403839, Total Loss 5.763169765472412\n",
      "54: Encoding Loss -0.7989457249641418, Transition Loss -12.625393867492676, Classifier Loss 0.02839483693242073, Total Loss 2.836958646774292\n",
      "54: Encoding Loss 0.04504041001200676, Transition Loss -6.440482139587402, Classifier Loss 0.057831600308418274, Total Loss 6.024654865264893\n",
      "54: Encoding Loss -2.6508302688598633, Transition Loss -6.010381698608398, Classifier Loss 0.0764596238732338, Total Loss 7.6447601318359375\n",
      "54: Encoding Loss -3.0437896251678467, Transition Loss -21.101890563964844, Classifier Loss 0.05542849376797676, Total Loss 5.538629055023193\n",
      "54: Encoding Loss -2.3918776512145996, Transition Loss -11.525940895080566, Classifier Loss 0.04427904263138771, Total Loss 4.425599098205566\n",
      "54: Encoding Loss -1.4014134407043457, Transition Loss -7.491180419921875, Classifier Loss 0.052265822887420654, Total Loss 5.22508430480957\n",
      "54: Encoding Loss -1.5537607669830322, Transition Loss -13.772979736328125, Classifier Loss 0.05401341989636421, Total Loss 5.398587226867676\n",
      "54: Encoding Loss -1.7736237049102783, Transition Loss -5.347986221313477, Classifier Loss 0.0630088597536087, Total Loss 6.299816608428955\n",
      "54: Encoding Loss -1.0301094055175781, Transition Loss -13.116150856018066, Classifier Loss 0.05275418981909752, Total Loss 5.272795677185059\n",
      "54: Encoding Loss -0.989428699016571, Transition Loss -8.032109260559082, Classifier Loss 0.15113815665245056, Total Loss 15.11220932006836\n",
      "54: Encoding Loss -2.3100922107696533, Transition Loss -14.80555534362793, Classifier Loss 0.035549916326999664, Total Loss 3.552030563354492\n",
      "54: Encoding Loss -1.2663356065750122, Transition Loss -9.857149124145508, Classifier Loss 0.03599309176206589, Total Loss 3.5973377227783203\n",
      "54: Encoding Loss -1.3979227542877197, Transition Loss -8.907733917236328, Classifier Loss 0.055393658578395844, Total Loss 5.53758430480957\n",
      "54: Encoding Loss -1.4867613315582275, Transition Loss -11.189592361450195, Classifier Loss 0.10113409161567688, Total Loss 10.111170768737793\n",
      "54: Encoding Loss -2.0646138191223145, Transition Loss -12.676084518432617, Classifier Loss 0.04897692799568176, Total Loss 4.895157337188721\n",
      "54: Encoding Loss -1.426095724105835, Transition Loss -13.5596284866333, Classifier Loss 0.051180969923734665, Total Loss 5.115385055541992\n",
      "54: Encoding Loss -1.408284068107605, Transition Loss -10.2529935836792, Classifier Loss 0.02627798169851303, Total Loss 2.6257474422454834\n",
      "54: Encoding Loss -1.8270251750946045, Transition Loss -18.307147979736328, Classifier Loss 0.056646764278411865, Total Loss 5.661015033721924\n",
      "54: Encoding Loss -2.0683727264404297, Transition Loss -17.635570526123047, Classifier Loss 0.025094646960496902, Total Loss 2.5059375762939453\n",
      "54: Encoding Loss -2.4066760540008545, Transition Loss -7.890806198120117, Classifier Loss 0.0510057769715786, Total Loss 5.098999500274658\n",
      "54: Encoding Loss -1.8086730241775513, Transition Loss -18.85059356689453, Classifier Loss 0.03618589788675308, Total Loss 3.6148197650909424\n",
      "54: Encoding Loss -0.4653119742870331, Transition Loss -9.502754211425781, Classifier Loss 0.05825202167034149, Total Loss 5.8232951164245605\n",
      "54: Encoding Loss -1.1773968935012817, Transition Loss -19.437667846679688, Classifier Loss 0.05026136338710785, Total Loss 5.0222487449646\n",
      "54: Encoding Loss -2.2456552982330322, Transition Loss -8.067207336425781, Classifier Loss 0.02543872781097889, Total Loss 2.542259454727173\n",
      "54: Encoding Loss -2.2576043605804443, Transition Loss -11.262755393981934, Classifier Loss 0.04219990596175194, Total Loss 4.217738151550293\n",
      "54: Encoding Loss -1.758060097694397, Transition Loss -17.848783493041992, Classifier Loss 0.04043840989470482, Total Loss 4.040271282196045\n",
      "54: Encoding Loss -2.453014850616455, Transition Loss -21.12335777282715, Classifier Loss 0.0419590100646019, Total Loss 4.191676139831543\n",
      "54: Encoding Loss -1.0080510377883911, Transition Loss -14.378509521484375, Classifier Loss 0.06156099587678909, Total Loss 6.153223991394043\n",
      "54: Encoding Loss -1.4419273138046265, Transition Loss -4.909821033477783, Classifier Loss 0.07452747225761414, Total Loss 7.451765537261963\n",
      "54: Encoding Loss -1.571367621421814, Transition Loss -13.688985824584961, Classifier Loss 0.032729871571063995, Total Loss 3.270249366760254\n",
      "54: Encoding Loss -0.955829381942749, Transition Loss -13.626374244689941, Classifier Loss 0.06280045211315155, Total Loss 6.27731990814209\n",
      "54: Encoding Loss -2.024294376373291, Transition Loss -6.913549423217773, Classifier Loss 0.07173839211463928, Total Loss 7.17245626449585\n",
      "54: Encoding Loss -0.9862858057022095, Transition Loss -12.490089416503906, Classifier Loss 0.025421258062124252, Total Loss 2.53962779045105\n",
      "54: Encoding Loss -2.2078816890716553, Transition Loss -14.454899787902832, Classifier Loss 0.04335277900099754, Total Loss 4.3323869705200195\n",
      "54: Encoding Loss -2.3462307453155518, Transition Loss -20.63282585144043, Classifier Loss 0.03120221011340618, Total Loss 3.1160943508148193\n",
      "54: Encoding Loss -1.9735339879989624, Transition Loss -17.79803466796875, Classifier Loss 0.05021457001566887, Total Loss 5.017897605895996\n",
      "54: Encoding Loss -1.7702701091766357, Transition Loss -10.684220314025879, Classifier Loss 0.050197429955005646, Total Loss 5.017606258392334\n",
      "54: Encoding Loss -2.2813847064971924, Transition Loss -9.518003463745117, Classifier Loss 0.08393911272287369, Total Loss 8.392007827758789\n",
      "54: Encoding Loss -2.0984606742858887, Transition Loss -11.226099014282227, Classifier Loss 0.04031553491950035, Total Loss 4.029307842254639\n",
      "54: Encoding Loss -1.270847201347351, Transition Loss -8.598274230957031, Classifier Loss 0.05785844102501869, Total Loss 5.784124851226807\n",
      "54: Encoding Loss -1.5932649374008179, Transition Loss -12.831974029541016, Classifier Loss 0.056280601769685745, Total Loss 5.625494003295898\n",
      "54: Encoding Loss -2.275991916656494, Transition Loss -6.218156814575195, Classifier Loss 0.08251769095659256, Total Loss 8.25052547454834\n",
      "54: Encoding Loss -1.0242246389389038, Transition Loss -3.0247645378112793, Classifier Loss 0.04709343612194061, Total Loss 4.708738327026367\n",
      "54: Encoding Loss -1.8973723649978638, Transition Loss -19.603755950927734, Classifier Loss 0.06438574939966202, Total Loss 6.434654235839844\n",
      "54: Encoding Loss -0.8304170370101929, Transition Loss -10.370241165161133, Classifier Loss 0.060082606971263885, Total Loss 6.006186485290527\n",
      "54: Encoding Loss -0.9525747299194336, Transition Loss -10.212738990783691, Classifier Loss 0.09232905507087708, Total Loss 9.230862617492676\n",
      "54: Encoding Loss -2.017280340194702, Transition Loss -10.025664329528809, Classifier Loss 0.057943761348724365, Total Loss 5.7923712730407715\n",
      "54: Encoding Loss -1.544265866279602, Transition Loss -10.088951110839844, Classifier Loss 0.06300017237663269, Total Loss 6.297999382019043\n",
      "54: Encoding Loss -1.0162620544433594, Transition Loss -6.827994346618652, Classifier Loss 0.07394722104072571, Total Loss 7.3933563232421875\n",
      "54: Encoding Loss -1.5184844732284546, Transition Loss -10.371947288513184, Classifier Loss 0.016899559646844864, Total Loss 1.687881588935852\n",
      "54: Encoding Loss -2.2004363536834717, Transition Loss -9.341231346130371, Classifier Loss 0.07570099830627441, Total Loss 7.568231582641602\n",
      "54: Encoding Loss -1.567150354385376, Transition Loss -15.724306106567383, Classifier Loss 0.029365791007876396, Total Loss 2.933434009552002\n",
      "54: Encoding Loss -1.755179524421692, Transition Loss -7.9957733154296875, Classifier Loss 0.061034686863422394, Total Loss 6.101869583129883\n",
      "54: Encoding Loss -2.573261022567749, Transition Loss -13.670370101928711, Classifier Loss 0.04664109647274017, Total Loss 4.661375522613525\n",
      "54: Encoding Loss -1.813910961151123, Transition Loss -14.833873748779297, Classifier Loss 0.021241679787635803, Total Loss 2.1212010383605957\n",
      "54: Encoding Loss -1.948008418083191, Transition Loss -11.310896873474121, Classifier Loss 0.081915482878685, Total Loss 8.189286231994629\n",
      "54: Encoding Loss -1.164581298828125, Transition Loss -12.465646743774414, Classifier Loss 0.03891821205615997, Total Loss 3.8893280029296875\n",
      "54: Encoding Loss -2.070629835128784, Transition Loss -10.196660995483398, Classifier Loss 0.0352570116519928, Total Loss 3.5236616134643555\n",
      "54: Encoding Loss -1.629326581954956, Transition Loss -6.837068557739258, Classifier Loss 0.07801639288663864, Total Loss 7.800271511077881\n",
      "54: Encoding Loss -0.7525604367256165, Transition Loss -12.259990692138672, Classifier Loss 0.06528335064649582, Total Loss 6.525883197784424\n",
      "54: Encoding Loss -2.8084161281585693, Transition Loss -9.141114234924316, Classifier Loss 0.04354216903448105, Total Loss 4.352388858795166\n",
      "54: Encoding Loss -1.5296883583068848, Transition Loss -7.165765285491943, Classifier Loss 0.06098395586013794, Total Loss 6.096961975097656\n",
      "54: Encoding Loss -2.153142213821411, Transition Loss -16.930877685546875, Classifier Loss 0.048762764781713486, Total Loss 4.872890472412109\n",
      "54: Encoding Loss -1.7219583988189697, Transition Loss -10.918365478515625, Classifier Loss 0.09097122400999069, Total Loss 9.094938278198242\n",
      "54: Encoding Loss -1.0619456768035889, Transition Loss -9.063867568969727, Classifier Loss 0.04473893344402313, Total Loss 4.472080230712891\n",
      "54: Encoding Loss -1.4007163047790527, Transition Loss -14.640186309814453, Classifier Loss 0.0434861034154892, Total Loss 4.345682144165039\n",
      "54: Encoding Loss -3.353463888168335, Transition Loss -22.817068099975586, Classifier Loss 0.03941004350781441, Total Loss 3.936440944671631\n",
      "54: Encoding Loss -1.31382417678833, Transition Loss -8.089235305786133, Classifier Loss 0.06744122505187988, Total Loss 6.742504596710205\n",
      "54: Encoding Loss -1.5448499917984009, Transition Loss -9.081794738769531, Classifier Loss 0.06416573375463486, Total Loss 6.414757251739502\n",
      "54: Encoding Loss -0.9890772700309753, Transition Loss -10.930373191833496, Classifier Loss 0.04934701323509216, Total Loss 4.9325151443481445\n",
      "54: Encoding Loss -1.5472471714019775, Transition Loss -5.329244613647461, Classifier Loss 0.06179489567875862, Total Loss 6.178423881530762\n",
      "54: Encoding Loss -1.8973450660705566, Transition Loss -14.266427993774414, Classifier Loss 0.061232514679431915, Total Loss 6.120398044586182\n",
      "54: Encoding Loss -1.6649760007858276, Transition Loss -17.261959075927734, Classifier Loss 0.061726585030555725, Total Loss 6.169206142425537\n",
      "54: Encoding Loss -1.8999172449111938, Transition Loss -14.336559295654297, Classifier Loss 0.045532070100307465, Total Loss 4.550339698791504\n",
      "54: Encoding Loss -1.2174981832504272, Transition Loss -5.747243881225586, Classifier Loss 0.027609212324023247, Total Loss 2.7597718238830566\n",
      "54: Encoding Loss -2.067506790161133, Transition Loss -8.378941535949707, Classifier Loss 0.08316808938980103, Total Loss 8.315133094787598\n",
      "54: Encoding Loss -2.283337116241455, Transition Loss -15.336526870727539, Classifier Loss 0.049382247030735016, Total Loss 4.935157299041748\n",
      "54: Encoding Loss -1.0139641761779785, Transition Loss -1.0222746133804321, Classifier Loss 0.09235886484384537, Total Loss 9.235682487487793\n",
      "54: Encoding Loss -1.140108585357666, Transition Loss -9.362056732177734, Classifier Loss 0.06392178684473038, Total Loss 6.390305995941162\n",
      "54: Encoding Loss -0.689261257648468, Transition Loss -5.791261196136475, Classifier Loss 0.06083589792251587, Total Loss 6.082431316375732\n",
      "54: Encoding Loss -1.2873600721359253, Transition Loss -12.7526273727417, Classifier Loss 0.040408506989479065, Total Loss 4.038300037384033\n",
      "54: Encoding Loss -1.9986622333526611, Transition Loss -16.70191192626953, Classifier Loss 0.06381811946630478, Total Loss 6.378471851348877\n",
      "54: Encoding Loss -1.1610275506973267, Transition Loss -6.052728652954102, Classifier Loss 0.06051003932952881, Total Loss 6.049793243408203\n",
      "54: Encoding Loss -1.3035966157913208, Transition Loss -10.243857383728027, Classifier Loss 0.0401189811527729, Total Loss 4.0098490715026855\n",
      "54: Encoding Loss -1.47715163230896, Transition Loss -9.917867660522461, Classifier Loss 0.041833799332380295, Total Loss 4.181396484375\n",
      "54: Encoding Loss -0.872075617313385, Transition Loss -15.215067863464355, Classifier Loss 0.04535462707281113, Total Loss 4.532419681549072\n",
      "54: Encoding Loss -1.4691907167434692, Transition Loss -14.77682113647461, Classifier Loss 0.056739386171102524, Total Loss 5.67098331451416\n",
      "54: Encoding Loss -1.108777403831482, Transition Loss -2.855900764465332, Classifier Loss 0.033049046993255615, Total Loss 3.304333448410034\n",
      "54: Encoding Loss -1.652622938156128, Transition Loss -18.73087501525879, Classifier Loss 0.03120923787355423, Total Loss 3.1171774864196777\n",
      "54: Encoding Loss -2.1708884239196777, Transition Loss -17.92774772644043, Classifier Loss 0.027954258024692535, Total Loss 2.7918403148651123\n",
      "54: Encoding Loss -1.0318915843963623, Transition Loss -2.188727378845215, Classifier Loss 0.045373961329460144, Total Loss 4.53695821762085\n",
      "54: Encoding Loss -0.6808100938796997, Transition Loss 0.6958291530609131, Classifier Loss 0.04730256646871567, Total Loss 4.869422435760498\n",
      "54: Encoding Loss -1.0347281694412231, Transition Loss -12.790642738342285, Classifier Loss 0.030877690762281418, Total Loss 3.0852108001708984\n",
      "54: Encoding Loss -1.517577886581421, Transition Loss -11.989466667175293, Classifier Loss 0.04251555725932121, Total Loss 4.249157905578613\n",
      "54: Encoding Loss -2.3910887241363525, Transition Loss -14.165489196777344, Classifier Loss 0.04674869775772095, Total Loss 4.672036647796631\n",
      "54: Encoding Loss -1.4828633069992065, Transition Loss -7.667980194091797, Classifier Loss 0.05014592409133911, Total Loss 5.013058662414551\n",
      "54: Encoding Loss -2.6241540908813477, Transition Loss -16.233715057373047, Classifier Loss 0.03378141298890114, Total Loss 3.374894618988037\n",
      "54: Encoding Loss -1.4271557331085205, Transition Loss -9.974035263061523, Classifier Loss 0.05920197814702988, Total Loss 5.918203353881836\n",
      "54: Encoding Loss -1.4900171756744385, Transition Loss -13.462024688720703, Classifier Loss 0.07221535593271255, Total Loss 7.218843460083008\n",
      "54: Encoding Loss -2.6566507816314697, Transition Loss -20.800291061401367, Classifier Loss 0.04285232350230217, Total Loss 4.281072616577148\n",
      "54: Encoding Loss -2.2192838191986084, Transition Loss -9.704595565795898, Classifier Loss 0.054788459092378616, Total Loss 5.476905345916748\n",
      "54: Encoding Loss -1.8508883714675903, Transition Loss -11.144965171813965, Classifier Loss 0.07914501428604126, Total Loss 7.912271976470947\n",
      "54: Encoding Loss -1.9387400150299072, Transition Loss -16.96343421936035, Classifier Loss 0.06505269557237625, Total Loss 6.5018768310546875\n",
      "54: Encoding Loss -1.948132872581482, Transition Loss -13.737041473388672, Classifier Loss 0.024369895458221436, Total Loss 2.4342422485351562\n",
      "54: Encoding Loss -0.998870313167572, Transition Loss -12.408327102661133, Classifier Loss 0.04701002687215805, Total Loss 4.698521137237549\n",
      "54: Encoding Loss -2.0153071880340576, Transition Loss -6.046954154968262, Classifier Loss 0.03681899979710579, Total Loss 3.6806905269622803\n",
      "54: Encoding Loss -1.2852473258972168, Transition Loss -9.613128662109375, Classifier Loss 0.05217379704117775, Total Loss 5.215456962585449\n",
      "54: Encoding Loss -1.7992336750030518, Transition Loss -10.35921859741211, Classifier Loss 0.046971336007118225, Total Loss 4.695061683654785\n",
      "54: Encoding Loss -1.307426929473877, Transition Loss -6.005741119384766, Classifier Loss 0.026824377477169037, Total Loss 2.681236505508423\n",
      "54: Encoding Loss -1.38509202003479, Transition Loss -11.262675285339355, Classifier Loss 0.02468002773821354, Total Loss 2.465750217437744\n",
      "54: Encoding Loss -1.4859890937805176, Transition Loss -13.526446342468262, Classifier Loss 0.06664559245109558, Total Loss 6.661854267120361\n",
      "54: Encoding Loss -0.6734613180160522, Transition Loss -13.229349136352539, Classifier Loss 0.04662950336933136, Total Loss 4.660304546356201\n",
      "54: Encoding Loss -1.5163553953170776, Transition Loss -1.5745317935943604, Classifier Loss 0.07315678894519806, Total Loss 7.315364360809326\n",
      "54: Encoding Loss -1.5744836330413818, Transition Loss -9.624747276306152, Classifier Loss 0.029725737869739532, Total Loss 2.970648765563965\n",
      "54: Encoding Loss -0.9226236343383789, Transition Loss -12.229473114013672, Classifier Loss 0.06456460803747177, Total Loss 6.454015254974365\n",
      "54: Encoding Loss -0.37573161721229553, Transition Loss -10.749627113342285, Classifier Loss 0.04065263271331787, Total Loss 4.062855243682861\n",
      "54: Encoding Loss -1.2487366199493408, Transition Loss -10.294940948486328, Classifier Loss 0.04450242593884468, Total Loss 4.448183536529541\n",
      "54: Encoding Loss -2.533843517303467, Transition Loss -6.2328057289123535, Classifier Loss 0.0646156445145607, Total Loss 6.460318088531494\n",
      "54: Encoding Loss -2.4084348678588867, Transition Loss -15.3009614944458, Classifier Loss 0.029841285198926926, Total Loss 2.9810683727264404\n",
      "54: Encoding Loss -0.21549805998802185, Transition Loss -2.6193923950195312, Classifier Loss 0.04294329136610031, Total Loss 4.266942501068115\n",
      "54: Encoding Loss -1.2718833684921265, Transition Loss -8.994680404663086, Classifier Loss 0.025805098935961723, Total Loss 2.5787110328674316\n",
      "54: Encoding Loss -0.7141605615615845, Transition Loss -8.407774925231934, Classifier Loss 0.07577364891767502, Total Loss 7.57568359375\n",
      "54: Encoding Loss -0.9400743842124939, Transition Loss -17.183244705200195, Classifier Loss 0.12731397151947021, Total Loss 12.727960586547852\n",
      "54: Encoding Loss -2.3078362941741943, Transition Loss -17.031017303466797, Classifier Loss 0.06102607026696205, Total Loss 6.099201202392578\n",
      "54: Encoding Loss -2.4458250999450684, Transition Loss -22.678333282470703, Classifier Loss 0.03465194255113602, Total Loss 3.460658550262451\n",
      "54: Encoding Loss -1.2340149879455566, Transition Loss -12.522977828979492, Classifier Loss 0.07872236520051956, Total Loss 7.869731903076172\n",
      "54: Encoding Loss -2.076996088027954, Transition Loss -14.113720893859863, Classifier Loss 0.03999055549502373, Total Loss 3.9962329864501953\n",
      "54: Encoding Loss -2.573180675506592, Transition Loss -20.28458023071289, Classifier Loss 0.043652452528476715, Total Loss 4.3611884117126465\n",
      "54: Encoding Loss -2.0029842853546143, Transition Loss -8.194543838500977, Classifier Loss 0.07103503495454788, Total Loss 7.101864814758301\n",
      "54: Encoding Loss -1.114838719367981, Transition Loss -16.79239845275879, Classifier Loss 0.07015788555145264, Total Loss 7.012430191040039\n",
      "54: Encoding Loss -1.3959003686904907, Transition Loss -17.770421981811523, Classifier Loss 0.018567945808172226, Total Loss 1.8532404899597168\n",
      "54: Encoding Loss -1.7566683292388916, Transition Loss -20.834592819213867, Classifier Loss 0.050746262073516846, Total Loss 5.070458889007568\n",
      "54: Encoding Loss -1.881708025932312, Transition Loss -26.838504791259766, Classifier Loss 0.011269612237811089, Total Loss 1.1215934753417969\n",
      "55: Encoding Loss -0.950122058391571, Transition Loss -6.302916049957275, Classifier Loss 0.02745893783867359, Total Loss 2.744633197784424\n",
      "55: Encoding Loss -1.910977840423584, Transition Loss -7.160496711730957, Classifier Loss 0.08060668408870697, Total Loss 8.059235572814941\n",
      "55: Encoding Loss -1.6001012325286865, Transition Loss -2.090961217880249, Classifier Loss 0.030731167644262314, Total Loss 3.0726985931396484\n",
      "55: Encoding Loss -1.569461703300476, Transition Loss -19.842639923095703, Classifier Loss 0.048891011625528336, Total Loss 4.885132312774658\n",
      "55: Encoding Loss -1.348135232925415, Transition Loss -10.352161407470703, Classifier Loss 0.04047838971018791, Total Loss 4.045768737792969\n",
      "55: Encoding Loss -1.6198011636734009, Transition Loss -16.51200294494629, Classifier Loss 0.039720289409160614, Total Loss 3.968726634979248\n",
      "55: Encoding Loss -2.435494899749756, Transition Loss -12.299325942993164, Classifier Loss 0.04511890932917595, Total Loss 4.509430885314941\n",
      "55: Encoding Loss -1.187954068183899, Transition Loss -9.75241470336914, Classifier Loss 0.03955136612057686, Total Loss 3.95318603515625\n",
      "55: Encoding Loss -1.4498841762542725, Transition Loss -16.507225036621094, Classifier Loss 0.028884245082736015, Total Loss 2.8851230144500732\n",
      "55: Encoding Loss -2.408890724182129, Transition Loss -13.730070114135742, Classifier Loss 0.04941162094473839, Total Loss 4.938416004180908\n",
      "55: Encoding Loss -0.07234208285808563, Transition Loss -10.022124290466309, Classifier Loss 0.04281569644808769, Total Loss 4.143729209899902\n",
      "55: Encoding Loss -1.2210856676101685, Transition Loss -6.289481163024902, Classifier Loss 0.051960285753011703, Total Loss 5.194770812988281\n",
      "55: Encoding Loss -1.5648341178894043, Transition Loss -8.350674629211426, Classifier Loss 0.0369955338537693, Total Loss 3.697883367538452\n",
      "55: Encoding Loss -1.071598768234253, Transition Loss -9.062714576721191, Classifier Loss 0.0468766987323761, Total Loss 4.68585729598999\n",
      "55: Encoding Loss -2.0993504524230957, Transition Loss -16.12846565246582, Classifier Loss 0.027164675295352936, Total Loss 2.7132418155670166\n",
      "55: Encoding Loss -2.2972633838653564, Transition Loss -20.080657958984375, Classifier Loss 0.05578888580203056, Total Loss 5.5748724937438965\n",
      "55: Encoding Loss -1.087046504020691, Transition Loss -12.342950820922852, Classifier Loss 0.01730566844344139, Total Loss 1.7280982732772827\n",
      "55: Encoding Loss -0.712256133556366, Transition Loss -8.35251235961914, Classifier Loss 0.034619349986314774, Total Loss 3.4602644443511963\n",
      "55: Encoding Loss -0.7323826551437378, Transition Loss -15.36256217956543, Classifier Loss 0.07009123265743256, Total Loss 7.0060505867004395\n",
      "55: Encoding Loss -0.6124222278594971, Transition Loss -7.677180290222168, Classifier Loss 0.04193602129817009, Total Loss 4.192066669464111\n",
      "55: Encoding Loss -1.5587860345840454, Transition Loss -17.22459602355957, Classifier Loss 0.06762140244245529, Total Loss 6.758695125579834\n",
      "55: Encoding Loss -1.654444932937622, Transition Loss -13.347777366638184, Classifier Loss 0.03127588331699371, Total Loss 3.1249186992645264\n",
      "55: Encoding Loss -1.3673133850097656, Transition Loss -14.088874816894531, Classifier Loss 0.04948434978723526, Total Loss 4.945617198944092\n",
      "55: Encoding Loss -1.3107823133468628, Transition Loss -5.951680660247803, Classifier Loss 0.06921028345823288, Total Loss 6.919837951660156\n",
      "55: Encoding Loss -1.789204716682434, Transition Loss -13.915852546691895, Classifier Loss 0.02815171889960766, Total Loss 2.8123888969421387\n",
      "55: Encoding Loss -0.5888785123825073, Transition Loss -7.324012756347656, Classifier Loss 0.05701517313718796, Total Loss 5.700052261352539\n",
      "55: Encoding Loss -2.3365395069122314, Transition Loss -6.052800178527832, Classifier Loss 0.07787536084651947, Total Loss 7.786325454711914\n",
      "55: Encoding Loss -2.7383620738983154, Transition Loss -20.923126220703125, Classifier Loss 0.055263835936784744, Total Loss 5.522198677062988\n",
      "55: Encoding Loss -1.864824652671814, Transition Loss -11.475656509399414, Classifier Loss 0.044025104492902756, Total Loss 4.4002156257629395\n",
      "55: Encoding Loss -0.8825935125350952, Transition Loss -7.595439910888672, Classifier Loss 0.050896383821964264, Total Loss 5.088119029998779\n",
      "55: Encoding Loss -1.0555607080459595, Transition Loss -13.8157377243042, Classifier Loss 0.05384654179215431, Total Loss 5.381890773773193\n",
      "55: Encoding Loss -1.6721099615097046, Transition Loss -5.295016765594482, Classifier Loss 0.06451151520013809, Total Loss 6.450092315673828\n",
      "55: Encoding Loss -0.8624502420425415, Transition Loss -13.086320877075195, Classifier Loss 0.050927065312862396, Total Loss 5.090089321136475\n",
      "55: Encoding Loss -0.6951408982276917, Transition Loss -7.806795120239258, Classifier Loss 0.14212284982204437, Total Loss 14.210723876953125\n",
      "55: Encoding Loss -1.7243597507476807, Transition Loss -15.054637908935547, Classifier Loss 0.033675190061330795, Total Loss 3.3645079135894775\n",
      "55: Encoding Loss -0.8370442986488342, Transition Loss -9.921600341796875, Classifier Loss 0.03320874646306038, Total Loss 3.318890333175659\n",
      "55: Encoding Loss -0.7248531579971313, Transition Loss -9.168916702270508, Classifier Loss 0.05435746908187866, Total Loss 5.433913230895996\n",
      "55: Encoding Loss -1.0088168382644653, Transition Loss -11.289609909057617, Classifier Loss 0.09251902997493744, Total Loss 9.24964427947998\n",
      "55: Encoding Loss -1.6421781778335571, Transition Loss -12.869627952575684, Classifier Loss 0.0452764593064785, Total Loss 4.52507209777832\n",
      "55: Encoding Loss -0.8143669366836548, Transition Loss -13.9076509475708, Classifier Loss 0.050662651658058167, Total Loss 5.063483715057373\n",
      "55: Encoding Loss -0.6303810477256775, Transition Loss -10.65494155883789, Classifier Loss 0.02562517113983631, Total Loss 2.5603861808776855\n",
      "55: Encoding Loss -1.3544480800628662, Transition Loss -18.474721908569336, Classifier Loss 0.05801527947187424, Total Loss 5.79783296585083\n",
      "55: Encoding Loss -1.6136895418167114, Transition Loss -17.840728759765625, Classifier Loss 0.026647863909602165, Total Loss 2.6612181663513184\n",
      "55: Encoding Loss -2.023294687271118, Transition Loss -8.072577476501465, Classifier Loss 0.04627550020813942, Total Loss 4.6259355545043945\n",
      "55: Encoding Loss -1.2525670528411865, Transition Loss -18.951513290405273, Classifier Loss 0.03581702709197998, Total Loss 3.5779123306274414\n",
      "55: Encoding Loss 0.084146648645401, Transition Loss -9.56566047668457, Classifier Loss 0.05759354680776596, Total Loss 6.295950889587402\n",
      "55: Encoding Loss 0.7197163701057434, Transition Loss -17.321353912353516, Classifier Loss 0.048492610454559326, Total Loss 10.603528022766113\n",
      "55: Encoding Loss -0.2295701652765274, Transition Loss -7.203780651092529, Classifier Loss 0.029204580932855606, Total Loss 2.899097204208374\n",
      "55: Encoding Loss 1.2584539651870728, Transition Loss -9.977999687194824, Classifier Loss 0.04713798686861992, Total Loss 14.779434204101562\n",
      "55: Encoding Loss -0.38792678713798523, Transition Loss -17.245359420776367, Classifier Loss 0.03649284690618515, Total Loss 3.6456730365753174\n",
      "55: Encoding Loss -0.5516789555549622, Transition Loss -20.355667114257812, Classifier Loss 0.04718406870961189, Total Loss 4.7143354415893555\n",
      "55: Encoding Loss 0.37591326236724854, Transition Loss -13.917985916137695, Classifier Loss 0.06509535759687424, Total Loss 9.513801574707031\n",
      "55: Encoding Loss -0.7921971678733826, Transition Loss -4.10523796081543, Classifier Loss 0.07690514624118805, Total Loss 7.689693450927734\n",
      "55: Encoding Loss -1.117305874824524, Transition Loss -12.865219116210938, Classifier Loss 0.031005747616291046, Total Loss 3.098001718521118\n",
      "55: Encoding Loss -0.8057706952095032, Transition Loss -12.872004508972168, Classifier Loss 0.062282171100378036, Total Loss 6.225642681121826\n",
      "55: Encoding Loss -1.66475510597229, Transition Loss -6.085432052612305, Classifier Loss 0.06954874098300934, Total Loss 6.953657150268555\n",
      "55: Encoding Loss -0.7549679279327393, Transition Loss -11.743058204650879, Classifier Loss 0.025416545569896698, Total Loss 2.5393059253692627\n",
      "55: Encoding Loss -1.8441922664642334, Transition Loss -13.600289344787598, Classifier Loss 0.04217030853033066, Total Loss 4.214311122894287\n",
      "55: Encoding Loss -1.7017861604690552, Transition Loss -19.951702117919922, Classifier Loss 0.030192935839295387, Total Loss 3.015303134918213\n",
      "55: Encoding Loss -1.5464175939559937, Transition Loss -16.977508544921875, Classifier Loss 0.04983913153409958, Total Loss 4.980517387390137\n",
      "55: Encoding Loss -1.0593292713165283, Transition Loss -9.831443786621094, Classifier Loss 0.0505058616399765, Total Loss 5.048619747161865\n",
      "55: Encoding Loss -1.8347755670547485, Transition Loss -8.691439628601074, Classifier Loss 0.08310332894325256, Total Loss 8.308594703674316\n",
      "55: Encoding Loss -1.7469308376312256, Transition Loss -10.184974670410156, Classifier Loss 0.039759501814842224, Total Loss 3.9739131927490234\n",
      "55: Encoding Loss -0.8806009292602539, Transition Loss -7.79360294342041, Classifier Loss 0.05688009783625603, Total Loss 5.686450958251953\n",
      "55: Encoding Loss -1.0963904857635498, Transition Loss -12.076547622680664, Classifier Loss 0.05380599573254585, Total Loss 5.3781843185424805\n",
      "55: Encoding Loss -1.772525429725647, Transition Loss -5.268820285797119, Classifier Loss 0.08533888310194016, Total Loss 8.53283405303955\n",
      "55: Encoding Loss -0.4883584678173065, Transition Loss -2.1318135261535645, Classifier Loss 0.04654349014163017, Total Loss 4.653920650482178\n",
      "55: Encoding Loss -1.2560007572174072, Transition Loss -18.915550231933594, Classifier Loss 0.06259297579526901, Total Loss 6.255514144897461\n",
      "55: Encoding Loss -0.518434464931488, Transition Loss -9.535524368286133, Classifier Loss 0.05856184661388397, Total Loss 5.85427713394165\n",
      "55: Encoding Loss -0.5177248120307922, Transition Loss -9.345232009887695, Classifier Loss 0.0908222645521164, Total Loss 9.08035659790039\n",
      "55: Encoding Loss -1.4755300283432007, Transition Loss -9.243927001953125, Classifier Loss 0.055395498871803284, Total Loss 5.53770112991333\n",
      "55: Encoding Loss -1.1166512966156006, Transition Loss -9.231233596801758, Classifier Loss 0.06138985604047775, Total Loss 6.137139320373535\n",
      "55: Encoding Loss -0.5719920992851257, Transition Loss -6.06046724319458, Classifier Loss 0.07252922654151917, Total Loss 7.251710414886475\n",
      "55: Encoding Loss -1.2665714025497437, Transition Loss -9.401961326599121, Classifier Loss 0.01646677404642105, Total Loss 1.6447969675064087\n",
      "55: Encoding Loss -1.8979861736297607, Transition Loss -8.562687873840332, Classifier Loss 0.07552580535411835, Total Loss 7.550868034362793\n",
      "55: Encoding Loss -1.15957772731781, Transition Loss -14.989248275756836, Classifier Loss 0.027868710458278656, Total Loss 2.7838730812072754\n",
      "55: Encoding Loss -1.359497308731079, Transition Loss -7.1819658279418945, Classifier Loss 0.05900576710700989, Total Loss 5.899140357971191\n",
      "55: Encoding Loss -2.1531293392181396, Transition Loss -12.790451049804688, Classifier Loss 0.0463811494410038, Total Loss 4.635556697845459\n",
      "55: Encoding Loss -1.3690677881240845, Transition Loss -13.954318046569824, Classifier Loss 0.020103739574551582, Total Loss 2.0075831413269043\n",
      "55: Encoding Loss -1.5243884325027466, Transition Loss -10.472325325012207, Classifier Loss 0.08316412568092346, Total Loss 8.314318656921387\n",
      "55: Encoding Loss -0.9474376440048218, Transition Loss -11.57213020324707, Classifier Loss 0.03909781575202942, Total Loss 3.9074671268463135\n",
      "55: Encoding Loss -1.429273247718811, Transition Loss -9.523499488830566, Classifier Loss 0.034924473613500595, Total Loss 3.4905426502227783\n",
      "55: Encoding Loss -1.2437628507614136, Transition Loss -6.007791042327881, Classifier Loss 0.08131229877471924, Total Loss 8.130027770996094\n",
      "55: Encoding Loss -0.6304045915603638, Transition Loss -11.750215530395508, Classifier Loss 0.06283892691135406, Total Loss 6.281542778015137\n",
      "55: Encoding Loss -2.4326672554016113, Transition Loss -8.219402313232422, Classifier Loss 0.042410530149936676, Total Loss 4.239409446716309\n",
      "55: Encoding Loss -1.3095858097076416, Transition Loss -6.39755916595459, Classifier Loss 0.05950994789600372, Total Loss 5.949715614318848\n",
      "55: Encoding Loss -1.6930791139602661, Transition Loss -16.251174926757812, Classifier Loss 0.04942293465137482, Total Loss 4.9390435218811035\n",
      "55: Encoding Loss -1.4357893466949463, Transition Loss -9.989555358886719, Classifier Loss 0.08641423285007477, Total Loss 8.639425277709961\n",
      "55: Encoding Loss -0.9460540413856506, Transition Loss -8.306583404541016, Classifier Loss 0.043816909193992615, Total Loss 4.380029678344727\n",
      "55: Encoding Loss -0.9370637536048889, Transition Loss -13.987957954406738, Classifier Loss 0.044732365757226944, Total Loss 4.4704389572143555\n",
      "55: Encoding Loss -2.6860177516937256, Transition Loss -22.129648208618164, Classifier Loss 0.04169788956642151, Total Loss 4.16536283493042\n",
      "55: Encoding Loss -0.9458300471305847, Transition Loss -7.478934288024902, Classifier Loss 0.06715556234121323, Total Loss 6.714060306549072\n",
      "55: Encoding Loss -0.9393779039382935, Transition Loss -8.324773788452148, Classifier Loss 0.06426961719989777, Total Loss 6.425296783447266\n",
      "55: Encoding Loss -0.660057783126831, Transition Loss -10.144698143005371, Classifier Loss 0.04857052490115166, Total Loss 4.855023384094238\n",
      "55: Encoding Loss -1.1041508913040161, Transition Loss -4.572762489318848, Classifier Loss 0.06079260632395744, Total Loss 6.078346252441406\n",
      "55: Encoding Loss -1.4329688549041748, Transition Loss -13.453926086425781, Classifier Loss 0.060048580169677734, Total Loss 6.002167224884033\n",
      "55: Encoding Loss -1.3671311140060425, Transition Loss -16.532991409301758, Classifier Loss 0.058710746467113495, Total Loss 5.867768287658691\n",
      "55: Encoding Loss -1.583041787147522, Transition Loss -13.465890884399414, Classifier Loss 0.04495040327310562, Total Loss 4.492347240447998\n",
      "55: Encoding Loss -1.0481832027435303, Transition Loss -4.89384651184082, Classifier Loss 0.02764240652322769, Total Loss 2.7632620334625244\n",
      "55: Encoding Loss -1.6729823350906372, Transition Loss -7.5659589767456055, Classifier Loss 0.08334192633628845, Total Loss 8.33267879486084\n",
      "55: Encoding Loss -1.9148037433624268, Transition Loss -14.374306678771973, Classifier Loss 0.04939394071698189, Total Loss 4.936519145965576\n",
      "55: Encoding Loss -0.5743669867515564, Transition Loss -0.06097769737243652, Classifier Loss 0.09045402705669403, Total Loss 9.045390129089355\n",
      "55: Encoding Loss -0.7781940698623657, Transition Loss -8.537408828735352, Classifier Loss 0.0639728531241417, Total Loss 6.395577907562256\n",
      "55: Encoding Loss -0.08119224011898041, Transition Loss -5.045008182525635, Classifier Loss 0.059769757091999054, Total Loss 5.8405914306640625\n",
      "55: Encoding Loss -0.7616811394691467, Transition Loss -11.993398666381836, Classifier Loss 0.04011952877044678, Total Loss 4.009554386138916\n",
      "55: Encoding Loss -1.6725831031799316, Transition Loss -15.857295989990234, Classifier Loss 0.06726325303316116, Total Loss 6.723154067993164\n",
      "55: Encoding Loss -0.781677782535553, Transition Loss -5.36630916595459, Classifier Loss 0.06337714195251465, Total Loss 6.33664083480835\n",
      "55: Encoding Loss -0.7478138208389282, Transition Loss -9.264860153198242, Classifier Loss 0.04023401439189911, Total Loss 4.021548271179199\n",
      "55: Encoding Loss -1.0428500175476074, Transition Loss -9.04929256439209, Classifier Loss 0.04174119234085083, Total Loss 4.172308921813965\n",
      "55: Encoding Loss -0.44684675335884094, Transition Loss -14.364147186279297, Classifier Loss 0.04332539439201355, Total Loss 4.329652786254883\n",
      "55: Encoding Loss -0.8758639097213745, Transition Loss -14.086432456970215, Classifier Loss 0.05738608166575432, Total Loss 5.735791206359863\n",
      "55: Encoding Loss -0.4044979214668274, Transition Loss -1.8387446403503418, Classifier Loss 0.03265378996729851, Total Loss 3.2649266719818115\n",
      "55: Encoding Loss -0.9087831974029541, Transition Loss -17.79998779296875, Classifier Loss 0.0318356454372406, Total Loss 3.180004596710205\n",
      "55: Encoding Loss -1.793749213218689, Transition Loss -17.11581039428711, Classifier Loss 0.026322951540350914, Total Loss 2.6288719177246094\n",
      "55: Encoding Loss -0.5286630988121033, Transition Loss -1.3362884521484375, Classifier Loss 0.04413214325904846, Total Loss 4.412946701049805\n",
      "55: Encoding Loss -0.29624658823013306, Transition Loss 1.6389579772949219, Classifier Loss 0.04984007030725479, Total Loss 5.308182239532471\n",
      "55: Encoding Loss -0.6496981382369995, Transition Loss -11.919256210327148, Classifier Loss 0.029803015291690826, Total Loss 2.9779176712036133\n",
      "55: Encoding Loss -0.9618045091629028, Transition Loss -11.131608009338379, Classifier Loss 0.04290361702442169, Total Loss 4.288135528564453\n",
      "55: Encoding Loss -1.6066282987594604, Transition Loss -13.143362045288086, Classifier Loss 0.045670609921216965, Total Loss 4.564432144165039\n",
      "55: Encoding Loss -0.7123421430587769, Transition Loss -6.8179931640625, Classifier Loss 0.049335379153490067, Total Loss 4.932174205780029\n",
      "55: Encoding Loss -2.164318323135376, Transition Loss -15.315591812133789, Classifier Loss 0.03283752501010895, Total Loss 3.280689239501953\n",
      "55: Encoding Loss -0.8675400018692017, Transition Loss -9.178511619567871, Classifier Loss 0.059906549751758575, Total Loss 5.988819122314453\n",
      "55: Encoding Loss -1.0616700649261475, Transition Loss -12.771018028259277, Classifier Loss 0.07790999859571457, Total Loss 7.788445472717285\n",
      "55: Encoding Loss -2.0234193801879883, Transition Loss -20.12900161743164, Classifier Loss 0.039354294538497925, Total Loss 3.931403875350952\n",
      "55: Encoding Loss -1.7590656280517578, Transition Loss -8.790410995483398, Classifier Loss 0.05400159955024719, Total Loss 5.398401737213135\n",
      "55: Encoding Loss -1.415817141532898, Transition Loss -10.391228675842285, Classifier Loss 0.077595554292202, Total Loss 7.757477283477783\n",
      "55: Encoding Loss -1.399633765220642, Transition Loss -16.2857723236084, Classifier Loss 0.06615196168422699, Total Loss 6.611938953399658\n",
      "55: Encoding Loss -1.6967822313308716, Transition Loss -12.905911445617676, Classifier Loss 0.02390499971807003, Total Loss 2.3879189491271973\n",
      "55: Encoding Loss -0.6846515536308289, Transition Loss -11.703004837036133, Classifier Loss 0.046939097344875336, Total Loss 4.691568851470947\n",
      "55: Encoding Loss -1.8499245643615723, Transition Loss -5.263195037841797, Classifier Loss 0.03625238314270973, Total Loss 3.624185800552368\n",
      "55: Encoding Loss -1.224039912223816, Transition Loss -8.888727188110352, Classifier Loss 0.05254845321178436, Total Loss 5.253067493438721\n",
      "55: Encoding Loss -1.4511741399765015, Transition Loss -9.6575345993042, Classifier Loss 0.04621715098619461, Total Loss 4.619783401489258\n",
      "55: Encoding Loss -1.0281684398651123, Transition Loss -5.239683151245117, Classifier Loss 0.027390992268919945, Total Loss 2.738051414489746\n",
      "55: Encoding Loss -1.1306642293930054, Transition Loss -10.571334838867188, Classifier Loss 0.023310167714953423, Total Loss 2.32890248298645\n",
      "55: Encoding Loss -0.9799802899360657, Transition Loss -12.948162078857422, Classifier Loss 0.0643007904291153, Total Loss 6.427489280700684\n",
      "55: Encoding Loss -0.44235897064208984, Transition Loss -12.67100715637207, Classifier Loss 0.04600042104721069, Total Loss 4.5974907875061035\n",
      "55: Encoding Loss -1.2763203382492065, Transition Loss -0.7257201671600342, Classifier Loss 0.06780441850423813, Total Loss 6.780296802520752\n",
      "55: Encoding Loss -1.368821382522583, Transition Loss -8.943340301513672, Classifier Loss 0.029359951615333557, Total Loss 2.934206485748291\n",
      "55: Encoding Loss -0.6918972134590149, Transition Loss -11.421512603759766, Classifier Loss 0.06381939351558685, Total Loss 6.379654884338379\n",
      "55: Encoding Loss -0.31438910961151123, Transition Loss -10.177274703979492, Classifier Loss 0.03843628987669945, Total Loss 3.8394968509674072\n",
      "55: Encoding Loss -0.7392510175704956, Transition Loss -9.561756134033203, Classifier Loss 0.042692434042692184, Total Loss 4.267331123352051\n",
      "55: Encoding Loss -2.0469377040863037, Transition Loss -5.410869121551514, Classifier Loss 0.06329754739999771, Total Loss 6.328672885894775\n",
      "55: Encoding Loss -1.914037823677063, Transition Loss -14.73257064819336, Classifier Loss 0.03021412156522274, Total Loss 3.018465518951416\n",
      "55: Encoding Loss 0.006449068430811167, Transition Loss -1.8577698469161987, Classifier Loss 0.04224042221903801, Total Loss 4.25079345703125\n",
      "55: Encoding Loss -1.0681841373443604, Transition Loss -7.713374614715576, Classifier Loss 0.025531291961669922, Total Loss 2.551586627960205\n",
      "55: Encoding Loss -0.5843221545219421, Transition Loss -7.220090389251709, Classifier Loss 0.07672646641731262, Total Loss 7.671202659606934\n",
      "55: Encoding Loss -0.5180016160011292, Transition Loss -15.694992065429688, Classifier Loss 0.13654762506484985, Total Loss 13.651622772216797\n",
      "55: Encoding Loss -2.483325719833374, Transition Loss -15.844635963439941, Classifier Loss 0.05880804359912872, Total Loss 5.877635478973389\n",
      "55: Encoding Loss -2.6138525009155273, Transition Loss -21.16461753845215, Classifier Loss 0.03387405350804329, Total Loss 3.3831725120544434\n",
      "55: Encoding Loss -1.096031665802002, Transition Loss -11.294278144836426, Classifier Loss 0.07933194935321808, Total Loss 7.930936336517334\n",
      "55: Encoding Loss -2.235053539276123, Transition Loss -13.025399208068848, Classifier Loss 0.03879208490252495, Total Loss 3.876603603363037\n",
      "55: Encoding Loss -2.759639263153076, Transition Loss -18.78144073486328, Classifier Loss 0.042119551450014114, Total Loss 4.208198547363281\n",
      "55: Encoding Loss -2.098701000213623, Transition Loss -7.148441314697266, Classifier Loss 0.07265176624059677, Total Loss 7.263747215270996\n",
      "55: Encoding Loss -1.1589118242263794, Transition Loss -15.488256454467773, Classifier Loss 0.06793767958879471, Total Loss 6.790670394897461\n",
      "55: Encoding Loss -1.6832432746887207, Transition Loss -16.36123275756836, Classifier Loss 0.01912880316376686, Total Loss 1.909608006477356\n",
      "55: Encoding Loss -1.7358171939849854, Transition Loss -19.50257682800293, Classifier Loss 0.051232993602752686, Total Loss 5.119399070739746\n",
      "55: Encoding Loss -1.8149882555007935, Transition Loss -25.266780853271484, Classifier Loss 0.010840416885912418, Total Loss 1.0789883136749268\n",
      "56: Encoding Loss -1.2542115449905396, Transition Loss -5.452813148498535, Classifier Loss 0.02661910280585289, Total Loss 2.6608197689056396\n",
      "56: Encoding Loss -1.9834421873092651, Transition Loss -6.204704284667969, Classifier Loss 0.08052794635295868, Total Loss 8.051553726196289\n",
      "56: Encoding Loss -1.753787636756897, Transition Loss -1.1360359191894531, Classifier Loss 0.029929356649518013, Total Loss 2.992708444595337\n",
      "56: Encoding Loss -1.5837301015853882, Transition Loss -18.469158172607422, Classifier Loss 0.04543117806315422, Total Loss 4.539423942565918\n",
      "56: Encoding Loss -1.5293991565704346, Transition Loss -9.210097312927246, Classifier Loss 0.03834545984864235, Total Loss 3.8327040672302246\n",
      "56: Encoding Loss -1.7238596677780151, Transition Loss -15.299272537231445, Classifier Loss 0.038754500448703766, Total Loss 3.8723902702331543\n",
      "56: Encoding Loss -2.437974214553833, Transition Loss -11.039525985717773, Classifier Loss 0.04460515081882477, Total Loss 4.458307266235352\n",
      "56: Encoding Loss -1.3748466968536377, Transition Loss -8.925707817077637, Classifier Loss 0.039520129561424255, Total Loss 3.950227975845337\n",
      "56: Encoding Loss -1.4135546684265137, Transition Loss -15.29736614227295, Classifier Loss 0.02778184972703457, Total Loss 2.775125503540039\n",
      "56: Encoding Loss -2.349815607070923, Transition Loss -12.700736999511719, Classifier Loss 0.04560888558626175, Total Loss 4.558348655700684\n",
      "56: Encoding Loss -0.21337465941905975, Transition Loss -8.875943183898926, Classifier Loss 0.043183017522096634, Total Loss 4.288477420806885\n",
      "56: Encoding Loss -1.2381693124771118, Transition Loss -5.894033908843994, Classifier Loss 0.05101873353123665, Total Loss 5.10069465637207\n",
      "56: Encoding Loss -1.7534449100494385, Transition Loss -7.543465614318848, Classifier Loss 0.03590230643749237, Total Loss 3.588721990585327\n",
      "56: Encoding Loss -1.1733585596084595, Transition Loss -8.355390548706055, Classifier Loss 0.045192230492830276, Total Loss 4.517551898956299\n",
      "56: Encoding Loss -2.3068313598632812, Transition Loss -14.898595809936523, Classifier Loss 0.026963122189044952, Total Loss 2.6933324337005615\n",
      "56: Encoding Loss -2.370039224624634, Transition Loss -18.81487464904785, Classifier Loss 0.0553324855864048, Total Loss 5.52948522567749\n",
      "56: Encoding Loss -1.1208406686782837, Transition Loss -11.244397163391113, Classifier Loss 0.016438061371445656, Total Loss 1.641557216644287\n",
      "56: Encoding Loss -0.7773645520210266, Transition Loss -7.5124125480651855, Classifier Loss 0.03004559688270092, Total Loss 3.0030572414398193\n",
      "56: Encoding Loss -0.8118584752082825, Transition Loss -14.471364974975586, Classifier Loss 0.06780092418193817, Total Loss 6.77719783782959\n",
      "56: Encoding Loss -0.5328699350357056, Transition Loss -6.905275344848633, Classifier Loss 0.04329774156212807, Total Loss 4.32839298248291\n",
      "56: Encoding Loss -1.5009236335754395, Transition Loss -16.1387939453125, Classifier Loss 0.06488251686096191, Total Loss 6.4850239753723145\n",
      "56: Encoding Loss -1.6437652111053467, Transition Loss -12.338197708129883, Classifier Loss 0.030602021142840385, Total Loss 3.057734489440918\n",
      "56: Encoding Loss -1.301114559173584, Transition Loss -12.851605415344238, Classifier Loss 0.04922734200954437, Total Loss 4.920164108276367\n",
      "56: Encoding Loss -1.3852061033248901, Transition Loss -5.448808193206787, Classifier Loss 0.06374682486057281, Total Loss 6.373592853546143\n",
      "56: Encoding Loss -1.8853317499160767, Transition Loss -12.84260368347168, Classifier Loss 0.027513381093740463, Total Loss 2.748769521713257\n",
      "56: Encoding Loss -0.6716926693916321, Transition Loss -6.688942909240723, Classifier Loss 0.05543883144855499, Total Loss 5.542545318603516\n",
      "56: Encoding Loss -2.4309277534484863, Transition Loss -5.270299911499023, Classifier Loss 0.07559464871883392, Total Loss 7.55841064453125\n",
      "56: Encoding Loss -2.9672231674194336, Transition Loss -19.4332275390625, Classifier Loss 0.05392533540725708, Total Loss 5.388646602630615\n",
      "56: Encoding Loss -2.0483601093292236, Transition Loss -10.734569549560547, Classifier Loss 0.041700348258018494, Total Loss 4.167888164520264\n",
      "56: Encoding Loss -0.8061150908470154, Transition Loss -6.957659721374512, Classifier Loss 0.048289693892002106, Total Loss 4.827578067779541\n",
      "56: Encoding Loss -1.3352071046829224, Transition Loss -12.810912132263184, Classifier Loss 0.05171244591474533, Total Loss 5.16868257522583\n",
      "56: Encoding Loss -1.6451839208602905, Transition Loss -4.983244895935059, Classifier Loss 0.0642714649438858, Total Loss 6.426149845123291\n",
      "56: Encoding Loss -0.7666239142417908, Transition Loss -12.183923721313477, Classifier Loss 0.05036040395498276, Total Loss 5.033603668212891\n",
      "56: Encoding Loss -0.7478968501091003, Transition Loss -7.251812934875488, Classifier Loss 0.14548633992671967, Total Loss 14.5471830368042\n",
      "56: Encoding Loss -1.7974263429641724, Transition Loss -13.985006332397461, Classifier Loss 0.033777426928281784, Total Loss 3.374945878982544\n",
      "56: Encoding Loss -0.8816182017326355, Transition Loss -9.372748374938965, Classifier Loss 0.033787135034799576, Total Loss 3.3768391609191895\n",
      "56: Encoding Loss -0.9703525304794312, Transition Loss -8.472634315490723, Classifier Loss 0.053698670119047165, Total Loss 5.3681721687316895\n",
      "56: Encoding Loss -1.1108752489089966, Transition Loss -10.565486907958984, Classifier Loss 0.0861051082611084, Total Loss 8.608397483825684\n",
      "56: Encoding Loss -1.777194619178772, Transition Loss -11.8827486038208, Classifier Loss 0.045542147010564804, Total Loss 4.551837921142578\n",
      "56: Encoding Loss -0.8303946256637573, Transition Loss -12.810342788696289, Classifier Loss 0.05021005496382713, Total Loss 5.018443584442139\n",
      "56: Encoding Loss -0.7057584524154663, Transition Loss -9.793368339538574, Classifier Loss 0.025039903819561005, Total Loss 2.5020318031311035\n",
      "56: Encoding Loss -1.4128326177597046, Transition Loss -17.168750762939453, Classifier Loss 0.05741230770945549, Total Loss 5.737797260284424\n",
      "56: Encoding Loss -1.7915558815002441, Transition Loss -16.4823055267334, Classifier Loss 0.025395432487130165, Total Loss 2.5362467765808105\n",
      "56: Encoding Loss -2.149043321609497, Transition Loss -7.3514018058776855, Classifier Loss 0.04434643313288689, Total Loss 4.433173179626465\n",
      "56: Encoding Loss -1.224419355392456, Transition Loss -17.75373077392578, Classifier Loss 0.03507297858595848, Total Loss 3.50374698638916\n",
      "56: Encoding Loss 0.0657161995768547, Transition Loss -8.697687149047852, Classifier Loss 0.056176573038101196, Total Loss 6.007303237915039\n",
      "56: Encoding Loss -0.32186073064804077, Transition Loss -19.649051666259766, Classifier Loss 0.04727869853377342, Total Loss 4.7222819328308105\n",
      "56: Encoding Loss -1.021175503730774, Transition Loss -6.8388991355896, Classifier Loss 0.022341739386320114, Total Loss 2.2328062057495117\n",
      "56: Encoding Loss -2.3150479793548584, Transition Loss -10.35456657409668, Classifier Loss 0.03874307870864868, Total Loss 3.872236967086792\n",
      "56: Encoding Loss -1.3646831512451172, Transition Loss -17.611848831176758, Classifier Loss 0.03562942519783974, Total Loss 3.559420108795166\n",
      "56: Encoding Loss -1.6008096933364868, Transition Loss -21.10829734802246, Classifier Loss 0.04971323534846306, Total Loss 4.96710205078125\n",
      "56: Encoding Loss -0.31203827261924744, Transition Loss -13.82392406463623, Classifier Loss 0.06440740078687668, Total Loss 6.435720920562744\n",
      "56: Encoding Loss -1.335227131843567, Transition Loss -3.2980358600616455, Classifier Loss 0.0815417617559433, Total Loss 8.153515815734863\n",
      "56: Encoding Loss -1.8808873891830444, Transition Loss -13.470648765563965, Classifier Loss 0.03014501929283142, Total Loss 3.011807918548584\n",
      "56: Encoding Loss -1.4286727905273438, Transition Loss -13.501249313354492, Classifier Loss 0.06283777952194214, Total Loss 6.281077861785889\n",
      "56: Encoding Loss -1.7052439451217651, Transition Loss -6.118539810180664, Classifier Loss 0.07453073561191559, Total Loss 7.451849937438965\n",
      "56: Encoding Loss -1.200255274772644, Transition Loss -12.345802307128906, Classifier Loss 0.02260293997824192, Total Loss 2.2578248977661133\n",
      "56: Encoding Loss -2.3635077476501465, Transition Loss -14.399652481079102, Classifier Loss 0.04031949117779732, Total Loss 4.029068946838379\n",
      "56: Encoding Loss -2.5149621963500977, Transition Loss -21.42935562133789, Classifier Loss 0.027645453810691833, Total Loss 2.7602596282958984\n",
      "56: Encoding Loss -1.7569680213928223, Transition Loss -17.929780960083008, Classifier Loss 0.05089545622467995, Total Loss 5.0859599113464355\n",
      "56: Encoding Loss -2.1172003746032715, Transition Loss -10.328868865966797, Classifier Loss 0.04856403172016144, Total Loss 4.854337692260742\n",
      "56: Encoding Loss -2.3521857261657715, Transition Loss -8.991024017333984, Classifier Loss 0.07811400294303894, Total Loss 7.8096022605896\n",
      "56: Encoding Loss -2.0366122722625732, Transition Loss -10.687979698181152, Classifier Loss 0.037598542869091034, Total Loss 3.757716655731201\n",
      "56: Encoding Loss -1.3179315328598022, Transition Loss -8.231562614440918, Classifier Loss 0.059954579919576645, Total Loss 5.99381160736084\n",
      "56: Encoding Loss -1.6727474927902222, Transition Loss -12.94066333770752, Classifier Loss 0.05658945441246033, Total Loss 5.656357288360596\n",
      "56: Encoding Loss -2.222297430038452, Transition Loss -5.600722312927246, Classifier Loss 0.08676677197217941, Total Loss 8.675556182861328\n",
      "56: Encoding Loss -0.668931245803833, Transition Loss -2.203645706176758, Classifier Loss 0.045250847935676575, Total Loss 4.524644374847412\n",
      "56: Encoding Loss -1.9997652769088745, Transition Loss -20.418598175048828, Classifier Loss 0.0644153505563736, Total Loss 6.437451362609863\n",
      "56: Encoding Loss -0.5932282209396362, Transition Loss -10.27785587310791, Classifier Loss 0.058201149106025696, Total Loss 5.81805944442749\n",
      "56: Encoding Loss -1.3387043476104736, Transition Loss -10.071649551391602, Classifier Loss 0.08885601162910461, Total Loss 8.883586883544922\n",
      "56: Encoding Loss -2.057382345199585, Transition Loss -10.095559120178223, Classifier Loss 0.054322145879268646, Total Loss 5.4301958084106445\n",
      "56: Encoding Loss -1.6137058734893799, Transition Loss -10.030332565307617, Classifier Loss 0.06217862665653229, Total Loss 6.215856552124023\n",
      "56: Encoding Loss -1.2877949476242065, Transition Loss -6.566194534301758, Classifier Loss 0.07164976000785828, Total Loss 7.163662910461426\n",
      "56: Encoding Loss -1.711150050163269, Transition Loss -10.06321907043457, Classifier Loss 0.01577099598944187, Total Loss 1.5750869512557983\n",
      "56: Encoding Loss -2.438138008117676, Transition Loss -9.208052635192871, Classifier Loss 0.0776519700884819, Total Loss 7.763355255126953\n",
      "56: Encoding Loss -1.7462189197540283, Transition Loss -16.175304412841797, Classifier Loss 0.027505485340952873, Total Loss 2.7473134994506836\n",
      "56: Encoding Loss -1.1147081851959229, Transition Loss -7.451087951660156, Classifier Loss 0.05727675184607506, Total Loss 5.726184844970703\n",
      "56: Encoding Loss -2.8411948680877686, Transition Loss -13.739858627319336, Classifier Loss 0.04540539160370827, Total Loss 4.5377912521362305\n",
      "56: Encoding Loss -1.9641207456588745, Transition Loss -14.920528411865234, Classifier Loss 0.018791761249303818, Total Loss 1.8761920928955078\n",
      "56: Encoding Loss -1.633732557296753, Transition Loss -11.132829666137695, Classifier Loss 0.08332088589668274, Total Loss 8.329861640930176\n",
      "56: Encoding Loss -0.9769408106803894, Transition Loss -12.422511100769043, Classifier Loss 0.03933937847614288, Total Loss 3.931453227996826\n",
      "56: Encoding Loss -1.6614525318145752, Transition Loss -10.17758846282959, Classifier Loss 0.03378850966691971, Total Loss 3.3768153190612793\n",
      "56: Encoding Loss -1.5151795148849487, Transition Loss -6.3482666015625, Classifier Loss 0.08495861291885376, Total Loss 8.49459171295166\n",
      "56: Encoding Loss -0.8428458571434021, Transition Loss -12.831501960754395, Classifier Loss 0.06136225163936615, Total Loss 6.1336588859558105\n",
      "56: Encoding Loss -2.5181772708892822, Transition Loss -8.728180885314941, Classifier Loss 0.04228822514414787, Total Loss 4.227077007293701\n",
      "56: Encoding Loss -1.5523812770843506, Transition Loss -6.815032958984375, Classifier Loss 0.05981305614113808, Total Loss 5.979942798614502\n",
      "56: Encoding Loss -2.284371852874756, Transition Loss -17.595706939697266, Classifier Loss 0.04632225260138512, Total Loss 4.628705978393555\n",
      "56: Encoding Loss -1.684418797492981, Transition Loss -10.612199783325195, Classifier Loss 0.09426003694534302, Total Loss 9.423880577087402\n",
      "56: Encoding Loss -1.4465152025222778, Transition Loss -9.102690696716309, Classifier Loss 0.042953867465257645, Total Loss 4.2935662269592285\n",
      "56: Encoding Loss -1.3332346677780151, Transition Loss -15.216377258300781, Classifier Loss 0.04239971190690994, Total Loss 4.2369279861450195\n",
      "56: Encoding Loss -3.53415584564209, Transition Loss -24.053762435913086, Classifier Loss 0.039339665323495865, Total Loss 3.9291558265686035\n",
      "56: Encoding Loss -1.2058852910995483, Transition Loss -8.230649948120117, Classifier Loss 0.06899148225784302, Total Loss 6.8975019454956055\n",
      "56: Encoding Loss -1.5525010824203491, Transition Loss -9.040273666381836, Classifier Loss 0.06112513691186905, Total Loss 6.110705375671387\n",
      "56: Encoding Loss -1.0220929384231567, Transition Loss -11.001681327819824, Classifier Loss 0.04714931547641754, Total Loss 4.71273136138916\n",
      "56: Encoding Loss -1.674496054649353, Transition Loss -4.976930141448975, Classifier Loss 0.060560308396816254, Total Loss 6.055035591125488\n",
      "56: Encoding Loss -1.8701237440109253, Transition Loss -14.598764419555664, Classifier Loss 0.05920945480465889, Total Loss 5.918025970458984\n",
      "56: Encoding Loss -1.81484854221344, Transition Loss -18.037622451782227, Classifier Loss 0.06103108450770378, Total Loss 6.09950065612793\n",
      "56: Encoding Loss -1.9603171348571777, Transition Loss -14.552677154541016, Classifier Loss 0.045756272971630096, Total Loss 4.57271671295166\n",
      "56: Encoding Loss -1.2458806037902832, Transition Loss -5.245695114135742, Classifier Loss 0.027807030826807022, Total Loss 2.779654026031494\n",
      "56: Encoding Loss -1.9432820081710815, Transition Loss -8.053376197814941, Classifier Loss 0.08321306109428406, Total Loss 8.319695472717285\n",
      "56: Encoding Loss -2.4089410305023193, Transition Loss -15.293293952941895, Classifier Loss 0.048727527260780334, Total Loss 4.869694232940674\n",
      "56: Encoding Loss -1.0013631582260132, Transition Loss 0.15703225135803223, Classifier Loss 0.09163560718297958, Total Loss 9.194967269897461\n",
      "56: Encoding Loss -1.2341396808624268, Transition Loss -9.324064254760742, Classifier Loss 0.06311404705047607, Total Loss 6.309539794921875\n",
      "56: Encoding Loss -0.5350990295410156, Transition Loss -5.761370658874512, Classifier Loss 0.059758927673101425, Total Loss 5.974740505218506\n",
      "56: Encoding Loss -1.2700213193893433, Transition Loss -13.186416625976562, Classifier Loss 0.039697855710983276, Total Loss 3.9671483039855957\n",
      "56: Encoding Loss -2.2807648181915283, Transition Loss -17.112159729003906, Classifier Loss 0.06037869304418564, Total Loss 6.034447193145752\n",
      "56: Encoding Loss -1.36989164352417, Transition Loss -5.9782209396362305, Classifier Loss 0.05843978002667427, Total Loss 5.842782497406006\n",
      "56: Encoding Loss -0.8340500593185425, Transition Loss -10.07222843170166, Classifier Loss 0.040454354137182236, Total Loss 4.043420791625977\n",
      "56: Encoding Loss -1.4462311267852783, Transition Loss -9.817793846130371, Classifier Loss 0.041528645902872086, Total Loss 4.150900840759277\n",
      "56: Encoding Loss -0.9747094511985779, Transition Loss -15.564172744750977, Classifier Loss 0.0419173389673233, Total Loss 4.1886210441589355\n",
      "56: Encoding Loss -1.323487639427185, Transition Loss -15.255136489868164, Classifier Loss 0.05664673447608948, Total Loss 5.661622524261475\n",
      "56: Encoding Loss -0.9326964020729065, Transition Loss -2.082017660140991, Classifier Loss 0.03366978093981743, Total Loss 3.3665616512298584\n",
      "56: Encoding Loss -1.206834077835083, Transition Loss -19.170215606689453, Classifier Loss 0.02961854822933674, Total Loss 2.9580209255218506\n",
      "56: Encoding Loss -2.19036865234375, Transition Loss -18.408824920654297, Classifier Loss 0.026306411251425743, Total Loss 2.6269595623016357\n",
      "56: Encoding Loss -0.9773220419883728, Transition Loss -1.3801283836364746, Classifier Loss 0.04288729652762413, Total Loss 4.288453578948975\n",
      "56: Encoding Loss -0.5306016206741333, Transition Loss 1.8089643716812134, Classifier Loss 0.0475957952439785, Total Loss 5.121372222900391\n",
      "56: Encoding Loss -0.7353262305259705, Transition Loss -12.893617630004883, Classifier Loss 0.02861209586262703, Total Loss 2.858630895614624\n",
      "56: Encoding Loss -1.392444372177124, Transition Loss -12.05081558227539, Classifier Loss 0.044131722301244736, Total Loss 4.410762310028076\n",
      "56: Encoding Loss -2.4122397899627686, Transition Loss -14.346963882446289, Classifier Loss 0.04547931253910065, Total Loss 4.5450615882873535\n",
      "56: Encoding Loss -1.3927967548370361, Transition Loss -7.524353504180908, Classifier Loss 0.04939674958586693, Total Loss 4.938169956207275\n",
      "56: Encoding Loss -2.782475709915161, Transition Loss -16.630489349365234, Classifier Loss 0.03243018686771393, Total Loss 3.239692449569702\n",
      "56: Encoding Loss -0.9534720182418823, Transition Loss -10.017290115356445, Classifier Loss 0.059905387461185455, Total Loss 5.988534927368164\n",
      "56: Encoding Loss -1.5744072198867798, Transition Loss -13.832280158996582, Classifier Loss 0.07231128960847855, Total Loss 7.228362560272217\n",
      "56: Encoding Loss -2.7549490928649902, Transition Loss -21.900360107421875, Classifier Loss 0.04188365861773491, Total Loss 4.183985710144043\n",
      "56: Encoding Loss -2.2285406589508057, Transition Loss -9.684455871582031, Classifier Loss 0.05513792484998703, Total Loss 5.511855602264404\n",
      "56: Encoding Loss -2.1499574184417725, Transition Loss -11.360235214233398, Classifier Loss 0.07529096305370331, Total Loss 7.526823997497559\n",
      "56: Encoding Loss -2.0081140995025635, Transition Loss -17.653310775756836, Classifier Loss 0.06154070422053337, Total Loss 6.150539875030518\n",
      "56: Encoding Loss -2.1331374645233154, Transition Loss -14.049400329589844, Classifier Loss 0.022937525063753128, Total Loss 2.290942668914795\n",
      "56: Encoding Loss -0.8189265131950378, Transition Loss -12.831391334533691, Classifier Loss 0.04516888037323952, Total Loss 4.514321804046631\n",
      "56: Encoding Loss -1.7884061336517334, Transition Loss -5.996212005615234, Classifier Loss 0.03739745542407036, Total Loss 3.738546371459961\n",
      "56: Encoding Loss -1.139354944229126, Transition Loss -9.788352966308594, Classifier Loss 0.052873097360134125, Total Loss 5.285351753234863\n",
      "56: Encoding Loss -1.915423035621643, Transition Loss -10.7031831741333, Classifier Loss 0.04544786736369133, Total Loss 4.542646408081055\n",
      "56: Encoding Loss -1.12279212474823, Transition Loss -5.841210842132568, Classifier Loss 0.02553923800587654, Total Loss 2.55275559425354\n",
      "56: Encoding Loss -1.251991868019104, Transition Loss -11.57967472076416, Classifier Loss 0.023067176342010498, Total Loss 2.3044016361236572\n",
      "56: Encoding Loss -1.379991888999939, Transition Loss -14.078221321105957, Classifier Loss 0.06656409054994583, Total Loss 6.65359354019165\n",
      "56: Encoding Loss -0.29279521107673645, Transition Loss -13.911613464355469, Classifier Loss 0.044992342591285706, Total Loss 4.492455959320068\n",
      "56: Encoding Loss -1.2470492124557495, Transition Loss -0.9420783519744873, Classifier Loss 0.07184667140245438, Total Loss 7.184478759765625\n",
      "56: Encoding Loss -1.4871565103530884, Transition Loss -9.879461288452148, Classifier Loss 0.02884737215936184, Total Loss 2.8827614784240723\n",
      "56: Encoding Loss -0.8194376826286316, Transition Loss -12.49250316619873, Classifier Loss 0.06416137516498566, Total Loss 6.413639068603516\n",
      "56: Encoding Loss -0.4641975462436676, Transition Loss -11.319458961486816, Classifier Loss 0.038459550589323044, Total Loss 3.8436849117279053\n",
      "56: Encoding Loss -0.9032968878746033, Transition Loss -10.509430885314941, Classifier Loss 0.04303829371929169, Total Loss 4.301727294921875\n",
      "56: Encoding Loss -2.426254987716675, Transition Loss -6.0818023681640625, Classifier Loss 0.06250323355197906, Total Loss 6.2491068840026855\n",
      "56: Encoding Loss -2.3851449489593506, Transition Loss -16.0650691986084, Classifier Loss 0.03024141676723957, Total Loss 3.0209288597106934\n",
      "56: Encoding Loss -0.3337840735912323, Transition Loss -2.2158524990081787, Classifier Loss 0.04169011861085892, Total Loss 4.1674418449401855\n",
      "56: Encoding Loss -1.013643503189087, Transition Loss -8.904522895812988, Classifier Loss 0.024946406483650208, Total Loss 2.4928596019744873\n",
      "56: Encoding Loss -0.29633188247680664, Transition Loss -8.255203247070312, Classifier Loss 0.07815919816493988, Total Loss 7.810661792755127\n",
      "56: Encoding Loss -0.8717416524887085, Transition Loss -18.238094329833984, Classifier Loss 0.12591266632080078, Total Loss 12.587618827819824\n",
      "56: Encoding Loss -2.391993999481201, Transition Loss -18.45640754699707, Classifier Loss 0.06718456000089645, Total Loss 6.714764595031738\n",
      "56: Encoding Loss -2.479395627975464, Transition Loss -24.424514770507812, Classifier Loss 0.034040145576000214, Total Loss 3.399129629135132\n",
      "56: Encoding Loss -1.34770667552948, Transition Loss -13.124677658081055, Classifier Loss 0.08326750248670578, Total Loss 8.324125289916992\n",
      "56: Encoding Loss -2.213426351547241, Transition Loss -15.066279411315918, Classifier Loss 0.03880136460065842, Total Loss 3.8771231174468994\n",
      "56: Encoding Loss -2.5778422355651855, Transition Loss -21.59583854675293, Classifier Loss 0.04149143397808075, Total Loss 4.144824028015137\n",
      "56: Encoding Loss -1.8352819681167603, Transition Loss -8.186674118041992, Classifier Loss 0.07138122618198395, Total Loss 7.1364850997924805\n",
      "56: Encoding Loss -1.1368670463562012, Transition Loss -17.66104507446289, Classifier Loss 0.06848270446062088, Total Loss 6.844738006591797\n",
      "56: Encoding Loss -1.4362152814865112, Transition Loss -18.78658103942871, Classifier Loss 0.020546026527881622, Total Loss 2.0508453845977783\n",
      "56: Encoding Loss -1.6907438039779663, Transition Loss -22.2314453125, Classifier Loss 0.050930626690387726, Total Loss 5.088616371154785\n",
      "56: Encoding Loss -2.2439324855804443, Transition Loss -28.619327545166016, Classifier Loss 0.011464731767773628, Total Loss 1.1407493352890015\n",
      "57: Encoding Loss -0.8336024880409241, Transition Loss -6.0510406494140625, Classifier Loss 0.026287412270903587, Total Loss 2.627531051635742\n",
      "57: Encoding Loss -2.0595812797546387, Transition Loss -6.9429473876953125, Classifier Loss 0.07761380076408386, Total Loss 7.759991645812988\n",
      "57: Encoding Loss -1.5355896949768066, Transition Loss -0.9532033205032349, Classifier Loss 0.03004579246044159, Total Loss 3.0043885707855225\n",
      "57: Encoding Loss -1.5296822786331177, Transition Loss -20.900390625, Classifier Loss 0.04492815583944321, Total Loss 4.488635540008545\n",
      "57: Encoding Loss -1.480239987373352, Transition Loss -10.339548110961914, Classifier Loss 0.03847890347242355, Total Loss 3.84582257270813\n",
      "57: Encoding Loss -1.647524356842041, Transition Loss -17.012893676757812, Classifier Loss 0.037963636219501495, Total Loss 3.7929611206054688\n",
      "57: Encoding Loss -2.2023584842681885, Transition Loss -12.1765775680542, Classifier Loss 0.04331134632229805, Total Loss 4.328699588775635\n",
      "57: Encoding Loss -0.9573981761932373, Transition Loss -9.741060256958008, Classifier Loss 0.03853471949696541, Total Loss 3.8515238761901855\n",
      "57: Encoding Loss -1.2008938789367676, Transition Loss -17.035625457763672, Classifier Loss 0.0271623432636261, Total Loss 2.71282696723938\n",
      "57: Encoding Loss -2.3076014518737793, Transition Loss -14.092466354370117, Classifier Loss 0.04771490395069122, Total Loss 4.768671989440918\n",
      "57: Encoding Loss -0.4645037353038788, Transition Loss -9.786933898925781, Classifier Loss 0.04264501482248306, Total Loss 4.262537956237793\n",
      "57: Encoding Loss -0.6649748086929321, Transition Loss -5.920316219329834, Classifier Loss 0.051977336406707764, Total Loss 5.196549892425537\n",
      "57: Encoding Loss -1.4699229001998901, Transition Loss -8.112442970275879, Classifier Loss 0.03541461378335953, Total Loss 3.539839029312134\n",
      "57: Encoding Loss -1.062525987625122, Transition Loss -9.06961727142334, Classifier Loss 0.045035991817712784, Total Loss 4.5017852783203125\n",
      "57: Encoding Loss -1.6526156663894653, Transition Loss -16.481563568115234, Classifier Loss 0.026890331879258156, Total Loss 2.685736894607544\n",
      "57: Encoding Loss -2.3105642795562744, Transition Loss -20.936338424682617, Classifier Loss 0.054369986057281494, Total Loss 5.432811260223389\n",
      "57: Encoding Loss -1.261782169342041, Transition Loss -12.491039276123047, Classifier Loss 0.017039313912391663, Total Loss 1.7014331817626953\n",
      "57: Encoding Loss -0.8022726774215698, Transition Loss -8.294339179992676, Classifier Loss 0.028462864458560944, Total Loss 2.844627618789673\n",
      "57: Encoding Loss -0.7727741599082947, Transition Loss -16.064531326293945, Classifier Loss 0.06518921256065369, Total Loss 6.5157084465026855\n",
      "57: Encoding Loss -0.6022170782089233, Transition Loss -7.511500835418701, Classifier Loss 0.0423455685377121, Total Loss 4.233054161071777\n",
      "57: Encoding Loss -1.271885633468628, Transition Loss -18.039697647094727, Classifier Loss 0.06487980484962463, Total Loss 6.484372615814209\n",
      "57: Encoding Loss -1.5921660661697388, Transition Loss -13.663969039916992, Classifier Loss 0.030002212151885033, Total Loss 2.997488498687744\n",
      "57: Encoding Loss -1.354318618774414, Transition Loss -14.287615776062012, Classifier Loss 0.04754966124892235, Total Loss 4.752108573913574\n",
      "57: Encoding Loss -1.1006451845169067, Transition Loss -5.633235931396484, Classifier Loss 0.059750497341156006, Total Loss 5.973923206329346\n",
      "57: Encoding Loss -1.6866143941879272, Transition Loss -14.237831115722656, Classifier Loss 0.027048384770751, Total Loss 2.701990842819214\n",
      "57: Encoding Loss -0.11393595486879349, Transition Loss -7.157479763031006, Classifier Loss 0.05066244676709175, Total Loss 4.9488019943237305\n",
      "57: Encoding Loss -2.0643324851989746, Transition Loss -4.975293159484863, Classifier Loss 0.07456918805837631, Total Loss 7.455923557281494\n",
      "57: Encoding Loss -1.6522725820541382, Transition Loss -22.389991760253906, Classifier Loss 0.052542440593242645, Total Loss 5.249765872955322\n",
      "57: Encoding Loss -1.020979404449463, Transition Loss -11.91031265258789, Classifier Loss 0.041512828320264816, Total Loss 4.148900508880615\n",
      "57: Encoding Loss -0.06043414771556854, Transition Loss -7.436158657073975, Classifier Loss 0.04857967793941498, Total Loss 4.724585056304932\n",
      "57: Encoding Loss -0.9319308400154114, Transition Loss -14.13582992553711, Classifier Loss 0.05146148055791855, Total Loss 5.1433210372924805\n",
      "57: Encoding Loss -1.6392050981521606, Transition Loss -4.9116902351379395, Classifier Loss 0.06255283206701279, Total Loss 6.254301071166992\n",
      "57: Encoding Loss -0.724727988243103, Transition Loss -13.294011116027832, Classifier Loss 0.05174728110432625, Total Loss 5.172069072723389\n",
      "57: Encoding Loss -0.3137041926383972, Transition Loss -7.468338966369629, Classifier Loss 0.14600516855716705, Total Loss 14.596882820129395\n",
      "57: Encoding Loss -1.4894421100616455, Transition Loss -15.705933570861816, Classifier Loss 0.035202838480472565, Total Loss 3.5171427726745605\n",
      "57: Encoding Loss -0.4792656898498535, Transition Loss -10.19271183013916, Classifier Loss 0.0335671566426754, Total Loss 3.3546741008758545\n",
      "57: Encoding Loss -0.6142231225967407, Transition Loss -9.420425415039062, Classifier Loss 0.05191598832607269, Total Loss 5.1897149085998535\n",
      "57: Encoding Loss -0.9415746927261353, Transition Loss -11.783804893493652, Classifier Loss 0.08901385962963104, Total Loss 8.899029731750488\n",
      "57: Encoding Loss -1.7667248249053955, Transition Loss -13.452422142028809, Classifier Loss 0.04647045210003853, Total Loss 4.644354820251465\n",
      "57: Encoding Loss -0.8126450181007385, Transition Loss -14.681529998779297, Classifier Loss 0.04975007846951485, Total Loss 4.972071647644043\n",
      "57: Encoding Loss -0.36921456456184387, Transition Loss -10.987282752990723, Classifier Loss 0.02474856749176979, Total Loss 2.4723308086395264\n",
      "57: Encoding Loss -1.3600735664367676, Transition Loss -19.460050582885742, Classifier Loss 0.05322596803307533, Total Loss 5.318705081939697\n",
      "57: Encoding Loss -1.7686588764190674, Transition Loss -18.60198211669922, Classifier Loss 0.025772230699658394, Total Loss 2.573502779006958\n",
      "57: Encoding Loss -2.030571460723877, Transition Loss -8.045027732849121, Classifier Loss 0.04847034066915512, Total Loss 4.845425128936768\n",
      "57: Encoding Loss -1.1722524166107178, Transition Loss -19.93275260925293, Classifier Loss 0.033904533833265305, Total Loss 3.3864667415618896\n",
      "57: Encoding Loss -0.09635554254055023, Transition Loss -9.556222915649414, Classifier Loss 0.05603872984647751, Total Loss 5.472741603851318\n",
      "57: Encoding Loss -1.150586724281311, Transition Loss -21.01671028137207, Classifier Loss 0.04625231400132179, Total Loss 4.621027946472168\n",
      "57: Encoding Loss -1.5183913707733154, Transition Loss -8.572221755981445, Classifier Loss 0.024900808930397034, Total Loss 2.4883663654327393\n",
      "57: Encoding Loss -1.6163601875305176, Transition Loss -11.691618919372559, Classifier Loss 0.04046633094549179, Total Loss 4.044294834136963\n",
      "57: Encoding Loss -1.134581208229065, Transition Loss -19.04885482788086, Classifier Loss 0.036892205476760864, Total Loss 3.685410737991333\n",
      "57: Encoding Loss -1.6978741884231567, Transition Loss -22.56610870361328, Classifier Loss 0.04297877103090286, Total Loss 4.29336404800415\n",
      "57: Encoding Loss -0.3876763582229614, Transition Loss -15.238052368164062, Classifier Loss 0.06116816774010658, Total Loss 6.113605499267578\n",
      "57: Encoding Loss -0.33816832304000854, Transition Loss -4.12497615814209, Classifier Loss 0.0739700049161911, Total Loss 7.395200729370117\n",
      "57: Encoding Loss -0.28229469060897827, Transition Loss -14.731846809387207, Classifier Loss 0.029412951320409775, Total Loss 2.9329757690429688\n",
      "57: Encoding Loss -0.080607108771801, Transition Loss -14.5219144821167, Classifier Loss 0.057528313249349594, Total Loss 5.614441871643066\n",
      "57: Encoding Loss -1.1572930812835693, Transition Loss -6.812273979187012, Classifier Loss 0.0723530501127243, Total Loss 7.23394250869751\n",
      "57: Encoding Loss 0.12642927467823029, Transition Loss -13.313610076904297, Classifier Loss 0.026583194732666016, Total Loss 3.562849998474121\n",
      "57: Encoding Loss 3.5029091835021973, Transition Loss -14.482173919677734, Classifier Loss 0.04104671999812126, Total Loss 32.12504959106445\n",
      "57: Encoding Loss 1.5045217275619507, Transition Loss -16.955707550048828, Classifier Loss 0.026562876999378204, Total Loss 14.689069747924805\n",
      "57: Encoding Loss -1.2509669065475464, Transition Loss -17.02050018310547, Classifier Loss 0.04925878718495369, Total Loss 4.922474384307861\n",
      "57: Encoding Loss -1.660638451576233, Transition Loss -10.188228607177734, Classifier Loss 0.0496390238404274, Total Loss 4.961864948272705\n",
      "57: Encoding Loss -1.958503007888794, Transition Loss -9.132993698120117, Classifier Loss 0.08117654174566269, Total Loss 8.115827560424805\n",
      "57: Encoding Loss -1.6630444526672363, Transition Loss -10.63288688659668, Classifier Loss 0.03881240636110306, Total Loss 3.8791139125823975\n",
      "57: Encoding Loss -0.6971685886383057, Transition Loss -8.274068832397461, Classifier Loss 0.05747102573513985, Total Loss 5.745448112487793\n",
      "57: Encoding Loss -1.164531946182251, Transition Loss -12.278079986572266, Classifier Loss 0.05364533141255379, Total Loss 5.362077236175537\n",
      "57: Encoding Loss -1.5718368291854858, Transition Loss -5.819399833679199, Classifier Loss 0.08487489819526672, Total Loss 8.486326217651367\n",
      "57: Encoding Loss -0.319555401802063, Transition Loss -2.7693123817443848, Classifier Loss 0.0437716543674469, Total Loss 4.3748273849487305\n",
      "57: Encoding Loss -1.6823880672454834, Transition Loss -18.99170684814453, Classifier Loss 0.0623115599155426, Total Loss 6.227357387542725\n",
      "57: Encoding Loss -0.385681688785553, Transition Loss -10.028300285339355, Classifier Loss 0.05720432847738266, Total Loss 5.718249797821045\n",
      "57: Encoding Loss -0.8087552785873413, Transition Loss -9.806900024414062, Classifier Loss 0.08968602865934372, Total Loss 8.966641426086426\n",
      "57: Encoding Loss -1.591267704963684, Transition Loss -9.612710952758789, Classifier Loss 0.05294802784919739, Total Loss 5.292880058288574\n",
      "57: Encoding Loss -1.1119014024734497, Transition Loss -9.699726104736328, Classifier Loss 0.06320783495903015, Total Loss 6.318843841552734\n",
      "57: Encoding Loss -0.9349425435066223, Transition Loss -6.5994133949279785, Classifier Loss 0.06998512148857117, Total Loss 6.9971923828125\n",
      "57: Encoding Loss -1.1873042583465576, Transition Loss -9.922590255737305, Classifier Loss 0.015775905922055244, Total Loss 1.575606107711792\n",
      "57: Encoding Loss -1.6171234846115112, Transition Loss -9.05642318725586, Classifier Loss 0.07804671674966812, Total Loss 7.802860260009766\n",
      "57: Encoding Loss -1.0323532819747925, Transition Loss -15.214088439941406, Classifier Loss 0.02763807214796543, Total Loss 2.7607643604278564\n",
      "57: Encoding Loss -1.1605415344238281, Transition Loss -7.6556196212768555, Classifier Loss 0.05616067349910736, Total Loss 5.614536285400391\n",
      "57: Encoding Loss -2.068814992904663, Transition Loss -13.274870872497559, Classifier Loss 0.0439470149576664, Total Loss 4.3920464515686035\n",
      "57: Encoding Loss -1.6014134883880615, Transition Loss -14.32660961151123, Classifier Loss 0.018799392506480217, Total Loss 1.877073884010315\n",
      "57: Encoding Loss -1.417480707168579, Transition Loss -10.963530540466309, Classifier Loss 0.07497349381446838, Total Loss 7.495156764984131\n",
      "57: Encoding Loss -0.7715684175491333, Transition Loss -12.058135986328125, Classifier Loss 0.0387844555079937, Total Loss 3.8760340213775635\n",
      "57: Encoding Loss -1.4706701040267944, Transition Loss -9.940716743469238, Classifier Loss 0.03400776535272598, Total Loss 3.3987884521484375\n",
      "57: Encoding Loss -1.1502501964569092, Transition Loss -6.569369792938232, Classifier Loss 0.07924340665340424, Total Loss 7.923027038574219\n",
      "57: Encoding Loss -0.5644167065620422, Transition Loss -12.02846622467041, Classifier Loss 0.06047344207763672, Total Loss 6.044938564300537\n",
      "57: Encoding Loss -1.8887009620666504, Transition Loss -8.763136863708496, Classifier Loss 0.04131493717432022, Total Loss 4.1297407150268555\n",
      "57: Encoding Loss -0.8591812252998352, Transition Loss -6.955831527709961, Classifier Loss 0.0588361918926239, Total Loss 5.882228374481201\n",
      "57: Encoding Loss -1.5140841007232666, Transition Loss -16.33916473388672, Classifier Loss 0.04845914617180824, Total Loss 4.842647075653076\n",
      "57: Encoding Loss -1.2520948648452759, Transition Loss -10.51663875579834, Classifier Loss 0.08804754912853241, Total Loss 8.802652359008789\n",
      "57: Encoding Loss -0.6471125483512878, Transition Loss -8.775625228881836, Classifier Loss 0.04170724004507065, Total Loss 4.168968677520752\n",
      "57: Encoding Loss -1.206405758857727, Transition Loss -14.210012435913086, Classifier Loss 0.042664289474487305, Total Loss 4.26358699798584\n",
      "57: Encoding Loss -2.7115275859832764, Transition Loss -22.107027053833008, Classifier Loss 0.04059229791164398, Total Loss 4.054808616638184\n",
      "57: Encoding Loss -0.8394760489463806, Transition Loss -7.903763294219971, Classifier Loss 0.06798047572374344, Total Loss 6.796466827392578\n",
      "57: Encoding Loss -1.4377731084823608, Transition Loss -8.786937713623047, Classifier Loss 0.05994690954685211, Total Loss 5.99293327331543\n",
      "57: Encoding Loss -0.49205276370048523, Transition Loss -10.542034149169922, Classifier Loss 0.04766073450446129, Total Loss 4.763963222503662\n",
      "57: Encoding Loss -1.2626186609268188, Transition Loss -5.208535671234131, Classifier Loss 0.059844180941581726, Total Loss 5.9833760261535645\n",
      "57: Encoding Loss -1.4400177001953125, Transition Loss -13.877456665039062, Classifier Loss 0.05848611891269684, Total Loss 5.845836162567139\n",
      "57: Encoding Loss -1.2561310529708862, Transition Loss -16.720108032226562, Classifier Loss 0.056844115257263184, Total Loss 5.68106746673584\n",
      "57: Encoding Loss -1.3155697584152222, Transition Loss -13.818742752075195, Classifier Loss 0.04542335122823715, Total Loss 4.539571285247803\n",
      "57: Encoding Loss -0.6679505109786987, Transition Loss -5.50434684753418, Classifier Loss 0.026192355901002884, Total Loss 2.6181347370147705\n",
      "57: Encoding Loss -1.6084442138671875, Transition Loss -8.070426940917969, Classifier Loss 0.08289477229118347, Total Loss 8.287863731384277\n",
      "57: Encoding Loss -2.0973405838012695, Transition Loss -14.828062057495117, Classifier Loss 0.04824519902467728, Total Loss 4.821554660797119\n",
      "57: Encoding Loss -0.8551681041717529, Transition Loss -0.9230617880821228, Classifier Loss 0.09205381572246552, Total Loss 9.205196380615234\n",
      "57: Encoding Loss -0.8359666466712952, Transition Loss -9.093107223510742, Classifier Loss 0.06399086862802505, Total Loss 6.397268295288086\n",
      "57: Encoding Loss -0.5333772301673889, Transition Loss -5.506991863250732, Classifier Loss 0.05690649151802063, Total Loss 5.689547061920166\n",
      "57: Encoding Loss -0.8665928244590759, Transition Loss -12.32463264465332, Classifier Loss 0.039671994745731354, Total Loss 3.9647345542907715\n",
      "57: Encoding Loss -1.561505913734436, Transition Loss -16.072172164916992, Classifier Loss 0.06013122573494911, Total Loss 6.009908199310303\n",
      "57: Encoding Loss -1.0188931226730347, Transition Loss -5.784914970397949, Classifier Loss 0.06040459871292114, Total Loss 6.039302825927734\n",
      "57: Encoding Loss -1.1459437608718872, Transition Loss -9.813566207885742, Classifier Loss 0.04075554013252258, Total Loss 4.073591232299805\n",
      "57: Encoding Loss -1.3193386793136597, Transition Loss -9.532341957092285, Classifier Loss 0.03978201001882553, Total Loss 3.97629451751709\n",
      "57: Encoding Loss -0.9564388990402222, Transition Loss -14.590165138244629, Classifier Loss 0.04177825525403023, Total Loss 4.174907207489014\n",
      "57: Encoding Loss -1.10236656665802, Transition Loss -14.29759407043457, Classifier Loss 0.05741303414106369, Total Loss 5.738443851470947\n",
      "57: Encoding Loss -1.0378423929214478, Transition Loss -2.6702046394348145, Classifier Loss 0.03246654197573662, Total Loss 3.2461202144622803\n",
      "57: Encoding Loss -1.4897643327713013, Transition Loss -17.95987319946289, Classifier Loss 0.0305573008954525, Total Loss 3.052138090133667\n",
      "57: Encoding Loss -1.6715806722640991, Transition Loss -17.32207679748535, Classifier Loss 0.026251716539263725, Total Loss 2.6217072010040283\n",
      "57: Encoding Loss -0.7496511936187744, Transition Loss -2.085191249847412, Classifier Loss 0.04264349117875099, Total Loss 4.263931751251221\n",
      "57: Encoding Loss -0.5224376320838928, Transition Loss 0.6025046110153198, Classifier Loss 0.04804633557796478, Total Loss 4.925134181976318\n",
      "57: Encoding Loss -0.8226772546768188, Transition Loss -12.323647499084473, Classifier Loss 0.028202805668115616, Total Loss 2.8178157806396484\n",
      "57: Encoding Loss -0.9617956280708313, Transition Loss -11.532734870910645, Classifier Loss 0.042169176042079926, Total Loss 4.214611053466797\n",
      "57: Encoding Loss -1.8986074924468994, Transition Loss -13.570042610168457, Classifier Loss 0.0440085344016552, Total Loss 4.398139476776123\n",
      "57: Encoding Loss -1.1827600002288818, Transition Loss -7.334449291229248, Classifier Loss 0.04916287213563919, Total Loss 4.914820671081543\n",
      "57: Encoding Loss -2.2918808460235596, Transition Loss -15.615880966186523, Classifier Loss 0.032290019094944, Total Loss 3.2258787155151367\n",
      "57: Encoding Loss -1.1167181730270386, Transition Loss -9.746005058288574, Classifier Loss 0.057744648307561874, Total Loss 5.772515296936035\n",
      "57: Encoding Loss -1.3516291379928589, Transition Loss -13.103897094726562, Classifier Loss 0.07652278244495392, Total Loss 7.649657726287842\n",
      "57: Encoding Loss -2.2482950687408447, Transition Loss -20.18259620666504, Classifier Loss 0.04184701666235924, Total Loss 4.180665016174316\n",
      "57: Encoding Loss -1.808144450187683, Transition Loss -9.430519104003906, Classifier Loss 0.053075097501277924, Total Loss 5.305624008178711\n",
      "57: Encoding Loss -1.4026120901107788, Transition Loss -10.739876747131348, Classifier Loss 0.07542557269334793, Total Loss 7.540409088134766\n",
      "57: Encoding Loss -1.7090355157852173, Transition Loss -16.425357818603516, Classifier Loss 0.062272943556308746, Total Loss 6.2240095138549805\n",
      "57: Encoding Loss -1.6940574645996094, Transition Loss -13.228198051452637, Classifier Loss 0.022714054211974144, Total Loss 2.2687597274780273\n",
      "57: Encoding Loss -0.6664247512817383, Transition Loss -12.027719497680664, Classifier Loss 0.04794593155384064, Total Loss 4.792187690734863\n",
      "57: Encoding Loss -1.3447970151901245, Transition Loss -5.781058311462402, Classifier Loss 0.03504875674843788, Total Loss 3.5037193298339844\n",
      "57: Encoding Loss -0.8120323419570923, Transition Loss -9.33405876159668, Classifier Loss 0.049535080790519714, Total Loss 4.951641082763672\n",
      "57: Encoding Loss -1.4344558715820312, Transition Loss -10.007511138916016, Classifier Loss 0.04461851343512535, Total Loss 4.459849834442139\n",
      "57: Encoding Loss -0.861122190952301, Transition Loss -5.712669372558594, Classifier Loss 0.02607426978647709, Total Loss 2.6062843799591064\n",
      "57: Encoding Loss -0.9328269958496094, Transition Loss -10.829154968261719, Classifier Loss 0.02303752675652504, Total Loss 2.301586866378784\n",
      "57: Encoding Loss -1.1310216188430786, Transition Loss -13.0829439163208, Classifier Loss 0.06404290348291397, Total Loss 6.401673793792725\n",
      "57: Encoding Loss -0.2894292175769806, Transition Loss -12.763463020324707, Classifier Loss 0.04396270215511322, Total Loss 4.389318466186523\n",
      "57: Encoding Loss -0.9265426397323608, Transition Loss -1.4967288970947266, Classifier Loss 0.0680617243051529, Total Loss 6.805872917175293\n",
      "57: Encoding Loss -0.9581853747367859, Transition Loss -9.390893936157227, Classifier Loss 0.028635486960411072, Total Loss 2.86167049407959\n",
      "57: Encoding Loss -0.4557606875896454, Transition Loss -11.734256744384766, Classifier Loss 0.06297840178012848, Total Loss 6.295483589172363\n",
      "57: Encoding Loss -0.3338742256164551, Transition Loss -10.366068840026855, Classifier Loss 0.038278158754110336, Total Loss 3.8246185779571533\n",
      "57: Encoding Loss -0.7952865958213806, Transition Loss -9.986953735351562, Classifier Loss 0.043157000094652176, Total Loss 4.313702583312988\n",
      "57: Encoding Loss -1.791137456893921, Transition Loss -6.030881881713867, Classifier Loss 0.06184862554073334, Total Loss 6.183656215667725\n",
      "57: Encoding Loss -1.8281261920928955, Transition Loss -14.925386428833008, Classifier Loss 0.028447387740015984, Total Loss 2.8417537212371826\n",
      "57: Encoding Loss -0.06953636556863785, Transition Loss -2.434640884399414, Classifier Loss 0.040804632008075714, Total Loss 3.9445676803588867\n",
      "57: Encoding Loss -0.6655457615852356, Transition Loss -8.591835975646973, Classifier Loss 0.024742985144257545, Total Loss 2.4725801944732666\n",
      "57: Encoding Loss -0.11471860110759735, Transition Loss -8.053826332092285, Classifier Loss 0.06960931420326233, Total Loss 6.844003200531006\n",
      "57: Encoding Loss 0.052067793905735016, Transition Loss -16.245607376098633, Classifier Loss 0.1377730518579483, Total Loss 14.065096855163574\n",
      "57: Encoding Loss -1.464625597000122, Transition Loss -16.77952766418457, Classifier Loss 0.07030189037322998, Total Loss 7.0268330574035645\n",
      "57: Encoding Loss -1.469827651977539, Transition Loss -22.365543365478516, Classifier Loss 0.037837423384189606, Total Loss 3.779269218444824\n",
      "57: Encoding Loss -0.9394610524177551, Transition Loss -12.070959091186523, Classifier Loss 0.08869357407093048, Total Loss 8.866943359375\n",
      "57: Encoding Loss -1.221777319908142, Transition Loss -13.702394485473633, Classifier Loss 0.04059955105185509, Total Loss 4.057214736938477\n",
      "57: Encoding Loss -1.863115668296814, Transition Loss -19.921628952026367, Classifier Loss 0.04268644005060196, Total Loss 4.264659404754639\n",
      "57: Encoding Loss -1.4783072471618652, Transition Loss -7.60026741027832, Classifier Loss 0.067044198513031, Total Loss 6.702899932861328\n",
      "57: Encoding Loss -0.41405466198921204, Transition Loss -16.144792556762695, Classifier Loss 0.06850949674844742, Total Loss 6.847663402557373\n",
      "57: Encoding Loss -0.5568760633468628, Transition Loss -17.152851104736328, Classifier Loss 0.018906401470303535, Total Loss 1.8872095346450806\n",
      "57: Encoding Loss -1.1305294036865234, Transition Loss -20.28221893310547, Classifier Loss 0.04948466643691063, Total Loss 4.94441032409668\n",
      "57: Encoding Loss -0.9170035123825073, Transition Loss -26.152658462524414, Classifier Loss 0.010963305830955505, Total Loss 1.0911000967025757\n",
      "58: Encoding Loss -0.3755667805671692, Transition Loss -5.402046203613281, Classifier Loss 0.026943327859044075, Total Loss 2.692992687225342\n",
      "58: Encoding Loss -1.7417545318603516, Transition Loss -6.474751949310303, Classifier Loss 0.07608621567487717, Total Loss 7.607326507568359\n",
      "58: Encoding Loss -1.2096292972564697, Transition Loss -1.1349868774414062, Classifier Loss 0.030056651681661606, Total Loss 3.0054380893707275\n",
      "58: Encoding Loss -1.0790506601333618, Transition Loss -19.28133773803711, Classifier Loss 0.047991808503866196, Total Loss 4.795324802398682\n",
      "58: Encoding Loss -0.9553582072257996, Transition Loss -9.630870819091797, Classifier Loss 0.03843345865607262, Total Loss 3.8414196968078613\n",
      "58: Encoding Loss -0.8469218611717224, Transition Loss -15.748273849487305, Classifier Loss 0.03844408690929413, Total Loss 3.841259002685547\n",
      "58: Encoding Loss -1.759738564491272, Transition Loss -11.52005672454834, Classifier Loss 0.0430314801633358, Total Loss 4.300843715667725\n",
      "58: Encoding Loss -0.27659574151039124, Transition Loss -8.889190673828125, Classifier Loss 0.040873702615499496, Total Loss 4.079313278198242\n",
      "58: Encoding Loss -1.0451403856277466, Transition Loss -15.836493492126465, Classifier Loss 0.027933716773986816, Total Loss 2.7902042865753174\n",
      "58: Encoding Loss -1.7435561418533325, Transition Loss -13.182207107543945, Classifier Loss 0.04584397003054619, Total Loss 4.581760406494141\n",
      "58: Encoding Loss -0.029298899695277214, Transition Loss -9.332258224487305, Classifier Loss 0.03932507708668709, Total Loss 3.8404557704925537\n",
      "58: Encoding Loss -0.4683433771133423, Transition Loss -5.26724100112915, Classifier Loss 0.05015414208173752, Total Loss 5.014355659484863\n",
      "58: Encoding Loss -1.347472906112671, Transition Loss -7.21561861038208, Classifier Loss 0.036232560873031616, Total Loss 3.6218130588531494\n",
      "58: Encoding Loss -0.9183471202850342, Transition Loss -8.30752182006836, Classifier Loss 0.0437956377863884, Total Loss 4.377902507781982\n",
      "58: Encoding Loss -1.4754836559295654, Transition Loss -15.069181442260742, Classifier Loss 0.027606496587395668, Total Loss 2.7576358318328857\n",
      "58: Encoding Loss -2.043349504470825, Transition Loss -19.05199432373047, Classifier Loss 0.057432521134614944, Total Loss 5.739441871643066\n",
      "58: Encoding Loss -1.1199612617492676, Transition Loss -11.380290031433105, Classifier Loss 0.01614031009376049, Total Loss 1.6117548942565918\n",
      "58: Encoding Loss -0.7804762125015259, Transition Loss -7.571343421936035, Classifier Loss 0.02827008068561554, Total Loss 2.825493812561035\n",
      "58: Encoding Loss -0.7179795503616333, Transition Loss -14.626971244812012, Classifier Loss 0.0664837509393692, Total Loss 6.645449638366699\n",
      "58: Encoding Loss -0.2634311318397522, Transition Loss -6.788774490356445, Classifier Loss 0.04093378782272339, Total Loss 4.083137512207031\n",
      "58: Encoding Loss -1.3062294721603394, Transition Loss -16.491121292114258, Classifier Loss 0.06447004526853561, Total Loss 6.443706035614014\n",
      "58: Encoding Loss -1.1397477388381958, Transition Loss -12.555665016174316, Classifier Loss 0.02893180213868618, Total Loss 2.890669107437134\n",
      "58: Encoding Loss -1.1445434093475342, Transition Loss -12.945717811584473, Classifier Loss 0.04712258279323578, Total Loss 4.70966911315918\n",
      "58: Encoding Loss -0.8309282660484314, Transition Loss -4.870802402496338, Classifier Loss 0.06000708043575287, Total Loss 5.999733924865723\n",
      "58: Encoding Loss -1.6227878332138062, Transition Loss -13.040044784545898, Classifier Loss 0.026945410296320915, Total Loss 2.6919329166412354\n",
      "58: Encoding Loss -0.3420032262802124, Transition Loss -6.646860122680664, Classifier Loss 0.053848765790462494, Total Loss 5.382690906524658\n",
      "58: Encoding Loss -2.1675240993499756, Transition Loss -4.94404411315918, Classifier Loss 0.0729372501373291, Total Loss 7.292736053466797\n",
      "58: Encoding Loss -1.9454853534698486, Transition Loss -19.88242530822754, Classifier Loss 0.05277018994092941, Total Loss 5.273042678833008\n",
      "58: Encoding Loss -1.4366501569747925, Transition Loss -10.384121894836426, Classifier Loss 0.04081808030605316, Total Loss 4.079731464385986\n",
      "58: Encoding Loss -0.4448140561580658, Transition Loss -6.5778350830078125, Classifier Loss 0.04653952643275261, Total Loss 4.652621746063232\n",
      "58: Encoding Loss -0.8353327512741089, Transition Loss -12.954401969909668, Classifier Loss 0.05074761062860489, Total Loss 5.072170257568359\n",
      "58: Encoding Loss -1.3254389762878418, Transition Loss -4.286157608032227, Classifier Loss 0.06151505932211876, Total Loss 6.150648593902588\n",
      "58: Encoding Loss -0.5953662395477295, Transition Loss -12.322966575622559, Classifier Loss 0.04907224699854851, Total Loss 4.904759883880615\n",
      "58: Encoding Loss -0.1299182027578354, Transition Loss -6.674822807312012, Classifier Loss 0.1433139443397522, Total Loss 14.229304313659668\n",
      "58: Encoding Loss -1.1635345220565796, Transition Loss -13.87657356262207, Classifier Loss 0.031914349645376205, Total Loss 3.188659429550171\n",
      "58: Encoding Loss -0.11219903826713562, Transition Loss -8.60255241394043, Classifier Loss 0.03729252144694328, Total Loss 3.6100070476531982\n",
      "58: Encoding Loss 0.23984533548355103, Transition Loss -7.12196159362793, Classifier Loss 0.05130963772535324, Total Loss 7.032506465911865\n",
      "58: Encoding Loss 0.648788332939148, Transition Loss -11.569202423095703, Classifier Loss 0.09783542156219482, Total Loss 14.971536636352539\n",
      "58: Encoding Loss -1.0315334796905518, Transition Loss -11.387580871582031, Classifier Loss 0.04497823864221573, Total Loss 4.495546340942383\n",
      "58: Encoding Loss -0.21270735561847687, Transition Loss -12.812695503234863, Classifier Loss 0.04614534601569176, Total Loss 4.583542346954346\n",
      "58: Encoding Loss -0.05052565038204193, Transition Loss -8.763545989990234, Classifier Loss 0.023950621485710144, Total Loss 2.2693440914154053\n",
      "58: Encoding Loss -0.9690148830413818, Transition Loss -17.36270523071289, Classifier Loss 0.05577162653207779, Total Loss 5.573690414428711\n",
      "58: Encoding Loss -1.2918049097061157, Transition Loss -16.646060943603516, Classifier Loss 0.024242231622338295, Total Loss 2.420893907546997\n",
      "58: Encoding Loss -1.949658751487732, Transition Loss -7.007528781890869, Classifier Loss 0.04559563845396042, Total Loss 4.558162212371826\n",
      "58: Encoding Loss -1.0448558330535889, Transition Loss -17.87925910949707, Classifier Loss 0.03438016027212143, Total Loss 3.4344401359558105\n",
      "58: Encoding Loss 0.05437372997403145, Transition Loss -8.433453559875488, Classifier Loss 0.05497877299785614, Total Loss 5.803593158721924\n",
      "58: Encoding Loss -0.4440291225910187, Transition Loss -19.209684371948242, Classifier Loss 0.04582194238901138, Total Loss 4.578336238861084\n",
      "58: Encoding Loss -1.6488986015319824, Transition Loss -7.502298355102539, Classifier Loss 0.022977784276008606, Total Loss 2.2962779998779297\n",
      "58: Encoding Loss -2.6056478023529053, Transition Loss -10.867477416992188, Classifier Loss 0.037521980702877045, Total Loss 3.7500245571136475\n",
      "58: Encoding Loss -1.7897847890853882, Transition Loss -17.241300582885742, Classifier Loss 0.03548785671591759, Total Loss 3.545337438583374\n",
      "58: Encoding Loss -2.3035709857940674, Transition Loss -20.217458724975586, Classifier Loss 0.04407323896884918, Total Loss 4.403280258178711\n",
      "58: Encoding Loss -0.7705562710762024, Transition Loss -13.718781471252441, Classifier Loss 0.06233673170208931, Total Loss 6.230929374694824\n",
      "58: Encoding Loss -1.659793734550476, Transition Loss -4.084534168243408, Classifier Loss 0.07988420873880386, Total Loss 7.987604141235352\n",
      "58: Encoding Loss -2.117781639099121, Transition Loss -13.113779067993164, Classifier Loss 0.029790688306093216, Total Loss 2.9764459133148193\n",
      "58: Encoding Loss -1.3597438335418701, Transition Loss -13.051737785339355, Classifier Loss 0.062116023153066635, Total Loss 6.208992004394531\n",
      "58: Encoding Loss -1.8686637878417969, Transition Loss -6.594362258911133, Classifier Loss 0.07160405814647675, Total Loss 7.15908670425415\n",
      "58: Encoding Loss -1.160173773765564, Transition Loss -12.024358749389648, Classifier Loss 0.02282123453915119, Total Loss 2.2797186374664307\n",
      "58: Encoding Loss -2.569202423095703, Transition Loss -13.927725791931152, Classifier Loss 0.039305657148361206, Total Loss 3.9277803897857666\n",
      "58: Encoding Loss -2.7121331691741943, Transition Loss -20.022640228271484, Classifier Loss 0.027310281991958618, Total Loss 2.7270236015319824\n",
      "58: Encoding Loss -1.987324833869934, Transition Loss -17.270891189575195, Classifier Loss 0.049609776586294174, Total Loss 4.957523345947266\n",
      "58: Encoding Loss -2.4121153354644775, Transition Loss -10.229421615600586, Classifier Loss 0.04735715314745903, Total Loss 4.733669281005859\n",
      "58: Encoding Loss -2.5639026165008545, Transition Loss -9.121383666992188, Classifier Loss 0.07856632769107819, Total Loss 7.854808330535889\n",
      "58: Encoding Loss -2.1603143215179443, Transition Loss -10.717909812927246, Classifier Loss 0.03763394057750702, Total Loss 3.7612504959106445\n",
      "58: Encoding Loss -1.3963217735290527, Transition Loss -8.257360458374023, Classifier Loss 0.05803186446428299, Total Loss 5.801535129547119\n",
      "58: Encoding Loss -1.9052708148956299, Transition Loss -12.225231170654297, Classifier Loss 0.052402716130018234, Total Loss 5.237826347351074\n",
      "58: Encoding Loss -2.2555816173553467, Transition Loss -5.832207679748535, Classifier Loss 0.081766277551651, Total Loss 8.175461769104004\n",
      "58: Encoding Loss -0.8346187472343445, Transition Loss -2.7780826091766357, Classifier Loss 0.04387489706277847, Total Loss 4.386934280395508\n",
      "58: Encoding Loss -2.4668543338775635, Transition Loss -19.287158966064453, Classifier Loss 0.06414363533258438, Total Loss 6.410505771636963\n",
      "58: Encoding Loss -0.44484612345695496, Transition Loss -10.286208152770996, Classifier Loss 0.057101331651210785, Total Loss 5.7080607414245605\n",
      "58: Encoding Loss -1.4683643579483032, Transition Loss -9.869787216186523, Classifier Loss 0.09063149243593216, Total Loss 9.061175346374512\n",
      "58: Encoding Loss -2.2308194637298584, Transition Loss -9.73558521270752, Classifier Loss 0.055092424154281616, Total Loss 5.507295608520508\n",
      "58: Encoding Loss -1.7149198055267334, Transition Loss -9.783414840698242, Classifier Loss 0.0620337575674057, Total Loss 6.201419353485107\n",
      "58: Encoding Loss -1.3832441568374634, Transition Loss -6.704373836517334, Classifier Loss 0.07061300426721573, Total Loss 7.059959411621094\n",
      "58: Encoding Loss -1.818596363067627, Transition Loss -10.087952613830566, Classifier Loss 0.015537728555500507, Total Loss 1.5517551898956299\n",
      "58: Encoding Loss -2.2560975551605225, Transition Loss -9.161803245544434, Classifier Loss 0.07426087558269501, Total Loss 7.424254894256592\n",
      "58: Encoding Loss -1.836699366569519, Transition Loss -15.427162170410156, Classifier Loss 0.026850927621126175, Total Loss 2.682007312774658\n",
      "58: Encoding Loss -1.443840742111206, Transition Loss -7.668283462524414, Classifier Loss 0.05627601593732834, Total Loss 5.626068115234375\n",
      "58: Encoding Loss -2.8753914833068848, Transition Loss -13.37476921081543, Classifier Loss 0.04541429132223129, Total Loss 4.538753986358643\n",
      "58: Encoding Loss -2.1475460529327393, Transition Loss -14.482160568237305, Classifier Loss 0.01903771236538887, Total Loss 1.9008748531341553\n",
      "58: Encoding Loss -1.8533416986465454, Transition Loss -11.127033233642578, Classifier Loss 0.07719022780656815, Total Loss 7.716797351837158\n",
      "58: Encoding Loss -1.1451334953308105, Transition Loss -12.253870010375977, Classifier Loss 0.03692292794585228, Total Loss 3.6898419857025146\n",
      "58: Encoding Loss -2.0544207096099854, Transition Loss -10.129829406738281, Classifier Loss 0.03240649402141571, Total Loss 3.2386233806610107\n",
      "58: Encoding Loss -1.6689976453781128, Transition Loss -6.641034126281738, Classifier Loss 0.07761272042989731, Total Loss 7.759943962097168\n",
      "58: Encoding Loss -0.9905514121055603, Transition Loss -12.177570343017578, Classifier Loss 0.06198091432452202, Total Loss 6.195655822753906\n",
      "58: Encoding Loss -2.701364278793335, Transition Loss -8.816447257995605, Classifier Loss 0.03962148353457451, Total Loss 3.9603850841522217\n",
      "58: Encoding Loss -1.600764513015747, Transition Loss -6.998420715332031, Classifier Loss 0.058101966977119446, Total Loss 5.808797359466553\n",
      "58: Encoding Loss -2.215952157974243, Transition Loss -16.552526473999023, Classifier Loss 0.045084018260240555, Total Loss 4.505091190338135\n",
      "58: Encoding Loss -1.858613133430481, Transition Loss -10.688316345214844, Classifier Loss 0.08891956508159637, Total Loss 8.88981819152832\n",
      "58: Encoding Loss -1.3614156246185303, Transition Loss -8.871010780334473, Classifier Loss 0.04179397597908974, Total Loss 4.177623271942139\n",
      "58: Encoding Loss -1.641088843345642, Transition Loss -14.483524322509766, Classifier Loss 0.0420503243803978, Total Loss 4.2021355628967285\n",
      "58: Encoding Loss -3.70732045173645, Transition Loss -22.42962074279785, Classifier Loss 0.037347473204135895, Total Loss 3.7302615642547607\n",
      "58: Encoding Loss -1.2901707887649536, Transition Loss -7.935344219207764, Classifier Loss 0.06568929553031921, Total Loss 6.567342758178711\n",
      "58: Encoding Loss -1.8592854738235474, Transition Loss -8.841114044189453, Classifier Loss 0.05903920531272888, Total Loss 5.9021525382995605\n",
      "58: Encoding Loss -1.1967765092849731, Transition Loss -10.619781494140625, Classifier Loss 0.045645009726285934, Total Loss 4.562376976013184\n",
      "58: Encoding Loss -1.8197619915008545, Transition Loss -5.207541465759277, Classifier Loss 0.058718714863061905, Total Loss 5.870830059051514\n",
      "58: Encoding Loss -2.1173150539398193, Transition Loss -14.063987731933594, Classifier Loss 0.05731893330812454, Total Loss 5.729080677032471\n",
      "58: Encoding Loss -1.9817718267440796, Transition Loss -16.966047286987305, Classifier Loss 0.057616911828517914, Total Loss 5.758297920227051\n",
      "58: Encoding Loss -2.1454293727874756, Transition Loss -13.971656799316406, Classifier Loss 0.04349580407142639, Total Loss 4.346786022186279\n",
      "58: Encoding Loss -1.2019107341766357, Transition Loss -5.594648838043213, Classifier Loss 0.026997718960046768, Total Loss 2.698652982711792\n",
      "58: Encoding Loss -2.2003493309020996, Transition Loss -8.183302879333496, Classifier Loss 0.08382122963666916, Total Loss 8.380486488342285\n",
      "58: Encoding Loss -2.687798261642456, Transition Loss -15.082300186157227, Classifier Loss 0.0485646054148674, Total Loss 4.8534440994262695\n",
      "58: Encoding Loss -1.2530995607376099, Transition Loss -0.9128703474998474, Classifier Loss 0.09089584648609161, Total Loss 9.089402198791504\n",
      "58: Encoding Loss -1.372206449508667, Transition Loss -9.18722152709961, Classifier Loss 0.06349262595176697, Total Loss 6.34742546081543\n",
      "58: Encoding Loss -1.0808758735656738, Transition Loss -5.633214950561523, Classifier Loss 0.0566420778632164, Total Loss 5.663081169128418\n",
      "58: Encoding Loss -1.620833158493042, Transition Loss -12.489803314208984, Classifier Loss 0.03900487348437309, Total Loss 3.897989511489868\n",
      "58: Encoding Loss -2.4898993968963623, Transition Loss -16.34404754638672, Classifier Loss 0.05974233150482178, Total Loss 5.970964431762695\n",
      "58: Encoding Loss -1.6326967477798462, Transition Loss -5.889738082885742, Classifier Loss 0.06156284734606743, Total Loss 6.155107021331787\n",
      "58: Encoding Loss -1.4238388538360596, Transition Loss -10.052791595458984, Classifier Loss 0.04049942269921303, Total Loss 4.047932147979736\n",
      "58: Encoding Loss -1.8545583486557007, Transition Loss -9.698515892028809, Classifier Loss 0.04040701314806938, Total Loss 4.038761615753174\n",
      "58: Encoding Loss -1.3914074897766113, Transition Loss -14.780920028686523, Classifier Loss 0.040135521441698074, Total Loss 4.010595798492432\n",
      "58: Encoding Loss -1.7920655012130737, Transition Loss -14.448980331420898, Classifier Loss 0.054890938103199005, Total Loss 5.486204147338867\n",
      "58: Encoding Loss -1.513490080833435, Transition Loss -2.788653612136841, Classifier Loss 0.03150729835033417, Total Loss 3.150172233581543\n",
      "58: Encoding Loss -2.0090889930725098, Transition Loss -18.269359588623047, Classifier Loss 0.02934492938220501, Total Loss 2.9308390617370605\n",
      "58: Encoding Loss -2.144634962081909, Transition Loss -17.711227416992188, Classifier Loss 0.02582314983010292, Total Loss 2.578772783279419\n",
      "58: Encoding Loss -1.2166728973388672, Transition Loss -2.1019163131713867, Classifier Loss 0.042125970125198364, Total Loss 4.212176322937012\n",
      "58: Encoding Loss -0.8184831142425537, Transition Loss 0.5975621938705444, Classifier Loss 0.04700540378689766, Total Loss 4.8200531005859375\n",
      "58: Encoding Loss -1.2358993291854858, Transition Loss -12.49312973022461, Classifier Loss 0.027478253468871117, Total Loss 2.7453267574310303\n",
      "58: Encoding Loss -1.6066899299621582, Transition Loss -11.697500228881836, Classifier Loss 0.04132590442895889, Total Loss 4.130250930786133\n",
      "58: Encoding Loss -2.660644769668579, Transition Loss -13.873193740844727, Classifier Loss 0.04412461444735527, Total Loss 4.40968656539917\n",
      "58: Encoding Loss -1.7766326665878296, Transition Loss -7.427370548248291, Classifier Loss 0.04823615029454231, Total Loss 4.822129726409912\n",
      "58: Encoding Loss -3.0500895977020264, Transition Loss -15.915542602539062, Classifier Loss 0.03191936016082764, Total Loss 3.1887528896331787\n",
      "58: Encoding Loss -1.4161759614944458, Transition Loss -9.829543113708496, Classifier Loss 0.0562761016190052, Total Loss 5.625644207000732\n",
      "58: Encoding Loss -1.9260594844818115, Transition Loss -13.360766410827637, Classifier Loss 0.0727747455239296, Total Loss 7.274802207946777\n",
      "58: Encoding Loss -3.0645909309387207, Transition Loss -20.418262481689453, Classifier Loss 0.041884567588567734, Total Loss 4.184372901916504\n",
      "58: Encoding Loss -2.3898770809173584, Transition Loss -9.67326545715332, Classifier Loss 0.05211333557963371, Total Loss 5.209399223327637\n",
      "58: Encoding Loss -2.232978582382202, Transition Loss -10.91137981414795, Classifier Loss 0.07380236685276031, Total Loss 7.378054141998291\n",
      "58: Encoding Loss -2.398247480392456, Transition Loss -16.591381072998047, Classifier Loss 0.06687695533037186, Total Loss 6.684377193450928\n",
      "58: Encoding Loss -2.200380802154541, Transition Loss -13.498929023742676, Classifier Loss 0.02303393743932247, Total Loss 2.300693988800049\n",
      "58: Encoding Loss -1.1995633840560913, Transition Loss -12.19126033782959, Classifier Loss 0.044289130717515945, Total Loss 4.4264750480651855\n",
      "58: Encoding Loss -1.8479801416397095, Transition Loss -5.927786827087402, Classifier Loss 0.03488742560148239, Total Loss 3.4875569343566895\n",
      "58: Encoding Loss -1.1882046461105347, Transition Loss -9.561476707458496, Classifier Loss 0.04971512407064438, Total Loss 4.969600200653076\n",
      "58: Encoding Loss -1.9978715181350708, Transition Loss -10.189220428466797, Classifier Loss 0.0438147708773613, Total Loss 4.379438877105713\n",
      "58: Encoding Loss -1.211577296257019, Transition Loss -5.8854756355285645, Classifier Loss 0.025418167933821678, Total Loss 2.540639638900757\n",
      "58: Encoding Loss -1.4554541110992432, Transition Loss -11.004923820495605, Classifier Loss 0.023087939247488976, Total Loss 2.3065929412841797\n",
      "58: Encoding Loss -1.6294827461242676, Transition Loss -13.331748962402344, Classifier Loss 0.06204117834568024, Total Loss 6.201451301574707\n",
      "58: Encoding Loss -0.5337530374526978, Transition Loss -13.013279914855957, Classifier Loss 0.04388939589262009, Total Loss 4.386336326599121\n",
      "58: Encoding Loss -1.3190776109695435, Transition Loss -1.6684956550598145, Classifier Loss 0.06374669075012207, Total Loss 6.374335289001465\n",
      "58: Encoding Loss -1.3376457691192627, Transition Loss -9.676335334777832, Classifier Loss 0.028985006734728813, Total Loss 2.8965654373168945\n",
      "58: Encoding Loss -0.902055561542511, Transition Loss -11.993934631347656, Classifier Loss 0.0625651404261589, Total Loss 6.254115104675293\n",
      "58: Encoding Loss -0.7086948156356812, Transition Loss -10.641010284423828, Classifier Loss 0.037819746881723404, Total Loss 3.779846668243408\n",
      "58: Encoding Loss -1.2519432306289673, Transition Loss -10.278573036193848, Classifier Loss 0.043990228325128555, Total Loss 4.39696741104126\n",
      "58: Encoding Loss -2.48701548576355, Transition Loss -6.283544063568115, Classifier Loss 0.06080560013651848, Total Loss 6.07930326461792\n",
      "58: Encoding Loss -2.534722328186035, Transition Loss -15.278188705444336, Classifier Loss 0.028396401554346085, Total Loss 2.8365845680236816\n",
      "58: Encoding Loss -0.4357999563217163, Transition Loss -2.5246572494506836, Classifier Loss 0.040226854383945465, Total Loss 4.022157669067383\n",
      "58: Encoding Loss -1.1472071409225464, Transition Loss -8.836350440979004, Classifier Loss 0.024817392230033875, Total Loss 2.4799721240997314\n",
      "58: Encoding Loss -0.3912281095981598, Transition Loss -8.210838317871094, Classifier Loss 0.07149388641119003, Total Loss 7.147603511810303\n",
      "58: Encoding Loss -0.7367042303085327, Transition Loss -16.78453826904297, Classifier Loss 0.12702281773090363, Total Loss 12.698925018310547\n",
      "58: Encoding Loss -2.6312737464904785, Transition Loss -16.73844337463379, Classifier Loss 0.05903606861829758, Total Loss 5.900259017944336\n",
      "58: Encoding Loss -2.839264392852783, Transition Loss -22.194438934326172, Classifier Loss 0.03283413499593735, Total Loss 3.2789745330810547\n",
      "58: Encoding Loss -1.3894634246826172, Transition Loss -12.22409439086914, Classifier Loss 0.08186043053865433, Total Loss 8.183597564697266\n",
      "58: Encoding Loss -2.524125814437866, Transition Loss -13.880461692810059, Classifier Loss 0.03801150619983673, Total Loss 3.7983744144439697\n",
      "58: Encoding Loss -2.934530019760132, Transition Loss -19.87270736694336, Classifier Loss 0.04020509868860245, Total Loss 4.01653528213501\n",
      "58: Encoding Loss -2.1410751342773438, Transition Loss -8.021882057189941, Classifier Loss 0.0690452829003334, Total Loss 6.902923583984375\n",
      "58: Encoding Loss -1.4278002977371216, Transition Loss -16.362342834472656, Classifier Loss 0.06711115688085556, Total Loss 6.70784330368042\n",
      "58: Encoding Loss -1.9655548334121704, Transition Loss -17.3501033782959, Classifier Loss 0.020564600825309753, Total Loss 2.052990198135376\n",
      "58: Encoding Loss -1.9175426959991455, Transition Loss -20.352201461791992, Classifier Loss 0.04969394579529762, Total Loss 4.965324401855469\n",
      "58: Encoding Loss -2.7502129077911377, Transition Loss -26.062454223632812, Classifier Loss 0.010200170800089836, Total Loss 1.014804482460022\n",
      "59: Encoding Loss -1.2714165449142456, Transition Loss -6.1540937423706055, Classifier Loss 0.025527071207761765, Total Loss 2.55147647857666\n",
      "59: Encoding Loss -2.015467405319214, Transition Loss -7.16008186340332, Classifier Loss 0.07478893548250198, Total Loss 7.477461814880371\n",
      "59: Encoding Loss -1.8959107398986816, Transition Loss -2.1551296710968018, Classifier Loss 0.02858269400894642, Total Loss 2.8578383922576904\n",
      "59: Encoding Loss -1.7915211915969849, Transition Loss -19.450698852539062, Classifier Loss 0.04688894376158714, Total Loss 4.685004234313965\n",
      "59: Encoding Loss -1.7618732452392578, Transition Loss -10.230867385864258, Classifier Loss 0.036586612462997437, Total Loss 3.6566152572631836\n",
      "59: Encoding Loss -1.885489821434021, Transition Loss -16.051097869873047, Classifier Loss 0.03814379870891571, Total Loss 3.8111696243286133\n",
      "59: Encoding Loss -2.4918148517608643, Transition Loss -12.071462631225586, Classifier Loss 0.0421353317797184, Total Loss 4.211119174957275\n",
      "59: Encoding Loss -1.468812346458435, Transition Loss -9.422874450683594, Classifier Loss 0.03924822434782982, Total Loss 3.9229378700256348\n",
      "59: Encoding Loss -1.366204857826233, Transition Loss -16.226943969726562, Classifier Loss 0.02738603763282299, Total Loss 2.735358476638794\n",
      "59: Encoding Loss -2.498406171798706, Transition Loss -13.513703346252441, Classifier Loss 0.04193303734064102, Total Loss 4.190600872039795\n",
      "59: Encoding Loss -0.6229800581932068, Transition Loss -9.82597827911377, Classifier Loss 0.043612197041511536, Total Loss 4.359254837036133\n",
      "59: Encoding Loss -0.9656428694725037, Transition Loss -6.193655967712402, Classifier Loss 0.049605775624513626, Total Loss 4.959338665008545\n",
      "59: Encoding Loss -1.9868541955947876, Transition Loss -8.152291297912598, Classifier Loss 0.034541673958301544, Total Loss 3.4525368213653564\n",
      "59: Encoding Loss -1.175886631011963, Transition Loss -9.049772262573242, Classifier Loss 0.044426921755075455, Total Loss 4.440882205963135\n",
      "59: Encoding Loss -2.3046019077301025, Transition Loss -15.759343147277832, Classifier Loss 0.02637545019388199, Total Loss 2.6343932151794434\n",
      "59: Encoding Loss -2.5952658653259277, Transition Loss -19.54692268371582, Classifier Loss 0.058872342109680176, Total Loss 5.88332462310791\n",
      "59: Encoding Loss -1.5634791851043701, Transition Loss -12.194061279296875, Classifier Loss 0.016613511368632317, Total Loss 1.6589123010635376\n",
      "59: Encoding Loss -1.0253403186798096, Transition Loss -8.305032730102539, Classifier Loss 0.02779226005077362, Total Loss 2.7775650024414062\n",
      "59: Encoding Loss -0.79261314868927, Transition Loss -15.152265548706055, Classifier Loss 0.06390371173620224, Total Loss 6.387341022491455\n",
      "59: Encoding Loss -0.9468702673912048, Transition Loss -7.606644153594971, Classifier Loss 0.04104020074009895, Total Loss 4.102499008178711\n",
      "59: Encoding Loss -1.4391461610794067, Transition Loss -16.91520881652832, Classifier Loss 0.06497545540332794, Total Loss 6.494162559509277\n",
      "59: Encoding Loss -1.9550822973251343, Transition Loss -13.258642196655273, Classifier Loss 0.029349002987146378, Total Loss 2.932248592376709\n",
      "59: Encoding Loss -1.5694862604141235, Transition Loss -13.768918991088867, Classifier Loss 0.045194342732429504, Total Loss 4.516680717468262\n",
      "59: Encoding Loss -1.3039588928222656, Transition Loss -5.89663553237915, Classifier Loss 0.06160809099674225, Total Loss 6.159629821777344\n",
      "59: Encoding Loss -2.34661602973938, Transition Loss -13.751893997192383, Classifier Loss 0.0263429656624794, Total Loss 2.6315462589263916\n",
      "59: Encoding Loss -0.8312619924545288, Transition Loss -7.396288871765137, Classifier Loss 0.057666171342134476, Total Loss 5.765138149261475\n",
      "59: Encoding Loss -2.848296642303467, Transition Loss -6.171877861022949, Classifier Loss 0.07297269999980927, Total Loss 7.296035289764404\n",
      "59: Encoding Loss -3.172356128692627, Transition Loss -20.471214294433594, Classifier Loss 0.051780592650175095, Total Loss 5.173964977264404\n",
      "59: Encoding Loss -2.1886610984802246, Transition Loss -11.30552864074707, Classifier Loss 0.03971501439809799, Total Loss 3.969240188598633\n",
      "59: Encoding Loss -1.1423792839050293, Transition Loss -7.597341537475586, Classifier Loss 0.046254225075244904, Total Loss 4.623902797698975\n",
      "59: Encoding Loss -1.3212460279464722, Transition Loss -13.671403884887695, Classifier Loss 0.05066940188407898, Total Loss 5.064206123352051\n",
      "59: Encoding Loss -1.7085360288619995, Transition Loss -5.350069999694824, Classifier Loss 0.06111759692430496, Total Loss 6.110689640045166\n",
      "59: Encoding Loss -1.0301350355148315, Transition Loss -12.885625839233398, Classifier Loss 0.049072206020355225, Total Loss 4.904643535614014\n",
      "59: Encoding Loss -0.7042145133018494, Transition Loss -7.651482582092285, Classifier Loss 0.13692407310009003, Total Loss 13.690876960754395\n",
      "59: Encoding Loss -2.028768301010132, Transition Loss -14.879743576049805, Classifier Loss 0.033142656087875366, Total Loss 3.3112897872924805\n",
      "59: Encoding Loss -0.7668451070785522, Transition Loss -9.877763748168945, Classifier Loss 0.03272164613008499, Total Loss 3.270189046859741\n",
      "59: Encoding Loss -1.28910493850708, Transition Loss -9.01737117767334, Classifier Loss 0.051593221724033356, Total Loss 5.1575188636779785\n",
      "59: Encoding Loss -1.328198790550232, Transition Loss -11.01522445678711, Classifier Loss 0.08386063575744629, Total Loss 8.38386058807373\n",
      "59: Encoding Loss -2.0905003547668457, Transition Loss -12.684062004089355, Classifier Loss 0.04286710545420647, Total Loss 4.284173965454102\n",
      "59: Encoding Loss -0.9707151651382446, Transition Loss -13.812703132629395, Classifier Loss 0.048436716198921204, Total Loss 4.840909481048584\n",
      "59: Encoding Loss -0.9711277484893799, Transition Loss -10.455965995788574, Classifier Loss 0.02423848956823349, Total Loss 2.421757698059082\n",
      "59: Encoding Loss -1.9091511964797974, Transition Loss -18.114234924316406, Classifier Loss 0.05394916236400604, Total Loss 5.391293048858643\n",
      "59: Encoding Loss -2.163566827774048, Transition Loss -17.595457077026367, Classifier Loss 0.02637709304690361, Total Loss 2.634190320968628\n",
      "59: Encoding Loss -2.3873627185821533, Transition Loss -8.125151634216309, Classifier Loss 0.044953204691410065, Total Loss 4.493695259094238\n",
      "59: Encoding Loss -1.73286771774292, Transition Loss -18.62411117553711, Classifier Loss 0.03451008349657059, Total Loss 3.4472835063934326\n",
      "59: Encoding Loss -0.4005224406719208, Transition Loss -9.407515525817871, Classifier Loss 0.053959526121616364, Total Loss 5.393971920013428\n",
      "59: Encoding Loss -1.1649032831192017, Transition Loss -19.420825958251953, Classifier Loss 0.04511293023824692, Total Loss 4.507408618927002\n",
      "59: Encoding Loss -2.0156235694885254, Transition Loss -8.180760383605957, Classifier Loss 0.025226792320609093, Total Loss 2.521043062210083\n",
      "59: Encoding Loss -2.3317999839782715, Transition Loss -11.193115234375, Classifier Loss 0.038773298263549805, Total Loss 3.875091314315796\n",
      "59: Encoding Loss -1.6340545415878296, Transition Loss -17.400110244750977, Classifier Loss 0.03661784529685974, Total Loss 3.6583046913146973\n",
      "59: Encoding Loss -2.2758939266204834, Transition Loss -20.196752548217773, Classifier Loss 0.042479418218135834, Total Loss 4.243902683258057\n",
      "59: Encoding Loss -0.9498777389526367, Transition Loss -13.967863082885742, Classifier Loss 0.061535246670246124, Total Loss 6.150731086730957\n",
      "59: Encoding Loss -1.315372347831726, Transition Loss -4.381176471710205, Classifier Loss 0.07708408683538437, Total Loss 7.7075324058532715\n",
      "59: Encoding Loss -1.4346349239349365, Transition Loss -13.559050559997559, Classifier Loss 0.028584815561771393, Total Loss 2.8557698726654053\n",
      "59: Encoding Loss -1.1164945363998413, Transition Loss -13.31644058227539, Classifier Loss 0.0571737065911293, Total Loss 5.714707374572754\n",
      "59: Encoding Loss -1.9110020399093628, Transition Loss -7.081335067749023, Classifier Loss 0.06953423470258713, Total Loss 6.952007293701172\n",
      "59: Encoding Loss -0.9327427744865417, Transition Loss -12.306015968322754, Classifier Loss 0.024508167058229446, Total Loss 2.4483554363250732\n",
      "59: Encoding Loss -2.140820026397705, Transition Loss -14.220110893249512, Classifier Loss 0.03884958103299141, Total Loss 3.8821139335632324\n",
      "59: Encoding Loss -2.4213757514953613, Transition Loss -20.08596420288086, Classifier Loss 0.027896106243133545, Total Loss 2.7855935096740723\n",
      "59: Encoding Loss -1.9409977197647095, Transition Loss -17.02861213684082, Classifier Loss 0.04798038676381111, Total Loss 4.794632911682129\n",
      "59: Encoding Loss -1.7164610624313354, Transition Loss -10.371337890625, Classifier Loss 0.046821072697639465, Total Loss 4.680033206939697\n",
      "59: Encoding Loss -2.3045010566711426, Transition Loss -9.167073249816895, Classifier Loss 0.07271401584148407, Total Loss 7.269567966461182\n",
      "59: Encoding Loss -2.2211878299713135, Transition Loss -10.872729301452637, Classifier Loss 0.03615185245871544, Total Loss 3.6130106449127197\n",
      "59: Encoding Loss -1.0453846454620361, Transition Loss -8.55751895904541, Classifier Loss 0.05640696734189987, Total Loss 5.6389851570129395\n",
      "59: Encoding Loss -1.3719793558120728, Transition Loss -12.453341484069824, Classifier Loss 0.05194183439016342, Total Loss 5.19169282913208\n",
      "59: Encoding Loss -2.2249176502227783, Transition Loss -6.178255558013916, Classifier Loss 0.07861806452274323, Total Loss 7.860570907592773\n",
      "59: Encoding Loss -0.733950674533844, Transition Loss -3.052377223968506, Classifier Loss 0.04569467529654503, Total Loss 4.568857192993164\n",
      "59: Encoding Loss -1.5945671796798706, Transition Loss -19.155235290527344, Classifier Loss 0.06153903156518936, Total Loss 6.15007209777832\n",
      "59: Encoding Loss -0.6596753597259521, Transition Loss -10.253960609436035, Classifier Loss 0.056959740817546844, Total Loss 5.693922996520996\n",
      "59: Encoding Loss -1.1654030084609985, Transition Loss -9.946274757385254, Classifier Loss 0.08144775778055191, Total Loss 8.142786026000977\n",
      "59: Encoding Loss -1.8759483098983765, Transition Loss -9.849526405334473, Classifier Loss 0.052809938788414, Total Loss 5.279024124145508\n",
      "59: Encoding Loss -1.4476906061172485, Transition Loss -9.93298625946045, Classifier Loss 0.06146492436528206, Total Loss 6.144505977630615\n",
      "59: Encoding Loss -0.9670260548591614, Transition Loss -6.741860866546631, Classifier Loss 0.06810621917247772, Total Loss 6.8092732429504395\n",
      "59: Encoding Loss -1.4985871315002441, Transition Loss -10.094991683959961, Classifier Loss 0.016146216541528702, Total Loss 1.6126025915145874\n",
      "59: Encoding Loss -2.0945518016815186, Transition Loss -9.08517837524414, Classifier Loss 0.07320770621299744, Total Loss 7.318953514099121\n",
      "59: Encoding Loss -1.5643506050109863, Transition Loss -15.286473274230957, Classifier Loss 0.02655925787985325, Total Loss 2.6528685092926025\n",
      "59: Encoding Loss -1.3029199838638306, Transition Loss -7.322052001953125, Classifier Loss 0.05375148355960846, Total Loss 5.373683929443359\n",
      "59: Encoding Loss -2.588632822036743, Transition Loss -13.291791915893555, Classifier Loss 0.04312857612967491, Total Loss 4.31019926071167\n",
      "59: Encoding Loss -1.7481096982955933, Transition Loss -14.08152961730957, Classifier Loss 0.018435169011354446, Total Loss 1.840700626373291\n",
      "59: Encoding Loss -1.6076878309249878, Transition Loss -10.800566673278809, Classifier Loss 0.08011997491121292, Total Loss 8.00983715057373\n",
      "59: Encoding Loss -1.0228829383850098, Transition Loss -12.128190040588379, Classifier Loss 0.03752363473176956, Total Loss 3.7499377727508545\n",
      "59: Encoding Loss -1.7220876216888428, Transition Loss -9.642603874206543, Classifier Loss 0.031723927706480026, Total Loss 3.170464277267456\n",
      "59: Encoding Loss -1.3628193140029907, Transition Loss -6.476566314697266, Classifier Loss 0.07845553010702133, Total Loss 7.844257831573486\n",
      "59: Encoding Loss -0.7301612496376038, Transition Loss -12.2393217086792, Classifier Loss 0.059497326612472534, Total Loss 5.947284698486328\n",
      "59: Encoding Loss -2.3747684955596924, Transition Loss -8.576132774353027, Classifier Loss 0.04015239700675011, Total Loss 4.013524532318115\n",
      "59: Encoding Loss -1.4897534847259521, Transition Loss -6.776306629180908, Classifier Loss 0.058850664645433426, Total Loss 5.883711338043213\n",
      "59: Encoding Loss -2.19098162651062, Transition Loss -16.185462951660156, Classifier Loss 0.0460633784532547, Total Loss 4.603100776672363\n",
      "59: Encoding Loss -1.6151729822158813, Transition Loss -10.208857536315918, Classifier Loss 0.09007560461759567, Total Loss 9.005518913269043\n",
      "59: Encoding Loss -1.1382197141647339, Transition Loss -8.943269729614258, Classifier Loss 0.040295228362083435, Total Loss 4.027734279632568\n",
      "59: Encoding Loss -1.1566295623779297, Transition Loss -14.358672142028809, Classifier Loss 0.04231012612581253, Total Loss 4.2281413078308105\n",
      "59: Encoding Loss -3.499250888824463, Transition Loss -22.006582260131836, Classifier Loss 0.037165090441703796, Total Loss 3.7121078968048096\n",
      "59: Encoding Loss -0.8867080807685852, Transition Loss -7.9060797691345215, Classifier Loss 0.06680938601493835, Total Loss 6.679357528686523\n",
      "59: Encoding Loss -1.2167258262634277, Transition Loss -8.33023738861084, Classifier Loss 0.06005426123738289, Total Loss 6.003759860992432\n",
      "59: Encoding Loss -0.7676594853401184, Transition Loss -10.139993667602539, Classifier Loss 0.04498957470059395, Total Loss 4.49692964553833\n",
      "59: Encoding Loss -1.5428218841552734, Transition Loss -4.838737487792969, Classifier Loss 0.05860963836312294, Total Loss 5.8599958419799805\n",
      "59: Encoding Loss -1.6947218179702759, Transition Loss -13.711780548095703, Classifier Loss 0.05752122402191162, Total Loss 5.749380111694336\n",
      "59: Encoding Loss -1.7559330463409424, Transition Loss -16.53391456604004, Classifier Loss 0.054084111005067825, Total Loss 5.405104160308838\n",
      "59: Encoding Loss -1.6568870544433594, Transition Loss -13.388996124267578, Classifier Loss 0.044085822999477386, Total Loss 4.405904293060303\n",
      "59: Encoding Loss -1.1157411336898804, Transition Loss -5.1831841468811035, Classifier Loss 0.027409587055444717, Total Loss 2.739922046661377\n",
      "59: Encoding Loss -1.8712188005447388, Transition Loss -7.6785454750061035, Classifier Loss 0.08050275593996048, Total Loss 8.04874038696289\n",
      "59: Encoding Loss -2.3369951248168945, Transition Loss -14.51470947265625, Classifier Loss 0.04886443912982941, Total Loss 4.883541107177734\n",
      "59: Encoding Loss -0.9184229373931885, Transition Loss -0.4133877754211426, Classifier Loss 0.08850952237844467, Total Loss 8.850869178771973\n",
      "59: Encoding Loss -0.9968420267105103, Transition Loss -8.872314453125, Classifier Loss 0.06354109942913055, Total Loss 6.352335453033447\n",
      "59: Encoding Loss -0.26913711428642273, Transition Loss -5.635595798492432, Classifier Loss 0.05741815268993378, Total Loss 5.733027458190918\n",
      "59: Encoding Loss -0.813764750957489, Transition Loss -12.285785675048828, Classifier Loss 0.038331229239702225, Total Loss 3.8306658267974854\n",
      "59: Encoding Loss -2.1928505897521973, Transition Loss -15.712211608886719, Classifier Loss 0.05721840634942055, Total Loss 5.718698501586914\n",
      "59: Encoding Loss -1.0184903144836426, Transition Loss -5.803241729736328, Classifier Loss 0.055084072053432465, Total Loss 5.507246494293213\n",
      "59: Encoding Loss -0.615446150302887, Transition Loss -9.381488800048828, Classifier Loss 0.03854610025882721, Total Loss 3.852733612060547\n",
      "59: Encoding Loss -0.9619811773300171, Transition Loss -9.126686096191406, Classifier Loss 0.03992142155766487, Total Loss 3.990316867828369\n",
      "59: Encoding Loss -0.7294581532478333, Transition Loss -14.208794593811035, Classifier Loss 0.04082058370113373, Total Loss 4.079216480255127\n",
      "59: Encoding Loss -1.0048972368240356, Transition Loss -13.919831275939941, Classifier Loss 0.052842386066913605, Total Loss 5.281455039978027\n",
      "59: Encoding Loss -0.4677146077156067, Transition Loss -2.532850980758667, Classifier Loss 0.032865505665540695, Total Loss 3.286038398742676\n",
      "59: Encoding Loss -0.7573989629745483, Transition Loss -17.477025985717773, Classifier Loss 0.028872745111584663, Total Loss 2.8837790489196777\n",
      "59: Encoding Loss -2.1475489139556885, Transition Loss -17.084129333496094, Classifier Loss 0.025106308981776237, Total Loss 2.507214069366455\n",
      "59: Encoding Loss -0.8265665769577026, Transition Loss -1.5386147499084473, Classifier Loss 0.041115377098321915, Total Loss 4.111230373382568\n",
      "59: Encoding Loss -0.22493591904640198, Transition Loss 0.820091724395752, Classifier Loss 0.04709802567958832, Total Loss 4.8517866134643555\n",
      "59: Encoding Loss -0.38434475660324097, Transition Loss -11.540115356445312, Classifier Loss 0.027451906353235245, Total Loss 2.7426960468292236\n",
      "59: Encoding Loss -0.9959713220596313, Transition Loss -10.861023902893066, Classifier Loss 0.04270241782069206, Total Loss 4.268069744110107\n",
      "59: Encoding Loss -2.025129556655884, Transition Loss -12.90750789642334, Classifier Loss 0.04359869658946991, Total Loss 4.357287883758545\n",
      "59: Encoding Loss -0.9693864583969116, Transition Loss -6.71965217590332, Classifier Loss 0.048194266855716705, Total Loss 4.818082809448242\n",
      "59: Encoding Loss -2.440603733062744, Transition Loss -15.028830528259277, Classifier Loss 0.03138631954789162, Total Loss 3.1356263160705566\n",
      "59: Encoding Loss -0.3033376634120941, Transition Loss -8.79270076751709, Classifier Loss 0.058171529322862625, Total Loss 5.812459945678711\n",
      "59: Encoding Loss -1.1325066089630127, Transition Loss -12.612981796264648, Classifier Loss 0.06619489192962646, Total Loss 6.616966724395752\n",
      "59: Encoding Loss -2.267646312713623, Transition Loss -19.28207778930664, Classifier Loss 0.04173269867897034, Total Loss 4.1694135665893555\n",
      "59: Encoding Loss -1.7835428714752197, Transition Loss -8.840721130371094, Classifier Loss 0.05344031751155853, Total Loss 5.342263698577881\n",
      "59: Encoding Loss -1.8592870235443115, Transition Loss -10.205389976501465, Classifier Loss 0.07124771177768707, Total Loss 7.122730255126953\n",
      "59: Encoding Loss -1.6446887254714966, Transition Loss -15.37678337097168, Classifier Loss 0.05539283528923988, Total Loss 5.536208629608154\n",
      "59: Encoding Loss -1.5774427652359009, Transition Loss -12.315861701965332, Classifier Loss 0.022513054311275482, Total Loss 2.248842239379883\n",
      "59: Encoding Loss -0.2812618613243103, Transition Loss -11.50023078918457, Classifier Loss 0.04424078017473221, Total Loss 4.4162492752075195\n",
      "59: Encoding Loss -1.394336223602295, Transition Loss -5.3642778396606445, Classifier Loss 0.03582913801074028, Total Loss 3.581840991973877\n",
      "59: Encoding Loss -0.7498482465744019, Transition Loss -8.793157577514648, Classifier Loss 0.04668492078781128, Total Loss 4.666733741760254\n",
      "59: Encoding Loss -1.561023473739624, Transition Loss -9.483837127685547, Classifier Loss 0.04380551353096962, Total Loss 4.378654479980469\n",
      "59: Encoding Loss -0.7496902942657471, Transition Loss -5.129238128662109, Classifier Loss 0.025225628167390823, Total Loss 2.5215368270874023\n",
      "59: Encoding Loss -1.0617852210998535, Transition Loss -10.115973472595215, Classifier Loss 0.022055119276046753, Total Loss 2.203488826751709\n",
      "59: Encoding Loss -0.7609768509864807, Transition Loss -12.274591445922852, Classifier Loss 0.06579190492630005, Total Loss 6.576735496520996\n",
      "59: Encoding Loss 0.014811080880463123, Transition Loss -12.265867233276367, Classifier Loss 0.0421142615377903, Total Loss 4.275192737579346\n",
      "59: Encoding Loss -1.3725301027297974, Transition Loss -2.8498733043670654, Classifier Loss 0.06698457896709442, Total Loss 6.697887897491455\n",
      "59: Encoding Loss -0.6648448705673218, Transition Loss -11.20583724975586, Classifier Loss 0.029862908646464348, Total Loss 2.9840497970581055\n",
      "59: Encoding Loss 0.26296645402908325, Transition Loss -14.02988052368164, Classifier Loss 0.06104370579123497, Total Loss 8.196306228637695\n",
      "59: Encoding Loss 4.073977947235107, Transition Loss -5.695178985595703, Classifier Loss 0.038945723325014114, Total Loss 36.48525619506836\n",
      "59: Encoding Loss -0.07877232879400253, Transition Loss -9.799147605895996, Classifier Loss 0.049525339156389236, Total Loss 4.814815044403076\n",
      "59: Encoding Loss -0.9607396721839905, Transition Loss -5.337631702423096, Classifier Loss 0.05335259437561035, Total Loss 5.334191799163818\n",
      "59: Encoding Loss -1.2761059999465942, Transition Loss -15.444758415222168, Classifier Loss 0.025886334478855133, Total Loss 2.5855445861816406\n",
      "59: Encoding Loss 0.46018102765083313, Transition Loss -1.653390645980835, Classifier Loss 0.043508801609277725, Total Loss 8.031990051269531\n",
      "59: Encoding Loss -1.020699381828308, Transition Loss -6.975496292114258, Classifier Loss 0.02389310859143734, Total Loss 2.387915849685669\n",
      "59: Encoding Loss -0.49213966727256775, Transition Loss -6.459910869598389, Classifier Loss 0.07498618960380554, Total Loss 7.497325897216797\n",
      "59: Encoding Loss -0.8818607330322266, Transition Loss -14.821739196777344, Classifier Loss 0.12454095482826233, Total Loss 12.451131820678711\n",
      "59: Encoding Loss -2.112946033477783, Transition Loss -15.061878204345703, Classifier Loss 0.05733620747923851, Total Loss 5.7306084632873535\n",
      "59: Encoding Loss -2.200575351715088, Transition Loss -20.136384963989258, Classifier Loss 0.0331120565533638, Total Loss 3.307178258895874\n",
      "59: Encoding Loss -1.3257880210876465, Transition Loss -10.291423797607422, Classifier Loss 0.08399780839681625, Total Loss 8.397723197937012\n",
      "59: Encoding Loss -1.9357469081878662, Transition Loss -12.213055610656738, Classifier Loss 0.037206169217824936, Total Loss 3.7181742191314697\n",
      "59: Encoding Loss -2.2962565422058105, Transition Loss -17.828269958496094, Classifier Loss 0.039677731692790985, Total Loss 3.964207649230957\n",
      "59: Encoding Loss -1.842445731163025, Transition Loss -6.0189666748046875, Classifier Loss 0.06674107909202576, Total Loss 6.672904014587402\n",
      "59: Encoding Loss -0.9928297400474548, Transition Loss -14.24876880645752, Classifier Loss 0.06881971657276154, Total Loss 6.879122257232666\n",
      "59: Encoding Loss -1.504865050315857, Transition Loss -15.27469539642334, Classifier Loss 0.018081320449709892, Total Loss 1.805077075958252\n",
      "59: Encoding Loss -1.6540300846099854, Transition Loss -18.5026912689209, Classifier Loss 0.048482317477464676, Total Loss 4.844531059265137\n",
      "59: Encoding Loss -2.251039981842041, Transition Loss -24.434947967529297, Classifier Loss 0.010257408022880554, Total Loss 1.020853877067566\n"
     ]
    }
   ],
   "source": [
    "# Train Forward Loss\n",
    "training_loop(in_data, out_data, initiations_s, initiations_s_prime,  n_epochs, optimizer_forward, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate\n",
    "def plot_z_space(data):\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_actions):\n",
    "            i_s = data[i][2]\n",
    "            s = data[i][0][...,:obs_dim]\n",
    "            a = data[i][0][...,obs_dim:]\n",
    "            s_prime = data[i][1][..., :obs_dim]\n",
    "            z = encoder(s[i_s[...,i]==1])\n",
    "            z_prime_encoded = encoder(s_prime)\n",
    "            z_prime = transition_model(torch.cat((z[:, 0:-1], a[i_s[...,i]==1]), dim=-1))\n",
    "            s_prime = grounding_model(z[..., 0:-1])\n",
    "            # plt.scatter(z[:, 0], z[:, 1], label=f\"encoded action {i}\")\n",
    "            \n",
    "            plt.scatter(z_prime_encoded[i_s[...,i]==1, 0], z_prime_encoded[i_s[...,i]==1, 1], label=f\"encoded s prime {i}\")\n",
    "            plt.scatter(z_prime[:, 0], z_prime[:, 1], label=f\"prediction action {i}\")\n",
    "            # plt.scatter(s_prime[:, 0], s_prime[:, 1], label=f\"action {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_z_space(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = data[0][0][..., :obs_dim]\n",
    "    a = data[0][0][...,obs_dim:]\n",
    "    z = encoder(x)\n",
    "    logits = initiation_classifier(z[:, :latent_dim])\n",
    "    z_prime = transition_model(torch.cat((z[:, 0:-1], a), dim=-1))[..., :latent_dim]\n",
    "    logits_z_prime = initiation_classifier(z_prime)\n",
    "    init_masks = logits\n",
    "    \n",
    "    for i in range(n_actions):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.scatter(s[init_masks[...,i] > 0.8, 0], s[init_masks[...,i] > 0.8, 1], label=f\"initiation_set {i}\")\n",
    "        # plt.scatter(s[data[i][2][...,i] == 1, 0], s[data[i][2][...,i] == 1, 1], label=f\"initiation_set (truth) {i}\")\n",
    "        # plt.scatter(s_prime[0][I_s_prime[i,..., i] == 1, 0], s_prime[0][I_s_prime[i,...,i] == 1, 1])\n",
    "        # plt.scatter(s_prime[0][logits_z_prime[..., i] > 0.7, 0], s_prime[0][logits_z_prime[...,i] > 0.7, 1])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        \n",
    "        x = data[i][0][..., :obs_dim]\n",
    "        i_s = data[i][2][...,i]\n",
    "        a = data[i][0][i_s ==1, ...,obs_dim:]\n",
    "        z = encoder(x[i_s==1])\n",
    "        epsilon = torch.normal(0, 1, (z.size(0), latent_dim))\n",
    "        z_prime_prediction = transition_model(torch.cat((z[:, :latent_dim], a), dim=-1))\n",
    "        # z_prime_prediction = epsilon * z_prime_prediction[..., latent_dim:] + z_prime_prediction[..., :latent_dim]\n",
    "        s_prime = grounding_model(z_prime_prediction[..., :latent_dim])\n",
    "\n",
    "        epsilon = torch.normal(0, 1, (s_prime.size(0), obs_dim))\n",
    "        s_prime_sample = torch.exp(s_prime[..., obs_dim:]) * epsilon + s_prime[..., :obs_dim]\n",
    "        # s_prime_sample = s_prime[..., :obs_dim]\n",
    "        z_prime_effect = encoder(s_prime_sample)\n",
    "\n",
    "        z_prime_truth = encoder(data[i][1][i_s==1,..., :obs_dim])\n",
    "\n",
    "\n",
    "        logits = initiation_classifier(z[:, :latent_dim])\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(z_prime_truth[..., 0], z_prime_truth[..., 1], label=f\"S_prime Truth action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.scatter(z_prime_effect[..., 0], z_prime_effect[..., 1], label=f\"S_prime Effect prediction action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(z_prime_prediction[..., 0], z_prime_prediction[..., 1])\n",
    "        # plt.legend()\n",
    "        \n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
