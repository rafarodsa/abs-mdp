{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "X_MIN, X_MAX, Y_MIN, Y_MAX = 0, 10, 0, 10\n",
    "obs_dim = 10\n",
    "state_dim = 2\n",
    "T = np.random.randn(obs_dim, state_dim)\n",
    "\n",
    "# Centroids\n",
    "centroids = []\n",
    "for x in (X_MAX//4, X_MAX * 3//4):\n",
    "    for y in (Y_MAX//4, Y_MAX * 3//4):\n",
    "        centroids.append(np.array((x, y)))\n",
    "\n",
    "rooms = []\n",
    "for x in (X_MAX//2, X_MAX):\n",
    "    for y in (Y_MAX//2, Y_MAX):\n",
    "        rooms.append((x-X_MAX//2, y-Y_MAX//2, x, y))\n",
    "def initiation_set(room_limits):\n",
    "    def _init(state):\n",
    "        low_limit = room_limits[0:2]\n",
    "        high_limit = room_limits[2:]\n",
    "        return (state[0] > low_limit[0] and state[0] < high_limit[0] and state[1] > low_limit[1] and state[1] < high_limit[1])\n",
    "    return _init\n",
    "\n",
    "\n",
    "def effect_sample(centroid_1, centroid_2):\n",
    "    def _effect(state):\n",
    "        c = np.random.randint(0, 2)\n",
    "        centroids = [centroid_1, centroid_2]\n",
    "        return np.random.multivariate_normal(mean=np.zeros(state_dim), cov=np.eye(state_dim)/2**6, size=1) + centroids[c]\n",
    "    return _effect\n",
    "\n",
    "\n",
    "# fix room order to be clockwise\n",
    "for l in (rooms, centroids):\n",
    "    _r = l[2]\n",
    "    l[2] = l[3]\n",
    "    l[3] = _r\n",
    "\n",
    "# Synthetic option definition\n",
    "effect_dists_params = []\n",
    "initiation_sets = []\n",
    "std_dev = 1\n",
    "for i, room in enumerate(rooms):\n",
    "    initiation_sets.append(initiation_set(room))\n",
    "    effect_dists_params.append(effect_sample(centroids[(i-1)%4], centroids[(i+1)%4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "\n",
    "N = 5000\n",
    "\n",
    "x = np.random.uniform(0, X_MAX, N)\n",
    "y = np.random.uniform(0, Y_MAX, N)\n",
    "\n",
    "s = np.array((x,y)).T\n",
    "\n",
    "masks = []\n",
    "for i, init_set in enumerate(initiation_sets):\n",
    "    mask = np.zeros((N,1))\n",
    "    for j in range(N):\n",
    "        mask[j] = float(init_set(s[j]))\n",
    "    masks.append(mask)\n",
    "\n",
    "masks = np.array(masks)\n",
    "I_s = np.array(masks).transpose((1,0,2))[...,0] # initiation vector at s\n",
    "\n",
    "# generate next states\n",
    "s_prime = []\n",
    "for i, effect_dist in enumerate(effect_dists_params):\n",
    "    _s_prime = []\n",
    "    for j in range(N):\n",
    "        _s_prime.append(effect_dist(s[j]))\n",
    "    s_prime.append(np.array(_s_prime).squeeze())\n",
    "\n",
    "\n",
    "# s_prime = np.array(s_prime) * masks  + s[np.newaxis] * (1-masks)\n",
    "s_prime = np.array(s_prime)\n",
    "\n",
    "masks = masks.reshape((-1,))\n",
    "\n",
    "I_s_prime = []\n",
    "for action in range(len(initiation_sets)):\n",
    "    _m = np.zeros((N,len(initiation_sets)))\n",
    "    for i, init_set in enumerate(initiation_sets):\n",
    "        for j in range(N):\n",
    "            _m[j, i] = float(init_set(s_prime[action][j]))\n",
    "    I_s_prime.append(_m)\n",
    "\n",
    "# Random affine transformation\n",
    "x = np.einsum('ij, kj->ki', T, s)\n",
    "x_prime = np.einsum('ij, lkj->lki', T, s_prime)\n",
    "\n",
    "data = []\n",
    "for i in range(len(effect_dists_params)):\n",
    "    data.append((x, x_prime[i], I_s, I_s_prime[i], s, s_prime[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot to Check\n",
    "for r in range(4):\n",
    "    plt.subplot(2, 2, r+1) \n",
    "    plt.scatter(s[I_s[:, r]==1, 0], s[I_s[:, r]==1, 1])\n",
    "    plt.scatter(s_prime[r, I_s[:, r]==1,0], s_prime[r, I_s[:, r]==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from inspect import trace\n",
    "\n",
    "\n",
    "n_actions = 4\n",
    "hidden_size = 128\n",
    "latent_dim = 2\n",
    "\n",
    "def init_models():\n",
    "    encoder = nn.Sequential(\n",
    "                                nn.Linear(obs_dim, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_size, latent_dim + 1)\n",
    "                        )\n",
    "\n",
    "    linear_1 = nn.Sequential(nn.Linear(latent_dim + n_actions, hidden_size),\n",
    "                             nn.Tanh())\n",
    "    linear_2 = nn.Sequential(nn.Linear(latent_dim + n_actions, hidden_size),\n",
    "                             nn.Sigmoid())\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    transition_model = nn.Sequential(\n",
    "                                    # nn.Linear(latent_dim + n_actions, hidden_size),\n",
    "                                    # nn.ReLU(),\n",
    "                                    # nn.Linear(hidden_size, hidden_size),\n",
    "                                    # nn.ReLU(),\n",
    "                                    # nn.Linear(hidden_size, hidden_size),\n",
    "                                    # nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, latent_dim + 1)\n",
    "                                )\n",
    "\n",
    "    grounding_model = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, obs_dim + 1)\n",
    "                                )\n",
    "    \n",
    "    initiation_classifier = nn.Sequential(\n",
    "                                    nn.Linear(latent_dim, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, n_actions),\n",
    "                                    nn.Sigmoid()\n",
    "                                )\n",
    "    return encoder, transition_model, grounding_model, initiation_classifier, linear_1, linear_2\n",
    "\n",
    "encoder, transition_model, grounding_model, initiation_classifier, linear_1, linear_2 = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training hyperparameters\n",
    "n_epochs = 60\n",
    "minibatch_size = 32\n",
    "n_samples = 30  # samples to approximate expectation\n",
    "learning_rate = 1e-4\n",
    "beta = 0.01  # hyperparameter to control information bottleneck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data to float torch tensor and add action one-hot encoding\n",
    "in_data = []\n",
    "out_data = []\n",
    "initiations_s = []\n",
    "initiations_s_prime = []\n",
    "for i in range(n_actions):\n",
    "    data[i] = [torch.from_numpy(_tensor).float() for _tensor in data[i]]\n",
    "    actions = nn.functional.one_hot(torch.ones(data[i][0].size(0)).long() * i, n_actions)\n",
    "    data[i] = (torch.cat((data[i][0], actions), dim=1), data[i][1], data[i][2], data[i][3], data[i][4], data[i][5]) \n",
    "    in_data.append(data[i][0])\n",
    "    out_data.append(data[i][1])\n",
    "    initiations_s.append(data[i][2])\n",
    "    initiations_s_prime.append(data[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = torch.cat(in_data, dim=0)[masks == 1]\n",
    "out_data = torch.cat(out_data, dim=0)[masks == 1]\n",
    "initiations_s = torch.cat(initiations_s, dim=0)[masks==1]\n",
    "initiations_s_prime = torch.cat(initiations_s_prime, dim=0)[masks==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "rand_order = torch.randperm(in_data.size(0))\n",
    "in_data = torch.index_select(in_data, 0, rand_order)\n",
    "out_data = torch.index_select(out_data, 0, rand_order)\n",
    "initiations_s = torch.index_select(initiations_s, 0, rand_order)\n",
    "initiations_s_prime = torch.index_select(initiations_s_prime, 0, rand_order)\n",
    "\n",
    "\n",
    "def minibatches(data_list, batch_size):\n",
    "    N = in_data.size(0)\n",
    "    n_batches = N//batch_size + int(N%batch_size != 0)\n",
    "    for i in range(n_batches):\n",
    "        minibatch = [d[i*batch_size:(i+1)*batch_size] for d in data_list]\n",
    "        # yield (in_data[i*batch_size:(i+1)*batch_size], out_data[i*batch_size:(i+1)*batch_size], initiations[i*batch_size:(i+1)*batch_size])\n",
    "        yield minibatch\n",
    "\n",
    "models = (encoder, transition_model, grounding_model, initiation_classifier)\n",
    "models_params = []\n",
    "for model in models:\n",
    "    for param in model.parameters():\n",
    "        models_params.append(param)\n",
    "optimizer_forward = torch.optim.RMSprop(models_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_loss = nn.BCELoss()\n",
    "epsilon=1\n",
    "def loss(target, predicted_s_prime, predicted_z):\n",
    "    \n",
    "    target = target.unsqueeze(1).unsqueeze(1)\n",
    "    encoding_loss = 0.5 * predicted_z[..., 0:latent_dim].pow(2).sum(dim=-1) + latent_dim * (torch.exp(predicted_z[..., -1]) - predicted_z[..., -1])\n",
    "    prediction_loss = -0.5 * (target - predicted_s_prime[..., 0:obs_dim]).pow(2).sum(dim=-1, keepdim=True)/(torch.exp(2*predicted_s_prime[..., obs_dim:]) + epsilon) - obs_dim * predicted_s_prime[..., obs_dim:]\n",
    "    _loss = -((1-beta)*prediction_loss - beta * encoding_loss)\n",
    "    return _loss\n",
    "\n",
    "def consistency_loss(target, encoded_z_prime, n_samples=10):\n",
    "\n",
    "    noise = torch.normal(0, 1, (target.size(0), n_samples, latent_dim))\n",
    "    z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * noise + encoded_z_prime.unsqueeze(1)[:,:, :-1]  \n",
    "    s_prime_params = grounding_model(z_prime_samples)\n",
    "    _means = s_prime_params[..., :obs_dim]\n",
    "    _vars = torch.exp(2*s_prime_params[..., obs_dim:])\n",
    "    _target_prediction_sim = torch.einsum('...bj,...bk->...b' ,target.unsqueeze(-2), _means)\n",
    "    _mean_norm = torch.einsum('...j,...k->...' , _means, _means)\n",
    "    _target_norm = torch.einsum('...bj,...bk->...b', target, target).unsqueeze(1)\n",
    "    _losses = obs_dim * torch.log(_vars.squeeze()) + (2 * _target_prediction_sim + _mean_norm + _target_norm)/_vars.squeeze() \n",
    "    return _losses\n",
    "\n",
    "def contrained_transition(transition_params, encoded_z_prime): # mean-seeking\n",
    "    epsilon=1e-5\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_t  + ((mean_z.pow(2) + torch.exp(2*log_sigma_z))/(torch.exp(2*log_sigma_t) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon) + torch.einsum('...i, ...j->...', mean_t, mean_t).unsqueeze(-1)/(torch.exp(2*log_sigma_t)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "def transition_consistency(transition_params, encoded_z_prime): # mode-seeking\n",
    "    epsilon=1e-5\n",
    "    mean_t = transition_params[..., :latent_dim]\n",
    "    log_sigma_t = transition_params[..., latent_dim:]\n",
    "    mean_z = encoded_z_prime[..., :latent_dim].unsqueeze(1)\n",
    "    log_sigma_z = encoded_z_prime[..., latent_dim:].unsqueeze(1)\n",
    "    entropy = latent_dim * 2 * log_sigma_t\n",
    "\n",
    "    _loss = 0.5 * (2 * latent_dim * log_sigma_z  + ((mean_t.pow(2) + torch.exp(2*log_sigma_t))/(torch.exp(2*log_sigma_z) + epsilon)).sum(dim=-1, keepdim=True) - 2*torch.einsum('...i, ...j->...', mean_t, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon) + torch.einsum('...i, ...j->...', mean_z, mean_z).unsqueeze(-1)/(torch.exp(2*log_sigma_z)+epsilon))\n",
    "    return (_loss - entropy).mean(-2)\n",
    "\n",
    "\n",
    "def forward_loss(inp, target, predicted_s_prime, predicted_z, predicted_z_prime, s_prime_params):\n",
    "    return loss(target, predicted_s_prime, predicted_z)\n",
    "\n",
    "def training_loop(in_data, out_data, initiations_s, initiations_s_prime, n_epochs, optimizer, loss_fn=forward_loss, print_loss=False):\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for _in, _out, _init_s, _init_s_prime in minibatches((in_data, out_data, initiations_s, initiations_s_prime), minibatch_size):\n",
    "            ## Forward pass\n",
    "            actions = _in[..., obs_dim:].unsqueeze(1).repeat_interleave(n_samples, dim=1)\n",
    "            z = encoder(_in[...,:obs_dim])  # encode\n",
    "            # Predict next abstract state\n",
    "            noise = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_ = torch.exp(z.unsqueeze(1)[:, :, -2:-1]) * noise + z.unsqueeze(1)[:,:, :-1]   \n",
    "            z_a = torch.cat((z_, actions), dim=-1)\n",
    "\n",
    "            \n",
    "            z_prime = transition_model(linear_1(z_a) * linear_2(z_a))\n",
    "            \n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, latent_dim))\n",
    "            z_prime_samples = torch.exp(z_prime[:, :, -2:-1]) * epsilon + z_prime[:,:, :-1]  \n",
    "            \n",
    "            encoded_z_prime = encoder(_out)\n",
    "            # z_prime_samples = torch.exp(encoded_z_prime.unsqueeze(1)[:, :, -2:-1]) * epsilon + encoded_z_prime.unsqueeze(1)[:,:, :-1] \n",
    "\n",
    "            # predict next ground state\n",
    "            s_prime = grounding_model(z_prime_samples).unsqueeze(2)\n",
    "            # s_prime = grounding_model(encoded_z_prime)\n",
    "            epsilon = torch.normal(0, 1, (_in.size(0), n_samples, n_samples, obs_dim))\n",
    "            s_prime_samples = torch.exp(s_prime[..., :obs_dim]) * epsilon + s_prime[..., :obs_dim]\n",
    "            # s_prime = s_prime.squeeze(1)\n",
    "            \n",
    "            # Predict initiation vector from (z, z')\n",
    "            predicted_I_s = initiation_classifier(z_[:, 0])\n",
    "            predicted_I_s_prime = initiation_classifier(z_prime_samples[:, 0])\n",
    "\n",
    "            # initiation_prediction = predicted_I_s.squeeze()#torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            # initiation_target = _init_s#torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            initiation_prediction = torch.cat((predicted_I_s, predicted_I_s_prime), dim=0).squeeze()\n",
    "            initiation_target = torch.cat((_init_s, _init_s_prime), dim=0)\n",
    "            \n",
    "            \n",
    "            # binary classifier loss\n",
    "            _classifier_loss = classifier_loss(initiation_prediction, initiation_target)\n",
    "            \n",
    "            _target = _out\n",
    "            # _transition_constraint = contrained_transition(z_prime.detach(), encoded_z_prime).mean()\n",
    "            _transition_constraint = transition_consistency(z_prime, encoded_z_prime.detach()).mean()\n",
    "            _consistency_loss = consistency_loss(_out, encoded_z_prime).mean()\n",
    "            _encoding_loss = loss_fn(_in, _target, s_prime, z, z_prime_samples, s_prime).mean()\n",
    "\n",
    "            trans_const = 0.2\n",
    "\n",
    "            _loss = (1-trans_const) * nn.functional.gelu(10 *_encoding_loss) + nn.functional.relu(0 * _consistency_loss) + 100 * _classifier_loss + trans_const * nn.functional.leaky_relu(1 * _transition_constraint, negative_slope=0.001)\n",
    "            # _loss = _encoding_loss + _consistency_loss + _transition_constraint\n",
    "\n",
    "            \n",
    "            if print_loss:\n",
    "                print(f\"{i}: Encoding Loss {_encoding_loss}, Transition Loss {_transition_constraint}, Classifier Loss {_classifier_loss}, Total Loss {_loss}\")\n",
    "            \n",
    "            ### zero grads \n",
    "            optimizer.zero_grad()\n",
    "            ### backward pass\n",
    "            _loss.backward()\n",
    "            \n",
    "            ### update\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Encoding Loss 197.0939178466797, Transition Loss 18373.435546875, Classifier Loss 0.6976369619369507, Total Loss 5321.2021484375\n",
      "0: Encoding Loss 207.88912963867188, Transition Loss 22103.064453125, Classifier Loss 0.6762166023254395, Total Loss 6151.34765625\n",
      "0: Encoding Loss 227.24862670898438, Transition Loss 24599.46484375, Classifier Loss 0.6608837246894836, Total Loss 6803.970703125\n",
      "0: Encoding Loss 179.86814880371094, Transition Loss 15748.9052734375, Classifier Loss 0.6476562023162842, Total Loss 4653.49169921875\n",
      "0: Encoding Loss 190.16867065429688, Transition Loss 19384.158203125, Classifier Loss 0.6300100088119507, Total Loss 5461.18212890625\n",
      "0: Encoding Loss 175.08116149902344, Transition Loss 9169.4140625, Classifier Loss 0.6315562129020691, Total Loss 3297.687744140625\n",
      "0: Encoding Loss 173.58213806152344, Transition Loss 14154.3955078125, Classifier Loss 0.6212636232376099, Total Loss 4281.66259765625\n",
      "0: Encoding Loss 210.2854461669922, Transition Loss 16513.43359375, Classifier Loss 0.6096660494804382, Total Loss 5045.93701171875\n",
      "0: Encoding Loss 168.84658813476562, Transition Loss 9226.7099609375, Classifier Loss 0.6002452373504639, Total Loss 3256.13916015625\n",
      "0: Encoding Loss 203.07090759277344, Transition Loss 13080.25390625, Classifier Loss 0.5970630645751953, Total Loss 4300.32421875\n",
      "0: Encoding Loss 169.3468475341797, Transition Loss 8090.986328125, Classifier Loss 0.5973414182662964, Total Loss 3032.7060546875\n",
      "0: Encoding Loss 151.77406311035156, Transition Loss 5917.740234375, Classifier Loss 0.5861772298812866, Total Loss 2456.3583984375\n",
      "0: Encoding Loss 129.46852111816406, Transition Loss 3612.04833984375, Classifier Loss 0.590985119342804, Total Loss 1817.25634765625\n",
      "0: Encoding Loss 152.8028106689453, Transition Loss 4012.1845703125, Classifier Loss 0.5963313579559326, Total Loss 2084.49267578125\n",
      "0: Encoding Loss 130.02145385742188, Transition Loss 2607.680908203125, Classifier Loss 0.6006074547767639, Total Loss 1621.768798828125\n",
      "0: Encoding Loss 132.5421600341797, Transition Loss 2859.51708984375, Classifier Loss 0.5756168365478516, Total Loss 1689.8023681640625\n",
      "0: Encoding Loss 123.39856719970703, Transition Loss 2158.756591796875, Classifier Loss 0.5811309814453125, Total Loss 1477.0531005859375\n",
      "0: Encoding Loss 131.02980041503906, Transition Loss 1961.943359375, Classifier Loss 0.5700713396072388, Total Loss 1497.6341552734375\n",
      "0: Encoding Loss 114.85594940185547, Transition Loss 1569.9649658203125, Classifier Loss 0.5725252628326416, Total Loss 1290.0931396484375\n",
      "0: Encoding Loss 143.03395080566406, Transition Loss 1851.74609375, Classifier Loss 0.5692645907402039, Total Loss 1571.54736328125\n",
      "0: Encoding Loss 115.13571166992188, Transition Loss 1093.758056640625, Classifier Loss 0.571998119354248, Total Loss 1197.0372314453125\n",
      "0: Encoding Loss 107.84777069091797, Transition Loss 863.3589477539062, Classifier Loss 0.5729499459266663, Total Loss 1092.7489013671875\n",
      "0: Encoding Loss 104.19638061523438, Transition Loss 648.6939697265625, Classifier Loss 0.5401209592819214, Total Loss 1017.322021484375\n",
      "0: Encoding Loss 89.99999237060547, Transition Loss 494.53851318359375, Classifier Loss 0.5902151465415955, Total Loss 877.92919921875\n",
      "0: Encoding Loss 106.54261016845703, Transition Loss 664.931396484375, Classifier Loss 0.5653355717658997, Total Loss 1041.86083984375\n",
      "0: Encoding Loss 129.40228271484375, Transition Loss 724.5770263671875, Classifier Loss 0.5716619491577148, Total Loss 1237.2998046875\n",
      "0: Encoding Loss 121.78623962402344, Transition Loss 628.3851318359375, Classifier Loss 0.5639564394950867, Total Loss 1156.3626708984375\n",
      "0: Encoding Loss 96.12095642089844, Transition Loss 401.5118713378906, Classifier Loss 0.5740904808044434, Total Loss 906.6791381835938\n",
      "0: Encoding Loss 90.2467269897461, Transition Loss 338.1699523925781, Classifier Loss 0.5598387718200684, Total Loss 845.5916748046875\n",
      "0: Encoding Loss 96.25132751464844, Transition Loss 315.1897888183594, Classifier Loss 0.5666893720626831, Total Loss 889.7175903320312\n",
      "0: Encoding Loss 95.47953033447266, Transition Loss 313.7607727050781, Classifier Loss 0.5548434257507324, Total Loss 882.0726928710938\n",
      "0: Encoding Loss 75.84577941894531, Transition Loss 161.2405242919922, Classifier Loss 0.570662260055542, Total Loss 696.08056640625\n",
      "0: Encoding Loss 103.31307220458984, Transition Loss 285.3865661621094, Classifier Loss 0.5632595419883728, Total Loss 939.9078369140625\n",
      "0: Encoding Loss 80.2840347290039, Transition Loss 193.87533569335938, Classifier Loss 0.5734820365905762, Total Loss 738.3955688476562\n",
      "0: Encoding Loss 88.04837036132812, Transition Loss 177.84515380859375, Classifier Loss 0.5536686778068542, Total Loss 795.3228759765625\n",
      "0: Encoding Loss 67.59830474853516, Transition Loss 127.16545867919922, Classifier Loss 0.5705737471580505, Total Loss 623.2769165039062\n",
      "0: Encoding Loss 83.11431121826172, Transition Loss 149.23416137695312, Classifier Loss 0.5428475737571716, Total Loss 749.0460205078125\n",
      "0: Encoding Loss 91.0705337524414, Transition Loss 142.78060913085938, Classifier Loss 0.5584236979484558, Total Loss 812.9627685546875\n",
      "0: Encoding Loss 72.38056945800781, Transition Loss 113.33012390136719, Classifier Loss 0.5491934418678284, Total Loss 656.6299438476562\n",
      "0: Encoding Loss 67.41468811035156, Transition Loss 91.30210876464844, Classifier Loss 0.5451132655143738, Total Loss 612.0892944335938\n",
      "0: Encoding Loss 76.24210357666016, Transition Loss 94.5921401977539, Classifier Loss 0.5525420308113098, Total Loss 684.1094970703125\n",
      "0: Encoding Loss 64.8655014038086, Transition Loss 84.10887908935547, Classifier Loss 0.5757573843002319, Total Loss 593.321533203125\n",
      "0: Encoding Loss 69.79212188720703, Transition Loss 85.92619323730469, Classifier Loss 0.5593625903129578, Total Loss 631.45849609375\n",
      "0: Encoding Loss 57.2030029296875, Transition Loss 69.63079833984375, Classifier Loss 0.5632418990135193, Total Loss 527.8743896484375\n",
      "0: Encoding Loss 75.25271606445312, Transition Loss 86.94424438476562, Classifier Loss 0.5818984508514404, Total Loss 677.6004028320312\n",
      "0: Encoding Loss 71.97836303710938, Transition Loss 73.3151626586914, Classifier Loss 0.5586756467819214, Total Loss 646.3574829101562\n",
      "0: Encoding Loss 69.1063461303711, Transition Loss 61.102630615234375, Classifier Loss 0.5632023215293884, Total Loss 621.3915405273438\n",
      "0: Encoding Loss 66.13285827636719, Transition Loss 63.41020965576172, Classifier Loss 0.5494865775108337, Total Loss 596.6936645507812\n",
      "0: Encoding Loss 67.97122955322266, Transition Loss 59.404075622558594, Classifier Loss 0.5568431615829468, Total Loss 611.3349609375\n",
      "0: Encoding Loss 68.391845703125, Transition Loss 51.431026458740234, Classifier Loss 0.5525739192962646, Total Loss 612.6783447265625\n",
      "0: Encoding Loss 48.502017974853516, Transition Loss 34.255218505859375, Classifier Loss 0.5686936378479004, Total Loss 451.7365417480469\n",
      "0: Encoding Loss 55.323246002197266, Transition Loss 40.71846008300781, Classifier Loss 0.593934953212738, Total Loss 510.1231689453125\n",
      "0: Encoding Loss 51.06013870239258, Transition Loss 34.436458587646484, Classifier Loss 0.5624164342880249, Total Loss 471.61004638671875\n",
      "0: Encoding Loss 49.15122604370117, Transition Loss 27.23390769958496, Classifier Loss 0.5361056327819824, Total Loss 452.26715087890625\n",
      "0: Encoding Loss 58.48733901977539, Transition Loss 34.32640838623047, Classifier Loss 0.5650691390037537, Total Loss 531.2709350585938\n",
      "0: Encoding Loss 51.09894943237305, Transition Loss 29.435012817382812, Classifier Loss 0.5650689005851746, Total Loss 471.18548583984375\n",
      "0: Encoding Loss 52.19050979614258, Transition Loss 30.49449348449707, Classifier Loss 0.5444269180297852, Total Loss 478.065673828125\n",
      "0: Encoding Loss 53.47632598876953, Transition Loss 30.959571838378906, Classifier Loss 0.5403680801391602, Total Loss 488.0393371582031\n",
      "0: Encoding Loss 62.875282287597656, Transition Loss 36.296722412109375, Classifier Loss 0.5553043484687805, Total Loss 565.7920532226562\n",
      "0: Encoding Loss 52.075050354003906, Transition Loss 27.284509658813477, Classifier Loss 0.5783636569976807, Total Loss 479.8936767578125\n",
      "0: Encoding Loss 57.39378356933594, Transition Loss 24.415943145751953, Classifier Loss 0.5407916307449341, Total Loss 518.1126708984375\n",
      "0: Encoding Loss 58.188541412353516, Transition Loss 26.651626586914062, Classifier Loss 0.5700147151947021, Total Loss 527.8401489257812\n",
      "0: Encoding Loss 56.70787811279297, Transition Loss 30.300086975097656, Classifier Loss 0.5489277839660645, Total Loss 514.6158447265625\n",
      "0: Encoding Loss 57.224220275878906, Transition Loss 29.272418975830078, Classifier Loss 0.5745313763618469, Total Loss 521.1013793945312\n",
      "0: Encoding Loss 44.572471618652344, Transition Loss 18.24549674987793, Classifier Loss 0.525141179561615, Total Loss 412.7430419921875\n",
      "0: Encoding Loss 49.2308235168457, Transition Loss 21.7375545501709, Classifier Loss 0.5437948703765869, Total Loss 452.5735778808594\n",
      "0: Encoding Loss 50.706642150878906, Transition Loss 21.209306716918945, Classifier Loss 0.5456743836402893, Total Loss 464.4624328613281\n",
      "0: Encoding Loss 54.4987678527832, Transition Loss 24.31230926513672, Classifier Loss 0.5413854718208313, Total Loss 494.99114990234375\n",
      "0: Encoding Loss 60.214542388916016, Transition Loss 26.541751861572266, Classifier Loss 0.5436898469924927, Total Loss 541.3936767578125\n",
      "0: Encoding Loss 50.15087127685547, Transition Loss 20.508525848388672, Classifier Loss 0.5492832660675049, Total Loss 460.2370300292969\n",
      "0: Encoding Loss 50.765380859375, Transition Loss 18.17376708984375, Classifier Loss 0.5500452518463135, Total Loss 464.7623291015625\n",
      "0: Encoding Loss 45.14978790283203, Transition Loss 14.643656730651855, Classifier Loss 0.564001202583313, Total Loss 420.52716064453125\n",
      "0: Encoding Loss 46.25996780395508, Transition Loss 16.026954650878906, Classifier Loss 0.5313873291015625, Total Loss 426.4238586425781\n",
      "0: Encoding Loss 48.53215026855469, Transition Loss 16.504579544067383, Classifier Loss 0.5471708178520203, Total Loss 446.2751770019531\n",
      "0: Encoding Loss 50.63357162475586, Transition Loss 17.302181243896484, Classifier Loss 0.5615444779396057, Total Loss 464.6834716796875\n",
      "0: Encoding Loss 50.54837417602539, Transition Loss 19.863197326660156, Classifier Loss 0.5641955137252808, Total Loss 464.7791748046875\n",
      "0: Encoding Loss 48.364383697509766, Transition Loss 16.098669052124023, Classifier Loss 0.5224564075469971, Total Loss 442.38043212890625\n",
      "0: Encoding Loss 51.2012939453125, Transition Loss 17.388648986816406, Classifier Loss 0.5638257265090942, Total Loss 469.47064208984375\n",
      "0: Encoding Loss 44.201026916503906, Transition Loss 13.755617141723633, Classifier Loss 0.5240071415901184, Total Loss 408.76007080078125\n",
      "0: Encoding Loss 46.68531036376953, Transition Loss 16.186687469482422, Classifier Loss 0.5686057209968567, Total Loss 433.58038330078125\n",
      "0: Encoding Loss 41.90968322753906, Transition Loss 12.985817909240723, Classifier Loss 0.5378743410110474, Total Loss 391.6620788574219\n",
      "0: Encoding Loss 43.653133392333984, Transition Loss 14.86121940612793, Classifier Loss 0.5674604773521423, Total Loss 408.943359375\n",
      "0: Encoding Loss 47.84513473510742, Transition Loss 14.707900047302246, Classifier Loss 0.5479574203491211, Total Loss 440.4984130859375\n",
      "0: Encoding Loss 50.276771545410156, Transition Loss 17.039348602294922, Classifier Loss 0.550335705280304, Total Loss 460.6556091308594\n",
      "0: Encoding Loss 50.813480377197266, Transition Loss 16.468107223510742, Classifier Loss 0.5735858082771301, Total Loss 467.1600341796875\n",
      "0: Encoding Loss 43.02046585083008, Transition Loss 12.023821830749512, Classifier Loss 0.5335262417793274, Total Loss 399.9211120605469\n",
      "0: Encoding Loss 47.004905700683594, Transition Loss 15.3543701171875, Classifier Loss 0.5763973593711853, Total Loss 436.7498779296875\n",
      "0: Encoding Loss 49.23672103881836, Transition Loss 14.974136352539062, Classifier Loss 0.5564289689064026, Total Loss 452.531494140625\n",
      "0: Encoding Loss 46.800846099853516, Transition Loss 15.833385467529297, Classifier Loss 0.55635666847229, Total Loss 433.2091064453125\n",
      "0: Encoding Loss 47.72898483276367, Transition Loss 16.188432693481445, Classifier Loss 0.5684773325920105, Total Loss 441.9173278808594\n",
      "0: Encoding Loss 46.96120071411133, Transition Loss 13.86322021484375, Classifier Loss 0.5616199970245361, Total Loss 434.6242370605469\n",
      "0: Encoding Loss 45.12503433227539, Transition Loss 14.921371459960938, Classifier Loss 0.5633190274238586, Total Loss 420.31646728515625\n",
      "0: Encoding Loss 43.98775863647461, Transition Loss 11.742694854736328, Classifier Loss 0.5418972969055176, Total Loss 408.4403381347656\n",
      "0: Encoding Loss 49.78728103637695, Transition Loss 15.749404907226562, Classifier Loss 0.5521596074104309, Total Loss 456.6640930175781\n",
      "0: Encoding Loss 46.906131744384766, Transition Loss 12.41122817993164, Classifier Loss 0.5617516040802002, Total Loss 433.9064636230469\n",
      "0: Encoding Loss 46.16850662231445, Transition Loss 13.039328575134277, Classifier Loss 0.5598610639572144, Total Loss 427.9420471191406\n",
      "0: Encoding Loss 45.02549743652344, Transition Loss 12.200348854064941, Classifier Loss 0.5502668023109436, Total Loss 417.67071533203125\n",
      "0: Encoding Loss 41.829837799072266, Transition Loss 11.50096321105957, Classifier Loss 0.5524629950523376, Total Loss 392.1852111816406\n",
      "0: Encoding Loss 43.7734375, Transition Loss 10.023633003234863, Classifier Loss 0.5653172731399536, Total Loss 408.7239685058594\n",
      "0: Encoding Loss 47.132179260253906, Transition Loss 14.647613525390625, Classifier Loss 0.5371740460395813, Total Loss 433.7043762207031\n",
      "0: Encoding Loss 48.981014251708984, Transition Loss 16.101892471313477, Classifier Loss 0.5667389035224915, Total Loss 451.74237060546875\n",
      "0: Encoding Loss 46.93938064575195, Transition Loss 12.44101619720459, Classifier Loss 0.5470807552337646, Total Loss 432.7113037109375\n",
      "0: Encoding Loss 41.02910232543945, Transition Loss 9.238251686096191, Classifier Loss 0.5503455400466919, Total Loss 385.1150207519531\n",
      "0: Encoding Loss 46.421630859375, Transition Loss 13.993119239807129, Classifier Loss 0.5548439025878906, Total Loss 429.6560363769531\n",
      "0: Encoding Loss 42.859867095947266, Transition Loss 11.548837661743164, Classifier Loss 0.521345317363739, Total Loss 397.3232116699219\n",
      "0: Encoding Loss 39.938602447509766, Transition Loss 8.021960258483887, Classifier Loss 0.5455089211463928, Total Loss 375.66412353515625\n",
      "0: Encoding Loss 43.26443862915039, Transition Loss 11.51230239868164, Classifier Loss 0.5658560991287231, Total Loss 405.0035705566406\n",
      "0: Encoding Loss 46.12583541870117, Transition Loss 12.484720230102539, Classifier Loss 0.531427264213562, Total Loss 424.6463623046875\n",
      "0: Encoding Loss 42.40929412841797, Transition Loss 9.915383338928223, Classifier Loss 0.5219753384590149, Total Loss 393.4549865722656\n",
      "0: Encoding Loss 45.401939392089844, Transition Loss 12.727669715881348, Classifier Loss 0.544674813747406, Total Loss 420.2285461425781\n",
      "0: Encoding Loss 44.50302505493164, Transition Loss 11.362411499023438, Classifier Loss 0.546198844909668, Total Loss 412.91656494140625\n",
      "0: Encoding Loss 43.46492385864258, Transition Loss 11.471714973449707, Classifier Loss 0.553915798664093, Total Loss 405.4053039550781\n",
      "0: Encoding Loss 43.037742614746094, Transition Loss 11.134526252746582, Classifier Loss 0.549409031867981, Total Loss 401.4697570800781\n",
      "0: Encoding Loss 48.01158905029297, Transition Loss 11.649658203125, Classifier Loss 0.5248783826828003, Total Loss 438.9104919433594\n",
      "0: Encoding Loss 43.240909576416016, Transition Loss 11.144798278808594, Classifier Loss 0.5696765780448914, Total Loss 405.1239013671875\n",
      "0: Encoding Loss 51.28715896606445, Transition Loss 14.358647346496582, Classifier Loss 0.5347000956535339, Total Loss 466.6390075683594\n",
      "0: Encoding Loss 41.059757232666016, Transition Loss 9.950517654418945, Classifier Loss 0.5577507019042969, Total Loss 386.24322509765625\n",
      "0: Encoding Loss 39.941226959228516, Transition Loss 8.526688575744629, Classifier Loss 0.5481091737747192, Total Loss 376.04608154296875\n",
      "0: Encoding Loss 43.3937873840332, Transition Loss 9.917378425598145, Classifier Loss 0.5547211170196533, Total Loss 404.60589599609375\n",
      "0: Encoding Loss 42.508750915527344, Transition Loss 10.392938613891602, Classifier Loss 0.5448702573776245, Total Loss 396.6356506347656\n",
      "0: Encoding Loss 40.22635269165039, Transition Loss 9.085538864135742, Classifier Loss 0.5500677824020386, Total Loss 378.63470458984375\n",
      "0: Encoding Loss 39.368080139160156, Transition Loss 9.479497909545898, Classifier Loss 0.5377295017242432, Total Loss 370.6134948730469\n",
      "0: Encoding Loss 44.808692932128906, Transition Loss 11.107854843139648, Classifier Loss 0.5591908097267151, Total Loss 416.61016845703125\n",
      "0: Encoding Loss 39.72212600708008, Transition Loss 8.704705238342285, Classifier Loss 0.5406907796859741, Total Loss 373.5870056152344\n",
      "0: Encoding Loss 43.56393814086914, Transition Loss 10.485708236694336, Classifier Loss 0.5529431700706482, Total Loss 405.9029541015625\n",
      "0: Encoding Loss 40.60686492919922, Transition Loss 9.252165794372559, Classifier Loss 0.5497918128967285, Total Loss 381.6845703125\n",
      "0: Encoding Loss 44.00074768066406, Transition Loss 10.553905487060547, Classifier Loss 0.5594919919967651, Total Loss 410.0659484863281\n",
      "0: Encoding Loss 41.76504135131836, Transition Loss 9.67973518371582, Classifier Loss 0.5458433032035828, Total Loss 390.6405944824219\n",
      "0: Encoding Loss 45.90374755859375, Transition Loss 11.89729118347168, Classifier Loss 0.5491641759872437, Total Loss 424.5258483886719\n",
      "0: Encoding Loss 42.56618118286133, Transition Loss 9.537886619567871, Classifier Loss 0.5635992288589478, Total Loss 398.79693603515625\n",
      "0: Encoding Loss 43.02984619140625, Transition Loss 10.256855010986328, Classifier Loss 0.5374988317489624, Total Loss 400.0400085449219\n",
      "0: Encoding Loss 43.77085494995117, Transition Loss 10.499236106872559, Classifier Loss 0.5483649969100952, Total Loss 407.1031799316406\n",
      "0: Encoding Loss 42.3693962097168, Transition Loss 10.255537986755371, Classifier Loss 0.545138955116272, Total Loss 395.5201721191406\n",
      "0: Encoding Loss 42.33600997924805, Transition Loss 10.253036499023438, Classifier Loss 0.5544525980949402, Total Loss 396.1839294433594\n",
      "0: Encoding Loss 40.050724029541016, Transition Loss 9.538607597351074, Classifier Loss 0.5538105368614197, Total Loss 377.6945495605469\n",
      "0: Encoding Loss 42.16453170776367, Transition Loss 9.32722282409668, Classifier Loss 0.553588330745697, Total Loss 394.54052734375\n",
      "0: Encoding Loss 42.95547866821289, Transition Loss 10.541954040527344, Classifier Loss 0.5411498546600342, Total Loss 399.8672180175781\n",
      "0: Encoding Loss 41.61486053466797, Transition Loss 9.041830062866211, Classifier Loss 0.5331286787986755, Total Loss 388.0401611328125\n",
      "0: Encoding Loss 41.431034088134766, Transition Loss 8.832422256469727, Classifier Loss 0.5587610602378845, Total Loss 389.0908508300781\n",
      "0: Encoding Loss 37.30867385864258, Transition Loss 6.8865180015563965, Classifier Loss 0.5280647277832031, Total Loss 352.6531677246094\n",
      "0: Encoding Loss 47.893680572509766, Transition Loss 12.353408813476562, Classifier Loss 0.5660581588745117, Total Loss 442.2259216308594\n",
      "0: Encoding Loss 44.65574645996094, Transition Loss 10.167484283447266, Classifier Loss 0.5427597761154175, Total Loss 413.5554504394531\n",
      "0: Encoding Loss 47.50022888183594, Transition Loss 10.92292308807373, Classifier Loss 0.5464381575584412, Total Loss 436.8302001953125\n",
      "0: Encoding Loss 44.61725616455078, Transition Loss 10.176215171813965, Classifier Loss 0.5510210394859314, Total Loss 414.0754089355469\n",
      "0: Encoding Loss 45.07550048828125, Transition Loss 10.37271499633789, Classifier Loss 0.5542359352111816, Total Loss 418.1021423339844\n",
      "0: Encoding Loss 39.86084747314453, Transition Loss 8.604278564453125, Classifier Loss 0.5383390784263611, Total Loss 374.4415588378906\n",
      "0: Encoding Loss 44.181026458740234, Transition Loss 10.409436225891113, Classifier Loss 0.5559102296829224, Total Loss 411.1211242675781\n",
      "0: Encoding Loss 39.618263244628906, Transition Loss 8.220020294189453, Classifier Loss 0.5411444306373596, Total Loss 372.7045593261719\n",
      "0: Encoding Loss 42.95337677001953, Transition Loss 9.404447555541992, Classifier Loss 0.5414193868637085, Total Loss 399.64984130859375\n",
      "0: Encoding Loss 38.26453399658203, Transition Loss 7.041388034820557, Classifier Loss 0.5339648723602295, Total Loss 360.9210205078125\n",
      "0: Encoding Loss 43.27275466918945, Transition Loss 9.206869125366211, Classifier Loss 0.5367342233657837, Total Loss 401.69683837890625\n",
      "0: Encoding Loss 42.17131805419922, Transition Loss 9.13792610168457, Classifier Loss 0.5327073931694031, Total Loss 392.4689025878906\n",
      "0: Encoding Loss 39.599266052246094, Transition Loss 7.8958821296691895, Classifier Loss 0.5306430459022522, Total Loss 371.4376220703125\n",
      "0: Encoding Loss 42.214576721191406, Transition Loss 10.045905113220215, Classifier Loss 0.5285763740539551, Total Loss 392.58343505859375\n",
      "0: Encoding Loss 44.27733612060547, Transition Loss 10.043105125427246, Classifier Loss 0.5339067578315735, Total Loss 409.6180114746094\n",
      "0: Encoding Loss 44.51277160644531, Transition Loss 9.427837371826172, Classifier Loss 0.5297638177871704, Total Loss 410.964111328125\n",
      "0: Encoding Loss 42.35057067871094, Transition Loss 8.994938850402832, Classifier Loss 0.5425534248352051, Total Loss 394.85888671875\n",
      "1: Encoding Loss 43.86061477661133, Transition Loss 9.017426490783691, Classifier Loss 0.5465032458305359, Total Loss 407.3387451171875\n",
      "1: Encoding Loss 44.82118606567383, Transition Loss 10.013202667236328, Classifier Loss 0.5402997136116028, Total Loss 414.60211181640625\n",
      "1: Encoding Loss 42.78722381591797, Transition Loss 10.549627304077148, Classifier Loss 0.537818968296051, Total Loss 398.18963623046875\n",
      "1: Encoding Loss 42.78167724609375, Transition Loss 8.500822067260742, Classifier Loss 0.5336443781852722, Total Loss 397.3180236816406\n",
      "1: Encoding Loss 41.293209075927734, Transition Loss 9.046664237976074, Classifier Loss 0.5592743754386902, Total Loss 388.0824279785156\n",
      "1: Encoding Loss 42.33744812011719, Transition Loss 9.20166301727295, Classifier Loss 0.5670432448387146, Total Loss 397.2442321777344\n",
      "1: Encoding Loss 40.25187301635742, Transition Loss 8.699087142944336, Classifier Loss 0.5588626861572266, Total Loss 379.64105224609375\n",
      "1: Encoding Loss 43.993080139160156, Transition Loss 10.838651657104492, Classifier Loss 0.5561296939849854, Total Loss 409.725341796875\n",
      "1: Encoding Loss 43.242130279541016, Transition Loss 9.600881576538086, Classifier Loss 0.5421836376190186, Total Loss 402.0755615234375\n",
      "1: Encoding Loss 45.18574142456055, Transition Loss 11.47995376586914, Classifier Loss 0.5409899353981018, Total Loss 417.88092041015625\n",
      "1: Encoding Loss 42.29987335205078, Transition Loss 10.1621675491333, Classifier Loss 0.533254086971283, Total Loss 393.7568359375\n",
      "1: Encoding Loss 44.013004302978516, Transition Loss 9.465279579162598, Classifier Loss 0.5249158143997192, Total Loss 406.4886779785156\n",
      "1: Encoding Loss 40.983272552490234, Transition Loss 8.253746032714844, Classifier Loss 0.5482093095779419, Total Loss 384.3378601074219\n",
      "1: Encoding Loss 42.206871032714844, Transition Loss 9.872200012207031, Classifier Loss 0.5502738952636719, Total Loss 394.65679931640625\n",
      "1: Encoding Loss 41.46549987792969, Transition Loss 9.079222679138184, Classifier Loss 0.5557337403297424, Total Loss 389.11322021484375\n",
      "1: Encoding Loss 40.30470275878906, Transition Loss 8.877022743225098, Classifier Loss 0.5371479392051697, Total Loss 377.92779541015625\n",
      "1: Encoding Loss 39.4181022644043, Transition Loss 8.41583251953125, Classifier Loss 0.5404841899871826, Total Loss 371.076416015625\n",
      "1: Encoding Loss 41.60150909423828, Transition Loss 8.81916332244873, Classifier Loss 0.5192352533340454, Total Loss 386.4994201660156\n",
      "1: Encoding Loss 38.310123443603516, Transition Loss 7.950440406799316, Classifier Loss 0.5305305123329163, Total Loss 361.1241149902344\n",
      "1: Encoding Loss 43.98301696777344, Transition Loss 9.993108749389648, Classifier Loss 0.5358956456184387, Total Loss 407.45233154296875\n",
      "1: Encoding Loss 41.55154800415039, Transition Loss 8.443319320678711, Classifier Loss 0.5564543008804321, Total Loss 389.7464599609375\n",
      "1: Encoding Loss 39.33454513549805, Transition Loss 7.9324188232421875, Classifier Loss 0.5424475073814392, Total Loss 370.5075988769531\n",
      "1: Encoding Loss 39.067893981933594, Transition Loss 8.385520935058594, Classifier Loss 0.5276726484298706, Total Loss 366.987548828125\n",
      "1: Encoding Loss 39.29153060913086, Transition Loss 7.462491989135742, Classifier Loss 0.5544922351837158, Total Loss 371.2739562988281\n",
      "1: Encoding Loss 39.88644790649414, Transition Loss 8.383066177368164, Classifier Loss 0.5374659299850464, Total Loss 374.5147705078125\n",
      "1: Encoding Loss 45.28937530517578, Transition Loss 10.391012191772461, Classifier Loss 0.5465087890625, Total Loss 419.0440979003906\n",
      "1: Encoding Loss 43.51622772216797, Transition Loss 10.268511772155762, Classifier Loss 0.5505508780479431, Total Loss 405.2386474609375\n",
      "1: Encoding Loss 38.97087478637695, Transition Loss 8.699542999267578, Classifier Loss 0.551121711730957, Total Loss 368.61907958984375\n",
      "1: Encoding Loss 41.67273712158203, Transition Loss 8.474543571472168, Classifier Loss 0.5299721956253052, Total Loss 388.07403564453125\n",
      "1: Encoding Loss 41.836036682128906, Transition Loss 9.152124404907227, Classifier Loss 0.5223644375801086, Total Loss 388.7551574707031\n",
      "1: Encoding Loss 42.36675262451172, Transition Loss 8.899572372436523, Classifier Loss 0.5284938216209412, Total Loss 393.5633544921875\n",
      "1: Encoding Loss 37.08699035644531, Transition Loss 7.1373515129089355, Classifier Loss 0.5625492334365845, Total Loss 354.3782958984375\n",
      "1: Encoding Loss 41.89243698120117, Transition Loss 9.78555679321289, Classifier Loss 0.5547004342079163, Total Loss 392.566650390625\n",
      "1: Encoding Loss 40.57146072387695, Transition Loss 8.086853981018066, Classifier Loss 0.5611438751220703, Total Loss 382.3034362792969\n",
      "1: Encoding Loss 40.3382682800293, Transition Loss 8.718437194824219, Classifier Loss 0.5430978536605835, Total Loss 378.7596130371094\n",
      "1: Encoding Loss 36.43088150024414, Transition Loss 6.968444347381592, Classifier Loss 0.5513588190078735, Total Loss 347.97662353515625\n",
      "1: Encoding Loss 41.13987350463867, Transition Loss 8.866053581237793, Classifier Loss 0.5387802124023438, Total Loss 384.7702331542969\n",
      "1: Encoding Loss 40.64944076538086, Transition Loss 9.782774925231934, Classifier Loss 0.5439950823783875, Total Loss 381.55157470703125\n",
      "1: Encoding Loss 39.901363372802734, Transition Loss 8.076299667358398, Classifier Loss 0.527698814868927, Total Loss 373.5960388183594\n",
      "1: Encoding Loss 37.80649948120117, Transition Loss 8.075343132019043, Classifier Loss 0.5298592448234558, Total Loss 357.0530090332031\n",
      "1: Encoding Loss 39.92008590698242, Transition Loss 8.659744262695312, Classifier Loss 0.5224496126174927, Total Loss 373.33758544921875\n",
      "1: Encoding Loss 40.03353500366211, Transition Loss 7.738428115844727, Classifier Loss 0.5465830564498901, Total Loss 376.4742736816406\n",
      "1: Encoding Loss 39.04678726196289, Transition Loss 7.801724433898926, Classifier Loss 0.5480549335479736, Total Loss 368.7401123046875\n",
      "1: Encoding Loss 36.91511535644531, Transition Loss 6.715332508087158, Classifier Loss 0.5443533062934875, Total Loss 351.0993347167969\n",
      "1: Encoding Loss 41.60384750366211, Transition Loss 8.337085723876953, Classifier Loss 0.5475801229476929, Total Loss 389.2562255859375\n",
      "1: Encoding Loss 40.981407165527344, Transition Loss 8.672698974609375, Classifier Loss 0.5546994209289551, Total Loss 385.0557556152344\n",
      "1: Encoding Loss 41.96583557128906, Transition Loss 8.463399887084961, Classifier Loss 0.527201771736145, Total Loss 390.1395568847656\n",
      "1: Encoding Loss 40.7751350402832, Transition Loss 8.581337928771973, Classifier Loss 0.5340967178344727, Total Loss 381.3270263671875\n",
      "1: Encoding Loss 41.08971405029297, Transition Loss 8.598834037780762, Classifier Loss 0.545495331287384, Total Loss 384.9870300292969\n",
      "1: Encoding Loss 39.81767272949219, Transition Loss 8.374329566955566, Classifier Loss 0.5335360765457153, Total Loss 373.5698547363281\n",
      "1: Encoding Loss 38.620296478271484, Transition Loss 6.417285442352295, Classifier Loss 0.5390610694885254, Total Loss 364.15191650390625\n",
      "1: Encoding Loss 37.506534576416016, Transition Loss 7.0313286781311035, Classifier Loss 0.5336775183677673, Total Loss 354.8263244628906\n",
      "1: Encoding Loss 35.78914260864258, Transition Loss 6.430234909057617, Classifier Loss 0.5316617488861084, Total Loss 340.7653503417969\n",
      "1: Encoding Loss 35.54887771606445, Transition Loss 6.888091087341309, Classifier Loss 0.5207678079605103, Total Loss 337.8454284667969\n",
      "1: Encoding Loss 37.90802001953125, Transition Loss 7.973481178283691, Classifier Loss 0.5438262224197388, Total Loss 359.2414855957031\n",
      "1: Encoding Loss 36.41862106323242, Transition Loss 6.942499160766602, Classifier Loss 0.5412874221801758, Total Loss 346.8662109375\n",
      "1: Encoding Loss 38.464210510253906, Transition Loss 7.410491466522217, Classifier Loss 0.5283783674240112, Total Loss 362.0335998535156\n",
      "1: Encoding Loss 39.13274002075195, Transition Loss 7.745946884155273, Classifier Loss 0.5119036436080933, Total Loss 365.8014831542969\n",
      "1: Encoding Loss 40.77054977416992, Transition Loss 8.392407417297363, Classifier Loss 0.5509147644042969, Total Loss 382.934326171875\n",
      "1: Encoding Loss 37.5488395690918, Transition Loss 7.062464714050293, Classifier Loss 0.548988938331604, Total Loss 356.7021179199219\n",
      "1: Encoding Loss 39.64081954956055, Transition Loss 8.081924438476562, Classifier Loss 0.5187543630599976, Total Loss 370.6183776855469\n",
      "1: Encoding Loss 40.93268585205078, Transition Loss 8.494430541992188, Classifier Loss 0.5389529466629028, Total Loss 383.0556640625\n",
      "1: Encoding Loss 38.68650817871094, Transition Loss 8.263373374938965, Classifier Loss 0.5198966860771179, Total Loss 363.1344299316406\n",
      "1: Encoding Loss 39.7185173034668, Transition Loss 8.421494483947754, Classifier Loss 0.5440233945846558, Total Loss 373.83477783203125\n",
      "1: Encoding Loss 35.38836669921875, Transition Loss 6.763404846191406, Classifier Loss 0.5085323452949524, Total Loss 335.3128662109375\n",
      "1: Encoding Loss 38.024173736572266, Transition Loss 7.090099334716797, Classifier Loss 0.5284382104873657, Total Loss 358.4552307128906\n",
      "1: Encoding Loss 39.12220001220703, Transition Loss 7.507007598876953, Classifier Loss 0.5275479555130005, Total Loss 367.2337951660156\n",
      "1: Encoding Loss 41.74574661254883, Transition Loss 8.81414794921875, Classifier Loss 0.5239086151123047, Total Loss 388.1196594238281\n",
      "1: Encoding Loss 43.19632339477539, Transition Loss 9.998007774353027, Classifier Loss 0.519564688205719, Total Loss 399.52667236328125\n",
      "1: Encoding Loss 37.55094909667969, Transition Loss 7.900042533874512, Classifier Loss 0.5337585210800171, Total Loss 355.36346435546875\n",
      "1: Encoding Loss 38.106285095214844, Transition Loss 7.87274694442749, Classifier Loss 0.5387153625488281, Total Loss 360.2964172363281\n",
      "1: Encoding Loss 36.09770965576172, Transition Loss 6.942396640777588, Classifier Loss 0.5282524824142456, Total Loss 342.9954528808594\n",
      "1: Encoding Loss 36.996986389160156, Transition Loss 7.524017333984375, Classifier Loss 0.5344756841659546, Total Loss 350.9282531738281\n",
      "1: Encoding Loss 38.98629379272461, Transition Loss 8.20506763458252, Classifier Loss 0.5275424718856812, Total Loss 366.2856140136719\n",
      "1: Encoding Loss 40.10042953491211, Transition Loss 8.696889877319336, Classifier Loss 0.5430271625518799, Total Loss 376.84552001953125\n",
      "1: Encoding Loss 40.19420623779297, Transition Loss 8.63369369506836, Classifier Loss 0.5443081259727478, Total Loss 377.71124267578125\n",
      "1: Encoding Loss 41.211692810058594, Transition Loss 8.620253562927246, Classifier Loss 0.5173434615135193, Total Loss 383.1519775390625\n",
      "1: Encoding Loss 38.686519622802734, Transition Loss 8.630434036254883, Classifier Loss 0.5310544371604919, Total Loss 364.32366943359375\n",
      "1: Encoding Loss 36.600528717041016, Transition Loss 7.544740200042725, Classifier Loss 0.5099033117294312, Total Loss 345.3034973144531\n",
      "1: Encoding Loss 36.96648406982422, Transition Loss 8.041004180908203, Classifier Loss 0.517361044883728, Total Loss 349.07623291015625\n",
      "1: Encoding Loss 36.15876388549805, Transition Loss 7.1865129470825195, Classifier Loss 0.5247728824615479, Total Loss 343.1847229003906\n",
      "1: Encoding Loss 37.59880447387695, Transition Loss 7.541390419006348, Classifier Loss 0.5250241756439209, Total Loss 354.8011169433594\n",
      "1: Encoding Loss 38.83959197998047, Transition Loss 8.140987396240234, Classifier Loss 0.5197455883026123, Total Loss 364.31951904296875\n",
      "1: Encoding Loss 39.89286804199219, Transition Loss 8.745116233825684, Classifier Loss 0.5431783199310303, Total Loss 375.2098083496094\n",
      "1: Encoding Loss 38.80317306518555, Transition Loss 8.215794563293457, Classifier Loss 0.5322139263153076, Total Loss 365.2899475097656\n",
      "1: Encoding Loss 35.570430755615234, Transition Loss 6.767297267913818, Classifier Loss 0.5016995072364807, Total Loss 336.08685302734375\n",
      "1: Encoding Loss 38.15711975097656, Transition Loss 7.969679832458496, Classifier Loss 0.5293018221855164, Total Loss 359.78106689453125\n",
      "1: Encoding Loss 38.981361389160156, Transition Loss 8.979747772216797, Classifier Loss 0.5337060689926147, Total Loss 367.0174560546875\n",
      "1: Encoding Loss 39.42202377319336, Transition Loss 8.420287132263184, Classifier Loss 0.5400181412696838, Total Loss 371.0620422363281\n",
      "1: Encoding Loss 40.01591491699219, Transition Loss 8.467124938964844, Classifier Loss 0.5483407974243164, Total Loss 376.6548156738281\n",
      "1: Encoding Loss 38.35135269165039, Transition Loss 7.914822101593018, Classifier Loss 0.5455066561698914, Total Loss 362.9444580078125\n",
      "1: Encoding Loss 38.32372283935547, Transition Loss 9.11074161529541, Classifier Loss 0.5406158566474915, Total Loss 362.4735412597656\n",
      "1: Encoding Loss 36.58601379394531, Transition Loss 7.820279121398926, Classifier Loss 0.5067451596260071, Total Loss 344.9266662597656\n",
      "1: Encoding Loss 39.88553237915039, Transition Loss 8.943695068359375, Classifier Loss 0.5437867641448975, Total Loss 375.2516784667969\n",
      "1: Encoding Loss 38.719627380371094, Transition Loss 7.920680999755859, Classifier Loss 0.5278805494308472, Total Loss 364.1292419433594\n",
      "1: Encoding Loss 36.36733627319336, Transition Loss 6.570202827453613, Classifier Loss 0.5480135083198547, Total Loss 347.0540771484375\n",
      "1: Encoding Loss 37.10270690917969, Transition Loss 8.360018730163574, Classifier Loss 0.5140097141265869, Total Loss 349.8946228027344\n",
      "1: Encoding Loss 36.5975341796875, Transition Loss 7.148777008056641, Classifier Loss 0.5486199259757996, Total Loss 349.072021484375\n",
      "1: Encoding Loss 36.38447189331055, Transition Loss 7.514260292053223, Classifier Loss 0.5513346791267395, Total Loss 347.7120666503906\n",
      "1: Encoding Loss 41.1541633605957, Transition Loss 8.37625503540039, Classifier Loss 0.5312685966491699, Total Loss 384.0354309082031\n",
      "1: Encoding Loss 39.32276153564453, Transition Loss 8.666461944580078, Classifier Loss 0.534991443157196, Total Loss 369.8145446777344\n",
      "1: Encoding Loss 37.945335388183594, Transition Loss 7.87977933883667, Classifier Loss 0.5283633470535278, Total Loss 357.9750061035156\n",
      "1: Encoding Loss 34.830482482910156, Transition Loss 7.300796031951904, Classifier Loss 0.5323591828346252, Total Loss 333.3399353027344\n",
      "1: Encoding Loss 37.136573791503906, Transition Loss 9.014734268188477, Classifier Loss 0.531509518623352, Total Loss 352.0464782714844\n",
      "1: Encoding Loss 37.09072494506836, Transition Loss 7.66036319732666, Classifier Loss 0.5173161029815674, Total Loss 349.9894714355469\n",
      "1: Encoding Loss 33.10190200805664, Transition Loss 6.945155620574951, Classifier Loss 0.5308464765548706, Total Loss 319.2889099121094\n",
      "1: Encoding Loss 36.46828079223633, Transition Loss 7.339522838592529, Classifier Loss 0.5382933616638184, Total Loss 347.0434875488281\n",
      "1: Encoding Loss 38.340599060058594, Transition Loss 8.264094352722168, Classifier Loss 0.5229558944702148, Total Loss 360.6732482910156\n",
      "1: Encoding Loss 34.97063064575195, Transition Loss 6.373382568359375, Classifier Loss 0.5008509159088135, Total Loss 331.12481689453125\n",
      "1: Encoding Loss 38.81694793701172, Transition Loss 9.123313903808594, Classifier Loss 0.5307186841964722, Total Loss 365.4321594238281\n",
      "1: Encoding Loss 37.32167053222656, Transition Loss 8.025099754333496, Classifier Loss 0.5110318064689636, Total Loss 351.28155517578125\n",
      "1: Encoding Loss 38.530941009521484, Transition Loss 8.671202659606934, Classifier Loss 0.5249249935150146, Total Loss 362.4742736816406\n",
      "1: Encoding Loss 37.184791564941406, Transition Loss 7.8494648933410645, Classifier Loss 0.5427760481834412, Total Loss 353.3258056640625\n",
      "1: Encoding Loss 39.666080474853516, Transition Loss 7.9559407234191895, Classifier Loss 0.5126289129257202, Total Loss 370.1827087402344\n",
      "1: Encoding Loss 37.52509307861328, Transition Loss 7.332418918609619, Classifier Loss 0.540336012840271, Total Loss 355.7008361816406\n",
      "1: Encoding Loss 42.86381912231445, Transition Loss 10.899377822875977, Classifier Loss 0.514590322971344, Total Loss 396.5494689941406\n",
      "1: Encoding Loss 36.32774353027344, Transition Loss 7.807748794555664, Classifier Loss 0.5281982421875, Total Loss 345.0033264160156\n",
      "1: Encoding Loss 35.02063751220703, Transition Loss 6.537846565246582, Classifier Loss 0.5205695629119873, Total Loss 333.52960205078125\n",
      "1: Encoding Loss 37.04815673828125, Transition Loss 7.7787885665893555, Classifier Loss 0.5159997940063477, Total Loss 349.5409851074219\n",
      "1: Encoding Loss 34.995094299316406, Transition Loss 7.55740213394165, Classifier Loss 0.541339099407196, Total Loss 335.60614013671875\n",
      "1: Encoding Loss 36.4409294128418, Transition Loss 6.114716053009033, Classifier Loss 0.5300896167755127, Total Loss 345.75933837890625\n",
      "1: Encoding Loss 34.283058166503906, Transition Loss 6.400809288024902, Classifier Loss 0.5380938649177551, Total Loss 329.35400390625\n",
      "1: Encoding Loss 37.42655563354492, Transition Loss 8.616565704345703, Classifier Loss 0.5522798299789429, Total Loss 356.36376953125\n",
      "1: Encoding Loss 34.745723724365234, Transition Loss 6.246282577514648, Classifier Loss 0.5104217529296875, Total Loss 330.2572326660156\n",
      "1: Encoding Loss 38.213985443115234, Transition Loss 7.458771705627441, Classifier Loss 0.532142162322998, Total Loss 360.4178466796875\n",
      "1: Encoding Loss 34.6429328918457, Transition Loss 7.194911479949951, Classifier Loss 0.532431960105896, Total Loss 331.8256530761719\n",
      "1: Encoding Loss 37.0442008972168, Transition Loss 8.29600715637207, Classifier Loss 0.5263293981552124, Total Loss 350.645751953125\n",
      "1: Encoding Loss 35.32480239868164, Transition Loss 7.574336051940918, Classifier Loss 0.5336118936538696, Total Loss 337.4744567871094\n",
      "1: Encoding Loss 37.91207504272461, Transition Loss 9.012711524963379, Classifier Loss 0.5374772548675537, Total Loss 358.84686279296875\n",
      "1: Encoding Loss 35.30672073364258, Transition Loss 8.759309768676758, Classifier Loss 0.5539014339447021, Total Loss 339.59576416015625\n",
      "1: Encoding Loss 37.285850524902344, Transition Loss 8.296165466308594, Classifier Loss 0.5311697721481323, Total Loss 353.06304931640625\n",
      "1: Encoding Loss 35.744483947753906, Transition Loss 8.063199043273926, Classifier Loss 0.525317370891571, Total Loss 340.1002502441406\n",
      "1: Encoding Loss 36.25614547729492, Transition Loss 7.469420909881592, Classifier Loss 0.5253735184669495, Total Loss 344.0804138183594\n",
      "1: Encoding Loss 35.218055725097656, Transition Loss 7.367279052734375, Classifier Loss 0.5254740118980408, Total Loss 335.7652893066406\n",
      "1: Encoding Loss 36.293312072753906, Transition Loss 7.792569637298584, Classifier Loss 0.5210199952125549, Total Loss 344.0069885253906\n",
      "1: Encoding Loss 35.67165756225586, Transition Loss 7.558986186981201, Classifier Loss 0.5351846218109131, Total Loss 340.4035339355469\n",
      "1: Encoding Loss 34.87794494628906, Transition Loss 7.398290634155273, Classifier Loss 0.5185596346855164, Total Loss 332.3591613769531\n",
      "1: Encoding Loss 35.5323486328125, Transition Loss 6.989322662353516, Classifier Loss 0.5145666599273682, Total Loss 337.1133117675781\n",
      "1: Encoding Loss 33.76679992675781, Transition Loss 6.3156352043151855, Classifier Loss 0.520881175994873, Total Loss 323.48565673828125\n",
      "1: Encoding Loss 31.59489631652832, Transition Loss 6.210103988647461, Classifier Loss 0.5056959390640259, Total Loss 304.5708312988281\n",
      "1: Encoding Loss 40.98797607421875, Transition Loss 8.815155982971191, Classifier Loss 0.537101149559021, Total Loss 383.376953125\n",
      "1: Encoding Loss 38.32667541503906, Transition Loss 8.347444534301758, Classifier Loss 0.5252230763435364, Total Loss 360.8052062988281\n",
      "1: Encoding Loss 37.205936431884766, Transition Loss 8.581955909729004, Classifier Loss 0.5186916589736938, Total Loss 351.2330627441406\n",
      "1: Encoding Loss 37.2130012512207, Transition Loss 8.92056655883789, Classifier Loss 0.5331016182899475, Total Loss 352.79827880859375\n",
      "1: Encoding Loss 37.91179656982422, Transition Loss 8.163074493408203, Classifier Loss 0.5265905857086182, Total Loss 357.5860900878906\n",
      "1: Encoding Loss 33.51539611816406, Transition Loss 7.175235748291016, Classifier Loss 0.5260495543479919, Total Loss 322.1631774902344\n",
      "1: Encoding Loss 36.588600158691406, Transition Loss 7.848150253295898, Classifier Loss 0.5396462678909302, Total Loss 348.2430725097656\n",
      "1: Encoding Loss 33.07175827026367, Transition Loss 5.828967094421387, Classifier Loss 0.5284196734428406, Total Loss 318.58184814453125\n",
      "1: Encoding Loss 36.052879333496094, Transition Loss 7.583930969238281, Classifier Loss 0.5236032009124756, Total Loss 342.3001708984375\n",
      "1: Encoding Loss 31.945812225341797, Transition Loss 6.147111892700195, Classifier Loss 0.5037493705749512, Total Loss 307.1708984375\n",
      "1: Encoding Loss 36.4545783996582, Transition Loss 6.29547643661499, Classifier Loss 0.5133657455444336, Total Loss 344.2322998046875\n",
      "1: Encoding Loss 35.77814483642578, Transition Loss 8.976696968078613, Classifier Loss 0.5155365467071533, Total Loss 339.57415771484375\n",
      "1: Encoding Loss 33.3541145324707, Transition Loss 6.895271301269531, Classifier Loss 0.5041314959526062, Total Loss 318.6251220703125\n",
      "1: Encoding Loss 37.16945266723633, Transition Loss 9.064806938171387, Classifier Loss 0.5195050239562988, Total Loss 351.11907958984375\n",
      "1: Encoding Loss 37.357181549072266, Transition Loss 9.212162017822266, Classifier Loss 0.51356440782547, Total Loss 352.05633544921875\n",
      "1: Encoding Loss 36.40116500854492, Transition Loss 9.684562683105469, Classifier Loss 0.5055251717567444, Total Loss 343.6987609863281\n",
      "1: Encoding Loss 33.6258430480957, Transition Loss 9.429755210876465, Classifier Loss 0.518033504486084, Total Loss 322.696044921875\n",
      "2: Encoding Loss 36.72111511230469, Transition Loss 7.98182487487793, Classifier Loss 0.5211295485496521, Total Loss 347.4782409667969\n",
      "2: Encoding Loss 37.18223190307617, Transition Loss 8.772747993469238, Classifier Loss 0.5218868851661682, Total Loss 351.4010925292969\n",
      "2: Encoding Loss 38.674461364746094, Transition Loss 9.196869850158691, Classifier Loss 0.5220595598220825, Total Loss 363.4410705566406\n",
      "2: Encoding Loss 34.05534362792969, Transition Loss 7.7418904304504395, Classifier Loss 0.5226320028305054, Total Loss 326.2543029785156\n",
      "2: Encoding Loss 36.50515365600586, Transition Loss 8.2364501953125, Classifier Loss 0.5284534096717834, Total Loss 346.5338439941406\n",
      "2: Encoding Loss 34.89980697631836, Transition Loss 7.97452974319458, Classifier Loss 0.5248299837112427, Total Loss 333.2763671875\n",
      "2: Encoding Loss 34.332420349121094, Transition Loss 7.31890869140625, Classifier Loss 0.51560378074646, Total Loss 327.6835632324219\n",
      "2: Encoding Loss 40.26197052001953, Transition Loss 9.570961952209473, Classifier Loss 0.5454109907150269, Total Loss 378.5510559082031\n",
      "2: Encoding Loss 38.195289611816406, Transition Loss 8.100510597229004, Classifier Loss 0.5300360321998596, Total Loss 360.18603515625\n",
      "2: Encoding Loss 40.876338958740234, Transition Loss 10.36373519897461, Classifier Loss 0.5291811227798462, Total Loss 382.0015869140625\n",
      "2: Encoding Loss 35.77446746826172, Transition Loss 8.763566970825195, Classifier Loss 0.5280255675315857, Total Loss 340.75103759765625\n",
      "2: Encoding Loss 36.64375305175781, Transition Loss 8.51362419128418, Classifier Loss 0.5034006834030151, Total Loss 345.19281005859375\n",
      "2: Encoding Loss 34.11100387573242, Transition Loss 7.266690254211426, Classifier Loss 0.5235657095909119, Total Loss 326.69793701171875\n",
      "2: Encoding Loss 35.76640319824219, Transition Loss 8.37089729309082, Classifier Loss 0.5292953252792358, Total Loss 340.7349548339844\n",
      "2: Encoding Loss 35.53728103637695, Transition Loss 7.964463233947754, Classifier Loss 0.5333594083786011, Total Loss 339.2270812988281\n",
      "2: Encoding Loss 34.962459564208984, Transition Loss 8.498690605163574, Classifier Loss 0.5122062563896179, Total Loss 332.6200256347656\n",
      "2: Encoding Loss 34.297035217285156, Transition Loss 7.778173923492432, Classifier Loss 0.518724799156189, Total Loss 327.80438232421875\n",
      "2: Encoding Loss 35.48176574707031, Transition Loss 8.142683029174805, Classifier Loss 0.5228099822998047, Total Loss 337.763671875\n",
      "2: Encoding Loss 34.41954040527344, Transition Loss 7.742137908935547, Classifier Loss 0.5129956603050232, Total Loss 328.2043151855469\n",
      "2: Encoding Loss 39.18037796020508, Transition Loss 9.358838081359863, Classifier Loss 0.5160702466964722, Total Loss 366.92181396484375\n",
      "2: Encoding Loss 34.61903762817383, Transition Loss 8.443050384521484, Classifier Loss 0.544781506061554, Total Loss 333.1190490722656\n",
      "2: Encoding Loss 35.57415008544922, Transition Loss 7.115785598754883, Classifier Loss 0.5231045484542847, Total Loss 338.32684326171875\n",
      "2: Encoding Loss 33.89883041381836, Transition Loss 9.059666633605957, Classifier Loss 0.5298583507537842, Total Loss 325.9884033203125\n",
      "2: Encoding Loss 32.96314239501953, Transition Loss 6.618463516235352, Classifier Loss 0.5192824006080627, Total Loss 316.95709228515625\n",
      "2: Encoding Loss 34.1456184387207, Transition Loss 7.203372001647949, Classifier Loss 0.5064610838890076, Total Loss 325.2517395019531\n",
      "2: Encoding Loss 39.27509307861328, Transition Loss 9.693338394165039, Classifier Loss 0.5181940197944641, Total Loss 367.95880126953125\n",
      "2: Encoding Loss 36.52497482299805, Transition Loss 10.011235237121582, Classifier Loss 0.5213016867637634, Total Loss 346.33221435546875\n",
      "2: Encoding Loss 33.21978759765625, Transition Loss 8.868842124938965, Classifier Loss 0.5212851762771606, Total Loss 319.66058349609375\n",
      "2: Encoding Loss 34.63291931152344, Transition Loss 8.235665321350098, Classifier Loss 0.5268632769584656, Total Loss 331.3968200683594\n",
      "2: Encoding Loss 36.18998718261719, Transition Loss 9.128469467163086, Classifier Loss 0.5195754766464233, Total Loss 343.3031311035156\n",
      "2: Encoding Loss 36.069725036621094, Transition Loss 8.65532112121582, Classifier Loss 0.5005755424499512, Total Loss 340.3464660644531\n",
      "2: Encoding Loss 31.027021408081055, Transition Loss 7.488548278808594, Classifier Loss 0.5239894986152649, Total Loss 302.1128234863281\n",
      "2: Encoding Loss 36.62105178833008, Transition Loss 9.657405853271484, Classifier Loss 0.5252518653869629, Total Loss 347.4250793457031\n",
      "2: Encoding Loss 34.38843536376953, Transition Loss 6.448225975036621, Classifier Loss 0.5568070411682129, Total Loss 332.07781982421875\n",
      "2: Encoding Loss 35.14579391479492, Transition Loss 8.522110939025879, Classifier Loss 0.5207295417785645, Total Loss 334.9437561035156\n",
      "2: Encoding Loss 33.11724090576172, Transition Loss 6.08009147644043, Classifier Loss 0.529149055480957, Total Loss 319.0688781738281\n",
      "2: Encoding Loss 34.933387756347656, Transition Loss 9.007989883422852, Classifier Loss 0.497506707906723, Total Loss 331.0193786621094\n",
      "2: Encoding Loss 36.38148880004883, Transition Loss 10.123661994934082, Classifier Loss 0.5166845321655273, Total Loss 344.7450866699219\n",
      "2: Encoding Loss 34.683345794677734, Transition Loss 8.272793769836426, Classifier Loss 0.49376922845840454, Total Loss 328.4982604980469\n",
      "2: Encoding Loss 33.29121780395508, Transition Loss 8.411090850830078, Classifier Loss 0.5052143931388855, Total Loss 318.5334167480469\n",
      "2: Encoding Loss 33.886478424072266, Transition Loss 8.814565658569336, Classifier Loss 0.5017644762992859, Total Loss 323.03118896484375\n",
      "2: Encoding Loss 33.431129455566406, Transition Loss 7.035101413726807, Classifier Loss 0.5392399430274963, Total Loss 322.7800598144531\n",
      "2: Encoding Loss 34.297752380371094, Transition Loss 7.084403038024902, Classifier Loss 0.524309515953064, Total Loss 328.2298583984375\n",
      "2: Encoding Loss 33.63064193725586, Transition Loss 5.907853126525879, Classifier Loss 0.5115362405776978, Total Loss 321.3803405761719\n",
      "2: Encoding Loss 34.30451202392578, Transition Loss 8.217117309570312, Classifier Loss 0.5358946323394775, Total Loss 329.66900634765625\n",
      "2: Encoding Loss 36.77125930786133, Transition Loss 9.146434783935547, Classifier Loss 0.5279058814048767, Total Loss 348.7899475097656\n",
      "2: Encoding Loss 33.99952697753906, Transition Loss 9.192336082458496, Classifier Loss 0.5020030736923218, Total Loss 324.0350036621094\n",
      "2: Encoding Loss 33.538509368896484, Transition Loss 9.092747688293457, Classifier Loss 0.5274574160575867, Total Loss 322.87237548828125\n",
      "2: Encoding Loss 34.989627838134766, Transition Loss 8.888379096984863, Classifier Loss 0.5123547911643982, Total Loss 332.93017578125\n",
      "2: Encoding Loss 34.67136764526367, Transition Loss 8.614944458007812, Classifier Loss 0.5053481459617615, Total Loss 329.6287536621094\n",
      "2: Encoding Loss 31.066104888916016, Transition Loss 6.407083034515381, Classifier Loss 0.5109608769416809, Total Loss 300.9063415527344\n",
      "2: Encoding Loss 32.58635711669922, Transition Loss 6.8012237548828125, Classifier Loss 0.5107386112213135, Total Loss 313.125\n",
      "2: Encoding Loss 31.632116317749023, Transition Loss 6.068491458892822, Classifier Loss 0.5165363550186157, Total Loss 305.92425537109375\n",
      "2: Encoding Loss 30.40037727355957, Transition Loss 7.699974060058594, Classifier Loss 0.505936861038208, Total Loss 295.33673095703125\n",
      "2: Encoding Loss 32.41607666015625, Transition Loss 8.33621597290039, Classifier Loss 0.5300973057746887, Total Loss 314.0055847167969\n",
      "2: Encoding Loss 30.753320693969727, Transition Loss 6.887898921966553, Classifier Loss 0.5101065039634705, Total Loss 298.4148254394531\n",
      "2: Encoding Loss 34.316646575927734, Transition Loss 7.775280952453613, Classifier Loss 0.5054366588592529, Total Loss 326.63189697265625\n",
      "2: Encoding Loss 33.8901252746582, Transition Loss 7.831704616546631, Classifier Loss 0.49545198678970337, Total Loss 322.2325439453125\n",
      "2: Encoding Loss 33.72029495239258, Transition Loss 8.148856163024902, Classifier Loss 0.529725193977356, Total Loss 324.3646240234375\n",
      "2: Encoding Loss 32.09745407104492, Transition Loss 7.079216957092285, Classifier Loss 0.5113047957420349, Total Loss 309.325927734375\n",
      "2: Encoding Loss 33.387168884277344, Transition Loss 9.225520133972168, Classifier Loss 0.5145909786224365, Total Loss 320.4015808105469\n",
      "2: Encoding Loss 33.9026985168457, Transition Loss 9.453136444091797, Classifier Loss 0.5132337808609009, Total Loss 324.43560791015625\n",
      "2: Encoding Loss 35.56074523925781, Transition Loss 8.319665908813477, Classifier Loss 0.5133481621742249, Total Loss 337.4847106933594\n",
      "2: Encoding Loss 34.611515045166016, Transition Loss 8.512781143188477, Classifier Loss 0.5130687952041626, Total Loss 329.90155029296875\n",
      "2: Encoding Loss 30.911989212036133, Transition Loss 7.463465690612793, Classifier Loss 0.5098199844360352, Total Loss 299.7706298828125\n",
      "2: Encoding Loss 31.61296844482422, Transition Loss 7.19754695892334, Classifier Loss 0.512119472026825, Total Loss 305.55523681640625\n",
      "2: Encoding Loss 33.35218811035156, Transition Loss 7.938772201538086, Classifier Loss 0.5109327435493469, Total Loss 319.49853515625\n",
      "2: Encoding Loss 35.39603805541992, Transition Loss 9.512295722961426, Classifier Loss 0.49564144015312195, Total Loss 334.6349182128906\n",
      "2: Encoding Loss 36.25955581665039, Transition Loss 10.911202430725098, Classifier Loss 0.508162260055542, Total Loss 343.0749206542969\n",
      "2: Encoding Loss 32.999820709228516, Transition Loss 8.468879699707031, Classifier Loss 0.5139130353927612, Total Loss 317.0836486816406\n",
      "2: Encoding Loss 32.584720611572266, Transition Loss 8.67809009552002, Classifier Loss 0.5286683440208435, Total Loss 315.28021240234375\n",
      "2: Encoding Loss 29.9827938079834, Transition Loss 7.549924373626709, Classifier Loss 0.5125412344932556, Total Loss 292.62646484375\n",
      "2: Encoding Loss 31.260974884033203, Transition Loss 8.184685707092285, Classifier Loss 0.5130699872970581, Total Loss 303.03173828125\n",
      "2: Encoding Loss 34.31789016723633, Transition Loss 9.126352310180664, Classifier Loss 0.5068600177764893, Total Loss 327.05438232421875\n",
      "2: Encoding Loss 33.26921844482422, Transition Loss 9.864004135131836, Classifier Loss 0.5276379585266113, Total Loss 320.890380859375\n",
      "2: Encoding Loss 32.90662384033203, Transition Loss 9.407036781311035, Classifier Loss 0.5509527325630188, Total Loss 320.22967529296875\n",
      "2: Encoding Loss 34.232059478759766, Transition Loss 9.864337921142578, Classifier Loss 0.5015266537666321, Total Loss 325.9820251464844\n",
      "2: Encoding Loss 33.147579193115234, Transition Loss 9.518860816955566, Classifier Loss 0.5031569004058838, Total Loss 317.4001159667969\n",
      "2: Encoding Loss 31.50420570373535, Transition Loss 8.478104591369629, Classifier Loss 0.48718592524528503, Total Loss 302.4478454589844\n",
      "2: Encoding Loss 33.86503601074219, Transition Loss 8.777228355407715, Classifier Loss 0.5181059241294861, Total Loss 324.486328125\n",
      "2: Encoding Loss 32.18881607055664, Transition Loss 8.012535095214844, Classifier Loss 0.4989716112613678, Total Loss 309.01019287109375\n",
      "2: Encoding Loss 31.740989685058594, Transition Loss 8.283355712890625, Classifier Loss 0.5049887299537659, Total Loss 306.08349609375\n",
      "2: Encoding Loss 33.184226989746094, Transition Loss 9.140101432800293, Classifier Loss 0.5031352639198303, Total Loss 317.61541748046875\n",
      "2: Encoding Loss 34.1057014465332, Transition Loss 9.523862838745117, Classifier Loss 0.5077650547027588, Total Loss 325.52691650390625\n",
      "2: Encoding Loss 35.01972961425781, Transition Loss 8.798888206481934, Classifier Loss 0.5036590695381165, Total Loss 332.28350830078125\n",
      "2: Encoding Loss 30.668825149536133, Transition Loss 7.378232002258301, Classifier Loss 0.49223873019218445, Total Loss 296.0501403808594\n",
      "2: Encoding Loss 32.859683990478516, Transition Loss 8.3270845413208, Classifier Loss 0.5089942812919617, Total Loss 315.44232177734375\n",
      "2: Encoding Loss 33.9449348449707, Transition Loss 10.1064453125, Classifier Loss 0.5144426226615906, Total Loss 325.0250549316406\n",
      "2: Encoding Loss 32.360328674316406, Transition Loss 9.077380180358887, Classifier Loss 0.512520432472229, Total Loss 311.9501647949219\n",
      "2: Encoding Loss 34.25542449951172, Transition Loss 8.882402420043945, Classifier Loss 0.519886314868927, Total Loss 327.80853271484375\n",
      "2: Encoding Loss 32.296653747558594, Transition Loss 8.650379180908203, Classifier Loss 0.5101669430732727, Total Loss 311.1200256347656\n",
      "2: Encoding Loss 34.490928649902344, Transition Loss 10.316444396972656, Classifier Loss 0.5136711597442627, Total Loss 329.3578796386719\n",
      "2: Encoding Loss 32.53248596191406, Transition Loss 9.012969970703125, Classifier Loss 0.5250580906867981, Total Loss 314.5682678222656\n",
      "2: Encoding Loss 33.734657287597656, Transition Loss 9.742467880249023, Classifier Loss 0.5231841802597046, Total Loss 324.1441650390625\n",
      "2: Encoding Loss 31.756200790405273, Transition Loss 8.800100326538086, Classifier Loss 0.5055822730064392, Total Loss 306.36785888671875\n",
      "2: Encoding Loss 31.316925048828125, Transition Loss 6.567278861999512, Classifier Loss 0.5099451541900635, Total Loss 302.8433532714844\n",
      "2: Encoding Loss 32.96109390258789, Transition Loss 9.478678703308105, Classifier Loss 0.4950631856918335, Total Loss 315.0907897949219\n",
      "2: Encoding Loss 31.78973388671875, Transition Loss 7.794046401977539, Classifier Loss 0.525976300239563, Total Loss 308.47430419921875\n",
      "2: Encoding Loss 30.171466827392578, Transition Loss 8.56821060180664, Classifier Loss 0.5202402472496033, Total Loss 295.1094055175781\n",
      "2: Encoding Loss 34.18730926513672, Transition Loss 8.929376602172852, Classifier Loss 0.5102922320365906, Total Loss 326.3136291503906\n",
      "2: Encoding Loss 33.07137680053711, Transition Loss 9.14350414276123, Classifier Loss 0.5049096941947937, Total Loss 316.89068603515625\n",
      "2: Encoding Loss 32.066707611083984, Transition Loss 8.715668678283691, Classifier Loss 0.505052924156189, Total Loss 308.7820739746094\n",
      "2: Encoding Loss 28.985197067260742, Transition Loss 8.45034122467041, Classifier Loss 0.5120754837989807, Total Loss 284.7791748046875\n",
      "2: Encoding Loss 33.624473571777344, Transition Loss 9.9492826461792, Classifier Loss 0.5153461694717407, Total Loss 322.5202941894531\n",
      "2: Encoding Loss 30.187007904052734, Transition Loss 8.618459701538086, Classifier Loss 0.47713541984558105, Total Loss 290.9333190917969\n",
      "2: Encoding Loss 28.90474510192871, Transition Loss 8.320515632629395, Classifier Loss 0.5089974999427795, Total Loss 283.8017883300781\n",
      "2: Encoding Loss 30.60495948791504, Transition Loss 8.201116561889648, Classifier Loss 0.5197489857673645, Total Loss 298.4548034667969\n",
      "2: Encoding Loss 32.00600051879883, Transition Loss 9.324495315551758, Classifier Loss 0.4943068325519562, Total Loss 307.3435974121094\n",
      "2: Encoding Loss 29.376930236816406, Transition Loss 6.8079047203063965, Classifier Loss 0.5427196621894836, Total Loss 290.6489562988281\n",
      "2: Encoding Loss 33.34100341796875, Transition Loss 10.280586242675781, Classifier Loss 0.5127075910568237, Total Loss 320.0549011230469\n",
      "2: Encoding Loss 31.565521240234375, Transition Loss 9.0531587600708, Classifier Loss 0.5059158802032471, Total Loss 304.9263916015625\n",
      "2: Encoding Loss 32.15676498413086, Transition Loss 9.942805290222168, Classifier Loss 0.5152198672294617, Total Loss 310.7646789550781\n",
      "2: Encoding Loss 32.25565719604492, Transition Loss 8.783647537231445, Classifier Loss 0.5062758922576904, Total Loss 310.4295959472656\n",
      "2: Encoding Loss 31.799970626831055, Transition Loss 8.894314765930176, Classifier Loss 0.49564680457115173, Total Loss 305.7433166503906\n",
      "2: Encoding Loss 31.032079696655273, Transition Loss 7.7598371505737305, Classifier Loss 0.5222879648208618, Total Loss 302.03741455078125\n",
      "2: Encoding Loss 34.82678985595703, Transition Loss 12.364583969116211, Classifier Loss 0.49693402647972107, Total Loss 330.7806396484375\n",
      "2: Encoding Loss 30.698999404907227, Transition Loss 8.895938873291016, Classifier Loss 0.5020882487297058, Total Loss 297.5799865722656\n",
      "2: Encoding Loss 28.905731201171875, Transition Loss 7.340056419372559, Classifier Loss 0.506531834602356, Total Loss 283.3670654296875\n",
      "2: Encoding Loss 31.3029842376709, Transition Loss 8.797842025756836, Classifier Loss 0.5058808922767639, Total Loss 302.77154541015625\n",
      "2: Encoding Loss 31.08821678161621, Transition Loss 8.489080429077148, Classifier Loss 0.5132166147232056, Total Loss 301.7252197265625\n",
      "2: Encoding Loss 29.156091690063477, Transition Loss 6.586513996124268, Classifier Loss 0.5134809613227844, Total Loss 285.91412353515625\n",
      "2: Encoding Loss 29.257177352905273, Transition Loss 6.930729866027832, Classifier Loss 0.4894561767578125, Total Loss 284.3891906738281\n",
      "2: Encoding Loss 31.683748245239258, Transition Loss 9.864462852478027, Classifier Loss 0.49745407700538635, Total Loss 305.18829345703125\n",
      "2: Encoding Loss 28.774873733520508, Transition Loss 6.963029384613037, Classifier Loss 0.4934045076370239, Total Loss 280.93206787109375\n",
      "2: Encoding Loss 31.137882232666016, Transition Loss 8.346794128417969, Classifier Loss 0.5201053619384766, Total Loss 302.782958984375\n",
      "2: Encoding Loss 29.4742488861084, Transition Loss 8.176532745361328, Classifier Loss 0.5281986594200134, Total Loss 290.2491760253906\n",
      "2: Encoding Loss 29.700857162475586, Transition Loss 9.473312377929688, Classifier Loss 0.519158124923706, Total Loss 291.4173278808594\n",
      "2: Encoding Loss 30.419580459594727, Transition Loss 8.512083053588867, Classifier Loss 0.4980190694332123, Total Loss 294.8609619140625\n",
      "2: Encoding Loss 32.089290618896484, Transition Loss 10.1876220703125, Classifier Loss 0.535381019115448, Total Loss 312.28997802734375\n",
      "2: Encoding Loss 30.220476150512695, Transition Loss 10.33387279510498, Classifier Loss 0.5281778573989868, Total Loss 296.64837646484375\n",
      "2: Encoding Loss 31.062715530395508, Transition Loss 9.55128288269043, Classifier Loss 0.5060350894927979, Total Loss 301.0155029296875\n",
      "2: Encoding Loss 31.146326065063477, Transition Loss 9.186328887939453, Classifier Loss 0.5021851658821106, Total Loss 301.2264099121094\n",
      "2: Encoding Loss 30.680553436279297, Transition Loss 8.204092979431152, Classifier Loss 0.5022357106208801, Total Loss 297.308837890625\n",
      "2: Encoding Loss 29.754398345947266, Transition Loss 8.077507972717285, Classifier Loss 0.5031396746635437, Total Loss 289.96466064453125\n",
      "2: Encoding Loss 28.717496871948242, Transition Loss 8.826581001281738, Classifier Loss 0.5037558674812317, Total Loss 281.8808898925781\n",
      "2: Encoding Loss 29.985328674316406, Transition Loss 8.677263259887695, Classifier Loss 0.5134803056716919, Total Loss 292.9660949707031\n",
      "2: Encoding Loss 30.44270133972168, Transition Loss 8.086352348327637, Classifier Loss 0.5092099905014038, Total Loss 296.07989501953125\n",
      "2: Encoding Loss 29.9149169921875, Transition Loss 7.869729518890381, Classifier Loss 0.49156370759010315, Total Loss 290.0496520996094\n",
      "2: Encoding Loss 27.630802154541016, Transition Loss 6.701402187347412, Classifier Loss 0.5137993097305298, Total Loss 273.7666320800781\n",
      "2: Encoding Loss 26.94287109375, Transition Loss 7.215799808502197, Classifier Loss 0.49226462841033936, Total Loss 266.21258544921875\n",
      "2: Encoding Loss 33.65406036376953, Transition Loss 9.665314674377441, Classifier Loss 0.5301600098609924, Total Loss 324.1815490722656\n",
      "2: Encoding Loss 29.741283416748047, Transition Loss 9.767175674438477, Classifier Loss 0.5034664869308472, Total Loss 290.2303466796875\n",
      "2: Encoding Loss 30.803220748901367, Transition Loss 9.634626388549805, Classifier Loss 0.5036891102790833, Total Loss 298.7215881347656\n",
      "2: Encoding Loss 31.830156326293945, Transition Loss 10.551518440246582, Classifier Loss 0.5127277970314026, Total Loss 308.02435302734375\n",
      "2: Encoding Loss 31.78533935546875, Transition Loss 9.230255126953125, Classifier Loss 0.5029871463775635, Total Loss 306.4274597167969\n",
      "2: Encoding Loss 27.33763885498047, Transition Loss 8.33021354675293, Classifier Loss 0.5016679763793945, Total Loss 270.5339660644531\n",
      "2: Encoding Loss 30.62984848022461, Transition Loss 8.830707550048828, Classifier Loss 0.5212883949279785, Total Loss 298.93377685546875\n",
      "2: Encoding Loss 28.54676055908203, Transition Loss 6.270386219024658, Classifier Loss 0.4892430305480957, Total Loss 278.5524597167969\n",
      "2: Encoding Loss 30.649433135986328, Transition Loss 8.53443431854248, Classifier Loss 0.48694634437561035, Total Loss 295.59698486328125\n",
      "2: Encoding Loss 27.347118377685547, Transition Loss 7.087860584259033, Classifier Loss 0.5063270330429077, Total Loss 270.8272399902344\n",
      "2: Encoding Loss 29.361696243286133, Transition Loss 6.571287155151367, Classifier Loss 0.498695433139801, Total Loss 286.077392578125\n",
      "2: Encoding Loss 31.760921478271484, Transition Loss 10.921789169311523, Classifier Loss 0.5004618167877197, Total Loss 306.31793212890625\n",
      "2: Encoding Loss 28.82057762145996, Transition Loss 8.144878387451172, Classifier Loss 0.4911654591560364, Total Loss 281.31011962890625\n",
      "2: Encoding Loss 31.62226104736328, Transition Loss 10.94186782836914, Classifier Loss 0.5121756196022034, Total Loss 306.384033203125\n",
      "2: Encoding Loss 32.08805465698242, Transition Loss 11.031216621398926, Classifier Loss 0.4826172888278961, Total Loss 307.1723937988281\n",
      "2: Encoding Loss 30.517642974853516, Transition Loss 11.816568374633789, Classifier Loss 0.49262210726737976, Total Loss 295.76666259765625\n",
      "2: Encoding Loss 27.92142677307129, Transition Loss 11.38251781463623, Classifier Loss 0.4688965976238251, Total Loss 272.5375671386719\n",
      "3: Encoding Loss 28.65102195739746, Transition Loss 9.160200119018555, Classifier Loss 0.5051241517066956, Total Loss 281.5526123046875\n",
      "3: Encoding Loss 30.69475555419922, Transition Loss 9.961740493774414, Classifier Loss 0.5047428607940674, Total Loss 298.0246887207031\n",
      "3: Encoding Loss 30.887441635131836, Transition Loss 10.524150848388672, Classifier Loss 0.4876661002635956, Total Loss 297.9709777832031\n",
      "3: Encoding Loss 29.08072853088379, Transition Loss 8.958626747131348, Classifier Loss 0.47325757145881653, Total Loss 281.7633056640625\n",
      "3: Encoding Loss 31.56406593322754, Transition Loss 9.629733085632324, Classifier Loss 0.5131857395172119, Total Loss 305.7570495605469\n",
      "3: Encoding Loss 30.20154571533203, Transition Loss 9.04334545135498, Classifier Loss 0.5001340508460999, Total Loss 293.4344177246094\n",
      "3: Encoding Loss 30.012056350708008, Transition Loss 8.404624938964844, Classifier Loss 0.5210689902305603, Total Loss 293.8843078613281\n",
      "3: Encoding Loss 31.85287857055664, Transition Loss 11.47082233428955, Classifier Loss 0.5248445272445679, Total Loss 309.6016540527344\n",
      "3: Encoding Loss 31.69928550720215, Transition Loss 9.128124237060547, Classifier Loss 0.5189290642738342, Total Loss 307.31280517578125\n",
      "3: Encoding Loss 32.272090911865234, Transition Loss 12.164941787719727, Classifier Loss 0.5155455470085144, Total Loss 312.1642761230469\n",
      "3: Encoding Loss 31.204822540283203, Transition Loss 9.802630424499512, Classifier Loss 0.5001007914543152, Total Loss 301.60919189453125\n",
      "3: Encoding Loss 30.678945541381836, Transition Loss 9.799772262573242, Classifier Loss 0.5027831792831421, Total Loss 297.6698303222656\n",
      "3: Encoding Loss 28.173063278198242, Transition Loss 8.260046005249023, Classifier Loss 0.5337338447570801, Total Loss 280.4098815917969\n",
      "3: Encoding Loss 30.152149200439453, Transition Loss 9.348200798034668, Classifier Loss 0.5007153749465942, Total Loss 293.1583557128906\n",
      "3: Encoding Loss 29.19316291809082, Transition Loss 9.045398712158203, Classifier Loss 0.526071310043335, Total Loss 287.9615478515625\n",
      "3: Encoding Loss 28.4085750579834, Transition Loss 10.10822868347168, Classifier Loss 0.4983082115650177, Total Loss 279.1210632324219\n",
      "3: Encoding Loss 28.3682918548584, Transition Loss 9.148795127868652, Classifier Loss 0.5033258199691772, Total Loss 279.10870361328125\n",
      "3: Encoding Loss 29.51573944091797, Transition Loss 9.424188613891602, Classifier Loss 0.5033833980560303, Total Loss 288.3490905761719\n",
      "3: Encoding Loss 28.53941535949707, Transition Loss 9.215445518493652, Classifier Loss 0.47899138927459717, Total Loss 278.05755615234375\n",
      "3: Encoding Loss 31.799863815307617, Transition Loss 11.000391960144043, Classifier Loss 0.4994937479496002, Total Loss 306.5483703613281\n",
      "3: Encoding Loss 28.860185623168945, Transition Loss 10.152507781982422, Classifier Loss 0.5023332834243774, Total Loss 283.14532470703125\n",
      "3: Encoding Loss 27.802196502685547, Transition Loss 8.12511157989502, Classifier Loss 0.5205754637718201, Total Loss 276.10015869140625\n",
      "3: Encoding Loss 28.016185760498047, Transition Loss 11.000982284545898, Classifier Loss 0.5243432521820068, Total Loss 278.7640380859375\n",
      "3: Encoding Loss 28.551456451416016, Transition Loss 7.36233377456665, Classifier Loss 0.5319772958755493, Total Loss 283.08184814453125\n",
      "3: Encoding Loss 29.139009475708008, Transition Loss 7.909299850463867, Classifier Loss 0.5002568960189819, Total Loss 284.7196350097656\n",
      "3: Encoding Loss 32.43299102783203, Transition Loss 11.022985458374023, Classifier Loss 0.5088664889335632, Total Loss 312.55517578125\n",
      "3: Encoding Loss 30.59977149963379, Transition Loss 11.844415664672852, Classifier Loss 0.5019128322601318, Total Loss 297.35833740234375\n",
      "3: Encoding Loss 28.554372787475586, Transition Loss 10.47104549407959, Classifier Loss 0.5081853270530701, Total Loss 281.34771728515625\n",
      "3: Encoding Loss 28.49833869934082, Transition Loss 9.561071395874023, Classifier Loss 0.5010554790496826, Total Loss 280.0044860839844\n",
      "3: Encoding Loss 27.702707290649414, Transition Loss 10.73894214630127, Classifier Loss 0.49392080307006836, Total Loss 273.1615295410156\n",
      "3: Encoding Loss 29.85934066772461, Transition Loss 9.941744804382324, Classifier Loss 0.517512857913971, Total Loss 292.6143798828125\n",
      "3: Encoding Loss 25.6845760345459, Transition Loss 8.830492973327637, Classifier Loss 0.4936388432979584, Total Loss 256.6065979003906\n",
      "3: Encoding Loss 30.20557975769043, Transition Loss 11.169830322265625, Classifier Loss 0.5188487768173218, Total Loss 295.76348876953125\n",
      "3: Encoding Loss 29.136417388916016, Transition Loss 6.650680065155029, Classifier Loss 0.5231248140335083, Total Loss 286.73394775390625\n",
      "3: Encoding Loss 28.496463775634766, Transition Loss 9.874104499816895, Classifier Loss 0.4947301745414734, Total Loss 279.4195556640625\n",
      "3: Encoding Loss 26.455045700073242, Transition Loss 6.482839584350586, Classifier Loss 0.5424097776412964, Total Loss 267.17791748046875\n",
      "3: Encoding Loss 27.701005935668945, Transition Loss 10.47815990447998, Classifier Loss 0.4898878335952759, Total Loss 272.6925048828125\n",
      "3: Encoding Loss 29.028587341308594, Transition Loss 11.940893173217773, Classifier Loss 0.5097902417182922, Total Loss 285.5959167480469\n",
      "3: Encoding Loss 28.534107208251953, Transition Loss 9.6454496383667, Classifier Loss 0.4899711012840271, Total Loss 279.19903564453125\n",
      "3: Encoding Loss 28.294824600219727, Transition Loss 9.899258613586426, Classifier Loss 0.49044615030288696, Total Loss 277.383056640625\n",
      "3: Encoding Loss 28.0843505859375, Transition Loss 10.397745132446289, Classifier Loss 0.5195088386535645, Total Loss 278.7052307128906\n",
      "3: Encoding Loss 30.07149887084961, Transition Loss 7.87751579284668, Classifier Loss 0.5168942809104919, Total Loss 293.8369445800781\n",
      "3: Encoding Loss 28.326269149780273, Transition Loss 7.7726922035217285, Classifier Loss 0.525640606880188, Total Loss 280.728759765625\n",
      "3: Encoding Loss 26.880802154541016, Transition Loss 6.190905570983887, Classifier Loss 0.5204893350601196, Total Loss 268.3335266113281\n",
      "3: Encoding Loss 28.578975677490234, Transition Loss 9.21976089477539, Classifier Loss 0.5055997371673584, Total Loss 281.0357666015625\n",
      "3: Encoding Loss 29.71128273010254, Transition Loss 10.68808364868164, Classifier Loss 0.4974622130393982, Total Loss 289.5740966796875\n",
      "3: Encoding Loss 28.44330596923828, Transition Loss 11.025445938110352, Classifier Loss 0.5119016766548157, Total Loss 280.9416809082031\n",
      "3: Encoding Loss 29.797021865844727, Transition Loss 10.738851547241211, Classifier Loss 0.5127856731414795, Total Loss 291.8025207519531\n",
      "3: Encoding Loss 29.10463523864746, Transition Loss 10.324625015258789, Classifier Loss 0.50495845079422, Total Loss 285.3978271484375\n",
      "3: Encoding Loss 27.4211368560791, Transition Loss 10.066178321838379, Classifier Loss 0.5056772232055664, Total Loss 271.9500732421875\n",
      "3: Encoding Loss 26.476234436035156, Transition Loss 7.133657932281494, Classifier Loss 0.5000846982002258, Total Loss 263.24505615234375\n",
      "3: Encoding Loss 26.7432804107666, Transition Loss 7.601912498474121, Classifier Loss 0.5089111328125, Total Loss 266.35772705078125\n",
      "3: Encoding Loss 26.154664993286133, Transition Loss 6.560027122497559, Classifier Loss 0.47739359736442566, Total Loss 258.2886962890625\n",
      "3: Encoding Loss 24.993160247802734, Transition Loss 9.265633583068848, Classifier Loss 0.4785720109939575, Total Loss 249.65562438964844\n",
      "3: Encoding Loss 26.607084274291992, Transition Loss 9.666666030883789, Classifier Loss 0.48487573862075806, Total Loss 263.2775573730469\n",
      "3: Encoding Loss 25.195926666259766, Transition Loss 7.663756370544434, Classifier Loss 0.5073988437652588, Total Loss 253.8400421142578\n",
      "3: Encoding Loss 28.1654109954834, Transition Loss 9.008208274841309, Classifier Loss 0.46817708015441895, Total Loss 273.942626953125\n",
      "3: Encoding Loss 27.455358505249023, Transition Loss 8.747089385986328, Classifier Loss 0.48224759101867676, Total Loss 269.6170349121094\n",
      "3: Encoding Loss 28.071025848388672, Transition Loss 9.01840877532959, Classifier Loss 0.4958128333091736, Total Loss 275.95318603515625\n",
      "3: Encoding Loss 26.190467834472656, Transition Loss 7.887657165527344, Classifier Loss 0.4991197884082794, Total Loss 261.01324462890625\n",
      "3: Encoding Loss 27.546749114990234, Transition Loss 10.998449325561523, Classifier Loss 0.4874076247215271, Total Loss 271.314453125\n",
      "3: Encoding Loss 28.354902267456055, Transition Loss 10.93446159362793, Classifier Loss 0.5099103450775146, Total Loss 280.01715087890625\n",
      "3: Encoding Loss 27.505374908447266, Transition Loss 8.873270034790039, Classifier Loss 0.510800302028656, Total Loss 272.8976745605469\n",
      "3: Encoding Loss 26.94230079650879, Transition Loss 9.132485389709473, Classifier Loss 0.4804009795188904, Total Loss 265.4049987792969\n",
      "3: Encoding Loss 25.351715087890625, Transition Loss 8.419978141784668, Classifier Loss 0.5150052905082703, Total Loss 255.9982452392578\n",
      "3: Encoding Loss 25.681581497192383, Transition Loss 7.763579368591309, Classifier Loss 0.4769374132156372, Total Loss 254.69912719726562\n",
      "3: Encoding Loss 29.322832107543945, Transition Loss 8.745588302612305, Classifier Loss 0.4948757588863373, Total Loss 285.8193664550781\n",
      "3: Encoding Loss 28.669477462768555, Transition Loss 10.693894386291504, Classifier Loss 0.456928014755249, Total Loss 277.1874084472656\n",
      "3: Encoding Loss 30.301963806152344, Transition Loss 12.230774879455566, Classifier Loss 0.4860920310020447, Total Loss 293.4710998535156\n",
      "3: Encoding Loss 26.927783966064453, Transition Loss 9.254364013671875, Classifier Loss 0.5130780339241028, Total Loss 268.5809326171875\n",
      "3: Encoding Loss 28.086957931518555, Transition Loss 9.633886337280273, Classifier Loss 0.5090433955192566, Total Loss 277.52679443359375\n",
      "3: Encoding Loss 24.84786033630371, Transition Loss 8.427632331848145, Classifier Loss 0.4903405010700226, Total Loss 249.50245666503906\n",
      "3: Encoding Loss 25.968881607055664, Transition Loss 9.283836364746094, Classifier Loss 0.5061545968055725, Total Loss 260.2232971191406\n",
      "3: Encoding Loss 28.41023826599121, Transition Loss 10.381126403808594, Classifier Loss 0.5154072642326355, Total Loss 280.89886474609375\n",
      "3: Encoding Loss 28.090320587158203, Transition Loss 11.27175235748291, Classifier Loss 0.5084924697875977, Total Loss 277.826171875\n",
      "3: Encoding Loss 28.589303970336914, Transition Loss 10.353042602539062, Classifier Loss 0.501446008682251, Total Loss 280.9296569824219\n",
      "3: Encoding Loss 27.482128143310547, Transition Loss 11.231879234313965, Classifier Loss 0.49579155445098877, Total Loss 271.68255615234375\n",
      "3: Encoding Loss 28.174787521362305, Transition Loss 10.64008903503418, Classifier Loss 0.5009472370147705, Total Loss 277.62103271484375\n",
      "3: Encoding Loss 26.839170455932617, Transition Loss 9.503687858581543, Classifier Loss 0.46748778223991394, Total Loss 263.3628845214844\n",
      "3: Encoding Loss 28.01185417175293, Transition Loss 9.633308410644531, Classifier Loss 0.47867411375045776, Total Loss 273.888916015625\n",
      "3: Encoding Loss 26.75589942932129, Transition Loss 8.767770767211914, Classifier Loss 0.5353700518608093, Total Loss 269.3377380371094\n",
      "3: Encoding Loss 26.6347599029541, Transition Loss 8.929487228393555, Classifier Loss 0.4969876706600189, Total Loss 264.562744140625\n",
      "3: Encoding Loss 27.05108642578125, Transition Loss 10.055923461914062, Classifier Loss 0.5045934319496155, Total Loss 268.87921142578125\n",
      "3: Encoding Loss 27.187204360961914, Transition Loss 10.22204875946045, Classifier Loss 0.49427729845046997, Total Loss 268.9697570800781\n",
      "3: Encoding Loss 29.675365447998047, Transition Loss 9.265289306640625, Classifier Loss 0.5050229430198669, Total Loss 289.75830078125\n",
      "3: Encoding Loss 25.562408447265625, Transition Loss 7.819880485534668, Classifier Loss 0.488415390253067, Total Loss 254.90478515625\n",
      "3: Encoding Loss 28.77829360961914, Transition Loss 8.452563285827637, Classifier Loss 0.5050835013389587, Total Loss 282.42523193359375\n",
      "3: Encoding Loss 27.95525550842285, Transition Loss 11.174108505249023, Classifier Loss 0.5045890212059021, Total Loss 276.3357849121094\n",
      "3: Encoding Loss 26.603357315063477, Transition Loss 9.411900520324707, Classifier Loss 0.501998782157898, Total Loss 264.90911865234375\n",
      "3: Encoding Loss 28.275583267211914, Transition Loss 8.876534461975098, Classifier Loss 0.49046844244003296, Total Loss 277.0268249511719\n",
      "3: Encoding Loss 26.130346298217773, Transition Loss 8.826421737670898, Classifier Loss 0.4828110933303833, Total Loss 259.08917236328125\n",
      "3: Encoding Loss 27.469675064086914, Transition Loss 11.026777267456055, Classifier Loss 0.5142301917076111, Total Loss 273.3857727050781\n",
      "3: Encoding Loss 26.3265438079834, Transition Loss 9.665750503540039, Classifier Loss 0.4921879470348358, Total Loss 261.7642822265625\n",
      "3: Encoding Loss 30.01439094543457, Transition Loss 9.916120529174805, Classifier Loss 0.4931938052177429, Total Loss 291.417724609375\n",
      "3: Encoding Loss 26.562620162963867, Transition Loss 9.104026794433594, Classifier Loss 0.48873504996299744, Total Loss 263.19525146484375\n",
      "3: Encoding Loss 26.489526748657227, Transition Loss 6.1199259757995605, Classifier Loss 0.5205420851707458, Total Loss 265.1944274902344\n",
      "3: Encoding Loss 26.810041427612305, Transition Loss 9.987142562866211, Classifier Loss 0.4923012852668762, Total Loss 265.7078857421875\n",
      "3: Encoding Loss 25.48925018310547, Transition Loss 7.77663516998291, Classifier Loss 0.49907249212265015, Total Loss 255.3765869140625\n",
      "3: Encoding Loss 25.30185317993164, Transition Loss 9.15865707397461, Classifier Loss 0.5111497640609741, Total Loss 255.3615264892578\n",
      "3: Encoding Loss 27.811405181884766, Transition Loss 8.625349044799805, Classifier Loss 0.5008865594863892, Total Loss 274.3049621582031\n",
      "3: Encoding Loss 27.007352828979492, Transition Loss 8.817370414733887, Classifier Loss 0.49311989545822144, Total Loss 267.1343078613281\n",
      "3: Encoding Loss 27.031084060668945, Transition Loss 8.693321228027344, Classifier Loss 0.484941303730011, Total Loss 266.48150634765625\n",
      "3: Encoding Loss 26.319843292236328, Transition Loss 8.938446044921875, Classifier Loss 0.48844218254089355, Total Loss 261.1906433105469\n",
      "3: Encoding Loss 26.7879581451416, Transition Loss 10.12101936340332, Classifier Loss 0.5240101218223572, Total Loss 268.7288818359375\n",
      "3: Encoding Loss 26.464086532592773, Transition Loss 8.642332077026367, Classifier Loss 0.48377758264541626, Total Loss 261.81890869140625\n",
      "3: Encoding Loss 24.26966094970703, Transition Loss 8.850699424743652, Classifier Loss 0.4722713828086853, Total Loss 243.15457153320312\n",
      "3: Encoding Loss 25.667367935180664, Transition Loss 8.19859504699707, Classifier Loss 0.4923470616340637, Total Loss 256.21337890625\n",
      "3: Encoding Loss 28.438079833984375, Transition Loss 9.26861572265625, Classifier Loss 0.5114830732345581, Total Loss 280.5066833496094\n",
      "3: Encoding Loss 24.861530303955078, Transition Loss 6.443035125732422, Classifier Loss 0.4937560260295868, Total Loss 249.55645751953125\n",
      "3: Encoding Loss 27.918140411376953, Transition Loss 10.20699405670166, Classifier Loss 0.49699530005455017, Total Loss 275.0860595703125\n",
      "3: Encoding Loss 28.83321762084961, Transition Loss 8.818421363830566, Classifier Loss 0.5122298002243042, Total Loss 283.65240478515625\n",
      "3: Encoding Loss 27.23017120361328, Transition Loss 9.789684295654297, Classifier Loss 0.483272522687912, Total Loss 268.1265563964844\n",
      "3: Encoding Loss 26.30777931213379, Transition Loss 8.453529357910156, Classifier Loss 0.5107921361923218, Total Loss 263.2321472167969\n",
      "3: Encoding Loss 27.29470443725586, Transition Loss 8.356782913208008, Classifier Loss 0.48531970381736755, Total Loss 268.56097412109375\n",
      "3: Encoding Loss 27.082841873168945, Transition Loss 6.984692573547363, Classifier Loss 0.5452962517738342, Total Loss 272.5893249511719\n",
      "3: Encoding Loss 30.213165283203125, Transition Loss 11.962271690368652, Classifier Loss 0.4803093373775482, Total Loss 292.12872314453125\n",
      "3: Encoding Loss 27.765682220458984, Transition Loss 8.566170692443848, Classifier Loss 0.5090239644050598, Total Loss 274.7410888671875\n",
      "3: Encoding Loss 26.370180130004883, Transition Loss 7.086798191070557, Classifier Loss 0.5145373344421387, Total Loss 263.8325500488281\n",
      "3: Encoding Loss 26.380395889282227, Transition Loss 8.502763748168945, Classifier Loss 0.4833836257457733, Total Loss 261.08209228515625\n",
      "3: Encoding Loss 27.311243057250977, Transition Loss 8.018747329711914, Classifier Loss 0.5111148357391357, Total Loss 271.2052001953125\n",
      "3: Encoding Loss 24.500667572021484, Transition Loss 5.917210102081299, Classifier Loss 0.4897555708885193, Total Loss 246.1643524169922\n",
      "3: Encoding Loss 24.23719024658203, Transition Loss 6.150963306427002, Classifier Loss 0.4894424080848694, Total Loss 244.07196044921875\n",
      "3: Encoding Loss 26.685121536254883, Transition Loss 9.495002746582031, Classifier Loss 0.49126240611076355, Total Loss 264.5062255859375\n",
      "3: Encoding Loss 24.39300537109375, Transition Loss 6.179190635681152, Classifier Loss 0.4830743968486786, Total Loss 244.68731689453125\n",
      "3: Encoding Loss 27.40242576599121, Transition Loss 7.58404016494751, Classifier Loss 0.514672577381134, Total Loss 272.2034912109375\n",
      "3: Encoding Loss 25.04229736328125, Transition Loss 7.755350112915039, Classifier Loss 0.5114108324050903, Total Loss 253.03053283691406\n",
      "3: Encoding Loss 26.196809768676758, Transition Loss 9.039628028869629, Classifier Loss 0.4900384843349457, Total Loss 260.3862609863281\n",
      "3: Encoding Loss 25.218297958374023, Transition Loss 7.93222188949585, Classifier Loss 0.48910707235336304, Total Loss 252.2435302734375\n",
      "3: Encoding Loss 26.853384017944336, Transition Loss 9.604171752929688, Classifier Loss 0.49606025218963623, Total Loss 266.35394287109375\n",
      "3: Encoding Loss 25.141843795776367, Transition Loss 10.34683895111084, Classifier Loss 0.5282695293426514, Total Loss 256.03106689453125\n",
      "3: Encoding Loss 26.119441986083984, Transition Loss 9.014886856079102, Classifier Loss 0.4771915674209595, Total Loss 258.4776916503906\n",
      "3: Encoding Loss 26.85627555847168, Transition Loss 8.395978927612305, Classifier Loss 0.480209618806839, Total Loss 264.55035400390625\n",
      "3: Encoding Loss 25.25100326538086, Transition Loss 7.204756736755371, Classifier Loss 0.490111380815506, Total Loss 252.4601287841797\n",
      "3: Encoding Loss 26.28226661682129, Transition Loss 7.04052734375, Classifier Loss 0.511474609375, Total Loss 262.8136901855469\n",
      "3: Encoding Loss 25.38848114013672, Transition Loss 8.071966171264648, Classifier Loss 0.5298756957054138, Total Loss 257.7098083496094\n",
      "3: Encoding Loss 24.646556854248047, Transition Loss 8.086079597473145, Classifier Loss 0.48651763796806335, Total Loss 247.4414520263672\n",
      "3: Encoding Loss 26.080785751342773, Transition Loss 6.843578338623047, Classifier Loss 0.5062437057495117, Total Loss 260.6393737792969\n",
      "3: Encoding Loss 26.341230392456055, Transition Loss 6.977548599243164, Classifier Loss 0.5070218443870544, Total Loss 262.8275451660156\n",
      "3: Encoding Loss 24.029470443725586, Transition Loss 5.802339553833008, Classifier Loss 0.478954017162323, Total Loss 241.2916259765625\n",
      "3: Encoding Loss 24.73002052307129, Transition Loss 7.028269290924072, Classifier Loss 0.46280530095100403, Total Loss 245.52635192871094\n",
      "3: Encoding Loss 29.092113494873047, Transition Loss 8.286436080932617, Classifier Loss 0.5034419298171997, Total Loss 284.7384033203125\n",
      "3: Encoding Loss 26.719337463378906, Transition Loss 9.354397773742676, Classifier Loss 0.4886810779571533, Total Loss 264.4936828613281\n",
      "3: Encoding Loss 28.59323501586914, Transition Loss 8.574861526489258, Classifier Loss 0.4833996891975403, Total Loss 278.8008117675781\n",
      "3: Encoding Loss 27.560150146484375, Transition Loss 9.883503913879395, Classifier Loss 0.5013182163238525, Total Loss 272.5897521972656\n",
      "3: Encoding Loss 27.442699432373047, Transition Loss 8.13597297668457, Classifier Loss 0.47555795311927795, Total Loss 268.724609375\n",
      "3: Encoding Loss 24.810483932495117, Transition Loss 7.681802272796631, Classifier Loss 0.5008147358894348, Total Loss 250.10169982910156\n",
      "3: Encoding Loss 27.668466567993164, Transition Loss 7.2397541999816895, Classifier Loss 0.4927189350128174, Total Loss 272.06756591796875\n",
      "3: Encoding Loss 25.17615509033203, Transition Loss 4.9557976722717285, Classifier Loss 0.4935418665409088, Total Loss 251.7545928955078\n",
      "3: Encoding Loss 25.179224014282227, Transition Loss 7.148122310638428, Classifier Loss 0.5065507888793945, Total Loss 253.51849365234375\n",
      "3: Encoding Loss 23.737016677856445, Transition Loss 6.324038982391357, Classifier Loss 0.4813220500946045, Total Loss 239.2931365966797\n",
      "3: Encoding Loss 25.198143005371094, Transition Loss 5.173667907714844, Classifier Loss 0.5135816335678101, Total Loss 253.97802734375\n",
      "3: Encoding Loss 26.861515045166016, Transition Loss 10.182146072387695, Classifier Loss 0.48231032490730286, Total Loss 265.15960693359375\n",
      "3: Encoding Loss 25.752981185913086, Transition Loss 7.150426864624023, Classifier Loss 0.4856298863887787, Total Loss 256.0169372558594\n",
      "3: Encoding Loss 26.942623138427734, Transition Loss 9.683999061584473, Classifier Loss 0.4766738712787628, Total Loss 265.1451721191406\n",
      "3: Encoding Loss 28.00295066833496, Transition Loss 9.769295692443848, Classifier Loss 0.4785471260547638, Total Loss 273.8321838378906\n",
      "3: Encoding Loss 26.346227645874023, Transition Loss 10.974184036254883, Classifier Loss 0.4755343794822693, Total Loss 260.51806640625\n",
      "3: Encoding Loss 24.50643539428711, Transition Loss 10.748661041259766, Classifier Loss 0.5068880915641785, Total Loss 248.89004516601562\n",
      "4: Encoding Loss 26.121721267700195, Transition Loss 7.772950172424316, Classifier Loss 0.472692608833313, Total Loss 257.7976379394531\n",
      "4: Encoding Loss 26.382389068603516, Transition Loss 8.276512145996094, Classifier Loss 0.5130131244659424, Total Loss 264.0157165527344\n",
      "4: Encoding Loss 27.84680938720703, Transition Loss 8.505337715148926, Classifier Loss 0.4917859137058258, Total Loss 273.6541442871094\n",
      "4: Encoding Loss 26.114206314086914, Transition Loss 7.311761379241943, Classifier Loss 0.4691805839538574, Total Loss 257.2940673828125\n",
      "4: Encoding Loss 28.0732421875, Transition Loss 7.7808661460876465, Classifier Loss 0.4912452697753906, Total Loss 275.2666320800781\n",
      "4: Encoding Loss 25.560136795043945, Transition Loss 7.4958577156066895, Classifier Loss 0.5230746269226074, Total Loss 258.2877502441406\n",
      "4: Encoding Loss 26.67052459716797, Transition Loss 6.371725559234619, Classifier Loss 0.4935312271118164, Total Loss 263.99169921875\n",
      "4: Encoding Loss 26.999677658081055, Transition Loss 9.09506893157959, Classifier Loss 0.5033248662948608, Total Loss 268.1488952636719\n",
      "4: Encoding Loss 27.320770263671875, Transition Loss 7.198144912719727, Classifier Loss 0.5161523222923279, Total Loss 271.62103271484375\n",
      "4: Encoding Loss 28.528776168823242, Transition Loss 9.750462532043457, Classifier Loss 0.48778772354125977, Total Loss 278.9590759277344\n",
      "4: Encoding Loss 26.405651092529297, Transition Loss 7.5573835372924805, Classifier Loss 0.4578266441822052, Total Loss 258.53936767578125\n",
      "4: Encoding Loss 25.856409072875977, Transition Loss 7.856510162353516, Classifier Loss 0.47763657569885254, Total Loss 256.18621826171875\n",
      "4: Encoding Loss 25.29939842224121, Transition Loss 6.5642290115356445, Classifier Loss 0.4865799844264984, Total Loss 252.3660430908203\n",
      "4: Encoding Loss 25.797060012817383, Transition Loss 7.179409503936768, Classifier Loss 0.5149422883987427, Total Loss 259.3066101074219\n",
      "4: Encoding Loss 25.545869827270508, Transition Loss 7.20574951171875, Classifier Loss 0.5016916990280151, Total Loss 255.97727966308594\n",
      "4: Encoding Loss 25.363676071166992, Transition Loss 8.323507308959961, Classifier Loss 0.4968096911907196, Total Loss 254.2550811767578\n",
      "4: Encoding Loss 25.102266311645508, Transition Loss 7.35765266418457, Classifier Loss 0.4946966767311096, Total Loss 251.7593231201172\n",
      "4: Encoding Loss 26.29605484008789, Transition Loss 7.543207168579102, Classifier Loss 0.4789806306362152, Total Loss 259.775146484375\n",
      "4: Encoding Loss 25.587980270385742, Transition Loss 7.763354778289795, Classifier Loss 0.5023221373558044, Total Loss 256.48870849609375\n",
      "4: Encoding Loss 27.25253677368164, Transition Loss 8.721943855285645, Classifier Loss 0.4639887511730194, Total Loss 266.1635437011719\n",
      "4: Encoding Loss 25.653461456298828, Transition Loss 8.564783096313477, Classifier Loss 0.48281174898147583, Total Loss 255.22181701660156\n",
      "4: Encoding Loss 25.006908416748047, Transition Loss 6.203351974487305, Classifier Loss 0.4854947030544281, Total Loss 249.84542846679688\n",
      "4: Encoding Loss 24.246389389038086, Transition Loss 9.845590591430664, Classifier Loss 0.4762330651283264, Total Loss 243.56353759765625\n",
      "4: Encoding Loss 24.77971076965332, Transition Loss 5.80210018157959, Classifier Loss 0.5083668828010559, Total Loss 250.2347869873047\n",
      "4: Encoding Loss 25.77054786682129, Transition Loss 5.71263313293457, Classifier Loss 0.46717721223831177, Total Loss 254.02464294433594\n",
      "4: Encoding Loss 26.052141189575195, Transition Loss 8.571378707885742, Classifier Loss 0.5055351257324219, Total Loss 260.6849060058594\n",
      "4: Encoding Loss 27.134492874145508, Transition Loss 9.472880363464355, Classifier Loss 0.49786296486854553, Total Loss 268.7568054199219\n",
      "4: Encoding Loss 25.540651321411133, Transition Loss 8.6800537109375, Classifier Loss 0.48318755626678467, Total Loss 254.37997436523438\n",
      "4: Encoding Loss 25.67094612121582, Transition Loss 7.8523030281066895, Classifier Loss 0.4888300895690918, Total Loss 255.82106018066406\n",
      "4: Encoding Loss 25.39457893371582, Transition Loss 8.941129684448242, Classifier Loss 0.4905358552932739, Total Loss 253.99844360351562\n",
      "4: Encoding Loss 26.13741111755371, Transition Loss 8.097041130065918, Classifier Loss 0.4796677231788635, Total Loss 258.68548583984375\n",
      "4: Encoding Loss 24.013647079467773, Transition Loss 7.727247714996338, Classifier Loss 0.5002931356430054, Total Loss 243.68394470214844\n",
      "4: Encoding Loss 26.156728744506836, Transition Loss 9.060754776000977, Classifier Loss 0.4819067716598511, Total Loss 259.2566833496094\n",
      "4: Encoding Loss 25.43173599243164, Transition Loss 4.488061428070068, Classifier Loss 0.510854959487915, Total Loss 255.43699645996094\n",
      "4: Encoding Loss 24.73164176940918, Transition Loss 7.939813137054443, Classifier Loss 0.5202339887619019, Total Loss 251.46449279785156\n",
      "4: Encoding Loss 23.990983963012695, Transition Loss 4.612058639526367, Classifier Loss 0.49266380071640015, Total Loss 242.1166534423828\n",
      "4: Encoding Loss 25.753786087036133, Transition Loss 8.485101699829102, Classifier Loss 0.4916383624076843, Total Loss 256.8911437988281\n",
      "4: Encoding Loss 25.843103408813477, Transition Loss 10.125219345092773, Classifier Loss 0.46707966923713684, Total Loss 255.4778289794922\n",
      "4: Encoding Loss 25.580989837646484, Transition Loss 7.568998336791992, Classifier Loss 0.47977858781814575, Total Loss 254.13958740234375\n",
      "4: Encoding Loss 25.06521224975586, Transition Loss 8.084362983703613, Classifier Loss 0.5151716470718384, Total Loss 253.65574645996094\n",
      "4: Encoding Loss 24.235273361206055, Transition Loss 8.6281156539917, Classifier Loss 0.4759148955345154, Total Loss 243.1992950439453\n",
      "4: Encoding Loss 26.26053237915039, Transition Loss 5.654241561889648, Classifier Loss 0.4832320213317871, Total Loss 259.538330078125\n",
      "4: Encoding Loss 25.086652755737305, Transition Loss 5.480930805206299, Classifier Loss 0.5070434212684631, Total Loss 252.49375915527344\n",
      "4: Encoding Loss 23.80139923095703, Transition Loss 4.102084636688232, Classifier Loss 0.48558729887008667, Total Loss 239.79034423828125\n",
      "4: Encoding Loss 26.727378845214844, Transition Loss 6.827113151550293, Classifier Loss 0.4790971279144287, Total Loss 263.09417724609375\n",
      "4: Encoding Loss 25.314208984375, Transition Loss 8.504347801208496, Classifier Loss 0.5020118951797485, Total Loss 254.4157257080078\n",
      "4: Encoding Loss 24.78209686279297, Transition Loss 8.809459686279297, Classifier Loss 0.4938576817512512, Total Loss 249.40443420410156\n",
      "4: Encoding Loss 26.02394676208496, Transition Loss 8.284975051879883, Classifier Loss 0.4956498146057129, Total Loss 259.4135437011719\n",
      "4: Encoding Loss 26.14260482788086, Transition Loss 7.927066802978516, Classifier Loss 0.49082738161087036, Total Loss 259.80902099609375\n",
      "4: Encoding Loss 25.158466339111328, Transition Loss 7.745300769805908, Classifier Loss 0.46784308552742004, Total Loss 249.60108947753906\n",
      "4: Encoding Loss 24.613080978393555, Transition Loss 5.237181663513184, Classifier Loss 0.5005377531051636, Total Loss 248.005859375\n",
      "4: Encoding Loss 24.556175231933594, Transition Loss 5.619063854217529, Classifier Loss 0.5059105157852173, Total Loss 248.1642608642578\n",
      "4: Encoding Loss 24.460865020751953, Transition Loss 4.709150791168213, Classifier Loss 0.47678348422050476, Total Loss 244.30709838867188\n",
      "4: Encoding Loss 25.307109832763672, Transition Loss 7.750064849853516, Classifier Loss 0.49123573303222656, Total Loss 253.1304931640625\n",
      "4: Encoding Loss 24.49053192138672, Transition Loss 7.845874786376953, Classifier Loss 0.48952487111091614, Total Loss 246.44590759277344\n",
      "4: Encoding Loss 23.643558502197266, Transition Loss 6.008749008178711, Classifier Loss 0.5074560642242432, Total Loss 241.0958251953125\n",
      "4: Encoding Loss 24.88856315612793, Transition Loss 6.945181846618652, Classifier Loss 0.4749299883842468, Total Loss 247.99053955078125\n",
      "4: Encoding Loss 25.014842987060547, Transition Loss 6.571183681488037, Classifier Loss 0.47495973110198975, Total Loss 248.92897033691406\n",
      "4: Encoding Loss 25.287797927856445, Transition Loss 6.854964256286621, Classifier Loss 0.4745236039161682, Total Loss 251.125732421875\n",
      "4: Encoding Loss 23.96402931213379, Transition Loss 5.893884658813477, Classifier Loss 0.48810499906539917, Total Loss 241.70150756835938\n",
      "4: Encoding Loss 25.747432708740234, Transition Loss 9.029396057128906, Classifier Loss 0.4526407718658447, Total Loss 253.0494384765625\n",
      "4: Encoding Loss 26.267423629760742, Transition Loss 8.564502716064453, Classifier Loss 0.4854845404624939, Total Loss 260.4007263183594\n",
      "4: Encoding Loss 26.623802185058594, Transition Loss 6.302844524383545, Classifier Loss 0.5163936614990234, Total Loss 265.8903503417969\n",
      "4: Encoding Loss 26.04467010498047, Transition Loss 6.689214706420898, Classifier Loss 0.45898646116256714, Total Loss 255.5938720703125\n",
      "4: Encoding Loss 23.204486846923828, Transition Loss 6.898653507232666, Classifier Loss 0.4936085343360901, Total Loss 236.37648010253906\n",
      "4: Encoding Loss 24.59975242614746, Transition Loss 6.111141681671143, Classifier Loss 0.47028517723083496, Total Loss 245.04876708984375\n",
      "4: Encoding Loss 25.16170310974121, Transition Loss 6.7518720626831055, Classifier Loss 0.48939013481140137, Total Loss 251.5830078125\n",
      "4: Encoding Loss 26.59113311767578, Transition Loss 8.01469898223877, Classifier Loss 0.47890135645866394, Total Loss 262.2221374511719\n",
      "4: Encoding Loss 25.879182815551758, Transition Loss 9.80077075958252, Classifier Loss 0.4607955813407898, Total Loss 255.0731964111328\n",
      "4: Encoding Loss 24.477270126342773, Transition Loss 7.104297637939453, Classifier Loss 0.49436742067337036, Total Loss 246.67575073242188\n",
      "4: Encoding Loss 25.63020896911621, Transition Loss 7.462045669555664, Classifier Loss 0.47812455892562866, Total Loss 254.34654235839844\n",
      "4: Encoding Loss 23.610332489013672, Transition Loss 6.948668479919434, Classifier Loss 0.4785939157009125, Total Loss 238.13180541992188\n",
      "4: Encoding Loss 24.915983200073242, Transition Loss 7.784642219543457, Classifier Loss 0.4865167737007141, Total Loss 249.53646850585938\n",
      "4: Encoding Loss 26.213468551635742, Transition Loss 8.54069709777832, Classifier Loss 0.4612043797969818, Total Loss 257.53631591796875\n",
      "4: Encoding Loss 24.650442123413086, Transition Loss 9.580183982849121, Classifier Loss 0.4918343722820282, Total Loss 248.30300903320312\n",
      "4: Encoding Loss 25.052370071411133, Transition Loss 7.880718231201172, Classifier Loss 0.4745604395866394, Total Loss 249.45114135742188\n",
      "4: Encoding Loss 26.132068634033203, Transition Loss 8.881497383117676, Classifier Loss 0.48657330870628357, Total Loss 259.49017333984375\n",
      "4: Encoding Loss 24.79833984375, Transition Loss 8.442434310913086, Classifier Loss 0.4977823495864868, Total Loss 249.8534393310547\n",
      "4: Encoding Loss 24.166383743286133, Transition Loss 7.459406852722168, Classifier Loss 0.45006075501441956, Total Loss 239.8290252685547\n",
      "4: Encoding Loss 25.333892822265625, Transition Loss 7.215677738189697, Classifier Loss 0.4891728162765503, Total Loss 253.03155517578125\n",
      "4: Encoding Loss 24.44508171081543, Transition Loss 6.582126617431641, Classifier Loss 0.46690651774406433, Total Loss 243.56773376464844\n",
      "4: Encoding Loss 24.73531723022461, Transition Loss 6.683689117431641, Classifier Loss 0.49780017137527466, Total Loss 248.99929809570312\n",
      "4: Encoding Loss 24.76490020751953, Transition Loss 7.625054836273193, Classifier Loss 0.4863271117210388, Total Loss 248.27691650390625\n",
      "4: Encoding Loss 24.579408645629883, Transition Loss 7.784905433654785, Classifier Loss 0.474855899810791, Total Loss 245.67782592773438\n",
      "4: Encoding Loss 26.195585250854492, Transition Loss 6.455593109130859, Classifier Loss 0.47656023502349854, Total Loss 258.5118103027344\n",
      "4: Encoding Loss 23.747282028198242, Transition Loss 5.940436840057373, Classifier Loss 0.4506569504737854, Total Loss 236.23202514648438\n",
      "4: Encoding Loss 25.56863021850586, Transition Loss 6.006310939788818, Classifier Loss 0.4994227886199951, Total Loss 255.69259643554688\n",
      "4: Encoding Loss 25.826068878173828, Transition Loss 8.763031959533691, Classifier Loss 0.48019659519195557, Total Loss 256.38079833984375\n",
      "4: Encoding Loss 24.508914947509766, Transition Loss 6.684366703033447, Classifier Loss 0.47665703296661377, Total Loss 245.0738983154297\n",
      "4: Encoding Loss 25.78012466430664, Transition Loss 6.020086288452148, Classifier Loss 0.48508670926094055, Total Loss 255.95367431640625\n",
      "4: Encoding Loss 24.383394241333008, Transition Loss 6.338027000427246, Classifier Loss 0.49594196677207947, Total Loss 245.928955078125\n",
      "4: Encoding Loss 24.88679313659668, Transition Loss 8.373212814331055, Classifier Loss 0.4873996675014496, Total Loss 249.5089569091797\n",
      "4: Encoding Loss 25.050107955932617, Transition Loss 7.411004066467285, Classifier Loss 0.47724223136901855, Total Loss 249.6072998046875\n",
      "4: Encoding Loss 25.185291290283203, Transition Loss 7.12098503112793, Classifier Loss 0.4708176255226135, Total Loss 249.98828125\n",
      "4: Encoding Loss 24.886587142944336, Transition Loss 6.748081207275391, Classifier Loss 0.46239006519317627, Total Loss 246.68130493164062\n",
      "4: Encoding Loss 23.563940048217773, Transition Loss 4.0988898277282715, Classifier Loss 0.46818768978118896, Total Loss 236.1500701904297\n",
      "4: Encoding Loss 25.87428855895996, Transition Loss 7.761709213256836, Classifier Loss 0.48181742429733276, Total Loss 256.7283935546875\n",
      "4: Encoding Loss 24.307933807373047, Transition Loss 5.501498222351074, Classifier Loss 0.49902859330177307, Total Loss 245.46664428710938\n",
      "4: Encoding Loss 23.279741287231445, Transition Loss 7.639957427978516, Classifier Loss 0.48178935050964355, Total Loss 235.94485473632812\n",
      "4: Encoding Loss 26.795719146728516, Transition Loss 5.687370777130127, Classifier Loss 0.48110827803611755, Total Loss 263.61407470703125\n",
      "4: Encoding Loss 25.99191665649414, Transition Loss 6.184528827667236, Classifier Loss 0.4853745698928833, Total Loss 257.7096862792969\n",
      "4: Encoding Loss 25.23065185546875, Transition Loss 6.266040325164795, Classifier Loss 0.5008682012557983, Total Loss 253.18524169921875\n",
      "4: Encoding Loss 23.220592498779297, Transition Loss 7.315011978149414, Classifier Loss 0.4780943989753723, Total Loss 235.0371856689453\n",
      "4: Encoding Loss 24.77766227722168, Transition Loss 7.609987735748291, Classifier Loss 0.4974531829357147, Total Loss 249.48861694335938\n",
      "4: Encoding Loss 24.441747665405273, Transition Loss 6.416309833526611, Classifier Loss 0.4887501001358032, Total Loss 245.6922607421875\n",
      "4: Encoding Loss 23.090375900268555, Transition Loss 7.583572864532471, Classifier Loss 0.49300146102905273, Total Loss 235.5398712158203\n",
      "4: Encoding Loss 24.992101669311523, Transition Loss 6.169199466705322, Classifier Loss 0.47565579414367676, Total Loss 248.73623657226562\n",
      "4: Encoding Loss 24.355358123779297, Transition Loss 6.923776626586914, Classifier Loss 0.5177926421165466, Total Loss 248.00689697265625\n",
      "4: Encoding Loss 22.67966651916504, Transition Loss 4.878600597381592, Classifier Loss 0.501052737236023, Total Loss 232.51832580566406\n",
      "4: Encoding Loss 25.074033737182617, Transition Loss 7.714916229248047, Classifier Loss 0.49702295660972595, Total Loss 251.83755493164062\n",
      "4: Encoding Loss 24.747251510620117, Transition Loss 6.727451801300049, Classifier Loss 0.46045589447021484, Total Loss 245.3690948486328\n",
      "4: Encoding Loss 25.862913131713867, Transition Loss 7.1034932136535645, Classifier Loss 0.46319276094436646, Total Loss 254.64328002929688\n",
      "4: Encoding Loss 24.326494216918945, Transition Loss 6.522952079772949, Classifier Loss 0.49087512493133545, Total Loss 245.00405883789062\n",
      "4: Encoding Loss 24.646135330200195, Transition Loss 5.482605457305908, Classifier Loss 0.4875628650188446, Total Loss 247.0218963623047\n",
      "4: Encoding Loss 24.738740921020508, Transition Loss 5.12957763671875, Classifier Loss 0.4974496364593506, Total Loss 248.68080139160156\n",
      "4: Encoding Loss 26.464113235473633, Transition Loss 8.332609176635742, Classifier Loss 0.48296481370925903, Total Loss 261.6759338378906\n",
      "4: Encoding Loss 24.941972732543945, Transition Loss 6.382487773895264, Classifier Loss 0.49301111698150635, Total Loss 250.1134033203125\n",
      "4: Encoding Loss 23.196910858154297, Transition Loss 5.306783199310303, Classifier Loss 0.4824519455432892, Total Loss 234.88185119628906\n",
      "4: Encoding Loss 24.102697372436523, Transition Loss 6.494890213012695, Classifier Loss 0.4698658585548401, Total Loss 241.10714721679688\n",
      "4: Encoding Loss 24.018516540527344, Transition Loss 5.540014266967773, Classifier Loss 0.48675066232681274, Total Loss 241.9311981201172\n",
      "4: Encoding Loss 24.360126495361328, Transition Loss 4.065889835357666, Classifier Loss 0.4754796028137207, Total Loss 243.2421417236328\n",
      "4: Encoding Loss 23.648893356323242, Transition Loss 4.236058235168457, Classifier Loss 0.47021329402923584, Total Loss 237.0596923828125\n",
      "4: Encoding Loss 25.356287002563477, Transition Loss 7.485025405883789, Classifier Loss 0.4614763557910919, Total Loss 250.49493408203125\n",
      "4: Encoding Loss 23.778467178344727, Transition Loss 4.1374077796936035, Classifier Loss 0.4848453998565674, Total Loss 239.53976440429688\n",
      "4: Encoding Loss 25.611894607543945, Transition Loss 5.240699768066406, Classifier Loss 0.5023784041404724, Total Loss 256.18115234375\n",
      "4: Encoding Loss 24.010719299316406, Transition Loss 6.085752010345459, Classifier Loss 0.4821470379829407, Total Loss 241.51760864257812\n",
      "4: Encoding Loss 23.978931427001953, Transition Loss 6.89543342590332, Classifier Loss 0.4670601487159729, Total Loss 239.91656494140625\n",
      "4: Encoding Loss 23.636737823486328, Transition Loss 5.937280178070068, Classifier Loss 0.48744192719459534, Total Loss 239.0255584716797\n",
      "4: Encoding Loss 25.49005699157715, Transition Loss 7.266288757324219, Classifier Loss 0.49436503648757935, Total Loss 254.8102264404297\n",
      "4: Encoding Loss 23.23154640197754, Transition Loss 8.457439422607422, Classifier Loss 0.5243747234344482, Total Loss 239.9813232421875\n",
      "4: Encoding Loss 25.105745315551758, Transition Loss 6.438177108764648, Classifier Loss 0.4731653928756714, Total Loss 249.45013427734375\n",
      "4: Encoding Loss 24.366771697998047, Transition Loss 5.953062534332275, Classifier Loss 0.45408526062965393, Total Loss 241.5333251953125\n",
      "4: Encoding Loss 23.491100311279297, Transition Loss 5.24676513671875, Classifier Loss 0.474602073431015, Total Loss 236.43836975097656\n",
      "4: Encoding Loss 24.096927642822266, Transition Loss 4.800998210906982, Classifier Loss 0.4756648540496826, Total Loss 241.30210876464844\n",
      "4: Encoding Loss 22.917516708374023, Transition Loss 6.2858710289001465, Classifier Loss 0.46729904413223267, Total Loss 231.32720947265625\n",
      "4: Encoding Loss 23.364524841308594, Transition Loss 5.984434604644775, Classifier Loss 0.46300023794174194, Total Loss 234.41310119628906\n",
      "4: Encoding Loss 23.581424713134766, Transition Loss 4.812746047973633, Classifier Loss 0.49287089705467224, Total Loss 238.9010467529297\n",
      "4: Encoding Loss 24.371232986450195, Transition Loss 4.926774501800537, Classifier Loss 0.4861358106136322, Total Loss 244.5688018798828\n",
      "4: Encoding Loss 22.661787033081055, Transition Loss 4.601797103881836, Classifier Loss 0.4685058891773224, Total Loss 229.06524658203125\n",
      "4: Encoding Loss 23.138166427612305, Transition Loss 5.820027828216553, Classifier Loss 0.4680643379688263, Total Loss 233.07577514648438\n",
      "4: Encoding Loss 26.68352699279785, Transition Loss 5.785196304321289, Classifier Loss 0.5170266032218933, Total Loss 266.3279113769531\n",
      "4: Encoding Loss 24.373775482177734, Transition Loss 7.183769702911377, Classifier Loss 0.5223352313041687, Total Loss 248.66049194335938\n",
      "4: Encoding Loss 25.292259216308594, Transition Loss 6.331775188446045, Classifier Loss 0.467917799949646, Total Loss 250.39620971679688\n",
      "4: Encoding Loss 25.148738861083984, Transition Loss 7.653275966644287, Classifier Loss 0.48589929938316345, Total Loss 251.31051635742188\n",
      "4: Encoding Loss 24.858699798583984, Transition Loss 6.011227607727051, Classifier Loss 0.46926528215408325, Total Loss 246.99838256835938\n",
      "4: Encoding Loss 23.4728946685791, Transition Loss 5.8954315185546875, Classifier Loss 0.47145870327949524, Total Loss 236.10812377929688\n",
      "4: Encoding Loss 26.063377380371094, Transition Loss 4.881756782531738, Classifier Loss 0.4845772087574005, Total Loss 257.94110107421875\n",
      "4: Encoding Loss 23.600370407104492, Transition Loss 3.301506519317627, Classifier Loss 0.4756905734539032, Total Loss 237.03233337402344\n",
      "4: Encoding Loss 24.507633209228516, Transition Loss 5.203925609588623, Classifier Loss 0.49081915616989136, Total Loss 246.1837615966797\n",
      "4: Encoding Loss 21.928165435791016, Transition Loss 5.0968427658081055, Classifier Loss 0.47329023480415344, Total Loss 223.77371215820312\n",
      "4: Encoding Loss 23.38438606262207, Transition Loss 3.5211596488952637, Classifier Loss 0.5001017451286316, Total Loss 237.7895050048828\n",
      "4: Encoding Loss 25.289403915405273, Transition Loss 8.021123886108398, Classifier Loss 0.47078633308410645, Total Loss 250.99807739257812\n",
      "4: Encoding Loss 22.89804458618164, Transition Loss 5.516200542449951, Classifier Loss 0.46624594926834106, Total Loss 230.91220092773438\n",
      "4: Encoding Loss 25.961946487426758, Transition Loss 6.928863525390625, Classifier Loss 0.4646887183189392, Total Loss 255.55023193359375\n",
      "4: Encoding Loss 24.675609588623047, Transition Loss 7.929915428161621, Classifier Loss 0.49900323152542114, Total Loss 248.8911895751953\n",
      "4: Encoding Loss 24.465389251708984, Transition Loss 8.654004096984863, Classifier Loss 0.4516175389289856, Total Loss 242.61569213867188\n",
      "4: Encoding Loss 22.151487350463867, Transition Loss 10.036075592041016, Classifier Loss 0.4715396761894226, Total Loss 226.3730926513672\n",
      "5: Encoding Loss 24.435039520263672, Transition Loss 5.4178667068481445, Classifier Loss 0.4895658791065216, Total Loss 245.52049255371094\n",
      "5: Encoding Loss 24.69864273071289, Transition Loss 6.81732177734375, Classifier Loss 0.4977293610572815, Total Loss 248.7255401611328\n",
      "5: Encoding Loss 25.984485626220703, Transition Loss 5.98616886138916, Classifier Loss 0.47228342294692993, Total Loss 256.30145263671875\n",
      "5: Encoding Loss 24.024633407592773, Transition Loss 5.984039306640625, Classifier Loss 0.4586710035800934, Total Loss 239.26097106933594\n",
      "5: Encoding Loss 25.207883834838867, Transition Loss 5.443186283111572, Classifier Loss 0.49599671363830566, Total Loss 252.35137939453125\n",
      "5: Encoding Loss 23.50154685974121, Transition Loss 6.587623596191406, Classifier Loss 0.4733563959598541, Total Loss 236.66554260253906\n",
      "5: Encoding Loss 24.420671463012695, Transition Loss 4.432918071746826, Classifier Loss 0.4727046489715576, Total Loss 243.5224151611328\n",
      "5: Encoding Loss 25.956298828125, Transition Loss 6.881114959716797, Classifier Loss 0.50543212890625, Total Loss 259.56982421875\n",
      "5: Encoding Loss 25.02973747253418, Transition Loss 5.243689060211182, Classifier Loss 0.49486643075942993, Total Loss 250.77328491210938\n",
      "5: Encoding Loss 26.978816986083984, Transition Loss 7.395907878875732, Classifier Loss 0.49528414011001587, Total Loss 266.8381652832031\n",
      "5: Encoding Loss 25.028526306152344, Transition Loss 5.367223739624023, Classifier Loss 0.4736229479312897, Total Loss 248.6639404296875\n",
      "5: Encoding Loss 24.647869110107422, Transition Loss 5.967399597167969, Classifier Loss 0.45379847288131714, Total Loss 243.7563018798828\n",
      "5: Encoding Loss 23.358539581298828, Transition Loss 5.098405361175537, Classifier Loss 0.49514254927635193, Total Loss 237.40225219726562\n",
      "5: Encoding Loss 24.24843406677246, Transition Loss 5.488470077514648, Classifier Loss 0.4468018412590027, Total Loss 239.76535034179688\n",
      "5: Encoding Loss 23.26504898071289, Transition Loss 5.582024574279785, Classifier Loss 0.4934841990470886, Total Loss 236.58522033691406\n",
      "5: Encoding Loss 23.584880828857422, Transition Loss 6.789854526519775, Classifier Loss 0.4632420241832733, Total Loss 236.36123657226562\n",
      "5: Encoding Loss 23.70363426208496, Transition Loss 5.6192474365234375, Classifier Loss 0.4737170338630676, Total Loss 238.1246337890625\n",
      "5: Encoding Loss 23.67853355407715, Transition Loss 6.162454605102539, Classifier Loss 0.4612571597099304, Total Loss 236.78648376464844\n",
      "5: Encoding Loss 23.40534019470215, Transition Loss 6.037925720214844, Classifier Loss 0.5130451321601868, Total Loss 239.75482177734375\n",
      "5: Encoding Loss 25.08051109313965, Transition Loss 6.809704780578613, Classifier Loss 0.4451497495174408, Total Loss 246.52099609375\n",
      "5: Encoding Loss 23.68191909790039, Transition Loss 7.006270885467529, Classifier Loss 0.4751870036125183, Total Loss 238.3753204345703\n",
      "5: Encoding Loss 23.433595657348633, Transition Loss 4.831111907958984, Classifier Loss 0.47733810544013977, Total Loss 236.16879272460938\n",
      "5: Encoding Loss 22.518924713134766, Transition Loss 8.821006774902344, Classifier Loss 0.486715167760849, Total Loss 230.5871124267578\n",
      "5: Encoding Loss 22.39893341064453, Transition Loss 4.534082889556885, Classifier Loss 0.5145671963691711, Total Loss 231.5550079345703\n",
      "5: Encoding Loss 24.385648727416992, Transition Loss 4.028303146362305, Classifier Loss 0.4593855142593384, Total Loss 241.82940673828125\n",
      "5: Encoding Loss 24.770044326782227, Transition Loss 6.400485038757324, Classifier Loss 0.45588821172714233, Total Loss 245.02926635742188\n",
      "5: Encoding Loss 25.539329528808594, Transition Loss 7.227241516113281, Classifier Loss 0.4846389889717102, Total Loss 254.22398376464844\n",
      "5: Encoding Loss 23.88908576965332, Transition Loss 6.891844749450684, Classifier Loss 0.48781535029411316, Total Loss 241.27259826660156\n",
      "5: Encoding Loss 24.5087890625, Transition Loss 6.320425033569336, Classifier Loss 0.4733675420284271, Total Loss 244.67115783691406\n",
      "5: Encoding Loss 23.386455535888672, Transition Loss 7.214327335357666, Classifier Loss 0.47881197929382324, Total Loss 236.4157257080078\n",
      "5: Encoding Loss 24.294334411621094, Transition Loss 6.32523250579834, Classifier Loss 0.45010483264923096, Total Loss 240.6302032470703\n",
      "5: Encoding Loss 22.80584144592285, Transition Loss 6.655073165893555, Classifier Loss 0.4777190089225769, Total Loss 231.5496368408203\n",
      "5: Encoding Loss 24.338764190673828, Transition Loss 7.265083312988281, Classifier Loss 0.48834681510925293, Total Loss 244.99781799316406\n",
      "5: Encoding Loss 23.984296798706055, Transition Loss 3.035784959793091, Classifier Loss 0.5257900357246399, Total Loss 245.06053161621094\n",
      "5: Encoding Loss 23.264062881469727, Transition Loss 6.663780689239502, Classifier Loss 0.4689479470252991, Total Loss 234.34005737304688\n",
      "5: Encoding Loss 22.591203689575195, Transition Loss 3.453265428543091, Classifier Loss 0.48803120851516724, Total Loss 230.22340393066406\n",
      "5: Encoding Loss 24.83140754699707, Transition Loss 7.116481781005859, Classifier Loss 0.5075055956840515, Total Loss 250.8251190185547\n",
      "5: Encoding Loss 23.687236785888672, Transition Loss 8.26033878326416, Classifier Loss 0.47687241435050964, Total Loss 238.83721923828125\n",
      "5: Encoding Loss 23.345849990844727, Transition Loss 6.182589054107666, Classifier Loss 0.44245508313179016, Total Loss 232.2488250732422\n",
      "5: Encoding Loss 23.54811668395996, Transition Loss 6.495595455169678, Classifier Loss 0.4565976858139038, Total Loss 235.3438262939453\n",
      "5: Encoding Loss 23.548038482666016, Transition Loss 7.628026962280273, Classifier Loss 0.477766752243042, Total Loss 237.68658447265625\n",
      "5: Encoding Loss 24.133275985717773, Transition Loss 3.9356496334075928, Classifier Loss 0.49127161502838135, Total Loss 242.98049926757812\n",
      "5: Encoding Loss 23.74881935119629, Transition Loss 4.462863922119141, Classifier Loss 0.4625796675682068, Total Loss 237.14109802246094\n",
      "5: Encoding Loss 23.03786277770996, Transition Loss 2.737205982208252, Classifier Loss 0.4747391939163208, Total Loss 232.3242645263672\n",
      "5: Encoding Loss 24.119251251220703, Transition Loss 5.430968284606934, Classifier Loss 0.47982609272003174, Total Loss 242.02281188964844\n",
      "5: Encoding Loss 24.42431640625, Transition Loss 6.234418869018555, Classifier Loss 0.4865550696849823, Total Loss 245.2969207763672\n",
      "5: Encoding Loss 24.54187774658203, Transition Loss 7.485589981079102, Classifier Loss 0.4930565655231476, Total Loss 247.13780212402344\n",
      "5: Encoding Loss 24.720081329345703, Transition Loss 6.196356296539307, Classifier Loss 0.4852639436721802, Total Loss 247.5263214111328\n",
      "5: Encoding Loss 24.80971908569336, Transition Loss 6.196889877319336, Classifier Loss 0.48197275400161743, Total Loss 247.9144287109375\n",
      "5: Encoding Loss 24.183855056762695, Transition Loss 5.917608261108398, Classifier Loss 0.46181201934814453, Total Loss 240.83555603027344\n",
      "5: Encoding Loss 23.34845733642578, Transition Loss 4.096043586730957, Classifier Loss 0.47794410586357117, Total Loss 235.40127563476562\n",
      "5: Encoding Loss 23.139095306396484, Transition Loss 4.159855365753174, Classifier Loss 0.43946805596351624, Total Loss 229.8915557861328\n",
      "5: Encoding Loss 22.385290145874023, Transition Loss 3.61930775642395, Classifier Loss 0.4543986916542053, Total Loss 225.2460479736328\n",
      "5: Encoding Loss 22.977802276611328, Transition Loss 6.4447550773620605, Classifier Loss 0.4528021812438965, Total Loss 230.39158630371094\n",
      "5: Encoding Loss 23.365367889404297, Transition Loss 6.583183288574219, Classifier Loss 0.4665658175945282, Total Loss 234.89617919921875\n",
      "5: Encoding Loss 22.92285919189453, Transition Loss 5.012841701507568, Classifier Loss 0.47688430547714233, Total Loss 232.07386779785156\n",
      "5: Encoding Loss 23.808725357055664, Transition Loss 5.535973072052002, Classifier Loss 0.4517923593521118, Total Loss 236.75624084472656\n",
      "5: Encoding Loss 23.44598388671875, Transition Loss 5.302359104156494, Classifier Loss 0.46670451760292053, Total Loss 235.29879760742188\n",
      "5: Encoding Loss 23.125459671020508, Transition Loss 5.492630481719971, Classifier Loss 0.46591559052467346, Total Loss 232.6937713623047\n",
      "5: Encoding Loss 22.59815216064453, Transition Loss 4.83144474029541, Classifier Loss 0.47736090421676636, Total Loss 229.4875946044922\n",
      "5: Encoding Loss 24.245220184326172, Transition Loss 7.470326900482178, Classifier Loss 0.47406086325645447, Total Loss 242.86192321777344\n",
      "5: Encoding Loss 25.41013526916504, Transition Loss 6.603313446044922, Classifier Loss 0.46624860167503357, Total Loss 251.2266082763672\n",
      "5: Encoding Loss 24.014028549194336, Transition Loss 4.569419860839844, Classifier Loss 0.4582214951515198, Total Loss 238.8482666015625\n",
      "5: Encoding Loss 23.939062118530273, Transition Loss 5.039804935455322, Classifier Loss 0.45987850427627563, Total Loss 238.50831604003906\n",
      "5: Encoding Loss 22.724132537841797, Transition Loss 5.8404059410095215, Classifier Loss 0.46102455258369446, Total Loss 229.06361389160156\n",
      "5: Encoding Loss 22.697368621826172, Transition Loss 5.051584243774414, Classifier Loss 0.47384655475616455, Total Loss 229.97393798828125\n",
      "5: Encoding Loss 23.53806495666504, Transition Loss 5.490900039672852, Classifier Loss 0.4424108862876892, Total Loss 233.64378356933594\n",
      "5: Encoding Loss 24.867300033569336, Transition Loss 5.921335220336914, Classifier Loss 0.4406944215297699, Total Loss 244.19210815429688\n",
      "5: Encoding Loss 25.015052795410156, Transition Loss 7.994640350341797, Classifier Loss 0.49090468883514404, Total Loss 250.809814453125\n",
      "5: Encoding Loss 23.25063705444336, Transition Loss 5.287952899932861, Classifier Loss 0.4676344692707062, Total Loss 233.82614135742188\n",
      "5: Encoding Loss 23.517879486083984, Transition Loss 6.188974857330322, Classifier Loss 0.4699554741382599, Total Loss 236.3763885498047\n",
      "5: Encoding Loss 22.466346740722656, Transition Loss 5.502354145050049, Classifier Loss 0.4641844928264618, Total Loss 227.24969482421875\n",
      "5: Encoding Loss 22.569440841674805, Transition Loss 6.961731433868408, Classifier Loss 0.4426063299179077, Total Loss 226.20851135253906\n",
      "5: Encoding Loss 23.752429962158203, Transition Loss 6.610246181488037, Classifier Loss 0.42932409048080444, Total Loss 234.27389526367188\n",
      "5: Encoding Loss 23.494855880737305, Transition Loss 8.44488525390625, Classifier Loss 0.4700061082839966, Total Loss 236.6484375\n",
      "5: Encoding Loss 24.743000030517578, Transition Loss 5.941734313964844, Classifier Loss 0.4757366180419922, Total Loss 246.70602416992188\n",
      "5: Encoding Loss 23.749675750732422, Transition Loss 7.372139930725098, Classifier Loss 0.44931963086128235, Total Loss 236.40380859375\n",
      "5: Encoding Loss 23.694860458374023, Transition Loss 6.631573677062988, Classifier Loss 0.49198484420776367, Total Loss 240.08367919921875\n",
      "5: Encoding Loss 22.46350860595703, Transition Loss 6.4715142250061035, Classifier Loss 0.4446897506713867, Total Loss 225.47134399414062\n",
      "5: Encoding Loss 24.06963348388672, Transition Loss 5.1111040115356445, Classifier Loss 0.48985037207603455, Total Loss 242.5643310546875\n",
      "5: Encoding Loss 23.872098922729492, Transition Loss 5.623177528381348, Classifier Loss 0.4591873586177826, Total Loss 238.02015686035156\n",
      "5: Encoding Loss 23.545568466186523, Transition Loss 4.970821380615234, Classifier Loss 0.5021051168441772, Total Loss 239.56922912597656\n",
      "5: Encoding Loss 22.98276710510254, Transition Loss 6.589012145996094, Classifier Loss 0.4621027112007141, Total Loss 231.39019775390625\n",
      "5: Encoding Loss 23.810070037841797, Transition Loss 5.942812442779541, Classifier Loss 0.47403597831726074, Total Loss 239.07273864746094\n",
      "5: Encoding Loss 24.837913513183594, Transition Loss 4.956435680389404, Classifier Loss 0.47281646728515625, Total Loss 246.9762420654297\n",
      "5: Encoding Loss 22.079694747924805, Transition Loss 5.040175437927246, Classifier Loss 0.4565279483795166, Total Loss 223.2983856201172\n",
      "5: Encoding Loss 23.66090202331543, Transition Loss 4.861398696899414, Classifier Loss 0.4846251606941223, Total Loss 238.7220001220703\n",
      "5: Encoding Loss 23.695276260375977, Transition Loss 6.855445384979248, Classifier Loss 0.4780641198158264, Total Loss 238.73971557617188\n",
      "5: Encoding Loss 23.58716583251953, Transition Loss 5.538693428039551, Classifier Loss 0.45394834876060486, Total Loss 235.1999053955078\n",
      "5: Encoding Loss 23.998592376708984, Transition Loss 4.124865531921387, Classifier Loss 0.46466904878616333, Total Loss 239.28062438964844\n",
      "5: Encoding Loss 23.299959182739258, Transition Loss 5.194211006164551, Classifier Loss 0.4738349914550781, Total Loss 234.822021484375\n",
      "5: Encoding Loss 24.231779098510742, Transition Loss 6.4409942626953125, Classifier Loss 0.4876255989074707, Total Loss 243.9049835205078\n",
      "5: Encoding Loss 22.8881893157959, Transition Loss 6.5133056640625, Classifier Loss 0.4909093677997589, Total Loss 233.49911499023438\n",
      "5: Encoding Loss 24.193206787109375, Transition Loss 5.169825553894043, Classifier Loss 0.45800304412841797, Total Loss 240.3799285888672\n",
      "5: Encoding Loss 23.65814208984375, Transition Loss 5.545976161956787, Classifier Loss 0.4375419020652771, Total Loss 234.12850952148438\n",
      "5: Encoding Loss 23.53671646118164, Transition Loss 3.377633571624756, Classifier Loss 0.4792049527168274, Total Loss 236.88975524902344\n",
      "5: Encoding Loss 23.882122039794922, Transition Loss 6.477949619293213, Classifier Loss 0.48438218235969543, Total Loss 240.79080200195312\n",
      "5: Encoding Loss 22.91936492919922, Transition Loss 4.379520416259766, Classifier Loss 0.45814716815948486, Total Loss 230.0455322265625\n",
      "5: Encoding Loss 22.21784782409668, Transition Loss 6.876911163330078, Classifier Loss 0.491406112909317, Total Loss 228.25877380371094\n",
      "5: Encoding Loss 25.165342330932617, Transition Loss 4.225988388061523, Classifier Loss 0.45260974764823914, Total Loss 247.4289093017578\n",
      "5: Encoding Loss 24.11107635498047, Transition Loss 4.9279279708862305, Classifier Loss 0.47774410247802734, Total Loss 241.6486053466797\n",
      "5: Encoding Loss 23.416919708251953, Transition Loss 5.001848220825195, Classifier Loss 0.459012508392334, Total Loss 234.23696899414062\n",
      "5: Encoding Loss 22.49715232849121, Transition Loss 6.634058952331543, Classifier Loss 0.4799154996871948, Total Loss 229.2955780029297\n",
      "5: Encoding Loss 23.495277404785156, Transition Loss 6.470793724060059, Classifier Loss 0.4679630696773529, Total Loss 236.0526885986328\n",
      "5: Encoding Loss 22.952823638916016, Transition Loss 5.558512210845947, Classifier Loss 0.48696765303611755, Total Loss 233.43106079101562\n",
      "5: Encoding Loss 22.077354431152344, Transition Loss 7.140490531921387, Classifier Loss 0.49652740359306335, Total Loss 227.69967651367188\n",
      "5: Encoding Loss 23.345853805541992, Transition Loss 5.398318290710449, Classifier Loss 0.4779915511608124, Total Loss 235.64564514160156\n",
      "5: Encoding Loss 23.482013702392578, Transition Loss 5.859309196472168, Classifier Loss 0.47828447818756104, Total Loss 236.85641479492188\n",
      "5: Encoding Loss 21.858762741088867, Transition Loss 4.5244903564453125, Classifier Loss 0.45753514766693115, Total Loss 221.52850341796875\n",
      "5: Encoding Loss 23.87885093688965, Transition Loss 6.626614570617676, Classifier Loss 0.47539445757865906, Total Loss 239.89556884765625\n",
      "5: Encoding Loss 23.940954208374023, Transition Loss 5.721242904663086, Classifier Loss 0.4488987624645233, Total Loss 237.56175231933594\n",
      "5: Encoding Loss 24.037084579467773, Transition Loss 6.114028453826904, Classifier Loss 0.43284475803375244, Total Loss 236.803955078125\n",
      "5: Encoding Loss 23.26930046081543, Transition Loss 5.461987495422363, Classifier Loss 0.44027453660964966, Total Loss 231.2742462158203\n",
      "5: Encoding Loss 23.63205909729004, Transition Loss 4.576862335205078, Classifier Loss 0.4378863275051117, Total Loss 233.76048278808594\n",
      "5: Encoding Loss 23.295042037963867, Transition Loss 4.225157260894775, Classifier Loss 0.46785491704940796, Total Loss 233.99085998535156\n",
      "5: Encoding Loss 25.601078033447266, Transition Loss 6.8757195472717285, Classifier Loss 0.45424824953079224, Total Loss 251.60858154296875\n",
      "5: Encoding Loss 24.027484893798828, Transition Loss 5.180302619934082, Classifier Loss 0.5325907468795776, Total Loss 246.5150146484375\n",
      "5: Encoding Loss 22.678333282470703, Transition Loss 4.786031723022461, Classifier Loss 0.502656877040863, Total Loss 232.64955139160156\n",
      "5: Encoding Loss 22.991016387939453, Transition Loss 5.720030307769775, Classifier Loss 0.45061343908309937, Total Loss 230.13348388671875\n",
      "5: Encoding Loss 22.831302642822266, Transition Loss 4.736542701721191, Classifier Loss 0.46810445189476013, Total Loss 230.40817260742188\n",
      "5: Encoding Loss 22.443878173828125, Transition Loss 3.3785736560821533, Classifier Loss 0.448620080947876, Total Loss 225.0887451171875\n",
      "5: Encoding Loss 22.14854621887207, Transition Loss 3.7581393718719482, Classifier Loss 0.46342524886131287, Total Loss 224.28253173828125\n",
      "5: Encoding Loss 22.938989639282227, Transition Loss 6.580836772918701, Classifier Loss 0.4853741228580475, Total Loss 233.36549377441406\n",
      "5: Encoding Loss 23.350805282592773, Transition Loss 3.3903026580810547, Classifier Loss 0.43624570965766907, Total Loss 231.10906982421875\n",
      "5: Encoding Loss 24.298429489135742, Transition Loss 4.150071144104004, Classifier Loss 0.49650833010673523, Total Loss 244.8682861328125\n",
      "5: Encoding Loss 21.896432876586914, Transition Loss 5.534656524658203, Classifier Loss 0.4640108048915863, Total Loss 222.67947387695312\n",
      "5: Encoding Loss 22.823551177978516, Transition Loss 6.053652286529541, Classifier Loss 0.4764246344566345, Total Loss 231.44158935546875\n",
      "5: Encoding Loss 22.890058517456055, Transition Loss 5.277968883514404, Classifier Loss 0.47832226753234863, Total Loss 232.00828552246094\n",
      "5: Encoding Loss 23.84271240234375, Transition Loss 6.097837448120117, Classifier Loss 0.48772668838500977, Total Loss 240.73394775390625\n",
      "5: Encoding Loss 22.48048973083496, Transition Loss 7.7580437660217285, Classifier Loss 0.5051290988922119, Total Loss 231.90843200683594\n",
      "5: Encoding Loss 23.056352615356445, Transition Loss 5.167710781097412, Classifier Loss 0.4420279860496521, Total Loss 229.68716430664062\n",
      "5: Encoding Loss 23.054859161376953, Transition Loss 4.997798919677734, Classifier Loss 0.48312312364578247, Total Loss 233.75074768066406\n",
      "5: Encoding Loss 21.9591121673584, Transition Loss 4.475480079650879, Classifier Loss 0.4425165057182312, Total Loss 220.81964111328125\n",
      "5: Encoding Loss 23.683862686157227, Transition Loss 4.150535583496094, Classifier Loss 0.48510339856147766, Total Loss 238.8113555908203\n",
      "5: Encoding Loss 22.48145866394043, Transition Loss 5.287209510803223, Classifier Loss 0.48112431168556213, Total Loss 229.02154541015625\n",
      "5: Encoding Loss 22.34646987915039, Transition Loss 5.5199384689331055, Classifier Loss 0.45126229524612427, Total Loss 225.00196838378906\n",
      "5: Encoding Loss 22.930919647216797, Transition Loss 3.7724766731262207, Classifier Loss 0.4733913540840149, Total Loss 231.54100036621094\n",
      "5: Encoding Loss 23.693897247314453, Transition Loss 4.440645217895508, Classifier Loss 0.4386596977710724, Total Loss 234.30526733398438\n",
      "5: Encoding Loss 21.36748504638672, Transition Loss 3.9130754470825195, Classifier Loss 0.45775654911994934, Total Loss 217.49813842773438\n",
      "5: Encoding Loss 21.651086807250977, Transition Loss 5.594927787780762, Classifier Loss 0.4247698485851288, Total Loss 216.80467224121094\n",
      "5: Encoding Loss 25.025943756103516, Transition Loss 4.133538246154785, Classifier Loss 0.4879220128059387, Total Loss 249.8264617919922\n",
      "5: Encoding Loss 22.731149673461914, Transition Loss 6.923557281494141, Classifier Loss 0.5103667974472046, Total Loss 234.2705841064453\n",
      "5: Encoding Loss 23.053354263305664, Transition Loss 4.839935302734375, Classifier Loss 0.49076607823371887, Total Loss 234.471435546875\n",
      "5: Encoding Loss 23.13800048828125, Transition Loss 7.091304779052734, Classifier Loss 0.4614649713039398, Total Loss 232.66876220703125\n",
      "5: Encoding Loss 23.27337074279785, Transition Loss 4.80112361907959, Classifier Loss 0.4566754698753357, Total Loss 232.81472778320312\n",
      "5: Encoding Loss 22.41920280456543, Transition Loss 5.559131145477295, Classifier Loss 0.4758028984069824, Total Loss 228.04574584960938\n",
      "5: Encoding Loss 23.825159072875977, Transition Loss 3.6359872817993164, Classifier Loss 0.47146591544151306, Total Loss 238.47506713867188\n",
      "5: Encoding Loss 22.23991584777832, Transition Loss 2.8127665519714355, Classifier Loss 0.4676746129989624, Total Loss 225.24932861328125\n",
      "5: Encoding Loss 22.862476348876953, Transition Loss 4.357639789581299, Classifier Loss 0.46292635798454285, Total Loss 230.06396484375\n",
      "5: Encoding Loss 21.172119140625, Transition Loss 4.897354602813721, Classifier Loss 0.4717903733253479, Total Loss 217.5354766845703\n",
      "5: Encoding Loss 23.024723052978516, Transition Loss 2.8778395652770996, Classifier Loss 0.4844808578491211, Total Loss 233.221435546875\n",
      "5: Encoding Loss 22.9804744720459, Transition Loss 7.424448490142822, Classifier Loss 0.4778156578540802, Total Loss 233.11026000976562\n",
      "5: Encoding Loss 22.062185287475586, Transition Loss 4.49240779876709, Classifier Loss 0.4649883210659027, Total Loss 223.89479064941406\n",
      "5: Encoding Loss 24.66411018371582, Transition Loss 6.178900718688965, Classifier Loss 0.46213793754577637, Total Loss 244.762451171875\n",
      "5: Encoding Loss 23.22476577758789, Transition Loss 6.063025951385498, Classifier Loss 0.46323391795158386, Total Loss 233.33412170410156\n",
      "5: Encoding Loss 23.207151412963867, Transition Loss 8.166001319885254, Classifier Loss 0.4553489685058594, Total Loss 232.82530212402344\n",
      "5: Encoding Loss 21.052392959594727, Transition Loss 8.395289421081543, Classifier Loss 0.47385838627815247, Total Loss 217.48403930664062\n",
      "6: Encoding Loss 23.715463638305664, Transition Loss 4.978089809417725, Classifier Loss 0.4712609648704529, Total Loss 237.84542846679688\n",
      "6: Encoding Loss 23.955686569213867, Transition Loss 4.682921409606934, Classifier Loss 0.4966437518596649, Total Loss 242.2464599609375\n",
      "6: Encoding Loss 23.77302360534668, Transition Loss 4.991631507873535, Classifier Loss 0.4603501260280609, Total Loss 237.217529296875\n",
      "6: Encoding Loss 22.808563232421875, Transition Loss 4.317449569702148, Classifier Loss 0.48989617824554443, Total Loss 232.32162475585938\n",
      "6: Encoding Loss 23.73807716369629, Transition Loss 4.687568664550781, Classifier Loss 0.5051984190940857, Total Loss 241.36196899414062\n",
      "6: Encoding Loss 22.62504768371582, Transition Loss 5.230585098266602, Classifier Loss 0.4711911082267761, Total Loss 229.1656036376953\n",
      "6: Encoding Loss 23.935789108276367, Transition Loss 3.6390557289123535, Classifier Loss 0.49674785137176514, Total Loss 241.888916015625\n",
      "6: Encoding Loss 24.020923614501953, Transition Loss 4.716728210449219, Classifier Loss 0.4801434874534607, Total Loss 241.1250762939453\n",
      "6: Encoding Loss 23.754545211791992, Transition Loss 4.421946048736572, Classifier Loss 0.51225745677948, Total Loss 242.14649963378906\n",
      "6: Encoding Loss 24.59309959411621, Transition Loss 5.437121391296387, Classifier Loss 0.48832952976226807, Total Loss 246.66517639160156\n",
      "6: Encoding Loss 23.34977149963379, Transition Loss 4.183692455291748, Classifier Loss 0.45504653453826904, Total Loss 233.13955688476562\n",
      "6: Encoding Loss 23.24471092224121, Transition Loss 4.714835166931152, Classifier Loss 0.4568975865840912, Total Loss 232.5904083251953\n",
      "6: Encoding Loss 22.37726402282715, Transition Loss 4.395839691162109, Classifier Loss 0.4695204198360443, Total Loss 226.8493194580078\n",
      "6: Encoding Loss 23.260740280151367, Transition Loss 4.550609111785889, Classifier Loss 0.47172874212265015, Total Loss 234.16891479492188\n",
      "6: Encoding Loss 22.53988265991211, Transition Loss 4.881195545196533, Classifier Loss 0.4657004475593567, Total Loss 227.86537170410156\n",
      "6: Encoding Loss 22.746248245239258, Transition Loss 5.767446041107178, Classifier Loss 0.46450990438461304, Total Loss 229.574462890625\n",
      "6: Encoding Loss 22.661678314208984, Transition Loss 4.883030891418457, Classifier Loss 0.45301198959350586, Total Loss 227.5712432861328\n",
      "6: Encoding Loss 22.82423973083496, Transition Loss 5.250455856323242, Classifier Loss 0.4583413004875183, Total Loss 229.47813415527344\n",
      "6: Encoding Loss 23.22057342529297, Transition Loss 5.278788089752197, Classifier Loss 0.4950210154056549, Total Loss 236.32244873046875\n",
      "6: Encoding Loss 24.791032791137695, Transition Loss 5.633031845092773, Classifier Loss 0.46854040026664734, Total Loss 246.30889892578125\n",
      "6: Encoding Loss 22.578556060791016, Transition Loss 6.181129455566406, Classifier Loss 0.47687965631484985, Total Loss 229.55262756347656\n",
      "6: Encoding Loss 22.898691177368164, Transition Loss 4.047530651092529, Classifier Loss 0.4414832592010498, Total Loss 228.14736938476562\n",
      "6: Encoding Loss 21.996557235717773, Transition Loss 8.151766777038574, Classifier Loss 0.4719090461730957, Total Loss 224.79371643066406\n",
      "6: Encoding Loss 21.572345733642578, Transition Loss 3.9373233318328857, Classifier Loss 0.4579634666442871, Total Loss 219.16258239746094\n",
      "6: Encoding Loss 23.083919525146484, Transition Loss 3.4015681743621826, Classifier Loss 0.4277395009994507, Total Loss 228.12562561035156\n",
      "6: Encoding Loss 24.272912979125977, Transition Loss 5.235678195953369, Classifier Loss 0.4548461139202118, Total Loss 240.7150421142578\n",
      "6: Encoding Loss 23.96109962463379, Transition Loss 6.069918632507324, Classifier Loss 0.4986666142940521, Total Loss 242.76943969726562\n",
      "6: Encoding Loss 22.76397705078125, Transition Loss 6.001570224761963, Classifier Loss 0.43262794613838196, Total Loss 226.57492065429688\n",
      "6: Encoding Loss 22.66315269470215, Transition Loss 5.559353351593018, Classifier Loss 0.44431591033935547, Total Loss 226.84869384765625\n",
      "6: Encoding Loss 22.800474166870117, Transition Loss 6.468416690826416, Classifier Loss 0.4923279881477356, Total Loss 232.93028259277344\n",
      "6: Encoding Loss 22.680753707885742, Transition Loss 5.582252025604248, Classifier Loss 0.460668683052063, Total Loss 228.6293487548828\n",
      "6: Encoding Loss 21.552852630615234, Transition Loss 6.334508895874023, Classifier Loss 0.4793190360069275, Total Loss 221.62164306640625\n",
      "6: Encoding Loss 23.58816909790039, Transition Loss 6.104344844818115, Classifier Loss 0.4503260850906372, Total Loss 234.95883178710938\n",
      "6: Encoding Loss 22.138303756713867, Transition Loss 2.640875816345215, Classifier Loss 0.47573623061180115, Total Loss 225.20822143554688\n",
      "6: Encoding Loss 22.09168815612793, Transition Loss 5.809948921203613, Classifier Loss 0.4861711263656616, Total Loss 226.51260375976562\n",
      "6: Encoding Loss 21.234933853149414, Transition Loss 3.1100707054138184, Classifier Loss 0.49668896198272705, Total Loss 220.17037963867188\n",
      "6: Encoding Loss 23.25921630859375, Transition Loss 6.017477512359619, Classifier Loss 0.4413555860519409, Total Loss 231.41278076171875\n",
      "6: Encoding Loss 22.769378662109375, Transition Loss 7.272237777709961, Classifier Loss 0.466702938079834, Total Loss 230.27976989746094\n",
      "6: Encoding Loss 22.58392333984375, Transition Loss 5.053539276123047, Classifier Loss 0.4431185722351074, Total Loss 225.99395751953125\n",
      "6: Encoding Loss 21.991558074951172, Transition Loss 5.793605327606201, Classifier Loss 0.4744534492492676, Total Loss 224.5365447998047\n",
      "6: Encoding Loss 22.24191665649414, Transition Loss 6.535098075866699, Classifier Loss 0.4790666997432709, Total Loss 227.14903259277344\n",
      "6: Encoding Loss 22.550764083862305, Transition Loss 3.558999538421631, Classifier Loss 0.4730151295661926, Total Loss 228.41943359375\n",
      "6: Encoding Loss 22.447731018066406, Transition Loss 3.6561880111694336, Classifier Loss 0.4525013864040375, Total Loss 225.56321716308594\n",
      "6: Encoding Loss 22.120450973510742, Transition Loss 2.4030163288116455, Classifier Loss 0.4831807017326355, Total Loss 225.7622833251953\n",
      "6: Encoding Loss 23.762500762939453, Transition Loss 4.244826316833496, Classifier Loss 0.48437264561653137, Total Loss 239.38623046875\n",
      "6: Encoding Loss 23.313709259033203, Transition Loss 5.291395664215088, Classifier Loss 0.4668999910354614, Total Loss 234.25794982910156\n",
      "6: Encoding Loss 23.836698532104492, Transition Loss 6.455241680145264, Classifier Loss 0.4791083037853241, Total Loss 239.8954620361328\n",
      "6: Encoding Loss 22.605146408081055, Transition Loss 5.05015230178833, Classifier Loss 0.45412036776542664, Total Loss 227.2632293701172\n",
      "6: Encoding Loss 22.713191986083984, Transition Loss 5.071844100952148, Classifier Loss 0.49422571063041687, Total Loss 232.14248657226562\n",
      "6: Encoding Loss 23.046579360961914, Transition Loss 5.133162021636963, Classifier Loss 0.47804757952690125, Total Loss 233.2040252685547\n",
      "6: Encoding Loss 22.508508682250977, Transition Loss 3.4995720386505127, Classifier Loss 0.4658034145832062, Total Loss 227.34832763671875\n",
      "6: Encoding Loss 22.15647315979004, Transition Loss 3.6702990531921387, Classifier Loss 0.4422805607318878, Total Loss 222.21389770507812\n",
      "6: Encoding Loss 21.73474884033203, Transition Loss 3.2354793548583984, Classifier Loss 0.4715195298194885, Total Loss 221.67703247070312\n",
      "6: Encoding Loss 21.892681121826172, Transition Loss 5.846829891204834, Classifier Loss 0.47771427035331726, Total Loss 224.08226013183594\n",
      "6: Encoding Loss 21.706209182739258, Transition Loss 5.864492416381836, Classifier Loss 0.45522671937942505, Total Loss 220.34524536132812\n",
      "6: Encoding Loss 21.029630661010742, Transition Loss 4.802428245544434, Classifier Loss 0.4534863233566284, Total Loss 214.54615783691406\n",
      "6: Encoding Loss 23.13593864440918, Transition Loss 4.998830795288086, Classifier Loss 0.46262359619140625, Total Loss 232.34963989257812\n",
      "6: Encoding Loss 22.23998260498047, Transition Loss 4.902149677276611, Classifier Loss 0.46491509675979614, Total Loss 225.3917999267578\n",
      "6: Encoding Loss 22.84339141845703, Transition Loss 4.832636833190918, Classifier Loss 0.4658810496330261, Total Loss 230.3017578125\n",
      "6: Encoding Loss 21.686378479003906, Transition Loss 4.445920467376709, Classifier Loss 0.45297718048095703, Total Loss 219.6779327392578\n",
      "6: Encoding Loss 23.1550235748291, Transition Loss 6.507665634155273, Classifier Loss 0.4565756916999817, Total Loss 232.1992950439453\n",
      "6: Encoding Loss 23.447879791259766, Transition Loss 5.5094733238220215, Classifier Loss 0.45354700088500977, Total Loss 234.03964233398438\n",
      "6: Encoding Loss 22.59267807006836, Transition Loss 3.8997223377227783, Classifier Loss 0.457943320274353, Total Loss 227.31570434570312\n",
      "6: Encoding Loss 23.390283584594727, Transition Loss 4.3834099769592285, Classifier Loss 0.4564918279647827, Total Loss 233.64813232421875\n",
      "6: Encoding Loss 21.760629653930664, Transition Loss 5.202730178833008, Classifier Loss 0.4482514560222626, Total Loss 219.9507293701172\n",
      "6: Encoding Loss 21.512706756591797, Transition Loss 4.690988540649414, Classifier Loss 0.4629107415676117, Total Loss 219.33094787597656\n",
      "6: Encoding Loss 22.833349227905273, Transition Loss 4.652939796447754, Classifier Loss 0.43725162744522095, Total Loss 227.32254028320312\n",
      "6: Encoding Loss 24.217443466186523, Transition Loss 5.064715385437012, Classifier Loss 0.42790764570236206, Total Loss 237.54324340820312\n",
      "6: Encoding Loss 23.316219329833984, Transition Loss 6.473128795623779, Classifier Loss 0.4610098600387573, Total Loss 233.92538452148438\n",
      "6: Encoding Loss 22.81586456298828, Transition Loss 4.7410478591918945, Classifier Loss 0.46095365285873413, Total Loss 229.57049560546875\n",
      "6: Encoding Loss 23.11591911315918, Transition Loss 4.824288368225098, Classifier Loss 0.4538728594779968, Total Loss 231.2794952392578\n",
      "6: Encoding Loss 21.36802864074707, Transition Loss 4.953625679016113, Classifier Loss 0.45130956172943115, Total Loss 217.06590270996094\n",
      "6: Encoding Loss 21.961864471435547, Transition Loss 5.856302738189697, Classifier Loss 0.43322598934173584, Total Loss 220.1887969970703\n",
      "6: Encoding Loss 22.581388473510742, Transition Loss 5.603353500366211, Classifier Loss 0.4396944046020508, Total Loss 225.7412109375\n",
      "6: Encoding Loss 22.006832122802734, Transition Loss 7.434204578399658, Classifier Loss 0.4995797872543335, Total Loss 227.49949645996094\n",
      "6: Encoding Loss 23.602123260498047, Transition Loss 4.9175124168396, Classifier Loss 0.46265357732772827, Total Loss 236.0658721923828\n",
      "6: Encoding Loss 22.72471809387207, Transition Loss 6.365386486053467, Classifier Loss 0.4693050980567932, Total Loss 230.00132751464844\n",
      "6: Encoding Loss 23.153079986572266, Transition Loss 5.530116081237793, Classifier Loss 0.46181201934814453, Total Loss 232.51185607910156\n",
      "6: Encoding Loss 21.872236251831055, Transition Loss 5.800060272216797, Classifier Loss 0.4295842945575714, Total Loss 219.09634399414062\n",
      "6: Encoding Loss 23.25266456604004, Transition Loss 3.892500162124634, Classifier Loss 0.4470166563987732, Total Loss 231.50148010253906\n",
      "6: Encoding Loss 22.13823699951172, Transition Loss 5.189718246459961, Classifier Loss 0.4460659623146057, Total Loss 222.7504425048828\n",
      "6: Encoding Loss 22.41737937927246, Transition Loss 3.921013355255127, Classifier Loss 0.4632827341556549, Total Loss 226.45152282714844\n",
      "6: Encoding Loss 22.895959854125977, Transition Loss 5.935596942901611, Classifier Loss 0.46536973118782043, Total Loss 230.8917694091797\n",
      "6: Encoding Loss 23.289974212646484, Transition Loss 4.707552909851074, Classifier Loss 0.43671882152557373, Total Loss 230.93321228027344\n",
      "6: Encoding Loss 23.18209457397461, Transition Loss 4.227303504943848, Classifier Loss 0.4582686722278595, Total Loss 232.12908935546875\n",
      "6: Encoding Loss 21.30166244506836, Transition Loss 4.292758464813232, Classifier Loss 0.46715477108955383, Total Loss 217.98733520507812\n",
      "6: Encoding Loss 22.85836410522461, Transition Loss 4.0453200340271, Classifier Loss 0.4761168658733368, Total Loss 231.2876739501953\n",
      "6: Encoding Loss 22.69771957397461, Transition Loss 5.744752407073975, Classifier Loss 0.5055721402168274, Total Loss 233.28793334960938\n",
      "6: Encoding Loss 22.60852813720703, Transition Loss 4.641176700592041, Classifier Loss 0.45307064056396484, Total Loss 227.10353088378906\n",
      "6: Encoding Loss 23.18414878845215, Transition Loss 3.508225202560425, Classifier Loss 0.43313685059547424, Total Loss 229.488525390625\n",
      "6: Encoding Loss 22.24700164794922, Transition Loss 4.204068183898926, Classifier Loss 0.41624611616134644, Total Loss 220.44143676757812\n",
      "6: Encoding Loss 22.47845458984375, Transition Loss 5.551577091217041, Classifier Loss 0.489319771528244, Total Loss 229.86993408203125\n",
      "6: Encoding Loss 22.160234451293945, Transition Loss 5.363144874572754, Classifier Loss 0.4537937641143799, Total Loss 223.73388671875\n",
      "6: Encoding Loss 22.868059158325195, Transition Loss 4.210764408111572, Classifier Loss 0.4306408762931824, Total Loss 226.8507080078125\n",
      "6: Encoding Loss 23.135469436645508, Transition Loss 4.4887237548828125, Classifier Loss 0.4729747474193573, Total Loss 233.2789764404297\n",
      "6: Encoding Loss 21.39580535888672, Transition Loss 3.0526998043060303, Classifier Loss 0.4684198498725891, Total Loss 218.61895751953125\n",
      "6: Encoding Loss 22.1099853515625, Transition Loss 5.3450164794921875, Classifier Loss 0.45788732171058655, Total Loss 223.73760986328125\n",
      "6: Encoding Loss 22.1884822845459, Transition Loss 3.836872100830078, Classifier Loss 0.4780882000923157, Total Loss 226.0840606689453\n",
      "6: Encoding Loss 21.339746475219727, Transition Loss 5.960101127624512, Classifier Loss 0.47845083475112915, Total Loss 219.75506591796875\n",
      "6: Encoding Loss 23.36591148376465, Transition Loss 3.3927180767059326, Classifier Loss 0.4642573893070221, Total Loss 234.0315704345703\n",
      "6: Encoding Loss 22.383325576782227, Transition Loss 4.016866207122803, Classifier Loss 0.4711461663246155, Total Loss 226.98458862304688\n",
      "6: Encoding Loss 22.79073143005371, Transition Loss 4.12962007522583, Classifier Loss 0.45220041275024414, Total Loss 228.371826171875\n",
      "6: Encoding Loss 21.91386604309082, Transition Loss 5.679652690887451, Classifier Loss 0.45074185729026794, Total Loss 221.5210418701172\n",
      "6: Encoding Loss 22.863521575927734, Transition Loss 5.599408149719238, Classifier Loss 0.47333860397338867, Total Loss 231.3619384765625\n",
      "6: Encoding Loss 22.115766525268555, Transition Loss 4.814777374267578, Classifier Loss 0.4524233937263489, Total Loss 223.1314239501953\n",
      "6: Encoding Loss 21.045673370361328, Transition Loss 6.484384536743164, Classifier Loss 0.46830734610557556, Total Loss 216.4929962158203\n",
      "6: Encoding Loss 22.735445022583008, Transition Loss 4.732253074645996, Classifier Loss 0.48122701048851013, Total Loss 230.9527130126953\n",
      "6: Encoding Loss 22.42860984802246, Transition Loss 5.03517484664917, Classifier Loss 0.4723968505859375, Total Loss 227.67559814453125\n",
      "6: Encoding Loss 20.801488876342773, Transition Loss 4.220243453979492, Classifier Loss 0.4573283791542053, Total Loss 212.98880004882812\n",
      "6: Encoding Loss 22.500173568725586, Transition Loss 5.727370262145996, Classifier Loss 0.45561832189559937, Total Loss 226.7086944580078\n",
      "6: Encoding Loss 22.41457176208496, Transition Loss 4.825745582580566, Classifier Loss 0.4571349322795868, Total Loss 225.99520874023438\n",
      "6: Encoding Loss 22.85141372680664, Transition Loss 5.0413289070129395, Classifier Loss 0.4801465570926666, Total Loss 231.834228515625\n",
      "6: Encoding Loss 22.240175247192383, Transition Loss 4.622872352600098, Classifier Loss 0.4846944808959961, Total Loss 227.3154296875\n",
      "6: Encoding Loss 22.68919563293457, Transition Loss 3.8937439918518066, Classifier Loss 0.4366467595100403, Total Loss 225.9569854736328\n",
      "6: Encoding Loss 22.276729583740234, Transition Loss 3.4557976722717285, Classifier Loss 0.48897141218185425, Total Loss 227.80215454101562\n",
      "6: Encoding Loss 23.694063186645508, Transition Loss 5.793349742889404, Classifier Loss 0.44106122851371765, Total Loss 234.8173065185547\n",
      "6: Encoding Loss 22.400745391845703, Transition Loss 4.011265277862549, Classifier Loss 0.4603923559188843, Total Loss 226.0474395751953\n",
      "6: Encoding Loss 21.796613693237305, Transition Loss 4.446495532989502, Classifier Loss 0.4650622606277466, Total Loss 221.7684326171875\n",
      "6: Encoding Loss 21.885215759277344, Transition Loss 4.7949419021606445, Classifier Loss 0.4193711280822754, Total Loss 217.97781372070312\n",
      "6: Encoding Loss 22.029218673706055, Transition Loss 4.287437915802002, Classifier Loss 0.47421106696128845, Total Loss 224.51234436035156\n",
      "6: Encoding Loss 21.727628707885742, Transition Loss 2.908147096633911, Classifier Loss 0.4462062120437622, Total Loss 219.02328491210938\n",
      "6: Encoding Loss 22.254865646362305, Transition Loss 3.4237935543060303, Classifier Loss 0.4430820643901825, Total Loss 223.03189086914062\n",
      "6: Encoding Loss 22.643115997314453, Transition Loss 5.634139537811279, Classifier Loss 0.4811280071735382, Total Loss 230.38455200195312\n",
      "6: Encoding Loss 22.00116539001465, Transition Loss 2.883207321166992, Classifier Loss 0.4444741904735565, Total Loss 221.03338623046875\n",
      "6: Encoding Loss 22.918996810913086, Transition Loss 3.347862958908081, Classifier Loss 0.4573773443698883, Total Loss 229.75927734375\n",
      "6: Encoding Loss 21.086620330810547, Transition Loss 4.954227924346924, Classifier Loss 0.4672709107398987, Total Loss 216.41091918945312\n",
      "6: Encoding Loss 21.87771987915039, Transition Loss 5.34473991394043, Classifier Loss 0.45378029346466064, Total Loss 221.46873474121094\n",
      "6: Encoding Loss 21.74831771850586, Transition Loss 4.72360897064209, Classifier Loss 0.47076886892318726, Total Loss 222.00816345214844\n",
      "6: Encoding Loss 22.32335662841797, Transition Loss 5.292698860168457, Classifier Loss 0.45445793867111206, Total Loss 225.0911865234375\n",
      "6: Encoding Loss 20.988115310668945, Transition Loss 7.090104579925537, Classifier Loss 0.5358816981315613, Total Loss 222.91110229492188\n",
      "6: Encoding Loss 22.60519027709961, Transition Loss 4.464658737182617, Classifier Loss 0.44294366240501404, Total Loss 226.02882385253906\n",
      "6: Encoding Loss 22.25498390197754, Transition Loss 4.348215103149414, Classifier Loss 0.46216878294944763, Total Loss 225.1263885498047\n",
      "6: Encoding Loss 21.50579261779785, Transition Loss 4.057116985321045, Classifier Loss 0.45668065547943115, Total Loss 218.52581787109375\n",
      "6: Encoding Loss 21.905895233154297, Transition Loss 3.8352952003479004, Classifier Loss 0.47024327516555786, Total Loss 223.03855895996094\n",
      "6: Encoding Loss 21.593114852905273, Transition Loss 4.6442790031433105, Classifier Loss 0.4399426281452179, Total Loss 217.66802978515625\n",
      "6: Encoding Loss 22.132169723510742, Transition Loss 5.118442535400391, Classifier Loss 0.44857025146484375, Total Loss 222.9380645751953\n",
      "6: Encoding Loss 22.103057861328125, Transition Loss 3.2072839736938477, Classifier Loss 0.45164385437965393, Total Loss 222.63031005859375\n",
      "6: Encoding Loss 22.14759063720703, Transition Loss 3.959592819213867, Classifier Loss 0.45930784940719604, Total Loss 223.90342712402344\n",
      "6: Encoding Loss 20.89204216003418, Transition Loss 3.569899082183838, Classifier Loss 0.46426182985305786, Total Loss 214.2764892578125\n",
      "6: Encoding Loss 20.38551139831543, Transition Loss 5.111788749694824, Classifier Loss 0.456747829914093, Total Loss 209.78123474121094\n",
      "6: Encoding Loss 23.66895294189453, Transition Loss 3.5742931365966797, Classifier Loss 0.4972658157348633, Total Loss 239.79306030273438\n",
      "6: Encoding Loss 22.303314208984375, Transition Loss 6.146008491516113, Classifier Loss 0.5324747562408447, Total Loss 232.90318298339844\n",
      "6: Encoding Loss 22.55270004272461, Transition Loss 4.330809116363525, Classifier Loss 0.4561440944671631, Total Loss 226.90219116210938\n",
      "6: Encoding Loss 22.97751235961914, Transition Loss 6.034548282623291, Classifier Loss 0.453802227973938, Total Loss 230.4072265625\n",
      "6: Encoding Loss 22.759790420532227, Transition Loss 4.274982452392578, Classifier Loss 0.44998401403427124, Total Loss 227.9317169189453\n",
      "6: Encoding Loss 21.3164119720459, Transition Loss 5.097958564758301, Classifier Loss 0.4427650570869446, Total Loss 215.827392578125\n",
      "6: Encoding Loss 22.96999740600586, Transition Loss 3.104597806930542, Classifier Loss 0.45907843112945557, Total Loss 230.2887725830078\n",
      "6: Encoding Loss 21.50408172607422, Transition Loss 2.594064474105835, Classifier Loss 0.4550974667072296, Total Loss 218.06121826171875\n",
      "6: Encoding Loss 22.092687606811523, Transition Loss 3.9233458042144775, Classifier Loss 0.4316343069076538, Total Loss 220.68960571289062\n",
      "6: Encoding Loss 20.871990203857422, Transition Loss 4.562573432922363, Classifier Loss 0.489341676235199, Total Loss 216.82261657714844\n",
      "6: Encoding Loss 21.33718490600586, Transition Loss 2.7995829582214355, Classifier Loss 0.4761113226413727, Total Loss 218.86854553222656\n",
      "6: Encoding Loss 22.541950225830078, Transition Loss 6.696798324584961, Classifier Loss 0.4484916627407074, Total Loss 226.5241241455078\n",
      "6: Encoding Loss 21.287784576416016, Transition Loss 4.183871269226074, Classifier Loss 0.4581420421600342, Total Loss 216.95326232910156\n",
      "6: Encoding Loss 23.599573135375977, Transition Loss 5.419617176055908, Classifier Loss 0.4423117935657501, Total Loss 234.1116943359375\n",
      "6: Encoding Loss 23.125886917114258, Transition Loss 5.413487911224365, Classifier Loss 0.470226526260376, Total Loss 233.11245727539062\n",
      "6: Encoding Loss 22.395536422729492, Transition Loss 7.29340124130249, Classifier Loss 0.4557180106639862, Total Loss 226.19476318359375\n",
      "6: Encoding Loss 21.094058990478516, Transition Loss 7.888171195983887, Classifier Loss 0.42011237144470215, Total Loss 212.34133911132812\n",
      "7: Encoding Loss 22.09567642211914, Transition Loss 4.406357288360596, Classifier Loss 0.45130428671836853, Total Loss 222.77711486816406\n",
      "7: Encoding Loss 22.864564895629883, Transition Loss 4.146373271942139, Classifier Loss 0.4439098834991455, Total Loss 228.13677978515625\n",
      "7: Encoding Loss 23.111234664916992, Transition Loss 4.344900131225586, Classifier Loss 0.4490484297275543, Total Loss 230.6636962890625\n",
      "7: Encoding Loss 22.311323165893555, Transition Loss 4.017185688018799, Classifier Loss 0.4283711314201355, Total Loss 222.13113403320312\n",
      "7: Encoding Loss 23.17862319946289, Transition Loss 3.9760148525238037, Classifier Loss 0.44320133328437805, Total Loss 230.5443115234375\n",
      "7: Encoding Loss 21.81316566467285, Transition Loss 5.058045864105225, Classifier Loss 0.4546136260032654, Total Loss 220.97830200195312\n",
      "7: Encoding Loss 21.748708724975586, Transition Loss 3.194793701171875, Classifier Loss 0.4744146764278412, Total Loss 222.07009887695312\n",
      "7: Encoding Loss 23.601470947265625, Transition Loss 4.148612976074219, Classifier Loss 0.4516587257385254, Total Loss 234.80735778808594\n",
      "7: Encoding Loss 22.43036651611328, Transition Loss 3.9638726711273193, Classifier Loss 0.49557945132255554, Total Loss 229.7936553955078\n",
      "7: Encoding Loss 24.273807525634766, Transition Loss 4.778030872344971, Classifier Loss 0.45749038457870483, Total Loss 240.89511108398438\n",
      "7: Encoding Loss 22.768686294555664, Transition Loss 3.749569892883301, Classifier Loss 0.47575509548187256, Total Loss 230.47491455078125\n",
      "7: Encoding Loss 22.243995666503906, Transition Loss 4.287481784820557, Classifier Loss 0.4805002510547638, Total Loss 226.85948181152344\n",
      "7: Encoding Loss 21.51311683654785, Transition Loss 4.083290100097656, Classifier Loss 0.4600391387939453, Total Loss 218.92550659179688\n",
      "7: Encoding Loss 22.23467254638672, Transition Loss 4.240954399108887, Classifier Loss 0.46649983525276184, Total Loss 225.37554931640625\n",
      "7: Encoding Loss 21.31290054321289, Transition Loss 4.483208179473877, Classifier Loss 0.4477495551109314, Total Loss 216.1748046875\n",
      "7: Encoding Loss 22.361042022705078, Transition Loss 5.288423538208008, Classifier Loss 0.4456636607646942, Total Loss 224.5123748779297\n",
      "7: Encoding Loss 21.547733306884766, Transition Loss 4.56503438949585, Classifier Loss 0.4442358911037445, Total Loss 217.7184600830078\n",
      "7: Encoding Loss 21.86292266845703, Transition Loss 4.7146711349487305, Classifier Loss 0.4438724219799042, Total Loss 220.23355102539062\n",
      "7: Encoding Loss 22.154132843017578, Transition Loss 4.882266521453857, Classifier Loss 0.4234037697315216, Total Loss 220.54989624023438\n",
      "7: Encoding Loss 23.255510330200195, Transition Loss 4.7742390632629395, Classifier Loss 0.4392545223236084, Total Loss 230.9243927001953\n",
      "7: Encoding Loss 21.741313934326172, Transition Loss 5.890721321105957, Classifier Loss 0.4521012604236603, Total Loss 220.3188018798828\n",
      "7: Encoding Loss 21.2684268951416, Transition Loss 3.6252260208129883, Classifier Loss 0.4654279947280884, Total Loss 217.41526794433594\n",
      "7: Encoding Loss 21.016345977783203, Transition Loss 7.466914176940918, Classifier Loss 0.46908196806907654, Total Loss 216.5323486328125\n",
      "7: Encoding Loss 21.624998092651367, Transition Loss 3.702577829360962, Classifier Loss 0.4458938241004944, Total Loss 218.3298797607422\n",
      "7: Encoding Loss 22.387258529663086, Transition Loss 3.1009440422058105, Classifier Loss 0.4423729181289673, Total Loss 223.95555114746094\n",
      "7: Encoding Loss 23.354694366455078, Transition Loss 4.579765319824219, Classifier Loss 0.4550536870956421, Total Loss 233.25888061523438\n",
      "7: Encoding Loss 22.827423095703125, Transition Loss 5.190914154052734, Classifier Loss 0.4590241014957428, Total Loss 229.55996704101562\n",
      "7: Encoding Loss 21.721113204956055, Transition Loss 5.265359401702881, Classifier Loss 0.4541575312614441, Total Loss 220.23773193359375\n",
      "7: Encoding Loss 21.58081817626953, Transition Loss 4.888933181762695, Classifier Loss 0.4413052201271057, Total Loss 217.75485229492188\n",
      "7: Encoding Loss 21.375530242919922, Transition Loss 5.770449161529541, Classifier Loss 0.4744163751602173, Total Loss 219.5999755859375\n",
      "7: Encoding Loss 21.78529930114746, Transition Loss 4.879366874694824, Classifier Loss 0.4440746307373047, Total Loss 219.6657257080078\n",
      "7: Encoding Loss 20.684946060180664, Transition Loss 5.756350040435791, Classifier Loss 0.46008220314979553, Total Loss 212.63906860351562\n",
      "7: Encoding Loss 22.24156951904297, Transition Loss 5.1801252365112305, Classifier Loss 0.45917707681655884, Total Loss 224.88629150390625\n",
      "7: Encoding Loss 21.382999420166016, Transition Loss 2.507805585861206, Classifier Loss 0.4644502103328705, Total Loss 218.0105743408203\n",
      "7: Encoding Loss 21.556852340698242, Transition Loss 5.120275497436523, Classifier Loss 0.4388541281223297, Total Loss 217.36428833007812\n",
      "7: Encoding Loss 21.03866195678711, Transition Loss 2.9929895401000977, Classifier Loss 0.4315415918827057, Total Loss 212.06207275390625\n",
      "7: Encoding Loss 21.989545822143555, Transition Loss 5.087301254272461, Classifier Loss 0.45279818773269653, Total Loss 222.21363830566406\n",
      "7: Encoding Loss 22.321245193481445, Transition Loss 6.4387922286987305, Classifier Loss 0.4494224786758423, Total Loss 224.7999725341797\n",
      "7: Encoding Loss 21.9337158203125, Transition Loss 4.184419631958008, Classifier Loss 0.4377813935279846, Total Loss 220.08474731445312\n",
      "7: Encoding Loss 21.440555572509766, Transition Loss 5.234812259674072, Classifier Loss 0.466464638710022, Total Loss 219.21788024902344\n",
      "7: Encoding Loss 21.34040641784668, Transition Loss 5.7133097648620605, Classifier Loss 0.45414450764656067, Total Loss 217.2803497314453\n",
      "7: Encoding Loss 22.278650283813477, Transition Loss 3.4403369426727295, Classifier Loss 0.47487661242485046, Total Loss 226.40492248535156\n",
      "7: Encoding Loss 21.20191192626953, Transition Loss 3.2808332443237305, Classifier Loss 0.4499383568763733, Total Loss 215.2653045654297\n",
      "7: Encoding Loss 21.28802490234375, Transition Loss 2.4920082092285156, Classifier Loss 0.44872167706489563, Total Loss 215.6747589111328\n",
      "7: Encoding Loss 22.326448440551758, Transition Loss 3.503997564315796, Classifier Loss 0.4725777506828308, Total Loss 226.57015991210938\n",
      "7: Encoding Loss 22.49008560180664, Transition Loss 5.032883644104004, Classifier Loss 0.4549317955970764, Total Loss 226.42044067382812\n",
      "7: Encoding Loss 22.144174575805664, Transition Loss 5.671887397766113, Classifier Loss 0.4752894639968872, Total Loss 225.8167266845703\n",
      "7: Encoding Loss 22.720552444458008, Transition Loss 4.668489456176758, Classifier Loss 0.4567764103412628, Total Loss 228.37576293945312\n",
      "7: Encoding Loss 22.201702117919922, Transition Loss 4.3968658447265625, Classifier Loss 0.4504551589488983, Total Loss 223.5385284423828\n",
      "7: Encoding Loss 21.92426109313965, Transition Loss 4.850234031677246, Classifier Loss 0.43429645895957947, Total Loss 219.7937774658203\n",
      "7: Encoding Loss 21.285526275634766, Transition Loss 3.0853490829467773, Classifier Loss 0.46706029772758484, Total Loss 217.6072998046875\n",
      "7: Encoding Loss 21.255725860595703, Transition Loss 3.511016607284546, Classifier Loss 0.47067660093307495, Total Loss 217.815673828125\n",
      "7: Encoding Loss 20.767841339111328, Transition Loss 2.9856820106506348, Classifier Loss 0.4431338608264923, Total Loss 211.05325317382812\n",
      "7: Encoding Loss 21.145334243774414, Transition Loss 5.638304710388184, Classifier Loss 0.45464643836021423, Total Loss 215.75497436523438\n",
      "7: Encoding Loss 21.71273422241211, Transition Loss 5.202117919921875, Classifier Loss 0.46255752444267273, Total Loss 220.99806213378906\n",
      "7: Encoding Loss 20.490631103515625, Transition Loss 4.690072059631348, Classifier Loss 0.4689878821372986, Total Loss 211.76185607910156\n",
      "7: Encoding Loss 21.773996353149414, Transition Loss 4.520689964294434, Classifier Loss 0.4411242604255676, Total Loss 219.2085418701172\n",
      "7: Encoding Loss 21.917924880981445, Transition Loss 4.479469299316406, Classifier Loss 0.45348048210144043, Total Loss 221.58734130859375\n",
      "7: Encoding Loss 21.543432235717773, Transition Loss 4.373266696929932, Classifier Loss 0.44372957944869995, Total Loss 217.59506225585938\n",
      "7: Encoding Loss 21.071996688842773, Transition Loss 4.041455268859863, Classifier Loss 0.4846092462539673, Total Loss 217.84518432617188\n",
      "7: Encoding Loss 22.66461181640625, Transition Loss 5.787221908569336, Classifier Loss 0.4412081241607666, Total Loss 226.59515380859375\n",
      "7: Encoding Loss 22.921051025390625, Transition Loss 4.612385272979736, Classifier Loss 0.44871577620506287, Total Loss 229.16246032714844\n",
      "7: Encoding Loss 22.25811767578125, Transition Loss 3.6639416217803955, Classifier Loss 0.4472277760505676, Total Loss 223.5205078125\n",
      "7: Encoding Loss 22.088075637817383, Transition Loss 3.987342596054077, Classifier Loss 0.44676879048347473, Total Loss 222.178955078125\n",
      "7: Encoding Loss 20.840147018432617, Transition Loss 4.832284450531006, Classifier Loss 0.4278903901576996, Total Loss 210.4766845703125\n",
      "7: Encoding Loss 20.78688621520996, Transition Loss 4.528130054473877, Classifier Loss 0.4610479772090912, Total Loss 213.30551147460938\n",
      "7: Encoding Loss 21.67607879638672, Transition Loss 4.261076927185059, Classifier Loss 0.4385761022567749, Total Loss 218.1184539794922\n",
      "7: Encoding Loss 22.804121017456055, Transition Loss 4.471465587615967, Classifier Loss 0.43121007084846497, Total Loss 226.44825744628906\n",
      "7: Encoding Loss 22.68915367126465, Transition Loss 5.651370048522949, Classifier Loss 0.4180315434932709, Total Loss 224.4466552734375\n",
      "7: Encoding Loss 21.679195404052734, Transition Loss 4.497509002685547, Classifier Loss 0.46537312865257263, Total Loss 220.87039184570312\n",
      "7: Encoding Loss 21.945011138916016, Transition Loss 4.08170747756958, Classifier Loss 0.45089781284332275, Total Loss 221.46621704101562\n",
      "7: Encoding Loss 20.687604904174805, Transition Loss 4.8341383934021, Classifier Loss 0.4438535273075104, Total Loss 210.85302734375\n",
      "7: Encoding Loss 20.972515106201172, Transition Loss 5.164775371551514, Classifier Loss 0.4314011335372925, Total Loss 211.95321655273438\n",
      "7: Encoding Loss 22.115568161010742, Transition Loss 5.270016193389893, Classifier Loss 0.4286957085132599, Total Loss 220.84811401367188\n",
      "7: Encoding Loss 21.804956436157227, Transition Loss 6.652978420257568, Classifier Loss 0.47043782472610474, Total Loss 222.81402587890625\n",
      "7: Encoding Loss 21.988142013549805, Transition Loss 4.559530258178711, Classifier Loss 0.47598427534103394, Total Loss 224.4154815673828\n",
      "7: Encoding Loss 22.172386169433594, Transition Loss 5.442050457000732, Classifier Loss 0.43046262860298157, Total Loss 221.51376342773438\n",
      "7: Encoding Loss 21.291410446166992, Transition Loss 5.325363636016846, Classifier Loss 0.4458937346935272, Total Loss 215.98573303222656\n",
      "7: Encoding Loss 20.689029693603516, Transition Loss 4.850572109222412, Classifier Loss 0.42301851511001587, Total Loss 208.78419494628906\n",
      "7: Encoding Loss 22.31310272216797, Transition Loss 3.9373831748962402, Classifier Loss 0.44038447737693787, Total Loss 223.33074951171875\n",
      "7: Encoding Loss 21.724834442138672, Transition Loss 4.029216766357422, Classifier Loss 0.4692458510398865, Total Loss 221.52911376953125\n",
      "7: Encoding Loss 21.66337776184082, Transition Loss 4.283304214477539, Classifier Loss 0.47675812244415283, Total Loss 221.83949279785156\n",
      "7: Encoding Loss 21.605907440185547, Transition Loss 4.517002105712891, Classifier Loss 0.4390362501144409, Total Loss 217.654296875\n",
      "7: Encoding Loss 21.94933319091797, Transition Loss 4.924210071563721, Classifier Loss 0.44875311851501465, Total Loss 221.45481872558594\n",
      "7: Encoding Loss 22.86443519592285, Transition Loss 3.2529191970825195, Classifier Loss 0.46106135845184326, Total Loss 229.67221069335938\n",
      "7: Encoding Loss 21.06964874267578, Transition Loss 4.256736755371094, Classifier Loss 0.4459420442581177, Total Loss 214.00274658203125\n",
      "7: Encoding Loss 22.13120460510254, Transition Loss 3.449152946472168, Classifier Loss 0.4556257426738739, Total Loss 223.3020477294922\n",
      "7: Encoding Loss 21.390777587890625, Transition Loss 5.501450538635254, Classifier Loss 0.449768990278244, Total Loss 217.2034149169922\n",
      "7: Encoding Loss 21.67668914794922, Transition Loss 4.043557643890381, Classifier Loss 0.46746310591697693, Total Loss 220.96853637695312\n",
      "7: Encoding Loss 22.70702362060547, Transition Loss 3.305546283721924, Classifier Loss 0.45495477318763733, Total Loss 227.81277465820312\n",
      "7: Encoding Loss 21.723493576049805, Transition Loss 3.51448392868042, Classifier Loss 0.43857136368751526, Total Loss 218.3479766845703\n",
      "7: Encoding Loss 22.20199203491211, Transition Loss 5.128360271453857, Classifier Loss 0.44673728942871094, Total Loss 223.31533813476562\n",
      "7: Encoding Loss 21.837839126586914, Transition Loss 4.565254211425781, Classifier Loss 0.45664262771606445, Total Loss 221.280029296875\n",
      "7: Encoding Loss 22.16196632385254, Transition Loss 3.6440446376800537, Classifier Loss 0.4409438967704773, Total Loss 222.11892700195312\n",
      "7: Encoding Loss 22.374189376831055, Transition Loss 3.8332245349884033, Classifier Loss 0.435576468706131, Total Loss 223.31781005859375\n",
      "7: Encoding Loss 21.4549503326416, Transition Loss 2.948566198348999, Classifier Loss 0.45571622252464294, Total Loss 217.80093383789062\n",
      "7: Encoding Loss 21.775732040405273, Transition Loss 4.622090816497803, Classifier Loss 0.4746212065219879, Total Loss 222.59239196777344\n",
      "7: Encoding Loss 21.080631256103516, Transition Loss 3.5006134510040283, Classifier Loss 0.4816259741783142, Total Loss 217.5077667236328\n",
      "7: Encoding Loss 20.83836555480957, Transition Loss 5.394300937652588, Classifier Loss 0.4686828851699829, Total Loss 214.65406799316406\n",
      "7: Encoding Loss 22.715660095214844, Transition Loss 2.950958490371704, Classifier Loss 0.4735379219055176, Total Loss 229.6692657470703\n",
      "7: Encoding Loss 21.91360092163086, Transition Loss 3.5332865715026855, Classifier Loss 0.45067620277404785, Total Loss 221.0830841064453\n",
      "7: Encoding Loss 22.029754638671875, Transition Loss 3.6615452766418457, Classifier Loss 0.44037625193595886, Total Loss 221.0079803466797\n",
      "7: Encoding Loss 20.76310157775879, Transition Loss 5.119422435760498, Classifier Loss 0.4588185250759125, Total Loss 213.0105438232422\n",
      "7: Encoding Loss 21.958057403564453, Transition Loss 4.995403289794922, Classifier Loss 0.45007044076919556, Total Loss 221.67059326171875\n",
      "7: Encoding Loss 21.026601791381836, Transition Loss 4.375762462615967, Classifier Loss 0.4318773150444031, Total Loss 212.27569580078125\n",
      "7: Encoding Loss 20.091646194458008, Transition Loss 5.93764591217041, Classifier Loss 0.4360068440437317, Total Loss 205.52139282226562\n",
      "7: Encoding Loss 21.45425033569336, Transition Loss 4.237492084503174, Classifier Loss 0.4716039001941681, Total Loss 219.64190673828125\n",
      "7: Encoding Loss 22.393781661987305, Transition Loss 4.333063125610352, Classifier Loss 0.45206698775291443, Total Loss 225.2235565185547\n",
      "7: Encoding Loss 20.06102752685547, Transition Loss 3.986753463745117, Classifier Loss 0.459497332572937, Total Loss 207.2353057861328\n",
      "7: Encoding Loss 21.264822006225586, Transition Loss 4.9343342781066895, Classifier Loss 0.4418566823005676, Total Loss 215.29110717773438\n",
      "7: Encoding Loss 21.548551559448242, Transition Loss 4.260894298553467, Classifier Loss 0.45509734749794006, Total Loss 218.7503204345703\n",
      "7: Encoding Loss 22.550106048583984, Transition Loss 4.156872272491455, Classifier Loss 0.44313639402389526, Total Loss 225.54588317871094\n",
      "7: Encoding Loss 21.5750732421875, Transition Loss 4.065029144287109, Classifier Loss 0.44465914368629456, Total Loss 217.87950134277344\n",
      "7: Encoding Loss 22.411054611206055, Transition Loss 3.305396556854248, Classifier Loss 0.42028045654296875, Total Loss 221.97755432128906\n",
      "7: Encoding Loss 21.5, Transition Loss 3.081916093826294, Classifier Loss 0.46029412746429443, Total Loss 218.6457977294922\n",
      "7: Encoding Loss 23.310546875, Transition Loss 4.833940505981445, Classifier Loss 0.4269981384277344, Total Loss 230.15097045898438\n",
      "7: Encoding Loss 21.546783447265625, Transition Loss 3.342177629470825, Classifier Loss 0.4598565101623535, Total Loss 219.0283660888672\n",
      "7: Encoding Loss 20.785951614379883, Transition Loss 3.947765827178955, Classifier Loss 0.4619753956794739, Total Loss 213.2747039794922\n",
      "7: Encoding Loss 21.40657615661621, Transition Loss 4.233748912811279, Classifier Loss 0.4475421607494354, Total Loss 216.85357666015625\n",
      "7: Encoding Loss 21.53806495666504, Transition Loss 3.683898448944092, Classifier Loss 0.45794785022735596, Total Loss 218.83609008789062\n",
      "7: Encoding Loss 21.04291343688965, Transition Loss 2.597764253616333, Classifier Loss 0.4814022481441498, Total Loss 217.00308227539062\n",
      "7: Encoding Loss 20.91213035583496, Transition Loss 3.104367971420288, Classifier Loss 0.4514428377151489, Total Loss 213.0622100830078\n",
      "7: Encoding Loss 21.42382049560547, Transition Loss 4.858572006225586, Classifier Loss 0.49085843563079834, Total Loss 221.4481201171875\n",
      "7: Encoding Loss 21.58396339416504, Transition Loss 2.559460163116455, Classifier Loss 0.4591110050678253, Total Loss 219.09469604492188\n",
      "7: Encoding Loss 21.95266342163086, Transition Loss 2.8173022270202637, Classifier Loss 0.44023603200912476, Total Loss 220.20838928222656\n",
      "7: Encoding Loss 20.50697135925293, Transition Loss 4.544977188110352, Classifier Loss 0.4476032853126526, Total Loss 209.72509765625\n",
      "7: Encoding Loss 21.475341796875, Transition Loss 4.886876583099365, Classifier Loss 0.49215829372406006, Total Loss 221.9959259033203\n",
      "7: Encoding Loss 21.08648109436035, Transition Loss 4.310965538024902, Classifier Loss 0.4681842029094696, Total Loss 216.37246704101562\n",
      "7: Encoding Loss 21.2746524810791, Transition Loss 4.736057281494141, Classifier Loss 0.4933135211467743, Total Loss 220.47576904296875\n",
      "7: Encoding Loss 20.931255340576172, Transition Loss 6.338135719299316, Classifier Loss 0.46602025628089905, Total Loss 215.3197021484375\n",
      "7: Encoding Loss 22.266443252563477, Transition Loss 3.9348549842834473, Classifier Loss 0.48016849160194397, Total Loss 226.93536376953125\n",
      "7: Encoding Loss 21.65031623840332, Transition Loss 3.7752819061279297, Classifier Loss 0.45298925042152405, Total Loss 219.25650024414062\n",
      "7: Encoding Loss 20.926998138427734, Transition Loss 3.8476505279541016, Classifier Loss 0.449288547039032, Total Loss 213.1143798828125\n",
      "7: Encoding Loss 21.502811431884766, Transition Loss 3.544736623764038, Classifier Loss 0.4437457025051117, Total Loss 217.10601806640625\n",
      "7: Encoding Loss 21.15659523010254, Transition Loss 4.336021900177002, Classifier Loss 0.4548618793487549, Total Loss 215.6061553955078\n",
      "7: Encoding Loss 21.066980361938477, Transition Loss 4.522810935974121, Classifier Loss 0.4485328793525696, Total Loss 214.29368591308594\n",
      "7: Encoding Loss 21.617881774902344, Transition Loss 2.9664266109466553, Classifier Loss 0.4618774652481079, Total Loss 219.72409057617188\n",
      "7: Encoding Loss 21.02134132385254, Transition Loss 3.599766969680786, Classifier Loss 0.44584447145462036, Total Loss 213.47512817382812\n",
      "7: Encoding Loss 19.816993713378906, Transition Loss 3.4154398441314697, Classifier Loss 0.4357606768608093, Total Loss 202.79510498046875\n",
      "7: Encoding Loss 19.9425048828125, Transition Loss 4.723087310791016, Classifier Loss 0.4230060279369354, Total Loss 202.78524780273438\n",
      "7: Encoding Loss 23.503047943115234, Transition Loss 3.1553966999053955, Classifier Loss 0.45947569608688354, Total Loss 234.60304260253906\n",
      "7: Encoding Loss 20.98581314086914, Transition Loss 5.65867280960083, Classifier Loss 0.4777870774269104, Total Loss 216.7969512939453\n",
      "7: Encoding Loss 21.50735092163086, Transition Loss 3.80621600151062, Classifier Loss 0.45598894357681274, Total Loss 218.41896057128906\n",
      "7: Encoding Loss 21.73335075378418, Transition Loss 5.232718467712402, Classifier Loss 0.4545544981956482, Total Loss 220.36878967285156\n",
      "7: Encoding Loss 21.693504333496094, Transition Loss 3.764925479888916, Classifier Loss 0.45180878043174744, Total Loss 219.48190307617188\n",
      "7: Encoding Loss 20.812406539916992, Transition Loss 4.596164703369141, Classifier Loss 0.46938711404800415, Total Loss 214.35719299316406\n",
      "7: Encoding Loss 22.060405731201172, Transition Loss 2.6390819549560547, Classifier Loss 0.4570092558860779, Total Loss 222.71200561523438\n",
      "7: Encoding Loss 21.028514862060547, Transition Loss 2.394364595413208, Classifier Loss 0.43326619267463684, Total Loss 212.0336151123047\n",
      "7: Encoding Loss 21.847986221313477, Transition Loss 3.502182722091675, Classifier Loss 0.42951345443725586, Total Loss 218.4356689453125\n",
      "7: Encoding Loss 20.246562957763672, Transition Loss 4.108734607696533, Classifier Loss 0.42887941002845764, Total Loss 205.6822052001953\n",
      "7: Encoding Loss 20.83881950378418, Transition Loss 2.5916738510131836, Classifier Loss 0.43283748626708984, Total Loss 210.5126495361328\n",
      "7: Encoding Loss 21.93592643737793, Transition Loss 5.713796138763428, Classifier Loss 0.45917025208473206, Total Loss 222.5471954345703\n",
      "7: Encoding Loss 21.323131561279297, Transition Loss 3.6544106006622314, Classifier Loss 0.44120460748672485, Total Loss 215.43641662597656\n",
      "7: Encoding Loss 22.25921630859375, Transition Loss 4.401570796966553, Classifier Loss 0.4378663897514343, Total Loss 222.7406768798828\n",
      "7: Encoding Loss 22.681324005126953, Transition Loss 4.67241096496582, Classifier Loss 0.4315046966075897, Total Loss 225.53553771972656\n",
      "7: Encoding Loss 21.423831939697266, Transition Loss 6.242932319641113, Classifier Loss 0.44669508934020996, Total Loss 217.30874633789062\n",
      "7: Encoding Loss 19.97906494140625, Transition Loss 7.1051344871521, Classifier Loss 0.4467298686504364, Total Loss 205.92652893066406\n",
      "8: Encoding Loss 21.832487106323242, Transition Loss 3.697526216506958, Classifier Loss 0.4392518699169159, Total Loss 219.3245849609375\n",
      "8: Encoding Loss 21.28614044189453, Transition Loss 3.673691749572754, Classifier Loss 0.42791250348091125, Total Loss 213.81512451171875\n",
      "8: Encoding Loss 22.62541961669922, Transition Loss 3.665916919708252, Classifier Loss 0.4171788990497589, Total Loss 223.45443725585938\n",
      "8: Encoding Loss 21.5859375, Transition Loss 3.5512092113494873, Classifier Loss 0.45465195178985596, Total Loss 218.86293029785156\n",
      "8: Encoding Loss 22.70507049560547, Transition Loss 3.2470338344573975, Classifier Loss 0.4586900770664215, Total Loss 228.1589813232422\n",
      "8: Encoding Loss 20.847368240356445, Transition Loss 4.671262741088867, Classifier Loss 0.4788966178894043, Total Loss 215.6028594970703\n",
      "8: Encoding Loss 21.88433265686035, Transition Loss 2.769136905670166, Classifier Loss 0.4462326765060425, Total Loss 220.25177001953125\n",
      "8: Encoding Loss 23.302701950073242, Transition Loss 3.5156843662261963, Classifier Loss 0.45020103454589844, Total Loss 232.1448516845703\n",
      "8: Encoding Loss 21.58545684814453, Transition Loss 3.545867919921875, Classifier Loss 0.4695844352245331, Total Loss 220.35125732421875\n",
      "8: Encoding Loss 22.741670608520508, Transition Loss 4.052889823913574, Classifier Loss 0.45872682332992554, Total Loss 228.6166229248047\n",
      "8: Encoding Loss 22.12152671813965, Transition Loss 3.279928207397461, Classifier Loss 0.4663105010986328, Total Loss 224.25926208496094\n",
      "8: Encoding Loss 21.840526580810547, Transition Loss 3.762514591217041, Classifier Loss 0.43522077798843384, Total Loss 218.99880981445312\n",
      "8: Encoding Loss 20.977947235107422, Transition Loss 3.6543753147125244, Classifier Loss 0.4774976670742035, Total Loss 216.3042449951172\n",
      "8: Encoding Loss 21.105690002441406, Transition Loss 3.8341684341430664, Classifier Loss 0.43176254630088806, Total Loss 212.78860473632812\n",
      "8: Encoding Loss 21.41741180419922, Transition Loss 4.059200763702393, Classifier Loss 0.4636777937412262, Total Loss 218.5189208984375\n",
      "8: Encoding Loss 21.717557907104492, Transition Loss 4.674694061279297, Classifier Loss 0.4261116087436676, Total Loss 217.28656005859375\n",
      "8: Encoding Loss 21.07884979248047, Transition Loss 4.145923614501953, Classifier Loss 0.42697250843048096, Total Loss 212.1572265625\n",
      "8: Encoding Loss 20.929912567138672, Transition Loss 4.118130207061768, Classifier Loss 0.3952094316482544, Total Loss 207.7838897705078\n",
      "8: Encoding Loss 21.557466506958008, Transition Loss 4.441897392272949, Classifier Loss 0.4521434009075165, Total Loss 218.5624542236328\n",
      "8: Encoding Loss 22.662220001220703, Transition Loss 4.035964488983154, Classifier Loss 0.41199737787246704, Total Loss 223.3046875\n",
      "8: Encoding Loss 21.243009567260742, Transition Loss 5.3890533447265625, Classifier Loss 0.46270880103111267, Total Loss 217.2927703857422\n",
      "8: Encoding Loss 21.264616012573242, Transition Loss 3.342787504196167, Classifier Loss 0.46531474590301514, Total Loss 217.3169708251953\n",
      "8: Encoding Loss 20.44162368774414, Transition Loss 6.820873737335205, Classifier Loss 0.4447236657142639, Total Loss 209.36953735351562\n",
      "8: Encoding Loss 20.40814781188965, Transition Loss 3.560346841812134, Classifier Loss 0.4544755220413208, Total Loss 209.4248046875\n",
      "8: Encoding Loss 21.378501892089844, Transition Loss 2.998081684112549, Classifier Loss 0.41049835085868835, Total Loss 212.67745971679688\n",
      "8: Encoding Loss 22.230125427246094, Transition Loss 4.2424516677856445, Classifier Loss 0.5062340497970581, Total Loss 229.3129119873047\n",
      "8: Encoding Loss 22.123279571533203, Transition Loss 4.847790241241455, Classifier Loss 0.45514756441116333, Total Loss 223.47055053710938\n",
      "8: Encoding Loss 21.13841438293457, Transition Loss 5.02922248840332, Classifier Loss 0.45401325821876526, Total Loss 215.5144805908203\n",
      "8: Encoding Loss 20.99691390991211, Transition Loss 4.674654960632324, Classifier Loss 0.4117605686187744, Total Loss 210.08631896972656\n",
      "8: Encoding Loss 21.25934600830078, Transition Loss 5.718725681304932, Classifier Loss 0.4430488646030426, Total Loss 215.5233917236328\n",
      "8: Encoding Loss 21.19857406616211, Transition Loss 4.656869411468506, Classifier Loss 0.4436487555503845, Total Loss 214.88487243652344\n",
      "8: Encoding Loss 19.878173828125, Transition Loss 5.732284069061279, Classifier Loss 0.45989930629730225, Total Loss 206.16177368164062\n",
      "8: Encoding Loss 21.7294979095459, Transition Loss 4.93595552444458, Classifier Loss 0.47858738899230957, Total Loss 222.68191528320312\n",
      "8: Encoding Loss 20.801166534423828, Transition Loss 2.5729784965515137, Classifier Loss 0.44699838757514954, Total Loss 211.623779296875\n",
      "8: Encoding Loss 20.817167282104492, Transition Loss 4.933807849884033, Classifier Loss 0.4468091130256653, Total Loss 212.2050018310547\n",
      "8: Encoding Loss 20.622663497924805, Transition Loss 3.053454875946045, Classifier Loss 0.45022648572921753, Total Loss 210.61463928222656\n",
      "8: Encoding Loss 21.802265167236328, Transition Loss 4.796757221221924, Classifier Loss 0.4488205909729004, Total Loss 220.259521484375\n",
      "8: Encoding Loss 21.536197662353516, Transition Loss 6.3258538246154785, Classifier Loss 0.4747146666049957, Total Loss 221.02621459960938\n",
      "8: Encoding Loss 21.261411666870117, Transition Loss 3.958576202392578, Classifier Loss 0.4177926182746887, Total Loss 212.6622772216797\n",
      "8: Encoding Loss 20.938968658447266, Transition Loss 5.063083171844482, Classifier Loss 0.4443511366844177, Total Loss 212.95948791503906\n",
      "8: Encoding Loss 20.664392471313477, Transition Loss 5.537045478820801, Classifier Loss 0.46878039836883545, Total Loss 213.3005828857422\n",
      "8: Encoding Loss 21.45011329650879, Transition Loss 3.405151128768921, Classifier Loss 0.4473302960395813, Total Loss 217.0149688720703\n",
      "8: Encoding Loss 21.176597595214844, Transition Loss 3.1694672107696533, Classifier Loss 0.4535442590713501, Total Loss 215.40110778808594\n",
      "8: Encoding Loss 20.68878746032715, Transition Loss 2.4974899291992188, Classifier Loss 0.45675739645957947, Total Loss 211.68553161621094\n",
      "8: Encoding Loss 22.041532516479492, Transition Loss 3.2147598266601562, Classifier Loss 0.43220409750938416, Total Loss 220.19561767578125\n",
      "8: Encoding Loss 21.803104400634766, Transition Loss 4.647938251495361, Classifier Loss 0.4384981691837311, Total Loss 219.20423889160156\n",
      "8: Encoding Loss 21.204296112060547, Transition Loss 5.148382663726807, Classifier Loss 0.427238404750824, Total Loss 213.3878936767578\n",
      "8: Encoding Loss 21.582263946533203, Transition Loss 4.09554386138916, Classifier Loss 0.42833101749420166, Total Loss 216.31031799316406\n",
      "8: Encoding Loss 21.697336196899414, Transition Loss 4.04962682723999, Classifier Loss 0.41928717494010925, Total Loss 216.3173370361328\n",
      "8: Encoding Loss 21.273746490478516, Transition Loss 4.440871238708496, Classifier Loss 0.4238433837890625, Total Loss 213.4624786376953\n",
      "8: Encoding Loss 20.6448917388916, Transition Loss 2.9404449462890625, Classifier Loss 0.42475759983062744, Total Loss 208.2229766845703\n",
      "8: Encoding Loss 20.81962013244629, Transition Loss 3.238558292388916, Classifier Loss 0.4776279330253601, Total Loss 214.96746826171875\n",
      "8: Encoding Loss 20.29235076904297, Transition Loss 2.89180326461792, Classifier Loss 0.4025730490684509, Total Loss 203.17446899414062\n",
      "8: Encoding Loss 20.533456802368164, Transition Loss 4.993833541870117, Classifier Loss 0.42641687393188477, Total Loss 207.90809631347656\n",
      "8: Encoding Loss 20.793659210205078, Transition Loss 4.857372760772705, Classifier Loss 0.4562211036682129, Total Loss 212.94285583496094\n",
      "8: Encoding Loss 20.138690948486328, Transition Loss 4.355635166168213, Classifier Loss 0.42635416984558105, Total Loss 204.61607360839844\n",
      "8: Encoding Loss 21.248111724853516, Transition Loss 4.238729000091553, Classifier Loss 0.42682892084121704, Total Loss 213.51553344726562\n",
      "8: Encoding Loss 21.299894332885742, Transition Loss 4.069167613983154, Classifier Loss 0.45381712913513184, Total Loss 216.59469604492188\n",
      "8: Encoding Loss 20.99513816833496, Transition Loss 4.240988731384277, Classifier Loss 0.4612850844860077, Total Loss 214.93780517578125\n",
      "8: Encoding Loss 20.278865814208984, Transition Loss 3.699302911758423, Classifier Loss 0.44478291273117065, Total Loss 207.4490966796875\n",
      "8: Encoding Loss 21.980451583862305, Transition Loss 5.198513031005859, Classifier Loss 0.43565478920936584, Total Loss 220.44879150390625\n",
      "8: Encoding Loss 22.83643341064453, Transition Loss 3.946009397506714, Classifier Loss 0.4831967353820801, Total Loss 231.8003387451172\n",
      "8: Encoding Loss 21.9062557220459, Transition Loss 3.5177791118621826, Classifier Loss 0.43717828392982483, Total Loss 219.67141723632812\n",
      "8: Encoding Loss 21.387731552124023, Transition Loss 3.6975722312927246, Classifier Loss 0.45973873138427734, Total Loss 217.81524658203125\n",
      "8: Encoding Loss 20.271589279174805, Transition Loss 4.503178119659424, Classifier Loss 0.4399975836277008, Total Loss 207.07310485839844\n",
      "8: Encoding Loss 20.547521591186523, Transition Loss 4.357311725616455, Classifier Loss 0.45656701922416687, Total Loss 210.9083251953125\n",
      "8: Encoding Loss 21.07234764099121, Transition Loss 4.035583972930908, Classifier Loss 0.4432950019836426, Total Loss 213.71539306640625\n",
      "8: Encoding Loss 22.33706283569336, Transition Loss 4.120307922363281, Classifier Loss 0.4248289167881012, Total Loss 222.00347900390625\n",
      "8: Encoding Loss 22.082630157470703, Transition Loss 5.225270748138428, Classifier Loss 0.4339323937892914, Total Loss 221.09933471679688\n",
      "8: Encoding Loss 21.285127639770508, Transition Loss 4.379438400268555, Classifier Loss 0.4432823061943054, Total Loss 215.48513793945312\n",
      "8: Encoding Loss 21.848613739013672, Transition Loss 3.794769287109375, Classifier Loss 0.4188966751098633, Total Loss 217.4375457763672\n",
      "8: Encoding Loss 20.386011123657227, Transition Loss 4.630764484405518, Classifier Loss 0.40211227536201477, Total Loss 204.2254638671875\n",
      "8: Encoding Loss 20.401342391967773, Transition Loss 4.834347724914551, Classifier Loss 0.41308000683784485, Total Loss 205.48561096191406\n",
      "8: Encoding Loss 20.9960880279541, Transition Loss 4.751420021057129, Classifier Loss 0.4213681221008301, Total Loss 211.05580139160156\n",
      "8: Encoding Loss 20.807348251342773, Transition Loss 6.259939193725586, Classifier Loss 0.46487537026405334, Total Loss 214.19830322265625\n",
      "8: Encoding Loss 21.51723289489746, Transition Loss 4.265202522277832, Classifier Loss 0.45692503452301025, Total Loss 218.68341064453125\n",
      "8: Encoding Loss 21.5062313079834, Transition Loss 5.100622653961182, Classifier Loss 0.43961453437805176, Total Loss 217.03143310546875\n",
      "8: Encoding Loss 20.966157913208008, Transition Loss 5.0665283203125, Classifier Loss 0.4425415098667145, Total Loss 212.99671936035156\n",
      "8: Encoding Loss 20.72295379638672, Transition Loss 4.559696674346924, Classifier Loss 0.4167390763759613, Total Loss 208.36947631835938\n",
      "8: Encoding Loss 21.516145706176758, Transition Loss 3.758058547973633, Classifier Loss 0.4684593975543976, Total Loss 219.72671508789062\n",
      "8: Encoding Loss 20.932836532592773, Transition Loss 3.9315452575683594, Classifier Loss 0.4489661455154419, Total Loss 213.1456298828125\n",
      "8: Encoding Loss 21.20955467224121, Transition Loss 4.186689376831055, Classifier Loss 0.4363773465156555, Total Loss 214.15151977539062\n",
      "8: Encoding Loss 21.01383399963379, Transition Loss 4.460209846496582, Classifier Loss 0.4391026794910431, Total Loss 212.91297912597656\n",
      "8: Encoding Loss 21.410457611083984, Transition Loss 4.7066826820373535, Classifier Loss 0.4462869167327881, Total Loss 216.85369873046875\n",
      "8: Encoding Loss 22.05626106262207, Transition Loss 3.30373215675354, Classifier Loss 0.4237337112426758, Total Loss 219.4842071533203\n",
      "8: Encoding Loss 20.744686126708984, Transition Loss 4.205438613891602, Classifier Loss 0.43246105313301086, Total Loss 210.04470825195312\n",
      "8: Encoding Loss 21.357051849365234, Transition Loss 3.5185749530792236, Classifier Loss 0.4410264194011688, Total Loss 215.6627960205078\n",
      "8: Encoding Loss 21.167367935180664, Transition Loss 5.433868408203125, Classifier Loss 0.4635491669178009, Total Loss 216.7806396484375\n",
      "8: Encoding Loss 21.19429588317871, Transition Loss 4.080052375793457, Classifier Loss 0.4215714931488037, Total Loss 212.52752685546875\n",
      "8: Encoding Loss 21.43263053894043, Transition Loss 3.3081817626953125, Classifier Loss 0.48668399453163147, Total Loss 220.79107666015625\n",
      "8: Encoding Loss 21.08234214782715, Transition Loss 3.4682719707489014, Classifier Loss 0.42237648367881775, Total Loss 211.5900421142578\n",
      "8: Encoding Loss 21.567487716674805, Transition Loss 5.075972557067871, Classifier Loss 0.44428345561027527, Total Loss 217.9834442138672\n",
      "8: Encoding Loss 20.93645477294922, Transition Loss 4.506179332733154, Classifier Loss 0.45917272567749023, Total Loss 214.3101348876953\n",
      "8: Encoding Loss 21.87549591064453, Transition Loss 3.662648916244507, Classifier Loss 0.4322928190231323, Total Loss 218.9657745361328\n",
      "8: Encoding Loss 21.18052864074707, Transition Loss 3.8232672214508057, Classifier Loss 0.42470771074295044, Total Loss 212.67965698242188\n",
      "8: Encoding Loss 20.589393615722656, Transition Loss 3.0645742416381836, Classifier Loss 0.44329047203063965, Total Loss 209.6571044921875\n",
      "8: Encoding Loss 21.494152069091797, Transition Loss 4.5171966552734375, Classifier Loss 0.46305981278419495, Total Loss 219.16265869140625\n",
      "8: Encoding Loss 21.044727325439453, Transition Loss 3.58565354347229, Classifier Loss 0.43741098046302795, Total Loss 212.8160400390625\n",
      "8: Encoding Loss 20.17826271057129, Transition Loss 5.354428291320801, Classifier Loss 0.4329547584056854, Total Loss 205.79246520996094\n",
      "8: Encoding Loss 21.799264907836914, Transition Loss 2.9878833293914795, Classifier Loss 0.4421207308769226, Total Loss 219.2037811279297\n",
      "8: Encoding Loss 21.677419662475586, Transition Loss 3.6209189891815186, Classifier Loss 0.4298591613769531, Total Loss 217.12945556640625\n",
      "8: Encoding Loss 21.45391845703125, Transition Loss 3.6108336448669434, Classifier Loss 0.4348022937774658, Total Loss 215.833740234375\n",
      "8: Encoding Loss 20.633344650268555, Transition Loss 5.1371026039123535, Classifier Loss 0.43303656578063965, Total Loss 209.39784240722656\n",
      "8: Encoding Loss 21.143310546875, Transition Loss 4.9918904304504395, Classifier Loss 0.419213205575943, Total Loss 212.06619262695312\n",
      "8: Encoding Loss 20.841920852661133, Transition Loss 4.4063334465026855, Classifier Loss 0.4287722706794739, Total Loss 210.49386596679688\n",
      "8: Encoding Loss 19.892059326171875, Transition Loss 5.8404107093811035, Classifier Loss 0.44573774933815, Total Loss 204.87832641601562\n",
      "8: Encoding Loss 20.968563079833984, Transition Loss 4.352092742919922, Classifier Loss 0.4683210551738739, Total Loss 215.4510498046875\n",
      "8: Encoding Loss 21.3134822845459, Transition Loss 4.380134582519531, Classifier Loss 0.43393486738204956, Total Loss 214.77735900878906\n",
      "8: Encoding Loss 19.776437759399414, Transition Loss 4.200340270996094, Classifier Loss 0.4468766450881958, Total Loss 203.73924255371094\n",
      "8: Encoding Loss 21.08857536315918, Transition Loss 4.9987664222717285, Classifier Loss 0.4610091745853424, Total Loss 215.8092803955078\n",
      "8: Encoding Loss 21.3172607421875, Transition Loss 4.411271095275879, Classifier Loss 0.43474382162094116, Total Loss 214.89471435546875\n",
      "8: Encoding Loss 22.18213653564453, Transition Loss 4.281346797943115, Classifier Loss 0.41638025641441345, Total Loss 219.95138549804688\n",
      "8: Encoding Loss 20.891925811767578, Transition Loss 4.271870136260986, Classifier Loss 0.4498552083969116, Total Loss 212.9752960205078\n",
      "8: Encoding Loss 21.711498260498047, Transition Loss 3.521151542663574, Classifier Loss 0.40017375349998474, Total Loss 214.41360473632812\n",
      "8: Encoding Loss 20.87236976623535, Transition Loss 3.233290433883667, Classifier Loss 0.4937934875488281, Total Loss 217.0049591064453\n",
      "8: Encoding Loss 22.868738174438477, Transition Loss 4.935940742492676, Classifier Loss 0.4464867115020752, Total Loss 228.58575439453125\n",
      "8: Encoding Loss 21.490964889526367, Transition Loss 3.4244956970214844, Classifier Loss 0.44489410519599915, Total Loss 217.10203552246094\n",
      "8: Encoding Loss 20.46038055419922, Transition Loss 3.9402947425842285, Classifier Loss 0.4448637068271637, Total Loss 208.9574737548828\n",
      "8: Encoding Loss 20.502559661865234, Transition Loss 4.301236152648926, Classifier Loss 0.4215608239173889, Total Loss 207.0368194580078\n",
      "8: Encoding Loss 21.264604568481445, Transition Loss 3.6462478637695312, Classifier Loss 0.4540494382381439, Total Loss 216.25103759765625\n",
      "8: Encoding Loss 20.799617767333984, Transition Loss 2.633348226547241, Classifier Loss 0.4619867205619812, Total Loss 213.12229919433594\n",
      "8: Encoding Loss 20.381616592407227, Transition Loss 3.124567747116089, Classifier Loss 0.4292023777961731, Total Loss 206.59808349609375\n",
      "8: Encoding Loss 20.803110122680664, Transition Loss 4.7338547706604, Classifier Loss 0.42991364002227783, Total Loss 210.36302185058594\n",
      "8: Encoding Loss 21.67145538330078, Transition Loss 2.5461816787719727, Classifier Loss 0.43814098834991455, Total Loss 217.69497680664062\n",
      "8: Encoding Loss 21.46779441833496, Transition Loss 2.7323269844055176, Classifier Loss 0.44895869493484497, Total Loss 217.1846923828125\n",
      "8: Encoding Loss 20.257747650146484, Transition Loss 4.494334697723389, Classifier Loss 0.453191339969635, Total Loss 208.27999877929688\n",
      "8: Encoding Loss 21.01653289794922, Transition Loss 4.944797039031982, Classifier Loss 0.47249895334243774, Total Loss 216.37110900878906\n",
      "8: Encoding Loss 20.932374954223633, Transition Loss 4.2780561447143555, Classifier Loss 0.43831461668014526, Total Loss 212.1460723876953\n",
      "8: Encoding Loss 21.199949264526367, Transition Loss 4.690001487731934, Classifier Loss 0.4454003572463989, Total Loss 215.07763671875\n",
      "8: Encoding Loss 20.714426040649414, Transition Loss 6.119682312011719, Classifier Loss 0.4667125344276428, Total Loss 213.610595703125\n",
      "8: Encoding Loss 21.281795501708984, Transition Loss 3.844271659851074, Classifier Loss 0.44461140036582947, Total Loss 215.484375\n",
      "8: Encoding Loss 21.047475814819336, Transition Loss 3.6145997047424316, Classifier Loss 0.4118431806564331, Total Loss 210.2870330810547\n",
      "8: Encoding Loss 20.401601791381836, Transition Loss 3.897597312927246, Classifier Loss 0.4172499477863312, Total Loss 205.7173309326172\n",
      "8: Encoding Loss 20.902624130249023, Transition Loss 3.503744125366211, Classifier Loss 0.47106069326400757, Total Loss 215.02780151367188\n",
      "8: Encoding Loss 20.461999893188477, Transition Loss 4.304236888885498, Classifier Loss 0.39380624890327454, Total Loss 203.93746948242188\n",
      "8: Encoding Loss 20.457210540771484, Transition Loss 4.292118549346924, Classifier Loss 0.4583457112312317, Total Loss 210.3507080078125\n",
      "8: Encoding Loss 21.185216903686523, Transition Loss 3.018834114074707, Classifier Loss 0.42837223410606384, Total Loss 212.92271423339844\n",
      "8: Encoding Loss 21.26244354248047, Transition Loss 3.4862287044525146, Classifier Loss 0.43636924028396606, Total Loss 214.43373107910156\n",
      "8: Encoding Loss 19.604726791381836, Transition Loss 3.5066051483154297, Classifier Loss 0.4402157664299011, Total Loss 201.5607147216797\n",
      "8: Encoding Loss 19.647897720336914, Transition Loss 4.570795059204102, Classifier Loss 0.4124864935874939, Total Loss 199.34597778320312\n",
      "8: Encoding Loss 23.117326736450195, Transition Loss 3.0646064281463623, Classifier Loss 0.45502105355262756, Total Loss 231.0536346435547\n",
      "8: Encoding Loss 20.745023727416992, Transition Loss 5.48116397857666, Classifier Loss 0.4600798189640045, Total Loss 213.0644073486328\n",
      "8: Encoding Loss 21.70454978942871, Transition Loss 3.731194019317627, Classifier Loss 0.4605802893638611, Total Loss 220.440673828125\n",
      "8: Encoding Loss 21.066438674926758, Transition Loss 5.047501087188721, Classifier Loss 0.4452803432941437, Total Loss 214.0690460205078\n",
      "8: Encoding Loss 21.482858657836914, Transition Loss 3.6866421699523926, Classifier Loss 0.44240617752075195, Total Loss 216.8408203125\n",
      "8: Encoding Loss 19.698217391967773, Transition Loss 4.585592269897461, Classifier Loss 0.4303480088710785, Total Loss 201.53765869140625\n",
      "8: Encoding Loss 21.63905143737793, Transition Loss 2.5291903018951416, Classifier Loss 0.43763870000839233, Total Loss 217.3821258544922\n",
      "8: Encoding Loss 20.328922271728516, Transition Loss 2.426525115966797, Classifier Loss 0.4167560338973999, Total Loss 204.7922821044922\n",
      "8: Encoding Loss 21.09367561340332, Transition Loss 3.468701124191284, Classifier Loss 0.42772820591926575, Total Loss 212.21595764160156\n",
      "8: Encoding Loss 20.016557693481445, Transition Loss 4.057084083557129, Classifier Loss 0.4445340037345886, Total Loss 205.39727783203125\n",
      "8: Encoding Loss 20.02547836303711, Transition Loss 2.7135913372039795, Classifier Loss 0.4561854898929596, Total Loss 206.3651123046875\n",
      "8: Encoding Loss 21.55339813232422, Transition Loss 5.322425365447998, Classifier Loss 0.4514274597167969, Total Loss 218.63441467285156\n",
      "8: Encoding Loss 20.6676082611084, Transition Loss 3.5853707790374756, Classifier Loss 0.4519203305244446, Total Loss 211.24996948242188\n",
      "8: Encoding Loss 22.140165328979492, Transition Loss 3.972168445587158, Classifier Loss 0.4165290296077728, Total Loss 219.56866455078125\n",
      "8: Encoding Loss 22.029611587524414, Transition Loss 4.367977619171143, Classifier Loss 0.430764377117157, Total Loss 220.18692016601562\n",
      "8: Encoding Loss 21.210264205932617, Transition Loss 5.720529556274414, Classifier Loss 0.4363427460193634, Total Loss 214.4604949951172\n",
      "8: Encoding Loss 19.679243087768555, Transition Loss 6.894804954528809, Classifier Loss 0.4639374911785126, Total Loss 205.2066650390625\n",
      "9: Encoding Loss 21.51491355895996, Transition Loss 3.4545774459838867, Classifier Loss 0.4562540352344513, Total Loss 218.43563842773438\n",
      "9: Encoding Loss 21.075204849243164, Transition Loss 3.597946882247925, Classifier Loss 0.4229571223258972, Total Loss 211.616943359375\n",
      "9: Encoding Loss 22.117795944213867, Transition Loss 3.560075283050537, Classifier Loss 0.4547509253025055, Total Loss 223.12948608398438\n",
      "9: Encoding Loss 20.835323333740234, Transition Loss 3.559342861175537, Classifier Loss 0.44068285822868347, Total Loss 211.46275329589844\n",
      "9: Encoding Loss 21.624547958374023, Transition Loss 3.1213345527648926, Classifier Loss 0.44450247287750244, Total Loss 218.07089233398438\n",
      "9: Encoding Loss 20.317411422729492, Transition Loss 4.638214588165283, Classifier Loss 0.4605792760848999, Total Loss 209.5248565673828\n",
      "9: Encoding Loss 21.45723533630371, Transition Loss 2.7793421745300293, Classifier Loss 0.4549686312675476, Total Loss 217.7106170654297\n",
      "9: Encoding Loss 22.821672439575195, Transition Loss 3.3243606090545654, Classifier Loss 0.4475401043891907, Total Loss 227.9922637939453\n",
      "9: Encoding Loss 21.266338348388672, Transition Loss 3.5071351528167725, Classifier Loss 0.4805336594581604, Total Loss 218.88551330566406\n",
      "9: Encoding Loss 22.343843460083008, Transition Loss 3.858032703399658, Classifier Loss 0.4150281846523285, Total Loss 221.02517700195312\n",
      "9: Encoding Loss 21.655067443847656, Transition Loss 3.2163281440734863, Classifier Loss 0.4356926679611206, Total Loss 217.4530792236328\n",
      "9: Encoding Loss 21.298337936401367, Transition Loss 3.7479021549224854, Classifier Loss 0.4010666608810425, Total Loss 211.2429656982422\n",
      "9: Encoding Loss 20.713403701782227, Transition Loss 3.5667874813079834, Classifier Loss 0.432063490152359, Total Loss 209.62693786621094\n",
      "9: Encoding Loss 20.83620834350586, Transition Loss 3.8332009315490723, Classifier Loss 0.419182151556015, Total Loss 209.37454223632812\n",
      "9: Encoding Loss 20.74799346923828, Transition Loss 3.9781134128570557, Classifier Loss 0.44384631514549255, Total Loss 211.16419982910156\n",
      "9: Encoding Loss 20.915355682373047, Transition Loss 4.526535511016846, Classifier Loss 0.44376784563064575, Total Loss 212.60494995117188\n",
      "9: Encoding Loss 20.664852142333984, Transition Loss 4.007404327392578, Classifier Loss 0.4312370717525482, Total Loss 209.2440185546875\n",
      "9: Encoding Loss 20.862951278686523, Transition Loss 4.0182976722717285, Classifier Loss 0.414236843585968, Total Loss 209.13096618652344\n",
      "9: Encoding Loss 20.943239212036133, Transition Loss 4.288409233093262, Classifier Loss 0.4217606484889984, Total Loss 210.5796661376953\n",
      "9: Encoding Loss 22.07640266418457, Transition Loss 3.8700852394104004, Classifier Loss 0.42714759707450867, Total Loss 220.10000610351562\n",
      "9: Encoding Loss 20.688533782958984, Transition Loss 5.290031909942627, Classifier Loss 0.4413301944732666, Total Loss 210.69931030273438\n",
      "9: Encoding Loss 20.92235565185547, Transition Loss 3.276120185852051, Classifier Loss 0.4185474216938019, Total Loss 209.88880920410156\n",
      "9: Encoding Loss 20.06427001953125, Transition Loss 6.641541957855225, Classifier Loss 0.4387472867965698, Total Loss 205.71719360351562\n",
      "9: Encoding Loss 20.344572067260742, Transition Loss 3.447491407394409, Classifier Loss 0.41988107562065125, Total Loss 205.43418884277344\n",
      "9: Encoding Loss 21.30097198486328, Transition Loss 2.9743356704711914, Classifier Loss 0.41610240936279297, Total Loss 212.61288452148438\n",
      "9: Encoding Loss 22.15120506286621, Transition Loss 4.075068950653076, Classifier Loss 0.448776513338089, Total Loss 222.90231323242188\n",
      "9: Encoding Loss 21.533071517944336, Transition Loss 4.604405879974365, Classifier Loss 0.4482342302799225, Total Loss 218.00888061523438\n",
      "9: Encoding Loss 20.74631690979004, Transition Loss 4.698936462402344, Classifier Loss 0.4478664994239807, Total Loss 211.6969757080078\n",
      "9: Encoding Loss 20.75052833557129, Transition Loss 4.47378396987915, Classifier Loss 0.41699326038360596, Total Loss 208.5983123779297\n",
      "9: Encoding Loss 20.897113800048828, Transition Loss 5.579840183258057, Classifier Loss 0.46212098002433777, Total Loss 214.50497436523438\n",
      "9: Encoding Loss 21.290876388549805, Transition Loss 4.432039260864258, Classifier Loss 0.4274783730506897, Total Loss 213.9612579345703\n",
      "9: Encoding Loss 19.694761276245117, Transition Loss 5.6409220695495605, Classifier Loss 0.4701829254627228, Total Loss 205.70457458496094\n",
      "9: Encoding Loss 21.419292449951172, Transition Loss 4.704780101776123, Classifier Loss 0.47230327129364014, Total Loss 219.525634765625\n",
      "9: Encoding Loss 20.415340423583984, Transition Loss 2.6527867317199707, Classifier Loss 0.46513670682907104, Total Loss 210.36697387695312\n",
      "9: Encoding Loss 20.797338485717773, Transition Loss 4.821813583374023, Classifier Loss 0.4460428059101105, Total Loss 211.9473419189453\n",
      "9: Encoding Loss 20.035566329956055, Transition Loss 3.122262477874756, Classifier Loss 0.4510571360588074, Total Loss 206.0146942138672\n",
      "9: Encoding Loss 21.314449310302734, Transition Loss 4.6613030433654785, Classifier Loss 0.4410387873649597, Total Loss 215.5517578125\n",
      "9: Encoding Loss 21.69571876525879, Transition Loss 6.169648170471191, Classifier Loss 0.4294377565383911, Total Loss 217.7434539794922\n",
      "9: Encoding Loss 20.972721099853516, Transition Loss 3.8394060134887695, Classifier Loss 0.3871392607688904, Total Loss 207.26358032226562\n",
      "9: Encoding Loss 20.80451011657715, Transition Loss 4.9507317543029785, Classifier Loss 0.42087990045547485, Total Loss 209.51422119140625\n",
      "9: Encoding Loss 20.829618453979492, Transition Loss 5.4046502113342285, Classifier Loss 0.4232012629508972, Total Loss 210.0380096435547\n",
      "9: Encoding Loss 20.596519470214844, Transition Loss 3.398847818374634, Classifier Loss 0.450309157371521, Total Loss 210.4828338623047\n",
      "9: Encoding Loss 20.342405319213867, Transition Loss 3.1449334621429443, Classifier Loss 0.40480276942253113, Total Loss 203.84849548339844\n",
      "9: Encoding Loss 20.676382064819336, Transition Loss 2.525595188140869, Classifier Loss 0.4872795045375824, Total Loss 214.6441192626953\n",
      "9: Encoding Loss 21.76990509033203, Transition Loss 3.1643359661102295, Classifier Loss 0.4369165301322937, Total Loss 218.4837646484375\n",
      "9: Encoding Loss 21.18994903564453, Transition Loss 4.491820812225342, Classifier Loss 0.4752883315086365, Total Loss 217.94679260253906\n",
      "9: Encoding Loss 20.876222610473633, Transition Loss 5.120175838470459, Classifier Loss 0.4677295386791229, Total Loss 214.8067626953125\n",
      "9: Encoding Loss 21.661161422729492, Transition Loss 3.936293840408325, Classifier Loss 0.4633951783180237, Total Loss 220.41607666015625\n",
      "9: Encoding Loss 21.853137969970703, Transition Loss 4.0133867263793945, Classifier Loss 0.4452148675918579, Total Loss 220.14926147460938\n",
      "9: Encoding Loss 20.822010040283203, Transition Loss 4.314087390899658, Classifier Loss 0.44952765107154846, Total Loss 212.39166259765625\n",
      "9: Encoding Loss 20.4238224029541, Transition Loss 2.9793965816497803, Classifier Loss 0.41598114371299744, Total Loss 205.58457946777344\n",
      "9: Encoding Loss 20.67657470703125, Transition Loss 3.23126482963562, Classifier Loss 0.4087628722190857, Total Loss 206.9351348876953\n",
      "9: Encoding Loss 20.201391220092773, Transition Loss 2.974689245223999, Classifier Loss 0.4540923833847046, Total Loss 207.6153106689453\n",
      "9: Encoding Loss 20.39628791809082, Transition Loss 4.947351932525635, Classifier Loss 0.4329005479812622, Total Loss 207.4498291015625\n",
      "9: Encoding Loss 20.0224666595459, Transition Loss 4.91340446472168, Classifier Loss 0.41642504930496216, Total Loss 202.80491638183594\n",
      "9: Encoding Loss 20.032846450805664, Transition Loss 4.435824394226074, Classifier Loss 0.4483846127986908, Total Loss 205.98838806152344\n",
      "9: Encoding Loss 20.859498977661133, Transition Loss 4.199167728424072, Classifier Loss 0.41171663999557495, Total Loss 208.88748168945312\n",
      "9: Encoding Loss 20.515586853027344, Transition Loss 4.161406517028809, Classifier Loss 0.40564748644828796, Total Loss 205.52171325683594\n",
      "9: Encoding Loss 20.681203842163086, Transition Loss 4.202030181884766, Classifier Loss 0.45232096314430237, Total Loss 211.5221405029297\n",
      "9: Encoding Loss 20.10557746887207, Transition Loss 3.737992763519287, Classifier Loss 0.4435652494430542, Total Loss 205.94874572753906\n",
      "9: Encoding Loss 21.17144775390625, Transition Loss 5.129851818084717, Classifier Loss 0.43893295526504517, Total Loss 214.2908477783203\n",
      "9: Encoding Loss 21.615367889404297, Transition Loss 3.778454542160034, Classifier Loss 0.44960135221481323, Total Loss 218.63877868652344\n",
      "9: Encoding Loss 21.2785587310791, Transition Loss 3.542550563812256, Classifier Loss 0.4250290095806122, Total Loss 213.43988037109375\n",
      "9: Encoding Loss 20.902353286743164, Transition Loss 3.6323747634887695, Classifier Loss 0.46306920051574707, Total Loss 214.25221252441406\n",
      "9: Encoding Loss 19.960426330566406, Transition Loss 4.44671106338501, Classifier Loss 0.46048325300216675, Total Loss 206.62107849121094\n",
      "9: Encoding Loss 19.835908889770508, Transition Loss 4.300601482391357, Classifier Loss 0.4381740093231201, Total Loss 203.3647918701172\n",
      "9: Encoding Loss 20.93253517150879, Transition Loss 3.9543919563293457, Classifier Loss 0.42979082465171814, Total Loss 211.23023986816406\n",
      "9: Encoding Loss 21.771638870239258, Transition Loss 3.922090530395508, Classifier Loss 0.41416987776756287, Total Loss 216.37452697753906\n",
      "9: Encoding Loss 21.623443603515625, Transition Loss 4.956460475921631, Classifier Loss 0.3938148021697998, Total Loss 213.36032104492188\n",
      "9: Encoding Loss 20.991952896118164, Transition Loss 4.276024341583252, Classifier Loss 0.458680123090744, Total Loss 214.65884399414062\n",
      "9: Encoding Loss 21.17678451538086, Transition Loss 3.61523699760437, Classifier Loss 0.4460406005382538, Total Loss 214.7414093017578\n",
      "9: Encoding Loss 20.03731346130371, Transition Loss 4.554199695587158, Classifier Loss 0.41859108209609985, Total Loss 203.06846618652344\n",
      "9: Encoding Loss 20.454187393188477, Transition Loss 4.711451053619385, Classifier Loss 0.42479318380355835, Total Loss 207.05511474609375\n",
      "9: Encoding Loss 21.41160774230957, Transition Loss 4.649842262268066, Classifier Loss 0.4266659915447235, Total Loss 214.88941955566406\n",
      "9: Encoding Loss 20.324085235595703, Transition Loss 6.086252212524414, Classifier Loss 0.46667778491973877, Total Loss 210.47772216796875\n",
      "9: Encoding Loss 21.37403678894043, Transition Loss 4.22543478012085, Classifier Loss 0.4372391104698181, Total Loss 215.56129455566406\n",
      "9: Encoding Loss 20.956026077270508, Transition Loss 5.046351432800293, Classifier Loss 0.3836112320423126, Total Loss 207.01861572265625\n",
      "9: Encoding Loss 21.04874038696289, Transition Loss 5.115194797515869, Classifier Loss 0.4219379425048828, Total Loss 211.60675048828125\n",
      "9: Encoding Loss 20.410648345947266, Transition Loss 4.508828163146973, Classifier Loss 0.37460970878601074, Total Loss 201.64791870117188\n",
      "9: Encoding Loss 21.605085372924805, Transition Loss 3.697544574737549, Classifier Loss 0.4406408369541168, Total Loss 217.64427185058594\n",
      "9: Encoding Loss 20.523174285888672, Transition Loss 3.9032559394836426, Classifier Loss 0.43350762128829956, Total Loss 208.3168182373047\n",
      "9: Encoding Loss 21.33807373046875, Transition Loss 4.065424919128418, Classifier Loss 0.43357980251312256, Total Loss 214.87564086914062\n",
      "9: Encoding Loss 20.825420379638672, Transition Loss 4.371685028076172, Classifier Loss 0.41159504652023315, Total Loss 208.63722229003906\n",
      "9: Encoding Loss 21.425315856933594, Transition Loss 4.550258636474609, Classifier Loss 0.44627684354782104, Total Loss 216.9402618408203\n",
      "9: Encoding Loss 21.18889045715332, Transition Loss 3.15087890625, Classifier Loss 0.455830454826355, Total Loss 215.72434997558594\n",
      "9: Encoding Loss 20.027482986450195, Transition Loss 4.114240646362305, Classifier Loss 0.3880353569984436, Total Loss 199.84625244140625\n",
      "9: Encoding Loss 21.02099609375, Transition Loss 3.3817150592803955, Classifier Loss 0.4377366304397583, Total Loss 212.61798095703125\n",
      "9: Encoding Loss 21.338804244995117, Transition Loss 5.166640758514404, Classifier Loss 0.445442795753479, Total Loss 216.2880401611328\n",
      "9: Encoding Loss 21.17388916015625, Transition Loss 3.942077159881592, Classifier Loss 0.4025961458683014, Total Loss 210.43914794921875\n",
      "9: Encoding Loss 21.091703414916992, Transition Loss 3.2142982482910156, Classifier Loss 0.43259936571121216, Total Loss 212.63641357421875\n",
      "9: Encoding Loss 21.13483428955078, Transition Loss 3.3202576637268066, Classifier Loss 0.4269874095916748, Total Loss 212.44146728515625\n",
      "9: Encoding Loss 21.21456527709961, Transition Loss 4.856548309326172, Classifier Loss 0.453296959400177, Total Loss 216.01754760742188\n",
      "9: Encoding Loss 20.712995529174805, Transition Loss 4.249772071838379, Classifier Loss 0.4488670825958252, Total Loss 211.4406280517578\n",
      "9: Encoding Loss 21.546815872192383, Transition Loss 3.493154525756836, Classifier Loss 0.4327235817909241, Total Loss 216.3455047607422\n",
      "9: Encoding Loss 21.17908477783203, Transition Loss 3.656883955001831, Classifier Loss 0.43360576033592224, Total Loss 213.52464294433594\n",
      "9: Encoding Loss 20.12688446044922, Transition Loss 3.0111820697784424, Classifier Loss 0.4257432520389557, Total Loss 204.19163513183594\n",
      "9: Encoding Loss 20.73911476135254, Transition Loss 4.321425437927246, Classifier Loss 0.4291934370994568, Total Loss 209.69654846191406\n",
      "9: Encoding Loss 20.832408905029297, Transition Loss 3.429354667663574, Classifier Loss 0.4233301281929016, Total Loss 209.67816162109375\n",
      "9: Encoding Loss 19.964282989501953, Transition Loss 5.112063407897949, Classifier Loss 0.4382002055644989, Total Loss 204.55670166015625\n",
      "9: Encoding Loss 21.66769790649414, Transition Loss 2.8028292655944824, Classifier Loss 0.4331895112991333, Total Loss 217.22109985351562\n",
      "9: Encoding Loss 21.48810386657715, Transition Loss 3.4524478912353516, Classifier Loss 0.44225695729255676, Total Loss 216.82101440429688\n",
      "9: Encoding Loss 21.141244888305664, Transition Loss 3.3629488945007324, Classifier Loss 0.45868563652038574, Total Loss 215.67111206054688\n",
      "9: Encoding Loss 20.290660858154297, Transition Loss 4.9178314208984375, Classifier Loss 0.42771661281585693, Total Loss 206.0805206298828\n",
      "9: Encoding Loss 21.04718589782715, Transition Loss 4.704682350158691, Classifier Loss 0.418834388256073, Total Loss 211.2018585205078\n",
      "9: Encoding Loss 20.48115348815918, Transition Loss 4.1921706199646, Classifier Loss 0.42859312891960144, Total Loss 207.54698181152344\n",
      "9: Encoding Loss 19.359804153442383, Transition Loss 5.612156391143799, Classifier Loss 0.4476224184036255, Total Loss 200.7631072998047\n",
      "9: Encoding Loss 20.26546859741211, Transition Loss 4.131083965301514, Classifier Loss 0.44796738028526306, Total Loss 207.74671936035156\n",
      "9: Encoding Loss 20.981473922729492, Transition Loss 4.247540473937988, Classifier Loss 0.4258790910243988, Total Loss 211.28919982910156\n",
      "9: Encoding Loss 19.61954116821289, Transition Loss 4.073385238647461, Classifier Loss 0.4145929217338562, Total Loss 199.2303009033203\n",
      "9: Encoding Loss 21.188505172729492, Transition Loss 4.837583541870117, Classifier Loss 0.42639803886413574, Total Loss 213.1153564453125\n",
      "9: Encoding Loss 21.04738998413086, Transition Loss 4.199359893798828, Classifier Loss 0.45171070098876953, Total Loss 214.39007568359375\n",
      "9: Encoding Loss 21.568912506103516, Transition Loss 4.140228748321533, Classifier Loss 0.42945730686187744, Total Loss 216.32508850097656\n",
      "9: Encoding Loss 20.58670425415039, Transition Loss 4.120301246643066, Classifier Loss 0.4227501153945923, Total Loss 207.79270935058594\n",
      "9: Encoding Loss 20.802570343017578, Transition Loss 3.457643747329712, Classifier Loss 0.4088391959667206, Total Loss 207.99600219726562\n",
      "9: Encoding Loss 20.69184684753418, Transition Loss 3.181600332260132, Classifier Loss 0.4390062689781189, Total Loss 210.07171630859375\n",
      "9: Encoding Loss 22.462690353393555, Transition Loss 4.863084316253662, Classifier Loss 0.45138001441955566, Total Loss 225.8121337890625\n",
      "9: Encoding Loss 20.92701530456543, Transition Loss 3.412121057510376, Classifier Loss 0.4282030463218689, Total Loss 210.91883850097656\n",
      "9: Encoding Loss 20.141803741455078, Transition Loss 3.984131097793579, Classifier Loss 0.4488207995891571, Total Loss 206.81333923339844\n",
      "9: Encoding Loss 20.693445205688477, Transition Loss 4.2649688720703125, Classifier Loss 0.4190608263015747, Total Loss 208.306640625\n",
      "9: Encoding Loss 20.797901153564453, Transition Loss 3.7243714332580566, Classifier Loss 0.4383687973022461, Total Loss 210.9649658203125\n",
      "9: Encoding Loss 19.898683547973633, Transition Loss 2.629748821258545, Classifier Loss 0.4142830967903137, Total Loss 201.14373779296875\n",
      "9: Encoding Loss 20.236494064331055, Transition Loss 3.1318905353546143, Classifier Loss 0.4268181025981903, Total Loss 205.20013427734375\n",
      "9: Encoding Loss 20.963504791259766, Transition Loss 4.716405868530273, Classifier Loss 0.4481334686279297, Total Loss 213.4646759033203\n",
      "9: Encoding Loss 20.88849639892578, Transition Loss 2.552367687225342, Classifier Loss 0.4418448209762573, Total Loss 211.80291748046875\n",
      "9: Encoding Loss 21.43805694580078, Transition Loss 2.7452025413513184, Classifier Loss 0.43137162923812866, Total Loss 215.19065856933594\n",
      "9: Encoding Loss 20.091259002685547, Transition Loss 4.4415283203125, Classifier Loss 0.4043616056442261, Total Loss 202.05455017089844\n",
      "9: Encoding Loss 20.72868537902832, Transition Loss 4.887014865875244, Classifier Loss 0.42813611030578613, Total Loss 209.62049865722656\n",
      "9: Encoding Loss 20.299827575683594, Transition Loss 4.209780216217041, Classifier Loss 0.42033255100250244, Total Loss 205.27383422851562\n",
      "9: Encoding Loss 21.452129364013672, Transition Loss 4.673352241516113, Classifier Loss 0.47486716508865356, Total Loss 220.03843688964844\n",
      "9: Encoding Loss 20.45146942138672, Transition Loss 6.005184650421143, Classifier Loss 0.4477858245372772, Total Loss 209.59136962890625\n",
      "9: Encoding Loss 21.42487907409668, Transition Loss 3.7784998416900635, Classifier Loss 0.43694576621055603, Total Loss 215.8493194580078\n",
      "9: Encoding Loss 20.696992874145508, Transition Loss 3.5121116638183594, Classifier Loss 0.40547093749046326, Total Loss 206.82545471191406\n",
      "9: Encoding Loss 20.412059783935547, Transition Loss 3.7704765796661377, Classifier Loss 0.4347226917743683, Total Loss 207.52285766601562\n",
      "9: Encoding Loss 20.769407272338867, Transition Loss 3.4145359992980957, Classifier Loss 0.45681217312812805, Total Loss 212.51937866210938\n",
      "9: Encoding Loss 20.191089630126953, Transition Loss 4.130734920501709, Classifier Loss 0.4934709668159485, Total Loss 211.7019500732422\n",
      "9: Encoding Loss 20.2705078125, Transition Loss 4.16948127746582, Classifier Loss 0.4038538634777069, Total Loss 203.3833465576172\n",
      "9: Encoding Loss 20.600801467895508, Transition Loss 2.986243963241577, Classifier Loss 0.42811277508735657, Total Loss 208.21493530273438\n",
      "9: Encoding Loss 20.689245223999023, Transition Loss 3.4005627632141113, Classifier Loss 0.4155649244785309, Total Loss 207.7505645751953\n",
      "9: Encoding Loss 19.63871192932129, Transition Loss 3.4791934490203857, Classifier Loss 0.4193386733531952, Total Loss 199.73939514160156\n",
      "9: Encoding Loss 19.285429000854492, Transition Loss 4.501740455627441, Classifier Loss 0.40211597084999084, Total Loss 195.39537048339844\n",
      "9: Encoding Loss 22.357860565185547, Transition Loss 3.031996488571167, Classifier Loss 0.46616506576538086, Total Loss 226.08580017089844\n",
      "9: Encoding Loss 20.839757919311523, Transition Loss 5.423879623413086, Classifier Loss 0.4599505364894867, Total Loss 213.7978973388672\n",
      "9: Encoding Loss 21.277769088745117, Transition Loss 3.6980934143066406, Classifier Loss 0.4372950792312622, Total Loss 214.6912841796875\n",
      "9: Encoding Loss 21.09957504272461, Transition Loss 4.997424125671387, Classifier Loss 0.4425618350505829, Total Loss 214.05227661132812\n",
      "9: Encoding Loss 21.01732063293457, Transition Loss 3.6675660610198975, Classifier Loss 0.45200127363204956, Total Loss 214.07220458984375\n",
      "9: Encoding Loss 19.574735641479492, Transition Loss 4.517114639282227, Classifier Loss 0.4771519899368286, Total Loss 205.2165069580078\n",
      "9: Encoding Loss 21.34099006652832, Transition Loss 2.4811081886291504, Classifier Loss 0.4661019742488861, Total Loss 217.83433532714844\n",
      "9: Encoding Loss 20.237031936645508, Transition Loss 2.4117512702941895, Classifier Loss 0.4398913085460663, Total Loss 206.36773681640625\n",
      "9: Encoding Loss 20.761409759521484, Transition Loss 3.4137115478515625, Classifier Loss 0.4118955433368683, Total Loss 207.96359252929688\n",
      "9: Encoding Loss 19.933820724487305, Transition Loss 4.074618339538574, Classifier Loss 0.4250434637069702, Total Loss 202.7898406982422\n",
      "9: Encoding Loss 19.951595306396484, Transition Loss 2.707491636276245, Classifier Loss 0.4200937747955322, Total Loss 202.16366577148438\n",
      "9: Encoding Loss 21.082536697387695, Transition Loss 5.331851005554199, Classifier Loss 0.437140554189682, Total Loss 213.4407196044922\n",
      "9: Encoding Loss 20.50768280029297, Transition Loss 3.5427136421203613, Classifier Loss 0.3938772976398468, Total Loss 204.15773010253906\n",
      "9: Encoding Loss 22.147001266479492, Transition Loss 3.9198410511016846, Classifier Loss 0.43036460876464844, Total Loss 220.99642944335938\n",
      "9: Encoding Loss 21.555078506469727, Transition Loss 4.314361095428467, Classifier Loss 0.4365634322166443, Total Loss 216.9598388671875\n",
      "9: Encoding Loss 20.760332107543945, Transition Loss 5.663847923278809, Classifier Loss 0.44156402349472046, Total Loss 211.371826171875\n",
      "9: Encoding Loss 19.50967788696289, Transition Loss 6.828462600708008, Classifier Loss 0.4059012830257416, Total Loss 198.0332489013672\n",
      "10: Encoding Loss 21.0529842376709, Transition Loss 3.278024435043335, Classifier Loss 0.40088003873825073, Total Loss 209.16748046875\n",
      "10: Encoding Loss 21.425703048706055, Transition Loss 3.44576358795166, Classifier Loss 0.4271312952041626, Total Loss 214.8079071044922\n",
      "10: Encoding Loss 21.600337982177734, Transition Loss 3.3726139068603516, Classifier Loss 0.4398346543312073, Total Loss 217.46072387695312\n",
      "10: Encoding Loss 20.355552673339844, Transition Loss 3.4410061836242676, Classifier Loss 0.42566558718681335, Total Loss 206.09918212890625\n",
      "10: Encoding Loss 22.04438018798828, Transition Loss 2.935455322265625, Classifier Loss 0.45111963152885437, Total Loss 222.05410766601562\n",
      "10: Encoding Loss 20.283580780029297, Transition Loss 4.481534481048584, Classifier Loss 0.45447027683258057, Total Loss 208.61199951171875\n",
      "10: Encoding Loss 21.10984992980957, Transition Loss 2.705672264099121, Classifier Loss 0.43426093459129333, Total Loss 212.84603881835938\n",
      "10: Encoding Loss 22.246620178222656, Transition Loss 3.115964651107788, Classifier Loss 0.43799036741256714, Total Loss 222.39520263671875\n",
      "10: Encoding Loss 20.806562423706055, Transition Loss 3.4067773818969727, Classifier Loss 0.4822617471218109, Total Loss 215.3600311279297\n",
      "10: Encoding Loss 22.386308670043945, Transition Loss 3.6158480644226074, Classifier Loss 0.44690370559692383, Total Loss 224.50401306152344\n",
      "10: Encoding Loss 21.174373626708984, Transition Loss 3.1153721809387207, Classifier Loss 0.44833946228027344, Total Loss 214.85202026367188\n",
      "10: Encoding Loss 21.13328742980957, Transition Loss 3.563551902770996, Classifier Loss 0.4078056216239929, Total Loss 210.5595703125\n",
      "10: Encoding Loss 20.565336227416992, Transition Loss 3.536268949508667, Classifier Loss 0.4359356760978699, Total Loss 208.82351684570312\n",
      "10: Encoding Loss 20.664369583129883, Transition Loss 3.7177793979644775, Classifier Loss 0.4178587794303894, Total Loss 207.84439086914062\n",
      "10: Encoding Loss 20.3338565826416, Transition Loss 3.9054081439971924, Classifier Loss 0.4392260015010834, Total Loss 207.37452697753906\n",
      "10: Encoding Loss 20.9160099029541, Transition Loss 4.389126300811768, Classifier Loss 0.4375040829181671, Total Loss 211.95631408691406\n",
      "10: Encoding Loss 20.503684997558594, Transition Loss 3.9070703983306885, Classifier Loss 0.4163532853126526, Total Loss 206.44622802734375\n",
      "10: Encoding Loss 20.426050186157227, Transition Loss 3.917198419570923, Classifier Loss 0.3945139944553375, Total Loss 203.64324951171875\n",
      "10: Encoding Loss 20.911556243896484, Transition Loss 4.155503749847412, Classifier Loss 0.424625426530838, Total Loss 210.5861053466797\n",
      "10: Encoding Loss 21.98652458190918, Transition Loss 3.6671142578125, Classifier Loss 0.4297511577606201, Total Loss 219.60073852539062\n",
      "10: Encoding Loss 20.436620712280273, Transition Loss 5.074237823486328, Classifier Loss 0.4362942576408386, Total Loss 208.13723754882812\n",
      "10: Encoding Loss 20.454917907714844, Transition Loss 3.175520896911621, Classifier Loss 0.39595526456832886, Total Loss 203.86996459960938\n",
      "10: Encoding Loss 20.072925567626953, Transition Loss 6.389804840087891, Classifier Loss 0.43114015460014343, Total Loss 204.97537231445312\n",
      "10: Encoding Loss 20.178510665893555, Transition Loss 3.2836334705352783, Classifier Loss 0.4307844638824463, Total Loss 205.1632537841797\n",
      "10: Encoding Loss 20.49634552001953, Transition Loss 2.8417274951934814, Classifier Loss 0.3950626850128174, Total Loss 204.04537963867188\n",
      "10: Encoding Loss 21.60133934020996, Transition Loss 3.7951719760894775, Classifier Loss 0.4048922657966614, Total Loss 214.05897521972656\n",
      "10: Encoding Loss 21.02170753479004, Transition Loss 4.224169731140137, Classifier Loss 0.4422779083251953, Total Loss 213.2462921142578\n",
      "10: Encoding Loss 20.435102462768555, Transition Loss 4.3340044021606445, Classifier Loss 0.4095262289047241, Total Loss 205.3002471923828\n",
      "10: Encoding Loss 20.59663200378418, Transition Loss 4.079526901245117, Classifier Loss 0.410869300365448, Total Loss 206.67588806152344\n",
      "10: Encoding Loss 20.591163635253906, Transition Loss 5.135012626647949, Classifier Loss 0.42572885751724243, Total Loss 208.32920837402344\n",
      "10: Encoding Loss 20.646373748779297, Transition Loss 4.080780029296875, Classifier Loss 0.4180614948272705, Total Loss 207.79331970214844\n",
      "10: Encoding Loss 19.808996200561523, Transition Loss 5.184728622436523, Classifier Loss 0.412487268447876, Total Loss 200.7576446533203\n",
      "10: Encoding Loss 21.326005935668945, Transition Loss 4.192680835723877, Classifier Loss 0.44612181186676025, Total Loss 216.0587615966797\n",
      "10: Encoding Loss 20.011720657348633, Transition Loss 2.5490219593048096, Classifier Loss 0.48973745107650757, Total Loss 209.5773162841797\n",
      "10: Encoding Loss 20.50714874267578, Transition Loss 4.367824554443359, Classifier Loss 0.473145455121994, Total Loss 212.24530029296875\n",
      "10: Encoding Loss 19.851152420043945, Transition Loss 2.9997551441192627, Classifier Loss 0.4232481122016907, Total Loss 201.73397827148438\n",
      "10: Encoding Loss 20.490901947021484, Transition Loss 4.2414374351501465, Classifier Loss 0.4349207878112793, Total Loss 208.26759338378906\n",
      "10: Encoding Loss 20.877248764038086, Transition Loss 5.6150617599487305, Classifier Loss 0.4469647705554962, Total Loss 212.8374786376953\n",
      "10: Encoding Loss 20.973087310791016, Transition Loss 3.447605848312378, Classifier Loss 0.3892049789428711, Total Loss 207.39471435546875\n",
      "10: Encoding Loss 20.218990325927734, Transition Loss 4.513343334197998, Classifier Loss 0.4298655390739441, Total Loss 205.6411590576172\n",
      "10: Encoding Loss 20.320701599121094, Transition Loss 4.959120273590088, Classifier Loss 0.43935301899909973, Total Loss 207.49273681640625\n",
      "10: Encoding Loss 20.360713958740234, Transition Loss 3.1792078018188477, Classifier Loss 0.4127040505409241, Total Loss 204.79197692871094\n",
      "10: Encoding Loss 20.461091995239258, Transition Loss 2.9342825412750244, Classifier Loss 0.4527498483657837, Total Loss 209.55056762695312\n",
      "10: Encoding Loss 19.69267463684082, Transition Loss 2.374272346496582, Classifier Loss 0.4254540801048279, Total Loss 200.56166076660156\n",
      "10: Encoding Loss 21.388696670532227, Transition Loss 2.8984534740448, Classifier Loss 0.4429904520511627, Total Loss 215.98831176757812\n",
      "10: Encoding Loss 21.418190002441406, Transition Loss 4.053893089294434, Classifier Loss 0.42143166065216064, Total Loss 214.29945373535156\n",
      "10: Encoding Loss 20.49481964111328, Transition Loss 4.715224266052246, Classifier Loss 0.4464508891105652, Total Loss 209.5466766357422\n",
      "10: Encoding Loss 21.7996826171875, Transition Loss 3.613596200942993, Classifier Loss 0.43115171790122986, Total Loss 218.2353515625\n",
      "10: Encoding Loss 21.502403259277344, Transition Loss 3.675135850906372, Classifier Loss 0.4301348626613617, Total Loss 215.7677459716797\n",
      "10: Encoding Loss 20.82317352294922, Transition Loss 3.987470865249634, Classifier Loss 0.43054986000061035, Total Loss 210.43788146972656\n",
      "10: Encoding Loss 20.09993553161621, Transition Loss 2.7640750408172607, Classifier Loss 0.43548861145973206, Total Loss 204.90115356445312\n",
      "10: Encoding Loss 20.103843688964844, Transition Loss 3.0192036628723145, Classifier Loss 0.45499104261398315, Total Loss 206.93368530273438\n",
      "10: Encoding Loss 19.909914016723633, Transition Loss 2.763561487197876, Classifier Loss 0.41897767782211304, Total Loss 201.72979736328125\n",
      "10: Encoding Loss 20.306835174560547, Transition Loss 4.612176895141602, Classifier Loss 0.41680824756622314, Total Loss 205.05796813964844\n",
      "10: Encoding Loss 20.333919525146484, Transition Loss 4.600309371948242, Classifier Loss 0.416531503200531, Total Loss 205.2445831298828\n",
      "10: Encoding Loss 19.63184356689453, Transition Loss 4.1703338623046875, Classifier Loss 0.4573780596256256, Total Loss 203.62661743164062\n",
      "10: Encoding Loss 20.736570358276367, Transition Loss 3.899268627166748, Classifier Loss 0.4195479154586792, Total Loss 208.627197265625\n",
      "10: Encoding Loss 20.473840713500977, Transition Loss 3.8770499229431152, Classifier Loss 0.4047321081161499, Total Loss 205.03933715820312\n",
      "10: Encoding Loss 20.631906509399414, Transition Loss 3.8824472427368164, Classifier Loss 0.43039751052856445, Total Loss 208.87149047851562\n",
      "10: Encoding Loss 20.392330169677734, Transition Loss 3.4461822509765625, Classifier Loss 0.42108845710754395, Total Loss 205.93673706054688\n",
      "10: Encoding Loss 21.045215606689453, Transition Loss 4.636349678039551, Classifier Loss 0.44922083616256714, Total Loss 214.21109008789062\n",
      "10: Encoding Loss 21.323637008666992, Transition Loss 3.5355892181396484, Classifier Loss 0.4136046767234802, Total Loss 212.65667724609375\n",
      "10: Encoding Loss 21.060964584350586, Transition Loss 3.336320161819458, Classifier Loss 0.4185280203819275, Total Loss 211.00778198242188\n",
      "10: Encoding Loss 20.636693954467773, Transition Loss 3.3465158939361572, Classifier Loss 0.41612398624420166, Total Loss 207.375244140625\n",
      "10: Encoding Loss 20.200593948364258, Transition Loss 4.067788600921631, Classifier Loss 0.4483903646469116, Total Loss 207.25733947753906\n",
      "10: Encoding Loss 19.75536346435547, Transition Loss 3.9411168098449707, Classifier Loss 0.4099915623664856, Total Loss 199.83029174804688\n",
      "10: Encoding Loss 20.53451156616211, Transition Loss 3.5363779067993164, Classifier Loss 0.39493486285209656, Total Loss 204.47686767578125\n",
      "10: Encoding Loss 21.646448135375977, Transition Loss 3.4480271339416504, Classifier Loss 0.4183054566383362, Total Loss 215.69174194335938\n",
      "10: Encoding Loss 21.509977340698242, Transition Loss 4.313809394836426, Classifier Loss 0.42219269275665283, Total Loss 215.16184997558594\n",
      "10: Encoding Loss 20.609378814697266, Transition Loss 3.7866780757904053, Classifier Loss 0.4325602650642395, Total Loss 208.88839721679688\n",
      "10: Encoding Loss 21.31212615966797, Transition Loss 3.108250141143799, Classifier Loss 0.4207245111465454, Total Loss 213.19110107421875\n",
      "10: Encoding Loss 19.658355712890625, Transition Loss 4.071707725524902, Classifier Loss 0.38344496488571167, Total Loss 196.42568969726562\n",
      "10: Encoding Loss 20.096120834350586, Transition Loss 4.16337251663208, Classifier Loss 0.42897316813468933, Total Loss 204.49896240234375\n",
      "10: Encoding Loss 20.429668426513672, Transition Loss 4.060162544250488, Classifier Loss 0.4222479462623596, Total Loss 206.47418212890625\n",
      "10: Encoding Loss 20.025020599365234, Transition Loss 5.471913814544678, Classifier Loss 0.4560946822166443, Total Loss 206.90402221679688\n",
      "10: Encoding Loss 21.125940322875977, Transition Loss 3.721557378768921, Classifier Loss 0.4315064251422882, Total Loss 212.90248107910156\n",
      "10: Encoding Loss 20.655094146728516, Transition Loss 4.470633506774902, Classifier Loss 0.3874047100543976, Total Loss 204.87535095214844\n",
      "10: Encoding Loss 20.698993682861328, Transition Loss 4.558111667633057, Classifier Loss 0.44874539971351624, Total Loss 211.37811279296875\n",
      "10: Encoding Loss 19.935720443725586, Transition Loss 4.1184539794921875, Classifier Loss 0.39853739738464355, Total Loss 200.16319274902344\n",
      "10: Encoding Loss 21.459936141967773, Transition Loss 3.2259252071380615, Classifier Loss 0.4356081485748291, Total Loss 215.88548278808594\n",
      "10: Encoding Loss 20.098974227905273, Transition Loss 3.5867326259613037, Classifier Loss 0.3819688558578491, Total Loss 199.70602416992188\n",
      "10: Encoding Loss 20.49105453491211, Transition Loss 3.7083334922790527, Classifier Loss 0.42224806547164917, Total Loss 206.89492797851562\n",
      "10: Encoding Loss 20.867229461669922, Transition Loss 3.9515013694763184, Classifier Loss 0.45436447858810425, Total Loss 213.1645965576172\n",
      "10: Encoding Loss 20.67572784423828, Transition Loss 4.244818210601807, Classifier Loss 0.42564281821250916, Total Loss 208.81907653808594\n",
      "10: Encoding Loss 21.28749656677246, Transition Loss 2.881129503250122, Classifier Loss 0.4463673233985901, Total Loss 215.512939453125\n",
      "10: Encoding Loss 19.800724029541016, Transition Loss 3.860637664794922, Classifier Loss 0.3839777112007141, Total Loss 197.57568359375\n",
      "10: Encoding Loss 20.538387298583984, Transition Loss 3.1938507556915283, Classifier Loss 0.447696715593338, Total Loss 209.71554565429688\n",
      "10: Encoding Loss 20.818078994750977, Transition Loss 4.781618118286133, Classifier Loss 0.4558325707912445, Total Loss 213.08421325683594\n",
      "10: Encoding Loss 20.590856552124023, Transition Loss 3.6652963161468506, Classifier Loss 0.4247417449951172, Total Loss 207.93408203125\n",
      "10: Encoding Loss 20.775495529174805, Transition Loss 3.021183967590332, Classifier Loss 0.4217688739299774, Total Loss 208.98509216308594\n",
      "10: Encoding Loss 20.92479705810547, Transition Loss 3.1242623329162598, Classifier Loss 0.40148836374282837, Total Loss 208.17205810546875\n",
      "10: Encoding Loss 20.94745635986328, Transition Loss 4.528948783874512, Classifier Loss 0.5075775384902954, Total Loss 219.24319458007812\n",
      "10: Encoding Loss 20.529199600219727, Transition Loss 3.995109796524048, Classifier Loss 0.4265921711921692, Total Loss 207.6918487548828\n",
      "10: Encoding Loss 21.07038116455078, Transition Loss 3.2041373252868652, Classifier Loss 0.43186184763908386, Total Loss 212.3900604248047\n",
      "10: Encoding Loss 21.0468807220459, Transition Loss 3.3528575897216797, Classifier Loss 0.46157243847846985, Total Loss 215.20286560058594\n",
      "10: Encoding Loss 19.856115341186523, Transition Loss 2.895648956298828, Classifier Loss 0.4189528226852417, Total Loss 201.32333374023438\n",
      "10: Encoding Loss 20.60400390625, Transition Loss 3.9825830459594727, Classifier Loss 0.43656548857688904, Total Loss 209.2851104736328\n",
      "10: Encoding Loss 20.18377685546875, Transition Loss 3.222987174987793, Classifier Loss 0.4133947491645813, Total Loss 203.45428466796875\n",
      "10: Encoding Loss 19.79001808166504, Transition Loss 4.742307662963867, Classifier Loss 0.4190669655799866, Total Loss 201.17529296875\n",
      "10: Encoding Loss 21.606834411621094, Transition Loss 2.6568896770477295, Classifier Loss 0.42967459559440613, Total Loss 216.353515625\n",
      "10: Encoding Loss 20.87055778503418, Transition Loss 3.2607827186584473, Classifier Loss 0.43047499656677246, Total Loss 210.66412353515625\n",
      "10: Encoding Loss 20.658754348754883, Transition Loss 3.134451389312744, Classifier Loss 0.44514238834381104, Total Loss 210.41116333007812\n",
      "10: Encoding Loss 19.933399200439453, Transition Loss 4.533547401428223, Classifier Loss 0.42226991057395935, Total Loss 202.60089111328125\n",
      "10: Encoding Loss 20.84047508239746, Transition Loss 4.377015590667725, Classifier Loss 0.4131774306297302, Total Loss 208.91693115234375\n",
      "10: Encoding Loss 20.458656311035156, Transition Loss 3.879317283630371, Classifier Loss 0.4405699372291565, Total Loss 208.50210571289062\n",
      "10: Encoding Loss 19.33477210998535, Transition Loss 5.206255912780762, Classifier Loss 0.4237390160560608, Total Loss 198.09332275390625\n",
      "10: Encoding Loss 20.19211769104004, Transition Loss 3.7897727489471436, Classifier Loss 0.4543260335922241, Total Loss 207.7274932861328\n",
      "10: Encoding Loss 20.922758102416992, Transition Loss 3.908936023712158, Classifier Loss 0.3897132873535156, Total Loss 207.1351776123047\n",
      "10: Encoding Loss 19.40387725830078, Transition Loss 3.9156861305236816, Classifier Loss 0.4249137043952942, Total Loss 198.50552368164062\n",
      "10: Encoding Loss 20.59538459777832, Transition Loss 4.455419540405273, Classifier Loss 0.41063401103019714, Total Loss 206.71755981445312\n",
      "10: Encoding Loss 20.295650482177734, Transition Loss 3.8645241260528564, Classifier Loss 0.4132724404335022, Total Loss 204.46536254882812\n",
      "10: Encoding Loss 21.134204864501953, Transition Loss 3.742403984069824, Classifier Loss 0.4261554479598999, Total Loss 212.43765258789062\n",
      "10: Encoding Loss 20.36927604675293, Transition Loss 3.788933515548706, Classifier Loss 0.43688371777534485, Total Loss 207.40036010742188\n",
      "10: Encoding Loss 21.19127655029297, Transition Loss 3.106140613555908, Classifier Loss 0.41098999977111816, Total Loss 211.2504425048828\n",
      "10: Encoding Loss 20.164085388183594, Transition Loss 2.9950661659240723, Classifier Loss 0.452205091714859, Total Loss 207.1322021484375\n",
      "10: Encoding Loss 21.89305305480957, Transition Loss 4.289203643798828, Classifier Loss 0.40366530418395996, Total Loss 216.36878967285156\n",
      "10: Encoding Loss 20.86979866027832, Transition Loss 3.035942554473877, Classifier Loss 0.43849965929985046, Total Loss 211.41554260253906\n",
      "10: Encoding Loss 19.939128875732422, Transition Loss 3.636608839035034, Classifier Loss 0.4184812605381012, Total Loss 202.0885009765625\n",
      "10: Encoding Loss 20.386058807373047, Transition Loss 3.8102991580963135, Classifier Loss 0.4212219715118408, Total Loss 205.9727325439453\n",
      "10: Encoding Loss 20.68548011779785, Transition Loss 3.3852882385253906, Classifier Loss 0.3995393216609955, Total Loss 206.11483764648438\n",
      "10: Encoding Loss 19.85984230041504, Transition Loss 2.432450294494629, Classifier Loss 0.3999224901199341, Total Loss 199.35748291015625\n",
      "10: Encoding Loss 19.977083206176758, Transition Loss 2.9713516235351562, Classifier Loss 0.42784127593040466, Total Loss 203.195068359375\n",
      "10: Encoding Loss 20.447114944458008, Transition Loss 4.26019811630249, Classifier Loss 0.43385887145996094, Total Loss 207.81483459472656\n",
      "10: Encoding Loss 20.6082820892334, Transition Loss 2.3353636264801025, Classifier Loss 0.41020846366882324, Total Loss 206.3541717529297\n",
      "10: Encoding Loss 21.043989181518555, Transition Loss 2.410740613937378, Classifier Loss 0.45704707503318787, Total Loss 214.5387725830078\n",
      "10: Encoding Loss 20.032176971435547, Transition Loss 4.137758255004883, Classifier Loss 0.4180790185928345, Total Loss 202.89288330078125\n",
      "10: Encoding Loss 19.987550735473633, Transition Loss 4.584433555603027, Classifier Loss 0.43329721689224243, Total Loss 204.1470184326172\n",
      "10: Encoding Loss 19.87026023864746, Transition Loss 3.8668713569641113, Classifier Loss 0.4427042007446289, Total Loss 204.00587463378906\n",
      "10: Encoding Loss 20.535932540893555, Transition Loss 4.299488067626953, Classifier Loss 0.45305487513542175, Total Loss 210.45285034179688\n",
      "10: Encoding Loss 19.88376808166504, Transition Loss 5.408931255340576, Classifier Loss 0.43122977018356323, Total Loss 203.27490234375\n",
      "10: Encoding Loss 20.962284088134766, Transition Loss 3.3535103797912598, Classifier Loss 0.4650232195854187, Total Loss 214.8712921142578\n",
      "10: Encoding Loss 21.141468048095703, Transition Loss 3.211779832839966, Classifier Loss 0.4025707542896271, Total Loss 210.0311737060547\n",
      "10: Encoding Loss 20.123098373413086, Transition Loss 3.572476863861084, Classifier Loss 0.4389737546443939, Total Loss 205.59666442871094\n",
      "10: Encoding Loss 20.132488250732422, Transition Loss 3.2230119705200195, Classifier Loss 0.431708425283432, Total Loss 204.8753662109375\n",
      "10: Encoding Loss 20.004932403564453, Transition Loss 3.850586414337158, Classifier Loss 0.4287993013858795, Total Loss 203.6894989013672\n",
      "10: Encoding Loss 20.073387145996094, Transition Loss 3.7589099407196045, Classifier Loss 0.4025641679763794, Total Loss 201.5952911376953\n",
      "10: Encoding Loss 20.196069717407227, Transition Loss 2.7453103065490723, Classifier Loss 0.3952130079269409, Total Loss 201.638916015625\n",
      "10: Encoding Loss 20.044301986694336, Transition Loss 3.0336110591888428, Classifier Loss 0.4245356023311615, Total Loss 203.41470336914062\n",
      "10: Encoding Loss 19.146841049194336, Transition Loss 3.2749581336975098, Classifier Loss 0.43247687816619873, Total Loss 197.07742309570312\n",
      "10: Encoding Loss 19.203075408935547, Transition Loss 3.961202383041382, Classifier Loss 0.4003792405128479, Total Loss 194.45477294921875\n",
      "10: Encoding Loss 21.965295791625977, Transition Loss 2.67874813079834, Classifier Loss 0.4399733543395996, Total Loss 220.2554473876953\n",
      "10: Encoding Loss 20.11733627319336, Transition Loss 4.80208683013916, Classifier Loss 0.45196589827537537, Total Loss 207.09571838378906\n",
      "10: Encoding Loss 20.904644012451172, Transition Loss 3.229543924331665, Classifier Loss 0.42958182096481323, Total Loss 210.84124755859375\n",
      "10: Encoding Loss 21.010005950927734, Transition Loss 4.311657905578613, Classifier Loss 0.4549139142036438, Total Loss 214.4337921142578\n",
      "10: Encoding Loss 20.771015167236328, Transition Loss 3.20926570892334, Classifier Loss 0.443493127822876, Total Loss 211.15927124023438\n",
      "10: Encoding Loss 19.714019775390625, Transition Loss 4.02198600769043, Classifier Loss 0.42402029037475586, Total Loss 200.91859436035156\n",
      "10: Encoding Loss 21.270252227783203, Transition Loss 2.16013765335083, Classifier Loss 0.42012643814086914, Total Loss 212.60667419433594\n",
      "10: Encoding Loss 19.8438777923584, Transition Loss 2.20137619972229, Classifier Loss 0.4034411907196045, Total Loss 199.53541564941406\n",
      "10: Encoding Loss 20.74302101135254, Transition Loss 3.079359292984009, Classifier Loss 0.4250396490097046, Total Loss 209.0640106201172\n",
      "10: Encoding Loss 19.264041900634766, Transition Loss 3.7098495960235596, Classifier Loss 0.4147305190563202, Total Loss 196.32736206054688\n",
      "10: Encoding Loss 19.65089988708496, Transition Loss 2.505918025970459, Classifier Loss 0.4423061013221741, Total Loss 201.93899536132812\n",
      "10: Encoding Loss 20.923681259155273, Transition Loss 4.636412620544434, Classifier Loss 0.41465437412261963, Total Loss 209.78216552734375\n",
      "10: Encoding Loss 20.527103424072266, Transition Loss 3.1166188716888428, Classifier Loss 0.3944604992866516, Total Loss 204.28619384765625\n",
      "10: Encoding Loss 21.71695327758789, Transition Loss 3.3576157093048096, Classifier Loss 0.4366847276687622, Total Loss 218.07562255859375\n",
      "10: Encoding Loss 21.354503631591797, Transition Loss 3.6994922161102295, Classifier Loss 0.4285692870616913, Total Loss 214.43287658691406\n",
      "10: Encoding Loss 20.438392639160156, Transition Loss 4.8657636642456055, Classifier Loss 0.42510634660720825, Total Loss 206.99093627929688\n",
      "10: Encoding Loss 19.05927848815918, Transition Loss 6.059682369232178, Classifier Loss 0.48693007230758667, Total Loss 202.37916564941406\n",
      "11: Encoding Loss 20.572607040405273, Transition Loss 2.8876872062683105, Classifier Loss 0.3896753489971161, Total Loss 204.12591552734375\n",
      "11: Encoding Loss 20.518714904785156, Transition Loss 3.0202860832214355, Classifier Loss 0.4191862940788269, Total Loss 206.67239379882812\n",
      "11: Encoding Loss 21.65894889831543, Transition Loss 2.9847421646118164, Classifier Loss 0.432005912065506, Total Loss 217.0691375732422\n",
      "11: Encoding Loss 20.339378356933594, Transition Loss 3.042058229446411, Classifier Loss 0.4265553653240204, Total Loss 205.97897338867188\n",
      "11: Encoding Loss 21.18277359008789, Transition Loss 2.498887300491333, Classifier Loss 0.4143865406513214, Total Loss 211.40061950683594\n",
      "11: Encoding Loss 19.653873443603516, Transition Loss 4.104128837585449, Classifier Loss 0.4407019317150116, Total Loss 202.12200927734375\n",
      "11: Encoding Loss 21.429353713989258, Transition Loss 2.4143433570861816, Classifier Loss 0.44950786232948303, Total Loss 216.8684844970703\n",
      "11: Encoding Loss 22.064908981323242, Transition Loss 2.6629791259765625, Classifier Loss 0.44037124514579773, Total Loss 221.0889892578125\n",
      "11: Encoding Loss 20.628732681274414, Transition Loss 3.0900442600250244, Classifier Loss 0.47471582889556885, Total Loss 213.1194610595703\n",
      "11: Encoding Loss 21.692222595214844, Transition Loss 3.121504068374634, Classifier Loss 0.4411929249763489, Total Loss 218.2813720703125\n",
      "11: Encoding Loss 20.996463775634766, Transition Loss 2.8091964721679688, Classifier Loss 0.46279940009117126, Total Loss 214.81349182128906\n",
      "11: Encoding Loss 20.427289962768555, Transition Loss 3.2196011543273926, Classifier Loss 0.4342615306377411, Total Loss 207.48838806152344\n",
      "11: Encoding Loss 20.387914657592773, Transition Loss 3.21071195602417, Classifier Loss 0.4313335716724396, Total Loss 206.87881469726562\n",
      "11: Encoding Loss 20.699615478515625, Transition Loss 3.445227861404419, Classifier Loss 0.4379827082157135, Total Loss 210.08424377441406\n",
      "11: Encoding Loss 20.05755043029785, Transition Loss 3.6522274017333984, Classifier Loss 0.464773565530777, Total Loss 207.66819763183594\n",
      "11: Encoding Loss 20.643348693847656, Transition Loss 4.012746810913086, Classifier Loss 0.42998579144477844, Total Loss 208.9479217529297\n",
      "11: Encoding Loss 20.403038024902344, Transition Loss 3.615046501159668, Classifier Loss 0.4322068691253662, Total Loss 207.16799926757812\n",
      "11: Encoding Loss 20.41206932067871, Transition Loss 3.668224811553955, Classifier Loss 0.4356718361377716, Total Loss 207.59738159179688\n",
      "11: Encoding Loss 20.3979549407959, Transition Loss 3.833881139755249, Classifier Loss 0.41850903630256653, Total Loss 205.8013153076172\n",
      "11: Encoding Loss 21.078481674194336, Transition Loss 3.414660692214966, Classifier Loss 0.4042775630950928, Total Loss 209.73854064941406\n",
      "11: Encoding Loss 20.348979949951172, Transition Loss 4.700047016143799, Classifier Loss 0.43141478300094604, Total Loss 206.8733367919922\n",
      "11: Encoding Loss 20.08355712890625, Transition Loss 2.9664571285247803, Classifier Loss 0.4026459753513336, Total Loss 201.52635192871094\n",
      "11: Encoding Loss 19.745210647583008, Transition Loss 5.9403157234191895, Classifier Loss 0.4038160443305969, Total Loss 199.53135681152344\n",
      "11: Encoding Loss 19.845129013061523, Transition Loss 3.079873561859131, Classifier Loss 0.4214973449707031, Total Loss 201.52674865722656\n",
      "11: Encoding Loss 20.623727798461914, Transition Loss 2.6891257762908936, Classifier Loss 0.41117072105407715, Total Loss 206.6447296142578\n",
      "11: Encoding Loss 21.069416046142578, Transition Loss 3.4950363636016846, Classifier Loss 0.42517751455307007, Total Loss 211.77207946777344\n",
      "11: Encoding Loss 21.06805419921875, Transition Loss 3.864213228225708, Classifier Loss 0.42582792043685913, Total Loss 211.9000701904297\n",
      "11: Encoding Loss 20.445697784423828, Transition Loss 4.038461208343506, Classifier Loss 0.4373408854007721, Total Loss 208.10736083984375\n",
      "11: Encoding Loss 19.962377548217773, Transition Loss 3.919982671737671, Classifier Loss 0.4082489609718323, Total Loss 201.30792236328125\n",
      "11: Encoding Loss 19.937227249145508, Transition Loss 4.834408760070801, Classifier Loss 0.4399092495441437, Total Loss 204.45562744140625\n",
      "11: Encoding Loss 20.478078842163086, Transition Loss 3.8175296783447266, Classifier Loss 0.41402769088745117, Total Loss 205.99090576171875\n",
      "11: Encoding Loss 19.350278854370117, Transition Loss 4.915938377380371, Classifier Loss 0.45293447375297546, Total Loss 201.078857421875\n",
      "11: Encoding Loss 20.944063186645508, Transition Loss 3.9967892169952393, Classifier Loss 0.4545920789241791, Total Loss 213.8110809326172\n",
      "11: Encoding Loss 19.98375701904297, Transition Loss 2.467034101486206, Classifier Loss 0.4400990605354309, Total Loss 204.3733673095703\n",
      "11: Encoding Loss 20.149803161621094, Transition Loss 4.200104236602783, Classifier Loss 0.4345397353172302, Total Loss 205.492431640625\n",
      "11: Encoding Loss 19.326114654541016, Transition Loss 2.8427088260650635, Classifier Loss 0.4172118604183197, Total Loss 196.89865112304688\n",
      "11: Encoding Loss 20.736398696899414, Transition Loss 3.896284580230713, Classifier Loss 0.4094264805316925, Total Loss 207.6130828857422\n",
      "11: Encoding Loss 20.671680450439453, Transition Loss 5.164107799530029, Classifier Loss 0.41707587242126465, Total Loss 208.1138458251953\n",
      "11: Encoding Loss 20.952192306518555, Transition Loss 3.1947689056396484, Classifier Loss 0.41099146008491516, Total Loss 209.3556365966797\n",
      "11: Encoding Loss 19.980224609375, Transition Loss 4.209076404571533, Classifier Loss 0.3964982032775879, Total Loss 200.3334197998047\n",
      "11: Encoding Loss 20.234203338623047, Transition Loss 4.596045970916748, Classifier Loss 0.4766049385070801, Total Loss 210.45333862304688\n",
      "11: Encoding Loss 20.211334228515625, Transition Loss 3.010024309158325, Classifier Loss 0.43035954236984253, Total Loss 205.32862854003906\n",
      "11: Encoding Loss 20.126220703125, Transition Loss 2.793079137802124, Classifier Loss 0.4126884937286377, Total Loss 202.83721923828125\n",
      "11: Encoding Loss 19.88751983642578, Transition Loss 2.280714988708496, Classifier Loss 0.4247363209724426, Total Loss 202.02993774414062\n",
      "11: Encoding Loss 21.111703872680664, Transition Loss 2.6779000759124756, Classifier Loss 0.4055878520011902, Total Loss 209.98800659179688\n",
      "11: Encoding Loss 21.10372543334961, Transition Loss 3.752885103225708, Classifier Loss 0.4488784074783325, Total Loss 214.46823120117188\n",
      "11: Encoding Loss 20.461156845092773, Transition Loss 4.389386177062988, Classifier Loss 0.4256923496723175, Total Loss 207.13636779785156\n",
      "11: Encoding Loss 21.101299285888672, Transition Loss 3.318955421447754, Classifier Loss 0.42329561710357666, Total Loss 211.8037567138672\n",
      "11: Encoding Loss 20.717016220092773, Transition Loss 3.402686595916748, Classifier Loss 0.4124029278755188, Total Loss 207.65696716308594\n",
      "11: Encoding Loss 20.614011764526367, Transition Loss 3.7413926124572754, Classifier Loss 0.4223592281341553, Total Loss 207.8962860107422\n",
      "11: Encoding Loss 19.933748245239258, Transition Loss 2.6110455989837646, Classifier Loss 0.41539639234542847, Total Loss 201.53182983398438\n",
      "11: Encoding Loss 20.015409469604492, Transition Loss 2.904841661453247, Classifier Loss 0.4039640426635742, Total Loss 201.10064697265625\n",
      "11: Encoding Loss 19.941869735717773, Transition Loss 2.6690385341644287, Classifier Loss 0.4127270579338074, Total Loss 201.3414764404297\n",
      "11: Encoding Loss 19.95438003540039, Transition Loss 4.343567371368408, Classifier Loss 0.4496222734451294, Total Loss 205.46597290039062\n",
      "11: Encoding Loss 19.90599250793457, Transition Loss 4.382765769958496, Classifier Loss 0.4190070331096649, Total Loss 202.02520751953125\n",
      "11: Encoding Loss 19.294294357299805, Transition Loss 4.097424030303955, Classifier Loss 0.44164448976516724, Total Loss 199.33828735351562\n",
      "11: Encoding Loss 20.205707550048828, Transition Loss 3.7226357460021973, Classifier Loss 0.3907566964626312, Total Loss 201.46585083007812\n",
      "11: Encoding Loss 20.09552764892578, Transition Loss 3.7752797603607178, Classifier Loss 0.4248959720134735, Total Loss 204.0088653564453\n",
      "11: Encoding Loss 20.315149307250977, Transition Loss 3.726879358291626, Classifier Loss 0.4211617708206177, Total Loss 205.38275146484375\n",
      "11: Encoding Loss 20.03194236755371, Transition Loss 3.3220064640045166, Classifier Loss 0.4289274513721466, Total Loss 203.81268310546875\n",
      "11: Encoding Loss 20.855403900146484, Transition Loss 4.383132457733154, Classifier Loss 0.4471643269062042, Total Loss 212.43630981445312\n",
      "11: Encoding Loss 21.506166458129883, Transition Loss 3.2922637462615967, Classifier Loss 0.4241131842136383, Total Loss 215.1190948486328\n",
      "11: Encoding Loss 20.99919891357422, Transition Loss 3.2547037601470947, Classifier Loss 0.4092976152896881, Total Loss 209.5742950439453\n",
      "11: Encoding Loss 20.782222747802734, Transition Loss 3.2201924324035645, Classifier Loss 0.41734036803245544, Total Loss 208.63587951660156\n",
      "11: Encoding Loss 19.600887298583984, Transition Loss 3.966343879699707, Classifier Loss 0.43584179878234863, Total Loss 201.1845703125\n",
      "11: Encoding Loss 19.600481033325195, Transition Loss 3.8099637031555176, Classifier Loss 0.4225708544254303, Total Loss 199.8229217529297\n",
      "11: Encoding Loss 20.461278915405273, Transition Loss 3.480004072189331, Classifier Loss 0.422706663608551, Total Loss 206.6569061279297\n",
      "11: Encoding Loss 21.54570770263672, Transition Loss 3.293297529220581, Classifier Loss 0.4262741506099701, Total Loss 215.6517333984375\n",
      "11: Encoding Loss 21.531892776489258, Transition Loss 4.277130603790283, Classifier Loss 0.4373626708984375, Total Loss 216.84683227539062\n",
      "11: Encoding Loss 20.50713348388672, Transition Loss 3.7078869342803955, Classifier Loss 0.447581022977829, Total Loss 209.55674743652344\n",
      "11: Encoding Loss 20.945476531982422, Transition Loss 3.115553855895996, Classifier Loss 0.42448508739471436, Total Loss 210.63543701171875\n",
      "11: Encoding Loss 19.398714065551758, Transition Loss 3.949981212615967, Classifier Loss 0.39999091625213623, Total Loss 195.9788055419922\n",
      "11: Encoding Loss 19.977052688598633, Transition Loss 4.134279727935791, Classifier Loss 0.4112388491630554, Total Loss 201.7671661376953\n",
      "11: Encoding Loss 20.93727684020996, Transition Loss 4.063354015350342, Classifier Loss 0.40935689210891724, Total Loss 209.24656677246094\n",
      "11: Encoding Loss 20.2409725189209, Transition Loss 5.443233966827393, Classifier Loss 0.4401198625564575, Total Loss 207.02841186523438\n",
      "11: Encoding Loss 21.093280792236328, Transition Loss 3.702350616455078, Classifier Loss 0.40292423963546753, Total Loss 209.7791290283203\n",
      "11: Encoding Loss 20.70395278930664, Transition Loss 4.479850769042969, Classifier Loss 0.3912540376186371, Total Loss 205.65298461914062\n",
      "11: Encoding Loss 20.763423919677734, Transition Loss 4.464825630187988, Classifier Loss 0.4051573872566223, Total Loss 207.51609802246094\n",
      "11: Encoding Loss 20.22892189025879, Transition Loss 4.175577640533447, Classifier Loss 0.3973982632160187, Total Loss 202.40631103515625\n",
      "11: Encoding Loss 21.109323501586914, Transition Loss 3.2432234287261963, Classifier Loss 0.40689656138420105, Total Loss 210.212890625\n",
      "11: Encoding Loss 19.779699325561523, Transition Loss 3.564450263977051, Classifier Loss 0.4336855411529541, Total Loss 202.31903076171875\n",
      "11: Encoding Loss 20.387128829956055, Transition Loss 3.752748966217041, Classifier Loss 0.45947563648223877, Total Loss 209.79513549804688\n",
      "11: Encoding Loss 20.212501525878906, Transition Loss 3.945903778076172, Classifier Loss 0.43748944997787476, Total Loss 206.23814392089844\n",
      "11: Encoding Loss 20.72896385192871, Transition Loss 4.249784469604492, Classifier Loss 0.4271150231361389, Total Loss 209.39317321777344\n",
      "11: Encoding Loss 21.529855728149414, Transition Loss 2.780996799468994, Classifier Loss 0.4048551619052887, Total Loss 213.2805633544922\n",
      "11: Encoding Loss 19.918615341186523, Transition Loss 3.80702805519104, Classifier Loss 0.4136498272418976, Total Loss 201.4752960205078\n",
      "11: Encoding Loss 20.667163848876953, Transition Loss 3.104702949523926, Classifier Loss 0.4193599820137024, Total Loss 207.89425659179688\n",
      "11: Encoding Loss 20.60514259338379, Transition Loss 4.668054580688477, Classifier Loss 0.4385375380516052, Total Loss 209.6284942626953\n",
      "11: Encoding Loss 20.796234130859375, Transition Loss 3.522966146469116, Classifier Loss 0.4312938153743744, Total Loss 210.20384216308594\n",
      "11: Encoding Loss 20.524595260620117, Transition Loss 2.9067602157592773, Classifier Loss 0.43087565898895264, Total Loss 207.8656768798828\n",
      "11: Encoding Loss 20.68544578552246, Transition Loss 2.9696364402770996, Classifier Loss 0.4243171811103821, Total Loss 208.50921630859375\n",
      "11: Encoding Loss 20.7275390625, Transition Loss 4.311557292938232, Classifier Loss 0.44478940963745117, Total Loss 211.16156005859375\n",
      "11: Encoding Loss 20.476787567138672, Transition Loss 3.873059034347534, Classifier Loss 0.4183776080608368, Total Loss 206.4266815185547\n",
      "11: Encoding Loss 21.337244033813477, Transition Loss 3.1222004890441895, Classifier Loss 0.4257562458515167, Total Loss 213.89801025390625\n",
      "11: Encoding Loss 20.545381546020508, Transition Loss 3.277730941772461, Classifier Loss 0.40331703424453735, Total Loss 205.35031127929688\n",
      "11: Encoding Loss 19.822214126586914, Transition Loss 2.7935373783111572, Classifier Loss 0.394431471824646, Total Loss 198.57955932617188\n",
      "11: Encoding Loss 20.697086334228516, Transition Loss 3.862027406692505, Classifier Loss 0.4233279228210449, Total Loss 208.681884765625\n",
      "11: Encoding Loss 20.016435623168945, Transition Loss 3.114208221435547, Classifier Loss 0.45725345611572266, Total Loss 206.47967529296875\n",
      "11: Encoding Loss 19.530004501342773, Transition Loss 4.640970706939697, Classifier Loss 0.4436189830303192, Total Loss 201.53012084960938\n",
      "11: Encoding Loss 21.21682357788086, Transition Loss 2.599860668182373, Classifier Loss 0.4275992512702942, Total Loss 213.01451110839844\n",
      "11: Encoding Loss 21.14621925354004, Transition Loss 3.2136268615722656, Classifier Loss 0.4422268271446228, Total Loss 214.03517150878906\n",
      "11: Encoding Loss 20.50324249267578, Transition Loss 3.1431467533111572, Classifier Loss 0.4351503252983093, Total Loss 208.1696014404297\n",
      "11: Encoding Loss 19.773040771484375, Transition Loss 4.582487106323242, Classifier Loss 0.43984711170196533, Total Loss 203.08554077148438\n",
      "11: Encoding Loss 20.443445205688477, Transition Loss 4.420428276062012, Classifier Loss 0.4416522979736328, Total Loss 208.5968780517578\n",
      "11: Encoding Loss 19.922700881958008, Transition Loss 3.924846649169922, Classifier Loss 0.42226889729499817, Total Loss 202.39346313476562\n",
      "11: Encoding Loss 18.88653564453125, Transition Loss 5.362212181091309, Classifier Loss 0.402157723903656, Total Loss 192.38050842285156\n",
      "11: Encoding Loss 19.851598739624023, Transition Loss 3.8854851722717285, Classifier Loss 0.437429279088974, Total Loss 203.33282470703125\n",
      "11: Encoding Loss 20.538373947143555, Transition Loss 3.9667422771453857, Classifier Loss 0.4032033681869507, Total Loss 205.42066955566406\n",
      "11: Encoding Loss 19.13026237487793, Transition Loss 3.874863862991333, Classifier Loss 0.43395519256591797, Total Loss 197.2126007080078\n",
      "11: Encoding Loss 20.30720329284668, Transition Loss 4.437924385070801, Classifier Loss 0.4416213929653168, Total Loss 207.50735473632812\n",
      "11: Encoding Loss 20.5210018157959, Transition Loss 3.9071133136749268, Classifier Loss 0.4056849777698517, Total Loss 205.51792907714844\n",
      "11: Encoding Loss 20.83633804321289, Transition Loss 3.8247263431549072, Classifier Loss 0.42459627985954285, Total Loss 209.91526794433594\n",
      "11: Encoding Loss 20.309864044189453, Transition Loss 3.8024535179138184, Classifier Loss 0.4077363610267639, Total Loss 204.01304626464844\n",
      "11: Encoding Loss 20.9215087890625, Transition Loss 3.1301753520965576, Classifier Loss 0.3973808288574219, Total Loss 207.73619079589844\n",
      "11: Encoding Loss 19.826961517333984, Transition Loss 2.9753308296203613, Classifier Loss 0.42264270782470703, Total Loss 201.47503662109375\n",
      "11: Encoding Loss 22.02458381652832, Transition Loss 4.327837944030762, Classifier Loss 0.4213030934333801, Total Loss 219.1925506591797\n",
      "11: Encoding Loss 20.81064224243164, Transition Loss 3.0420024394989014, Classifier Loss 0.4154573082923889, Total Loss 208.63926696777344\n",
      "11: Encoding Loss 19.26767921447754, Transition Loss 3.5991129875183105, Classifier Loss 0.4121743142604828, Total Loss 196.07867431640625\n",
      "11: Encoding Loss 20.059017181396484, Transition Loss 3.767134428024292, Classifier Loss 0.39519646763801575, Total Loss 200.74522399902344\n",
      "11: Encoding Loss 20.44806671142578, Transition Loss 3.321301221847534, Classifier Loss 0.421443372964859, Total Loss 206.39312744140625\n",
      "11: Encoding Loss 19.914043426513672, Transition Loss 2.323744058609009, Classifier Loss 0.4284498393535614, Total Loss 202.62210083007812\n",
      "11: Encoding Loss 19.76812744140625, Transition Loss 2.90838360786438, Classifier Loss 0.4369584918022156, Total Loss 202.42254638671875\n",
      "11: Encoding Loss 20.640722274780273, Transition Loss 4.206491470336914, Classifier Loss 0.42968013882637024, Total Loss 208.93508911132812\n",
      "11: Encoding Loss 20.41725730895996, Transition Loss 2.292722463607788, Classifier Loss 0.39305251836776733, Total Loss 203.1018524169922\n",
      "11: Encoding Loss 20.897319793701172, Transition Loss 2.442960023880005, Classifier Loss 0.4387052357196808, Total Loss 211.53768920898438\n",
      "11: Encoding Loss 19.747148513793945, Transition Loss 4.083135604858398, Classifier Loss 0.4462859630584717, Total Loss 203.4224090576172\n",
      "11: Encoding Loss 20.461095809936523, Transition Loss 4.600309371948242, Classifier Loss 0.4107430577278137, Total Loss 205.68313598632812\n",
      "11: Encoding Loss 19.805843353271484, Transition Loss 3.8825478553771973, Classifier Loss 0.3957236409187317, Total Loss 198.79563903808594\n",
      "11: Encoding Loss 20.550296783447266, Transition Loss 4.409618377685547, Classifier Loss 0.43879127502441406, Total Loss 209.16342163085938\n",
      "11: Encoding Loss 19.750789642333984, Transition Loss 5.463084697723389, Classifier Loss 0.42676272988319397, Total Loss 201.7752227783203\n",
      "11: Encoding Loss 20.940595626831055, Transition Loss 3.375756025314331, Classifier Loss 0.4163013696670532, Total Loss 209.83006286621094\n",
      "11: Encoding Loss 20.534912109375, Transition Loss 3.132622480392456, Classifier Loss 0.401767373085022, Total Loss 205.0825653076172\n",
      "11: Encoding Loss 19.80112648010254, Transition Loss 3.5305240154266357, Classifier Loss 0.4042465388774872, Total Loss 199.53976440429688\n",
      "11: Encoding Loss 19.945152282714844, Transition Loss 3.142388105392456, Classifier Loss 0.4068670868873596, Total Loss 200.87640380859375\n",
      "11: Encoding Loss 20.10233497619629, Transition Loss 3.7458674907684326, Classifier Loss 0.4592694044113159, Total Loss 207.4947967529297\n",
      "11: Encoding Loss 20.103979110717773, Transition Loss 3.663067102432251, Classifier Loss 0.39919424057006836, Total Loss 201.48387145996094\n",
      "11: Encoding Loss 20.272565841674805, Transition Loss 2.746047019958496, Classifier Loss 0.4172787666320801, Total Loss 204.45761108398438\n",
      "11: Encoding Loss 20.11662483215332, Transition Loss 3.0164706707000732, Classifier Loss 0.42498302459716797, Total Loss 204.0345916748047\n",
      "11: Encoding Loss 18.84652328491211, Transition Loss 3.286653518676758, Classifier Loss 0.39945146441459656, Total Loss 191.3746795654297\n",
      "11: Encoding Loss 18.744474411010742, Transition Loss 3.922779083251953, Classifier Loss 0.3686734735965729, Total Loss 187.6077117919922\n",
      "11: Encoding Loss 21.992334365844727, Transition Loss 2.6913843154907227, Classifier Loss 0.4532562494277954, Total Loss 221.80258178710938\n",
      "11: Encoding Loss 20.075841903686523, Transition Loss 4.695343971252441, Classifier Loss 0.4290253520011902, Total Loss 204.44834899902344\n",
      "11: Encoding Loss 20.704627990722656, Transition Loss 3.2554726600646973, Classifier Loss 0.4031466245651245, Total Loss 206.602783203125\n",
      "11: Encoding Loss 20.728273391723633, Transition Loss 4.3022541999816895, Classifier Loss 0.42062872648239136, Total Loss 208.74952697753906\n",
      "11: Encoding Loss 20.69371795654297, Transition Loss 3.1882429122924805, Classifier Loss 0.4738768935203552, Total Loss 213.57508850097656\n",
      "11: Encoding Loss 19.459627151489258, Transition Loss 4.020212173461914, Classifier Loss 0.40479347109794617, Total Loss 196.96041870117188\n",
      "11: Encoding Loss 21.00551986694336, Transition Loss 2.1379308700561523, Classifier Loss 0.40447160601615906, Total Loss 208.91891479492188\n",
      "11: Encoding Loss 19.6723575592041, Transition Loss 2.1870172023773193, Classifier Loss 0.3873327970504761, Total Loss 196.54954528808594\n",
      "11: Encoding Loss 20.37445640563965, Transition Loss 3.064858913421631, Classifier Loss 0.3930674195289612, Total Loss 202.91537475585938\n",
      "11: Encoding Loss 19.40692901611328, Transition Loss 3.6334972381591797, Classifier Loss 0.43321195244789124, Total Loss 199.30332946777344\n",
      "11: Encoding Loss 19.72861099243164, Transition Loss 2.473135471343994, Classifier Loss 0.40253889560699463, Total Loss 198.57740783691406\n",
      "11: Encoding Loss 20.530683517456055, Transition Loss 4.579504489898682, Classifier Loss 0.4145648181438446, Total Loss 206.6178436279297\n",
      "11: Encoding Loss 20.141000747680664, Transition Loss 3.06459903717041, Classifier Loss 0.4175216853618622, Total Loss 203.4930877685547\n",
      "11: Encoding Loss 21.463436126708984, Transition Loss 3.3170766830444336, Classifier Loss 0.4271509349346161, Total Loss 215.0860137939453\n",
      "11: Encoding Loss 20.81536102294922, Transition Loss 3.6281237602233887, Classifier Loss 0.37674835324287415, Total Loss 204.92335510253906\n",
      "11: Encoding Loss 20.503206253051758, Transition Loss 4.775676250457764, Classifier Loss 0.4263345003128052, Total Loss 207.61424255371094\n",
      "11: Encoding Loss 19.18125343322754, Transition Loss 5.910721778869629, Classifier Loss 0.44343018531799316, Total Loss 198.97518920898438\n",
      "12: Encoding Loss 21.049650192260742, Transition Loss 2.835873603820801, Classifier Loss 0.39693325757980347, Total Loss 208.65769958496094\n",
      "12: Encoding Loss 20.510587692260742, Transition Loss 2.907686948776245, Classifier Loss 0.41940754652023315, Total Loss 206.60699462890625\n",
      "12: Encoding Loss 21.080705642700195, Transition Loss 2.967240571975708, Classifier Loss 0.4224035441875458, Total Loss 211.4794464111328\n",
      "12: Encoding Loss 20.278697967529297, Transition Loss 2.930194616317749, Classifier Loss 0.4537197947502136, Total Loss 208.1876220703125\n",
      "12: Encoding Loss 21.46571159362793, Transition Loss 2.4943196773529053, Classifier Loss 0.4204961657524109, Total Loss 214.27418518066406\n",
      "12: Encoding Loss 19.68132781982422, Transition Loss 4.009698390960693, Classifier Loss 0.4513179063796997, Total Loss 203.3843536376953\n",
      "12: Encoding Loss 20.682466506958008, Transition Loss 2.3483405113220215, Classifier Loss 0.41267555952072144, Total Loss 207.19696044921875\n",
      "12: Encoding Loss 21.60218048095703, Transition Loss 2.5523316860198975, Classifier Loss 0.4205401539802551, Total Loss 215.38192749023438\n",
      "12: Encoding Loss 20.567516326904297, Transition Loss 2.9876902103424072, Classifier Loss 0.45383286476135254, Total Loss 210.52096557617188\n",
      "12: Encoding Loss 21.83817481994629, Transition Loss 2.9602439403533936, Classifier Loss 0.4392549693584442, Total Loss 219.2229461669922\n",
      "12: Encoding Loss 20.967424392700195, Transition Loss 2.6958069801330566, Classifier Loss 0.3935602307319641, Total Loss 207.6345672607422\n",
      "12: Encoding Loss 20.58232879638672, Transition Loss 3.067511796951294, Classifier Loss 0.4090539813041687, Total Loss 206.17752075195312\n",
      "12: Encoding Loss 19.716005325317383, Transition Loss 3.0528383255004883, Classifier Loss 0.401826411485672, Total Loss 198.521240234375\n",
      "12: Encoding Loss 20.175670623779297, Transition Loss 3.2858965396881104, Classifier Loss 0.4267841577529907, Total Loss 204.74098205566406\n",
      "12: Encoding Loss 19.989103317260742, Transition Loss 3.4618654251098633, Classifier Loss 0.41268816590309143, Total Loss 201.87400817871094\n",
      "12: Encoding Loss 20.194765090942383, Transition Loss 3.7611477375030518, Classifier Loss 0.4158173203468323, Total Loss 203.89208984375\n",
      "12: Encoding Loss 20.215028762817383, Transition Loss 3.4012694358825684, Classifier Loss 0.4275215268135071, Total Loss 205.1526336669922\n",
      "12: Encoding Loss 19.973033905029297, Transition Loss 3.393061876296997, Classifier Loss 0.4305773377418518, Total Loss 203.52064514160156\n",
      "12: Encoding Loss 20.556903839111328, Transition Loss 3.513380527496338, Classifier Loss 0.4029315710067749, Total Loss 205.45106506347656\n",
      "12: Encoding Loss 21.301795959472656, Transition Loss 3.1699421405792236, Classifier Loss 0.37634915113449097, Total Loss 208.6832733154297\n",
      "12: Encoding Loss 20.260698318481445, Transition Loss 4.349621295928955, Classifier Loss 0.39401981234550476, Total Loss 202.35748291015625\n",
      "12: Encoding Loss 20.106752395629883, Transition Loss 2.8056421279907227, Classifier Loss 0.44887542724609375, Total Loss 206.3026885986328\n",
      "12: Encoding Loss 19.8137149810791, Transition Loss 5.475259780883789, Classifier Loss 0.44495582580566406, Total Loss 204.10035705566406\n",
      "12: Encoding Loss 19.48859405517578, Transition Loss 2.9533185958862305, Classifier Loss 0.4006020426750183, Total Loss 196.55963134765625\n",
      "12: Encoding Loss 20.335664749145508, Transition Loss 2.590071201324463, Classifier Loss 0.39349013566970825, Total Loss 202.55235290527344\n",
      "12: Encoding Loss 20.906280517578125, Transition Loss 3.3150882720947266, Classifier Loss 0.40600764751434326, Total Loss 208.5140380859375\n",
      "12: Encoding Loss 20.637819290161133, Transition Loss 3.5519535541534424, Classifier Loss 0.4088764786720276, Total Loss 206.70059204101562\n",
      "12: Encoding Loss 19.870861053466797, Transition Loss 3.752920150756836, Classifier Loss 0.41509875655174255, Total Loss 201.22735595703125\n",
      "12: Encoding Loss 19.964073181152344, Transition Loss 3.578549861907959, Classifier Loss 0.38670140504837036, Total Loss 199.0984344482422\n",
      "12: Encoding Loss 19.97983169555664, Transition Loss 4.469095230102539, Classifier Loss 0.4404425323009491, Total Loss 204.77671813964844\n",
      "12: Encoding Loss 20.41718101501465, Transition Loss 3.5522544384002686, Classifier Loss 0.38916242122650146, Total Loss 202.96414184570312\n",
      "12: Encoding Loss 19.351682662963867, Transition Loss 4.516544342041016, Classifier Loss 0.43305733799934387, Total Loss 199.0225067138672\n",
      "12: Encoding Loss 20.939809799194336, Transition Loss 3.6646547317504883, Classifier Loss 0.43164804577827454, Total Loss 211.4162139892578\n",
      "12: Encoding Loss 19.569480895996094, Transition Loss 2.4232711791992188, Classifier Loss 0.40683263540267944, Total Loss 197.7237548828125\n",
      "12: Encoding Loss 19.94272804260254, Transition Loss 3.9184582233428955, Classifier Loss 0.47171854972839355, Total Loss 207.49737548828125\n",
      "12: Encoding Loss 19.21550750732422, Transition Loss 2.7598490715026855, Classifier Loss 0.4247252345085144, Total Loss 196.74855041503906\n",
      "12: Encoding Loss 20.41337013244629, Transition Loss 3.6690118312835693, Classifier Loss 0.37752482295036316, Total Loss 201.79324340820312\n",
      "12: Encoding Loss 20.453596115112305, Transition Loss 4.774253845214844, Classifier Loss 0.43241965770721436, Total Loss 207.82557678222656\n",
      "12: Encoding Loss 20.592363357543945, Transition Loss 2.9473655223846436, Classifier Loss 0.3957782983779907, Total Loss 204.90621948242188\n",
      "12: Encoding Loss 20.098655700683594, Transition Loss 3.982017993927002, Classifier Loss 0.4074970483779907, Total Loss 202.33535766601562\n",
      "12: Encoding Loss 20.045475006103516, Transition Loss 4.311960220336914, Classifier Loss 0.46637415885925293, Total Loss 207.86361694335938\n",
      "12: Encoding Loss 20.030481338500977, Transition Loss 2.837411880493164, Classifier Loss 0.42131975293159485, Total Loss 202.9433135986328\n",
      "12: Encoding Loss 19.79510498046875, Transition Loss 2.649610996246338, Classifier Loss 0.42501896619796753, Total Loss 201.3926544189453\n",
      "12: Encoding Loss 19.67518424987793, Transition Loss 2.1865806579589844, Classifier Loss 0.4249812960624695, Total Loss 200.3369140625\n",
      "12: Encoding Loss 21.015329360961914, Transition Loss 2.488126754760742, Classifier Loss 0.38985633850097656, Total Loss 207.60589599609375\n",
      "12: Encoding Loss 20.950401306152344, Transition Loss 3.4078598022460938, Classifier Loss 0.4289420247077942, Total Loss 211.17898559570312\n",
      "12: Encoding Loss 20.37696647644043, Transition Loss 4.033935070037842, Classifier Loss 0.4424273669719696, Total Loss 208.0652618408203\n",
      "12: Encoding Loss 21.008323669433594, Transition Loss 2.9190480709075928, Classifier Loss 0.43337079882621765, Total Loss 211.98748779296875\n",
      "12: Encoding Loss 20.80929183959961, Transition Loss 3.0660388469696045, Classifier Loss 0.40540000796318054, Total Loss 207.62754821777344\n",
      "12: Encoding Loss 20.81224822998047, Transition Loss 3.3593909740448, Classifier Loss 0.42098528146743774, Total Loss 209.2683868408203\n",
      "12: Encoding Loss 19.829679489135742, Transition Loss 2.4304275512695312, Classifier Loss 0.3847290277481079, Total Loss 197.59642028808594\n",
      "12: Encoding Loss 20.031652450561523, Transition Loss 2.6294937133789062, Classifier Loss 0.38186657428741455, Total Loss 198.9657745361328\n",
      "12: Encoding Loss 19.38181495666504, Transition Loss 2.4643983840942383, Classifier Loss 0.3887482285499573, Total Loss 194.42222595214844\n",
      "12: Encoding Loss 20.082576751708984, Transition Loss 3.777609348297119, Classifier Loss 0.4599875807762146, Total Loss 207.4149169921875\n",
      "12: Encoding Loss 19.87523651123047, Transition Loss 3.854144811630249, Classifier Loss 0.43718087673187256, Total Loss 203.4907989501953\n",
      "12: Encoding Loss 19.150880813598633, Transition Loss 3.6938838958740234, Classifier Loss 0.41082048416137695, Total Loss 195.02786254882812\n",
      "12: Encoding Loss 20.463499069213867, Transition Loss 3.2345032691955566, Classifier Loss 0.3992222547531128, Total Loss 204.27711486816406\n",
      "12: Encoding Loss 20.00637435913086, Transition Loss 3.341075897216797, Classifier Loss 0.41553768515586853, Total Loss 202.2729949951172\n",
      "12: Encoding Loss 19.922954559326172, Transition Loss 3.2937510013580322, Classifier Loss 0.4200546145439148, Total Loss 202.04786682128906\n",
      "12: Encoding Loss 19.61747932434082, Transition Loss 2.9729959964752197, Classifier Loss 0.40683865547180176, Total Loss 198.2183074951172\n",
      "12: Encoding Loss 20.637189865112305, Transition Loss 3.741952657699585, Classifier Loss 0.3981062173843384, Total Loss 205.6565399169922\n",
      "12: Encoding Loss 21.44158935546875, Transition Loss 2.7899296283721924, Classifier Loss 0.40815824270248413, Total Loss 212.90652465820312\n",
      "12: Encoding Loss 20.430736541748047, Transition Loss 2.891071319580078, Classifier Loss 0.39599037170410156, Total Loss 203.6231689453125\n",
      "12: Encoding Loss 20.121061325073242, Transition Loss 2.8742592334747314, Classifier Loss 0.43394675850868225, Total Loss 204.93800354003906\n",
      "12: Encoding Loss 19.66948699951172, Transition Loss 3.467284679412842, Classifier Loss 0.4247845411300659, Total Loss 200.52780151367188\n",
      "12: Encoding Loss 19.328643798828125, Transition Loss 3.4082987308502197, Classifier Loss 0.4112094044685364, Total Loss 196.43174743652344\n",
      "12: Encoding Loss 20.29762077331543, Transition Loss 3.068526029586792, Classifier Loss 0.4122487008571625, Total Loss 204.21954345703125\n",
      "12: Encoding Loss 21.80133628845215, Transition Loss 2.857440948486328, Classifier Loss 0.4047190845012665, Total Loss 215.45408630371094\n",
      "12: Encoding Loss 21.365808486938477, Transition Loss 3.7823450565338135, Classifier Loss 0.43258315324783325, Total Loss 214.94125366210938\n",
      "12: Encoding Loss 20.31452178955078, Transition Loss 3.302522659301758, Classifier Loss 0.41519543528556824, Total Loss 204.69622802734375\n",
      "12: Encoding Loss 21.04796600341797, Transition Loss 2.775012493133545, Classifier Loss 0.40405362844467163, Total Loss 209.34410095214844\n",
      "12: Encoding Loss 19.7222843170166, Transition Loss 3.5843446254730225, Classifier Loss 0.3913370966911316, Total Loss 197.6288604736328\n",
      "12: Encoding Loss 19.5599308013916, Transition Loss 3.7915005683898926, Classifier Loss 0.4139373302459717, Total Loss 198.63148498535156\n",
      "12: Encoding Loss 20.65355682373047, Transition Loss 3.5953280925750732, Classifier Loss 0.38666605949401855, Total Loss 204.6141357421875\n",
      "12: Encoding Loss 19.789928436279297, Transition Loss 4.981541633605957, Classifier Loss 0.4627121090888977, Total Loss 205.5869598388672\n",
      "12: Encoding Loss 20.640254974365234, Transition Loss 3.304316520690918, Classifier Loss 0.41537654399871826, Total Loss 207.32057189941406\n",
      "12: Encoding Loss 20.660005569458008, Transition Loss 4.029946327209473, Classifier Loss 0.3749162256717682, Total Loss 203.57765197753906\n",
      "12: Encoding Loss 20.39886474609375, Transition Loss 3.9711217880249023, Classifier Loss 0.41099023818969727, Total Loss 205.08416748046875\n",
      "12: Encoding Loss 19.944557189941406, Transition Loss 3.680281400680542, Classifier Loss 0.40445956587791443, Total Loss 200.73846435546875\n",
      "12: Encoding Loss 20.7175350189209, Transition Loss 2.808708906173706, Classifier Loss 0.39435267448425293, Total Loss 205.73728942871094\n",
      "12: Encoding Loss 19.441404342651367, Transition Loss 3.1988391876220703, Classifier Loss 0.42158937454223633, Total Loss 198.32994079589844\n",
      "12: Encoding Loss 20.64278793334961, Transition Loss 3.3718643188476562, Classifier Loss 0.4294250011444092, Total Loss 208.7592010498047\n",
      "12: Encoding Loss 20.014963150024414, Transition Loss 3.5085701942443848, Classifier Loss 0.4244060814380646, Total Loss 203.2620391845703\n",
      "12: Encoding Loss 20.390295028686523, Transition Loss 3.886728286743164, Classifier Loss 0.4054146409034729, Total Loss 204.441162109375\n",
      "12: Encoding Loss 20.98196029663086, Transition Loss 2.482680082321167, Classifier Loss 0.4154021739959717, Total Loss 209.8924560546875\n",
      "12: Encoding Loss 19.281217575073242, Transition Loss 3.543804407119751, Classifier Loss 0.38648557662963867, Total Loss 193.6070556640625\n",
      "12: Encoding Loss 20.22578239440918, Transition Loss 2.8873469829559326, Classifier Loss 0.4255013167858124, Total Loss 204.93385314941406\n",
      "12: Encoding Loss 20.432647705078125, Transition Loss 4.329733848571777, Classifier Loss 0.4147060513496399, Total Loss 205.79774475097656\n",
      "12: Encoding Loss 20.399534225463867, Transition Loss 3.2614104747772217, Classifier Loss 0.4265686273574829, Total Loss 206.5054168701172\n",
      "12: Encoding Loss 20.64519500732422, Transition Loss 2.750070095062256, Classifier Loss 0.4414637088775635, Total Loss 209.85794067382812\n",
      "12: Encoding Loss 20.35082244873047, Transition Loss 2.7702527046203613, Classifier Loss 0.37904202938079834, Total Loss 201.26483154296875\n",
      "12: Encoding Loss 20.538917541503906, Transition Loss 3.972820997238159, Classifier Loss 0.45089685916900635, Total Loss 210.1956024169922\n",
      "12: Encoding Loss 20.123043060302734, Transition Loss 3.5144920349121094, Classifier Loss 0.44043147563934326, Total Loss 205.73040771484375\n",
      "12: Encoding Loss 21.078929901123047, Transition Loss 2.837186098098755, Classifier Loss 0.39578360319137573, Total Loss 208.77725219726562\n",
      "12: Encoding Loss 20.635114669799805, Transition Loss 2.992811679840088, Classifier Loss 0.4128701090812683, Total Loss 206.9664764404297\n",
      "12: Encoding Loss 19.30462646484375, Transition Loss 2.6560027599334717, Classifier Loss 0.4046329855918884, Total Loss 195.4315185546875\n",
      "12: Encoding Loss 20.47587776184082, Transition Loss 3.444101572036743, Classifier Loss 0.4340454339981079, Total Loss 207.900390625\n",
      "12: Encoding Loss 20.11476707458496, Transition Loss 2.915325880050659, Classifier Loss 0.4319424629211426, Total Loss 204.69544982910156\n",
      "12: Encoding Loss 19.720123291015625, Transition Loss 4.27092981338501, Classifier Loss 0.42553964257240295, Total Loss 201.16912841796875\n",
      "12: Encoding Loss 21.166259765625, Transition Loss 2.413358211517334, Classifier Loss 0.3929724097251892, Total Loss 209.1099853515625\n",
      "12: Encoding Loss 20.57612419128418, Transition Loss 2.8976798057556152, Classifier Loss 0.41207677125930786, Total Loss 206.3961944580078\n",
      "12: Encoding Loss 20.43140983581543, Transition Loss 2.7488884925842285, Classifier Loss 0.4440639317035675, Total Loss 208.40744018554688\n",
      "12: Encoding Loss 19.64619255065918, Transition Loss 4.053096771240234, Classifier Loss 0.3978363275527954, Total Loss 197.7637939453125\n",
      "12: Encoding Loss 20.56760025024414, Transition Loss 3.9653358459472656, Classifier Loss 0.3872797191143036, Total Loss 204.06182861328125\n",
      "12: Encoding Loss 20.069171905517578, Transition Loss 3.4878721237182617, Classifier Loss 0.3819301724433899, Total Loss 199.4439697265625\n",
      "12: Encoding Loss 18.973581314086914, Transition Loss 4.741394996643066, Classifier Loss 0.38712966442108154, Total Loss 191.44989013671875\n",
      "12: Encoding Loss 20.287639617919922, Transition Loss 3.36398983001709, Classifier Loss 0.4283701181411743, Total Loss 205.81094360351562\n",
      "12: Encoding Loss 20.291303634643555, Transition Loss 3.508725881576538, Classifier Loss 0.449490487575531, Total Loss 207.98123168945312\n",
      "12: Encoding Loss 18.680835723876953, Transition Loss 3.6008100509643555, Classifier Loss 0.4033244848251343, Total Loss 190.49929809570312\n",
      "12: Encoding Loss 20.649991989135742, Transition Loss 3.9955406188964844, Classifier Loss 0.4554637670516968, Total Loss 211.54541015625\n",
      "12: Encoding Loss 20.3791446685791, Transition Loss 3.5125105381011963, Classifier Loss 0.3625151813030243, Total Loss 199.98716735839844\n",
      "12: Encoding Loss 20.78234100341797, Transition Loss 3.438474655151367, Classifier Loss 0.4532775282859802, Total Loss 212.27418518066406\n",
      "12: Encoding Loss 20.15813636779785, Transition Loss 3.4842989444732666, Classifier Loss 0.413144588470459, Total Loss 203.2764129638672\n",
      "12: Encoding Loss 20.711681365966797, Transition Loss 2.840681791305542, Classifier Loss 0.3829388916492462, Total Loss 204.55548095703125\n",
      "12: Encoding Loss 19.880001068115234, Transition Loss 2.774257183074951, Classifier Loss 0.44307926297187805, Total Loss 203.90280151367188\n",
      "12: Encoding Loss 21.63992691040039, Transition Loss 3.8095052242279053, Classifier Loss 0.40884318947792053, Total Loss 214.76564025878906\n",
      "12: Encoding Loss 20.48319435119629, Transition Loss 2.6819162368774414, Classifier Loss 0.4451206922531128, Total Loss 208.91400146484375\n",
      "12: Encoding Loss 19.56609344482422, Transition Loss 3.26342511177063, Classifier Loss 0.401941180229187, Total Loss 197.37554931640625\n",
      "12: Encoding Loss 20.406583786010742, Transition Loss 3.442068338394165, Classifier Loss 0.41750243306159973, Total Loss 205.6913299560547\n",
      "12: Encoding Loss 20.219053268432617, Transition Loss 3.0926010608673096, Classifier Loss 0.4221116304397583, Total Loss 204.5821075439453\n",
      "12: Encoding Loss 19.6318302154541, Transition Loss 2.168656349182129, Classifier Loss 0.3813764452934265, Total Loss 195.6260223388672\n",
      "12: Encoding Loss 19.729801177978516, Transition Loss 2.7034389972686768, Classifier Loss 0.4194444417953491, Total Loss 200.32354736328125\n",
      "12: Encoding Loss 20.2169132232666, Transition Loss 3.789302110671997, Classifier Loss 0.42321664094924927, Total Loss 204.81483459472656\n",
      "12: Encoding Loss 20.37139892578125, Transition Loss 2.1058080196380615, Classifier Loss 0.3981664180755615, Total Loss 203.20899963378906\n",
      "12: Encoding Loss 20.711118698120117, Transition Loss 2.1796822547912598, Classifier Loss 0.4672236442565918, Total Loss 212.84725952148438\n",
      "12: Encoding Loss 19.495473861694336, Transition Loss 3.7721176147460938, Classifier Loss 0.4329633116722107, Total Loss 200.01454162597656\n",
      "12: Encoding Loss 20.116422653198242, Transition Loss 4.182985305786133, Classifier Loss 0.41214627027511597, Total Loss 202.98260498046875\n",
      "12: Encoding Loss 19.56147003173828, Transition Loss 3.5019474029541016, Classifier Loss 0.41688427329063416, Total Loss 198.88058471679688\n",
      "12: Encoding Loss 20.06826400756836, Transition Loss 3.887690782546997, Classifier Loss 0.4086846709251404, Total Loss 202.192138671875\n",
      "12: Encoding Loss 19.535978317260742, Transition Loss 4.796080589294434, Classifier Loss 0.4628489911556244, Total Loss 203.5319366455078\n",
      "12: Encoding Loss 20.338171005249023, Transition Loss 2.9384560585021973, Classifier Loss 0.39278504252433777, Total Loss 202.57156372070312\n",
      "12: Encoding Loss 20.359203338623047, Transition Loss 2.762063503265381, Classifier Loss 0.3758584260940552, Total Loss 201.01190185546875\n",
      "12: Encoding Loss 19.66314697265625, Transition Loss 3.216881275177002, Classifier Loss 0.4066293239593506, Total Loss 198.61148071289062\n",
      "12: Encoding Loss 19.692567825317383, Transition Loss 2.852337121963501, Classifier Loss 0.4059663414955139, Total Loss 198.7076416015625\n",
      "12: Encoding Loss 19.90859031677246, Transition Loss 3.3646554946899414, Classifier Loss 0.4037903845310211, Total Loss 200.32069396972656\n",
      "12: Encoding Loss 19.567338943481445, Transition Loss 3.212919235229492, Classifier Loss 0.419050395488739, Total Loss 199.08633422851562\n",
      "12: Encoding Loss 20.045103073120117, Transition Loss 2.46787166595459, Classifier Loss 0.4286857843399048, Total Loss 203.7229766845703\n",
      "12: Encoding Loss 19.886995315551758, Transition Loss 2.6814088821411133, Classifier Loss 0.41518187522888184, Total Loss 201.1504364013672\n",
      "12: Encoding Loss 18.78546905517578, Transition Loss 3.0498485565185547, Classifier Loss 0.39317142963409424, Total Loss 190.2108612060547\n",
      "12: Encoding Loss 18.773656845092773, Transition Loss 3.490623712539673, Classifier Loss 0.3688860237598419, Total Loss 187.77597045898438\n",
      "12: Encoding Loss 21.80974578857422, Transition Loss 2.43760347366333, Classifier Loss 0.4579119086265564, Total Loss 220.75668334960938\n",
      "12: Encoding Loss 20.00995445251465, Transition Loss 4.255298137664795, Classifier Loss 0.46685346961021423, Total Loss 207.6160430908203\n",
      "12: Encoding Loss 20.493328094482422, Transition Loss 2.965944290161133, Classifier Loss 0.40225517749786377, Total Loss 204.7653350830078\n",
      "12: Encoding Loss 20.37635040283203, Transition Loss 3.915477752685547, Classifier Loss 0.4245535135269165, Total Loss 206.24925231933594\n",
      "12: Encoding Loss 20.469865798950195, Transition Loss 2.921349048614502, Classifier Loss 0.4132663607597351, Total Loss 205.6698455810547\n",
      "12: Encoding Loss 19.30189323425293, Transition Loss 3.6548380851745605, Classifier Loss 0.40867385268211365, Total Loss 196.0135040283203\n",
      "12: Encoding Loss 21.174692153930664, Transition Loss 1.9287527799606323, Classifier Loss 0.4375094771385193, Total Loss 213.53424072265625\n",
      "12: Encoding Loss 19.459867477416992, Transition Loss 2.051025867462158, Classifier Loss 0.4005700945854187, Total Loss 196.14614868164062\n",
      "12: Encoding Loss 20.151121139526367, Transition Loss 2.819279432296753, Classifier Loss 0.37918636202812195, Total Loss 199.69146728515625\n",
      "12: Encoding Loss 18.877531051635742, Transition Loss 3.3644139766693115, Classifier Loss 0.4219476580619812, Total Loss 193.8878936767578\n",
      "12: Encoding Loss 19.35881805419922, Transition Loss 2.3048810958862305, Classifier Loss 0.40017861127853394, Total Loss 195.34938049316406\n",
      "12: Encoding Loss 20.497400283813477, Transition Loss 4.112942695617676, Classifier Loss 0.4116806983947754, Total Loss 205.96986389160156\n",
      "12: Encoding Loss 20.0739803314209, Transition Loss 2.7293307781219482, Classifier Loss 0.4010034203529358, Total Loss 201.23805236816406\n",
      "12: Encoding Loss 21.366037368774414, Transition Loss 2.8515007495880127, Classifier Loss 0.39523106813430786, Total Loss 211.02169799804688\n",
      "12: Encoding Loss 20.726173400878906, Transition Loss 3.1383793354034424, Classifier Loss 0.39042019844055176, Total Loss 205.4790802001953\n",
      "12: Encoding Loss 20.23927116394043, Transition Loss 4.081328868865967, Classifier Loss 0.4175379276275635, Total Loss 204.48423767089844\n",
      "12: Encoding Loss 19.064762115478516, Transition Loss 5.274967193603516, Classifier Loss 0.4434754252433777, Total Loss 197.92062377929688\n",
      "13: Encoding Loss 20.39472770690918, Transition Loss 2.474003553390503, Classifier Loss 0.42912617325782776, Total Loss 206.5652313232422\n",
      "13: Encoding Loss 20.644319534301758, Transition Loss 2.5976619720458984, Classifier Loss 0.43278589844703674, Total Loss 208.9526824951172\n",
      "13: Encoding Loss 21.519495010375977, Transition Loss 2.638477325439453, Classifier Loss 0.4350208342075348, Total Loss 216.1857452392578\n",
      "13: Encoding Loss 20.2613525390625, Transition Loss 2.6655588150024414, Classifier Loss 0.4239993095397949, Total Loss 205.02386474609375\n",
      "13: Encoding Loss 21.448902130126953, Transition Loss 2.2167115211486816, Classifier Loss 0.4214281141757965, Total Loss 214.1773681640625\n",
      "13: Encoding Loss 19.422740936279297, Transition Loss 3.690291404724121, Classifier Loss 0.4214164614677429, Total Loss 198.2616424560547\n",
      "13: Encoding Loss 20.86103630065918, Transition Loss 2.1522860527038574, Classifier Loss 0.4345252811908722, Total Loss 210.77127075195312\n",
      "13: Encoding Loss 21.681907653808594, Transition Loss 2.255983829498291, Classifier Loss 0.43393734097480774, Total Loss 217.30020141601562\n",
      "13: Encoding Loss 20.44045066833496, Transition Loss 2.7587289810180664, Classifier Loss 0.47921597957611084, Total Loss 211.9969482421875\n",
      "13: Encoding Loss 21.345149993896484, Transition Loss 2.66538667678833, Classifier Loss 0.46088093519210815, Total Loss 217.38238525390625\n",
      "13: Encoding Loss 20.775304794311523, Transition Loss 2.490318536758423, Classifier Loss 0.4190562069416046, Total Loss 208.6061248779297\n",
      "13: Encoding Loss 20.504749298095703, Transition Loss 2.832333564758301, Classifier Loss 0.39607200026512146, Total Loss 204.211669921875\n",
      "13: Encoding Loss 19.86102294921875, Transition Loss 2.8249828815460205, Classifier Loss 0.4092305898666382, Total Loss 200.37625122070312\n",
      "13: Encoding Loss 20.133708953857422, Transition Loss 3.0598301887512207, Classifier Loss 0.4182620048522949, Total Loss 203.5078582763672\n",
      "13: Encoding Loss 19.80418586730957, Transition Loss 3.173473358154297, Classifier Loss 0.4220813512802124, Total Loss 201.27630615234375\n",
      "13: Encoding Loss 20.092939376831055, Transition Loss 3.4018969535827637, Classifier Loss 0.4398132562637329, Total Loss 205.40521240234375\n",
      "13: Encoding Loss 19.959142684936523, Transition Loss 3.1439523696899414, Classifier Loss 0.42525625228881836, Total Loss 202.82754516601562\n",
      "13: Encoding Loss 19.977113723754883, Transition Loss 3.139294385910034, Classifier Loss 0.388913094997406, Total Loss 199.33607482910156\n",
      "13: Encoding Loss 20.40163803100586, Transition Loss 3.14454984664917, Classifier Loss 0.38574618101119995, Total Loss 202.41664123535156\n",
      "13: Encoding Loss 21.120182037353516, Transition Loss 2.8428750038146973, Classifier Loss 0.4161243438720703, Total Loss 211.1424560546875\n",
      "13: Encoding Loss 20.07051658630371, Transition Loss 3.941074848175049, Classifier Loss 0.43039968609809875, Total Loss 204.39230346679688\n",
      "13: Encoding Loss 19.933382034301758, Transition Loss 2.609984874725342, Classifier Loss 0.40357980132102966, Total Loss 200.3470458984375\n",
      "13: Encoding Loss 19.634000778198242, Transition Loss 5.088018417358398, Classifier Loss 0.41703394055366516, Total Loss 199.7930145263672\n",
      "13: Encoding Loss 19.185598373413086, Transition Loss 2.7529008388519287, Classifier Loss 0.37937700748443604, Total Loss 191.9730682373047\n",
      "13: Encoding Loss 20.128841400146484, Transition Loss 2.4545857906341553, Classifier Loss 0.3889727294445038, Total Loss 200.4189453125\n",
      "13: Encoding Loss 20.705747604370117, Transition Loss 3.039374351501465, Classifier Loss 0.38295650482177734, Total Loss 204.5495147705078\n",
      "13: Encoding Loss 20.818302154541016, Transition Loss 3.250901460647583, Classifier Loss 0.4527171850204468, Total Loss 212.4683074951172\n",
      "13: Encoding Loss 20.036394119262695, Transition Loss 3.4077372550964355, Classifier Loss 0.4271515905857086, Total Loss 203.6878662109375\n",
      "13: Encoding Loss 19.869197845458984, Transition Loss 3.34669828414917, Classifier Loss 0.4068026542663574, Total Loss 200.30320739746094\n",
      "13: Encoding Loss 19.928884506225586, Transition Loss 4.175991058349609, Classifier Loss 0.4424310028553009, Total Loss 204.50938415527344\n",
      "13: Encoding Loss 19.975955963134766, Transition Loss 3.3107693195343018, Classifier Loss 0.40828177332878113, Total Loss 201.29798889160156\n",
      "13: Encoding Loss 19.16999053955078, Transition Loss 4.221282005310059, Classifier Loss 0.3905397653579712, Total Loss 193.25816345214844\n",
      "13: Encoding Loss 20.6844539642334, Transition Loss 3.4322152137756348, Classifier Loss 0.4273270070552826, Total Loss 208.894775390625\n",
      "13: Encoding Loss 19.813535690307617, Transition Loss 2.3129935264587402, Classifier Loss 0.41515764594078064, Total Loss 200.4866485595703\n",
      "13: Encoding Loss 19.655248641967773, Transition Loss 3.6583054065704346, Classifier Loss 0.4512249827384949, Total Loss 203.0961456298828\n",
      "13: Encoding Loss 19.438350677490234, Transition Loss 2.632781505584717, Classifier Loss 0.42623090744018555, Total Loss 198.65646362304688\n",
      "13: Encoding Loss 20.43090057373047, Transition Loss 3.4967386722564697, Classifier Loss 0.4164225459098816, Total Loss 205.78880310058594\n",
      "13: Encoding Loss 20.309040069580078, Transition Loss 4.565901279449463, Classifier Loss 0.4390963315963745, Total Loss 207.29513549804688\n",
      "13: Encoding Loss 20.442668914794922, Transition Loss 2.799031972885132, Classifier Loss 0.3990658223628998, Total Loss 204.00775146484375\n",
      "13: Encoding Loss 19.806324005126953, Transition Loss 3.744732141494751, Classifier Loss 0.4056209921836853, Total Loss 199.7616424560547\n",
      "13: Encoding Loss 19.710847854614258, Transition Loss 4.126697063446045, Classifier Loss 0.42316126823425293, Total Loss 200.8282470703125\n",
      "13: Encoding Loss 19.698230743408203, Transition Loss 2.739192485809326, Classifier Loss 0.4098227620124817, Total Loss 199.11595153808594\n",
      "13: Encoding Loss 19.85162353515625, Transition Loss 2.578444004058838, Classifier Loss 0.38611847162246704, Total Loss 197.94052124023438\n",
      "13: Encoding Loss 19.571544647216797, Transition Loss 2.1251332759857178, Classifier Loss 0.41871750354766846, Total Loss 198.86915588378906\n",
      "13: Encoding Loss 20.95116424560547, Transition Loss 2.4399266242980957, Classifier Loss 0.4156076908111572, Total Loss 209.65806579589844\n",
      "13: Encoding Loss 20.788991928100586, Transition Loss 3.30472993850708, Classifier Loss 0.4395234286785126, Total Loss 210.92523193359375\n",
      "13: Encoding Loss 20.211345672607422, Transition Loss 3.9172074794769287, Classifier Loss 0.4012189507484436, Total Loss 202.59613037109375\n",
      "13: Encoding Loss 20.4392147064209, Transition Loss 2.760190010070801, Classifier Loss 0.4029565751552582, Total Loss 204.3614044189453\n",
      "13: Encoding Loss 20.53398323059082, Transition Loss 2.896111249923706, Classifier Loss 0.41548025608062744, Total Loss 206.39910888671875\n",
      "13: Encoding Loss 20.402647018432617, Transition Loss 3.133441925048828, Classifier Loss 0.4159294366836548, Total Loss 205.44081115722656\n",
      "13: Encoding Loss 19.477872848510742, Transition Loss 2.2814462184906006, Classifier Loss 0.3847144842147827, Total Loss 194.75071716308594\n",
      "13: Encoding Loss 19.67502212524414, Transition Loss 2.4874351024627686, Classifier Loss 0.40663981437683105, Total Loss 198.5616455078125\n",
      "13: Encoding Loss 19.110010147094727, Transition Loss 2.3736371994018555, Classifier Loss 0.37724587321281433, Total Loss 191.07940673828125\n",
      "13: Encoding Loss 19.90123176574707, Transition Loss 3.522475481033325, Classifier Loss 0.387637197971344, Total Loss 198.67807006835938\n",
      "13: Encoding Loss 19.551877975463867, Transition Loss 3.626462936401367, Classifier Loss 0.43687087297439575, Total Loss 200.82740783691406\n",
      "13: Encoding Loss 18.89105796813965, Transition Loss 3.552117347717285, Classifier Loss 0.4408831000328064, Total Loss 195.92718505859375\n",
      "13: Encoding Loss 20.06810760498047, Transition Loss 3.0593838691711426, Classifier Loss 0.41527801752090454, Total Loss 202.68453979492188\n",
      "13: Encoding Loss 19.606233596801758, Transition Loss 3.2619991302490234, Classifier Loss 0.3800692856311798, Total Loss 195.5092010498047\n",
      "13: Encoding Loss 19.75012969970703, Transition Loss 3.1115827560424805, Classifier Loss 0.39758968353271484, Total Loss 198.38232421875\n",
      "13: Encoding Loss 19.431116104125977, Transition Loss 2.870965003967285, Classifier Loss 0.43552446365356445, Total Loss 199.5755615234375\n",
      "13: Encoding Loss 20.44951629638672, Transition Loss 3.43371844291687, Classifier Loss 0.41536134481430054, Total Loss 205.81900024414062\n",
      "13: Encoding Loss 21.099878311157227, Transition Loss 2.64559006690979, Classifier Loss 0.41241344809532166, Total Loss 210.56948852539062\n",
      "13: Encoding Loss 20.461212158203125, Transition Loss 2.783254861831665, Classifier Loss 0.38411077857017517, Total Loss 202.6574249267578\n",
      "13: Encoding Loss 19.959962844848633, Transition Loss 2.790710926055908, Classifier Loss 0.4018176198005676, Total Loss 200.4196014404297\n",
      "13: Encoding Loss 19.353147506713867, Transition Loss 3.237882614135742, Classifier Loss 0.3907086253166199, Total Loss 194.5436248779297\n",
      "13: Encoding Loss 19.09648895263672, Transition Loss 3.1979286670684814, Classifier Loss 0.3962753713130951, Total Loss 193.03903198242188\n",
      "13: Encoding Loss 20.04874038696289, Transition Loss 2.8172926902770996, Classifier Loss 0.3732415735721588, Total Loss 198.2775421142578\n",
      "13: Encoding Loss 21.14150619506836, Transition Loss 2.55625581741333, Classifier Loss 0.385459303855896, Total Loss 208.18923950195312\n",
      "13: Encoding Loss 20.990318298339844, Transition Loss 3.292585611343384, Classifier Loss 0.4013192057609558, Total Loss 208.71299743652344\n",
      "13: Encoding Loss 20.200542449951172, Transition Loss 2.980849266052246, Classifier Loss 0.4073810577392578, Total Loss 202.9386444091797\n",
      "13: Encoding Loss 20.580890655517578, Transition Loss 2.410498857498169, Classifier Loss 0.42677074670791626, Total Loss 207.80630493164062\n",
      "13: Encoding Loss 19.176713943481445, Transition Loss 3.2241601943969727, Classifier Loss 0.4052893817424774, Total Loss 194.58749389648438\n",
      "13: Encoding Loss 19.298097610473633, Transition Loss 3.3486008644104004, Classifier Loss 0.417271226644516, Total Loss 196.78163146972656\n",
      "13: Encoding Loss 20.61842918395996, Transition Loss 3.125812530517578, Classifier Loss 0.3745846748352051, Total Loss 203.03106689453125\n",
      "13: Encoding Loss 19.675073623657227, Transition Loss 4.360048294067383, Classifier Loss 0.4378335475921631, Total Loss 202.0559539794922\n",
      "13: Encoding Loss 20.874242782592773, Transition Loss 2.9697790145874023, Classifier Loss 0.40070804953575134, Total Loss 207.65869140625\n",
      "13: Encoding Loss 20.39408302307129, Transition Loss 3.5224342346191406, Classifier Loss 0.38625016808509827, Total Loss 202.48216247558594\n",
      "13: Encoding Loss 20.193273544311523, Transition Loss 3.566535711288452, Classifier Loss 0.40001988410949707, Total Loss 202.261474609375\n",
      "13: Encoding Loss 19.441139221191406, Transition Loss 3.2249841690063477, Classifier Loss 0.4159817397594452, Total Loss 197.7722930908203\n",
      "13: Encoding Loss 20.809961318969727, Transition Loss 2.532633066177368, Classifier Loss 0.4105307459831238, Total Loss 208.03929138183594\n",
      "13: Encoding Loss 19.71306610107422, Transition Loss 2.863882064819336, Classifier Loss 0.3686489760875702, Total Loss 195.14219665527344\n",
      "13: Encoding Loss 20.234312057495117, Transition Loss 3.0335967540740967, Classifier Loss 0.40491998195648193, Total Loss 202.97320556640625\n",
      "13: Encoding Loss 19.667749404907227, Transition Loss 3.0444693565368652, Classifier Loss 0.42032334208488464, Total Loss 199.98321533203125\n",
      "13: Encoding Loss 20.721454620361328, Transition Loss 3.355107307434082, Classifier Loss 0.40943312644958496, Total Loss 207.38597106933594\n",
      "13: Encoding Loss 20.680469512939453, Transition Loss 2.1488406658172607, Classifier Loss 0.4107397198677063, Total Loss 206.94749450683594\n",
      "13: Encoding Loss 19.136981964111328, Transition Loss 3.1437416076660156, Classifier Loss 0.38998061418533325, Total Loss 192.72267150878906\n",
      "13: Encoding Loss 20.144542694091797, Transition Loss 2.564755916595459, Classifier Loss 0.4270511865615845, Total Loss 204.3744354248047\n",
      "13: Encoding Loss 20.036802291870117, Transition Loss 3.7255289554595947, Classifier Loss 0.43167778849601746, Total Loss 204.20729064941406\n",
      "13: Encoding Loss 20.062162399291992, Transition Loss 2.9260475635528564, Classifier Loss 0.3927351236343384, Total Loss 200.35601806640625\n",
      "13: Encoding Loss 20.273874282836914, Transition Loss 2.4789986610412598, Classifier Loss 0.4070488214492798, Total Loss 203.3916778564453\n",
      "13: Encoding Loss 20.05095100402832, Transition Loss 2.3571529388427734, Classifier Loss 0.3807511329650879, Total Loss 198.95416259765625\n",
      "13: Encoding Loss 20.50678062438965, Transition Loss 3.2974743843078613, Classifier Loss 0.4374188780784607, Total Loss 208.45562744140625\n",
      "13: Encoding Loss 19.66958999633789, Transition Loss 3.031115770339966, Classifier Loss 0.3846304416656494, Total Loss 196.4259796142578\n",
      "13: Encoding Loss 20.919769287109375, Transition Loss 2.3331589698791504, Classifier Loss 0.4031919538974762, Total Loss 208.14398193359375\n",
      "13: Encoding Loss 20.49500846862793, Transition Loss 2.5687315464019775, Classifier Loss 0.3976636230945587, Total Loss 204.24017333984375\n",
      "13: Encoding Loss 19.09769058227539, Transition Loss 2.453723192214966, Classifier Loss 0.39044567942619324, Total Loss 192.31683349609375\n",
      "13: Encoding Loss 20.228763580322266, Transition Loss 2.945380449295044, Classifier Loss 0.4384339451789856, Total Loss 206.26258850097656\n",
      "13: Encoding Loss 20.02385711669922, Transition Loss 2.586760997772217, Classifier Loss 0.43271443247795105, Total Loss 203.97964477539062\n",
      "13: Encoding Loss 19.330810546875, Transition Loss 3.7113423347473145, Classifier Loss 0.4376462697982788, Total Loss 199.1533660888672\n",
      "13: Encoding Loss 21.048255920410156, Transition Loss 2.1242258548736572, Classifier Loss 0.3899916410446167, Total Loss 207.81005859375\n",
      "13: Encoding Loss 20.56647491455078, Transition Loss 2.532248020172119, Classifier Loss 0.40659046173095703, Total Loss 205.69729614257812\n",
      "13: Encoding Loss 20.08603286743164, Transition Loss 2.4100210666656494, Classifier Loss 0.407898485660553, Total Loss 201.9601287841797\n",
      "13: Encoding Loss 19.6673583984375, Transition Loss 3.543560028076172, Classifier Loss 0.4284777045249939, Total Loss 200.89535522460938\n",
      "13: Encoding Loss 20.018693923950195, Transition Loss 3.544527530670166, Classifier Loss 0.4217025339603424, Total Loss 203.02871704101562\n",
      "13: Encoding Loss 19.63205909729004, Transition Loss 3.105283260345459, Classifier Loss 0.38778290152549744, Total Loss 196.45582580566406\n",
      "13: Encoding Loss 19.152515411376953, Transition Loss 4.216830253601074, Classifier Loss 0.3811257481575012, Total Loss 192.1760711669922\n",
      "13: Encoding Loss 19.47726821899414, Transition Loss 3.0044078826904297, Classifier Loss 0.4346871078014374, Total Loss 199.8877410888672\n",
      "13: Encoding Loss 20.510202407836914, Transition Loss 3.143160343170166, Classifier Loss 0.4058948755264282, Total Loss 205.29974365234375\n",
      "13: Encoding Loss 18.951383590698242, Transition Loss 3.271430015563965, Classifier Loss 0.39563000202178955, Total Loss 191.82835388183594\n",
      "13: Encoding Loss 20.243661880493164, Transition Loss 3.610051155090332, Classifier Loss 0.3914588987827301, Total Loss 201.81719970703125\n",
      "13: Encoding Loss 19.861726760864258, Transition Loss 3.086515188217163, Classifier Loss 0.40118011832237244, Total Loss 199.62913513183594\n",
      "13: Encoding Loss 20.521034240722656, Transition Loss 2.928736448287964, Classifier Loss 0.43279871344566345, Total Loss 208.03390502929688\n",
      "13: Encoding Loss 19.704877853393555, Transition Loss 3.061994791030884, Classifier Loss 0.4067084789276123, Total Loss 198.92227172851562\n",
      "13: Encoding Loss 20.562780380249023, Transition Loss 2.4809536933898926, Classifier Loss 0.3605582118034363, Total Loss 201.0542449951172\n",
      "13: Encoding Loss 19.92493438720703, Transition Loss 2.472912549972534, Classifier Loss 0.4787892699241638, Total Loss 207.77297973632812\n",
      "13: Encoding Loss 21.4382266998291, Transition Loss 3.3748626708984375, Classifier Loss 0.42195335030555725, Total Loss 214.37611389160156\n",
      "13: Encoding Loss 20.088232040405273, Transition Loss 2.3412532806396484, Classifier Loss 0.4235391914844513, Total Loss 203.5280303955078\n",
      "13: Encoding Loss 19.222240447998047, Transition Loss 2.9210383892059326, Classifier Loss 0.3876318633556366, Total Loss 193.12533569335938\n",
      "13: Encoding Loss 19.931011199951172, Transition Loss 3.0139987468719482, Classifier Loss 0.41257959604263306, Total Loss 201.30886840820312\n",
      "13: Encoding Loss 20.107280731201172, Transition Loss 2.659493923187256, Classifier Loss 0.42663705348968506, Total Loss 204.0538787841797\n",
      "13: Encoding Loss 19.663434982299805, Transition Loss 1.930253028869629, Classifier Loss 0.3899783492088318, Total Loss 196.6913604736328\n",
      "13: Encoding Loss 19.439311981201172, Transition Loss 2.4481077194213867, Classifier Loss 0.3909480571746826, Total Loss 195.09893798828125\n",
      "13: Encoding Loss 20.17478370666504, Transition Loss 3.2923853397369385, Classifier Loss 0.39318472146987915, Total Loss 201.37521362304688\n",
      "13: Encoding Loss 19.870643615722656, Transition Loss 1.840552806854248, Classifier Loss 0.40071338415145874, Total Loss 199.4045867919922\n",
      "13: Encoding Loss 20.47172737121582, Transition Loss 1.8390053510665894, Classifier Loss 0.42417824268341064, Total Loss 206.5594482421875\n",
      "13: Encoding Loss 19.380016326904297, Transition Loss 3.3148319721221924, Classifier Loss 0.39721009135246277, Total Loss 195.4241180419922\n",
      "13: Encoding Loss 19.912376403808594, Transition Loss 3.682878017425537, Classifier Loss 0.41408571600914, Total Loss 201.44415283203125\n",
      "13: Encoding Loss 19.623388290405273, Transition Loss 3.1443228721618652, Classifier Loss 0.4417140781879425, Total Loss 201.78736877441406\n",
      "13: Encoding Loss 19.829065322875977, Transition Loss 3.4745068550109863, Classifier Loss 0.4411800801753998, Total Loss 203.4454345703125\n",
      "13: Encoding Loss 19.54270362854004, Transition Loss 4.285861968994141, Classifier Loss 0.44363269209861755, Total Loss 201.56207275390625\n",
      "13: Encoding Loss 20.393463134765625, Transition Loss 2.6408274173736572, Classifier Loss 0.38635915517807007, Total Loss 202.31178283691406\n",
      "13: Encoding Loss 20.490385055541992, Transition Loss 2.4706649780273438, Classifier Loss 0.4011693298816681, Total Loss 204.53414916992188\n",
      "13: Encoding Loss 19.28874969482422, Transition Loss 2.9883859157562256, Classifier Loss 0.38138216733932495, Total Loss 193.04588317871094\n",
      "13: Encoding Loss 19.600507736206055, Transition Loss 2.5936343669891357, Classifier Loss 0.41284894943237305, Total Loss 198.60768127441406\n",
      "13: Encoding Loss 19.75724220275879, Transition Loss 3.0662436485290527, Classifier Loss 0.38635483384132385, Total Loss 197.30667114257812\n",
      "13: Encoding Loss 19.597063064575195, Transition Loss 2.841670274734497, Classifier Loss 0.42154619097709656, Total Loss 199.49945068359375\n",
      "13: Encoding Loss 19.965343475341797, Transition Loss 2.253983736038208, Classifier Loss 0.40575695037841797, Total Loss 200.74925231933594\n",
      "13: Encoding Loss 19.9229793548584, Transition Loss 2.3823108673095703, Classifier Loss 0.3978530466556549, Total Loss 199.64559936523438\n",
      "13: Encoding Loss 18.83955955505371, Transition Loss 2.8580007553100586, Classifier Loss 0.3794054388999939, Total Loss 189.22860717773438\n",
      "13: Encoding Loss 18.659053802490234, Transition Loss 3.157491445541382, Classifier Loss 0.36835259199142456, Total Loss 186.73919677734375\n",
      "13: Encoding Loss 21.57070541381836, Transition Loss 2.2283108234405518, Classifier Loss 0.43978679180145264, Total Loss 216.99000549316406\n",
      "13: Encoding Loss 19.751670837402344, Transition Loss 3.692391872406006, Classifier Loss 0.41482028365135193, Total Loss 200.23387145996094\n",
      "13: Encoding Loss 20.400617599487305, Transition Loss 2.6348695755004883, Classifier Loss 0.41572532057762146, Total Loss 205.304443359375\n",
      "13: Encoding Loss 20.29258918762207, Transition Loss 3.3417818546295166, Classifier Loss 0.4055035412311554, Total Loss 203.55941772460938\n",
      "13: Encoding Loss 20.40806007385254, Transition Loss 2.587904453277588, Classifier Loss 0.4609447419643402, Total Loss 209.87652587890625\n",
      "13: Encoding Loss 19.209156036376953, Transition Loss 3.221792697906494, Classifier Loss 0.40610790252685547, Total Loss 194.92840576171875\n",
      "13: Encoding Loss 21.164714813232422, Transition Loss 1.66489577293396, Classifier Loss 0.42668962478637695, Total Loss 212.31967163085938\n",
      "13: Encoding Loss 19.497844696044922, Transition Loss 1.8562002182006836, Classifier Loss 0.3724989891052246, Total Loss 193.60391235351562\n",
      "13: Encoding Loss 19.846012115478516, Transition Loss 2.559783697128296, Classifier Loss 0.38969969749450684, Total Loss 198.25003051757812\n",
      "13: Encoding Loss 18.706056594848633, Transition Loss 2.998194694519043, Classifier Loss 0.380342960357666, Total Loss 188.28237915039062\n",
      "13: Encoding Loss 19.089092254638672, Transition Loss 2.1501309871673584, Classifier Loss 0.40936917066574097, Total Loss 194.07969665527344\n",
      "13: Encoding Loss 20.55657386779785, Transition Loss 3.5121617317199707, Classifier Loss 0.42932596802711487, Total Loss 208.08763122558594\n",
      "13: Encoding Loss 20.146759033203125, Transition Loss 2.4095826148986816, Classifier Loss 0.39940792322158813, Total Loss 201.59678649902344\n",
      "13: Encoding Loss 21.237451553344727, Transition Loss 2.4334311485290527, Classifier Loss 0.4036366939544678, Total Loss 210.7499542236328\n",
      "13: Encoding Loss 20.857587814331055, Transition Loss 2.710897445678711, Classifier Loss 0.40910398960113525, Total Loss 208.3132781982422\n",
      "13: Encoding Loss 20.20669174194336, Transition Loss 3.635446548461914, Classifier Loss 0.38850778341293335, Total Loss 201.23143005371094\n",
      "13: Encoding Loss 18.9077091217041, Transition Loss 4.661007404327393, Classifier Loss 0.3484182059764862, Total Loss 187.0356903076172\n",
      "14: Encoding Loss 20.739620208740234, Transition Loss 2.186577081680298, Classifier Loss 0.43393996357917786, Total Loss 209.748291015625\n",
      "14: Encoding Loss 20.658775329589844, Transition Loss 2.300266981124878, Classifier Loss 0.426268994808197, Total Loss 208.35714721679688\n",
      "14: Encoding Loss 20.849918365478516, Transition Loss 2.3166961669921875, Classifier Loss 0.40229880809783936, Total Loss 207.49256896972656\n",
      "14: Encoding Loss 19.93136215209961, Transition Loss 2.326669454574585, Classifier Loss 0.4165050983428955, Total Loss 201.56675720214844\n",
      "14: Encoding Loss 20.986722946166992, Transition Loss 1.840375304222107, Classifier Loss 0.43102777004241943, Total Loss 211.3646240234375\n",
      "14: Encoding Loss 19.564374923706055, Transition Loss 3.3664023876190186, Classifier Loss 0.4464583694934845, Total Loss 201.8341064453125\n",
      "14: Encoding Loss 20.556669235229492, Transition Loss 1.8832080364227295, Classifier Loss 0.4474533796310425, Total Loss 209.57534790039062\n",
      "14: Encoding Loss 21.372549057006836, Transition Loss 1.9164056777954102, Classifier Loss 0.4000449478626251, Total Loss 211.36817932128906\n",
      "14: Encoding Loss 20.20868492126465, Transition Loss 2.492581605911255, Classifier Loss 0.45458754897117615, Total Loss 207.6267547607422\n",
      "14: Encoding Loss 21.17462921142578, Transition Loss 2.202986478805542, Classifier Loss 0.4247635006904602, Total Loss 212.31398010253906\n",
      "14: Encoding Loss 19.906810760498047, Transition Loss 2.1729559898376465, Classifier Loss 0.4019618332386017, Total Loss 199.88526916503906\n",
      "14: Encoding Loss 20.32061195373535, Transition Loss 2.3934569358825684, Classifier Loss 0.39180561900138855, Total Loss 202.22415161132812\n",
      "14: Encoding Loss 19.606569290161133, Transition Loss 2.4343795776367188, Classifier Loss 0.4333455562591553, Total Loss 200.67398071289062\n",
      "14: Encoding Loss 20.019702911376953, Transition Loss 2.7382640838623047, Classifier Loss 0.39775213599205017, Total Loss 200.48048400878906\n",
      "14: Encoding Loss 19.542787551879883, Transition Loss 2.8548331260681152, Classifier Loss 0.478027880191803, Total Loss 204.71604919433594\n",
      "14: Encoding Loss 19.9073543548584, Transition Loss 2.9587841033935547, Classifier Loss 0.4181244373321533, Total Loss 201.66302490234375\n",
      "14: Encoding Loss 19.985469818115234, Transition Loss 2.815200090408325, Classifier Loss 0.41755443811416626, Total Loss 202.20225524902344\n",
      "14: Encoding Loss 19.84728240966797, Transition Loss 2.759976863861084, Classifier Loss 0.4051302671432495, Total Loss 199.84329223632812\n",
      "14: Encoding Loss 19.994585037231445, Transition Loss 2.728001356124878, Classifier Loss 0.4337005615234375, Total Loss 203.87232971191406\n",
      "14: Encoding Loss 21.14997673034668, Transition Loss 2.4361796379089355, Classifier Loss 0.4040181636810303, Total Loss 210.08885192871094\n",
      "14: Encoding Loss 19.910554885864258, Transition Loss 3.3988306522369385, Classifier Loss 0.44708701968193054, Total Loss 204.6728973388672\n",
      "14: Encoding Loss 19.601957321166992, Transition Loss 2.327573299407959, Classifier Loss 0.3755149841308594, Total Loss 194.83267211914062\n",
      "14: Encoding Loss 19.490713119506836, Transition Loss 4.351441860198975, Classifier Loss 0.40690627694129944, Total Loss 197.4866180419922\n",
      "14: Encoding Loss 19.402326583862305, Transition Loss 2.4115066528320312, Classifier Loss 0.41889500617980957, Total Loss 197.5904083251953\n",
      "14: Encoding Loss 20.085826873779297, Transition Loss 2.244185447692871, Classifier Loss 0.3962640166282654, Total Loss 200.76187133789062\n",
      "14: Encoding Loss 20.906314849853516, Transition Loss 2.663515567779541, Classifier Loss 0.3980647623538971, Total Loss 207.58969116210938\n",
      "14: Encoding Loss 20.592571258544922, Transition Loss 2.798748016357422, Classifier Loss 0.4187779128551483, Total Loss 207.17813110351562\n",
      "14: Encoding Loss 19.61642074584961, Transition Loss 2.930076837539673, Classifier Loss 0.3970010280609131, Total Loss 197.21749877929688\n",
      "14: Encoding Loss 19.50164031982422, Transition Loss 2.801365375518799, Classifier Loss 0.39979153871536255, Total Loss 196.55255126953125\n",
      "14: Encoding Loss 19.399755477905273, Transition Loss 3.549447536468506, Classifier Loss 0.40363651514053345, Total Loss 196.27157592773438\n",
      "14: Encoding Loss 19.737117767333984, Transition Loss 2.813668727874756, Classifier Loss 0.3823532462120056, Total Loss 196.69500732421875\n",
      "14: Encoding Loss 19.068498611450195, Transition Loss 3.458552837371826, Classifier Loss 0.385044664144516, Total Loss 191.74417114257812\n",
      "14: Encoding Loss 20.592388153076172, Transition Loss 2.8397066593170166, Classifier Loss 0.42246270179748535, Total Loss 207.55332946777344\n",
      "14: Encoding Loss 19.458385467529297, Transition Loss 2.2062180042266846, Classifier Loss 0.39754870533943176, Total Loss 195.8632049560547\n",
      "14: Encoding Loss 19.637042999267578, Transition Loss 3.1043686866760254, Classifier Loss 0.4203857481479645, Total Loss 199.75579833984375\n",
      "14: Encoding Loss 18.87236976623535, Transition Loss 2.402594566345215, Classifier Loss 0.4233766198158264, Total Loss 193.79713439941406\n",
      "14: Encoding Loss 20.24675941467285, Transition Loss 2.9124319553375244, Classifier Loss 0.41596969962120056, Total Loss 204.15353393554688\n",
      "14: Encoding Loss 20.01605987548828, Transition Loss 3.7510595321655273, Classifier Loss 0.41060981154441833, Total Loss 201.93966674804688\n",
      "14: Encoding Loss 20.196992874145508, Transition Loss 2.3145201206207275, Classifier Loss 0.39812928438186646, Total Loss 201.85177612304688\n",
      "14: Encoding Loss 19.762184143066406, Transition Loss 3.1851024627685547, Classifier Loss 0.39318084716796875, Total Loss 198.05258178710938\n",
      "14: Encoding Loss 20.0313663482666, Transition Loss 3.503511905670166, Classifier Loss 0.4136051833629608, Total Loss 202.31214904785156\n",
      "14: Encoding Loss 19.554651260375977, Transition Loss 2.381991147994995, Classifier Loss 0.41713887453079224, Total Loss 198.6274871826172\n",
      "14: Encoding Loss 19.592920303344727, Transition Loss 2.236293077468872, Classifier Loss 0.39878880977630615, Total Loss 197.0695037841797\n",
      "14: Encoding Loss 19.238636016845703, Transition Loss 1.9479142427444458, Classifier Loss 0.4231162667274475, Total Loss 196.6103057861328\n",
      "14: Encoding Loss 21.02137565612793, Transition Loss 1.9940779209136963, Classifier Loss 0.413453072309494, Total Loss 209.91513061523438\n",
      "14: Encoding Loss 20.511859893798828, Transition Loss 2.6885387897491455, Classifier Loss 0.42996591329574585, Total Loss 207.62916564941406\n",
      "14: Encoding Loss 20.383094787597656, Transition Loss 3.2370100021362305, Classifier Loss 0.42437028884887695, Total Loss 206.14918518066406\n",
      "14: Encoding Loss 20.495031356811523, Transition Loss 2.2283613681793213, Classifier Loss 0.42635583877563477, Total Loss 207.04150390625\n",
      "14: Encoding Loss 20.21546173095703, Transition Loss 2.435594081878662, Classifier Loss 0.4033711552619934, Total Loss 202.5479278564453\n",
      "14: Encoding Loss 19.950380325317383, Transition Loss 2.677121162414551, Classifier Loss 0.3968896269798279, Total Loss 199.8274383544922\n",
      "14: Encoding Loss 19.685115814208984, Transition Loss 1.994438648223877, Classifier Loss 0.39817333221435547, Total Loss 197.69717407226562\n",
      "14: Encoding Loss 19.54909896850586, Transition Loss 2.141252279281616, Classifier Loss 0.3870168924331665, Total Loss 195.5227508544922\n",
      "14: Encoding Loss 19.03193473815918, Transition Loss 2.144043445587158, Classifier Loss 0.39481034874916077, Total Loss 192.16531372070312\n",
      "14: Encoding Loss 19.696216583251953, Transition Loss 3.0081100463867188, Classifier Loss 0.38795411586761475, Total Loss 196.96676635742188\n",
      "14: Encoding Loss 19.36209487915039, Transition Loss 3.1466259956359863, Classifier Loss 0.38646620512008667, Total Loss 194.17269897460938\n",
      "14: Encoding Loss 18.628864288330078, Transition Loss 3.164754629135132, Classifier Loss 0.4019811749458313, Total Loss 189.8619842529297\n",
      "14: Encoding Loss 19.964305877685547, Transition Loss 2.651550769805908, Classifier Loss 0.4077109694480896, Total Loss 201.015869140625\n",
      "14: Encoding Loss 19.667573928833008, Transition Loss 2.852187156677246, Classifier Loss 0.385115385055542, Total Loss 196.4225616455078\n",
      "14: Encoding Loss 19.80046844482422, Transition Loss 2.7786142826080322, Classifier Loss 0.40166038274765015, Total Loss 199.12551879882812\n",
      "14: Encoding Loss 19.635671615600586, Transition Loss 2.531132221221924, Classifier Loss 0.40789470076560974, Total Loss 198.38107299804688\n",
      "14: Encoding Loss 20.576250076293945, Transition Loss 2.954118490219116, Classifier Loss 0.41362640261650085, Total Loss 206.56346130371094\n",
      "14: Encoding Loss 21.069189071655273, Transition Loss 2.1707518100738525, Classifier Loss 0.4055488705635071, Total Loss 209.54254150390625\n",
      "14: Encoding Loss 20.4927921295166, Transition Loss 2.527592182159424, Classifier Loss 0.4107711613178253, Total Loss 205.5249786376953\n",
      "14: Encoding Loss 20.316553115844727, Transition Loss 2.4917449951171875, Classifier Loss 0.4462462067604065, Total Loss 207.6553955078125\n",
      "14: Encoding Loss 19.04453468322754, Transition Loss 2.90922474861145, Classifier Loss 0.3913467824459076, Total Loss 192.0727996826172\n",
      "14: Encoding Loss 19.1109561920166, Transition Loss 2.859022617340088, Classifier Loss 0.4263530373573303, Total Loss 196.09475708007812\n",
      "14: Encoding Loss 20.07038116455078, Transition Loss 2.609729528427124, Classifier Loss 0.41421324014663696, Total Loss 202.50631713867188\n",
      "14: Encoding Loss 21.230899810791016, Transition Loss 2.2579290866851807, Classifier Loss 0.40082797408103943, Total Loss 210.38157653808594\n",
      "14: Encoding Loss 20.961280822753906, Transition Loss 3.066605567932129, Classifier Loss 0.4079304337501526, Total Loss 209.09661865234375\n",
      "14: Encoding Loss 19.8891544342041, Transition Loss 2.6548309326171875, Classifier Loss 0.41894057393074036, Total Loss 201.5382537841797\n",
      "14: Encoding Loss 20.702791213989258, Transition Loss 2.253174304962158, Classifier Loss 0.4121386706829071, Total Loss 207.28683471679688\n",
      "14: Encoding Loss 19.290931701660156, Transition Loss 2.8379125595092773, Classifier Loss 0.38608089089393616, Total Loss 193.5031280517578\n",
      "14: Encoding Loss 19.465335845947266, Transition Loss 3.138887405395508, Classifier Loss 0.3948444724082947, Total Loss 195.8349151611328\n",
      "14: Encoding Loss 20.36482048034668, Transition Loss 2.6842148303985596, Classifier Loss 0.40588533878326416, Total Loss 204.0439453125\n",
      "14: Encoding Loss 19.47222900390625, Transition Loss 4.046019554138184, Classifier Loss 0.46887990832328796, Total Loss 203.4750213623047\n",
      "14: Encoding Loss 20.49113655090332, Transition Loss 2.5809504985809326, Classifier Loss 0.4216947555541992, Total Loss 206.61476135253906\n",
      "14: Encoding Loss 20.50712013244629, Transition Loss 3.3003556728363037, Classifier Loss 0.4125685393810272, Total Loss 205.973876953125\n",
      "14: Encoding Loss 19.95168113708496, Transition Loss 3.107724189758301, Classifier Loss 0.39926621317863464, Total Loss 200.16162109375\n",
      "14: Encoding Loss 19.5383243560791, Transition Loss 3.1059446334838867, Classifier Loss 0.38352975249290466, Total Loss 195.28076171875\n",
      "14: Encoding Loss 20.654186248779297, Transition Loss 2.0373661518096924, Classifier Loss 0.413361132144928, Total Loss 206.97708129882812\n",
      "14: Encoding Loss 19.60206413269043, Transition Loss 2.8755815029144287, Classifier Loss 0.3788871467113495, Total Loss 195.2803497314453\n",
      "14: Encoding Loss 19.929750442504883, Transition Loss 2.5923831462860107, Classifier Loss 0.4241172671318054, Total Loss 202.3682098388672\n",
      "14: Encoding Loss 19.723966598510742, Transition Loss 2.916964530944824, Classifier Loss 0.39603191614151, Total Loss 197.9783172607422\n",
      "14: Encoding Loss 20.440675735473633, Transition Loss 3.135718584060669, Classifier Loss 0.47593286633491516, Total Loss 211.74583435058594\n",
      "14: Encoding Loss 20.49375343322754, Transition Loss 1.9402542114257812, Classifier Loss 0.42925411462783813, Total Loss 207.26348876953125\n",
      "14: Encoding Loss 19.16493797302246, Transition Loss 3.0098958015441895, Classifier Loss 0.38215386867523193, Total Loss 192.13685607910156\n",
      "14: Encoding Loss 19.957319259643555, Transition Loss 2.3654348850250244, Classifier Loss 0.43699899315834045, Total Loss 203.83154296875\n",
      "14: Encoding Loss 20.287378311157227, Transition Loss 3.6126198768615723, Classifier Loss 0.4477725028991699, Total Loss 207.79879760742188\n",
      "14: Encoding Loss 20.091064453125, Transition Loss 2.6440482139587402, Classifier Loss 0.42826226353645325, Total Loss 204.08355712890625\n",
      "14: Encoding Loss 20.37957191467285, Transition Loss 2.496943950653076, Classifier Loss 0.3954721689224243, Total Loss 203.08319091796875\n",
      "14: Encoding Loss 20.015695571899414, Transition Loss 2.1118650436401367, Classifier Loss 0.355086088180542, Total Loss 196.05654907226562\n",
      "14: Encoding Loss 20.345523834228516, Transition Loss 3.2167887687683105, Classifier Loss 0.4599044620990753, Total Loss 209.3979949951172\n",
      "14: Encoding Loss 20.09539222717285, Transition Loss 2.683675765991211, Classifier Loss 0.37654128670692444, Total Loss 198.95399475097656\n",
      "14: Encoding Loss 20.53403663635254, Transition Loss 2.227273464202881, Classifier Loss 0.4139955937862396, Total Loss 206.11729431152344\n",
      "14: Encoding Loss 20.2996826171875, Transition Loss 2.317898750305176, Classifier Loss 0.39019298553466797, Total Loss 201.88034057617188\n",
      "14: Encoding Loss 19.16629409790039, Transition Loss 2.350839138031006, Classifier Loss 0.4143597185611725, Total Loss 195.2364959716797\n",
      "14: Encoding Loss 20.224151611328125, Transition Loss 2.6787338256835938, Classifier Loss 0.44882726669311523, Total Loss 207.21168518066406\n",
      "14: Encoding Loss 19.750179290771484, Transition Loss 2.4298553466796875, Classifier Loss 0.4157812297344208, Total Loss 200.0655517578125\n",
      "14: Encoding Loss 19.273027420043945, Transition Loss 3.4214251041412354, Classifier Loss 0.4054996073246002, Total Loss 195.41845703125\n",
      "14: Encoding Loss 21.006458282470703, Transition Loss 1.994040846824646, Classifier Loss 0.4156070351600647, Total Loss 210.01116943359375\n",
      "14: Encoding Loss 20.539302825927734, Transition Loss 2.3547403812408447, Classifier Loss 0.41358375549316406, Total Loss 206.14376831054688\n",
      "14: Encoding Loss 20.23052215576172, Transition Loss 2.220060348510742, Classifier Loss 0.44492998719215393, Total Loss 206.78118896484375\n",
      "14: Encoding Loss 19.6469783782959, Transition Loss 3.2994842529296875, Classifier Loss 0.3947010636329651, Total Loss 197.3058319091797\n",
      "14: Encoding Loss 20.241682052612305, Transition Loss 3.340430736541748, Classifier Loss 0.4092620015144348, Total Loss 203.52774047851562\n",
      "14: Encoding Loss 19.809917449951172, Transition Loss 2.9376423358917236, Classifier Loss 0.41419175267219543, Total Loss 200.48605346679688\n",
      "14: Encoding Loss 18.706722259521484, Transition Loss 4.024142742156982, Classifier Loss 0.3958573639392853, Total Loss 190.0443572998047\n",
      "14: Encoding Loss 19.704662322998047, Transition Loss 2.831226348876953, Classifier Loss 0.4353768229484558, Total Loss 201.74122619628906\n",
      "14: Encoding Loss 20.06136131286621, Transition Loss 2.9649486541748047, Classifier Loss 0.4322734475135803, Total Loss 204.31121826171875\n",
      "14: Encoding Loss 18.471698760986328, Transition Loss 3.1447980403900146, Classifier Loss 0.40162086486816406, Total Loss 188.5646209716797\n",
      "14: Encoding Loss 20.298198699951172, Transition Loss 3.359523296356201, Classifier Loss 0.4353230595588684, Total Loss 206.58981323242188\n",
      "14: Encoding Loss 19.88895034790039, Transition Loss 2.9259581565856934, Classifier Loss 0.3901991546154022, Total Loss 198.71670532226562\n",
      "14: Encoding Loss 20.65102195739746, Transition Loss 2.7503795623779297, Classifier Loss 0.39787063002586365, Total Loss 205.54531860351562\n",
      "14: Encoding Loss 19.78683090209961, Transition Loss 2.918152093887329, Classifier Loss 0.3984525799751282, Total Loss 198.7235565185547\n",
      "14: Encoding Loss 20.21519660949707, Transition Loss 2.353740692138672, Classifier Loss 0.3735959529876709, Total Loss 199.5519256591797\n",
      "14: Encoding Loss 19.644615173339844, Transition Loss 2.389098882675171, Classifier Loss 0.4374719262123108, Total Loss 201.38192749023438\n",
      "14: Encoding Loss 21.277673721313477, Transition Loss 3.1256825923919678, Classifier Loss 0.41235750913619995, Total Loss 212.082275390625\n",
      "14: Encoding Loss 20.036497116088867, Transition Loss 2.1701548099517822, Classifier Loss 0.4071720838546753, Total Loss 201.44322204589844\n",
      "14: Encoding Loss 19.35987663269043, Transition Loss 2.7114789485931396, Classifier Loss 0.3901882469654083, Total Loss 194.4401397705078\n",
      "14: Encoding Loss 19.73062515258789, Transition Loss 2.8763134479522705, Classifier Loss 0.38657674193382263, Total Loss 197.0779266357422\n",
      "14: Encoding Loss 19.71091651916504, Transition Loss 2.5282275676727295, Classifier Loss 0.41286027431488037, Total Loss 199.47900390625\n",
      "14: Encoding Loss 19.08592414855957, Transition Loss 1.848756194114685, Classifier Loss 0.37478891015052795, Total Loss 190.53604125976562\n",
      "14: Encoding Loss 19.393596649169922, Transition Loss 2.3735463619232178, Classifier Loss 0.40224599838256836, Total Loss 195.8480987548828\n",
      "14: Encoding Loss 20.15489959716797, Transition Loss 3.1454198360443115, Classifier Loss 0.4061051905155182, Total Loss 202.4788055419922\n",
      "14: Encoding Loss 19.76835060119629, Transition Loss 1.7771726846694946, Classifier Loss 0.4197154641151428, Total Loss 200.47378540039062\n",
      "14: Encoding Loss 20.374013900756836, Transition Loss 1.7921357154846191, Classifier Loss 0.4099150002002716, Total Loss 204.342041015625\n",
      "14: Encoding Loss 19.257402420043945, Transition Loss 3.201406717300415, Classifier Loss 0.41873276233673096, Total Loss 196.57276916503906\n",
      "14: Encoding Loss 19.5753173828125, Transition Loss 3.6017959117889404, Classifier Loss 0.42192769050598145, Total Loss 199.51565551757812\n",
      "14: Encoding Loss 19.361766815185547, Transition Loss 3.0753653049468994, Classifier Loss 0.4260847270488739, Total Loss 198.11769104003906\n",
      "14: Encoding Loss 19.941795349121094, Transition Loss 3.361386299133301, Classifier Loss 0.3934418559074402, Total Loss 199.55081176757812\n",
      "14: Encoding Loss 19.525850296020508, Transition Loss 4.203482151031494, Classifier Loss 0.42545613646507263, Total Loss 199.59310913085938\n",
      "14: Encoding Loss 20.294370651245117, Transition Loss 2.5372469425201416, Classifier Loss 0.40842771530151367, Total Loss 203.70518493652344\n",
      "14: Encoding Loss 20.42636489868164, Transition Loss 2.3819079399108887, Classifier Loss 0.38929909467697144, Total Loss 202.81719970703125\n",
      "14: Encoding Loss 19.310026168823242, Transition Loss 2.919900894165039, Classifier Loss 0.3798779845237732, Total Loss 193.05198669433594\n",
      "14: Encoding Loss 19.597606658935547, Transition Loss 2.538001298904419, Classifier Loss 0.3919197618961334, Total Loss 196.48043823242188\n",
      "14: Encoding Loss 19.561065673828125, Transition Loss 2.9801034927368164, Classifier Loss 0.37859344482421875, Total Loss 194.9438934326172\n",
      "14: Encoding Loss 19.35137939453125, Transition Loss 2.7146003246307373, Classifier Loss 0.40167784690856934, Total Loss 195.52174377441406\n",
      "14: Encoding Loss 19.811092376708984, Transition Loss 2.161160945892334, Classifier Loss 0.39314430952072144, Total Loss 198.2354278564453\n",
      "14: Encoding Loss 19.784982681274414, Transition Loss 2.2721428871154785, Classifier Loss 0.41548866033554077, Total Loss 200.2831573486328\n",
      "14: Encoding Loss 18.598514556884766, Transition Loss 2.778203248977661, Classifier Loss 0.3957720398902893, Total Loss 188.92095947265625\n",
      "14: Encoding Loss 18.557008743286133, Transition Loss 3.021984815597534, Classifier Loss 0.37140268087387085, Total Loss 186.20074462890625\n",
      "14: Encoding Loss 21.254899978637695, Transition Loss 2.167658567428589, Classifier Loss 0.45582470297813416, Total Loss 216.05520629882812\n",
      "14: Encoding Loss 19.852685928344727, Transition Loss 3.6189396381378174, Classifier Loss 0.42962175607681274, Total Loss 202.5074462890625\n",
      "14: Encoding Loss 20.133852005004883, Transition Loss 2.5862796306610107, Classifier Loss 0.40161946415901184, Total Loss 201.75001525878906\n",
      "14: Encoding Loss 20.031795501708984, Transition Loss 3.294872999191284, Classifier Loss 0.4192800521850586, Total Loss 202.84136962890625\n",
      "14: Encoding Loss 20.255578994750977, Transition Loss 2.536396026611328, Classifier Loss 0.43144017457962036, Total Loss 205.6959228515625\n",
      "14: Encoding Loss 19.038867950439453, Transition Loss 3.1705398559570312, Classifier Loss 0.41457849740982056, Total Loss 194.4029083251953\n",
      "14: Encoding Loss 20.40658187866211, Transition Loss 1.6355011463165283, Classifier Loss 0.4116688072681427, Total Loss 204.7466583251953\n",
      "14: Encoding Loss 18.9611873626709, Transition Loss 1.8517799377441406, Classifier Loss 0.3914928734302521, Total Loss 191.2091522216797\n",
      "14: Encoding Loss 19.646684646606445, Transition Loss 2.5241897106170654, Classifier Loss 0.3871329426765442, Total Loss 196.39161682128906\n",
      "14: Encoding Loss 18.835887908935547, Transition Loss 2.996985673904419, Classifier Loss 0.43429145216941833, Total Loss 194.71566772460938\n",
      "14: Encoding Loss 18.949851989746094, Transition Loss 2.1068320274353027, Classifier Loss 0.4027803838253021, Total Loss 192.29823303222656\n",
      "14: Encoding Loss 20.384723663330078, Transition Loss 3.5531373023986816, Classifier Loss 0.3955453038215637, Total Loss 203.34295654296875\n",
      "14: Encoding Loss 19.844806671142578, Transition Loss 2.3710813522338867, Classifier Loss 0.37897348403930664, Total Loss 197.1300048828125\n",
      "14: Encoding Loss 21.31821632385254, Transition Loss 2.4438929557800293, Classifier Loss 0.39059901237487793, Total Loss 210.09442138671875\n",
      "14: Encoding Loss 20.801267623901367, Transition Loss 2.637744426727295, Classifier Loss 0.41026782989501953, Total Loss 207.96446228027344\n",
      "14: Encoding Loss 19.94890594482422, Transition Loss 3.6067442893981934, Classifier Loss 0.41843143105506897, Total Loss 202.15573120117188\n",
      "14: Encoding Loss 18.748435974121094, Transition Loss 4.47795295715332, Classifier Loss 0.3838709592819214, Total Loss 189.27017211914062\n",
      "15: Encoding Loss 20.50236701965332, Transition Loss 2.202493667602539, Classifier Loss 0.4189261198043823, Total Loss 206.35205078125\n",
      "15: Encoding Loss 20.292102813720703, Transition Loss 2.2308902740478516, Classifier Loss 0.42712995409965515, Total Loss 205.49600219726562\n",
      "15: Encoding Loss 20.81779670715332, Transition Loss 2.3316187858581543, Classifier Loss 0.39624500274658203, Total Loss 206.63319396972656\n",
      "15: Encoding Loss 19.48330307006836, Transition Loss 2.273991107940674, Classifier Loss 0.4059484004974365, Total Loss 196.91607666015625\n",
      "15: Encoding Loss 21.002601623535156, Transition Loss 1.8453115224838257, Classifier Loss 0.4249095022678375, Total Loss 210.88082885742188\n",
      "15: Encoding Loss 19.048015594482422, Transition Loss 3.2790746688842773, Classifier Loss 0.421189546585083, Total Loss 195.15890502929688\n",
      "15: Encoding Loss 20.73371124267578, Transition Loss 1.8514151573181152, Classifier Loss 0.41152796149253845, Total Loss 207.3927764892578\n",
      "15: Encoding Loss 21.54421615600586, Transition Loss 1.8940876722335815, Classifier Loss 0.42793792486190796, Total Loss 215.52635192871094\n",
      "15: Encoding Loss 20.05719757080078, Transition Loss 2.4604101181030273, Classifier Loss 0.43542057275772095, Total Loss 204.49171447753906\n",
      "15: Encoding Loss 21.387365341186523, Transition Loss 2.2197377681732178, Classifier Loss 0.42247268557548523, Total Loss 213.79014587402344\n",
      "15: Encoding Loss 20.314794540405273, Transition Loss 2.161875009536743, Classifier Loss 0.42895251512527466, Total Loss 205.84597778320312\n",
      "15: Encoding Loss 20.11402130126953, Transition Loss 2.4475700855255127, Classifier Loss 0.39845505356788635, Total Loss 201.2471923828125\n",
      "15: Encoding Loss 19.696306228637695, Transition Loss 2.426386594772339, Classifier Loss 0.44687411189079285, Total Loss 202.74313354492188\n",
      "15: Encoding Loss 19.96473503112793, Transition Loss 2.734459638595581, Classifier Loss 0.4035113751888275, Total Loss 200.61590576171875\n",
      "15: Encoding Loss 19.1884765625, Transition Loss 2.841172456741333, Classifier Loss 0.4345833659172058, Total Loss 197.53439331054688\n",
      "15: Encoding Loss 20.103679656982422, Transition Loss 2.9195525646209717, Classifier Loss 0.40411895513534546, Total Loss 201.82525634765625\n",
      "15: Encoding Loss 19.631603240966797, Transition Loss 2.7704665660858154, Classifier Loss 0.4285995364189148, Total Loss 200.46688842773438\n",
      "15: Encoding Loss 19.881893157958984, Transition Loss 2.726811408996582, Classifier Loss 0.3915121555328369, Total Loss 198.75173950195312\n",
      "15: Encoding Loss 20.150232315063477, Transition Loss 2.709904432296753, Classifier Loss 0.3759075403213501, Total Loss 199.3345947265625\n",
      "15: Encoding Loss 20.91888999938965, Transition Loss 2.5260446071624756, Classifier Loss 0.41810041666030884, Total Loss 209.66636657714844\n",
      "15: Encoding Loss 19.723482131958008, Transition Loss 3.401446580886841, Classifier Loss 0.4179784655570984, Total Loss 200.2659912109375\n",
      "15: Encoding Loss 19.494464874267578, Transition Loss 2.36500883102417, Classifier Loss 0.38428282737731934, Total Loss 194.8570098876953\n",
      "15: Encoding Loss 19.130847930908203, Transition Loss 4.406383037567139, Classifier Loss 0.4007027745246887, Total Loss 193.9983367919922\n",
      "15: Encoding Loss 19.210975646972656, Transition Loss 2.4867336750030518, Classifier Loss 0.4050484597682953, Total Loss 194.69000244140625\n",
      "15: Encoding Loss 20.038415908813477, Transition Loss 2.2476770877838135, Classifier Loss 0.4108234643936157, Total Loss 201.83921813964844\n",
      "15: Encoding Loss 20.54884147644043, Transition Loss 2.7232701778411865, Classifier Loss 0.42744675278663635, Total Loss 207.6800537109375\n",
      "15: Encoding Loss 20.713680267333984, Transition Loss 2.798983335494995, Classifier Loss 0.41751614212989807, Total Loss 208.0208740234375\n",
      "15: Encoding Loss 19.789791107177734, Transition Loss 3.0095319747924805, Classifier Loss 0.3938034176826477, Total Loss 198.30059814453125\n",
      "15: Encoding Loss 19.384981155395508, Transition Loss 2.9187896251678467, Classifier Loss 0.3883019983768463, Total Loss 194.49380493164062\n",
      "15: Encoding Loss 19.543067932128906, Transition Loss 3.619300603866577, Classifier Loss 0.41115498542785645, Total Loss 198.18389892578125\n",
      "15: Encoding Loss 19.816051483154297, Transition Loss 2.8968100547790527, Classifier Loss 0.39388787746429443, Total Loss 198.49656677246094\n",
      "15: Encoding Loss 18.60158920288086, Transition Loss 3.6019816398620605, Classifier Loss 0.3962482511997223, Total Loss 189.157958984375\n",
      "15: Encoding Loss 20.245182037353516, Transition Loss 2.9118542671203613, Classifier Loss 0.42344480752944946, Total Loss 204.8883056640625\n",
      "15: Encoding Loss 19.224157333374023, Transition Loss 2.180954933166504, Classifier Loss 0.42111968994140625, Total Loss 196.34141540527344\n",
      "15: Encoding Loss 19.418540954589844, Transition Loss 3.0795485973358154, Classifier Loss 0.3772631287574768, Total Loss 193.6905517578125\n",
      "15: Encoding Loss 18.766767501831055, Transition Loss 2.391744375228882, Classifier Loss 0.4296126067638397, Total Loss 193.57374572753906\n",
      "15: Encoding Loss 19.822582244873047, Transition Loss 2.8599634170532227, Classifier Loss 0.40815985202789307, Total Loss 199.96865844726562\n",
      "15: Encoding Loss 20.025615692138672, Transition Loss 3.743316411972046, Classifier Loss 0.43619799613952637, Total Loss 204.57339477539062\n",
      "15: Encoding Loss 20.174551010131836, Transition Loss 2.2985763549804688, Classifier Loss 0.40753689408302307, Total Loss 202.6098175048828\n",
      "15: Encoding Loss 19.819223403930664, Transition Loss 3.1959476470947266, Classifier Loss 0.3953760862350464, Total Loss 198.7305908203125\n",
      "15: Encoding Loss 19.698909759521484, Transition Loss 3.4876322746276855, Classifier Loss 0.43140658736228943, Total Loss 201.42947387695312\n",
      "15: Encoding Loss 19.354839324951172, Transition Loss 2.4222676753997803, Classifier Loss 0.4068859815597534, Total Loss 196.01177978515625\n",
      "15: Encoding Loss 19.20892333984375, Transition Loss 2.255483388900757, Classifier Loss 0.4067457616329193, Total Loss 194.79705810546875\n",
      "15: Encoding Loss 19.035015106201172, Transition Loss 1.9597094058990479, Classifier Loss 0.41391444206237793, Total Loss 194.0635223388672\n",
      "15: Encoding Loss 20.742605209350586, Transition Loss 2.046032190322876, Classifier Loss 0.4354788064956665, Total Loss 209.89793395996094\n",
      "15: Encoding Loss 20.40284538269043, Transition Loss 2.6928508281707764, Classifier Loss 0.42505335807800293, Total Loss 206.2666778564453\n",
      "15: Encoding Loss 19.904996871948242, Transition Loss 3.2823877334594727, Classifier Loss 0.4320438504219055, Total Loss 203.10084533691406\n",
      "15: Encoding Loss 20.881216049194336, Transition Loss 2.22698712348938, Classifier Loss 0.40828150510787964, Total Loss 208.3232879638672\n",
      "15: Encoding Loss 20.35370635986328, Transition Loss 2.52336049079895, Classifier Loss 0.38161855936050415, Total Loss 201.49618530273438\n",
      "15: Encoding Loss 20.15511703491211, Transition Loss 2.690948486328125, Classifier Loss 0.41774845123291016, Total Loss 203.55398559570312\n",
      "15: Encoding Loss 19.33751678466797, Transition Loss 2.022676944732666, Classifier Loss 0.40941640734672546, Total Loss 196.0463104248047\n",
      "15: Encoding Loss 19.423891067504883, Transition Loss 2.1799073219299316, Classifier Loss 0.3994920253753662, Total Loss 195.77630615234375\n",
      "15: Encoding Loss 19.148923873901367, Transition Loss 2.188033103942871, Classifier Loss 0.38052690029144287, Total Loss 191.6816864013672\n",
      "15: Encoding Loss 19.911277770996094, Transition Loss 3.016719102859497, Classifier Loss 0.41906026005744934, Total Loss 201.79959106445312\n",
      "15: Encoding Loss 19.301937103271484, Transition Loss 3.250366687774658, Classifier Loss 0.3840276002883911, Total Loss 193.4683380126953\n",
      "15: Encoding Loss 18.72734832763672, Transition Loss 3.1654107570648193, Classifier Loss 0.40046924352645874, Total Loss 190.49879455566406\n",
      "15: Encoding Loss 19.674428939819336, Transition Loss 2.6398539543151855, Classifier Loss 0.37779298424720764, Total Loss 195.70269775390625\n",
      "15: Encoding Loss 19.52897071838379, Transition Loss 2.825288772583008, Classifier Loss 0.3962632417678833, Total Loss 196.42315673828125\n",
      "15: Encoding Loss 19.546077728271484, Transition Loss 2.73449969291687, Classifier Loss 0.4338133931159973, Total Loss 200.296875\n",
      "15: Encoding Loss 19.351415634155273, Transition Loss 2.5200488567352295, Classifier Loss 0.3973350524902344, Total Loss 195.04884338378906\n",
      "15: Encoding Loss 20.314613342285156, Transition Loss 2.7892374992370605, Classifier Loss 0.39073920249938965, Total Loss 202.14866638183594\n",
      "15: Encoding Loss 21.111848831176758, Transition Loss 2.0988829135894775, Classifier Loss 0.41435563564300537, Total Loss 210.7501220703125\n",
      "15: Encoding Loss 19.961517333984375, Transition Loss 2.4444937705993652, Classifier Loss 0.3882879316806793, Total Loss 199.00982666015625\n",
      "15: Encoding Loss 19.99475860595703, Transition Loss 2.4438719749450684, Classifier Loss 0.4254947304725647, Total Loss 202.99630737304688\n",
      "15: Encoding Loss 19.34142303466797, Transition Loss 2.7882235050201416, Classifier Loss 0.38909274339675903, Total Loss 194.19830322265625\n",
      "15: Encoding Loss 18.84248924255371, Transition Loss 2.818483352661133, Classifier Loss 0.37626680731773376, Total Loss 188.93028259277344\n",
      "15: Encoding Loss 20.12122344970703, Transition Loss 2.501685619354248, Classifier Loss 0.4040815234184265, Total Loss 201.87828063964844\n",
      "15: Encoding Loss 20.84697914123535, Transition Loss 2.196165084838867, Classifier Loss 0.38793081045150757, Total Loss 206.00814819335938\n",
      "15: Encoding Loss 21.00533676147461, Transition Loss 2.894164800643921, Classifier Loss 0.44603002071380615, Total Loss 213.2245330810547\n",
      "15: Encoding Loss 20.079076766967773, Transition Loss 2.6115448474884033, Classifier Loss 0.40073496103286743, Total Loss 201.22842407226562\n",
      "15: Encoding Loss 20.27201271057129, Transition Loss 2.1653122901916504, Classifier Loss 0.3776584565639496, Total Loss 200.37501525878906\n",
      "15: Encoding Loss 19.050762176513672, Transition Loss 2.7673070430755615, Classifier Loss 0.42277607321739197, Total Loss 195.2371826171875\n",
      "15: Encoding Loss 19.32555389404297, Transition Loss 3.0355732440948486, Classifier Loss 0.40904414653778076, Total Loss 196.115966796875\n",
      "15: Encoding Loss 20.1864013671875, Transition Loss 2.639033794403076, Classifier Loss 0.3625335991382599, Total Loss 198.27236938476562\n",
      "15: Encoding Loss 19.338529586791992, Transition Loss 3.921257495880127, Classifier Loss 0.4221883714199066, Total Loss 197.71133422851562\n",
      "15: Encoding Loss 20.24007797241211, Transition Loss 2.555572271347046, Classifier Loss 0.40208640694618225, Total Loss 202.640380859375\n",
      "15: Encoding Loss 20.48555564880371, Transition Loss 3.2487008571624756, Classifier Loss 0.37771084904670715, Total Loss 202.30526733398438\n",
      "15: Encoding Loss 19.825464248657227, Transition Loss 3.0874762535095215, Classifier Loss 0.3879810869693756, Total Loss 198.01931762695312\n",
      "15: Encoding Loss 19.428251266479492, Transition Loss 2.9769439697265625, Classifier Loss 0.3970143496990204, Total Loss 195.7228240966797\n",
      "15: Encoding Loss 20.575403213500977, Transition Loss 2.0296287536621094, Classifier Loss 0.4288794994354248, Total Loss 207.89710998535156\n",
      "15: Encoding Loss 19.1479434967041, Transition Loss 2.746677875518799, Classifier Loss 0.3947291970252991, Total Loss 193.20579528808594\n",
      "15: Encoding Loss 19.71156120300293, Transition Loss 2.677865982055664, Classifier Loss 0.41078418493270874, Total Loss 199.3064727783203\n",
      "15: Encoding Loss 19.72740364074707, Transition Loss 2.715494155883789, Classifier Loss 0.40318554639816284, Total Loss 198.68089294433594\n",
      "15: Encoding Loss 20.2883243560791, Transition Loss 3.1574208736419678, Classifier Loss 0.4111939072608948, Total Loss 204.05747985839844\n",
      "15: Encoding Loss 20.474374771118164, Transition Loss 1.802961826324463, Classifier Loss 0.4082895517349243, Total Loss 204.98455810546875\n",
      "15: Encoding Loss 19.081748962402344, Transition Loss 2.9451098442077637, Classifier Loss 0.3593181073665619, Total Loss 189.17481994628906\n",
      "15: Encoding Loss 19.91788673400879, Transition Loss 2.2947986125946045, Classifier Loss 0.396355003118515, Total Loss 199.4375457763672\n",
      "15: Encoding Loss 20.103700637817383, Transition Loss 3.4187541007995605, Classifier Loss 0.4437815546989441, Total Loss 205.89151000976562\n",
      "15: Encoding Loss 20.023561477661133, Transition Loss 2.574751615524292, Classifier Loss 0.39070752263069153, Total Loss 199.77420043945312\n",
      "15: Encoding Loss 20.018253326416016, Transition Loss 2.3808913230895996, Classifier Loss 0.3880351185798645, Total Loss 199.42572021484375\n",
      "15: Encoding Loss 19.79274559020996, Transition Loss 2.067063808441162, Classifier Loss 0.37980103492736816, Total Loss 196.7354736328125\n",
      "15: Encoding Loss 20.2689266204834, Transition Loss 2.9726953506469727, Classifier Loss 0.4348040223121643, Total Loss 206.2263641357422\n",
      "15: Encoding Loss 19.904081344604492, Transition Loss 2.6010804176330566, Classifier Loss 0.39894890785217285, Total Loss 199.6477508544922\n",
      "15: Encoding Loss 20.369895935058594, Transition Loss 2.09739351272583, Classifier Loss 0.37243929505348206, Total Loss 200.62257385253906\n",
      "15: Encoding Loss 20.096431732177734, Transition Loss 2.261826992034912, Classifier Loss 0.4226379990577698, Total Loss 203.48764038085938\n",
      "15: Encoding Loss 19.15264320373535, Transition Loss 2.3156118392944336, Classifier Loss 0.38955381512641907, Total Loss 192.6396484375\n",
      "15: Encoding Loss 20.23175621032715, Transition Loss 2.525496482849121, Classifier Loss 0.4572724997997284, Total Loss 208.08639526367188\n",
      "15: Encoding Loss 19.83804702758789, Transition Loss 2.387249708175659, Classifier Loss 0.4052817225456238, Total Loss 199.70999145507812\n",
      "15: Encoding Loss 19.101940155029297, Transition Loss 3.367967367172241, Classifier Loss 0.41993477940559387, Total Loss 195.4826202392578\n",
      "15: Encoding Loss 20.854114532470703, Transition Loss 1.9545005559921265, Classifier Loss 0.3931844234466553, Total Loss 206.54226684570312\n",
      "15: Encoding Loss 20.194194793701172, Transition Loss 2.323479652404785, Classifier Loss 0.41880306601524353, Total Loss 203.89857482910156\n",
      "15: Encoding Loss 19.99317741394043, Transition Loss 2.139453887939453, Classifier Loss 0.4034118354320526, Total Loss 200.71449279785156\n",
      "15: Encoding Loss 19.30048179626465, Transition Loss 3.163729190826416, Classifier Loss 0.3976919651031494, Total Loss 194.80580139160156\n",
      "15: Encoding Loss 19.95892906188965, Transition Loss 3.1589813232421875, Classifier Loss 0.40992769598960876, Total Loss 201.29598999023438\n",
      "15: Encoding Loss 19.469295501708984, Transition Loss 2.7614407539367676, Classifier Loss 0.3940590023994446, Total Loss 195.7125701904297\n",
      "15: Encoding Loss 18.566736221313477, Transition Loss 3.7473437786102295, Classifier Loss 0.39547061920166016, Total Loss 188.83041381835938\n",
      "15: Encoding Loss 19.525238037109375, Transition Loss 2.7070326805114746, Classifier Loss 0.4058161675930023, Total Loss 197.32493591308594\n",
      "15: Encoding Loss 20.022647857666016, Transition Loss 2.8068597316741943, Classifier Loss 0.39376020431518555, Total Loss 200.1185760498047\n",
      "15: Encoding Loss 18.43472671508789, Transition Loss 3.070270299911499, Classifier Loss 0.41713154315948486, Total Loss 189.80502319335938\n",
      "15: Encoding Loss 19.94043731689453, Transition Loss 3.1530165672302246, Classifier Loss 0.420192688703537, Total Loss 202.17337036132812\n",
      "15: Encoding Loss 19.559375762939453, Transition Loss 2.715684413909912, Classifier Loss 0.38583314418792725, Total Loss 195.60145568847656\n",
      "15: Encoding Loss 20.38844871520996, Transition Loss 2.530046224594116, Classifier Loss 0.4049365818500519, Total Loss 204.10726928710938\n",
      "15: Encoding Loss 19.426090240478516, Transition Loss 2.7428946495056152, Classifier Loss 0.41850656270980835, Total Loss 197.80795288085938\n",
      "15: Encoding Loss 20.180164337158203, Transition Loss 2.221010208129883, Classifier Loss 0.3835005462169647, Total Loss 200.23556518554688\n",
      "15: Encoding Loss 19.37735366821289, Transition Loss 2.326437473297119, Classifier Loss 0.45429596304893494, Total Loss 200.91371154785156\n",
      "15: Encoding Loss 21.092205047607422, Transition Loss 2.902451992034912, Classifier Loss 0.4100632071495056, Total Loss 210.324462890625\n",
      "15: Encoding Loss 20.16409683227539, Transition Loss 2.1081464290618896, Classifier Loss 0.43860524892807007, Total Loss 205.5949249267578\n",
      "15: Encoding Loss 18.92591667175293, Transition Loss 2.628527879714966, Classifier Loss 0.39187926054000854, Total Loss 191.1209716796875\n",
      "15: Encoding Loss 19.819095611572266, Transition Loss 2.7650203704833984, Classifier Loss 0.4098300337791443, Total Loss 200.08877563476562\n",
      "15: Encoding Loss 19.799434661865234, Transition Loss 2.4459571838378906, Classifier Loss 0.43466952443122864, Total Loss 202.3516387939453\n",
      "15: Encoding Loss 19.326948165893555, Transition Loss 1.7975159883499146, Classifier Loss 0.39502036571502686, Total Loss 194.47711181640625\n",
      "15: Encoding Loss 19.11835479736328, Transition Loss 2.3268847465515137, Classifier Loss 0.3858308494091034, Total Loss 191.99530029296875\n",
      "15: Encoding Loss 19.7547607421875, Transition Loss 3.0225329399108887, Classifier Loss 0.4266282618045807, Total Loss 201.305419921875\n",
      "15: Encoding Loss 19.624475479125977, Transition Loss 1.7439461946487427, Classifier Loss 0.3958311080932617, Total Loss 196.92770385742188\n",
      "15: Encoding Loss 20.100528717041016, Transition Loss 1.7144393920898438, Classifier Loss 0.42320334911346436, Total Loss 203.46746826171875\n",
      "15: Encoding Loss 18.88522720336914, Transition Loss 3.077747106552124, Classifier Loss 0.3912322521209717, Total Loss 190.8206024169922\n",
      "15: Encoding Loss 19.681339263916016, Transition Loss 3.391812562942505, Classifier Loss 0.4153152108192444, Total Loss 199.6605987548828\n",
      "15: Encoding Loss 19.288219451904297, Transition Loss 2.927915096282959, Classifier Loss 0.4088028371334076, Total Loss 195.77163696289062\n",
      "15: Encoding Loss 20.041913986206055, Transition Loss 3.1074414253234863, Classifier Loss 0.4304552674293518, Total Loss 204.00233459472656\n",
      "15: Encoding Loss 19.401315689086914, Transition Loss 3.9533395767211914, Classifier Loss 0.4056212902069092, Total Loss 196.56332397460938\n",
      "15: Encoding Loss 20.343595504760742, Transition Loss 2.3968026638031006, Classifier Loss 0.4168235659599304, Total Loss 204.9104766845703\n",
      "15: Encoding Loss 20.11270523071289, Transition Loss 2.3042263984680176, Classifier Loss 0.37112870812416077, Total Loss 198.47535705566406\n",
      "15: Encoding Loss 19.23084259033203, Transition Loss 2.841038703918457, Classifier Loss 0.4017801284790039, Total Loss 194.59295654296875\n",
      "15: Encoding Loss 19.47809600830078, Transition Loss 2.4538252353668213, Classifier Loss 0.4108341336250305, Total Loss 197.39894104003906\n",
      "15: Encoding Loss 19.209394454956055, Transition Loss 2.8900363445281982, Classifier Loss 0.39402830600738525, Total Loss 193.65599060058594\n",
      "15: Encoding Loss 19.28885841369629, Transition Loss 2.6698055267333984, Classifier Loss 0.4241330921649933, Total Loss 197.25814819335938\n",
      "15: Encoding Loss 19.846805572509766, Transition Loss 2.0900914669036865, Classifier Loss 0.3965998589992523, Total Loss 198.85244750976562\n",
      "15: Encoding Loss 19.492155075073242, Transition Loss 2.2608375549316406, Classifier Loss 0.3880007266998291, Total Loss 195.18946838378906\n",
      "15: Encoding Loss 18.42282485961914, Transition Loss 2.7112045288085938, Classifier Loss 0.35433268547058105, Total Loss 183.3581085205078\n",
      "15: Encoding Loss 18.657440185546875, Transition Loss 2.9941389560699463, Classifier Loss 0.3599180579185486, Total Loss 185.85015869140625\n",
      "15: Encoding Loss 20.885498046875, Transition Loss 2.0881640911102295, Classifier Loss 0.41485127806663513, Total Loss 208.9867401123047\n",
      "15: Encoding Loss 19.62684440612793, Transition Loss 3.466911554336548, Classifier Loss 0.42031779885292053, Total Loss 199.73992919921875\n",
      "15: Encoding Loss 19.905181884765625, Transition Loss 2.478276252746582, Classifier Loss 0.39935970306396484, Total Loss 199.67308044433594\n",
      "15: Encoding Loss 19.930875778198242, Transition Loss 3.106920003890991, Classifier Loss 0.3962424695491791, Total Loss 199.69264221191406\n",
      "15: Encoding Loss 19.772979736328125, Transition Loss 2.4582414627075195, Classifier Loss 0.43191736936569214, Total Loss 201.8672332763672\n",
      "15: Encoding Loss 18.919294357299805, Transition Loss 3.0371029376983643, Classifier Loss 0.4191063642501831, Total Loss 193.87240600585938\n",
      "15: Encoding Loss 20.25481414794922, Transition Loss 1.5662702322006226, Classifier Loss 0.42031192779541016, Total Loss 204.38294982910156\n",
      "15: Encoding Loss 19.091432571411133, Transition Loss 1.8048101663589478, Classifier Loss 0.39526721835136414, Total Loss 192.619140625\n",
      "15: Encoding Loss 19.677215576171875, Transition Loss 2.4769933223724365, Classifier Loss 0.3974398076534271, Total Loss 197.6571044921875\n",
      "15: Encoding Loss 18.286897659301758, Transition Loss 2.923703193664551, Classifier Loss 0.3912705183029175, Total Loss 186.00698852539062\n",
      "15: Encoding Loss 18.772682189941406, Transition Loss 2.0586023330688477, Classifier Loss 0.3896227777004242, Total Loss 189.5554656982422\n",
      "15: Encoding Loss 20.19968032836914, Transition Loss 3.342449903488159, Classifier Loss 0.39820054173469543, Total Loss 202.0859832763672\n",
      "15: Encoding Loss 19.650110244750977, Transition Loss 2.27101993560791, Classifier Loss 0.3700357675552368, Total Loss 194.65867614746094\n",
      "15: Encoding Loss 21.11212730407715, Transition Loss 2.26949405670166, Classifier Loss 0.37825000286102295, Total Loss 207.17591857910156\n",
      "15: Encoding Loss 20.690946578979492, Transition Loss 2.5260658264160156, Classifier Loss 0.4043755531311035, Total Loss 206.4703369140625\n",
      "15: Encoding Loss 19.925697326660156, Transition Loss 3.417168617248535, Classifier Loss 0.3827165365219116, Total Loss 198.3606719970703\n",
      "15: Encoding Loss 18.338298797607422, Transition Loss 4.433974266052246, Classifier Loss 0.36876553297042847, Total Loss 184.46975708007812\n",
      "16: Encoding Loss 19.957523345947266, Transition Loss 2.12367582321167, Classifier Loss 0.3665309250354767, Total Loss 196.73800659179688\n",
      "16: Encoding Loss 20.21038055419922, Transition Loss 2.124361038208008, Classifier Loss 0.3649478852748871, Total Loss 198.60269165039062\n",
      "16: Encoding Loss 20.887575149536133, Transition Loss 2.1839182376861572, Classifier Loss 0.39125555753707886, Total Loss 206.66294860839844\n",
      "16: Encoding Loss 19.776302337646484, Transition Loss 2.1291160583496094, Classifier Loss 0.392917275428772, Total Loss 197.92799377441406\n",
      "16: Encoding Loss 20.859981536865234, Transition Loss 1.6922138929367065, Classifier Loss 0.419379323720932, Total Loss 209.15623474121094\n",
      "16: Encoding Loss 19.186071395874023, Transition Loss 3.1269054412841797, Classifier Loss 0.40236973762512207, Total Loss 194.35092163085938\n",
      "16: Encoding Loss 20.300046920776367, Transition Loss 1.7451756000518799, Classifier Loss 0.4101267158985138, Total Loss 203.7620849609375\n",
      "16: Encoding Loss 21.16627311706543, Transition Loss 1.6749844551086426, Classifier Loss 0.41069573163986206, Total Loss 210.7347412109375\n",
      "16: Encoding Loss 19.759057998657227, Transition Loss 2.3450965881347656, Classifier Loss 0.4765552878379822, Total Loss 206.197021484375\n",
      "16: Encoding Loss 20.847352981567383, Transition Loss 1.9404065608978271, Classifier Loss 0.4130650758743286, Total Loss 208.47340393066406\n",
      "16: Encoding Loss 19.91596221923828, Transition Loss 2.0087311267852783, Classifier Loss 0.3794097304344177, Total Loss 197.67042541503906\n",
      "16: Encoding Loss 20.482215881347656, Transition Loss 2.2026753425598145, Classifier Loss 0.4006642699241638, Total Loss 204.36468505859375\n",
      "16: Encoding Loss 19.72869110107422, Transition Loss 2.2384495735168457, Classifier Loss 0.4095199704170227, Total Loss 199.22921752929688\n",
      "16: Encoding Loss 19.72502899169922, Transition Loss 2.5461266040802, Classifier Loss 0.394398957490921, Total Loss 197.74935913085938\n",
      "16: Encoding Loss 19.264842987060547, Transition Loss 2.6372523307800293, Classifier Loss 0.4223922789096832, Total Loss 196.88543701171875\n",
      "16: Encoding Loss 19.730716705322266, Transition Loss 2.6164791584014893, Classifier Loss 0.3960200548171997, Total Loss 197.97103881835938\n",
      "16: Encoding Loss 19.567951202392578, Transition Loss 2.5621612071990967, Classifier Loss 0.4107997715473175, Total Loss 198.1360321044922\n",
      "16: Encoding Loss 19.63280487060547, Transition Loss 2.503939390182495, Classifier Loss 0.3841313421726227, Total Loss 195.9763641357422\n",
      "16: Encoding Loss 20.05919075012207, Transition Loss 2.4027857780456543, Classifier Loss 0.38998720049858093, Total Loss 199.9528045654297\n",
      "16: Encoding Loss 20.46112060546875, Transition Loss 2.2680399417877197, Classifier Loss 0.3883879780769348, Total Loss 202.98138427734375\n",
      "16: Encoding Loss 19.652706146240234, Transition Loss 3.0362110137939453, Classifier Loss 0.4344507157802582, Total Loss 201.2739715576172\n",
      "16: Encoding Loss 19.63513946533203, Transition Loss 2.21392560005188, Classifier Loss 0.366700679063797, Total Loss 194.1939697265625\n",
      "16: Encoding Loss 19.053871154785156, Transition Loss 3.9731032848358154, Classifier Loss 0.3948776125907898, Total Loss 192.71334838867188\n",
      "16: Encoding Loss 19.090110778808594, Transition Loss 2.2984352111816406, Classifier Loss 0.39328745007514954, Total Loss 192.5093231201172\n",
      "16: Encoding Loss 19.50242042541504, Transition Loss 2.211134672164917, Classifier Loss 0.3566623628139496, Total Loss 192.12783813476562\n",
      "16: Encoding Loss 20.685094833374023, Transition Loss 2.4628705978393555, Classifier Loss 0.39704421162605286, Total Loss 205.67774963378906\n",
      "16: Encoding Loss 20.68682098388672, Transition Loss 2.517763137817383, Classifier Loss 0.4098008871078491, Total Loss 206.97821044921875\n",
      "16: Encoding Loss 19.66506576538086, Transition Loss 2.7426414489746094, Classifier Loss 0.45265311002731323, Total Loss 203.13436889648438\n",
      "16: Encoding Loss 19.312044143676758, Transition Loss 2.673682689666748, Classifier Loss 0.3868824243545532, Total Loss 193.71932983398438\n",
      "16: Encoding Loss 19.63324546813965, Transition Loss 3.379972219467163, Classifier Loss 0.4401024281978607, Total Loss 201.752197265625\n",
      "16: Encoding Loss 19.588367462158203, Transition Loss 2.7097363471984863, Classifier Loss 0.39242762327194214, Total Loss 196.4916534423828\n",
      "16: Encoding Loss 18.473848342895508, Transition Loss 3.3078951835632324, Classifier Loss 0.39488115906715393, Total Loss 187.9404754638672\n",
      "16: Encoding Loss 20.209632873535156, Transition Loss 2.629610061645508, Classifier Loss 0.41459232568740845, Total Loss 203.66221618652344\n",
      "16: Encoding Loss 19.106863021850586, Transition Loss 2.20703125, Classifier Loss 0.4094139039516449, Total Loss 194.23770141601562\n",
      "16: Encoding Loss 19.11001205444336, Transition Loss 2.8599140644073486, Classifier Loss 0.4159145653247833, Total Loss 195.04354858398438\n",
      "16: Encoding Loss 18.569820404052734, Transition Loss 2.378631353378296, Classifier Loss 0.40923988819122314, Total Loss 189.9582977294922\n",
      "16: Encoding Loss 19.96541976928711, Transition Loss 2.650405168533325, Classifier Loss 0.405489981174469, Total Loss 200.8024444580078\n",
      "16: Encoding Loss 19.9542179107666, Transition Loss 3.3952834606170654, Classifier Loss 0.42599207162857056, Total Loss 202.91201782226562\n",
      "16: Encoding Loss 20.04026222229004, Transition Loss 2.1268208026885986, Classifier Loss 0.3689047694206238, Total Loss 197.637939453125\n",
      "16: Encoding Loss 19.403594970703125, Transition Loss 2.8923304080963135, Classifier Loss 0.38826340436935425, Total Loss 194.63356018066406\n",
      "16: Encoding Loss 19.469104766845703, Transition Loss 3.189260482788086, Classifier Loss 0.39860066771507263, Total Loss 196.25074768066406\n",
      "16: Encoding Loss 19.263484954833984, Transition Loss 2.2475061416625977, Classifier Loss 0.4287742078304291, Total Loss 197.43482971191406\n",
      "16: Encoding Loss 19.35953140258789, Transition Loss 2.1333699226379395, Classifier Loss 0.3762495815753937, Total Loss 192.92788696289062\n",
      "16: Encoding Loss 19.332977294921875, Transition Loss 1.9440406560897827, Classifier Loss 0.40466955304145813, Total Loss 195.5195770263672\n",
      "16: Encoding Loss 20.416332244873047, Transition Loss 1.8523505926132202, Classifier Loss 0.3908516466617584, Total Loss 202.7863006591797\n",
      "16: Encoding Loss 20.328062057495117, Transition Loss 2.41863751411438, Classifier Loss 0.38745278120040894, Total Loss 201.853515625\n",
      "16: Encoding Loss 19.775747299194336, Transition Loss 2.923579454421997, Classifier Loss 0.43176954984664917, Total Loss 201.9676513671875\n",
      "16: Encoding Loss 20.248138427734375, Transition Loss 1.930524468421936, Classifier Loss 0.38272589445114136, Total Loss 200.643798828125\n",
      "16: Encoding Loss 20.19104766845703, Transition Loss 2.2034473419189453, Classifier Loss 0.393836110830307, Total Loss 201.35267639160156\n",
      "16: Encoding Loss 19.94036102294922, Transition Loss 2.4464194774627686, Classifier Loss 0.3869374990463257, Total Loss 198.7059326171875\n",
      "16: Encoding Loss 19.268932342529297, Transition Loss 1.9708867073059082, Classifier Loss 0.36727914214134216, Total Loss 191.27357482910156\n",
      "16: Encoding Loss 19.154651641845703, Transition Loss 2.0438833236694336, Classifier Loss 0.41483840346336365, Total Loss 195.1298370361328\n",
      "16: Encoding Loss 18.96563148498535, Transition Loss 2.1590988636016846, Classifier Loss 0.3896141052246094, Total Loss 191.1182861328125\n",
      "16: Encoding Loss 19.39287757873535, Transition Loss 2.791724681854248, Classifier Loss 0.39146938920021057, Total Loss 194.8483123779297\n",
      "16: Encoding Loss 19.09427261352539, Transition Loss 3.010050058364868, Classifier Loss 0.40198007225990295, Total Loss 193.55418395996094\n",
      "16: Encoding Loss 18.6588191986084, Transition Loss 3.0816988945007324, Classifier Loss 0.40288472175598145, Total Loss 190.17535400390625\n",
      "16: Encoding Loss 19.804485321044922, Transition Loss 2.5409417152404785, Classifier Loss 0.4361609220504761, Total Loss 202.5601806640625\n",
      "16: Encoding Loss 19.22356605529785, Transition Loss 2.820662021636963, Classifier Loss 0.361753910779953, Total Loss 190.52806091308594\n",
      "16: Encoding Loss 19.6959228515625, Transition Loss 2.6417524814605713, Classifier Loss 0.3716476261615753, Total Loss 195.260498046875\n",
      "16: Encoding Loss 19.052614212036133, Transition Loss 2.500926971435547, Classifier Loss 0.3760696351528168, Total Loss 190.52806091308594\n",
      "16: Encoding Loss 20.246482849121094, Transition Loss 2.59012508392334, Classifier Loss 0.42234525084495544, Total Loss 204.7244110107422\n",
      "16: Encoding Loss 20.821369171142578, Transition Loss 1.9658641815185547, Classifier Loss 0.40995046496391296, Total Loss 207.95916748046875\n",
      "16: Encoding Loss 19.771339416503906, Transition Loss 2.394014835357666, Classifier Loss 0.39351388812065125, Total Loss 198.0009002685547\n",
      "16: Encoding Loss 19.642314910888672, Transition Loss 2.449033260345459, Classifier Loss 0.39488306641578674, Total Loss 197.1166534423828\n",
      "16: Encoding Loss 18.96329689025879, Transition Loss 2.6225390434265137, Classifier Loss 0.3925861120223999, Total Loss 191.48948669433594\n",
      "16: Encoding Loss 18.818716049194336, Transition Loss 2.746169090270996, Classifier Loss 0.3939287066459656, Total Loss 190.49183654785156\n",
      "16: Encoding Loss 19.497705459594727, Transition Loss 2.362124443054199, Classifier Loss 0.38846680521965027, Total Loss 195.30075073242188\n",
      "16: Encoding Loss 20.617319107055664, Transition Loss 2.081958293914795, Classifier Loss 0.3892253041267395, Total Loss 204.27748107910156\n",
      "16: Encoding Loss 20.52168846130371, Transition Loss 2.563831329345703, Classifier Loss 0.41769346594810486, Total Loss 206.45562744140625\n",
      "16: Encoding Loss 19.52803611755371, Transition Loss 2.4661803245544434, Classifier Loss 0.39880841970443726, Total Loss 196.59837341308594\n",
      "16: Encoding Loss 20.26485252380371, Transition Loss 1.8345288038253784, Classifier Loss 0.4142744243144989, Total Loss 203.91317749023438\n",
      "16: Encoding Loss 19.01443862915039, Transition Loss 2.6655988693237305, Classifier Loss 0.383626788854599, Total Loss 191.0113067626953\n",
      "16: Encoding Loss 19.25566291809082, Transition Loss 2.6984291076660156, Classifier Loss 0.39369094371795654, Total Loss 193.95408630371094\n",
      "16: Encoding Loss 20.15019989013672, Transition Loss 2.4625895023345947, Classifier Loss 0.3860732316970825, Total Loss 200.30145263671875\n",
      "16: Encoding Loss 19.36079216003418, Transition Loss 3.4906413555145264, Classifier Loss 0.41584742069244385, Total Loss 197.16921997070312\n",
      "16: Encoding Loss 20.086071014404297, Transition Loss 2.445525646209717, Classifier Loss 0.39352816343307495, Total Loss 200.53050231933594\n",
      "16: Encoding Loss 19.802570343017578, Transition Loss 2.845588207244873, Classifier Loss 0.3568052649497986, Total Loss 194.6702117919922\n",
      "16: Encoding Loss 19.637046813964844, Transition Loss 2.9585611820220947, Classifier Loss 0.3941742181777954, Total Loss 197.10549926757812\n",
      "16: Encoding Loss 19.2183895111084, Transition Loss 2.5946218967437744, Classifier Loss 0.38113540410995483, Total Loss 192.37957763671875\n",
      "16: Encoding Loss 20.732677459716797, Transition Loss 1.942466139793396, Classifier Loss 0.41341444849967957, Total Loss 207.59136962890625\n",
      "16: Encoding Loss 19.44647789001465, Transition Loss 2.3847577571868896, Classifier Loss 0.38201141357421875, Total Loss 194.24990844726562\n",
      "16: Encoding Loss 19.65511131286621, Transition Loss 2.527657985687256, Classifier Loss 0.42296940088272095, Total Loss 200.04336547851562\n",
      "16: Encoding Loss 19.743135452270508, Transition Loss 2.449009895324707, Classifier Loss 0.41926664113998413, Total Loss 200.36155700683594\n",
      "16: Encoding Loss 19.99110984802246, Transition Loss 2.802199602127075, Classifier Loss 0.4064987301826477, Total Loss 201.13919067382812\n",
      "16: Encoding Loss 20.050485610961914, Transition Loss 1.6856344938278198, Classifier Loss 0.38785019516944885, Total Loss 199.52603149414062\n",
      "16: Encoding Loss 19.02223777770996, Transition Loss 2.674149751663208, Classifier Loss 0.3888665437698364, Total Loss 191.59939575195312\n",
      "16: Encoding Loss 19.952959060668945, Transition Loss 2.246624708175659, Classifier Loss 0.4045183062553406, Total Loss 200.5248260498047\n",
      "16: Encoding Loss 19.821287155151367, Transition Loss 3.0941081047058105, Classifier Loss 0.3887391984462738, Total Loss 198.0630340576172\n",
      "16: Encoding Loss 19.62617301940918, Transition Loss 2.5281529426574707, Classifier Loss 0.3731427490711212, Total Loss 194.82928466796875\n",
      "16: Encoding Loss 20.037944793701172, Transition Loss 2.1827914714813232, Classifier Loss 0.4213426113128662, Total Loss 202.8743896484375\n",
      "16: Encoding Loss 19.652782440185547, Transition Loss 1.9689762592315674, Classifier Loss 0.4183197021484375, Total Loss 199.4480438232422\n",
      "16: Encoding Loss 20.117650985717773, Transition Loss 2.628917932510376, Classifier Loss 0.4695519506931305, Total Loss 208.4221954345703\n",
      "16: Encoding Loss 19.625873565673828, Transition Loss 2.483720541000366, Classifier Loss 0.3997279405593872, Total Loss 197.47653198242188\n",
      "16: Encoding Loss 20.548748016357422, Transition Loss 1.8781492710113525, Classifier Loss 0.385373055934906, Total Loss 203.3029327392578\n",
      "16: Encoding Loss 20.0413761138916, Transition Loss 2.1450510025024414, Classifier Loss 0.3836711049079895, Total Loss 199.12713623046875\n",
      "16: Encoding Loss 18.740772247314453, Transition Loss 2.2374534606933594, Classifier Loss 0.40452253818511963, Total Loss 190.825927734375\n",
      "16: Encoding Loss 20.257450103759766, Transition Loss 2.3428733348846436, Classifier Loss 0.4393163323402405, Total Loss 206.45982360839844\n",
      "16: Encoding Loss 19.609987258911133, Transition Loss 2.2427010536193848, Classifier Loss 0.4207704961299896, Total Loss 199.40548706054688\n",
      "16: Encoding Loss 18.879518508911133, Transition Loss 3.1369996070861816, Classifier Loss 0.40186113119125366, Total Loss 191.8496551513672\n",
      "16: Encoding Loss 20.36577796936035, Transition Loss 1.8783459663391113, Classifier Loss 0.42639291286468506, Total Loss 205.94119262695312\n",
      "16: Encoding Loss 19.964683532714844, Transition Loss 2.168078899383545, Classifier Loss 0.4459654986858368, Total Loss 204.7476348876953\n",
      "16: Encoding Loss 19.856351852416992, Transition Loss 2.0258970260620117, Classifier Loss 0.41095179319381714, Total Loss 200.35118103027344\n",
      "16: Encoding Loss 18.971750259399414, Transition Loss 2.992741584777832, Classifier Loss 0.4036647379398346, Total Loss 192.739013671875\n",
      "16: Encoding Loss 19.967792510986328, Transition Loss 2.9988858699798584, Classifier Loss 0.41534996032714844, Total Loss 201.8771209716797\n",
      "16: Encoding Loss 19.454532623291016, Transition Loss 2.6141979694366455, Classifier Loss 0.38757696747779846, Total Loss 194.9167938232422\n",
      "16: Encoding Loss 18.470449447631836, Transition Loss 3.5415408611297607, Classifier Loss 0.37810319662094116, Total Loss 186.2822265625\n",
      "16: Encoding Loss 19.39354133605957, Transition Loss 2.5194666385650635, Classifier Loss 0.40579143166542053, Total Loss 196.2313690185547\n",
      "16: Encoding Loss 19.81973648071289, Transition Loss 2.6509652137756348, Classifier Loss 0.39380598068237305, Total Loss 198.46868896484375\n",
      "16: Encoding Loss 18.28322982788086, Transition Loss 2.9078097343444824, Classifier Loss 0.3928818106651306, Total Loss 186.13558959960938\n",
      "16: Encoding Loss 19.851484298706055, Transition Loss 3.008975028991699, Classifier Loss 0.4119514226913452, Total Loss 200.6088104248047\n",
      "16: Encoding Loss 19.496288299560547, Transition Loss 2.5737051963806152, Classifier Loss 0.39651185274124146, Total Loss 196.13624572753906\n",
      "16: Encoding Loss 20.424562454223633, Transition Loss 2.4065146446228027, Classifier Loss 0.4327409565448761, Total Loss 207.1519012451172\n",
      "16: Encoding Loss 19.634437561035156, Transition Loss 2.5684216022491455, Classifier Loss 0.4030846357345581, Total Loss 197.8976593017578\n",
      "16: Encoding Loss 20.1325740814209, Transition Loss 2.139025926589966, Classifier Loss 0.37186160683631897, Total Loss 198.674560546875\n",
      "16: Encoding Loss 19.098400115966797, Transition Loss 2.2042107582092285, Classifier Loss 0.4095727205276489, Total Loss 194.18533325195312\n",
      "16: Encoding Loss 21.076412200927734, Transition Loss 2.761066436767578, Classifier Loss 0.41323012113571167, Total Loss 210.48654174804688\n",
      "16: Encoding Loss 19.924959182739258, Transition Loss 1.9418704509735107, Classifier Loss 0.401969313621521, Total Loss 199.98497009277344\n",
      "16: Encoding Loss 18.61985206604004, Transition Loss 2.442190170288086, Classifier Loss 0.4173189103603363, Total Loss 191.17913818359375\n",
      "16: Encoding Loss 19.4510555267334, Transition Loss 2.5916528701782227, Classifier Loss 0.40650445222854614, Total Loss 196.77720642089844\n",
      "16: Encoding Loss 19.470487594604492, Transition Loss 2.2704885005950928, Classifier Loss 0.3985331654548645, Total Loss 196.07131958007812\n",
      "16: Encoding Loss 18.915088653564453, Transition Loss 1.6796648502349854, Classifier Loss 0.41786596179008484, Total Loss 193.4432373046875\n",
      "16: Encoding Loss 18.949878692626953, Transition Loss 2.207789659500122, Classifier Loss 0.37359198927879333, Total Loss 189.3997802734375\n",
      "16: Encoding Loss 19.40670394897461, Transition Loss 2.769943952560425, Classifier Loss 0.41054195165634155, Total Loss 196.86183166503906\n",
      "16: Encoding Loss 19.62886619567871, Transition Loss 1.6524410247802734, Classifier Loss 0.37280094623565674, Total Loss 194.64151000976562\n",
      "16: Encoding Loss 20.03832244873047, Transition Loss 1.5829521417617798, Classifier Loss 0.42021486163139343, Total Loss 202.6446533203125\n",
      "16: Encoding Loss 18.911861419677734, Transition Loss 2.9025957584381104, Classifier Loss 0.40827786922454834, Total Loss 192.70321655273438\n",
      "16: Encoding Loss 19.558837890625, Transition Loss 3.200301170349121, Classifier Loss 0.3854975402355194, Total Loss 195.6605224609375\n",
      "16: Encoding Loss 19.132566452026367, Transition Loss 2.7737984657287598, Classifier Loss 0.4167826175689697, Total Loss 195.29356384277344\n",
      "16: Encoding Loss 19.97492790222168, Transition Loss 2.8630211353302, Classifier Loss 0.4338873326778412, Total Loss 203.76075744628906\n",
      "16: Encoding Loss 19.181243896484375, Transition Loss 3.646693229675293, Classifier Loss 0.42828887701034546, Total Loss 197.0081787109375\n",
      "16: Encoding Loss 20.209869384765625, Transition Loss 2.209273099899292, Classifier Loss 0.38891398906707764, Total Loss 201.01220703125\n",
      "16: Encoding Loss 19.99297332763672, Transition Loss 2.114509105682373, Classifier Loss 0.3814176321029663, Total Loss 198.50843811035156\n",
      "16: Encoding Loss 18.859607696533203, Transition Loss 2.7403016090393066, Classifier Loss 0.3740714192390442, Total Loss 188.83206176757812\n",
      "16: Encoding Loss 18.942543029785156, Transition Loss 2.3235714435577393, Classifier Loss 0.39948463439941406, Total Loss 191.95352172851562\n",
      "16: Encoding Loss 19.22484588623047, Transition Loss 2.7241733074188232, Classifier Loss 0.4011392891407013, Total Loss 194.45751953125\n",
      "16: Encoding Loss 19.05914878845215, Transition Loss 2.391777753829956, Classifier Loss 0.380815327167511, Total Loss 191.0330810546875\n",
      "16: Encoding Loss 19.220529556274414, Transition Loss 2.0114567279815674, Classifier Loss 0.37673938274383545, Total Loss 191.84046936035156\n",
      "16: Encoding Loss 19.094486236572266, Transition Loss 2.031071662902832, Classifier Loss 0.4123294949531555, Total Loss 194.39505004882812\n",
      "16: Encoding Loss 18.234548568725586, Transition Loss 2.6486499309539795, Classifier Loss 0.3678553104400635, Total Loss 183.191650390625\n",
      "16: Encoding Loss 18.45006561279297, Transition Loss 2.7700729370117188, Classifier Loss 0.35069528222084045, Total Loss 183.22406005859375\n",
      "16: Encoding Loss 21.289403915405273, Transition Loss 2.0286810398101807, Classifier Loss 0.4239668548107147, Total Loss 213.11764526367188\n",
      "16: Encoding Loss 19.389677047729492, Transition Loss 3.2251365184783936, Classifier Loss 0.41713470220565796, Total Loss 197.47592163085938\n",
      "16: Encoding Loss 20.061134338378906, Transition Loss 2.377274990081787, Classifier Loss 0.4152984023094177, Total Loss 202.49436950683594\n",
      "16: Encoding Loss 20.006628036499023, Transition Loss 2.850135564804077, Classifier Loss 0.4158877432346344, Total Loss 202.21182250976562\n",
      "16: Encoding Loss 19.91527557373047, Transition Loss 2.35098934173584, Classifier Loss 0.37904733419418335, Total Loss 197.69712829589844\n",
      "16: Encoding Loss 18.908483505249023, Transition Loss 2.8358306884765625, Classifier Loss 0.38365310430526733, Total Loss 190.20034790039062\n",
      "16: Encoding Loss 20.33493995666504, Transition Loss 1.4977473020553589, Classifier Loss 0.3651670813560486, Total Loss 199.4957733154297\n",
      "16: Encoding Loss 19.02267837524414, Transition Loss 1.7735995054244995, Classifier Loss 0.3796791434288025, Total Loss 190.50405883789062\n",
      "16: Encoding Loss 19.514202117919922, Transition Loss 2.418348789215088, Classifier Loss 0.3825283646583557, Total Loss 194.8501434326172\n",
      "16: Encoding Loss 18.379047393798828, Transition Loss 2.7680282592773438, Classifier Loss 0.3906382620334625, Total Loss 186.64981079101562\n",
      "16: Encoding Loss 18.724748611450195, Transition Loss 2.0502922534942627, Classifier Loss 0.3980836868286133, Total Loss 190.01641845703125\n",
      "16: Encoding Loss 20.12607765197754, Transition Loss 3.0809314250946045, Classifier Loss 0.4130571484565735, Total Loss 202.93051147460938\n",
      "16: Encoding Loss 19.35801124572754, Transition Loss 2.1706929206848145, Classifier Loss 0.38521483540534973, Total Loss 193.81971740722656\n",
      "16: Encoding Loss 21.276805877685547, Transition Loss 2.087296485900879, Classifier Loss 0.4100460410118103, Total Loss 211.63653564453125\n",
      "16: Encoding Loss 20.83132553100586, Transition Loss 2.4073450565338135, Classifier Loss 0.3992157578468323, Total Loss 207.05368041992188\n",
      "16: Encoding Loss 19.735074996948242, Transition Loss 3.1695668697357178, Classifier Loss 0.4159403145313263, Total Loss 200.10855102539062\n",
      "16: Encoding Loss 18.38239288330078, Transition Loss 4.252979755401611, Classifier Loss 0.4285135865211487, Total Loss 190.7611083984375\n",
      "17: Encoding Loss 19.800748825073242, Transition Loss 2.028421401977539, Classifier Loss 0.3864012658596039, Total Loss 197.45179748535156\n",
      "17: Encoding Loss 19.98125457763672, Transition Loss 2.116884708404541, Classifier Loss 0.416748583316803, Total Loss 201.94827270507812\n",
      "17: Encoding Loss 20.520793914794922, Transition Loss 2.1025726795196533, Classifier Loss 0.4117553234100342, Total Loss 205.76242065429688\n",
      "17: Encoding Loss 19.63154411315918, Transition Loss 2.09265398979187, Classifier Loss 0.3728061020374298, Total Loss 194.75149536132812\n",
      "17: Encoding Loss 20.952592849731445, Transition Loss 1.624937653541565, Classifier Loss 0.44351819157600403, Total Loss 212.29754638671875\n",
      "17: Encoding Loss 18.838581085205078, Transition Loss 3.0803794860839844, Classifier Loss 0.4057239294052124, Total Loss 191.89710998535156\n",
      "17: Encoding Loss 20.214542388916016, Transition Loss 1.6939432621002197, Classifier Loss 0.38640424609184265, Total Loss 200.695556640625\n",
      "17: Encoding Loss 20.9879093170166, Transition Loss 1.5624794960021973, Classifier Loss 0.42038780450820923, Total Loss 210.25454711914062\n",
      "17: Encoding Loss 19.561664581298828, Transition Loss 2.263246536254883, Classifier Loss 0.4328904151916504, Total Loss 200.23500061035156\n",
      "17: Encoding Loss 20.871156692504883, Transition Loss 1.749403715133667, Classifier Loss 0.439102441072464, Total Loss 211.22938537597656\n",
      "17: Encoding Loss 20.010387420654297, Transition Loss 1.9100797176361084, Classifier Loss 0.37247323989868164, Total Loss 197.71246337890625\n",
      "17: Encoding Loss 19.626646041870117, Transition Loss 2.061589241027832, Classifier Loss 0.4082355201244354, Total Loss 198.24903869628906\n",
      "17: Encoding Loss 19.200273513793945, Transition Loss 2.110926389694214, Classifier Loss 0.39536941051483154, Total Loss 193.56130981445312\n",
      "17: Encoding Loss 19.663633346557617, Transition Loss 2.403416395187378, Classifier Loss 0.417490154504776, Total Loss 199.53875732421875\n",
      "17: Encoding Loss 19.28350830078125, Transition Loss 2.487001419067383, Classifier Loss 0.4207688570022583, Total Loss 196.8423614501953\n",
      "17: Encoding Loss 19.56391143798828, Transition Loss 2.4312312602996826, Classifier Loss 0.37998974323272705, Total Loss 194.99652099609375\n",
      "17: Encoding Loss 19.49025535583496, Transition Loss 2.435934066772461, Classifier Loss 0.4014369249343872, Total Loss 196.55291748046875\n",
      "17: Encoding Loss 19.67818260192871, Transition Loss 2.3146190643310547, Classifier Loss 0.3752099275588989, Total Loss 195.4093780517578\n",
      "17: Encoding Loss 19.713964462280273, Transition Loss 2.209235191345215, Classifier Loss 0.3593365550041199, Total Loss 194.08721923828125\n",
      "17: Encoding Loss 20.4062442779541, Transition Loss 2.014331340789795, Classifier Loss 0.41698944568634033, Total Loss 205.3517608642578\n",
      "17: Encoding Loss 19.404541015625, Transition Loss 2.828000068664551, Classifier Loss 0.39732927083969116, Total Loss 195.53485107421875\n",
      "17: Encoding Loss 19.47934913635254, Transition Loss 2.0803446769714355, Classifier Loss 0.37261706590652466, Total Loss 193.51255798339844\n",
      "17: Encoding Loss 19.19595718383789, Transition Loss 3.5746397972106934, Classifier Loss 0.41295522451400757, Total Loss 195.57810974121094\n",
      "17: Encoding Loss 18.61928367614746, Transition Loss 2.1647849082946777, Classifier Loss 0.3726803958415985, Total Loss 186.65525817871094\n",
      "17: Encoding Loss 19.450733184814453, Transition Loss 2.113424062728882, Classifier Loss 0.3817218542098999, Total Loss 194.2007293701172\n",
      "17: Encoding Loss 20.35202407836914, Transition Loss 2.279951333999634, Classifier Loss 0.37809598445892334, Total Loss 201.081787109375\n",
      "17: Encoding Loss 20.15886878967285, Transition Loss 2.2583632469177246, Classifier Loss 0.39413732290267944, Total Loss 201.1363525390625\n",
      "17: Encoding Loss 19.508020401000977, Transition Loss 2.468745231628418, Classifier Loss 0.4039396345615387, Total Loss 196.95187377929688\n",
      "17: Encoding Loss 19.405742645263672, Transition Loss 2.3882486820220947, Classifier Loss 0.382257342338562, Total Loss 193.9493408203125\n",
      "17: Encoding Loss 19.180919647216797, Transition Loss 3.1335902214050293, Classifier Loss 0.442697137594223, Total Loss 198.34381103515625\n",
      "17: Encoding Loss 19.454099655151367, Transition Loss 2.526231050491333, Classifier Loss 0.3739226758480072, Total Loss 193.5303192138672\n",
      "17: Encoding Loss 18.6519718170166, Transition Loss 2.996743679046631, Classifier Loss 0.39806675910949707, Total Loss 189.62179565429688\n",
      "17: Encoding Loss 20.0382137298584, Transition Loss 2.4742865562438965, Classifier Loss 0.4126124083995819, Total Loss 202.0618133544922\n",
      "17: Encoding Loss 18.672895431518555, Transition Loss 2.1599714756011963, Classifier Loss 0.40248245000839233, Total Loss 190.0634002685547\n",
      "17: Encoding Loss 19.350872039794922, Transition Loss 2.6256580352783203, Classifier Loss 0.3839699327945709, Total Loss 193.72911071777344\n",
      "17: Encoding Loss 18.487815856933594, Transition Loss 2.2931838035583496, Classifier Loss 0.3921510577201843, Total Loss 187.57626342773438\n",
      "17: Encoding Loss 19.757286071777344, Transition Loss 2.417146921157837, Classifier Loss 0.40178701281547546, Total Loss 198.7204132080078\n",
      "17: Encoding Loss 20.12027359008789, Transition Loss 3.136383295059204, Classifier Loss 0.4413229823112488, Total Loss 205.7217559814453\n",
      "17: Encoding Loss 19.726961135864258, Transition Loss 1.9843577146530151, Classifier Loss 0.36358532309532166, Total Loss 194.5710906982422\n",
      "17: Encoding Loss 19.400264739990234, Transition Loss 2.7369704246520996, Classifier Loss 0.3614662289619446, Total Loss 191.89614868164062\n",
      "17: Encoding Loss 19.341463088989258, Transition Loss 3.008322238922119, Classifier Loss 0.4199168384075165, Total Loss 197.32505798339844\n",
      "17: Encoding Loss 19.034122467041016, Transition Loss 2.1826260089874268, Classifier Loss 0.36943334341049194, Total Loss 189.65283203125\n",
      "17: Encoding Loss 19.160839080810547, Transition Loss 2.053804874420166, Classifier Loss 0.37051406502723694, Total Loss 190.7489013671875\n",
      "17: Encoding Loss 18.913402557373047, Transition Loss 1.9233125448226929, Classifier Loss 0.3727216422557831, Total Loss 188.9640655517578\n",
      "17: Encoding Loss 20.387502670288086, Transition Loss 1.7479686737060547, Classifier Loss 0.4303905665874481, Total Loss 206.48866271972656\n",
      "17: Encoding Loss 20.235504150390625, Transition Loss 2.286512613296509, Classifier Loss 0.43107306957244873, Total Loss 205.44863891601562\n",
      "17: Encoding Loss 19.48815155029297, Transition Loss 2.719236373901367, Classifier Loss 0.4376780688762665, Total Loss 200.2168731689453\n",
      "17: Encoding Loss 20.260068893432617, Transition Loss 1.7561829090118408, Classifier Loss 0.44733020663261414, Total Loss 207.16481018066406\n",
      "17: Encoding Loss 19.9952335357666, Transition Loss 2.0829739570617676, Classifier Loss 0.4090249538421631, Total Loss 201.2809600830078\n",
      "17: Encoding Loss 19.927274703979492, Transition Loss 2.350327730178833, Classifier Loss 0.3813464641571045, Total Loss 198.0229034423828\n",
      "17: Encoding Loss 18.89682388305664, Transition Loss 1.8840327262878418, Classifier Loss 0.3801650404930115, Total Loss 189.56790161132812\n",
      "17: Encoding Loss 19.18889617919922, Transition Loss 1.9657413959503174, Classifier Loss 0.3816623389720917, Total Loss 192.07054138183594\n",
      "17: Encoding Loss 18.653749465942383, Transition Loss 2.0797038078308105, Classifier Loss 0.38424956798553467, Total Loss 188.07089233398438\n",
      "17: Encoding Loss 19.230165481567383, Transition Loss 2.5788607597351074, Classifier Loss 0.38684237003326416, Total Loss 193.0413360595703\n",
      "17: Encoding Loss 18.908472061157227, Transition Loss 2.7759244441986084, Classifier Loss 0.3889402151107788, Total Loss 190.7169952392578\n",
      "17: Encoding Loss 18.228288650512695, Transition Loss 2.90736722946167, Classifier Loss 0.4227398633956909, Total Loss 188.6817626953125\n",
      "17: Encoding Loss 19.507516860961914, Transition Loss 2.338231325149536, Classifier Loss 0.38940462470054626, Total Loss 195.46824645996094\n",
      "17: Encoding Loss 19.036182403564453, Transition Loss 2.6143670082092285, Classifier Loss 0.39172396063804626, Total Loss 191.98472595214844\n",
      "17: Encoding Loss 18.980770111083984, Transition Loss 2.4967432022094727, Classifier Loss 0.39011529088020325, Total Loss 191.35704040527344\n",
      "17: Encoding Loss 18.745479583740234, Transition Loss 2.3282220363616943, Classifier Loss 0.3717896044254303, Total Loss 187.6084442138672\n",
      "17: Encoding Loss 20.165807723999023, Transition Loss 2.3892409801483154, Classifier Loss 0.3924618661403656, Total Loss 201.0504913330078\n",
      "17: Encoding Loss 20.874298095703125, Transition Loss 1.846981167793274, Classifier Loss 0.398261696100235, Total Loss 207.18995666503906\n",
      "17: Encoding Loss 19.819631576538086, Transition Loss 2.31201434135437, Classifier Loss 0.3975413739681244, Total Loss 198.77359008789062\n",
      "17: Encoding Loss 19.582881927490234, Transition Loss 2.332207202911377, Classifier Loss 0.38385456800460815, Total Loss 195.5149688720703\n",
      "17: Encoding Loss 18.751346588134766, Transition Loss 2.4559779167175293, Classifier Loss 0.35334551334381104, Total Loss 185.83651733398438\n",
      "17: Encoding Loss 18.31828498840332, Transition Loss 2.5692522525787354, Classifier Loss 0.37426573038101196, Total Loss 184.48670959472656\n",
      "17: Encoding Loss 19.5255069732666, Transition Loss 2.2362422943115234, Classifier Loss 0.3814765214920044, Total Loss 194.79896545410156\n",
      "17: Encoding Loss 20.365381240844727, Transition Loss 1.9150184392929077, Classifier Loss 0.3802171051502228, Total Loss 201.32777404785156\n",
      "17: Encoding Loss 20.626726150512695, Transition Loss 2.387500524520874, Classifier Loss 0.4360294044017792, Total Loss 209.09423828125\n",
      "17: Encoding Loss 19.515134811401367, Transition Loss 2.3150641918182373, Classifier Loss 0.40521442890167236, Total Loss 197.10552978515625\n",
      "17: Encoding Loss 20.11570167541504, Transition Loss 1.798708200454712, Classifier Loss 0.40081995725631714, Total Loss 201.3673553466797\n",
      "17: Encoding Loss 18.823930740356445, Transition Loss 2.45334529876709, Classifier Loss 0.3613949418067932, Total Loss 187.2216033935547\n",
      "17: Encoding Loss 18.877079010009766, Transition Loss 2.640484571456909, Classifier Loss 0.3880518078804016, Total Loss 190.3498992919922\n",
      "17: Encoding Loss 19.680706024169922, Transition Loss 2.233504295349121, Classifier Loss 0.3770385682582855, Total Loss 195.59622192382812\n",
      "17: Encoding Loss 19.1611385345459, Transition Loss 3.365968704223633, Classifier Loss 0.45277583599090576, Total Loss 199.23988342285156\n",
      "17: Encoding Loss 19.98703956604004, Transition Loss 2.2234385013580322, Classifier Loss 0.38162457942962646, Total Loss 198.5034637451172\n",
      "17: Encoding Loss 19.84036636352539, Transition Loss 2.743257999420166, Classifier Loss 0.38772153854370117, Total Loss 198.04373168945312\n",
      "17: Encoding Loss 19.478315353393555, Transition Loss 2.632972002029419, Classifier Loss 0.4158599376678467, Total Loss 197.93911743164062\n",
      "17: Encoding Loss 18.824127197265625, Transition Loss 2.578500270843506, Classifier Loss 0.3794439733028412, Total Loss 189.05311584472656\n",
      "17: Encoding Loss 20.298276901245117, Transition Loss 1.665028691291809, Classifier Loss 0.40854766964912415, Total Loss 203.57398986816406\n",
      "17: Encoding Loss 19.19566535949707, Transition Loss 2.4591026306152344, Classifier Loss 0.3738188147544861, Total Loss 191.43902587890625\n",
      "17: Encoding Loss 19.928075790405273, Transition Loss 2.296696901321411, Classifier Loss 0.4263034760951996, Total Loss 202.51429748535156\n",
      "17: Encoding Loss 19.463966369628906, Transition Loss 2.3950307369232178, Classifier Loss 0.3949268162250519, Total Loss 195.68341064453125\n",
      "17: Encoding Loss 19.99062728881836, Transition Loss 2.736570358276367, Classifier Loss 0.4268918037414551, Total Loss 203.16152954101562\n",
      "17: Encoding Loss 20.000885009765625, Transition Loss 1.5539056062698364, Classifier Loss 0.4129144549369812, Total Loss 201.6092987060547\n",
      "17: Encoding Loss 18.768646240234375, Transition Loss 2.6796908378601074, Classifier Loss 0.37154772877693176, Total Loss 187.83987426757812\n",
      "17: Encoding Loss 19.802217483520508, Transition Loss 2.054614782333374, Classifier Loss 0.4322739243507385, Total Loss 202.05606079101562\n",
      "17: Encoding Loss 19.86813735961914, Transition Loss 3.0899949073791504, Classifier Loss 0.39998698234558105, Total Loss 199.56179809570312\n",
      "17: Encoding Loss 19.97549819946289, Transition Loss 2.2573859691619873, Classifier Loss 0.3958081007003784, Total Loss 199.83627319335938\n",
      "17: Encoding Loss 20.135038375854492, Transition Loss 2.259270429611206, Classifier Loss 0.3950836658477783, Total Loss 201.04052734375\n",
      "17: Encoding Loss 19.739511489868164, Transition Loss 1.8213326930999756, Classifier Loss 0.3609665036201477, Total Loss 194.37701416015625\n",
      "17: Encoding Loss 19.855937957763672, Transition Loss 2.563995122909546, Classifier Loss 0.4388374984264374, Total Loss 203.2440643310547\n",
      "17: Encoding Loss 19.431058883666992, Transition Loss 2.2126340866088867, Classifier Loss 0.4097296893596649, Total Loss 196.8639678955078\n",
      "17: Encoding Loss 20.45360565185547, Transition Loss 1.7911511659622192, Classifier Loss 0.3848351538181305, Total Loss 202.47059631347656\n",
      "17: Encoding Loss 19.858089447021484, Transition Loss 1.993800401687622, Classifier Loss 0.3930632472038269, Total Loss 198.56980895996094\n",
      "17: Encoding Loss 18.779829025268555, Transition Loss 2.178941011428833, Classifier Loss 0.38108062744140625, Total Loss 188.78248596191406\n",
      "17: Encoding Loss 19.99976921081543, Transition Loss 2.2142703533172607, Classifier Loss 0.39682483673095703, Total Loss 200.1234893798828\n",
      "17: Encoding Loss 19.250829696655273, Transition Loss 2.1255762577056885, Classifier Loss 0.42953264713287354, Total Loss 197.385009765625\n",
      "17: Encoding Loss 18.84299659729004, Transition Loss 2.9183924198150635, Classifier Loss 0.3923022747039795, Total Loss 190.55787658691406\n",
      "17: Encoding Loss 20.397184371948242, Transition Loss 1.7887413501739502, Classifier Loss 0.38589099049568176, Total Loss 202.1243133544922\n",
      "17: Encoding Loss 19.90338134765625, Transition Loss 2.029374361038208, Classifier Loss 0.3765273690223694, Total Loss 197.28565979003906\n",
      "17: Encoding Loss 19.821571350097656, Transition Loss 1.8213034868240356, Classifier Loss 0.42822790145874023, Total Loss 201.75961303710938\n",
      "17: Encoding Loss 19.25680923461914, Transition Loss 2.7029271125793457, Classifier Loss 0.4082844853401184, Total Loss 195.4235076904297\n",
      "17: Encoding Loss 19.74274253845215, Transition Loss 2.8085808753967285, Classifier Loss 0.40640419721603394, Total Loss 199.1440887451172\n",
      "17: Encoding Loss 19.29787254333496, Transition Loss 2.436758518218994, Classifier Loss 0.371386855840683, Total Loss 192.00901794433594\n",
      "17: Encoding Loss 18.3538761138916, Transition Loss 3.2566962242126465, Classifier Loss 0.33977702260017395, Total Loss 181.46005249023438\n",
      "17: Encoding Loss 19.272924423217773, Transition Loss 2.327178955078125, Classifier Loss 0.4163149297237396, Total Loss 196.2803192138672\n",
      "17: Encoding Loss 19.693355560302734, Transition Loss 2.394327402114868, Classifier Loss 0.4024626910686493, Total Loss 198.27198791503906\n",
      "17: Encoding Loss 18.000728607177734, Transition Loss 2.7902820110321045, Classifier Loss 0.4118744432926178, Total Loss 185.7513427734375\n",
      "17: Encoding Loss 19.640377044677734, Transition Loss 2.791221857070923, Classifier Loss 0.40191686153411865, Total Loss 197.87295532226562\n",
      "17: Encoding Loss 19.374418258666992, Transition Loss 2.4330101013183594, Classifier Loss 0.36417150497436523, Total Loss 191.89910888671875\n",
      "17: Encoding Loss 20.36360740661621, Transition Loss 2.2182538509368896, Classifier Loss 0.3930457830429077, Total Loss 202.65708923339844\n",
      "17: Encoding Loss 19.400373458862305, Transition Loss 2.434190511703491, Classifier Loss 0.41953158378601074, Total Loss 197.64297485351562\n",
      "17: Encoding Loss 20.118642807006836, Transition Loss 1.9975303411483765, Classifier Loss 0.376671701669693, Total Loss 199.0158233642578\n",
      "17: Encoding Loss 19.010709762573242, Transition Loss 2.1247472763061523, Classifier Loss 0.41599053144454956, Total Loss 194.1096649169922\n",
      "17: Encoding Loss 20.840667724609375, Transition Loss 2.525754928588867, Classifier Loss 0.4231310486793518, Total Loss 209.54360961914062\n",
      "17: Encoding Loss 19.611106872558594, Transition Loss 1.8220101594924927, Classifier Loss 0.40027010440826416, Total Loss 197.28025817871094\n",
      "17: Encoding Loss 18.672748565673828, Transition Loss 2.3023593425750732, Classifier Loss 0.41509172320365906, Total Loss 191.35162353515625\n",
      "17: Encoding Loss 19.232412338256836, Transition Loss 2.443216562271118, Classifier Loss 0.399210661649704, Total Loss 194.26901245117188\n",
      "17: Encoding Loss 19.49949073791504, Transition Loss 2.1477370262145996, Classifier Loss 0.40791088342666626, Total Loss 197.21656799316406\n",
      "17: Encoding Loss 18.757307052612305, Transition Loss 1.6512184143066406, Classifier Loss 0.38660740852355957, Total Loss 189.0494384765625\n",
      "17: Encoding Loss 18.828256607055664, Transition Loss 2.1029531955718994, Classifier Loss 0.39210081100463867, Total Loss 190.25672912597656\n",
      "17: Encoding Loss 19.78424644470215, Transition Loss 2.6231324672698975, Classifier Loss 0.3469061255455017, Total Loss 193.4892120361328\n",
      "17: Encoding Loss 19.661762237548828, Transition Loss 1.6005007028579712, Classifier Loss 0.3982863426208496, Total Loss 197.44284057617188\n",
      "17: Encoding Loss 20.127979278564453, Transition Loss 1.5104268789291382, Classifier Loss 0.42087167501449585, Total Loss 203.41307067871094\n",
      "17: Encoding Loss 18.810388565063477, Transition Loss 2.7705905437469482, Classifier Loss 0.4186801314353943, Total Loss 192.90524291992188\n",
      "17: Encoding Loss 19.389497756958008, Transition Loss 3.024515390396118, Classifier Loss 0.39861905574798584, Total Loss 195.58279418945312\n",
      "17: Encoding Loss 19.027259826660156, Transition Loss 2.692507028579712, Classifier Loss 0.43848559260368347, Total Loss 196.60513305664062\n",
      "17: Encoding Loss 19.636402130126953, Transition Loss 2.788818359375, Classifier Loss 0.40504470467567444, Total Loss 198.15345764160156\n",
      "17: Encoding Loss 18.830060958862305, Transition Loss 3.5388193130493164, Classifier Loss 0.4193629324436188, Total Loss 193.2845458984375\n",
      "17: Encoding Loss 19.819095611572266, Transition Loss 2.0919928550720215, Classifier Loss 0.4024823009967804, Total Loss 199.21939086914062\n",
      "17: Encoding Loss 19.70499610900879, Transition Loss 1.9998376369476318, Classifier Loss 0.36425986886024475, Total Loss 194.46591186523438\n",
      "17: Encoding Loss 18.84689712524414, Transition Loss 2.5694611072540283, Classifier Loss 0.4259395897388458, Total Loss 193.88302612304688\n",
      "17: Encoding Loss 19.094816207885742, Transition Loss 2.210092782974243, Classifier Loss 0.4193871021270752, Total Loss 195.13925170898438\n",
      "17: Encoding Loss 18.994108200073242, Transition Loss 2.6053714752197266, Classifier Loss 0.3812316656112671, Total Loss 190.59710693359375\n",
      "17: Encoding Loss 19.042390823364258, Transition Loss 2.296050786972046, Classifier Loss 0.3737426996231079, Total Loss 190.172607421875\n",
      "17: Encoding Loss 19.64042091369629, Transition Loss 1.9124113321304321, Classifier Loss 0.38227927684783936, Total Loss 195.7337646484375\n",
      "17: Encoding Loss 19.40654182434082, Transition Loss 1.9945998191833496, Classifier Loss 0.38844916224479675, Total Loss 194.49618530273438\n",
      "17: Encoding Loss 17.89499282836914, Transition Loss 2.538447141647339, Classifier Loss 0.3723672926425934, Total Loss 180.90435791015625\n",
      "17: Encoding Loss 18.099369049072266, Transition Loss 2.6448705196380615, Classifier Loss 0.3379667401313782, Total Loss 179.12060546875\n",
      "17: Encoding Loss 20.847726821899414, Transition Loss 1.9001109600067139, Classifier Loss 0.39647728204727173, Total Loss 206.80955505371094\n",
      "17: Encoding Loss 19.24665641784668, Transition Loss 2.959912061691284, Classifier Loss 0.4163210690021515, Total Loss 196.1973419189453\n",
      "17: Encoding Loss 19.86347007751465, Transition Loss 2.2070577144622803, Classifier Loss 0.3779153525829315, Total Loss 197.1407012939453\n",
      "17: Encoding Loss 19.814287185668945, Transition Loss 2.609379768371582, Classifier Loss 0.43946200609207153, Total Loss 202.9823760986328\n",
      "17: Encoding Loss 19.673419952392578, Transition Loss 2.185877561569214, Classifier Loss 0.39551496505737305, Total Loss 197.37603759765625\n",
      "17: Encoding Loss 18.799020767211914, Transition Loss 2.641935110092163, Classifier Loss 0.4051312208175659, Total Loss 191.4336700439453\n",
      "17: Encoding Loss 20.316287994384766, Transition Loss 1.4046218395233154, Classifier Loss 0.39146551489830017, Total Loss 201.95777893066406\n",
      "17: Encoding Loss 19.078428268432617, Transition Loss 1.701134204864502, Classifier Loss 0.3788619935512543, Total Loss 190.85385131835938\n",
      "17: Encoding Loss 19.28592300415039, Transition Loss 2.2791683673858643, Classifier Loss 0.37724265456199646, Total Loss 192.46749877929688\n",
      "17: Encoding Loss 18.2066593170166, Transition Loss 2.617314338684082, Classifier Loss 0.39789697527885437, Total Loss 185.9664306640625\n",
      "17: Encoding Loss 18.466320037841797, Transition Loss 1.9465198516845703, Classifier Loss 0.3847176730632782, Total Loss 186.59164428710938\n",
      "17: Encoding Loss 19.9210205078125, Transition Loss 2.883368968963623, Classifier Loss 0.3964989185333252, Total Loss 199.5947265625\n",
      "17: Encoding Loss 19.268917083740234, Transition Loss 2.0294251441955566, Classifier Loss 0.39987924695014954, Total Loss 194.545166015625\n",
      "17: Encoding Loss 20.96802520751953, Transition Loss 1.9103299379348755, Classifier Loss 0.41072845458984375, Total Loss 209.19911193847656\n",
      "17: Encoding Loss 20.409156799316406, Transition Loss 2.150298595428467, Classifier Loss 0.37506595253944397, Total Loss 201.20989990234375\n",
      "17: Encoding Loss 19.54598617553711, Transition Loss 2.7390942573547363, Classifier Loss 0.372805655002594, Total Loss 194.1962890625\n",
      "17: Encoding Loss 18.402467727661133, Transition Loss 3.753713607788086, Classifier Loss 0.40129610896110535, Total Loss 188.10009765625\n",
      "18: Encoding Loss 19.97100257873535, Transition Loss 1.8228580951690674, Classifier Loss 0.3764438331127167, Total Loss 197.7769775390625\n",
      "18: Encoding Loss 19.941205978393555, Transition Loss 1.8794411420822144, Classifier Loss 0.4244890809059143, Total Loss 202.3544464111328\n",
      "18: Encoding Loss 20.534013748168945, Transition Loss 1.8776909112930298, Classifier Loss 0.4053620994091034, Total Loss 205.18385314941406\n",
      "18: Encoding Loss 19.58622169494629, Transition Loss 1.865871548652649, Classifier Loss 0.37702417373657227, Total Loss 194.76535034179688\n",
      "18: Encoding Loss 20.682594299316406, Transition Loss 1.419463038444519, Classifier Loss 0.43276646733283997, Total Loss 209.0212860107422\n",
      "18: Encoding Loss 18.6318359375, Transition Loss 2.819606065750122, Classifier Loss 0.42662984132766724, Total Loss 192.28158569335938\n",
      "18: Encoding Loss 20.0136661529541, Transition Loss 1.5686111450195312, Classifier Loss 0.405459463596344, Total Loss 200.968994140625\n",
      "18: Encoding Loss 20.779550552368164, Transition Loss 1.3947198390960693, Classifier Loss 0.4308215379714966, Total Loss 209.59750366210938\n",
      "18: Encoding Loss 19.409521102905273, Transition Loss 2.109436273574829, Classifier Loss 0.4454578757286072, Total Loss 200.2438507080078\n",
      "18: Encoding Loss 20.969587326049805, Transition Loss 1.596312403678894, Classifier Loss 0.4190997779369354, Total Loss 209.98593139648438\n",
      "18: Encoding Loss 19.79800033569336, Transition Loss 1.8136088848114014, Classifier Loss 0.4109223484992981, Total Loss 199.83897399902344\n",
      "18: Encoding Loss 19.695079803466797, Transition Loss 1.9566583633422852, Classifier Loss 0.3974098563194275, Total Loss 197.69296264648438\n",
      "18: Encoding Loss 18.981342315673828, Transition Loss 2.007392644882202, Classifier Loss 0.4031968414783478, Total Loss 192.5718994140625\n",
      "18: Encoding Loss 19.488147735595703, Transition Loss 2.279764413833618, Classifier Loss 0.40650445222854614, Total Loss 197.01158142089844\n",
      "18: Encoding Loss 19.215700149536133, Transition Loss 2.3768625259399414, Classifier Loss 0.41075924038887024, Total Loss 195.2769012451172\n",
      "18: Encoding Loss 19.685392379760742, Transition Loss 2.2863943576812744, Classifier Loss 0.4000769853591919, Total Loss 197.9481201171875\n",
      "18: Encoding Loss 19.26810073852539, Transition Loss 2.3399782180786133, Classifier Loss 0.3835705816745758, Total Loss 192.96986389160156\n",
      "18: Encoding Loss 19.383819580078125, Transition Loss 2.1659905910491943, Classifier Loss 0.3649110794067383, Total Loss 191.99485778808594\n",
      "18: Encoding Loss 19.72074317932129, Transition Loss 2.0929970741271973, Classifier Loss 0.3887473940849304, Total Loss 197.0592803955078\n",
      "18: Encoding Loss 20.292850494384766, Transition Loss 1.9277832508087158, Classifier Loss 0.3722105324268341, Total Loss 199.9494171142578\n",
      "18: Encoding Loss 19.511213302612305, Transition Loss 2.616645336151123, Classifier Loss 0.39427125453948975, Total Loss 196.0401611328125\n",
      "18: Encoding Loss 19.196495056152344, Transition Loss 2.0323421955108643, Classifier Loss 0.38956838846206665, Total Loss 192.93527221679688\n",
      "18: Encoding Loss 18.74785804748535, Transition Loss 3.4270315170288086, Classifier Loss 0.3670128583908081, Total Loss 187.3695526123047\n",
      "18: Encoding Loss 18.74247932434082, Transition Loss 2.0921266078948975, Classifier Loss 0.4030148983001709, Total Loss 190.65975952148438\n",
      "18: Encoding Loss 19.448238372802734, Transition Loss 2.054752826690674, Classifier Loss 0.3908146023750305, Total Loss 195.07833862304688\n",
      "18: Encoding Loss 20.317584991455078, Transition Loss 2.2164580821990967, Classifier Loss 0.3576425015926361, Total Loss 198.74822998046875\n",
      "18: Encoding Loss 20.176523208618164, Transition Loss 2.174654483795166, Classifier Loss 0.42495444416999817, Total Loss 204.34255981445312\n",
      "18: Encoding Loss 19.300697326660156, Transition Loss 2.3661272525787354, Classifier Loss 0.3897559642791748, Total Loss 193.85440063476562\n",
      "18: Encoding Loss 19.125377655029297, Transition Loss 2.286184310913086, Classifier Loss 0.42914408445358276, Total Loss 196.3746795654297\n",
      "18: Encoding Loss 19.03394317626953, Transition Loss 2.978240966796875, Classifier Loss 0.40240204334259033, Total Loss 193.10739135742188\n",
      "18: Encoding Loss 19.68639373779297, Transition Loss 2.3963804244995117, Classifier Loss 0.3825511634349823, Total Loss 196.2255401611328\n",
      "18: Encoding Loss 18.22039222717285, Transition Loss 2.8450326919555664, Classifier Loss 0.3933383524417877, Total Loss 185.6659698486328\n",
      "18: Encoding Loss 19.926361083984375, Transition Loss 2.3112316131591797, Classifier Loss 0.40201935172080994, Total Loss 200.0750732421875\n",
      "18: Encoding Loss 18.691978454589844, Transition Loss 2.1178011894226074, Classifier Loss 0.4298698306083679, Total Loss 192.9463653564453\n",
      "18: Encoding Loss 18.91973304748535, Transition Loss 2.5542755126953125, Classifier Loss 0.397298127412796, Total Loss 191.59852600097656\n",
      "18: Encoding Loss 18.356653213500977, Transition Loss 2.2403764724731445, Classifier Loss 0.40451812744140625, Total Loss 187.75311279296875\n",
      "18: Encoding Loss 19.8104248046875, Transition Loss 2.3582472801208496, Classifier Loss 0.41710972785949707, Total Loss 200.666015625\n",
      "18: Encoding Loss 19.67778778076172, Transition Loss 3.0206048488616943, Classifier Loss 0.43611180782318115, Total Loss 201.63760375976562\n",
      "18: Encoding Loss 19.68708038330078, Transition Loss 1.926995038986206, Classifier Loss 0.3547971248626709, Total Loss 193.36175537109375\n",
      "18: Encoding Loss 19.270719528198242, Transition Loss 2.653455972671509, Classifier Loss 0.39670330286026, Total Loss 194.3667755126953\n",
      "18: Encoding Loss 18.970239639282227, Transition Loss 2.932793378829956, Classifier Loss 0.42422935366630554, Total Loss 194.7714080810547\n",
      "18: Encoding Loss 19.00132179260254, Transition Loss 2.0996744632720947, Classifier Loss 0.41220200061798096, Total Loss 193.6507110595703\n",
      "18: Encoding Loss 18.992046356201172, Transition Loss 1.9824910163879395, Classifier Loss 0.3819998800754547, Total Loss 190.5328826904297\n",
      "18: Encoding Loss 18.65692901611328, Transition Loss 1.8614381551742554, Classifier Loss 0.3977494239807129, Total Loss 189.40264892578125\n",
      "18: Encoding Loss 20.33353614807129, Transition Loss 1.6895978450775146, Classifier Loss 0.3911673426628113, Total Loss 202.12294006347656\n",
      "18: Encoding Loss 19.901166915893555, Transition Loss 2.166221857070923, Classifier Loss 0.4143860340118408, Total Loss 201.0811767578125\n",
      "18: Encoding Loss 19.405622482299805, Transition Loss 2.5936577320098877, Classifier Loss 0.3949121832847595, Total Loss 195.25494384765625\n",
      "18: Encoding Loss 20.056798934936523, Transition Loss 1.6651954650878906, Classifier Loss 0.41128048300743103, Total Loss 201.9154815673828\n",
      "18: Encoding Loss 20.03875160217285, Transition Loss 1.9650540351867676, Classifier Loss 0.41748329997062683, Total Loss 202.45135498046875\n",
      "18: Encoding Loss 19.6484432220459, Transition Loss 2.196417808532715, Classifier Loss 0.38355231285095215, Total Loss 195.98207092285156\n",
      "18: Encoding Loss 18.791587829589844, Transition Loss 1.7898794412612915, Classifier Loss 0.41010114550590515, Total Loss 191.70079040527344\n",
      "18: Encoding Loss 18.826494216918945, Transition Loss 1.8523807525634766, Classifier Loss 0.3675960302352905, Total Loss 187.74203491210938\n",
      "18: Encoding Loss 18.597126007080078, Transition Loss 1.9914829730987549, Classifier Loss 0.36850860714912415, Total Loss 186.0261688232422\n",
      "18: Encoding Loss 18.872217178344727, Transition Loss 2.33384108543396, Classifier Loss 0.3797820508480072, Total Loss 189.42271423339844\n",
      "18: Encoding Loss 18.924888610839844, Transition Loss 2.5861427783966064, Classifier Loss 0.39789342880249023, Total Loss 191.70567321777344\n",
      "18: Encoding Loss 18.21267318725586, Transition Loss 2.7419726848602295, Classifier Loss 0.4362807869911194, Total Loss 189.8778839111328\n",
      "18: Encoding Loss 19.353788375854492, Transition Loss 2.2260453701019287, Classifier Loss 0.3753921389579773, Total Loss 192.81472778320312\n",
      "18: Encoding Loss 19.017122268676758, Transition Loss 2.4896559715270996, Classifier Loss 0.3654305636882782, Total Loss 189.17796325683594\n",
      "18: Encoding Loss 18.80396842956543, Transition Loss 2.3599114418029785, Classifier Loss 0.3780372738838196, Total Loss 188.70745849609375\n",
      "18: Encoding Loss 18.773778915405273, Transition Loss 2.203312873840332, Classifier Loss 0.3825516104698181, Total Loss 188.88604736328125\n",
      "18: Encoding Loss 19.794708251953125, Transition Loss 2.1322107315063477, Classifier Loss 0.39593085646629333, Total Loss 198.37718200683594\n",
      "18: Encoding Loss 20.4808406829834, Transition Loss 1.6295630931854248, Classifier Loss 0.3960561752319336, Total Loss 203.77825927734375\n",
      "18: Encoding Loss 19.2796688079834, Transition Loss 2.1815185546875, Classifier Loss 0.3917370140552521, Total Loss 193.8473663330078\n",
      "18: Encoding Loss 19.360689163208008, Transition Loss 2.1972808837890625, Classifier Loss 0.4279066026210785, Total Loss 198.11563110351562\n",
      "18: Encoding Loss 18.522859573364258, Transition Loss 2.2982685565948486, Classifier Loss 0.4014992415904999, Total Loss 188.79244995117188\n",
      "18: Encoding Loss 18.408857345581055, Transition Loss 2.369049549102783, Classifier Loss 0.3675699830055237, Total Loss 184.50167846679688\n",
      "18: Encoding Loss 19.329423904418945, Transition Loss 2.1394615173339844, Classifier Loss 0.42271852493286133, Total Loss 197.3351287841797\n",
      "18: Encoding Loss 20.52704429626465, Transition Loss 1.7859447002410889, Classifier Loss 0.39607852697372437, Total Loss 204.181396484375\n",
      "18: Encoding Loss 20.367733001708984, Transition Loss 2.3305816650390625, Classifier Loss 0.4184911251068115, Total Loss 205.25709533691406\n",
      "18: Encoding Loss 19.044593811035156, Transition Loss 2.178773880004883, Classifier Loss 0.41374894976615906, Total Loss 194.1674041748047\n",
      "18: Encoding Loss 19.82602882385254, Transition Loss 1.7522073984146118, Classifier Loss 0.39648520946502686, Total Loss 198.60719299316406\n",
      "18: Encoding Loss 18.509307861328125, Transition Loss 2.2621729373931885, Classifier Loss 0.3844934403896332, Total Loss 186.9762420654297\n",
      "18: Encoding Loss 18.907482147216797, Transition Loss 2.600301504135132, Classifier Loss 0.3644123375415802, Total Loss 188.22117614746094\n",
      "18: Encoding Loss 19.75564193725586, Transition Loss 2.0758883953094482, Classifier Loss 0.37083667516708374, Total Loss 195.5439910888672\n",
      "18: Encoding Loss 19.044445037841797, Transition Loss 3.3242948055267334, Classifier Loss 0.42596814036369324, Total Loss 195.61724853515625\n",
      "18: Encoding Loss 19.78324317932129, Transition Loss 2.12968111038208, Classifier Loss 0.3966106176376343, Total Loss 198.35293579101562\n",
      "18: Encoding Loss 19.433740615844727, Transition Loss 2.7127649784088135, Classifier Loss 0.3951607942581177, Total Loss 195.528564453125\n",
      "18: Encoding Loss 19.656641006469727, Transition Loss 2.48258376121521, Classifier Loss 0.3849570155143738, Total Loss 196.24534606933594\n",
      "18: Encoding Loss 18.844951629638672, Transition Loss 2.635921001434326, Classifier Loss 0.39966458082199097, Total Loss 191.25328063964844\n",
      "18: Encoding Loss 19.998950958251953, Transition Loss 1.5605674982070923, Classifier Loss 0.37745848298072815, Total Loss 198.04957580566406\n",
      "18: Encoding Loss 18.985929489135742, Transition Loss 2.5994460582733154, Classifier Loss 0.3879413306713104, Total Loss 191.20144653320312\n",
      "18: Encoding Loss 19.480417251586914, Transition Loss 2.0078673362731934, Classifier Loss 0.39625272154808044, Total Loss 195.8701934814453\n",
      "18: Encoding Loss 19.333354949951172, Transition Loss 2.5516085624694824, Classifier Loss 0.41925933957099915, Total Loss 197.1031036376953\n",
      "18: Encoding Loss 19.7695369720459, Transition Loss 2.309267282485962, Classifier Loss 0.3790256977081299, Total Loss 196.52072143554688\n",
      "18: Encoding Loss 20.139719009399414, Transition Loss 1.7276242971420288, Classifier Loss 0.4113231301307678, Total Loss 202.5955810546875\n",
      "18: Encoding Loss 18.72187614440918, Transition Loss 2.380957841873169, Classifier Loss 0.38003891706466675, Total Loss 188.25509643554688\n",
      "18: Encoding Loss 19.442420959472656, Transition Loss 2.3594813346862793, Classifier Loss 0.4065065085887909, Total Loss 196.6619110107422\n",
      "18: Encoding Loss 19.49439239501953, Transition Loss 2.605961322784424, Classifier Loss 0.397841215133667, Total Loss 196.2604522705078\n",
      "18: Encoding Loss 19.486865997314453, Transition Loss 2.5304553508758545, Classifier Loss 0.393540620803833, Total Loss 195.7550811767578\n",
      "18: Encoding Loss 19.763460159301758, Transition Loss 1.950516700744629, Classifier Loss 0.40400826930999756, Total Loss 198.89862060546875\n",
      "18: Encoding Loss 19.46185874938965, Transition Loss 1.9554834365844727, Classifier Loss 0.3628253936767578, Total Loss 192.36851501464844\n",
      "18: Encoding Loss 19.754261016845703, Transition Loss 2.324051856994629, Classifier Loss 0.41579368710517883, Total Loss 200.07827758789062\n",
      "18: Encoding Loss 19.624792098999023, Transition Loss 2.3204214572906494, Classifier Loss 0.37797224521636963, Total Loss 195.2596435546875\n",
      "18: Encoding Loss 20.201435089111328, Transition Loss 1.6990258693695068, Classifier Loss 0.4049852788448334, Total Loss 202.44981384277344\n",
      "18: Encoding Loss 19.692279815673828, Transition Loss 1.9740874767303467, Classifier Loss 0.3726503252983093, Total Loss 195.19808959960938\n",
      "18: Encoding Loss 18.394044876098633, Transition Loss 2.1599512100219727, Classifier Loss 0.38161423802375793, Total Loss 185.7457733154297\n",
      "18: Encoding Loss 19.713869094848633, Transition Loss 2.163350820541382, Classifier Loss 0.41463640332221985, Total Loss 199.6072540283203\n",
      "18: Encoding Loss 19.366676330566406, Transition Loss 2.116995096206665, Classifier Loss 0.4109739065170288, Total Loss 196.45419311523438\n",
      "18: Encoding Loss 18.826412200927734, Transition Loss 2.9061992168426514, Classifier Loss 0.4305953085422516, Total Loss 194.2520751953125\n",
      "18: Encoding Loss 20.42800521850586, Transition Loss 1.833116054534912, Classifier Loss 0.4007749855518341, Total Loss 203.86817932128906\n",
      "18: Encoding Loss 19.796403884887695, Transition Loss 2.108048439025879, Classifier Loss 0.41472384333610535, Total Loss 200.26522827148438\n",
      "18: Encoding Loss 19.64196014404297, Transition Loss 1.9302245378494263, Classifier Loss 0.39429861307144165, Total Loss 196.95159912109375\n",
      "18: Encoding Loss 18.705366134643555, Transition Loss 2.8239171504974365, Classifier Loss 0.39548659324645996, Total Loss 189.75637817382812\n",
      "18: Encoding Loss 19.649999618530273, Transition Loss 2.848310947418213, Classifier Loss 0.3869496285915375, Total Loss 196.46461486816406\n",
      "18: Encoding Loss 19.125286102294922, Transition Loss 2.483316421508789, Classifier Loss 0.3969833254814148, Total Loss 193.19729614257812\n",
      "18: Encoding Loss 17.929241180419922, Transition Loss 3.2637135982513428, Classifier Loss 0.35541680455207825, Total Loss 179.62835693359375\n",
      "18: Encoding Loss 19.230043411254883, Transition Loss 2.3595361709594727, Classifier Loss 0.405611127614975, Total Loss 194.8733673095703\n",
      "18: Encoding Loss 19.337892532348633, Transition Loss 2.430265188217163, Classifier Loss 0.3867824375629425, Total Loss 193.867431640625\n",
      "18: Encoding Loss 17.841218948364258, Transition Loss 2.8285908699035645, Classifier Loss 0.35920804738998413, Total Loss 179.21627807617188\n",
      "18: Encoding Loss 19.51119041442871, Transition Loss 2.678257703781128, Classifier Loss 0.3727837800979614, Total Loss 193.90354919433594\n",
      "18: Encoding Loss 19.120573043823242, Transition Loss 2.447845935821533, Classifier Loss 0.36496007442474365, Total Loss 189.9501495361328\n",
      "18: Encoding Loss 20.058589935302734, Transition Loss 2.13562273979187, Classifier Loss 0.38859108090400696, Total Loss 199.75497436523438\n",
      "18: Encoding Loss 19.0772705078125, Transition Loss 2.487826108932495, Classifier Loss 0.3981156051158905, Total Loss 192.92727661132812\n",
      "18: Encoding Loss 19.963239669799805, Transition Loss 1.9038546085357666, Classifier Loss 0.4059818685054779, Total Loss 200.68487548828125\n",
      "18: Encoding Loss 19.06439208984375, Transition Loss 2.2631568908691406, Classifier Loss 0.4039630591869354, Total Loss 193.36407470703125\n",
      "18: Encoding Loss 20.815576553344727, Transition Loss 2.4579415321350098, Classifier Loss 0.40829914808273315, Total Loss 207.8461151123047\n",
      "18: Encoding Loss 19.619762420654297, Transition Loss 1.9314498901367188, Classifier Loss 0.39646482467651367, Total Loss 196.9908905029297\n",
      "18: Encoding Loss 18.444934844970703, Transition Loss 2.258760929107666, Classifier Loss 0.4068838655948639, Total Loss 188.69961547851562\n",
      "18: Encoding Loss 19.138071060180664, Transition Loss 2.524587631225586, Classifier Loss 0.3665130138397217, Total Loss 190.2607879638672\n",
      "18: Encoding Loss 19.378684997558594, Transition Loss 2.1242034435272217, Classifier Loss 0.39648333191871643, Total Loss 195.10264587402344\n",
      "18: Encoding Loss 18.754663467407227, Transition Loss 1.709614872932434, Classifier Loss 0.3748282492160797, Total Loss 187.862060546875\n",
      "18: Encoding Loss 18.66171646118164, Transition Loss 2.1460800170898438, Classifier Loss 0.38284921646118164, Total Loss 188.0078582763672\n",
      "18: Encoding Loss 19.50261688232422, Transition Loss 2.7032787799835205, Classifier Loss 0.40631046891212463, Total Loss 197.192626953125\n",
      "18: Encoding Loss 19.414052963256836, Transition Loss 1.6554205417633057, Classifier Loss 0.3676944375038147, Total Loss 192.41294860839844\n",
      "18: Encoding Loss 19.84332275390625, Transition Loss 1.547507405281067, Classifier Loss 0.39936575293540955, Total Loss 198.99266052246094\n",
      "18: Encoding Loss 18.433868408203125, Transition Loss 2.815739154815674, Classifier Loss 0.35735124349594116, Total Loss 183.7692108154297\n",
      "18: Encoding Loss 18.949323654174805, Transition Loss 3.038471221923828, Classifier Loss 0.3883015215396881, Total Loss 191.03244018554688\n",
      "18: Encoding Loss 18.829261779785156, Transition Loss 2.710907220840454, Classifier Loss 0.3759680688381195, Total Loss 188.7730712890625\n",
      "18: Encoding Loss 19.359811782836914, Transition Loss 2.7624173164367676, Classifier Loss 0.39447665214538574, Total Loss 194.87864685058594\n",
      "18: Encoding Loss 19.106571197509766, Transition Loss 3.5396766662597656, Classifier Loss 0.4054226577281952, Total Loss 194.10276794433594\n",
      "18: Encoding Loss 19.7359676361084, Transition Loss 2.145336389541626, Classifier Loss 0.375118225812912, Total Loss 195.82862854003906\n",
      "18: Encoding Loss 19.400959014892578, Transition Loss 2.0397751331329346, Classifier Loss 0.35772958397865295, Total Loss 191.38858032226562\n",
      "18: Encoding Loss 18.91785430908203, Transition Loss 2.6675500869750977, Classifier Loss 0.42821750044822693, Total Loss 194.69808959960938\n",
      "18: Encoding Loss 18.901123046875, Transition Loss 2.289397716522217, Classifier Loss 0.39062172174453735, Total Loss 190.72903442382812\n",
      "18: Encoding Loss 18.719722747802734, Transition Loss 2.7126872539520264, Classifier Loss 0.38756847381591797, Total Loss 189.05718994140625\n",
      "18: Encoding Loss 18.70326805114746, Transition Loss 2.326066017150879, Classifier Loss 0.3805633783340454, Total Loss 188.14768981933594\n",
      "18: Encoding Loss 18.944438934326172, Transition Loss 1.9935028553009033, Classifier Loss 0.382041335105896, Total Loss 190.15835571289062\n",
      "18: Encoding Loss 19.178712844848633, Transition Loss 2.016496419906616, Classifier Loss 0.4077090620994568, Total Loss 194.60391235351562\n",
      "18: Encoding Loss 18.118743896484375, Transition Loss 2.644202709197998, Classifier Loss 0.38756853342056274, Total Loss 184.2356414794922\n",
      "18: Encoding Loss 18.110706329345703, Transition Loss 2.71010160446167, Classifier Loss 0.3480517864227295, Total Loss 180.23284912109375\n",
      "18: Encoding Loss 20.67090606689453, Transition Loss 2.01047420501709, Classifier Loss 0.4386409521102905, Total Loss 209.63343811035156\n",
      "18: Encoding Loss 19.146738052368164, Transition Loss 3.0508363246917725, Classifier Loss 0.3788677752017975, Total Loss 191.6708526611328\n",
      "18: Encoding Loss 19.71395492553711, Transition Loss 2.338944911956787, Classifier Loss 0.4009227752685547, Total Loss 198.27171325683594\n",
      "18: Encoding Loss 19.667478561401367, Transition Loss 2.731485605239868, Classifier Loss 0.4193557798862457, Total Loss 199.8217010498047\n",
      "18: Encoding Loss 19.41437530517578, Transition Loss 2.3069405555725098, Classifier Loss 0.39796528220176697, Total Loss 195.5729217529297\n",
      "18: Encoding Loss 18.59372901916504, Transition Loss 2.7163238525390625, Classifier Loss 0.38481616973876953, Total Loss 187.7747039794922\n",
      "18: Encoding Loss 20.25456428527832, Transition Loss 1.4651540517807007, Classifier Loss 0.407798707485199, Total Loss 203.10940551757812\n",
      "18: Encoding Loss 18.542573928833008, Transition Loss 1.767031192779541, Classifier Loss 0.3887712359428406, Total Loss 187.5711212158203\n",
      "18: Encoding Loss 19.268348693847656, Transition Loss 2.304961919784546, Classifier Loss 0.3958476483821869, Total Loss 194.1925506591797\n",
      "18: Encoding Loss 18.153745651245117, Transition Loss 2.7359580993652344, Classifier Loss 0.3742356598377228, Total Loss 183.2007293701172\n",
      "18: Encoding Loss 18.718509674072266, Transition Loss 1.946508526802063, Classifier Loss 0.3784990608692169, Total Loss 187.98728942871094\n",
      "18: Encoding Loss 19.894044876098633, Transition Loss 3.059389114379883, Classifier Loss 0.37199023365974426, Total Loss 196.9632568359375\n",
      "18: Encoding Loss 19.153108596801758, Transition Loss 2.046053409576416, Classifier Loss 0.36559754610061646, Total Loss 190.19383239746094\n",
      "18: Encoding Loss 20.639802932739258, Transition Loss 2.0827555656433105, Classifier Loss 0.37001585960388184, Total Loss 202.53656005859375\n",
      "18: Encoding Loss 20.40033721923828, Transition Loss 2.1509969234466553, Classifier Loss 0.4455496072769165, Total Loss 208.1878662109375\n",
      "18: Encoding Loss 19.429990768432617, Transition Loss 3.0612988471984863, Classifier Loss 0.3898078203201294, Total Loss 195.03297424316406\n",
      "18: Encoding Loss 18.015010833740234, Transition Loss 3.777648687362671, Classifier Loss 0.3864111006259918, Total Loss 183.51673889160156\n",
      "19: Encoding Loss 19.827600479125977, Transition Loss 2.037343978881836, Classifier Loss 0.4085952639579773, Total Loss 199.88780212402344\n",
      "19: Encoding Loss 19.730222702026367, Transition Loss 1.8998949527740479, Classifier Loss 0.38619160652160645, Total Loss 196.84091186523438\n",
      "19: Encoding Loss 20.28354263305664, Transition Loss 1.9869762659072876, Classifier Loss 0.3949922025203705, Total Loss 202.1649627685547\n",
      "19: Encoding Loss 19.293630599975586, Transition Loss 1.9451310634613037, Classifier Loss 0.4328136146068573, Total Loss 198.01942443847656\n",
      "19: Encoding Loss 20.586402893066406, Transition Loss 1.5165300369262695, Classifier Loss 0.4188332259654999, Total Loss 206.8778533935547\n",
      "19: Encoding Loss 18.40476417541504, Transition Loss 2.966501474380493, Classifier Loss 0.39879313111305237, Total Loss 187.7107391357422\n",
      "19: Encoding Loss 19.776458740234375, Transition Loss 1.5673925876617432, Classifier Loss 0.4115135371685028, Total Loss 199.67649841308594\n",
      "19: Encoding Loss 20.826419830322266, Transition Loss 1.463942050933838, Classifier Loss 0.4261789917945862, Total Loss 209.5220489501953\n",
      "19: Encoding Loss 18.969911575317383, Transition Loss 2.1155900955200195, Classifier Loss 0.4152209162712097, Total Loss 193.70449829101562\n",
      "19: Encoding Loss 20.904449462890625, Transition Loss 1.6507289409637451, Classifier Loss 0.4171442985534668, Total Loss 209.28016662597656\n",
      "19: Encoding Loss 19.57120704650879, Transition Loss 1.8259902000427246, Classifier Loss 0.3897760808467865, Total Loss 195.9124755859375\n",
      "19: Encoding Loss 19.384578704833984, Transition Loss 2.015113353729248, Classifier Loss 0.3841446340084076, Total Loss 193.8941192626953\n",
      "19: Encoding Loss 18.842432022094727, Transition Loss 2.0285778045654297, Classifier Loss 0.3963257074356079, Total Loss 190.77774047851562\n",
      "19: Encoding Loss 19.351423263549805, Transition Loss 2.342404842376709, Classifier Loss 0.3935965895652771, Total Loss 194.6395263671875\n",
      "19: Encoding Loss 18.763362884521484, Transition Loss 2.413433074951172, Classifier Loss 0.3913615942001343, Total Loss 189.7257537841797\n",
      "19: Encoding Loss 19.559986114501953, Transition Loss 2.315545082092285, Classifier Loss 0.3793909549713135, Total Loss 194.882080078125\n",
      "19: Encoding Loss 19.244346618652344, Transition Loss 2.3610215187072754, Classifier Loss 0.3803402781486511, Total Loss 192.46099853515625\n",
      "19: Encoding Loss 19.21533203125, Transition Loss 2.2126450538635254, Classifier Loss 0.37264499068260193, Total Loss 191.4296875\n",
      "19: Encoding Loss 19.42958641052246, Transition Loss 2.0620107650756836, Classifier Loss 0.3566581606864929, Total Loss 191.51490783691406\n",
      "19: Encoding Loss 20.441173553466797, Transition Loss 1.9721564054489136, Classifier Loss 0.37975120544433594, Total Loss 201.89894104003906\n",
      "19: Encoding Loss 19.42540168762207, Transition Loss 2.5440988540649414, Classifier Loss 0.3836725354194641, Total Loss 194.27928161621094\n",
      "19: Encoding Loss 18.878929138183594, Transition Loss 2.0738673210144043, Classifier Loss 0.3833715617656708, Total Loss 189.7833709716797\n",
      "19: Encoding Loss 18.629274368286133, Transition Loss 3.32238507270813, Classifier Loss 0.3845323324203491, Total Loss 188.1519012451172\n",
      "19: Encoding Loss 18.556358337402344, Transition Loss 2.1207311153411865, Classifier Loss 0.4049156606197357, Total Loss 189.3665771484375\n",
      "19: Encoding Loss 19.40027618408203, Transition Loss 2.031630516052246, Classifier Loss 0.3704359531402588, Total Loss 192.65213012695312\n",
      "19: Encoding Loss 19.98373794555664, Transition Loss 2.24159574508667, Classifier Loss 0.3818342089653015, Total Loss 198.50164794921875\n",
      "19: Encoding Loss 20.263044357299805, Transition Loss 2.1214730739593506, Classifier Loss 0.4411318004131317, Total Loss 206.64183044433594\n",
      "19: Encoding Loss 19.274805068969727, Transition Loss 2.3904597759246826, Classifier Loss 0.39037683606147766, Total Loss 193.71421813964844\n",
      "19: Encoding Loss 18.89900016784668, Transition Loss 2.3018178939819336, Classifier Loss 0.4172438383102417, Total Loss 193.37673950195312\n",
      "19: Encoding Loss 19.02934455871582, Transition Loss 2.95378041267395, Classifier Loss 0.39385825395584106, Total Loss 192.21133422851562\n",
      "19: Encoding Loss 19.49089813232422, Transition Loss 2.389711618423462, Classifier Loss 0.3735964000225067, Total Loss 193.76475524902344\n",
      "19: Encoding Loss 18.005943298339844, Transition Loss 2.841390371322632, Classifier Loss 0.3847610354423523, Total Loss 183.09193420410156\n",
      "19: Encoding Loss 19.69577980041504, Transition Loss 2.3178365230560303, Classifier Loss 0.4299306869506836, Total Loss 201.0228729248047\n",
      "19: Encoding Loss 18.64557647705078, Transition Loss 2.1641995906829834, Classifier Loss 0.40222299098968506, Total Loss 189.8197479248047\n",
      "19: Encoding Loss 18.780447006225586, Transition Loss 2.519075870513916, Classifier Loss 0.37928158044815063, Total Loss 188.67555236816406\n",
      "19: Encoding Loss 18.1032657623291, Transition Loss 2.2520816326141357, Classifier Loss 0.3953956067562103, Total Loss 184.81610107421875\n",
      "19: Encoding Loss 19.20592498779297, Transition Loss 2.2824244499206543, Classifier Loss 0.3815745711326599, Total Loss 192.26133728027344\n",
      "19: Encoding Loss 19.76660919189453, Transition Loss 2.870777130126953, Classifier Loss 0.4060930609703064, Total Loss 199.31634521484375\n",
      "19: Encoding Loss 19.385637283325195, Transition Loss 1.8920845985412598, Classifier Loss 0.38412249088287354, Total Loss 193.87576293945312\n",
      "19: Encoding Loss 19.273710250854492, Transition Loss 2.571866989135742, Classifier Loss 0.4075733423233032, Total Loss 195.46139526367188\n",
      "19: Encoding Loss 18.95511817932129, Transition Loss 2.839137077331543, Classifier Loss 0.3865731358528137, Total Loss 190.8660888671875\n",
      "19: Encoding Loss 18.61562156677246, Transition Loss 2.0627355575561523, Classifier Loss 0.3881705105304718, Total Loss 188.15457153320312\n",
      "19: Encoding Loss 18.993770599365234, Transition Loss 1.967755675315857, Classifier Loss 0.3769000768661499, Total Loss 190.0337371826172\n",
      "19: Encoding Loss 18.525684356689453, Transition Loss 1.8646132946014404, Classifier Loss 0.3971118927001953, Total Loss 188.28958129882812\n",
      "19: Encoding Loss 20.081256866455078, Transition Loss 1.7000274658203125, Classifier Loss 0.3835122585296631, Total Loss 199.34129333496094\n",
      "19: Encoding Loss 20.083106994628906, Transition Loss 2.081045389175415, Classifier Loss 0.4098937511444092, Total Loss 202.07044982910156\n",
      "19: Encoding Loss 19.501487731933594, Transition Loss 2.5673604011535645, Classifier Loss 0.40486639738082886, Total Loss 197.0120086669922\n",
      "19: Encoding Loss 20.11962127685547, Transition Loss 1.6891660690307617, Classifier Loss 0.41594183444976807, Total Loss 202.88897705078125\n",
      "19: Encoding Loss 19.64923858642578, Transition Loss 2.0102999210357666, Classifier Loss 0.39958587288856506, Total Loss 197.55455017089844\n",
      "19: Encoding Loss 19.491525650024414, Transition Loss 2.2055792808532715, Classifier Loss 0.3851029872894287, Total Loss 194.8836212158203\n",
      "19: Encoding Loss 18.90679168701172, Transition Loss 1.841651201248169, Classifier Loss 0.41851142048835754, Total Loss 193.4738006591797\n",
      "19: Encoding Loss 18.852989196777344, Transition Loss 1.8982497453689575, Classifier Loss 0.3714759945869446, Total Loss 188.35116577148438\n",
      "19: Encoding Loss 18.308712005615234, Transition Loss 2.0459485054016113, Classifier Loss 0.39733782410621643, Total Loss 186.61268615722656\n",
      "19: Encoding Loss 18.983028411865234, Transition Loss 2.3676137924194336, Classifier Loss 0.38957852125167847, Total Loss 191.29562377929688\n",
      "19: Encoding Loss 18.639598846435547, Transition Loss 2.673671007156372, Classifier Loss 0.38284963369369507, Total Loss 187.93649291992188\n",
      "19: Encoding Loss 17.887622833251953, Transition Loss 2.7433969974517822, Classifier Loss 0.3838724195957184, Total Loss 182.03689575195312\n",
      "19: Encoding Loss 19.08230209350586, Transition Loss 2.212156057357788, Classifier Loss 0.38126903772354126, Total Loss 191.22776794433594\n",
      "19: Encoding Loss 18.873323440551758, Transition Loss 2.475278854370117, Classifier Loss 0.3872400224208832, Total Loss 190.2056427001953\n",
      "19: Encoding Loss 18.85863494873047, Transition Loss 2.3848626613616943, Classifier Loss 0.36416277289390564, Total Loss 187.7623291015625\n",
      "19: Encoding Loss 18.674291610717773, Transition Loss 2.2259371280670166, Classifier Loss 0.3848768472671509, Total Loss 188.32720947265625\n",
      "19: Encoding Loss 19.652812957763672, Transition Loss 2.2330880165100098, Classifier Loss 0.4103829860687256, Total Loss 198.7074432373047\n",
      "19: Encoding Loss 20.530559539794922, Transition Loss 1.6839731931686401, Classifier Loss 0.417115718126297, Total Loss 206.2928466796875\n",
      "19: Encoding Loss 19.462976455688477, Transition Loss 2.216623067855835, Classifier Loss 0.3882855772972107, Total Loss 194.97569274902344\n",
      "19: Encoding Loss 19.172931671142578, Transition Loss 2.2037596702575684, Classifier Loss 0.3890641927719116, Total Loss 192.73062133789062\n",
      "19: Encoding Loss 18.44281578063965, Transition Loss 2.349252700805664, Classifier Loss 0.3875250518321991, Total Loss 186.76487731933594\n",
      "19: Encoding Loss 18.280824661254883, Transition Loss 2.399261713027954, Classifier Loss 0.39531761407852173, Total Loss 186.25820922851562\n",
      "19: Encoding Loss 19.182861328125, Transition Loss 2.1656436920166016, Classifier Loss 0.38329172134399414, Total Loss 192.22518920898438\n",
      "19: Encoding Loss 20.637653350830078, Transition Loss 1.769633412361145, Classifier Loss 0.3621772527694702, Total Loss 201.67288208007812\n",
      "19: Encoding Loss 20.403911590576172, Transition Loss 2.311992645263672, Classifier Loss 0.41816067695617676, Total Loss 205.50978088378906\n",
      "19: Encoding Loss 19.3321475982666, Transition Loss 2.131277322769165, Classifier Loss 0.4197244346141815, Total Loss 197.05587768554688\n",
      "19: Encoding Loss 19.866010665893555, Transition Loss 1.8017315864562988, Classifier Loss 0.36277297139167786, Total Loss 195.56573486328125\n",
      "19: Encoding Loss 18.55712127685547, Transition Loss 2.216470718383789, Classifier Loss 0.3668709397315979, Total Loss 185.58737182617188\n",
      "19: Encoding Loss 18.5457763671875, Transition Loss 2.62477970123291, Classifier Loss 0.39233076572418213, Total Loss 188.12425231933594\n",
      "19: Encoding Loss 19.583290100097656, Transition Loss 2.0207784175872803, Classifier Loss 0.37074318528175354, Total Loss 194.14479064941406\n",
      "19: Encoding Loss 18.861997604370117, Transition Loss 3.240396738052368, Classifier Loss 0.4149344265460968, Total Loss 193.03750610351562\n",
      "19: Encoding Loss 19.727460861206055, Transition Loss 2.040004253387451, Classifier Loss 0.3854038417339325, Total Loss 196.76808166503906\n",
      "19: Encoding Loss 19.481081008911133, Transition Loss 2.6627821922302246, Classifier Loss 0.3580755591392517, Total Loss 192.1887664794922\n",
      "19: Encoding Loss 19.278324127197266, Transition Loss 2.405287027359009, Classifier Loss 0.3995491862297058, Total Loss 194.66258239746094\n",
      "19: Encoding Loss 18.882837295532227, Transition Loss 2.4871201515197754, Classifier Loss 0.39282506704330444, Total Loss 190.84262084960938\n",
      "19: Encoding Loss 20.243711471557617, Transition Loss 1.4872723817825317, Classifier Loss 0.3785715699195862, Total Loss 200.10430908203125\n",
      "19: Encoding Loss 18.76462173461914, Transition Loss 2.512021064758301, Classifier Loss 0.37094423174858093, Total Loss 187.71380615234375\n",
      "19: Encoding Loss 19.28756332397461, Transition Loss 1.9605101346969604, Classifier Loss 0.39838510751724243, Total Loss 194.53114318847656\n",
      "19: Encoding Loss 18.973997116088867, Transition Loss 2.4653029441833496, Classifier Loss 0.384426474571228, Total Loss 190.72767639160156\n",
      "19: Encoding Loss 19.80345916748047, Transition Loss 2.220646381378174, Classifier Loss 0.41105830669403076, Total Loss 199.97763061523438\n",
      "19: Encoding Loss 20.008995056152344, Transition Loss 1.636917233467102, Classifier Loss 0.39201247692108154, Total Loss 199.6005859375\n",
      "19: Encoding Loss 18.60285186767578, Transition Loss 2.304882287979126, Classifier Loss 0.3374655246734619, Total Loss 183.0303497314453\n",
      "19: Encoding Loss 19.403818130493164, Transition Loss 2.216658115386963, Classifier Loss 0.4178953468799591, Total Loss 197.46340942382812\n",
      "19: Encoding Loss 19.459144592285156, Transition Loss 2.4636549949645996, Classifier Loss 0.39507192373275757, Total Loss 195.67308044433594\n",
      "19: Encoding Loss 19.19734764099121, Transition Loss 2.3730478286743164, Classifier Loss 0.3938749432563782, Total Loss 193.44088745117188\n",
      "19: Encoding Loss 19.302974700927734, Transition Loss 1.8985414505004883, Classifier Loss 0.4125000536441803, Total Loss 196.05352783203125\n",
      "19: Encoding Loss 19.52081871032715, Transition Loss 1.8270313739776611, Classifier Loss 0.39278531074523926, Total Loss 195.81048583984375\n",
      "19: Encoding Loss 19.586877822875977, Transition Loss 2.1845970153808594, Classifier Loss 0.43818721175193787, Total Loss 200.95066833496094\n",
      "19: Encoding Loss 19.302087783813477, Transition Loss 2.1676034927368164, Classifier Loss 0.38268643617630005, Total Loss 193.11886596679688\n",
      "19: Encoding Loss 20.049785614013672, Transition Loss 1.5876197814941406, Classifier Loss 0.35920941829681396, Total Loss 196.6367645263672\n",
      "19: Encoding Loss 19.406354904174805, Transition Loss 1.8773468732833862, Classifier Loss 0.3701155483722687, Total Loss 192.6378631591797\n",
      "19: Encoding Loss 18.55001449584961, Transition Loss 2.0803675651550293, Classifier Loss 0.40243399143218994, Total Loss 189.05960083007812\n",
      "19: Encoding Loss 19.635475158691406, Transition Loss 2.013134479522705, Classifier Loss 0.4061240553855896, Total Loss 198.0988311767578\n",
      "19: Encoding Loss 19.27037239074707, Transition Loss 2.022242307662964, Classifier Loss 0.430735319852829, Total Loss 197.64096069335938\n",
      "19: Encoding Loss 18.351558685302734, Transition Loss 2.7525289058685303, Classifier Loss 0.39663323760032654, Total Loss 187.02630615234375\n",
      "19: Encoding Loss 20.20722770690918, Transition Loss 1.7432119846343994, Classifier Loss 0.38664862513542175, Total Loss 200.6713409423828\n",
      "19: Encoding Loss 19.738264083862305, Transition Loss 1.9783236980438232, Classifier Loss 0.4007103145122528, Total Loss 198.372802734375\n",
      "19: Encoding Loss 19.417951583862305, Transition Loss 1.7465300559997559, Classifier Loss 0.3975021243095398, Total Loss 195.44313049316406\n",
      "19: Encoding Loss 18.81804847717285, Transition Loss 2.612821578979492, Classifier Loss 0.38274645805358887, Total Loss 189.3415985107422\n",
      "19: Encoding Loss 19.50356101989746, Transition Loss 2.620870590209961, Classifier Loss 0.3803161680698395, Total Loss 194.5842742919922\n",
      "19: Encoding Loss 18.897823333740234, Transition Loss 2.3089351654052734, Classifier Loss 0.3766455352306366, Total Loss 189.30894470214844\n",
      "19: Encoding Loss 17.79859733581543, Transition Loss 2.937070608139038, Classifier Loss 0.3407508134841919, Total Loss 177.05128479003906\n",
      "19: Encoding Loss 18.83201789855957, Transition Loss 2.168719530105591, Classifier Loss 0.38642749190330505, Total Loss 189.73263549804688\n",
      "19: Encoding Loss 19.588342666625977, Transition Loss 2.159991502761841, Classifier Loss 0.38654547929763794, Total Loss 195.7932891845703\n",
      "19: Encoding Loss 17.76418113708496, Transition Loss 2.6798465251922607, Classifier Loss 0.372847318649292, Total Loss 179.93414306640625\n",
      "19: Encoding Loss 19.424585342407227, Transition Loss 2.4595959186553955, Classifier Loss 0.3874415159225464, Total Loss 194.63275146484375\n",
      "19: Encoding Loss 19.520586013793945, Transition Loss 2.297236919403076, Classifier Loss 0.3833429217338562, Total Loss 194.9584197998047\n",
      "19: Encoding Loss 19.92106056213379, Transition Loss 2.012017250061035, Classifier Loss 0.4061281979084015, Total Loss 200.3837127685547\n",
      "19: Encoding Loss 18.88017463684082, Transition Loss 2.3367345333099365, Classifier Loss 0.3865422010421753, Total Loss 190.1629638671875\n",
      "19: Encoding Loss 19.654638290405273, Transition Loss 1.7986043691635132, Classifier Loss 0.3612942099571228, Total Loss 193.72625732421875\n",
      "19: Encoding Loss 18.78131675720215, Transition Loss 2.134671688079834, Classifier Loss 0.404308021068573, Total Loss 191.1082763671875\n",
      "19: Encoding Loss 20.703086853027344, Transition Loss 2.199103355407715, Classifier Loss 0.39423584938049316, Total Loss 205.48809814453125\n",
      "19: Encoding Loss 19.33963966369629, Transition Loss 1.7256990671157837, Classifier Loss 0.41981375217437744, Total Loss 197.0436248779297\n",
      "19: Encoding Loss 18.440887451171875, Transition Loss 2.042623996734619, Classifier Loss 0.37402376532554626, Total Loss 185.33799743652344\n",
      "19: Encoding Loss 18.80808448791504, Transition Loss 2.295900583267212, Classifier Loss 0.39031919836997986, Total Loss 189.95578002929688\n",
      "19: Encoding Loss 19.170488357543945, Transition Loss 1.9208587408065796, Classifier Loss 0.3981243073940277, Total Loss 193.5605010986328\n",
      "19: Encoding Loss 18.81992530822754, Transition Loss 1.6082357168197632, Classifier Loss 0.35994184017181396, Total Loss 186.87522888183594\n",
      "19: Encoding Loss 18.58490562438965, Transition Loss 2.036787986755371, Classifier Loss 0.3808906078338623, Total Loss 187.17567443847656\n",
      "19: Encoding Loss 19.42011070251465, Transition Loss 2.4705586433410645, Classifier Loss 0.39408230781555176, Total Loss 195.2632293701172\n",
      "19: Encoding Loss 19.403350830078125, Transition Loss 1.564004898071289, Classifier Loss 0.387526273727417, Total Loss 194.292236328125\n",
      "19: Encoding Loss 19.625043869018555, Transition Loss 1.45218825340271, Classifier Loss 0.40284785628318787, Total Loss 197.57557678222656\n",
      "19: Encoding Loss 18.335060119628906, Transition Loss 2.616166591644287, Classifier Loss 0.36796048283576965, Total Loss 183.99977111816406\n",
      "19: Encoding Loss 18.938642501831055, Transition Loss 2.83125376701355, Classifier Loss 0.3996851444244385, Total Loss 192.04391479492188\n",
      "19: Encoding Loss 18.608394622802734, Transition Loss 2.498751401901245, Classifier Loss 0.3954470157623291, Total Loss 188.91162109375\n",
      "19: Encoding Loss 19.458759307861328, Transition Loss 2.507822275161743, Classifier Loss 0.41559067368507385, Total Loss 197.730712890625\n",
      "19: Encoding Loss 18.59813117980957, Transition Loss 3.203084945678711, Classifier Loss 0.3916482627391815, Total Loss 188.59048461914062\n",
      "19: Encoding Loss 19.716148376464844, Transition Loss 1.9555772542953491, Classifier Loss 0.36123520135879517, Total Loss 194.2438201904297\n",
      "19: Encoding Loss 19.58329963684082, Transition Loss 1.8980512619018555, Classifier Loss 0.37480294704437256, Total Loss 194.52630615234375\n",
      "19: Encoding Loss 18.6606502532959, Transition Loss 2.51007342338562, Classifier Loss 0.3689224421977997, Total Loss 186.67945861816406\n",
      "19: Encoding Loss 18.91395378112793, Transition Loss 2.133307933807373, Classifier Loss 0.4012921154499054, Total Loss 191.8675079345703\n",
      "19: Encoding Loss 18.70001220703125, Transition Loss 2.4964487552642822, Classifier Loss 0.3727056384086609, Total Loss 187.3699493408203\n",
      "19: Encoding Loss 18.720653533935547, Transition Loss 2.1119606494903564, Classifier Loss 0.35701435804367065, Total Loss 185.88906860351562\n",
      "19: Encoding Loss 18.916601181030273, Transition Loss 1.851775884628296, Classifier Loss 0.37235790491104126, Total Loss 188.93896484375\n",
      "19: Encoding Loss 18.968751907348633, Transition Loss 1.8465937376022339, Classifier Loss 0.40812838077545166, Total Loss 192.9321746826172\n",
      "19: Encoding Loss 17.609256744384766, Transition Loss 2.523508310317993, Classifier Loss 0.3468720316886902, Total Loss 176.06594848632812\n",
      "19: Encoding Loss 17.965065002441406, Transition Loss 2.4794368743896484, Classifier Loss 0.34394973516464233, Total Loss 178.6113739013672\n",
      "19: Encoding Loss 20.87784767150879, Transition Loss 1.8717771768569946, Classifier Loss 0.3981873095035553, Total Loss 207.2158660888672\n",
      "19: Encoding Loss 18.97450065612793, Transition Loss 2.732090711593628, Classifier Loss 0.4338928163051605, Total Loss 195.73170471191406\n",
      "19: Encoding Loss 19.60065460205078, Transition Loss 2.157647132873535, Classifier Loss 0.3814018666744232, Total Loss 195.376953125\n",
      "19: Encoding Loss 19.773868560791016, Transition Loss 2.416356325149536, Classifier Loss 0.42061758041381836, Total Loss 200.73599243164062\n",
      "19: Encoding Loss 19.313440322875977, Transition Loss 2.163177490234375, Classifier Loss 0.4134664237499237, Total Loss 196.2867889404297\n",
      "19: Encoding Loss 18.3258113861084, Transition Loss 2.503164291381836, Classifier Loss 0.39506399631500244, Total Loss 186.61351013183594\n",
      "19: Encoding Loss 20.02776527404785, Transition Loss 1.3829154968261719, Classifier Loss 0.37265846133232117, Total Loss 197.76455688476562\n",
      "19: Encoding Loss 18.716524124145508, Transition Loss 1.7079856395721436, Classifier Loss 0.4396091103553772, Total Loss 194.03469848632812\n",
      "19: Encoding Loss 19.08224868774414, Transition Loss 2.235482692718506, Classifier Loss 0.3910818099975586, Total Loss 192.21327209472656\n",
      "19: Encoding Loss 17.657684326171875, Transition Loss 2.512101888656616, Classifier Loss 0.36765772104263306, Total Loss 178.52967834472656\n",
      "19: Encoding Loss 18.166898727416992, Transition Loss 1.9165170192718506, Classifier Loss 0.34773483872413635, Total Loss 180.49197387695312\n",
      "19: Encoding Loss 19.812530517578125, Transition Loss 2.5639047622680664, Classifier Loss 0.3806467056274414, Total Loss 197.07769775390625\n",
      "19: Encoding Loss 19.038848876953125, Transition Loss 1.9457886219024658, Classifier Loss 0.3896324038505554, Total Loss 191.66319274902344\n",
      "19: Encoding Loss 20.758298873901367, Transition Loss 1.785508632659912, Classifier Loss 0.3854873776435852, Total Loss 204.97222900390625\n",
      "19: Encoding Loss 20.166259765625, Transition Loss 2.039294481277466, Classifier Loss 0.4158864915370941, Total Loss 203.3265838623047\n",
      "19: Encoding Loss 19.356021881103516, Transition Loss 2.5595579147338867, Classifier Loss 0.4069083034992218, Total Loss 196.05091857910156\n",
      "19: Encoding Loss 18.110984802246094, Transition Loss 3.5758919715881348, Classifier Loss 0.3995930850505829, Total Loss 185.56236267089844\n",
      "20: Encoding Loss 19.532642364501953, Transition Loss 1.863478660583496, Classifier Loss 0.3734346330165863, Total Loss 193.977294921875\n",
      "20: Encoding Loss 19.564159393310547, Transition Loss 1.8564281463623047, Classifier Loss 0.3845970034599304, Total Loss 195.3442840576172\n",
      "20: Encoding Loss 20.136117935180664, Transition Loss 1.9195764064788818, Classifier Loss 0.38810399174690247, Total Loss 200.2832489013672\n",
      "20: Encoding Loss 18.998605728149414, Transition Loss 1.826301097869873, Classifier Loss 0.3975517749786377, Total Loss 192.10928344726562\n",
      "20: Encoding Loss 20.341190338134766, Transition Loss 1.5126944780349731, Classifier Loss 0.3977290391921997, Total Loss 202.80496215820312\n",
      "20: Encoding Loss 18.166536331176758, Transition Loss 2.7341184616088867, Classifier Loss 0.42988234758377075, Total Loss 188.8673553466797\n",
      "20: Encoding Loss 19.90675926208496, Transition Loss 1.5919092893600464, Classifier Loss 0.4169970154762268, Total Loss 201.27215576171875\n",
      "20: Encoding Loss 20.859647750854492, Transition Loss 1.3769948482513428, Classifier Loss 0.39749857783317566, Total Loss 206.90245056152344\n",
      "20: Encoding Loss 19.365673065185547, Transition Loss 2.1170713901519775, Classifier Loss 0.41853028535842896, Total Loss 197.20184326171875\n",
      "20: Encoding Loss 20.74404525756836, Transition Loss 1.5695068836212158, Classifier Loss 0.41739410161972046, Total Loss 208.0056915283203\n",
      "20: Encoding Loss 19.497568130493164, Transition Loss 1.8308134078979492, Classifier Loss 0.37783148884773254, Total Loss 194.12986755371094\n",
      "20: Encoding Loss 19.233097076416016, Transition Loss 1.943690538406372, Classifier Loss 0.392932653427124, Total Loss 193.54678344726562\n",
      "20: Encoding Loss 18.679624557495117, Transition Loss 1.9535725116729736, Classifier Loss 0.3835790157318115, Total Loss 188.18560791015625\n",
      "20: Encoding Loss 19.099695205688477, Transition Loss 2.2495317459106445, Classifier Loss 0.35550016164779663, Total Loss 188.7974853515625\n",
      "20: Encoding Loss 18.50766944885254, Transition Loss 2.2797515392303467, Classifier Loss 0.36800041794776917, Total Loss 185.3173370361328\n",
      "20: Encoding Loss 19.33547592163086, Transition Loss 2.152486801147461, Classifier Loss 0.404529333114624, Total Loss 195.56724548339844\n",
      "20: Encoding Loss 19.166616439819336, Transition Loss 2.2375195026397705, Classifier Loss 0.3984525501728058, Total Loss 193.62570190429688\n",
      "20: Encoding Loss 18.958080291748047, Transition Loss 2.149447441101074, Classifier Loss 0.38293659687042236, Total Loss 190.38819885253906\n",
      "20: Encoding Loss 19.624589920043945, Transition Loss 1.9978513717651367, Classifier Loss 0.3889744281768799, Total Loss 196.29373168945312\n",
      "20: Encoding Loss 20.05322265625, Transition Loss 1.9331523180007935, Classifier Loss 0.3756400942802429, Total Loss 198.3764190673828\n",
      "20: Encoding Loss 19.030927658081055, Transition Loss 2.476921796798706, Classifier Loss 0.4023861289024353, Total Loss 192.98143005371094\n",
      "20: Encoding Loss 18.793048858642578, Transition Loss 2.0119571685791016, Classifier Loss 0.3753589689731598, Total Loss 188.2826690673828\n",
      "20: Encoding Loss 18.450258255004883, Transition Loss 3.1906960010528564, Classifier Loss 0.4065265655517578, Total Loss 188.8928680419922\n",
      "20: Encoding Loss 18.180660247802734, Transition Loss 2.0426177978515625, Classifier Loss 0.3881766200065613, Total Loss 184.67147827148438\n",
      "20: Encoding Loss 19.367868423461914, Transition Loss 2.0134475231170654, Classifier Loss 0.34814703464508057, Total Loss 190.1603546142578\n",
      "20: Encoding Loss 19.87903594970703, Transition Loss 2.1652772426605225, Classifier Loss 0.369998574256897, Total Loss 196.4652099609375\n",
      "20: Encoding Loss 20.096967697143555, Transition Loss 2.088421583175659, Classifier Loss 0.3857720196247101, Total Loss 199.77061462402344\n",
      "20: Encoding Loss 19.075773239135742, Transition Loss 2.2659122943878174, Classifier Loss 0.390073299407959, Total Loss 192.06671142578125\n",
      "20: Encoding Loss 18.82440948486328, Transition Loss 2.1882340908050537, Classifier Loss 0.3784595727920532, Total Loss 188.87889099121094\n",
      "20: Encoding Loss 18.735105514526367, Transition Loss 2.791437864303589, Classifier Loss 0.4266170561313629, Total Loss 193.100830078125\n",
      "20: Encoding Loss 19.160457611083984, Transition Loss 2.2581586837768555, Classifier Loss 0.37224051356315613, Total Loss 190.9593505859375\n",
      "20: Encoding Loss 18.191450119018555, Transition Loss 2.684345245361328, Classifier Loss 0.361311674118042, Total Loss 182.1996307373047\n",
      "20: Encoding Loss 19.9447078704834, Transition Loss 2.237387180328369, Classifier Loss 0.41922464966773987, Total Loss 201.9276123046875\n",
      "20: Encoding Loss 18.336292266845703, Transition Loss 2.142895460128784, Classifier Loss 0.401930034160614, Total Loss 187.31192016601562\n",
      "20: Encoding Loss 18.62628936767578, Transition Loss 2.4536092281341553, Classifier Loss 0.3959396779537201, Total Loss 189.09500122070312\n",
      "20: Encoding Loss 17.945951461791992, Transition Loss 2.227003812789917, Classifier Loss 0.40560880303382874, Total Loss 184.5738983154297\n",
      "20: Encoding Loss 19.188066482543945, Transition Loss 2.19279408454895, Classifier Loss 0.39126673340797424, Total Loss 193.06976318359375\n",
      "20: Encoding Loss 19.247169494628906, Transition Loss 2.806568145751953, Classifier Loss 0.4345504343509674, Total Loss 197.99371337890625\n",
      "20: Encoding Loss 19.323047637939453, Transition Loss 1.8444106578826904, Classifier Loss 0.3731372654438019, Total Loss 192.26698303222656\n",
      "20: Encoding Loss 18.90557289123535, Transition Loss 2.469026565551758, Classifier Loss 0.415155291557312, Total Loss 193.25392150878906\n",
      "20: Encoding Loss 18.79448890686035, Transition Loss 2.722733736038208, Classifier Loss 0.4172450304031372, Total Loss 192.6249542236328\n",
      "20: Encoding Loss 18.209827423095703, Transition Loss 2.050912618637085, Classifier Loss 0.3979856073856354, Total Loss 185.88735961914062\n",
      "20: Encoding Loss 18.59580421447754, Transition Loss 1.936446189880371, Classifier Loss 0.41591283679008484, Total Loss 190.7449951171875\n",
      "20: Encoding Loss 18.35500717163086, Transition Loss 1.8806908130645752, Classifier Loss 0.37168899178504944, Total Loss 184.38511657714844\n",
      "20: Encoding Loss 19.946941375732422, Transition Loss 1.6841108798980713, Classifier Loss 0.39833125472068787, Total Loss 199.74549865722656\n",
      "20: Encoding Loss 19.84906005859375, Transition Loss 2.080653190612793, Classifier Loss 0.39221566915512085, Total Loss 198.43017578125\n",
      "20: Encoding Loss 19.231962203979492, Transition Loss 2.445784568786621, Classifier Loss 0.39262184500694275, Total Loss 193.60704040527344\n",
      "20: Encoding Loss 20.120452880859375, Transition Loss 1.6239640712738037, Classifier Loss 0.40552839636802673, Total Loss 201.8412628173828\n",
      "20: Encoding Loss 19.543663024902344, Transition Loss 1.9906576871871948, Classifier Loss 0.38212811946868896, Total Loss 194.9602508544922\n",
      "20: Encoding Loss 19.521787643432617, Transition Loss 2.163029909133911, Classifier Loss 0.387332022190094, Total Loss 195.34010314941406\n",
      "20: Encoding Loss 18.40528678894043, Transition Loss 1.809230089187622, Classifier Loss 0.40137606859207153, Total Loss 187.7417449951172\n",
      "20: Encoding Loss 18.904075622558594, Transition Loss 1.855101227760315, Classifier Loss 0.3755262792110443, Total Loss 189.15625\n",
      "20: Encoding Loss 18.167701721191406, Transition Loss 2.014451026916504, Classifier Loss 0.3941969573497772, Total Loss 185.16419982910156\n",
      "20: Encoding Loss 18.617830276489258, Transition Loss 2.350055694580078, Classifier Loss 0.40795430541038513, Total Loss 190.2080841064453\n",
      "20: Encoding Loss 18.4483585357666, Transition Loss 2.6129117012023926, Classifier Loss 0.38486364483833313, Total Loss 186.5958251953125\n",
      "20: Encoding Loss 17.850425720214844, Transition Loss 2.7344706058502197, Classifier Loss 0.4080738127231598, Total Loss 184.1576690673828\n",
      "20: Encoding Loss 19.107378005981445, Transition Loss 2.2015621662139893, Classifier Loss 0.3897134065628052, Total Loss 192.2706756591797\n",
      "20: Encoding Loss 18.7393856048584, Transition Loss 2.5086867809295654, Classifier Loss 0.3793637156486511, Total Loss 188.3531951904297\n",
      "20: Encoding Loss 18.536415100097656, Transition Loss 2.304492473602295, Classifier Loss 0.3807818293571472, Total Loss 186.8303985595703\n",
      "20: Encoding Loss 18.386661529541016, Transition Loss 2.242199659347534, Classifier Loss 0.34373146295547485, Total Loss 181.9148712158203\n",
      "20: Encoding Loss 19.77515983581543, Transition Loss 2.097667694091797, Classifier Loss 0.39715516567230225, Total Loss 198.33633422851562\n",
      "20: Encoding Loss 20.169340133666992, Transition Loss 1.6963145732879639, Classifier Loss 0.3774597644805908, Total Loss 199.43995666503906\n",
      "20: Encoding Loss 19.245176315307617, Transition Loss 2.167497396469116, Classifier Loss 0.4002397656440735, Total Loss 194.41888427734375\n",
      "20: Encoding Loss 19.090864181518555, Transition Loss 2.24505615234375, Classifier Loss 0.38497263193130493, Total Loss 191.67318725585938\n",
      "20: Encoding Loss 18.402141571044922, Transition Loss 2.233830213546753, Classifier Loss 0.3748549818992615, Total Loss 185.14939880371094\n",
      "20: Encoding Loss 18.188926696777344, Transition Loss 2.4199986457824707, Classifier Loss 0.38570618629455566, Total Loss 184.56602478027344\n",
      "20: Encoding Loss 19.318614959716797, Transition Loss 2.0982489585876465, Classifier Loss 0.3621482849121094, Total Loss 191.18341064453125\n",
      "20: Encoding Loss 20.414392471313477, Transition Loss 1.8389403820037842, Classifier Loss 0.38333579897880554, Total Loss 202.01649475097656\n",
      "20: Encoding Loss 20.131868362426758, Transition Loss 2.195775032043457, Classifier Loss 0.40158766508102417, Total Loss 201.65286254882812\n",
      "20: Encoding Loss 19.105976104736328, Transition Loss 2.2493646144866943, Classifier Loss 0.38822630047798157, Total Loss 192.1203155517578\n",
      "20: Encoding Loss 19.535383224487305, Transition Loss 1.6039659976959229, Classifier Loss 0.37132275104522705, Total Loss 193.73614501953125\n",
      "20: Encoding Loss 18.335298538208008, Transition Loss 2.3554067611694336, Classifier Loss 0.34977471828460693, Total Loss 182.1309356689453\n",
      "20: Encoding Loss 18.583919525146484, Transition Loss 2.3223698139190674, Classifier Loss 0.39337560534477234, Total Loss 188.47340393066406\n",
      "20: Encoding Loss 19.639307022094727, Transition Loss 2.0752902030944824, Classifier Loss 0.35112524032592773, Total Loss 192.6420440673828\n",
      "20: Encoding Loss 18.655733108520508, Transition Loss 2.9512600898742676, Classifier Loss 0.42620691657066345, Total Loss 192.45681762695312\n",
      "20: Encoding Loss 19.816072463989258, Transition Loss 2.126255512237549, Classifier Loss 0.3754359185695648, Total Loss 196.49742126464844\n",
      "20: Encoding Loss 19.035404205322266, Transition Loss 2.469047784805298, Classifier Loss 0.36420148611068726, Total Loss 189.1971893310547\n",
      "20: Encoding Loss 19.128252029418945, Transition Loss 2.5804407596588135, Classifier Loss 0.3814326524734497, Total Loss 191.68536376953125\n",
      "20: Encoding Loss 18.460052490234375, Transition Loss 2.235924243927002, Classifier Loss 0.3880191147327423, Total Loss 186.9295196533203\n",
      "20: Encoding Loss 19.963539123535156, Transition Loss 1.7400634288787842, Classifier Loss 0.3878765404224396, Total Loss 198.84397888183594\n",
      "20: Encoding Loss 18.48601531982422, Transition Loss 2.1516964435577393, Classifier Loss 0.3951272964477539, Total Loss 187.83119201660156\n",
      "20: Encoding Loss 19.18585205078125, Transition Loss 2.2483811378479004, Classifier Loss 0.3803101181983948, Total Loss 191.96749877929688\n",
      "20: Encoding Loss 19.110809326171875, Transition Loss 2.122539758682251, Classifier Loss 0.381750226020813, Total Loss 191.4860076904297\n",
      "20: Encoding Loss 19.457612991333008, Transition Loss 2.4534339904785156, Classifier Loss 0.3753998875617981, Total Loss 193.69158935546875\n",
      "20: Encoding Loss 19.79616928100586, Transition Loss 1.4859882593154907, Classifier Loss 0.3727814555168152, Total Loss 195.94471740722656\n",
      "20: Encoding Loss 18.16261863708496, Transition Loss 2.3762311935424805, Classifier Loss 0.35910892486572266, Total Loss 181.6870880126953\n",
      "20: Encoding Loss 19.182228088378906, Transition Loss 2.038452386856079, Classifier Loss 0.41231244802474976, Total Loss 195.0967559814453\n",
      "20: Encoding Loss 19.11273956298828, Transition Loss 2.559706687927246, Classifier Loss 0.3917625844478607, Total Loss 192.59011840820312\n",
      "20: Encoding Loss 19.08573341369629, Transition Loss 2.2223386764526367, Classifier Loss 0.392270565032959, Total Loss 192.35740661621094\n",
      "20: Encoding Loss 19.497074127197266, Transition Loss 2.0157923698425293, Classifier Loss 0.3909611701965332, Total Loss 195.47586059570312\n",
      "20: Encoding Loss 19.27465057373047, Transition Loss 1.7768734693527222, Classifier Loss 0.35658660531044006, Total Loss 190.21124267578125\n",
      "20: Encoding Loss 19.636913299560547, Transition Loss 2.2203617095947266, Classifier Loss 0.43072620034217834, Total Loss 200.6120147705078\n",
      "20: Encoding Loss 19.32659339904785, Transition Loss 2.0845956802368164, Classifier Loss 0.3848264217376709, Total Loss 193.51231384277344\n",
      "20: Encoding Loss 20.00150489807129, Transition Loss 1.6511728763580322, Classifier Loss 0.3312388062477112, Total Loss 193.46615600585938\n",
      "20: Encoding Loss 19.381969451904297, Transition Loss 1.866542100906372, Classifier Loss 0.36415210366249084, Total Loss 191.8442840576172\n",
      "20: Encoding Loss 18.249677658081055, Transition Loss 2.114300012588501, Classifier Loss 0.3821501135826111, Total Loss 184.6352996826172\n",
      "20: Encoding Loss 19.459115982055664, Transition Loss 1.9877572059631348, Classifier Loss 0.411791056394577, Total Loss 197.2495880126953\n",
      "20: Encoding Loss 18.964046478271484, Transition Loss 2.0268664360046387, Classifier Loss 0.3707883358001709, Total Loss 189.1966094970703\n",
      "20: Encoding Loss 18.40907859802246, Transition Loss 2.6987171173095703, Classifier Loss 0.39649254083633423, Total Loss 187.4616241455078\n",
      "20: Encoding Loss 19.963844299316406, Transition Loss 1.794435977935791, Classifier Loss 0.37952008843421936, Total Loss 198.0216522216797\n",
      "20: Encoding Loss 19.51054573059082, Transition Loss 2.020380735397339, Classifier Loss 0.4105370342731476, Total Loss 197.54214477539062\n",
      "20: Encoding Loss 19.309648513793945, Transition Loss 1.7956762313842773, Classifier Loss 0.41310009360313416, Total Loss 196.14633178710938\n",
      "20: Encoding Loss 18.69870948791504, Transition Loss 2.604843854904175, Classifier Loss 0.3951515555381775, Total Loss 189.62579345703125\n",
      "20: Encoding Loss 19.30136489868164, Transition Loss 2.6639723777770996, Classifier Loss 0.3794631361961365, Total Loss 192.89002990722656\n",
      "20: Encoding Loss 18.79635238647461, Transition Loss 2.3210160732269287, Classifier Loss 0.3791767954826355, Total Loss 188.75271606445312\n",
      "20: Encoding Loss 17.64884376525879, Transition Loss 3.029879093170166, Classifier Loss 0.3603459894657135, Total Loss 177.83131408691406\n",
      "20: Encoding Loss 18.584148406982422, Transition Loss 2.1999146938323975, Classifier Loss 0.3772622048854828, Total Loss 186.8394012451172\n",
      "20: Encoding Loss 19.060129165649414, Transition Loss 2.2645351886749268, Classifier Loss 0.3579658269882202, Total Loss 188.73052978515625\n",
      "20: Encoding Loss 17.57968521118164, Transition Loss 2.6484742164611816, Classifier Loss 0.35207998752593994, Total Loss 176.37518310546875\n",
      "20: Encoding Loss 19.119016647338867, Transition Loss 2.565451145172119, Classifier Loss 0.3758374750614166, Total Loss 191.04898071289062\n",
      "20: Encoding Loss 19.182331085205078, Transition Loss 2.2570042610168457, Classifier Loss 0.3665043115615845, Total Loss 190.56048583984375\n",
      "20: Encoding Loss 20.015623092651367, Transition Loss 2.0842669010162354, Classifier Loss 0.39339110255241394, Total Loss 199.88095092773438\n",
      "20: Encoding Loss 18.91851234436035, Transition Loss 2.312030076980591, Classifier Loss 0.3627016544342041, Total Loss 188.08065795898438\n",
      "20: Encoding Loss 19.53046417236328, Transition Loss 1.9112234115600586, Classifier Loss 0.3993111252784729, Total Loss 196.5570831298828\n",
      "20: Encoding Loss 18.608285903930664, Transition Loss 2.1093649864196777, Classifier Loss 0.40219807624816895, Total Loss 189.50796508789062\n",
      "20: Encoding Loss 20.523046493530273, Transition Loss 2.3756299018859863, Classifier Loss 0.370257169008255, Total Loss 201.68521118164062\n",
      "20: Encoding Loss 19.31070327758789, Transition Loss 1.760482907295227, Classifier Loss 0.379334032535553, Total Loss 192.77113342285156\n",
      "20: Encoding Loss 18.25478744506836, Transition Loss 2.165919065475464, Classifier Loss 0.37834277749061584, Total Loss 184.30577087402344\n",
      "20: Encoding Loss 19.076366424560547, Transition Loss 2.2880446910858154, Classifier Loss 0.39935004711151123, Total Loss 193.00357055664062\n",
      "20: Encoding Loss 19.21306800842285, Transition Loss 2.0413665771484375, Classifier Loss 0.37414658069610596, Total Loss 191.52748107910156\n",
      "20: Encoding Loss 18.408077239990234, Transition Loss 1.6379560232162476, Classifier Loss 0.36289384961128235, Total Loss 183.88160705566406\n",
      "20: Encoding Loss 18.482730865478516, Transition Loss 2.105081796646118, Classifier Loss 0.3741912245750427, Total Loss 185.70199584960938\n",
      "20: Encoding Loss 19.230865478515625, Transition Loss 2.456705093383789, Classifier Loss 0.40452903509140015, Total Loss 194.7911834716797\n",
      "20: Encoding Loss 18.943927764892578, Transition Loss 1.62662935256958, Classifier Loss 0.3797951638698578, Total Loss 189.8562774658203\n",
      "20: Encoding Loss 19.45701789855957, Transition Loss 1.4758456945419312, Classifier Loss 0.4066287875175476, Total Loss 196.61419677734375\n",
      "20: Encoding Loss 18.113847732543945, Transition Loss 2.6232903003692627, Classifier Loss 0.3752495348453522, Total Loss 182.96038818359375\n",
      "20: Encoding Loss 18.729307174682617, Transition Loss 2.83357834815979, Classifier Loss 0.3578958511352539, Total Loss 186.1907501220703\n",
      "20: Encoding Loss 18.50220489501953, Transition Loss 2.507819175720215, Classifier Loss 0.40396299958229065, Total Loss 188.91549682617188\n",
      "20: Encoding Loss 19.267175674438477, Transition Loss 2.5181424617767334, Classifier Loss 0.3914737403392792, Total Loss 193.7884063720703\n",
      "20: Encoding Loss 18.585668563842773, Transition Loss 3.142305850982666, Classifier Loss 0.37859630584716797, Total Loss 187.17344665527344\n",
      "20: Encoding Loss 19.607036590576172, Transition Loss 1.9573763608932495, Classifier Loss 0.39126843214035034, Total Loss 196.3746337890625\n",
      "20: Encoding Loss 19.419769287109375, Transition Loss 1.91978919506073, Classifier Loss 0.3713076114654541, Total Loss 192.8728790283203\n",
      "20: Encoding Loss 18.420156478881836, Transition Loss 2.5335023403167725, Classifier Loss 0.37492358684539795, Total Loss 185.3603057861328\n",
      "20: Encoding Loss 18.476736068725586, Transition Loss 2.1666338443756104, Classifier Loss 0.3757958710193634, Total Loss 185.82681274414062\n",
      "20: Encoding Loss 18.727476119995117, Transition Loss 2.500546455383301, Classifier Loss 0.4272838532924652, Total Loss 193.0482940673828\n",
      "20: Encoding Loss 18.556245803833008, Transition Loss 2.177088975906372, Classifier Loss 0.3981974422931671, Total Loss 188.70513916015625\n",
      "20: Encoding Loss 18.867650985717773, Transition Loss 1.8844125270843506, Classifier Loss 0.3689732551574707, Total Loss 188.2154083251953\n",
      "20: Encoding Loss 18.734647750854492, Transition Loss 1.89190673828125, Classifier Loss 0.37434300780296326, Total Loss 187.6898651123047\n",
      "20: Encoding Loss 17.52419090270996, Transition Loss 2.4804162979125977, Classifier Loss 0.3470662534236908, Total Loss 175.39622497558594\n",
      "20: Encoding Loss 17.65072250366211, Transition Loss 2.4550108909606934, Classifier Loss 0.36191391944885254, Total Loss 177.88818359375\n",
      "20: Encoding Loss 20.3317928314209, Transition Loss 1.8993780612945557, Classifier Loss 0.41286784410476685, Total Loss 204.32101440429688\n",
      "20: Encoding Loss 18.725574493408203, Transition Loss 2.7098398208618164, Classifier Loss 0.3764888048171997, Total Loss 187.9954376220703\n",
      "20: Encoding Loss 19.322845458984375, Transition Loss 2.143972158432007, Classifier Loss 0.39255350828170776, Total Loss 194.26690673828125\n",
      "20: Encoding Loss 19.273597717285156, Transition Loss 2.386922836303711, Classifier Loss 0.4222586452960968, Total Loss 196.89202880859375\n",
      "20: Encoding Loss 18.854454040527344, Transition Loss 2.104300022125244, Classifier Loss 0.3959575891494751, Total Loss 190.8522491455078\n",
      "20: Encoding Loss 18.175058364868164, Transition Loss 2.4445762634277344, Classifier Loss 0.34912198781967163, Total Loss 180.8015899658203\n",
      "20: Encoding Loss 19.69622039794922, Transition Loss 1.3985965251922607, Classifier Loss 0.36651355028152466, Total Loss 194.50083923339844\n",
      "20: Encoding Loss 18.402605056762695, Transition Loss 1.7196688652038574, Classifier Loss 0.40274009108543396, Total Loss 187.83877563476562\n",
      "20: Encoding Loss 18.87480926513672, Transition Loss 2.2192118167877197, Classifier Loss 0.3820246458053589, Total Loss 189.64479064941406\n",
      "20: Encoding Loss 17.649871826171875, Transition Loss 2.503200054168701, Classifier Loss 0.3799230754375458, Total Loss 179.69192504882812\n",
      "20: Encoding Loss 17.891794204711914, Transition Loss 1.9339408874511719, Classifier Loss 0.35442861914634705, Total Loss 178.96400451660156\n",
      "20: Encoding Loss 19.40513801574707, Transition Loss 2.608604669570923, Classifier Loss 0.40059956908226013, Total Loss 195.82278442382812\n",
      "20: Encoding Loss 18.956348419189453, Transition Loss 1.9495153427124023, Classifier Loss 0.3802466094493866, Total Loss 190.0653533935547\n",
      "20: Encoding Loss 20.621967315673828, Transition Loss 1.798364520072937, Classifier Loss 0.3715287148952484, Total Loss 202.48828125\n",
      "20: Encoding Loss 20.154300689697266, Transition Loss 2.0630791187286377, Classifier Loss 0.38292980194091797, Total Loss 199.94000244140625\n",
      "20: Encoding Loss 19.460102081298828, Transition Loss 2.573914051055908, Classifier Loss 0.3799690306186676, Total Loss 194.1925048828125\n",
      "20: Encoding Loss 17.780920028686523, Transition Loss 3.573725700378418, Classifier Loss 0.36909157037734985, Total Loss 179.87127685546875\n",
      "21: Encoding Loss 19.67221450805664, Transition Loss 1.8743572235107422, Classifier Loss 0.36958903074264526, Total Loss 194.7115020751953\n",
      "21: Encoding Loss 19.43222999572754, Transition Loss 1.8821741342544556, Classifier Loss 0.38351771235466003, Total Loss 194.18605041503906\n",
      "21: Encoding Loss 20.01340675354004, Transition Loss 1.9459986686706543, Classifier Loss 0.3828747272491455, Total Loss 198.783935546875\n",
      "21: Encoding Loss 18.970577239990234, Transition Loss 1.8917393684387207, Classifier Loss 0.38865795731544495, Total Loss 191.00877380371094\n",
      "21: Encoding Loss 20.24273109436035, Transition Loss 1.548173189163208, Classifier Loss 0.3699325919151306, Total Loss 199.24473571777344\n",
      "21: Encoding Loss 18.167259216308594, Transition Loss 2.7048802375793457, Classifier Loss 0.36472806334495544, Total Loss 182.3518524169922\n",
      "21: Encoding Loss 19.752742767333984, Transition Loss 1.628373622894287, Classifier Loss 0.39341259002685547, Total Loss 197.6888885498047\n",
      "21: Encoding Loss 20.623538970947266, Transition Loss 1.3988230228424072, Classifier Loss 0.3988349735736847, Total Loss 205.15158081054688\n",
      "21: Encoding Loss 19.081621170043945, Transition Loss 2.1572299003601074, Classifier Loss 0.47574862837791443, Total Loss 200.65927124023438\n",
      "21: Encoding Loss 20.504854202270508, Transition Loss 1.5829591751098633, Classifier Loss 0.43891412019729614, Total Loss 208.246826171875\n",
      "21: Encoding Loss 19.618335723876953, Transition Loss 1.8866366147994995, Classifier Loss 0.4060906767845154, Total Loss 197.93309020996094\n",
      "21: Encoding Loss 19.21101188659668, Transition Loss 1.9645135402679443, Classifier Loss 0.3791472613811493, Total Loss 191.99571228027344\n",
      "21: Encoding Loss 18.566349029541016, Transition Loss 2.003279447555542, Classifier Loss 0.3752237558364868, Total Loss 186.4538116455078\n",
      "21: Encoding Loss 18.90100860595703, Transition Loss 2.2841150760650635, Classifier Loss 0.3906967043876648, Total Loss 190.73455810546875\n",
      "21: Encoding Loss 18.64219093322754, Transition Loss 2.3095905780792236, Classifier Loss 0.3942858874797821, Total Loss 189.0280303955078\n",
      "21: Encoding Loss 19.160099029541016, Transition Loss 2.2258975505828857, Classifier Loss 0.3844902515411377, Total Loss 192.17498779296875\n",
      "21: Encoding Loss 18.669063568115234, Transition Loss 2.3123257160186768, Classifier Loss 0.38849639892578125, Total Loss 188.6646270751953\n",
      "21: Encoding Loss 18.73996925354004, Transition Loss 2.151862382888794, Classifier Loss 0.3832682967185974, Total Loss 188.6769561767578\n",
      "21: Encoding Loss 19.067989349365234, Transition Loss 2.0460102558135986, Classifier Loss 0.3638008236885071, Total Loss 189.3332061767578\n",
      "21: Encoding Loss 19.990596771240234, Transition Loss 1.9437360763549805, Classifier Loss 0.3798588216304779, Total Loss 198.29942321777344\n",
      "21: Encoding Loss 19.030004501342773, Transition Loss 2.474015235900879, Classifier Loss 0.41768085956573486, Total Loss 194.50291442871094\n",
      "21: Encoding Loss 18.654376983642578, Transition Loss 2.053138494491577, Classifier Loss 0.3635649085044861, Total Loss 186.00213623046875\n",
      "21: Encoding Loss 18.19635772705078, Transition Loss 3.2036094665527344, Classifier Loss 0.3725389540195465, Total Loss 183.46546936035156\n",
      "21: Encoding Loss 18.01988983154297, Transition Loss 2.088066339492798, Classifier Loss 0.386252760887146, Total Loss 183.20201110839844\n",
      "21: Encoding Loss 18.65752410888672, Transition Loss 2.062570571899414, Classifier Loss 0.36740055680274963, Total Loss 186.4127655029297\n",
      "21: Encoding Loss 19.685121536254883, Transition Loss 2.189504384994507, Classifier Loss 0.4053143858909607, Total Loss 198.45030212402344\n",
      "21: Encoding Loss 19.886228561401367, Transition Loss 2.1697041988372803, Classifier Loss 0.37301474809646606, Total Loss 196.8252410888672\n",
      "21: Encoding Loss 18.895280838012695, Transition Loss 2.362395763397217, Classifier Loss 0.38588082790374756, Total Loss 190.22280883789062\n",
      "21: Encoding Loss 18.904325485229492, Transition Loss 2.320112466812134, Classifier Loss 0.38598939776420593, Total Loss 190.2975616455078\n",
      "21: Encoding Loss 18.4852237701416, Transition Loss 2.914785623550415, Classifier Loss 0.36867597699165344, Total Loss 185.3323516845703\n",
      "21: Encoding Loss 18.97734832763672, Transition Loss 2.364922523498535, Classifier Loss 0.35291787981987, Total Loss 187.58355712890625\n",
      "21: Encoding Loss 17.896175384521484, Transition Loss 2.7977828979492188, Classifier Loss 0.41733747720718384, Total Loss 185.4627227783203\n",
      "21: Encoding Loss 19.53881072998047, Transition Loss 2.324366807937622, Classifier Loss 0.42560991644859314, Total Loss 199.3363494873047\n",
      "21: Encoding Loss 17.946720123291016, Transition Loss 2.180269956588745, Classifier Loss 0.4106742739677429, Total Loss 185.07723999023438\n",
      "21: Encoding Loss 18.42327880859375, Transition Loss 2.500412940979004, Classifier Loss 0.3737192451953888, Total Loss 185.2582244873047\n",
      "21: Encoding Loss 17.689897537231445, Transition Loss 2.285658359527588, Classifier Loss 0.37571319937705994, Total Loss 179.54763793945312\n",
      "21: Encoding Loss 19.312837600708008, Transition Loss 2.2289299964904785, Classifier Loss 0.36103686690330505, Total Loss 191.0521697998047\n",
      "21: Encoding Loss 19.315956115722656, Transition Loss 2.916875123977661, Classifier Loss 0.4102565348148346, Total Loss 196.1366729736328\n",
      "21: Encoding Loss 19.45710563659668, Transition Loss 1.8906608819961548, Classifier Loss 0.3494356572628021, Total Loss 190.97853088378906\n",
      "21: Encoding Loss 18.707796096801758, Transition Loss 2.5996387004852295, Classifier Loss 0.34230154752731323, Total Loss 184.41244506835938\n",
      "21: Encoding Loss 18.491788864135742, Transition Loss 2.7647058963775635, Classifier Loss 0.38507694005966187, Total Loss 186.9949493408203\n",
      "21: Encoding Loss 18.360370635986328, Transition Loss 2.1170966625213623, Classifier Loss 0.33779558539390564, Total Loss 181.0859375\n",
      "21: Encoding Loss 18.593303680419922, Transition Loss 1.9727165699005127, Classifier Loss 0.3674609661102295, Total Loss 185.8870849609375\n",
      "21: Encoding Loss 18.032573699951172, Transition Loss 1.9199817180633545, Classifier Loss 0.3619707226753235, Total Loss 180.8416748046875\n",
      "21: Encoding Loss 20.001821517944336, Transition Loss 1.6964648962020874, Classifier Loss 0.4048881530761719, Total Loss 200.84268188476562\n",
      "21: Encoding Loss 19.597097396850586, Transition Loss 2.1322455406188965, Classifier Loss 0.3908027410507202, Total Loss 196.28350830078125\n",
      "21: Encoding Loss 19.14018440246582, Transition Loss 2.5021677017211914, Classifier Loss 0.4025707244873047, Total Loss 193.87896728515625\n",
      "21: Encoding Loss 19.93355369567871, Transition Loss 1.690481185913086, Classifier Loss 0.36573144793510437, Total Loss 196.37965393066406\n",
      "21: Encoding Loss 19.402069091796875, Transition Loss 2.00321364402771, Classifier Loss 0.3828938603401184, Total Loss 193.90658569335938\n",
      "21: Encoding Loss 19.413368225097656, Transition Loss 2.1908864974975586, Classifier Loss 0.3928643465042114, Total Loss 195.03155517578125\n",
      "21: Encoding Loss 18.524028778076172, Transition Loss 1.834312915802002, Classifier Loss 0.40690404176712036, Total Loss 189.24951171875\n",
      "21: Encoding Loss 18.55599021911621, Transition Loss 1.918367862701416, Classifier Loss 0.3428500294685364, Total Loss 183.11659240722656\n",
      "21: Encoding Loss 17.994497299194336, Transition Loss 2.041292190551758, Classifier Loss 0.3634517192840576, Total Loss 180.70941162109375\n",
      "21: Encoding Loss 18.813289642333984, Transition Loss 2.3309030532836914, Classifier Loss 0.39167144894599915, Total Loss 190.13966369628906\n",
      "21: Encoding Loss 18.4412841796875, Transition Loss 2.6472253799438477, Classifier Loss 0.35498929023742676, Total Loss 183.55865478515625\n",
      "21: Encoding Loss 17.605993270874023, Transition Loss 2.7164461612701416, Classifier Loss 0.3661714494228363, Total Loss 178.0083770751953\n",
      "21: Encoding Loss 18.764997482299805, Transition Loss 2.2381908893585205, Classifier Loss 0.3922184705734253, Total Loss 189.78945922851562\n",
      "21: Encoding Loss 18.58956527709961, Transition Loss 2.5060863494873047, Classifier Loss 0.3561781644821167, Total Loss 184.8355712890625\n",
      "21: Encoding Loss 18.368282318115234, Transition Loss 2.35727858543396, Classifier Loss 0.37502574920654297, Total Loss 184.92030334472656\n",
      "21: Encoding Loss 18.2701358795166, Transition Loss 2.2511558532714844, Classifier Loss 0.3653389811515808, Total Loss 183.14520263671875\n",
      "21: Encoding Loss 19.69887351989746, Transition Loss 2.102391481399536, Classifier Loss 0.37935417890548706, Total Loss 195.94686889648438\n",
      "21: Encoding Loss 20.127256393432617, Transition Loss 1.6601462364196777, Classifier Loss 0.40203243494033813, Total Loss 201.55332946777344\n",
      "21: Encoding Loss 19.017480850219727, Transition Loss 2.185478925704956, Classifier Loss 0.38983404636383057, Total Loss 191.56036376953125\n",
      "21: Encoding Loss 18.9295711517334, Transition Loss 2.2337417602539062, Classifier Loss 0.3506886661052704, Total Loss 186.95217895507812\n",
      "21: Encoding Loss 18.21236228942871, Transition Loss 2.236311912536621, Classifier Loss 0.3812295198440552, Total Loss 184.2691192626953\n",
      "21: Encoding Loss 17.965736389160156, Transition Loss 2.3951668739318848, Classifier Loss 0.36315175890922546, Total Loss 180.5200958251953\n",
      "21: Encoding Loss 18.896970748901367, Transition Loss 2.1018259525299072, Classifier Loss 0.35395926237106323, Total Loss 186.99205017089844\n",
      "21: Encoding Loss 19.97780418395996, Transition Loss 1.8016083240509033, Classifier Loss 0.36802223324775696, Total Loss 196.9849853515625\n",
      "21: Encoding Loss 20.073205947875977, Transition Loss 2.161781072616577, Classifier Loss 0.38380131125450134, Total Loss 199.39813232421875\n",
      "21: Encoding Loss 18.68023109436035, Transition Loss 2.2104291915893555, Classifier Loss 0.3997294008731842, Total Loss 189.8568878173828\n",
      "21: Encoding Loss 19.756710052490234, Transition Loss 1.6426751613616943, Classifier Loss 0.36542803049087524, Total Loss 194.92503356933594\n",
      "21: Encoding Loss 18.308025360107422, Transition Loss 2.319915771484375, Classifier Loss 0.3718557357788086, Total Loss 184.11378479003906\n",
      "21: Encoding Loss 18.5966739654541, Transition Loss 2.4032983779907227, Classifier Loss 0.35960230231285095, Total Loss 185.21429443359375\n",
      "21: Encoding Loss 19.492307662963867, Transition Loss 2.110225200653076, Classifier Loss 0.38633832335472107, Total Loss 194.9943389892578\n",
      "21: Encoding Loss 18.35623550415039, Transition Loss 3.080395221710205, Classifier Loss 0.4231933057308197, Total Loss 189.78529357910156\n",
      "21: Encoding Loss 19.56259536743164, Transition Loss 2.190948963165283, Classifier Loss 0.3833543062210083, Total Loss 195.27438354492188\n",
      "21: Encoding Loss 19.152801513671875, Transition Loss 2.5736958980560303, Classifier Loss 0.3462696075439453, Total Loss 188.36410522460938\n",
      "21: Encoding Loss 18.872644424438477, Transition Loss 2.5990242958068848, Classifier Loss 0.39806756377220154, Total Loss 191.30770874023438\n",
      "21: Encoding Loss 18.293764114379883, Transition Loss 2.3184359073638916, Classifier Loss 0.36307695508003235, Total Loss 183.12149047851562\n",
      "21: Encoding Loss 19.698169708251953, Transition Loss 1.7445108890533447, Classifier Loss 0.3899601399898529, Total Loss 196.93028259277344\n",
      "21: Encoding Loss 18.287006378173828, Transition Loss 2.1653835773468018, Classifier Loss 0.35973381996154785, Total Loss 182.7025146484375\n",
      "21: Encoding Loss 18.98518943786621, Transition Loss 2.294499635696411, Classifier Loss 0.4146657884120941, Total Loss 193.80699157714844\n",
      "21: Encoding Loss 18.798084259033203, Transition Loss 2.0689220428466797, Classifier Loss 0.39241480827331543, Total Loss 190.03994750976562\n",
      "21: Encoding Loss 19.37308692932129, Transition Loss 2.5192415714263916, Classifier Loss 0.3808656930923462, Total Loss 193.57510375976562\n",
      "21: Encoding Loss 19.462114334106445, Transition Loss 1.5019721984863281, Classifier Loss 0.3583288788795471, Total Loss 191.8302001953125\n",
      "21: Encoding Loss 18.032018661499023, Transition Loss 2.4084577560424805, Classifier Loss 0.36917251348495483, Total Loss 181.65509033203125\n",
      "21: Encoding Loss 18.82918357849121, Transition Loss 2.0595948696136475, Classifier Loss 0.4094986319541931, Total Loss 191.99525451660156\n",
      "21: Encoding Loss 19.211332321166992, Transition Loss 2.528203248977661, Classifier Loss 0.3963250517845154, Total Loss 193.8288116455078\n",
      "21: Encoding Loss 18.910310745239258, Transition Loss 2.230618476867676, Classifier Loss 0.36964309215545654, Total Loss 188.6929168701172\n",
      "21: Encoding Loss 19.17815399169922, Transition Loss 2.03334641456604, Classifier Loss 0.38766244053840637, Total Loss 192.59812927246094\n",
      "21: Encoding Loss 19.234777450561523, Transition Loss 1.7549796104431152, Classifier Loss 0.33732447028160095, Total Loss 187.961669921875\n",
      "21: Encoding Loss 19.195926666259766, Transition Loss 2.1983232498168945, Classifier Loss 0.4299899935722351, Total Loss 197.00607299804688\n",
      "21: Encoding Loss 19.104969024658203, Transition Loss 2.0464446544647217, Classifier Loss 0.3990654945373535, Total Loss 193.1555938720703\n",
      "21: Encoding Loss 19.75192642211914, Transition Loss 1.6657276153564453, Classifier Loss 0.3881189227104187, Total Loss 197.1604461669922\n",
      "21: Encoding Loss 19.316186904907227, Transition Loss 1.9015038013458252, Classifier Loss 0.38761377334594727, Total Loss 193.67115783691406\n",
      "21: Encoding Loss 18.07767105102539, Transition Loss 2.1257755756378174, Classifier Loss 0.3856203854084015, Total Loss 183.6085662841797\n",
      "21: Encoding Loss 19.2186279296875, Transition Loss 2.0568883419036865, Classifier Loss 0.3992774784564972, Total Loss 194.08815002441406\n",
      "21: Encoding Loss 18.79673957824707, Transition Loss 2.0411596298217773, Classifier Loss 0.3766024708747864, Total Loss 188.44239807128906\n",
      "21: Encoding Loss 18.097929000854492, Transition Loss 2.6738834381103516, Classifier Loss 0.40152212977409363, Total Loss 185.4704132080078\n",
      "21: Encoding Loss 19.914804458618164, Transition Loss 1.7881028652191162, Classifier Loss 0.3734099864959717, Total Loss 197.01705932617188\n",
      "21: Encoding Loss 19.43033790588379, Transition Loss 2.0200958251953125, Classifier Loss 0.44315826892852783, Total Loss 200.1625518798828\n",
      "21: Encoding Loss 19.116613388061523, Transition Loss 1.8086543083190918, Classifier Loss 0.40451356768608093, Total Loss 193.74598693847656\n",
      "21: Encoding Loss 18.374332427978516, Transition Loss 2.5963151454925537, Classifier Loss 0.3848731517791748, Total Loss 186.00123596191406\n",
      "21: Encoding Loss 19.24451446533203, Transition Loss 2.658682107925415, Classifier Loss 0.4073619246482849, Total Loss 195.2240447998047\n",
      "21: Encoding Loss 18.708126068115234, Transition Loss 2.36625075340271, Classifier Loss 0.3783887028694153, Total Loss 187.97714233398438\n",
      "21: Encoding Loss 17.607263565063477, Transition Loss 3.0630295276641846, Classifier Loss 0.3686164915561676, Total Loss 178.33236694335938\n",
      "21: Encoding Loss 18.44548797607422, Transition Loss 2.258481025695801, Classifier Loss 0.3976529836654663, Total Loss 187.7808837890625\n",
      "21: Encoding Loss 19.390480041503906, Transition Loss 2.306112289428711, Classifier Loss 0.40207409858703613, Total Loss 195.79248046875\n",
      "21: Encoding Loss 17.4930477142334, Transition Loss 2.7068634033203125, Classifier Loss 0.3697168529033661, Total Loss 177.45742797851562\n",
      "21: Encoding Loss 18.920360565185547, Transition Loss 2.6181375980377197, Classifier Loss 0.37574756145477295, Total Loss 189.46127319335938\n",
      "21: Encoding Loss 18.898347854614258, Transition Loss 2.3375296592712402, Classifier Loss 0.3871423900127411, Total Loss 190.36851501464844\n",
      "21: Encoding Loss 19.896343231201172, Transition Loss 2.1520533561706543, Classifier Loss 0.3831391930580139, Total Loss 197.9150848388672\n",
      "21: Encoding Loss 18.60260772705078, Transition Loss 2.367957353591919, Classifier Loss 0.3861849904060364, Total Loss 187.91294860839844\n",
      "21: Encoding Loss 19.201725006103516, Transition Loss 1.9164103269577026, Classifier Loss 0.3673859238624573, Total Loss 190.7356719970703\n",
      "21: Encoding Loss 18.58979034423828, Transition Loss 2.075935125350952, Classifier Loss 0.38664770126342773, Total Loss 187.79827880859375\n",
      "21: Encoding Loss 20.41305160522461, Transition Loss 2.3331634998321533, Classifier Loss 0.40078115463256836, Total Loss 203.8491668701172\n",
      "21: Encoding Loss 19.23975372314453, Transition Loss 1.7796833515167236, Classifier Loss 0.3914579451084137, Total Loss 193.41976928710938\n",
      "21: Encoding Loss 17.898027420043945, Transition Loss 2.122123956680298, Classifier Loss 0.39039456844329834, Total Loss 182.64810180664062\n",
      "21: Encoding Loss 18.888225555419922, Transition Loss 2.289276599884033, Classifier Loss 0.36626121401786804, Total Loss 188.18978881835938\n",
      "21: Encoding Loss 18.76961898803711, Transition Loss 2.023690938949585, Classifier Loss 0.3877057433128357, Total Loss 189.332275390625\n",
      "21: Encoding Loss 18.236188888549805, Transition Loss 1.630788803100586, Classifier Loss 0.35330355167388916, Total Loss 181.5460205078125\n",
      "21: Encoding Loss 18.184043884277344, Transition Loss 2.0662384033203125, Classifier Loss 0.35776469111442566, Total Loss 181.66207885742188\n",
      "21: Encoding Loss 19.170061111450195, Transition Loss 2.4365363121032715, Classifier Loss 0.40182211995124817, Total Loss 194.02999877929688\n",
      "21: Encoding Loss 18.784027099609375, Transition Loss 1.6221643686294556, Classifier Loss 0.37176328897476196, Total Loss 187.77297973632812\n",
      "21: Encoding Loss 19.377765655517578, Transition Loss 1.4825448989868164, Classifier Loss 0.39861834049224854, Total Loss 195.1804656982422\n",
      "21: Encoding Loss 18.067020416259766, Transition Loss 2.6004648208618164, Classifier Loss 0.3718603253364563, Total Loss 182.24229431152344\n",
      "21: Encoding Loss 18.65566635131836, Transition Loss 2.8039355278015137, Classifier Loss 0.37787288427352905, Total Loss 187.5934295654297\n",
      "21: Encoding Loss 18.377506256103516, Transition Loss 2.5234289169311523, Classifier Loss 0.4246743321418762, Total Loss 189.99217224121094\n",
      "21: Encoding Loss 19.047880172729492, Transition Loss 2.5293731689453125, Classifier Loss 0.3845832049846649, Total Loss 191.3472442626953\n",
      "21: Encoding Loss 18.40616798400879, Transition Loss 3.152684211730957, Classifier Loss 0.389522910118103, Total Loss 186.83216857910156\n",
      "21: Encoding Loss 19.290802001953125, Transition Loss 1.953748106956482, Classifier Loss 0.3993982672691345, Total Loss 194.656982421875\n",
      "21: Encoding Loss 19.17261505126953, Transition Loss 1.9052656888961792, Classifier Loss 0.38963356614112854, Total Loss 192.72532653808594\n",
      "21: Encoding Loss 18.067501068115234, Transition Loss 2.47232985496521, Classifier Loss 0.37523069977760315, Total Loss 182.55755615234375\n",
      "21: Encoding Loss 18.386049270629883, Transition Loss 2.117072105407715, Classifier Loss 0.37938231229782104, Total Loss 185.45004272460938\n",
      "21: Encoding Loss 18.40165138244629, Transition Loss 2.4799141883850098, Classifier Loss 0.3737910985946655, Total Loss 185.0883026123047\n",
      "21: Encoding Loss 18.37986183166504, Transition Loss 2.1179239749908447, Classifier Loss 0.3850319981575012, Total Loss 185.96568298339844\n",
      "21: Encoding Loss 18.699296951293945, Transition Loss 1.868302822113037, Classifier Loss 0.3741961419582367, Total Loss 187.3876495361328\n",
      "21: Encoding Loss 18.743370056152344, Transition Loss 1.8783891201019287, Classifier Loss 0.391576886177063, Total Loss 189.48031616210938\n",
      "21: Encoding Loss 17.508106231689453, Transition Loss 2.4565162658691406, Classifier Loss 0.3790617287158966, Total Loss 178.4623260498047\n",
      "21: Encoding Loss 17.38100242614746, Transition Loss 2.422769784927368, Classifier Loss 0.3358079791069031, Total Loss 173.11337280273438\n",
      "21: Encoding Loss 20.28464126586914, Transition Loss 1.8711435794830322, Classifier Loss 0.4149434268474579, Total Loss 204.14569091796875\n",
      "21: Encoding Loss 18.506916046142578, Transition Loss 2.637784481048584, Classifier Loss 0.4033329486846924, Total Loss 188.9161834716797\n",
      "21: Encoding Loss 19.090038299560547, Transition Loss 2.12699294090271, Classifier Loss 0.3755852282047272, Total Loss 190.70423889160156\n",
      "21: Encoding Loss 19.50499725341797, Transition Loss 2.342864990234375, Classifier Loss 0.40600112080574036, Total Loss 197.10865783691406\n",
      "21: Encoding Loss 18.91836929321289, Transition Loss 2.145429849624634, Classifier Loss 0.3723963499069214, Total Loss 189.01568603515625\n",
      "21: Encoding Loss 17.90332794189453, Transition Loss 2.448964834213257, Classifier Loss 0.4059584140777588, Total Loss 184.312255859375\n",
      "21: Encoding Loss 19.81391716003418, Transition Loss 1.4147785902023315, Classifier Loss 0.38898414373397827, Total Loss 197.69271850585938\n",
      "21: Encoding Loss 18.03887939453125, Transition Loss 1.7071980237960815, Classifier Loss 0.37735146284103394, Total Loss 182.3876190185547\n",
      "21: Encoding Loss 18.620786666870117, Transition Loss 2.182032585144043, Classifier Loss 0.3550306558609009, Total Loss 184.90576171875\n",
      "21: Encoding Loss 17.514894485473633, Transition Loss 2.433791160583496, Classifier Loss 0.35527846217155457, Total Loss 176.13375854492188\n",
      "21: Encoding Loss 17.80929183959961, Transition Loss 1.9037972688674927, Classifier Loss 0.3921451270580292, Total Loss 182.06961059570312\n",
      "21: Encoding Loss 19.47328758239746, Transition Loss 2.533113956451416, Classifier Loss 0.3764265775680542, Total Loss 193.93557739257812\n",
      "21: Encoding Loss 18.606090545654297, Transition Loss 1.9435139894485474, Classifier Loss 0.3480182886123657, Total Loss 184.03927612304688\n",
      "21: Encoding Loss 20.486642837524414, Transition Loss 1.7633963823318481, Classifier Loss 0.38533318042755127, Total Loss 202.7791290283203\n",
      "21: Encoding Loss 19.949665069580078, Transition Loss 2.0667574405670166, Classifier Loss 0.3635989725589752, Total Loss 196.3705596923828\n",
      "21: Encoding Loss 18.979150772094727, Transition Loss 2.4611780643463135, Classifier Loss 0.39933356642723083, Total Loss 192.25880432128906\n",
      "21: Encoding Loss 17.590375900268555, Transition Loss 3.561473846435547, Classifier Loss 0.49288585782051086, Total Loss 190.72389221191406\n",
      "22: Encoding Loss 19.1676025390625, Transition Loss 1.7534639835357666, Classifier Loss 0.38374462723731995, Total Loss 192.06597900390625\n",
      "22: Encoding Loss 19.182262420654297, Transition Loss 1.9023548364639282, Classifier Loss 0.36698198318481445, Total Loss 190.5367889404297\n",
      "22: Encoding Loss 19.769479751586914, Transition Loss 1.7710798978805542, Classifier Loss 0.37649694085121155, Total Loss 196.1597442626953\n",
      "22: Encoding Loss 18.643817901611328, Transition Loss 1.900556206703186, Classifier Loss 0.37393683195114136, Total Loss 186.92433166503906\n",
      "22: Encoding Loss 20.291770935058594, Transition Loss 1.3861840963363647, Classifier Loss 0.41859903931617737, Total Loss 204.4713134765625\n",
      "22: Encoding Loss 17.80853271484375, Transition Loss 2.6979594230651855, Classifier Loss 0.39273801445961, Total Loss 182.2816619873047\n",
      "22: Encoding Loss 19.31831932067871, Transition Loss 1.553689956665039, Classifier Loss 0.3754251003265381, Total Loss 192.39981079101562\n",
      "22: Encoding Loss 20.18219566345215, Transition Loss 1.3200260400772095, Classifier Loss 0.39954936504364014, Total Loss 201.676513671875\n",
      "22: Encoding Loss 18.738554000854492, Transition Loss 2.0793797969818115, Classifier Loss 0.4455411732196808, Total Loss 194.87843322753906\n",
      "22: Encoding Loss 20.361909866333008, Transition Loss 1.4208769798278809, Classifier Loss 0.41499680280685425, Total Loss 204.67913818359375\n",
      "22: Encoding Loss 19.4169921875, Transition Loss 1.8618589639663696, Classifier Loss 0.3629131317138672, Total Loss 191.9996337890625\n",
      "22: Encoding Loss 19.137439727783203, Transition Loss 1.7579894065856934, Classifier Loss 0.38388755917549133, Total Loss 191.83987426757812\n",
      "22: Encoding Loss 18.722476959228516, Transition Loss 1.991462230682373, Classifier Loss 0.4269394874572754, Total Loss 192.87203979492188\n",
      "22: Encoding Loss 18.72772216796875, Transition Loss 2.0922861099243164, Classifier Loss 0.39541393518447876, Total Loss 189.78163146972656\n",
      "22: Encoding Loss 18.1903018951416, Transition Loss 2.316725492477417, Classifier Loss 0.4008888602256775, Total Loss 186.07464599609375\n",
      "22: Encoding Loss 19.184415817260742, Transition Loss 2.070937395095825, Classifier Loss 0.35462889075279236, Total Loss 189.35240173339844\n",
      "22: Encoding Loss 18.678674697875977, Transition Loss 2.2681779861450195, Classifier Loss 0.37987613677978516, Total Loss 187.87063598632812\n",
      "22: Encoding Loss 18.74065589904785, Transition Loss 2.0015010833740234, Classifier Loss 0.3722578287124634, Total Loss 187.55133056640625\n",
      "22: Encoding Loss 19.24347496032715, Transition Loss 1.9892185926437378, Classifier Loss 0.387447714805603, Total Loss 193.0904083251953\n",
      "22: Encoding Loss 19.879863739013672, Transition Loss 1.8187469244003296, Classifier Loss 0.35387247800827026, Total Loss 194.78993225097656\n",
      "22: Encoding Loss 18.704097747802734, Transition Loss 2.533440351486206, Classifier Loss 0.3795884847640991, Total Loss 188.09832763671875\n",
      "22: Encoding Loss 18.529993057250977, Transition Loss 1.9357550144195557, Classifier Loss 0.3778395354747772, Total Loss 186.41104125976562\n",
      "22: Encoding Loss 17.996532440185547, Transition Loss 3.080683708190918, Classifier Loss 0.4051620662212372, Total Loss 185.1046142578125\n",
      "22: Encoding Loss 18.002473831176758, Transition Loss 1.9546966552734375, Classifier Loss 0.3628571331501007, Total Loss 180.6964569091797\n",
      "22: Encoding Loss 18.95436668395996, Transition Loss 1.9904452562332153, Classifier Loss 0.35685765743255615, Total Loss 187.71878051757812\n",
      "22: Encoding Loss 19.595232009887695, Transition Loss 2.1225697994232178, Classifier Loss 0.36441075801849365, Total Loss 193.62744140625\n",
      "22: Encoding Loss 19.668193817138672, Transition Loss 2.0892114639282227, Classifier Loss 0.39502885937690735, Total Loss 197.26629638671875\n",
      "22: Encoding Loss 18.78226661682129, Transition Loss 2.269998073577881, Classifier Loss 0.37353792786598206, Total Loss 188.06591796875\n",
      "22: Encoding Loss 18.565570831298828, Transition Loss 2.2051432132720947, Classifier Loss 0.38094058632850647, Total Loss 187.0596466064453\n",
      "22: Encoding Loss 18.51313591003418, Transition Loss 2.771944522857666, Classifier Loss 0.39620792865753174, Total Loss 188.28025817871094\n",
      "22: Encoding Loss 18.79938507080078, Transition Loss 2.280914783477783, Classifier Loss 0.38189682364463806, Total Loss 189.0409393310547\n",
      "22: Encoding Loss 17.653614044189453, Transition Loss 2.6685142517089844, Classifier Loss 0.3897218108177185, Total Loss 180.73480224609375\n",
      "22: Encoding Loss 19.259357452392578, Transition Loss 2.259216547012329, Classifier Loss 0.38053181767463684, Total Loss 192.5798797607422\n",
      "22: Encoding Loss 17.72294807434082, Transition Loss 2.0915114879608154, Classifier Loss 0.3711593449115753, Total Loss 179.3178253173828\n",
      "22: Encoding Loss 18.437341690063477, Transition Loss 2.369877815246582, Classifier Loss 0.3755515515804291, Total Loss 185.52786254882812\n",
      "22: Encoding Loss 17.492319107055664, Transition Loss 2.1947121620178223, Classifier Loss 0.41421186923980713, Total Loss 181.79869079589844\n",
      "22: Encoding Loss 19.15326690673828, Transition Loss 2.1808786392211914, Classifier Loss 0.37391236424446106, Total Loss 191.0535430908203\n",
      "22: Encoding Loss 18.983491897583008, Transition Loss 2.778224468231201, Classifier Loss 0.42098885774612427, Total Loss 194.52247619628906\n",
      "22: Encoding Loss 18.98894691467285, Transition Loss 1.8586243391036987, Classifier Loss 0.36256030201911926, Total Loss 188.53932189941406\n",
      "22: Encoding Loss 18.515140533447266, Transition Loss 2.4566826820373535, Classifier Loss 0.3655541241168976, Total Loss 185.16787719726562\n",
      "22: Encoding Loss 18.38901710510254, Transition Loss 2.6751034259796143, Classifier Loss 0.3934118151664734, Total Loss 186.98834228515625\n",
      "22: Encoding Loss 18.186248779296875, Transition Loss 1.9865379333496094, Classifier Loss 0.3898143768310547, Total Loss 184.86874389648438\n",
      "22: Encoding Loss 18.19999122619629, Transition Loss 1.932084560394287, Classifier Loss 0.3888474702835083, Total Loss 184.87109375\n",
      "22: Encoding Loss 17.79045295715332, Transition Loss 1.8432679176330566, Classifier Loss 0.36707186698913574, Total Loss 179.3994598388672\n",
      "22: Encoding Loss 19.77014923095703, Transition Loss 1.6864627599716187, Classifier Loss 0.39006611704826355, Total Loss 197.50509643554688\n",
      "22: Encoding Loss 19.58870506286621, Transition Loss 2.022059917449951, Classifier Loss 0.4255090355873108, Total Loss 199.6649627685547\n",
      "22: Encoding Loss 18.985116958618164, Transition Loss 2.447075128555298, Classifier Loss 0.4011669158935547, Total Loss 192.48703002929688\n",
      "22: Encoding Loss 19.71170997619629, Transition Loss 1.6168488264083862, Classifier Loss 0.37070485949516296, Total Loss 195.0875244140625\n",
      "22: Encoding Loss 19.349201202392578, Transition Loss 1.9542938470840454, Classifier Loss 0.3723296821117401, Total Loss 192.4174346923828\n",
      "22: Encoding Loss 19.04686164855957, Transition Loss 2.126127243041992, Classifier Loss 0.3560861349105835, Total Loss 188.4087371826172\n",
      "22: Encoding Loss 18.44506072998047, Transition Loss 1.811002254486084, Classifier Loss 0.36700859665870667, Total Loss 184.62355041503906\n",
      "22: Encoding Loss 18.298471450805664, Transition Loss 1.8299554586410522, Classifier Loss 0.3735729455947876, Total Loss 184.11106872558594\n",
      "22: Encoding Loss 17.658132553100586, Transition Loss 1.9791889190673828, Classifier Loss 0.367377907037735, Total Loss 178.39869689941406\n",
      "22: Encoding Loss 18.684589385986328, Transition Loss 2.2115137577056885, Classifier Loss 0.38324403762817383, Total Loss 188.24342346191406\n",
      "22: Encoding Loss 18.089866638183594, Transition Loss 2.550443649291992, Classifier Loss 0.3730301260948181, Total Loss 182.5320281982422\n",
      "22: Encoding Loss 17.434968948364258, Transition Loss 2.6055915355682373, Classifier Loss 0.37498319149017334, Total Loss 177.4991912841797\n",
      "22: Encoding Loss 18.471214294433594, Transition Loss 2.153806447982788, Classifier Loss 0.35881391167640686, Total Loss 184.0818634033203\n",
      "22: Encoding Loss 18.31262969970703, Transition Loss 2.3784022331237793, Classifier Loss 0.3624429702758789, Total Loss 183.22100830078125\n",
      "22: Encoding Loss 18.250995635986328, Transition Loss 2.282174825668335, Classifier Loss 0.3739698827266693, Total Loss 183.86138916015625\n",
      "22: Encoding Loss 18.186519622802734, Transition Loss 2.161531925201416, Classifier Loss 0.3847319483757019, Total Loss 184.39767456054688\n",
      "22: Encoding Loss 19.257686614990234, Transition Loss 2.0537168979644775, Classifier Loss 0.40540164709091187, Total Loss 195.0124053955078\n",
      "22: Encoding Loss 20.284889221191406, Transition Loss 1.6446001529693604, Classifier Loss 0.3937745690345764, Total Loss 201.98548889160156\n",
      "22: Encoding Loss 18.935523986816406, Transition Loss 2.142880439758301, Classifier Loss 0.41834044456481934, Total Loss 193.74681091308594\n",
      "22: Encoding Loss 18.81094741821289, Transition Loss 2.169863224029541, Classifier Loss 0.37388935685157776, Total Loss 188.31048583984375\n",
      "22: Encoding Loss 17.970571517944336, Transition Loss 2.2425668239593506, Classifier Loss 0.395790696144104, Total Loss 183.7921600341797\n",
      "22: Encoding Loss 17.851125717163086, Transition Loss 2.3099796772003174, Classifier Loss 0.37257927656173706, Total Loss 180.52891540527344\n",
      "22: Encoding Loss 18.8934383392334, Transition Loss 2.082510232925415, Classifier Loss 0.37231823801994324, Total Loss 188.79583740234375\n",
      "22: Encoding Loss 19.947593688964844, Transition Loss 1.7709921598434448, Classifier Loss 0.34125369787216187, Total Loss 194.06031799316406\n",
      "22: Encoding Loss 19.78425407409668, Transition Loss 2.219341278076172, Classifier Loss 0.35679125785827637, Total Loss 194.3970184326172\n",
      "22: Encoding Loss 18.699846267700195, Transition Loss 2.1244559288024902, Classifier Loss 0.40233176946640015, Total Loss 190.2568359375\n",
      "22: Encoding Loss 19.397449493408203, Transition Loss 1.7157684564590454, Classifier Loss 0.37271177768707275, Total Loss 192.79393005371094\n",
      "22: Encoding Loss 17.96381950378418, Transition Loss 2.19647216796875, Classifier Loss 0.36490046977996826, Total Loss 180.63990783691406\n",
      "22: Encoding Loss 18.236446380615234, Transition Loss 2.4363131523132324, Classifier Loss 0.3874577581882477, Total Loss 185.12461853027344\n",
      "22: Encoding Loss 19.29132843017578, Transition Loss 1.9884883165359497, Classifier Loss 0.3679395318031311, Total Loss 191.52227783203125\n",
      "22: Encoding Loss 18.490903854370117, Transition Loss 3.0645995140075684, Classifier Loss 0.43018999695777893, Total Loss 191.55914306640625\n",
      "22: Encoding Loss 19.255277633666992, Transition Loss 2.0610411167144775, Classifier Loss 0.3878532350063324, Total Loss 193.23974609375\n",
      "22: Encoding Loss 18.852474212646484, Transition Loss 2.597172737121582, Classifier Loss 0.3286384642124176, Total Loss 184.20309448242188\n",
      "22: Encoding Loss 18.97879409790039, Transition Loss 2.3740594387054443, Classifier Loss 0.39849162101745605, Total Loss 192.15432739257812\n",
      "22: Encoding Loss 18.180681228637695, Transition Loss 2.4675445556640625, Classifier Loss 0.35392722487449646, Total Loss 181.33168029785156\n",
      "22: Encoding Loss 19.680879592895508, Transition Loss 1.5200546979904175, Classifier Loss 0.3850293457508087, Total Loss 196.2539825439453\n",
      "22: Encoding Loss 18.317617416381836, Transition Loss 2.475269079208374, Classifier Loss 0.3428872227668762, Total Loss 181.32472229003906\n",
      "22: Encoding Loss 18.975894927978516, Transition Loss 1.8855454921722412, Classifier Loss 0.38706088066101074, Total Loss 190.89035034179688\n",
      "22: Encoding Loss 18.75465965270996, Transition Loss 2.4612278938293457, Classifier Loss 0.3776729106903076, Total Loss 188.29681396484375\n",
      "22: Encoding Loss 19.270465850830078, Transition Loss 2.1599278450012207, Classifier Loss 0.3910210430622101, Total Loss 193.6978302001953\n",
      "22: Encoding Loss 19.538318634033203, Transition Loss 1.7490060329437256, Classifier Loss 0.37615469098091125, Total Loss 194.27183532714844\n",
      "22: Encoding Loss 18.14066505432129, Transition Loss 2.199711322784424, Classifier Loss 0.3588774800300598, Total Loss 181.4530029296875\n",
      "22: Encoding Loss 18.871654510498047, Transition Loss 2.2379150390625, Classifier Loss 0.418212890625, Total Loss 193.24212646484375\n",
      "22: Encoding Loss 18.904911041259766, Transition Loss 2.305781364440918, Classifier Loss 0.3912632465362549, Total Loss 190.82676696777344\n",
      "22: Encoding Loss 18.660131454467773, Transition Loss 2.338841199874878, Classifier Loss 0.34136006236076355, Total Loss 183.88482666015625\n",
      "22: Encoding Loss 18.93848991394043, Transition Loss 1.8585681915283203, Classifier Loss 0.4104563891887665, Total Loss 192.92527770996094\n",
      "22: Encoding Loss 19.020793914794922, Transition Loss 1.8055187463760376, Classifier Loss 0.3320927023887634, Total Loss 185.7367401123047\n",
      "22: Encoding Loss 19.27013397216797, Transition Loss 2.069059371948242, Classifier Loss 0.44892996549606323, Total Loss 199.4678955078125\n",
      "22: Encoding Loss 18.782808303833008, Transition Loss 2.066575050354004, Classifier Loss 0.3800073564052582, Total Loss 188.676513671875\n",
      "22: Encoding Loss 19.66681480407715, Transition Loss 1.561208724975586, Classifier Loss 0.36954349279403687, Total Loss 194.60110473632812\n",
      "22: Encoding Loss 19.255632400512695, Transition Loss 1.8414092063903809, Classifier Loss 0.35859328508377075, Total Loss 190.27267456054688\n",
      "22: Encoding Loss 17.783851623535156, Transition Loss 2.04042911529541, Classifier Loss 0.3880518674850464, Total Loss 181.4840850830078\n",
      "22: Encoding Loss 19.13793182373047, Transition Loss 1.919306755065918, Classifier Loss 0.38465726375579834, Total Loss 191.9530487060547\n",
      "22: Encoding Loss 18.61190414428711, Transition Loss 1.9452577829360962, Classifier Loss 0.3855991065502167, Total Loss 187.84420776367188\n",
      "22: Encoding Loss 17.99859619140625, Transition Loss 2.5345301628112793, Classifier Loss 0.4124048054218292, Total Loss 185.7361602783203\n",
      "22: Encoding Loss 19.63639259338379, Transition Loss 1.733785629272461, Classifier Loss 0.3762679696083069, Total Loss 195.064697265625\n",
      "22: Encoding Loss 19.291141510009766, Transition Loss 1.9221441745758057, Classifier Loss 0.3883447051048279, Total Loss 193.54803466796875\n",
      "22: Encoding Loss 19.008481979370117, Transition Loss 1.68853759765625, Classifier Loss 0.3858779966831207, Total Loss 190.9933624267578\n",
      "22: Encoding Loss 18.258317947387695, Transition Loss 2.488182783126831, Classifier Loss 0.3722487688064575, Total Loss 183.78904724121094\n",
      "22: Encoding Loss 18.99125862121582, Transition Loss 2.5033106803894043, Classifier Loss 0.3799876272678375, Total Loss 190.4294891357422\n",
      "22: Encoding Loss 18.305158615112305, Transition Loss 2.236933708190918, Classifier Loss 0.35422030091285706, Total Loss 182.31068420410156\n",
      "22: Encoding Loss 17.29100799560547, Transition Loss 2.7993736267089844, Classifier Loss 0.3641224503517151, Total Loss 175.30018615722656\n",
      "22: Encoding Loss 18.555339813232422, Transition Loss 2.13826584815979, Classifier Loss 0.38781529664993286, Total Loss 187.65191650390625\n",
      "22: Encoding Loss 18.847063064575195, Transition Loss 2.147688627243042, Classifier Loss 0.3797926902770996, Total Loss 189.185302734375\n",
      "22: Encoding Loss 17.25829315185547, Transition Loss 2.5766549110412598, Classifier Loss 0.3823326826095581, Total Loss 176.81495666503906\n",
      "22: Encoding Loss 18.951650619506836, Transition Loss 2.4205739498138428, Classifier Loss 0.3697563111782074, Total Loss 189.0729522705078\n",
      "22: Encoding Loss 18.795421600341797, Transition Loss 2.24574875831604, Classifier Loss 0.3890889585018158, Total Loss 189.72142028808594\n",
      "22: Encoding Loss 19.668575286865234, Transition Loss 2.004810333251953, Classifier Loss 0.3745330572128296, Total Loss 195.202880859375\n",
      "22: Encoding Loss 18.44627571105957, Transition Loss 2.295644998550415, Classifier Loss 0.3700457215309143, Total Loss 185.03390502929688\n",
      "22: Encoding Loss 19.000072479248047, Transition Loss 1.8263384103775024, Classifier Loss 0.3679375946521759, Total Loss 189.1596221923828\n",
      "22: Encoding Loss 18.329355239868164, Transition Loss 2.0748772621154785, Classifier Loss 0.36982667446136475, Total Loss 184.03248596191406\n",
      "22: Encoding Loss 20.252656936645508, Transition Loss 2.1919937133789062, Classifier Loss 0.3912374973297119, Total Loss 201.58340454101562\n",
      "22: Encoding Loss 18.92380714416504, Transition Loss 1.7628637552261353, Classifier Loss 0.3750326633453369, Total Loss 189.2462921142578\n",
      "22: Encoding Loss 17.800357818603516, Transition Loss 2.015780448913574, Classifier Loss 0.37549784779548645, Total Loss 180.35580444335938\n",
      "22: Encoding Loss 18.53765106201172, Transition Loss 2.214569330215454, Classifier Loss 0.3391329050064087, Total Loss 182.6574249267578\n",
      "22: Encoding Loss 18.764469146728516, Transition Loss 1.9360471963882446, Classifier Loss 0.4025983512401581, Total Loss 190.76278686523438\n",
      "22: Encoding Loss 18.163419723510742, Transition Loss 1.6194281578063965, Classifier Loss 0.3607505261898041, Total Loss 181.706298828125\n",
      "22: Encoding Loss 18.104480743408203, Transition Loss 2.020448684692383, Classifier Loss 0.3499374985694885, Total Loss 180.23367309570312\n",
      "22: Encoding Loss 18.823564529418945, Transition Loss 2.4076199531555176, Classifier Loss 0.38356199860572815, Total Loss 189.42623901367188\n",
      "22: Encoding Loss 18.720783233642578, Transition Loss 1.6075462102890015, Classifier Loss 0.33716899156570435, Total Loss 183.80467224121094\n",
      "22: Encoding Loss 19.36383819580078, Transition Loss 1.4766895771026611, Classifier Loss 0.3799843490123749, Total Loss 193.20448303222656\n",
      "22: Encoding Loss 17.905834197998047, Transition Loss 2.5472142696380615, Classifier Loss 0.3548426628112793, Total Loss 179.2404022216797\n",
      "22: Encoding Loss 18.27330780029297, Transition Loss 2.740187168121338, Classifier Loss 0.3795647621154785, Total Loss 184.69097900390625\n",
      "22: Encoding Loss 18.052425384521484, Transition Loss 2.456125497817993, Classifier Loss 0.39803504943847656, Total Loss 184.7141571044922\n",
      "22: Encoding Loss 18.924299240112305, Transition Loss 2.460317611694336, Classifier Loss 0.38386139273643494, Total Loss 190.27259826660156\n",
      "22: Encoding Loss 18.279584884643555, Transition Loss 3.1121416091918945, Classifier Loss 0.41684412956237793, Total Loss 188.54351806640625\n",
      "22: Encoding Loss 19.37836456298828, Transition Loss 1.9350571632385254, Classifier Loss 0.3679228723049164, Total Loss 192.20620727539062\n",
      "22: Encoding Loss 18.960575103759766, Transition Loss 1.9189567565917969, Classifier Loss 0.3598031997680664, Total Loss 188.0487060546875\n",
      "22: Encoding Loss 18.053165435791016, Transition Loss 2.425164222717285, Classifier Loss 0.3830298185348511, Total Loss 183.2133331298828\n",
      "22: Encoding Loss 18.36223793029785, Transition Loss 2.0933821201324463, Classifier Loss 0.4125343859195709, Total Loss 188.57000732421875\n",
      "22: Encoding Loss 18.18239402770996, Transition Loss 2.431612968444824, Classifier Loss 0.3664672374725342, Total Loss 182.5922088623047\n",
      "22: Encoding Loss 18.35794448852539, Transition Loss 2.109323024749756, Classifier Loss 0.3612348139286041, Total Loss 183.4088897705078\n",
      "22: Encoding Loss 18.431360244750977, Transition Loss 1.8510669469833374, Classifier Loss 0.37348857522010803, Total Loss 185.16995239257812\n",
      "22: Encoding Loss 18.607412338256836, Transition Loss 1.8609416484832764, Classifier Loss 0.39189618825912476, Total Loss 188.42111206054688\n",
      "22: Encoding Loss 17.118236541748047, Transition Loss 2.3924341201782227, Classifier Loss 0.3749834895133972, Total Loss 174.92274475097656\n",
      "22: Encoding Loss 17.213037490844727, Transition Loss 2.4324631690979004, Classifier Loss 0.34014999866485596, Total Loss 172.20579528808594\n",
      "22: Encoding Loss 20.019550323486328, Transition Loss 1.8273115158081055, Classifier Loss 0.3780089318752289, Total Loss 198.32275390625\n",
      "22: Encoding Loss 18.32059669494629, Transition Loss 2.613743305206299, Classifier Loss 0.39469489455223083, Total Loss 186.55702209472656\n",
      "22: Encoding Loss 18.99443244934082, Transition Loss 2.0590286254882812, Classifier Loss 0.3810288608074188, Total Loss 190.47015380859375\n",
      "22: Encoding Loss 18.988529205322266, Transition Loss 2.262808084487915, Classifier Loss 0.3914952874183655, Total Loss 191.5103302001953\n",
      "22: Encoding Loss 18.739118576049805, Transition Loss 2.044877290725708, Classifier Loss 0.3811633884906769, Total Loss 188.4382781982422\n",
      "22: Encoding Loss 17.715009689331055, Transition Loss 2.3705854415893555, Classifier Loss 0.3972771167755127, Total Loss 181.92190551757812\n",
      "22: Encoding Loss 19.658376693725586, Transition Loss 1.3870904445648193, Classifier Loss 0.4186785817146301, Total Loss 199.41229248046875\n",
      "22: Encoding Loss 18.129783630371094, Transition Loss 1.7068817615509033, Classifier Loss 0.37755969166755676, Total Loss 183.13560485839844\n",
      "22: Encoding Loss 18.62984275817871, Transition Loss 2.161921501159668, Classifier Loss 0.36363551020622253, Total Loss 185.83468627929688\n",
      "22: Encoding Loss 17.52885627746582, Transition Loss 2.4719841480255127, Classifier Loss 0.3547826409339905, Total Loss 176.20350646972656\n",
      "22: Encoding Loss 17.501779556274414, Transition Loss 1.8776218891143799, Classifier Loss 0.3737819194793701, Total Loss 177.7679443359375\n",
      "22: Encoding Loss 19.07393455505371, Transition Loss 2.575018882751465, Classifier Loss 0.35991719365119934, Total Loss 189.0981903076172\n",
      "22: Encoding Loss 18.574689865112305, Transition Loss 1.8955845832824707, Classifier Loss 0.35936135053634644, Total Loss 184.91278076171875\n",
      "22: Encoding Loss 20.304027557373047, Transition Loss 1.7878683805465698, Classifier Loss 0.38693422079086304, Total Loss 201.4832305908203\n",
      "22: Encoding Loss 19.889196395874023, Transition Loss 1.9957642555236816, Classifier Loss 0.4006478786468506, Total Loss 199.5775146484375\n",
      "22: Encoding Loss 19.158893585205078, Transition Loss 2.528336763381958, Classifier Loss 0.3766772150993347, Total Loss 191.4445343017578\n",
      "22: Encoding Loss 17.563373565673828, Transition Loss 3.3864364624023438, Classifier Loss 0.40909746289253235, Total Loss 182.09402465820312\n",
      "23: Encoding Loss 19.04280662536621, Transition Loss 1.8896101713180542, Classifier Loss 0.36995258927345276, Total Loss 189.7156219482422\n",
      "23: Encoding Loss 19.258317947387695, Transition Loss 1.797748327255249, Classifier Loss 0.39463672041893005, Total Loss 193.88975524902344\n",
      "23: Encoding Loss 19.79186248779297, Transition Loss 1.9110981225967407, Classifier Loss 0.3850371837615967, Total Loss 197.22084045410156\n",
      "23: Encoding Loss 18.682636260986328, Transition Loss 1.7976521253585815, Classifier Loss 0.38232868909835815, Total Loss 188.05348205566406\n",
      "23: Encoding Loss 19.97112274169922, Transition Loss 1.562828779220581, Classifier Loss 0.3608531355857849, Total Loss 196.16685485839844\n",
      "23: Encoding Loss 17.74955177307129, Transition Loss 2.5798401832580566, Classifier Loss 0.37963998317718506, Total Loss 180.47637939453125\n",
      "23: Encoding Loss 19.32997703552246, Transition Loss 1.5846551656723022, Classifier Loss 0.3823337256908417, Total Loss 193.1901092529297\n",
      "23: Encoding Loss 20.346040725708008, Transition Loss 1.3918354511260986, Classifier Loss 0.4172937870025635, Total Loss 204.7760772705078\n",
      "23: Encoding Loss 18.54352378845215, Transition Loss 2.0527312755584717, Classifier Loss 0.4322645366191864, Total Loss 191.98519897460938\n",
      "23: Encoding Loss 20.134540557861328, Transition Loss 1.508954644203186, Classifier Loss 0.39906179904937744, Total Loss 201.2843017578125\n",
      "23: Encoding Loss 19.227094650268555, Transition Loss 1.7747946977615356, Classifier Loss 0.37337902188301086, Total Loss 191.50962829589844\n",
      "23: Encoding Loss 18.886281967163086, Transition Loss 1.8685595989227295, Classifier Loss 0.417458713054657, Total Loss 193.2098388671875\n",
      "23: Encoding Loss 18.416656494140625, Transition Loss 1.8633148670196533, Classifier Loss 0.3820657730102539, Total Loss 185.91249084472656\n",
      "23: Encoding Loss 18.618938446044922, Transition Loss 2.1558263301849365, Classifier Loss 0.38224589824676514, Total Loss 187.60728454589844\n",
      "23: Encoding Loss 17.932390213012695, Transition Loss 2.1463329792022705, Classifier Loss 0.4222215414047241, Total Loss 186.11053466796875\n",
      "23: Encoding Loss 18.850584030151367, Transition Loss 2.0694425106048584, Classifier Loss 0.37297549843788147, Total Loss 188.51611328125\n",
      "23: Encoding Loss 18.563793182373047, Transition Loss 2.1253366470336914, Classifier Loss 0.3977936804294586, Total Loss 188.7147979736328\n",
      "23: Encoding Loss 18.60675048828125, Transition Loss 2.028782606124878, Classifier Loss 0.3647329807281494, Total Loss 185.73306274414062\n",
      "23: Encoding Loss 19.095794677734375, Transition Loss 1.8796000480651855, Classifier Loss 0.34960463643074036, Total Loss 188.1027374267578\n",
      "23: Encoding Loss 19.544292449951172, Transition Loss 1.891754150390625, Classifier Loss 0.37485969066619873, Total Loss 194.21868896484375\n",
      "23: Encoding Loss 18.503442764282227, Transition Loss 2.304778575897217, Classifier Loss 0.40468162298202515, Total Loss 188.95664978027344\n",
      "23: Encoding Loss 18.314678192138672, Transition Loss 1.9825167655944824, Classifier Loss 0.35241690278053284, Total Loss 182.15562438964844\n",
      "23: Encoding Loss 17.974369049072266, Transition Loss 2.9733829498291016, Classifier Loss 0.36739009618759155, Total Loss 181.12864685058594\n",
      "23: Encoding Loss 17.8148193359375, Transition Loss 2.0020995140075684, Classifier Loss 0.3839055597782135, Total Loss 181.30953979492188\n",
      "23: Encoding Loss 18.455272674560547, Transition Loss 1.9399749040603638, Classifier Loss 0.34945422410964966, Total Loss 182.97561645507812\n",
      "23: Encoding Loss 19.426698684692383, Transition Loss 2.089895009994507, Classifier Loss 0.37755221128463745, Total Loss 193.5867919921875\n",
      "23: Encoding Loss 19.409351348876953, Transition Loss 2.047173500061035, Classifier Loss 0.4067608118057251, Total Loss 196.36033630371094\n",
      "23: Encoding Loss 18.67106056213379, Transition Loss 2.21750807762146, Classifier Loss 0.3660105764865875, Total Loss 186.4130401611328\n",
      "23: Encoding Loss 18.50370216369629, Transition Loss 2.156186103820801, Classifier Loss 0.3461228013038635, Total Loss 183.07315063476562\n",
      "23: Encoding Loss 18.13804817199707, Transition Loss 2.6988282203674316, Classifier Loss 0.4180566668510437, Total Loss 187.44981384277344\n",
      "23: Encoding Loss 18.530771255493164, Transition Loss 2.21864914894104, Classifier Loss 0.36006268858909607, Total Loss 184.6961669921875\n",
      "23: Encoding Loss 17.529390335083008, Transition Loss 2.5887444019317627, Classifier Loss 0.3989044725894928, Total Loss 180.643310546875\n",
      "23: Encoding Loss 19.251537322998047, Transition Loss 2.1774539947509766, Classifier Loss 0.3997032344341278, Total Loss 194.41812133789062\n",
      "23: Encoding Loss 17.85846710205078, Transition Loss 2.0301218032836914, Classifier Loss 0.409105509519577, Total Loss 184.18431091308594\n",
      "23: Encoding Loss 18.282604217529297, Transition Loss 2.304072856903076, Classifier Loss 0.38831204175949097, Total Loss 185.55287170410156\n",
      "23: Encoding Loss 17.405241012573242, Transition Loss 2.130303144454956, Classifier Loss 0.4068133234977722, Total Loss 180.3493194580078\n",
      "23: Encoding Loss 18.803380966186523, Transition Loss 2.1316733360290527, Classifier Loss 0.36596885323524475, Total Loss 187.45025634765625\n",
      "23: Encoding Loss 18.893428802490234, Transition Loss 2.7593624591827393, Classifier Loss 0.41082826256752014, Total Loss 192.7821502685547\n",
      "23: Encoding Loss 18.97950553894043, Transition Loss 1.8125333786010742, Classifier Loss 0.3478785753250122, Total Loss 186.9864044189453\n",
      "23: Encoding Loss 18.209918975830078, Transition Loss 2.4324140548706055, Classifier Loss 0.38452428579330444, Total Loss 184.61825561523438\n",
      "23: Encoding Loss 18.17908477783203, Transition Loss 2.538571357727051, Classifier Loss 0.39506208896636963, Total Loss 185.4466094970703\n",
      "23: Encoding Loss 18.02474594116211, Transition Loss 1.967591404914856, Classifier Loss 0.3896556496620178, Total Loss 183.55706787109375\n",
      "23: Encoding Loss 18.232690811157227, Transition Loss 1.8407530784606934, Classifier Loss 0.3715206980705261, Total Loss 183.38174438476562\n",
      "23: Encoding Loss 17.689979553222656, Transition Loss 1.8236613273620605, Classifier Loss 0.3618328869342804, Total Loss 178.06785583496094\n",
      "23: Encoding Loss 19.636226654052734, Transition Loss 1.630185842514038, Classifier Loss 0.38076096773147583, Total Loss 195.49195861816406\n",
      "23: Encoding Loss 19.40628433227539, Transition Loss 2.0531842708587646, Classifier Loss 0.3810086250305176, Total Loss 193.7617645263672\n",
      "23: Encoding Loss 18.710155487060547, Transition Loss 2.363640785217285, Classifier Loss 0.37222692370414734, Total Loss 187.37667846679688\n",
      "23: Encoding Loss 19.6199951171875, Transition Loss 1.6433403491973877, Classifier Loss 0.38557732105255127, Total Loss 195.84637451171875\n",
      "23: Encoding Loss 19.092721939086914, Transition Loss 1.9266360998153687, Classifier Loss 0.36680740118026733, Total Loss 189.80784606933594\n",
      "23: Encoding Loss 19.0130615234375, Transition Loss 2.1001369953155518, Classifier Loss 0.3687370717525482, Total Loss 189.39822387695312\n",
      "23: Encoding Loss 18.2492618560791, Transition Loss 1.765380620956421, Classifier Loss 0.3625105321407318, Total Loss 182.5982208251953\n",
      "23: Encoding Loss 18.302547454833984, Transition Loss 1.827540397644043, Classifier Loss 0.3551272749900818, Total Loss 182.2986297607422\n",
      "23: Encoding Loss 17.642057418823242, Transition Loss 1.9334044456481934, Classifier Loss 0.35799095034599304, Total Loss 177.32223510742188\n",
      "23: Encoding Loss 18.265790939331055, Transition Loss 2.283944845199585, Classifier Loss 0.3776426315307617, Total Loss 184.34738159179688\n",
      "23: Encoding Loss 17.826322555541992, Transition Loss 2.5183839797973633, Classifier Loss 0.3708007335662842, Total Loss 180.1943359375\n",
      "23: Encoding Loss 17.25769805908203, Transition Loss 2.551894187927246, Classifier Loss 0.40794429183006287, Total Loss 179.36639404296875\n",
      "23: Encoding Loss 18.56174087524414, Transition Loss 2.0976760387420654, Classifier Loss 0.3623776435852051, Total Loss 185.15122985839844\n",
      "23: Encoding Loss 18.050329208374023, Transition Loss 2.368579864501953, Classifier Loss 0.34358125925064087, Total Loss 179.23446655273438\n",
      "23: Encoding Loss 18.066484451293945, Transition Loss 2.1888086795806885, Classifier Loss 0.38349103927612305, Total Loss 183.31874084472656\n",
      "23: Encoding Loss 17.968908309936523, Transition Loss 2.1548149585723877, Classifier Loss 0.376869261264801, Total Loss 181.86917114257812\n",
      "23: Encoding Loss 19.309326171875, Transition Loss 2.122959613800049, Classifier Loss 0.38123440742492676, Total Loss 193.02264404296875\n",
      "23: Encoding Loss 20.004789352416992, Transition Loss 1.7146445512771606, Classifier Loss 0.37068894505500793, Total Loss 197.45013427734375\n",
      "23: Encoding Loss 18.725278854370117, Transition Loss 2.103731155395508, Classifier Loss 0.35140421986579895, Total Loss 185.3634033203125\n",
      "23: Encoding Loss 18.66520118713379, Transition Loss 2.136913776397705, Classifier Loss 0.3576405644416809, Total Loss 185.51304626464844\n",
      "23: Encoding Loss 17.753877639770508, Transition Loss 2.2253711223602295, Classifier Loss 0.39013391733169556, Total Loss 181.48948669433594\n",
      "23: Encoding Loss 17.45144271850586, Transition Loss 2.2897355556488037, Classifier Loss 0.3525412380695343, Total Loss 175.32362365722656\n",
      "23: Encoding Loss 18.624040603637695, Transition Loss 2.036439895629883, Classifier Loss 0.3767531216144562, Total Loss 187.07492065429688\n",
      "23: Encoding Loss 19.71149444580078, Transition Loss 1.8040552139282227, Classifier Loss 0.347709596157074, Total Loss 192.82373046875\n",
      "23: Encoding Loss 19.59062957763672, Transition Loss 2.1610164642333984, Classifier Loss 0.41790542006492615, Total Loss 198.94778442382812\n",
      "23: Encoding Loss 18.393964767456055, Transition Loss 2.1323962211608887, Classifier Loss 0.4153017997741699, Total Loss 189.10838317871094\n",
      "23: Encoding Loss 19.377948760986328, Transition Loss 1.6632697582244873, Classifier Loss 0.374779611825943, Total Loss 192.83421325683594\n",
      "23: Encoding Loss 17.865934371948242, Transition Loss 2.2648446559906006, Classifier Loss 0.3870885968208313, Total Loss 182.0893096923828\n",
      "23: Encoding Loss 18.1656494140625, Transition Loss 2.3213467597961426, Classifier Loss 0.33250364661216736, Total Loss 179.03982543945312\n",
      "23: Encoding Loss 19.11922264099121, Transition Loss 2.059828996658325, Classifier Loss 0.3491833209991455, Total Loss 188.28408813476562\n",
      "23: Encoding Loss 18.07763671875, Transition Loss 2.933539628982544, Classifier Loss 0.4108371138572693, Total Loss 186.29150390625\n",
      "23: Encoding Loss 19.173978805541992, Transition Loss 2.084195852279663, Classifier Loss 0.3689509630203247, Total Loss 190.70376586914062\n",
      "23: Encoding Loss 19.000659942626953, Transition Loss 2.4518682956695557, Classifier Loss 0.3608311414718628, Total Loss 188.57876586914062\n",
      "23: Encoding Loss 18.880292892456055, Transition Loss 2.5004935264587402, Classifier Loss 0.37693846225738525, Total Loss 189.2362823486328\n",
      "23: Encoding Loss 18.066070556640625, Transition Loss 2.2864136695861816, Classifier Loss 0.38049787282943726, Total Loss 183.03562927246094\n",
      "23: Encoding Loss 19.515295028686523, Transition Loss 1.7496519088745117, Classifier Loss 0.36691030859947205, Total Loss 193.16331481933594\n",
      "23: Encoding Loss 18.04121208190918, Transition Loss 2.119964122772217, Classifier Loss 0.3623281717300415, Total Loss 180.98651123046875\n",
      "23: Encoding Loss 18.65154266357422, Transition Loss 2.2640202045440674, Classifier Loss 0.3863140642642975, Total Loss 188.2965545654297\n",
      "23: Encoding Loss 18.449920654296875, Transition Loss 2.0524866580963135, Classifier Loss 0.3671114444732666, Total Loss 184.72100830078125\n",
      "23: Encoding Loss 19.043737411499023, Transition Loss 2.494230270385742, Classifier Loss 0.3728567361831665, Total Loss 190.13441467285156\n",
      "23: Encoding Loss 19.333650588989258, Transition Loss 1.4942454099655151, Classifier Loss 0.36478281021118164, Total Loss 191.4463348388672\n",
      "23: Encoding Loss 17.594648361206055, Transition Loss 2.3366384506225586, Classifier Loss 0.3734639883041382, Total Loss 178.57090759277344\n",
      "23: Encoding Loss 18.575199127197266, Transition Loss 1.9572323560714722, Classifier Loss 0.36356300115585327, Total Loss 185.34933471679688\n",
      "23: Encoding Loss 18.80094337463379, Transition Loss 2.5329856872558594, Classifier Loss 0.3901818096637726, Total Loss 189.93231201171875\n",
      "23: Encoding Loss 18.621923446655273, Transition Loss 2.1355419158935547, Classifier Loss 0.3655100464820862, Total Loss 185.95350646972656\n",
      "23: Encoding Loss 18.89129638671875, Transition Loss 1.9946064949035645, Classifier Loss 0.372903436422348, Total Loss 188.81964111328125\n",
      "23: Encoding Loss 18.823625564575195, Transition Loss 1.7718851566314697, Classifier Loss 0.36262762546539307, Total Loss 187.20614624023438\n",
      "23: Encoding Loss 19.07986831665039, Transition Loss 2.2379963397979736, Classifier Loss 0.43567192554473877, Total Loss 196.65374755859375\n",
      "23: Encoding Loss 18.682621002197266, Transition Loss 2.0482258796691895, Classifier Loss 0.36111029982566833, Total Loss 185.9816436767578\n",
      "23: Encoding Loss 19.410823822021484, Transition Loss 1.6981223821640015, Classifier Loss 0.36103397607803345, Total Loss 191.72962951660156\n",
      "23: Encoding Loss 18.992145538330078, Transition Loss 1.9050891399383545, Classifier Loss 0.36903998255729675, Total Loss 189.22216796875\n",
      "23: Encoding Loss 17.523584365844727, Transition Loss 2.03143310546875, Classifier Loss 0.3556540012359619, Total Loss 176.1603546142578\n",
      "23: Encoding Loss 19.088741302490234, Transition Loss 2.014803886413574, Classifier Loss 0.41891300678253174, Total Loss 195.0041961669922\n",
      "23: Encoding Loss 18.601274490356445, Transition Loss 2.0015361309051514, Classifier Loss 0.3893905580043793, Total Loss 188.14956665039062\n",
      "23: Encoding Loss 17.729841232299805, Transition Loss 2.6837751865386963, Classifier Loss 0.3783290386199951, Total Loss 180.20838928222656\n",
      "23: Encoding Loss 19.35525894165039, Transition Loss 1.7863729000091553, Classifier Loss 0.36948564648628235, Total Loss 192.1479034423828\n",
      "23: Encoding Loss 19.182039260864258, Transition Loss 2.007657766342163, Classifier Loss 0.40237197279930115, Total Loss 194.0950469970703\n",
      "23: Encoding Loss 18.890758514404297, Transition Loss 1.809382677078247, Classifier Loss 0.39968204498291016, Total Loss 191.45616149902344\n",
      "23: Encoding Loss 17.9061336517334, Transition Loss 2.624441623687744, Classifier Loss 0.37954938411712646, Total Loss 181.72889709472656\n",
      "23: Encoding Loss 18.841068267822266, Transition Loss 2.6093039512634277, Classifier Loss 0.37788885831832886, Total Loss 189.03929138183594\n",
      "23: Encoding Loss 18.308921813964844, Transition Loss 2.3421244621276855, Classifier Loss 0.36620640754699707, Total Loss 183.56044006347656\n",
      "23: Encoding Loss 17.24095916748047, Transition Loss 2.9734485149383545, Classifier Loss 0.3521937131881714, Total Loss 173.7417449951172\n",
      "23: Encoding Loss 18.166934967041016, Transition Loss 2.2407402992248535, Classifier Loss 0.37395164370536804, Total Loss 183.17880249023438\n",
      "23: Encoding Loss 18.756025314331055, Transition Loss 2.2749953269958496, Classifier Loss 0.38410496711730957, Total Loss 188.9136962890625\n",
      "23: Encoding Loss 17.060279846191406, Transition Loss 2.5700674057006836, Classifier Loss 0.3745388686656952, Total Loss 174.45013427734375\n",
      "23: Encoding Loss 18.586841583251953, Transition Loss 2.538614511489868, Classifier Loss 0.3850180208683014, Total Loss 187.70425415039062\n",
      "23: Encoding Loss 18.581989288330078, Transition Loss 2.3070292472839355, Classifier Loss 0.37103742361068726, Total Loss 186.2210693359375\n",
      "23: Encoding Loss 19.566577911376953, Transition Loss 2.1558568477630615, Classifier Loss 0.3847755789756775, Total Loss 195.44134521484375\n",
      "23: Encoding Loss 18.399452209472656, Transition Loss 2.3319809436798096, Classifier Loss 0.34505966305732727, Total Loss 182.16798400878906\n",
      "23: Encoding Loss 18.76776885986328, Transition Loss 1.8939812183380127, Classifier Loss 0.34591907262802124, Total Loss 185.11285400390625\n",
      "23: Encoding Loss 17.84126853942871, Transition Loss 2.0295679569244385, Classifier Loss 0.3863178491592407, Total Loss 181.76785278320312\n",
      "23: Encoding Loss 20.049541473388672, Transition Loss 2.2764179706573486, Classifier Loss 0.37099596858024597, Total Loss 197.9512176513672\n",
      "23: Encoding Loss 18.911500930786133, Transition Loss 1.802587866783142, Classifier Loss 0.3807876706123352, Total Loss 189.73129272460938\n",
      "23: Encoding Loss 17.594846725463867, Transition Loss 2.0640413761138916, Classifier Loss 0.36769065260887146, Total Loss 177.94064331054688\n",
      "23: Encoding Loss 18.50151824951172, Transition Loss 2.242363691329956, Classifier Loss 0.3638076186180115, Total Loss 184.8413848876953\n",
      "23: Encoding Loss 18.681194305419922, Transition Loss 1.9947876930236816, Classifier Loss 0.37881195545196533, Total Loss 187.72972106933594\n",
      "23: Encoding Loss 17.891897201538086, Transition Loss 1.6214793920516968, Classifier Loss 0.37627503275871277, Total Loss 181.08697509765625\n",
      "23: Encoding Loss 17.82431983947754, Transition Loss 2.0104188919067383, Classifier Loss 0.3545963168144226, Total Loss 178.45628356933594\n",
      "23: Encoding Loss 18.757427215576172, Transition Loss 2.458115339279175, Classifier Loss 0.38188499212265015, Total Loss 188.7395477294922\n",
      "23: Encoding Loss 18.4909725189209, Transition Loss 1.6272791624069214, Classifier Loss 0.34830477833747864, Total Loss 183.08370971679688\n",
      "23: Encoding Loss 19.153451919555664, Transition Loss 1.5228215456008911, Classifier Loss 0.3858542740345001, Total Loss 192.11761474609375\n",
      "23: Encoding Loss 17.549283981323242, Transition Loss 2.5480120182037354, Classifier Loss 0.37490391731262207, Total Loss 178.39425659179688\n",
      "23: Encoding Loss 18.36924934387207, Transition Loss 2.7487266063690186, Classifier Loss 0.34913673996925354, Total Loss 182.41741943359375\n",
      "23: Encoding Loss 18.019378662109375, Transition Loss 2.4595139026641846, Classifier Loss 0.40691351890563965, Total Loss 185.33827209472656\n",
      "23: Encoding Loss 18.876842498779297, Transition Loss 2.510869264602661, Classifier Loss 0.3975711166858673, Total Loss 191.27403259277344\n",
      "23: Encoding Loss 18.19911766052246, Transition Loss 3.169832706451416, Classifier Loss 0.39448365569114685, Total Loss 185.67527770996094\n",
      "23: Encoding Loss 19.126808166503906, Transition Loss 1.9946460723876953, Classifier Loss 0.3707617223262787, Total Loss 190.48956298828125\n",
      "23: Encoding Loss 18.866605758666992, Transition Loss 1.9529902935028076, Classifier Loss 0.3357200026512146, Total Loss 184.89544677734375\n",
      "23: Encoding Loss 17.823816299438477, Transition Loss 2.4085233211517334, Classifier Loss 0.34634461998939514, Total Loss 177.70669555664062\n",
      "23: Encoding Loss 18.075244903564453, Transition Loss 2.0751569271087646, Classifier Loss 0.3879845142364502, Total Loss 183.8154296875\n",
      "23: Encoding Loss 18.08086585998535, Transition Loss 2.381455183029175, Classifier Loss 0.3733992576599121, Total Loss 182.463134765625\n",
      "23: Encoding Loss 18.055965423583984, Transition Loss 2.1245269775390625, Classifier Loss 0.38077428936958313, Total Loss 182.95008850097656\n",
      "23: Encoding Loss 18.40343475341797, Transition Loss 1.8653523921966553, Classifier Loss 0.37247344851493835, Total Loss 184.847900390625\n",
      "23: Encoding Loss 18.49336814880371, Transition Loss 1.866777777671814, Classifier Loss 0.3729276955127716, Total Loss 185.61306762695312\n",
      "23: Encoding Loss 17.02341651916504, Transition Loss 2.3658318519592285, Classifier Loss 0.3505139946937561, Total Loss 171.71189880371094\n",
      "23: Encoding Loss 17.091806411743164, Transition Loss 2.4155750274658203, Classifier Loss 0.3785502314567566, Total Loss 175.0725860595703\n",
      "23: Encoding Loss 19.888643264770508, Transition Loss 1.8885947465896606, Classifier Loss 0.4087153971195221, Total Loss 200.3583984375\n",
      "23: Encoding Loss 18.261892318725586, Transition Loss 2.6941514015197754, Classifier Loss 0.35144487023353577, Total Loss 181.77845764160156\n",
      "23: Encoding Loss 18.884050369262695, Transition Loss 2.143979072570801, Classifier Loss 0.39565008878707886, Total Loss 191.06622314453125\n",
      "23: Encoding Loss 18.979890823364258, Transition Loss 2.4267919063568115, Classifier Loss 0.3872482478618622, Total Loss 191.04930114746094\n",
      "23: Encoding Loss 18.736047744750977, Transition Loss 2.1506545543670654, Classifier Loss 0.3691052794456482, Total Loss 187.22903442382812\n",
      "23: Encoding Loss 17.689958572387695, Transition Loss 2.4678211212158203, Classifier Loss 0.37788817286491394, Total Loss 179.8020477294922\n",
      "23: Encoding Loss 19.471607208251953, Transition Loss 1.4695472717285156, Classifier Loss 0.3592577278614044, Total Loss 191.9925537109375\n",
      "23: Encoding Loss 17.656152725219727, Transition Loss 1.7045544385910034, Classifier Loss 0.3553144037723541, Total Loss 177.12158203125\n",
      "23: Encoding Loss 18.39955711364746, Transition Loss 2.157691717147827, Classifier Loss 0.38540005683898926, Total Loss 186.16799926757812\n",
      "23: Encoding Loss 17.043455123901367, Transition Loss 2.4243063926696777, Classifier Loss 0.38597437739372253, Total Loss 175.42994689941406\n",
      "23: Encoding Loss 17.385799407958984, Transition Loss 1.862809419631958, Classifier Loss 0.3607552945613861, Total Loss 175.5345001220703\n",
      "23: Encoding Loss 19.157188415527344, Transition Loss 2.598562479019165, Classifier Loss 0.36564797163009644, Total Loss 190.34201049804688\n",
      "23: Encoding Loss 18.491329193115234, Transition Loss 1.9759111404418945, Classifier Loss 0.36121055483818054, Total Loss 184.44688415527344\n",
      "23: Encoding Loss 20.165271759033203, Transition Loss 1.901976227760315, Classifier Loss 0.3896603286266327, Total Loss 200.66860961914062\n",
      "23: Encoding Loss 19.69173812866211, Transition Loss 2.2047948837280273, Classifier Loss 0.3652110993862152, Total Loss 194.49598693847656\n",
      "23: Encoding Loss 18.79103660583496, Transition Loss 2.5565502643585205, Classifier Loss 0.34908172488212585, Total Loss 185.74777221679688\n",
      "23: Encoding Loss 18.09903907775879, Transition Loss 3.542644739151001, Classifier Loss 0.3666747212409973, Total Loss 182.16830444335938\n",
      "24: Encoding Loss 18.935382843017578, Transition Loss 1.837216854095459, Classifier Loss 0.3743743896484375, Total Loss 189.28794860839844\n",
      "24: Encoding Loss 19.018932342529297, Transition Loss 2.0123329162597656, Classifier Loss 0.3762318193912506, Total Loss 190.1771240234375\n",
      "24: Encoding Loss 19.620119094848633, Transition Loss 1.8826961517333984, Classifier Loss 0.3734678626060486, Total Loss 194.6842803955078\n",
      "24: Encoding Loss 18.45522689819336, Transition Loss 2.031604290008545, Classifier Loss 0.3921552300453186, Total Loss 187.26368713378906\n",
      "24: Encoding Loss 19.7257022857666, Transition Loss 1.5570021867752075, Classifier Loss 0.37504827976226807, Total Loss 195.62185668945312\n",
      "24: Encoding Loss 17.47876739501953, Transition Loss 2.7546446323394775, Classifier Loss 0.3788585960865021, Total Loss 178.26693725585938\n",
      "24: Encoding Loss 19.082096099853516, Transition Loss 1.6773004531860352, Classifier Loss 0.3949691653251648, Total Loss 192.48915100097656\n",
      "24: Encoding Loss 20.347684860229492, Transition Loss 1.5384703874588013, Classifier Loss 0.40326881408691406, Total Loss 203.4160614013672\n",
      "24: Encoding Loss 18.592945098876953, Transition Loss 2.178105354309082, Classifier Loss 0.4240194261074066, Total Loss 191.5811309814453\n",
      "24: Encoding Loss 20.029117584228516, Transition Loss 1.638370156288147, Classifier Loss 0.3831178545951843, Total Loss 198.8723907470703\n",
      "24: Encoding Loss 19.154558181762695, Transition Loss 1.935348391532898, Classifier Loss 0.40553542971611023, Total Loss 194.1770782470703\n",
      "24: Encoding Loss 18.780851364135742, Transition Loss 1.8826113939285278, Classifier Loss 0.3629571199417114, Total Loss 186.91905212402344\n",
      "24: Encoding Loss 18.094697952270508, Transition Loss 2.110912322998047, Classifier Loss 0.37655431032180786, Total Loss 182.83518981933594\n",
      "24: Encoding Loss 18.252321243286133, Transition Loss 2.0880672931671143, Classifier Loss 0.3730316460132599, Total Loss 183.73934936523438\n",
      "24: Encoding Loss 18.07204246520996, Transition Loss 2.369441509246826, Classifier Loss 0.3916630446910858, Total Loss 184.21653747558594\n",
      "24: Encoding Loss 18.63624382019043, Transition Loss 2.1536786556243896, Classifier Loss 0.3592163920402527, Total Loss 185.4423370361328\n",
      "24: Encoding Loss 18.503847122192383, Transition Loss 2.285719633102417, Classifier Loss 0.368287056684494, Total Loss 185.31661987304688\n",
      "24: Encoding Loss 18.351600646972656, Transition Loss 2.070319652557373, Classifier Loss 0.36294040083885193, Total Loss 183.52090454101562\n",
      "24: Encoding Loss 18.85967445373535, Transition Loss 2.129314422607422, Classifier Loss 0.32493650913238525, Total Loss 183.79690551757812\n",
      "24: Encoding Loss 19.505998611450195, Transition Loss 1.8872852325439453, Classifier Loss 0.34736254811286926, Total Loss 191.1616973876953\n",
      "24: Encoding Loss 18.593650817871094, Transition Loss 2.623809337615967, Classifier Loss 0.3983314633369446, Total Loss 189.10711669921875\n",
      "24: Encoding Loss 18.22568130493164, Transition Loss 1.9494577646255493, Classifier Loss 0.3553616404533386, Total Loss 181.73150634765625\n",
      "24: Encoding Loss 17.745464324951172, Transition Loss 3.269111394882202, Classifier Loss 0.360766738653183, Total Loss 178.69422912597656\n",
      "24: Encoding Loss 17.43646240234375, Transition Loss 2.0006914138793945, Classifier Loss 0.34884193539619446, Total Loss 174.77601623535156\n",
      "24: Encoding Loss 18.47722053527832, Transition Loss 1.9512183666229248, Classifier Loss 0.3419685661792755, Total Loss 182.4048614501953\n",
      "24: Encoding Loss 19.1995906829834, Transition Loss 2.18353271484375, Classifier Loss 0.3712812066078186, Total Loss 191.16156005859375\n",
      "24: Encoding Loss 19.4412899017334, Transition Loss 2.178377866744995, Classifier Loss 0.38079071044921875, Total Loss 194.04505920410156\n",
      "24: Encoding Loss 18.45891761779785, Transition Loss 2.3825418949127197, Classifier Loss 0.3819965124130249, Total Loss 186.3474884033203\n",
      "24: Encoding Loss 18.111413955688477, Transition Loss 2.3282852172851562, Classifier Loss 0.35461387038230896, Total Loss 180.81834411621094\n",
      "24: Encoding Loss 18.087350845336914, Transition Loss 2.832798957824707, Classifier Loss 0.40864479541778564, Total Loss 186.12985229492188\n",
      "24: Encoding Loss 18.409801483154297, Transition Loss 2.336862802505493, Classifier Loss 0.3719460666179657, Total Loss 184.94041442871094\n",
      "24: Encoding Loss 17.29576301574707, Transition Loss 2.746001720428467, Classifier Loss 0.3928757309913635, Total Loss 178.202880859375\n",
      "24: Encoding Loss 18.99036407470703, Transition Loss 2.361802816390991, Classifier Loss 0.39085209369659424, Total Loss 191.48048400878906\n",
      "24: Encoding Loss 17.600854873657227, Transition Loss 1.9910027980804443, Classifier Loss 0.38329386711120605, Total Loss 179.534423828125\n",
      "24: Encoding Loss 17.923870086669922, Transition Loss 2.436746597290039, Classifier Loss 0.3911360800266266, Total Loss 182.99192810058594\n",
      "24: Encoding Loss 17.18326187133789, Transition Loss 2.1140847206115723, Classifier Loss 0.3711017370223999, Total Loss 174.99908447265625\n",
      "24: Encoding Loss 18.8424015045166, Transition Loss 2.2909626960754395, Classifier Loss 0.38199523091316223, Total Loss 189.3969268798828\n",
      "24: Encoding Loss 18.64353370666504, Transition Loss 2.984539031982422, Classifier Loss 0.4047604203224182, Total Loss 190.22122192382812\n",
      "24: Encoding Loss 18.867292404174805, Transition Loss 1.9702162742614746, Classifier Loss 0.35096806287765503, Total Loss 186.42918395996094\n",
      "24: Encoding Loss 18.1722354888916, Transition Loss 2.606113910675049, Classifier Loss 0.3674620985984802, Total Loss 182.6453094482422\n",
      "24: Encoding Loss 18.049694061279297, Transition Loss 2.838066816329956, Classifier Loss 0.3965030312538147, Total Loss 184.615478515625\n",
      "24: Encoding Loss 17.959918975830078, Transition Loss 2.071591854095459, Classifier Loss 0.3691045343875885, Total Loss 181.00413513183594\n",
      "24: Encoding Loss 17.867734909057617, Transition Loss 1.9867335557937622, Classifier Loss 0.38684162497520447, Total Loss 182.0233917236328\n",
      "24: Encoding Loss 17.64346694946289, Transition Loss 1.8024307489395142, Classifier Loss 0.3947373926639557, Total Loss 180.98196411132812\n",
      "24: Encoding Loss 19.391050338745117, Transition Loss 1.8659968376159668, Classifier Loss 0.40985485911369324, Total Loss 196.48709106445312\n",
      "24: Encoding Loss 19.36505699157715, Transition Loss 2.2826144695281982, Classifier Loss 0.394268274307251, Total Loss 194.80381774902344\n",
      "24: Encoding Loss 18.685523986816406, Transition Loss 2.7451391220092773, Classifier Loss 0.39870205521583557, Total Loss 189.90342712402344\n",
      "24: Encoding Loss 19.437721252441406, Transition Loss 1.957069993019104, Classifier Loss 0.38847827911376953, Total Loss 194.7410125732422\n",
      "24: Encoding Loss 19.072715759277344, Transition Loss 2.226546287536621, Classifier Loss 0.3539811968803406, Total Loss 188.42515563964844\n",
      "24: Encoding Loss 18.7407169342041, Transition Loss 2.321554183959961, Classifier Loss 0.3538510799407959, Total Loss 185.77516174316406\n",
      "24: Encoding Loss 17.912755966186523, Transition Loss 1.8747471570968628, Classifier Loss 0.3668522238731384, Total Loss 180.3622283935547\n",
      "24: Encoding Loss 18.0267333984375, Transition Loss 1.9644315242767334, Classifier Loss 0.3615259528160095, Total Loss 180.75933837890625\n",
      "24: Encoding Loss 17.54251480102539, Transition Loss 2.0068023204803467, Classifier Loss 0.361538827419281, Total Loss 176.89537048339844\n",
      "24: Encoding Loss 18.331562042236328, Transition Loss 2.501176595687866, Classifier Loss 0.36989980936050415, Total Loss 184.14271545410156\n",
      "24: Encoding Loss 17.8919734954834, Transition Loss 2.77780818939209, Classifier Loss 0.3844626843929291, Total Loss 182.1376190185547\n",
      "24: Encoding Loss 17.045658111572266, Transition Loss 2.65765643119812, Classifier Loss 0.39621543884277344, Total Loss 176.51834106445312\n",
      "24: Encoding Loss 18.3140811920166, Transition Loss 2.261068344116211, Classifier Loss 0.35468122363090515, Total Loss 182.4329833984375\n",
      "24: Encoding Loss 17.829830169677734, Transition Loss 2.441948175430298, Classifier Loss 0.34205183386802673, Total Loss 177.3322296142578\n",
      "24: Encoding Loss 17.91016387939453, Transition Loss 2.3180148601531982, Classifier Loss 0.3498128652572632, Total Loss 178.72621154785156\n",
      "24: Encoding Loss 17.616273880004883, Transition Loss 2.1943416595458984, Classifier Loss 0.3546817898750305, Total Loss 176.83724975585938\n",
      "24: Encoding Loss 19.039661407470703, Transition Loss 2.2916481494903564, Classifier Loss 0.411040723323822, Total Loss 193.8796844482422\n",
      "24: Encoding Loss 19.85736846923828, Transition Loss 1.8291804790496826, Classifier Loss 0.36636286973953247, Total Loss 195.86106872558594\n",
      "24: Encoding Loss 18.58134651184082, Transition Loss 2.152791976928711, Classifier Loss 0.3521646559238434, Total Loss 184.29779052734375\n",
      "24: Encoding Loss 18.38193130493164, Transition Loss 2.1358795166015625, Classifier Loss 0.36539679765701294, Total Loss 184.0222930908203\n",
      "24: Encoding Loss 17.56502342224121, Transition Loss 2.3225908279418945, Classifier Loss 0.3647904694080353, Total Loss 177.46376037597656\n",
      "24: Encoding Loss 17.19886589050293, Transition Loss 2.3180887699127197, Classifier Loss 0.3790473937988281, Total Loss 175.95928955078125\n",
      "24: Encoding Loss 18.47972869873047, Transition Loss 2.1625070571899414, Classifier Loss 0.3730248808860779, Total Loss 185.57281494140625\n",
      "24: Encoding Loss 19.67694091796875, Transition Loss 1.8777636289596558, Classifier Loss 0.3560328185558319, Total Loss 193.3943634033203\n",
      "24: Encoding Loss 19.645030975341797, Transition Loss 2.4320597648620605, Classifier Loss 0.3897446393966675, Total Loss 196.6211395263672\n",
      "24: Encoding Loss 18.39507484436035, Transition Loss 2.202873706817627, Classifier Loss 0.3770143389701843, Total Loss 185.3026123046875\n",
      "24: Encoding Loss 19.12978744506836, Transition Loss 1.937501311302185, Classifier Loss 0.35090771317481995, Total Loss 188.51658630371094\n",
      "24: Encoding Loss 17.678617477416992, Transition Loss 2.306906223297119, Classifier Loss 0.34783971309661865, Total Loss 176.67428588867188\n",
      "24: Encoding Loss 17.82422637939453, Transition Loss 2.589472532272339, Classifier Loss 0.34894728660583496, Total Loss 178.00643920898438\n",
      "24: Encoding Loss 19.018972396850586, Transition Loss 2.1601452827453613, Classifier Loss 0.32877978682518005, Total Loss 185.46177673339844\n",
      "24: Encoding Loss 17.79683494567871, Transition Loss 3.239262342453003, Classifier Loss 0.4090413451194763, Total Loss 183.92666625976562\n",
      "24: Encoding Loss 19.004610061645508, Transition Loss 2.117520809173584, Classifier Loss 0.36597415804862976, Total Loss 189.05780029296875\n",
      "24: Encoding Loss 18.550867080688477, Transition Loss 2.723827600479126, Classifier Loss 0.35657042264938354, Total Loss 184.60874938964844\n",
      "24: Encoding Loss 18.6086368560791, Transition Loss 2.4359211921691895, Classifier Loss 0.35387516021728516, Total Loss 184.74378967285156\n",
      "24: Encoding Loss 17.773984909057617, Transition Loss 2.6053175926208496, Classifier Loss 0.3339024782180786, Total Loss 176.10317993164062\n",
      "24: Encoding Loss 19.35085105895996, Transition Loss 1.6246767044067383, Classifier Loss 0.3513037860393524, Total Loss 190.2621307373047\n",
      "24: Encoding Loss 17.98891830444336, Transition Loss 2.597566604614258, Classifier Loss 0.3391660153865814, Total Loss 178.3474884033203\n",
      "24: Encoding Loss 18.52207374572754, Transition Loss 1.962050437927246, Classifier Loss 0.35140982270240784, Total Loss 183.70997619628906\n",
      "24: Encoding Loss 18.382272720336914, Transition Loss 2.6015853881835938, Classifier Loss 0.35198578238487244, Total Loss 182.77706909179688\n",
      "24: Encoding Loss 18.918498992919922, Transition Loss 2.290426731109619, Classifier Loss 0.35947543382644653, Total Loss 187.75363159179688\n",
      "24: Encoding Loss 19.342199325561523, Transition Loss 1.8885140419006348, Classifier Loss 0.37078964710235596, Total Loss 192.1942596435547\n",
      "24: Encoding Loss 17.70054054260254, Transition Loss 2.2899675369262695, Classifier Loss 0.35849058628082275, Total Loss 177.911376953125\n",
      "24: Encoding Loss 18.49905776977539, Transition Loss 2.273458480834961, Classifier Loss 0.4150787591934204, Total Loss 189.9550323486328\n",
      "24: Encoding Loss 18.61332893371582, Transition Loss 2.6603870391845703, Classifier Loss 0.36475270986557007, Total Loss 185.91397094726562\n",
      "24: Encoding Loss 18.378986358642578, Transition Loss 2.399570941925049, Classifier Loss 0.40120044350624084, Total Loss 187.63185119628906\n",
      "24: Encoding Loss 18.57316017150879, Transition Loss 1.9782177209854126, Classifier Loss 0.3451084792613983, Total Loss 183.4917755126953\n",
      "24: Encoding Loss 18.605710983276367, Transition Loss 1.9979543685913086, Classifier Loss 0.35939648747444153, Total Loss 185.1849365234375\n",
      "24: Encoding Loss 19.022483825683594, Transition Loss 2.397582769393921, Classifier Loss 0.4412754476070404, Total Loss 196.7869415283203\n",
      "24: Encoding Loss 18.48329734802246, Transition Loss 2.29183292388916, Classifier Loss 0.349176824092865, Total Loss 183.242431640625\n",
      "24: Encoding Loss 19.348085403442383, Transition Loss 1.809009313583374, Classifier Loss 0.3357774019241333, Total Loss 188.72422790527344\n",
      "24: Encoding Loss 18.870527267456055, Transition Loss 2.059699535369873, Classifier Loss 0.36318421363830566, Total Loss 187.694580078125\n",
      "24: Encoding Loss 17.524152755737305, Transition Loss 2.0357818603515625, Classifier Loss 0.3675987720489502, Total Loss 177.36024475097656\n",
      "24: Encoding Loss 18.909271240234375, Transition Loss 2.1774120330810547, Classifier Loss 0.3981686234474182, Total Loss 191.52651977539062\n",
      "24: Encoding Loss 18.38815689086914, Transition Loss 2.0685036182403564, Classifier Loss 0.3787311017513275, Total Loss 185.39205932617188\n",
      "24: Encoding Loss 17.604684829711914, Transition Loss 2.785181999206543, Classifier Loss 0.37356188893318176, Total Loss 178.75070190429688\n",
      "24: Encoding Loss 19.339550018310547, Transition Loss 1.826788306236267, Classifier Loss 0.347189724445343, Total Loss 189.80075073242188\n",
      "24: Encoding Loss 18.97624969482422, Transition Loss 2.0638980865478516, Classifier Loss 0.383319616317749, Total Loss 190.55474853515625\n",
      "24: Encoding Loss 18.724130630493164, Transition Loss 1.8853890895843506, Classifier Loss 0.4163230061531067, Total Loss 191.80242919921875\n",
      "24: Encoding Loss 17.879308700561523, Transition Loss 2.7490310668945312, Classifier Loss 0.39041662216186523, Total Loss 182.62594604492188\n",
      "24: Encoding Loss 18.486421585083008, Transition Loss 2.6733782291412354, Classifier Loss 0.3819698691368103, Total Loss 186.623046875\n",
      "24: Encoding Loss 18.152334213256836, Transition Loss 2.394993543624878, Classifier Loss 0.37355607748031616, Total Loss 183.05328369140625\n",
      "24: Encoding Loss 17.011266708374023, Transition Loss 3.069821834564209, Classifier Loss 0.33809301257133484, Total Loss 170.51339721679688\n",
      "24: Encoding Loss 18.440031051635742, Transition Loss 2.3382821083068848, Classifier Loss 0.38408946990966797, Total Loss 186.3968505859375\n",
      "24: Encoding Loss 18.56223487854004, Transition Loss 2.3888649940490723, Classifier Loss 0.3920559585094452, Total Loss 188.18124389648438\n",
      "24: Encoding Loss 16.720272064208984, Transition Loss 2.6648082733154297, Classifier Loss 0.35604751110076904, Total Loss 169.89990234375\n",
      "24: Encoding Loss 18.603227615356445, Transition Loss 2.5986924171447754, Classifier Loss 0.3548375964164734, Total Loss 184.82933044433594\n",
      "24: Encoding Loss 18.331226348876953, Transition Loss 2.4346182346343994, Classifier Loss 0.3576068580150604, Total Loss 182.8974151611328\n",
      "24: Encoding Loss 19.19622039794922, Transition Loss 2.204629421234131, Classifier Loss 0.347192645072937, Total Loss 188.72996520996094\n",
      "24: Encoding Loss 18.320280075073242, Transition Loss 2.3871734142303467, Classifier Loss 0.37715041637420654, Total Loss 184.7547149658203\n",
      "24: Encoding Loss 18.868988037109375, Transition Loss 1.931096076965332, Classifier Loss 0.3785301446914673, Total Loss 189.19113159179688\n",
      "24: Encoding Loss 17.800729751586914, Transition Loss 2.1128411293029785, Classifier Loss 0.3590991795063019, Total Loss 178.7383270263672\n",
      "24: Encoding Loss 20.071733474731445, Transition Loss 2.406066417694092, Classifier Loss 0.3936465084552765, Total Loss 200.41973876953125\n",
      "24: Encoding Loss 18.643341064453125, Transition Loss 1.972632884979248, Classifier Loss 0.3830348253250122, Total Loss 187.84474182128906\n",
      "24: Encoding Loss 17.659069061279297, Transition Loss 2.113788604736328, Classifier Loss 0.34841388463974, Total Loss 176.53671264648438\n",
      "24: Encoding Loss 18.09755516052246, Transition Loss 2.3376052379608154, Classifier Loss 0.3802092671394348, Total Loss 183.2688751220703\n",
      "24: Encoding Loss 18.43339729309082, Transition Loss 2.049906015396118, Classifier Loss 0.3804689049720764, Total Loss 185.92405700683594\n",
      "24: Encoding Loss 17.703325271606445, Transition Loss 1.6567485332489014, Classifier Loss 0.3345324993133545, Total Loss 175.41119384765625\n",
      "24: Encoding Loss 17.50882339477539, Transition Loss 2.0250487327575684, Classifier Loss 0.38592180609703064, Total Loss 179.06777954101562\n",
      "24: Encoding Loss 18.663450241088867, Transition Loss 2.5021164417266846, Classifier Loss 0.3764781653881073, Total Loss 187.45584106445312\n",
      "24: Encoding Loss 18.24156379699707, Transition Loss 1.6734983921051025, Classifier Loss 0.35229790210723877, Total Loss 181.4969940185547\n",
      "24: Encoding Loss 19.097679138183594, Transition Loss 1.5783039331436157, Classifier Loss 0.3531617224216461, Total Loss 188.41326904296875\n",
      "24: Encoding Loss 17.621829986572266, Transition Loss 2.5581235885620117, Classifier Loss 0.36555320024490356, Total Loss 178.04159545898438\n",
      "24: Encoding Loss 18.074668884277344, Transition Loss 2.824446678161621, Classifier Loss 0.3716469705104828, Total Loss 182.3269500732422\n",
      "24: Encoding Loss 17.912752151489258, Transition Loss 2.525705337524414, Classifier Loss 0.38154730200767517, Total Loss 181.96189880371094\n",
      "24: Encoding Loss 18.410791397094727, Transition Loss 2.5766191482543945, Classifier Loss 0.3963199257850647, Total Loss 187.4336395263672\n",
      "24: Encoding Loss 17.94414520263672, Transition Loss 3.269747734069824, Classifier Loss 0.3891557455062866, Total Loss 183.1226806640625\n",
      "24: Encoding Loss 19.03379249572754, Transition Loss 2.0534515380859375, Classifier Loss 0.32695603370666504, Total Loss 185.3766326904297\n",
      "24: Encoding Loss 18.707347869873047, Transition Loss 2.0219337940216064, Classifier Loss 0.3526345491409302, Total Loss 185.32664489746094\n",
      "24: Encoding Loss 17.557239532470703, Transition Loss 2.420050621032715, Classifier Loss 0.35496458411216736, Total Loss 176.43838500976562\n",
      "24: Encoding Loss 17.880258560180664, Transition Loss 2.102762222290039, Classifier Loss 0.38410285115242004, Total Loss 181.87290954589844\n",
      "24: Encoding Loss 17.744016647338867, Transition Loss 2.4198036193847656, Classifier Loss 0.42579180002212524, Total Loss 185.01527404785156\n",
      "24: Encoding Loss 17.92629623413086, Transition Loss 2.1903796195983887, Classifier Loss 0.36633503437042236, Total Loss 180.48196411132812\n",
      "24: Encoding Loss 18.187746047973633, Transition Loss 1.9333431720733643, Classifier Loss 0.3474215865135193, Total Loss 180.63079833984375\n",
      "24: Encoding Loss 18.041494369506836, Transition Loss 1.975193738937378, Classifier Loss 0.40131625533103943, Total Loss 184.85861206054688\n",
      "24: Encoding Loss 16.76618766784668, Transition Loss 2.3678712844848633, Classifier Loss 0.3589091897010803, Total Loss 170.49398803710938\n",
      "24: Encoding Loss 16.959476470947266, Transition Loss 2.4675185680389404, Classifier Loss 0.30783790349960327, Total Loss 166.95309448242188\n",
      "24: Encoding Loss 19.73505401611328, Transition Loss 1.947990894317627, Classifier Loss 0.3926500380039215, Total Loss 197.5350341796875\n",
      "24: Encoding Loss 17.995908737182617, Transition Loss 2.769355297088623, Classifier Loss 0.3756557106971741, Total Loss 182.08670043945312\n",
      "24: Encoding Loss 18.70648765563965, Transition Loss 2.1962640285491943, Classifier Loss 0.4003239870071411, Total Loss 190.12355041503906\n",
      "24: Encoding Loss 18.694211959838867, Transition Loss 2.5129129886627197, Classifier Loss 0.37088829278945923, Total Loss 187.1450958251953\n",
      "24: Encoding Loss 18.523303985595703, Transition Loss 2.170910120010376, Classifier Loss 0.3795683979988098, Total Loss 186.5774688720703\n",
      "24: Encoding Loss 17.46099090576172, Transition Loss 2.4755136966705322, Classifier Loss 0.3565639853477478, Total Loss 175.8394317626953\n",
      "24: Encoding Loss 19.149131774902344, Transition Loss 1.5012751817703247, Classifier Loss 0.3687976598739624, Total Loss 190.37307739257812\n",
      "24: Encoding Loss 17.51616668701172, Transition Loss 1.7323665618896484, Classifier Loss 0.38316598534584045, Total Loss 178.79238891601562\n",
      "24: Encoding Loss 18.18911361694336, Transition Loss 2.1688523292541504, Classifier Loss 0.33932000398635864, Total Loss 179.87869262695312\n",
      "24: Encoding Loss 16.94991111755371, Transition Loss 2.4847004413604736, Classifier Loss 0.3394278287887573, Total Loss 170.03900146484375\n",
      "24: Encoding Loss 17.175785064697266, Transition Loss 1.846420407295227, Classifier Loss 0.3703530430793762, Total Loss 174.8108673095703\n",
      "24: Encoding Loss 18.79629898071289, Transition Loss 2.699957847595215, Classifier Loss 0.3851860761642456, Total Loss 189.4290008544922\n",
      "24: Encoding Loss 18.101144790649414, Transition Loss 1.97439444065094, Classifier Loss 0.3498799502849579, Total Loss 180.19203186035156\n",
      "24: Encoding Loss 19.98189926147461, Transition Loss 1.9864863157272339, Classifier Loss 0.369809091091156, Total Loss 197.23341369628906\n",
      "24: Encoding Loss 19.641040802001953, Transition Loss 2.1881515979766846, Classifier Loss 0.37247157096862793, Total Loss 194.81312561035156\n",
      "24: Encoding Loss 18.642948150634766, Transition Loss 2.7772481441497803, Classifier Loss 0.3655428886413574, Total Loss 186.25332641601562\n",
      "24: Encoding Loss 16.93851089477539, Transition Loss 3.5535051822662354, Classifier Loss 0.35705721378326416, Total Loss 171.9245147705078\n",
      "25: Encoding Loss 18.940305709838867, Transition Loss 1.9783775806427002, Classifier Loss 0.3430621027946472, Total Loss 186.22433471679688\n",
      "25: Encoding Loss 19.064367294311523, Transition Loss 1.904687523841858, Classifier Loss 0.3679242432117462, Total Loss 189.68829345703125\n",
      "25: Encoding Loss 19.42514419555664, Transition Loss 2.0353400707244873, Classifier Loss 0.38653677701950073, Total Loss 194.4619140625\n",
      "25: Encoding Loss 18.26642417907715, Transition Loss 1.9586927890777588, Classifier Loss 0.3566815257072449, Total Loss 182.1912841796875\n",
      "25: Encoding Loss 19.718671798706055, Transition Loss 1.756368637084961, Classifier Loss 0.375593900680542, Total Loss 195.6600341796875\n",
      "25: Encoding Loss 17.282438278198242, Transition Loss 2.6106674671173096, Classifier Loss 0.4169241786003113, Total Loss 180.47406005859375\n",
      "25: Encoding Loss 18.983922958374023, Transition Loss 1.7503252029418945, Classifier Loss 0.39271873235702515, Total Loss 191.49331665039062\n",
      "25: Encoding Loss 20.192140579223633, Transition Loss 1.5969669818878174, Classifier Loss 0.38265588879585266, Total Loss 200.1221160888672\n",
      "25: Encoding Loss 18.393220901489258, Transition Loss 2.134312391281128, Classifier Loss 0.4381576180458069, Total Loss 191.38839721679688\n",
      "25: Encoding Loss 19.981740951538086, Transition Loss 1.7193942070007324, Classifier Loss 0.40062105655670166, Total Loss 200.25990295410156\n",
      "25: Encoding Loss 18.822595596313477, Transition Loss 1.886549472808838, Classifier Loss 0.396884560585022, Total Loss 190.6465301513672\n",
      "25: Encoding Loss 18.539737701416016, Transition Loss 1.956913948059082, Classifier Loss 0.40037527680397034, Total Loss 188.746826171875\n",
      "25: Encoding Loss 18.077592849731445, Transition Loss 1.9836153984069824, Classifier Loss 0.39864835143089294, Total Loss 184.88230895996094\n",
      "25: Encoding Loss 18.065309524536133, Transition Loss 2.1954941749572754, Classifier Loss 0.3756738007068634, Total Loss 182.52896118164062\n",
      "25: Encoding Loss 17.76514434814453, Transition Loss 2.2776854038238525, Classifier Loss 0.3776954114437103, Total Loss 180.3462371826172\n",
      "25: Encoding Loss 18.522268295288086, Transition Loss 2.2657036781311035, Classifier Loss 0.40605539083480835, Total Loss 189.23683166503906\n",
      "25: Encoding Loss 18.168970108032227, Transition Loss 2.2239861488342285, Classifier Loss 0.38287171721458435, Total Loss 184.08372497558594\n",
      "25: Encoding Loss 18.163921356201172, Transition Loss 2.1443445682525635, Classifier Loss 0.33480843901634216, Total Loss 179.22109985351562\n",
      "25: Encoding Loss 18.479551315307617, Transition Loss 2.038607597351074, Classifier Loss 0.3421211838722229, Total Loss 182.45623779296875\n",
      "25: Encoding Loss 19.392549514770508, Transition Loss 2.049466609954834, Classifier Loss 0.32302126288414, Total Loss 187.8524169921875\n",
      "25: Encoding Loss 18.445499420166016, Transition Loss 2.450042963027954, Classifier Loss 0.3807215690612793, Total Loss 186.12615966796875\n",
      "25: Encoding Loss 17.868375778198242, Transition Loss 2.0218465328216553, Classifier Loss 0.3328738212585449, Total Loss 176.63876342773438\n",
      "25: Encoding Loss 17.61157989501953, Transition Loss 3.14042067527771, Classifier Loss 0.3701644241809845, Total Loss 178.53717041015625\n",
      "25: Encoding Loss 17.49441909790039, Transition Loss 2.0079357624053955, Classifier Loss 0.3692178726196289, Total Loss 177.27871704101562\n",
      "25: Encoding Loss 18.28914451599121, Transition Loss 1.9137887954711914, Classifier Loss 0.3407815396785736, Total Loss 180.77406311035156\n",
      "25: Encoding Loss 19.0684757232666, Transition Loss 2.158254623413086, Classifier Loss 0.3846762776374817, Total Loss 191.4470977783203\n",
      "25: Encoding Loss 19.244630813598633, Transition Loss 2.210577964782715, Classifier Loss 0.36661601066589355, Total Loss 191.06076049804688\n",
      "25: Encoding Loss 18.406618118286133, Transition Loss 2.3097081184387207, Classifier Loss 0.3516925275325775, Total Loss 182.88414001464844\n",
      "25: Encoding Loss 18.01310157775879, Transition Loss 2.309528350830078, Classifier Loss 0.37377479672431946, Total Loss 181.94419860839844\n",
      "25: Encoding Loss 17.857927322387695, Transition Loss 2.7579617500305176, Classifier Loss 0.38416317105293274, Total Loss 181.83132934570312\n",
      "25: Encoding Loss 18.19013214111328, Transition Loss 2.3092832565307617, Classifier Loss 0.35652679204940796, Total Loss 181.63558959960938\n",
      "25: Encoding Loss 17.060787200927734, Transition Loss 2.7100701332092285, Classifier Loss 0.3446009159088135, Total Loss 171.48841857910156\n",
      "25: Encoding Loss 18.96090316772461, Transition Loss 2.3456618785858154, Classifier Loss 0.36571404337882996, Total Loss 188.72776794433594\n",
      "25: Encoding Loss 17.24910545349121, Transition Loss 1.9363933801651, Classifier Loss 0.36351343989372253, Total Loss 174.73147583007812\n",
      "25: Encoding Loss 17.77374267578125, Transition Loss 2.366248369216919, Classifier Loss 0.37904229760169983, Total Loss 180.5674285888672\n",
      "25: Encoding Loss 16.87700080871582, Transition Loss 2.0554800033569336, Classifier Loss 0.36709001660346985, Total Loss 172.1361083984375\n",
      "25: Encoding Loss 18.510358810424805, Transition Loss 2.216139316558838, Classifier Loss 0.3578149080276489, Total Loss 184.30758666992188\n",
      "25: Encoding Loss 18.723995208740234, Transition Loss 2.913217544555664, Classifier Loss 0.3684547245502472, Total Loss 187.2200927734375\n",
      "25: Encoding Loss 18.795011520385742, Transition Loss 1.90411376953125, Classifier Loss 0.3487193286418915, Total Loss 185.61285400390625\n",
      "25: Encoding Loss 18.1219425201416, Transition Loss 2.5112369060516357, Classifier Loss 0.3582538366317749, Total Loss 181.30316162109375\n",
      "25: Encoding Loss 17.75403594970703, Transition Loss 2.6347036361694336, Classifier Loss 0.3792895972728729, Total Loss 180.48818969726562\n",
      "25: Encoding Loss 17.61861228942871, Transition Loss 2.027505874633789, Classifier Loss 0.37155550718307495, Total Loss 178.50994873046875\n",
      "25: Encoding Loss 17.731718063354492, Transition Loss 1.867503046989441, Classifier Loss 0.33181828260421753, Total Loss 175.40907287597656\n",
      "25: Encoding Loss 17.270822525024414, Transition Loss 1.7975614070892334, Classifier Loss 0.3783974349498749, Total Loss 176.36582946777344\n",
      "25: Encoding Loss 19.228900909423828, Transition Loss 1.7683271169662476, Classifier Loss 0.3899172246456146, Total Loss 193.17660522460938\n",
      "25: Encoding Loss 19.047420501708984, Transition Loss 2.2529854774475098, Classifier Loss 0.4008866846561432, Total Loss 192.91864013671875\n",
      "25: Encoding Loss 18.44412612915039, Transition Loss 2.596785545349121, Classifier Loss 0.37340396642684937, Total Loss 185.4127655029297\n",
      "25: Encoding Loss 19.2568359375, Transition Loss 1.9471017122268677, Classifier Loss 0.37507155537605286, Total Loss 191.95126342773438\n",
      "25: Encoding Loss 18.62868881225586, Transition Loss 2.112658739089966, Classifier Loss 0.36279213428497314, Total Loss 185.7312774658203\n",
      "25: Encoding Loss 18.66835594177246, Transition Loss 2.2882392406463623, Classifier Loss 0.35188040137290955, Total Loss 184.99252319335938\n",
      "25: Encoding Loss 17.9444522857666, Transition Loss 1.808504343032837, Classifier Loss 0.3605220913887024, Total Loss 179.96951293945312\n",
      "25: Encoding Loss 17.859935760498047, Transition Loss 1.942771315574646, Classifier Loss 0.34836170077323914, Total Loss 178.10421752929688\n",
      "25: Encoding Loss 17.352787017822266, Transition Loss 1.9384543895721436, Classifier Loss 0.3563249707221985, Total Loss 174.8424835205078\n",
      "25: Encoding Loss 18.185863494873047, Transition Loss 2.478252649307251, Classifier Loss 0.36958664655685425, Total Loss 182.9412384033203\n",
      "25: Encoding Loss 17.493864059448242, Transition Loss 2.666569471359253, Classifier Loss 0.3666388690471649, Total Loss 177.1481170654297\n",
      "25: Encoding Loss 16.80704689025879, Transition Loss 2.6401216983795166, Classifier Loss 0.383752703666687, Total Loss 173.35968017578125\n",
      "25: Encoding Loss 18.00100326538086, Transition Loss 2.212085247039795, Classifier Loss 0.39093172550201416, Total Loss 183.5436248779297\n",
      "25: Encoding Loss 17.6524715423584, Transition Loss 2.4789810180664062, Classifier Loss 0.36682939529418945, Total Loss 178.39849853515625\n",
      "25: Encoding Loss 17.655364990234375, Transition Loss 2.2065110206604004, Classifier Loss 0.3778217136859894, Total Loss 179.4663848876953\n",
      "25: Encoding Loss 17.438873291015625, Transition Loss 2.2457962036132812, Classifier Loss 0.3479887843132019, Total Loss 174.75901794433594\n",
      "25: Encoding Loss 18.92133331298828, Transition Loss 2.238959789276123, Classifier Loss 0.3704015016555786, Total Loss 188.8585968017578\n",
      "25: Encoding Loss 19.895273208618164, Transition Loss 1.8916875123977661, Classifier Loss 0.40215012431144714, Total Loss 199.7555389404297\n",
      "25: Encoding Loss 18.427400588989258, Transition Loss 2.1448264122009277, Classifier Loss 0.3637620210647583, Total Loss 184.22438049316406\n",
      "25: Encoding Loss 18.180082321166992, Transition Loss 2.181079626083374, Classifier Loss 0.35378187894821167, Total Loss 181.25506591796875\n",
      "25: Encoding Loss 17.33091163635254, Transition Loss 2.2983524799346924, Classifier Loss 0.3580317497253418, Total Loss 174.91014099121094\n",
      "25: Encoding Loss 17.04969024658203, Transition Loss 2.3526546955108643, Classifier Loss 0.34428879618644714, Total Loss 171.29693603515625\n",
      "25: Encoding Loss 18.27168083190918, Transition Loss 2.1166443824768066, Classifier Loss 0.3493620753288269, Total Loss 181.53297424316406\n",
      "25: Encoding Loss 19.484973907470703, Transition Loss 1.9559520483016968, Classifier Loss 0.3336918354034424, Total Loss 189.64016723632812\n",
      "25: Encoding Loss 19.407379150390625, Transition Loss 2.3375213146209717, Classifier Loss 0.37542590498924255, Total Loss 193.2691192626953\n",
      "25: Encoding Loss 17.934255599975586, Transition Loss 2.2762913703918457, Classifier Loss 0.39345189929008484, Total Loss 183.2744903564453\n",
      "25: Encoding Loss 18.95000648498535, Transition Loss 1.7962870597839355, Classifier Loss 0.3521801233291626, Total Loss 187.1773223876953\n",
      "25: Encoding Loss 17.451993942260742, Transition Loss 2.404304265975952, Classifier Loss 0.3485887348651886, Total Loss 174.9556884765625\n",
      "25: Encoding Loss 17.81434440612793, Transition Loss 2.3886334896087646, Classifier Loss 0.3530261516571045, Total Loss 178.2950897216797\n",
      "25: Encoding Loss 18.86640739440918, Transition Loss 2.250220775604248, Classifier Loss 0.3471877872943878, Total Loss 186.10008239746094\n",
      "25: Encoding Loss 17.741411209106445, Transition Loss 3.1045942306518555, Classifier Loss 0.41508567333221436, Total Loss 184.06077575683594\n",
      "25: Encoding Loss 18.867816925048828, Transition Loss 2.2676172256469727, Classifier Loss 0.37515607476234436, Total Loss 188.9116668701172\n",
      "25: Encoding Loss 18.43582534790039, Transition Loss 2.6508662700653076, Classifier Loss 0.3636316657066345, Total Loss 184.3799285888672\n",
      "25: Encoding Loss 18.47744369506836, Transition Loss 2.690971851348877, Classifier Loss 0.35671788454055786, Total Loss 184.029541015625\n",
      "25: Encoding Loss 17.56649398803711, Transition Loss 2.443000078201294, Classifier Loss 0.34485965967178345, Total Loss 175.50653076171875\n",
      "25: Encoding Loss 19.134241104125977, Transition Loss 1.9404361248016357, Classifier Loss 0.35122978687286377, Total Loss 188.58499145507812\n",
      "25: Encoding Loss 17.946489334106445, Transition Loss 2.2648990154266357, Classifier Loss 0.3665695786476135, Total Loss 180.68185424804688\n",
      "25: Encoding Loss 18.360811233520508, Transition Loss 2.393798589706421, Classifier Loss 0.3754316568374634, Total Loss 184.90841674804688\n",
      "25: Encoding Loss 18.07973289489746, Transition Loss 2.2484803199768066, Classifier Loss 0.3778263032436371, Total Loss 182.8701934814453\n",
      "25: Encoding Loss 18.624670028686523, Transition Loss 2.6525189876556396, Classifier Loss 0.3591150641441345, Total Loss 185.4393768310547\n",
      "25: Encoding Loss 19.059106826782227, Transition Loss 1.7284719944000244, Classifier Loss 0.35575491189956665, Total Loss 188.39402770996094\n",
      "25: Encoding Loss 17.26667594909668, Transition Loss 2.4531500339508057, Classifier Loss 0.35303133726119995, Total Loss 173.9271697998047\n",
      "25: Encoding Loss 18.095943450927734, Transition Loss 2.0828521251678467, Classifier Loss 0.3818740248680115, Total Loss 183.37152099609375\n",
      "25: Encoding Loss 18.63434600830078, Transition Loss 2.78218412399292, Classifier Loss 0.3816301226615906, Total Loss 187.79421997070312\n",
      "25: Encoding Loss 18.2506160736084, Transition Loss 2.247490644454956, Classifier Loss 0.36041465401649475, Total Loss 182.49588012695312\n",
      "25: Encoding Loss 18.31387710571289, Transition Loss 2.1159143447875977, Classifier Loss 0.35242438316345215, Total Loss 182.1766357421875\n",
      "25: Encoding Loss 18.588809967041016, Transition Loss 1.9510014057159424, Classifier Loss 0.3376255929470062, Total Loss 182.8632354736328\n",
      "25: Encoding Loss 18.881488800048828, Transition Loss 2.558150291442871, Classifier Loss 0.405941367149353, Total Loss 192.1576690673828\n",
      "25: Encoding Loss 18.31780242919922, Transition Loss 2.325531482696533, Classifier Loss 0.37242189049720764, Total Loss 184.2497100830078\n",
      "25: Encoding Loss 19.205944061279297, Transition Loss 1.9086918830871582, Classifier Loss 0.37246137857437134, Total Loss 191.27545166015625\n",
      "25: Encoding Loss 18.673297882080078, Transition Loss 2.106738328933716, Classifier Loss 0.34944549202919006, Total Loss 184.7522735595703\n",
      "25: Encoding Loss 17.26652717590332, Transition Loss 2.0729832649230957, Classifier Loss 0.3560236394405365, Total Loss 174.14918518066406\n",
      "25: Encoding Loss 18.660646438598633, Transition Loss 2.246519088745117, Classifier Loss 0.41384536027908325, Total Loss 191.1190185546875\n",
      "25: Encoding Loss 18.1801815032959, Transition Loss 2.121532917022705, Classifier Loss 0.367633581161499, Total Loss 182.6291046142578\n",
      "25: Encoding Loss 17.389774322509766, Transition Loss 2.8506767749786377, Classifier Loss 0.37110021710395813, Total Loss 176.79833984375\n",
      "25: Encoding Loss 19.145219802856445, Transition Loss 1.879615306854248, Classifier Loss 0.36466556787490845, Total Loss 190.0042266845703\n",
      "25: Encoding Loss 18.66857147216797, Transition Loss 2.1437807083129883, Classifier Loss 0.3857601284980774, Total Loss 188.3533477783203\n",
      "25: Encoding Loss 18.5230770111084, Transition Loss 1.9257400035858154, Classifier Loss 0.3896832764148712, Total Loss 187.5380859375\n",
      "25: Encoding Loss 17.701868057250977, Transition Loss 2.7698779106140137, Classifier Loss 0.40027108788490295, Total Loss 182.19602966308594\n",
      "25: Encoding Loss 18.21843147277832, Transition Loss 2.7300302982330322, Classifier Loss 0.3751061260700226, Total Loss 183.80406188964844\n",
      "25: Encoding Loss 17.766590118408203, Transition Loss 2.4745101928710938, Classifier Loss 0.3507576286792755, Total Loss 177.70338439941406\n",
      "25: Encoding Loss 16.878196716308594, Transition Loss 3.1215953826904297, Classifier Loss 0.3592589199542999, Total Loss 171.57577514648438\n",
      "25: Encoding Loss 17.998788833618164, Transition Loss 2.418548583984375, Classifier Loss 0.3909737169742584, Total Loss 183.57138061523438\n",
      "25: Encoding Loss 18.40669822692871, Transition Loss 2.4672775268554688, Classifier Loss 0.36779308319091797, Total Loss 184.52635192871094\n",
      "25: Encoding Loss 16.45878791809082, Transition Loss 2.64350962638855, Classifier Loss 0.3618829846382141, Total Loss 168.38729858398438\n",
      "25: Encoding Loss 18.307628631591797, Transition Loss 2.662454843521118, Classifier Loss 0.34065186977386475, Total Loss 181.0587158203125\n",
      "25: Encoding Loss 18.215251922607422, Transition Loss 2.47987699508667, Classifier Loss 0.3418882489204407, Total Loss 180.40682983398438\n",
      "25: Encoding Loss 19.14713478088379, Transition Loss 2.3387856483459473, Classifier Loss 0.38982173800468445, Total Loss 192.62701416015625\n",
      "25: Encoding Loss 18.01270294189453, Transition Loss 2.4789633750915527, Classifier Loss 0.38640516996383667, Total Loss 183.23793029785156\n",
      "25: Encoding Loss 18.557641983032227, Transition Loss 2.027090549468994, Classifier Loss 0.3361515402793884, Total Loss 182.48170471191406\n",
      "25: Encoding Loss 17.64385223388672, Transition Loss 2.134509801864624, Classifier Loss 0.39844322204589844, Total Loss 181.4220428466797\n",
      "25: Encoding Loss 19.996850967407227, Transition Loss 2.521252393722534, Classifier Loss 0.37484210729599, Total Loss 197.96327209472656\n",
      "25: Encoding Loss 18.42412567138672, Transition Loss 2.039076805114746, Classifier Loss 0.3799348771572113, Total Loss 185.7943115234375\n",
      "25: Encoding Loss 17.11946678161621, Transition Loss 2.211416006088257, Classifier Loss 0.3323330581188202, Total Loss 170.63131713867188\n",
      "25: Encoding Loss 18.013586044311523, Transition Loss 2.3968310356140137, Classifier Loss 0.338739275932312, Total Loss 178.4619903564453\n",
      "25: Encoding Loss 18.177398681640625, Transition Loss 2.122433662414551, Classifier Loss 0.37728890776634216, Total Loss 183.5725555419922\n",
      "25: Encoding Loss 17.4273681640625, Transition Loss 1.6814815998077393, Classifier Loss 0.3596135377883911, Total Loss 175.7165985107422\n",
      "25: Encoding Loss 17.38323211669922, Transition Loss 2.0329291820526123, Classifier Loss 0.3375227749347687, Total Loss 173.22471618652344\n",
      "25: Encoding Loss 18.426677703857422, Transition Loss 2.5267531871795654, Classifier Loss 0.35600095987319946, Total Loss 183.51889038085938\n",
      "25: Encoding Loss 18.354114532470703, Transition Loss 1.6905558109283447, Classifier Loss 0.3532135784626007, Total Loss 182.49237060546875\n",
      "25: Encoding Loss 18.711111068725586, Transition Loss 1.6514414548873901, Classifier Loss 0.36429065465927124, Total Loss 186.4482421875\n",
      "25: Encoding Loss 17.29152488708496, Transition Loss 2.6026110649108887, Classifier Loss 0.35433533787727356, Total Loss 174.2862548828125\n",
      "25: Encoding Loss 17.778688430786133, Transition Loss 2.8305671215057373, Classifier Loss 0.35971692204475403, Total Loss 178.76731872558594\n",
      "25: Encoding Loss 17.705907821655273, Transition Loss 2.514569044113159, Classifier Loss 0.36861762404441833, Total Loss 179.01194763183594\n",
      "25: Encoding Loss 18.407752990722656, Transition Loss 2.640570878982544, Classifier Loss 0.38464993238449097, Total Loss 186.255126953125\n",
      "25: Encoding Loss 17.63924217224121, Transition Loss 3.305614948272705, Classifier Loss 0.3907961845397949, Total Loss 180.85467529296875\n",
      "25: Encoding Loss 18.883426666259766, Transition Loss 2.14945650100708, Classifier Loss 0.3691074252128601, Total Loss 188.4080352783203\n",
      "25: Encoding Loss 18.560136795043945, Transition Loss 2.1007184982299805, Classifier Loss 0.3357839286327362, Total Loss 182.47962951660156\n",
      "25: Encoding Loss 17.232830047607422, Transition Loss 2.463378667831421, Classifier Loss 0.3541984558105469, Total Loss 173.77517700195312\n",
      "25: Encoding Loss 17.67876625061035, Transition Loss 2.1666789054870605, Classifier Loss 0.3732283115386963, Total Loss 179.18629455566406\n",
      "25: Encoding Loss 17.63874626159668, Transition Loss 2.4999167919158936, Classifier Loss 0.34491202235221863, Total Loss 176.1011505126953\n",
      "25: Encoding Loss 17.78864288330078, Transition Loss 2.266714096069336, Classifier Loss 0.3462407886981964, Total Loss 177.38656616210938\n",
      "25: Encoding Loss 17.844459533691406, Transition Loss 1.9927858114242554, Classifier Loss 0.37121307849884033, Total Loss 180.27554321289062\n",
      "25: Encoding Loss 17.99485969543457, Transition Loss 2.0295515060424805, Classifier Loss 0.378581702709198, Total Loss 182.22296142578125\n",
      "25: Encoding Loss 16.496807098388672, Transition Loss 2.400580883026123, Classifier Loss 0.321973979473114, Total Loss 164.65199279785156\n",
      "25: Encoding Loss 17.089757919311523, Transition Loss 2.5169644355773926, Classifier Loss 0.35447949171066284, Total Loss 172.66940307617188\n",
      "25: Encoding Loss 19.534610748291016, Transition Loss 2.0064406394958496, Classifier Loss 0.3878333866596222, Total Loss 195.46151733398438\n",
      "25: Encoding Loss 17.81703758239746, Transition Loss 2.882359504699707, Classifier Loss 0.38278892636299133, Total Loss 181.39166259765625\n",
      "25: Encoding Loss 18.58780860900879, Transition Loss 2.266606330871582, Classifier Loss 0.3869391083717346, Total Loss 187.84970092773438\n",
      "25: Encoding Loss 18.66952896118164, Transition Loss 2.6337451934814453, Classifier Loss 0.3958100378513336, Total Loss 189.4639892578125\n",
      "25: Encoding Loss 18.35942268371582, Transition Loss 2.2791919708251953, Classifier Loss 0.35981881618499756, Total Loss 183.3131103515625\n",
      "25: Encoding Loss 17.338199615478516, Transition Loss 2.6129074096679688, Classifier Loss 0.36432331800460815, Total Loss 175.66050720214844\n",
      "25: Encoding Loss 19.3386173248291, Transition Loss 1.596754550933838, Classifier Loss 0.35389459133148193, Total Loss 190.41773986816406\n",
      "25: Encoding Loss 17.228925704956055, Transition Loss 1.7775788307189941, Classifier Loss 0.35283732414245605, Total Loss 173.4706573486328\n",
      "25: Encoding Loss 17.98993682861328, Transition Loss 2.208193778991699, Classifier Loss 0.35950714349746704, Total Loss 180.3118438720703\n",
      "25: Encoding Loss 16.67222023010254, Transition Loss 2.5150508880615234, Classifier Loss 0.38050657510757446, Total Loss 171.93142700195312\n",
      "25: Encoding Loss 17.012422561645508, Transition Loss 1.8546738624572754, Classifier Loss 0.37345659732818604, Total Loss 173.81597900390625\n",
      "25: Encoding Loss 18.548582077026367, Transition Loss 2.804269313812256, Classifier Loss 0.3656509518623352, Total Loss 185.5146026611328\n",
      "25: Encoding Loss 17.99231719970703, Transition Loss 2.024168014526367, Classifier Loss 0.3508741855621338, Total Loss 179.4307861328125\n",
      "25: Encoding Loss 20.014026641845703, Transition Loss 2.1291351318359375, Classifier Loss 0.35943055152893066, Total Loss 196.48109436035156\n",
      "25: Encoding Loss 19.473793029785156, Transition Loss 2.354071617126465, Classifier Loss 0.3377637267112732, Total Loss 190.0375213623047\n",
      "25: Encoding Loss 18.488859176635742, Transition Loss 2.975101947784424, Classifier Loss 0.32715290784835815, Total Loss 181.22117614746094\n",
      "25: Encoding Loss 16.500444412231445, Transition Loss 3.7106072902679443, Classifier Loss 0.4001719057559967, Total Loss 172.76287841796875\n",
      "26: Encoding Loss 18.765743255615234, Transition Loss 2.0876283645629883, Classifier Loss 0.3435288071632385, Total Loss 184.89637756347656\n",
      "26: Encoding Loss 18.651994705200195, Transition Loss 2.0129339694976807, Classifier Loss 0.3633688688278198, Total Loss 185.95542907714844\n",
      "26: Encoding Loss 19.322004318237305, Transition Loss 2.164898157119751, Classifier Loss 0.37230050563812256, Total Loss 192.23907470703125\n",
      "26: Encoding Loss 18.095966339111328, Transition Loss 2.0530600547790527, Classifier Loss 0.35771942138671875, Total Loss 180.95028686523438\n",
      "26: Encoding Loss 19.35609245300293, Transition Loss 1.8925292491912842, Classifier Loss 0.3741004168987274, Total Loss 192.63729858398438\n",
      "26: Encoding Loss 16.976896286010742, Transition Loss 2.64929461479187, Classifier Loss 0.4043772518634796, Total Loss 176.7827606201172\n",
      "26: Encoding Loss 18.81547737121582, Transition Loss 1.8659530878067017, Classifier Loss 0.35770899057388306, Total Loss 186.66790771484375\n",
      "26: Encoding Loss 20.198606491088867, Transition Loss 1.7290419340133667, Classifier Loss 0.3787449598312378, Total Loss 199.8091583251953\n",
      "26: Encoding Loss 18.001163482666016, Transition Loss 2.254049301147461, Classifier Loss 0.4341522455215454, Total Loss 187.87533569335938\n",
      "26: Encoding Loss 19.94575309753418, Transition Loss 1.8577756881713867, Classifier Loss 0.3934342563152313, Total Loss 199.281005859375\n",
      "26: Encoding Loss 18.643695831298828, Transition Loss 1.9545819759368896, Classifier Loss 0.3324240744113922, Total Loss 182.7828826904297\n",
      "26: Encoding Loss 18.455936431884766, Transition Loss 2.0406410694122314, Classifier Loss 0.38470032811164856, Total Loss 186.52565002441406\n",
      "26: Encoding Loss 17.823522567749023, Transition Loss 2.1119301319122314, Classifier Loss 0.3473852276802063, Total Loss 177.74908447265625\n",
      "26: Encoding Loss 18.04662322998047, Transition Loss 2.293837308883667, Classifier Loss 0.34720635414123535, Total Loss 179.55239868164062\n",
      "26: Encoding Loss 17.44053077697754, Transition Loss 2.3829729557037354, Classifier Loss 0.3783787190914154, Total Loss 177.83871459960938\n",
      "26: Encoding Loss 18.365171432495117, Transition Loss 2.3940982818603516, Classifier Loss 0.3541966378688812, Total Loss 182.81985473632812\n",
      "26: Encoding Loss 17.94245719909668, Transition Loss 2.3150296211242676, Classifier Loss 0.3556501567363739, Total Loss 179.56768798828125\n",
      "26: Encoding Loss 18.031612396240234, Transition Loss 2.2613015174865723, Classifier Loss 0.3444988429546356, Total Loss 179.15504455566406\n",
      "26: Encoding Loss 18.420255661010742, Transition Loss 2.1473042964935303, Classifier Loss 0.31501898169517517, Total Loss 179.29339599609375\n",
      "26: Encoding Loss 19.20325469970703, Transition Loss 2.17427659034729, Classifier Loss 0.37555229663848877, Total Loss 191.6161346435547\n",
      "26: Encoding Loss 18.087427139282227, Transition Loss 2.5554754734039307, Classifier Loss 0.3911287486553192, Total Loss 184.32337951660156\n",
      "26: Encoding Loss 17.768465042114258, Transition Loss 2.0836102962493896, Classifier Loss 0.32821017503738403, Total Loss 175.3854522705078\n",
      "26: Encoding Loss 17.283206939697266, Transition Loss 3.284148693084717, Classifier Loss 0.35615718364715576, Total Loss 174.5382080078125\n",
      "26: Encoding Loss 17.188657760620117, Transition Loss 2.0868420600891113, Classifier Loss 0.3836226761341095, Total Loss 176.2888946533203\n",
      "26: Encoding Loss 17.852466583251953, Transition Loss 1.954797387123108, Classifier Loss 0.3599461019039154, Total Loss 179.20530700683594\n",
      "26: Encoding Loss 18.821348190307617, Transition Loss 2.258446455001831, Classifier Loss 0.3523891568183899, Total Loss 186.26138305664062\n",
      "26: Encoding Loss 18.94790267944336, Transition Loss 2.315742015838623, Classifier Loss 0.37164515256881714, Total Loss 189.21090698242188\n",
      "26: Encoding Loss 18.301645278930664, Transition Loss 2.4386439323425293, Classifier Loss 0.3639739453792572, Total Loss 183.2982940673828\n",
      "26: Encoding Loss 17.660884857177734, Transition Loss 2.4207184314727783, Classifier Loss 0.3497081398963928, Total Loss 176.74205017089844\n",
      "26: Encoding Loss 17.692155838012695, Transition Loss 2.8506298065185547, Classifier Loss 0.370668888092041, Total Loss 179.17425537109375\n",
      "26: Encoding Loss 18.256254196166992, Transition Loss 2.4021074771881104, Classifier Loss 0.36836951971054077, Total Loss 183.36741638183594\n",
      "26: Encoding Loss 16.807676315307617, Transition Loss 2.855687379837036, Classifier Loss 0.3759692311286926, Total Loss 172.6294708251953\n",
      "26: Encoding Loss 18.971126556396484, Transition Loss 2.469339370727539, Classifier Loss 0.3789403438568115, Total Loss 190.15692138671875\n",
      "26: Encoding Loss 17.15534019470215, Transition Loss 1.9594058990478516, Classifier Loss 0.36799508333206177, Total Loss 174.43411254882812\n",
      "26: Encoding Loss 17.552349090576172, Transition Loss 2.459300994873047, Classifier Loss 0.38465777039527893, Total Loss 179.37644958496094\n",
      "26: Encoding Loss 16.662525177001953, Transition Loss 2.1053082942962646, Classifier Loss 0.3844553828239441, Total Loss 172.16680908203125\n",
      "26: Encoding Loss 18.312110900878906, Transition Loss 2.326634168624878, Classifier Loss 0.3689248561859131, Total Loss 183.85470581054688\n",
      "26: Encoding Loss 18.511140823364258, Transition Loss 3.061495780944824, Classifier Loss 0.36248454451560974, Total Loss 184.94989013671875\n",
      "26: Encoding Loss 18.567663192749023, Transition Loss 1.9731677770614624, Classifier Loss 0.3641335964202881, Total Loss 185.34930419921875\n",
      "26: Encoding Loss 17.999267578125, Transition Loss 2.6616978645324707, Classifier Loss 0.34590139985084534, Total Loss 179.1166229248047\n",
      "26: Encoding Loss 17.53929328918457, Transition Loss 2.705669403076172, Classifier Loss 0.40643346309661865, Total Loss 181.4988250732422\n",
      "26: Encoding Loss 17.456157684326172, Transition Loss 2.102651596069336, Classifier Loss 0.39039334654808044, Total Loss 179.10914611816406\n",
      "26: Encoding Loss 17.434783935546875, Transition Loss 1.9213169813156128, Classifier Loss 0.34695377945899963, Total Loss 174.5579071044922\n",
      "26: Encoding Loss 17.07400131225586, Transition Loss 1.8189454078674316, Classifier Loss 0.37797319889068604, Total Loss 174.7531280517578\n",
      "26: Encoding Loss 19.050981521606445, Transition Loss 1.8326685428619385, Classifier Loss 0.3777620792388916, Total Loss 190.5505828857422\n",
      "26: Encoding Loss 18.871301651000977, Transition Loss 2.353123426437378, Classifier Loss 0.37621891498565674, Total Loss 189.06292724609375\n",
      "26: Encoding Loss 18.186351776123047, Transition Loss 2.6044673919677734, Classifier Loss 0.37657928466796875, Total Loss 183.66964721679688\n",
      "26: Encoding Loss 19.21562957763672, Transition Loss 2.006654977798462, Classifier Loss 0.3815973103046417, Total Loss 192.28610229492188\n",
      "26: Encoding Loss 18.68780517578125, Transition Loss 2.1379101276397705, Classifier Loss 0.3649594783782959, Total Loss 186.42596435546875\n",
      "26: Encoding Loss 18.497827529907227, Transition Loss 2.343743324279785, Classifier Loss 0.35495755076408386, Total Loss 183.94712829589844\n",
      "26: Encoding Loss 17.477319717407227, Transition Loss 1.8238389492034912, Classifier Loss 0.36317986249923706, Total Loss 176.5012969970703\n",
      "26: Encoding Loss 17.554508209228516, Transition Loss 1.9808275699615479, Classifier Loss 0.3504171371459961, Total Loss 175.8739471435547\n",
      "26: Encoding Loss 16.968687057495117, Transition Loss 1.9182097911834717, Classifier Loss 0.3614088296890259, Total Loss 172.27401733398438\n",
      "26: Encoding Loss 17.922466278076172, Transition Loss 2.443674325942993, Classifier Loss 0.36304447054862976, Total Loss 180.1729278564453\n",
      "26: Encoding Loss 17.451763153076172, Transition Loss 2.6034696102142334, Classifier Loss 0.3511822819709778, Total Loss 175.25303649902344\n",
      "26: Encoding Loss 16.58927345275879, Transition Loss 2.552678108215332, Classifier Loss 0.40094757080078125, Total Loss 173.31947326660156\n",
      "26: Encoding Loss 17.879491806030273, Transition Loss 2.15604829788208, Classifier Loss 0.3481045961380005, Total Loss 178.27760314941406\n",
      "26: Encoding Loss 17.37596321105957, Transition Loss 2.3927674293518066, Classifier Loss 0.3453909754753113, Total Loss 174.0253448486328\n",
      "26: Encoding Loss 17.43671417236328, Transition Loss 2.1753270626068115, Classifier Loss 0.35690733790397644, Total Loss 175.6195068359375\n",
      "26: Encoding Loss 17.30644416809082, Transition Loss 2.192753791809082, Classifier Loss 0.34789401292800903, Total Loss 173.67950439453125\n",
      "26: Encoding Loss 18.862279891967773, Transition Loss 2.244597911834717, Classifier Loss 0.3618435263633728, Total Loss 187.53150939941406\n",
      "26: Encoding Loss 19.59954071044922, Transition Loss 1.9177769422531128, Classifier Loss 0.3631698489189148, Total Loss 193.4968719482422\n",
      "26: Encoding Loss 18.117311477661133, Transition Loss 2.1310982704162598, Classifier Loss 0.3634885549545288, Total Loss 181.7135772705078\n",
      "26: Encoding Loss 18.01079559326172, Transition Loss 2.136885404586792, Classifier Loss 0.3810470402240753, Total Loss 182.6184539794922\n",
      "26: Encoding Loss 17.30051040649414, Transition Loss 2.2624387741088867, Classifier Loss 0.3348415493965149, Total Loss 172.34072875976562\n",
      "26: Encoding Loss 16.872648239135742, Transition Loss 2.305788993835449, Classifier Loss 0.3358628749847412, Total Loss 169.02862548828125\n",
      "26: Encoding Loss 18.027555465698242, Transition Loss 2.1064720153808594, Classifier Loss 0.3502298593521118, Total Loss 179.66473388671875\n",
      "26: Encoding Loss 19.281583786010742, Transition Loss 1.9563007354736328, Classifier Loss 0.32587289810180664, Total Loss 187.23123168945312\n",
      "26: Encoding Loss 19.26787757873535, Transition Loss 2.3821160793304443, Classifier Loss 0.3703984320163727, Total Loss 191.65928649902344\n",
      "26: Encoding Loss 17.9525089263916, Transition Loss 2.2700610160827637, Classifier Loss 0.39440420269966125, Total Loss 183.51449584960938\n",
      "26: Encoding Loss 18.852869033813477, Transition Loss 1.8753588199615479, Classifier Loss 0.3449244499206543, Total Loss 185.6904754638672\n",
      "26: Encoding Loss 17.15618324279785, Transition Loss 2.4243884086608887, Classifier Loss 0.34256690740585327, Total Loss 171.9910430908203\n",
      "26: Encoding Loss 17.48769187927246, Transition Loss 2.4259138107299805, Classifier Loss 0.3438737392425537, Total Loss 174.7740936279297\n",
      "26: Encoding Loss 18.574039459228516, Transition Loss 2.2893178462982178, Classifier Loss 0.34126728773117065, Total Loss 183.17691040039062\n",
      "26: Encoding Loss 17.491519927978516, Transition Loss 3.1034622192382812, Classifier Loss 0.3774987757205963, Total Loss 178.302734375\n",
      "26: Encoding Loss 18.611417770385742, Transition Loss 2.2818691730499268, Classifier Loss 0.3709624707698822, Total Loss 186.4439697265625\n",
      "26: Encoding Loss 18.23336410522461, Transition Loss 2.6772055625915527, Classifier Loss 0.3259675204753876, Total Loss 178.99913024902344\n",
      "26: Encoding Loss 18.324865341186523, Transition Loss 2.694204330444336, Classifier Loss 0.3551598787307739, Total Loss 182.65374755859375\n",
      "26: Encoding Loss 17.50896453857422, Transition Loss 2.414463758468628, Classifier Loss 0.35626882314682007, Total Loss 176.18148803710938\n",
      "26: Encoding Loss 19.088459014892578, Transition Loss 1.9574897289276123, Classifier Loss 0.36064770817756653, Total Loss 189.1639404296875\n",
      "26: Encoding Loss 17.490154266357422, Transition Loss 2.267730712890625, Classifier Loss 0.3667975664138794, Total Loss 177.0545654296875\n",
      "26: Encoding Loss 18.2642822265625, Transition Loss 2.4132256507873535, Classifier Loss 0.4011996388435364, Total Loss 186.7168731689453\n",
      "26: Encoding Loss 17.97764778137207, Transition Loss 2.2782068252563477, Classifier Loss 0.37120428681373596, Total Loss 181.3972625732422\n",
      "26: Encoding Loss 18.644712448120117, Transition Loss 2.737663984298706, Classifier Loss 0.3590598702430725, Total Loss 185.61122131347656\n",
      "26: Encoding Loss 18.975345611572266, Transition Loss 1.820816993713379, Classifier Loss 0.33988919854164124, Total Loss 186.15585327148438\n",
      "26: Encoding Loss 17.062864303588867, Transition Loss 2.4791078567504883, Classifier Loss 0.33806997537612915, Total Loss 170.80572509765625\n",
      "26: Encoding Loss 18.111522674560547, Transition Loss 2.115422248840332, Classifier Loss 0.36915796995162964, Total Loss 182.2310791015625\n",
      "26: Encoding Loss 18.198997497558594, Transition Loss 2.942826986312866, Classifier Loss 0.35600948333740234, Total Loss 181.781494140625\n",
      "26: Encoding Loss 17.9416561126709, Transition Loss 2.254375457763672, Classifier Loss 0.3367227613925934, Total Loss 177.65640258789062\n",
      "26: Encoding Loss 18.419532775878906, Transition Loss 2.127351760864258, Classifier Loss 0.3647834062576294, Total Loss 184.26007080078125\n",
      "26: Encoding Loss 18.065731048583984, Transition Loss 1.996911644935608, Classifier Loss 0.33966517448425293, Total Loss 178.8917694091797\n",
      "26: Encoding Loss 18.756900787353516, Transition Loss 2.5574235916137695, Classifier Loss 0.40199053287506104, Total Loss 190.7657470703125\n",
      "26: Encoding Loss 18.029865264892578, Transition Loss 2.292754650115967, Classifier Loss 0.34007909893989563, Total Loss 178.70538330078125\n",
      "26: Encoding Loss 19.15978240966797, Transition Loss 1.9687589406967163, Classifier Loss 0.32554715871810913, Total Loss 186.2267303466797\n",
      "26: Encoding Loss 18.55577278137207, Transition Loss 2.1612982749938965, Classifier Loss 0.3517278730869293, Total Loss 184.05123901367188\n",
      "26: Encoding Loss 17.091968536376953, Transition Loss 2.069913864135742, Classifier Loss 0.3225996792316437, Total Loss 169.40969848632812\n",
      "26: Encoding Loss 18.28165626525879, Transition Loss 2.369081735610962, Classifier Loss 0.3723863661289215, Total Loss 183.9656982421875\n",
      "26: Encoding Loss 17.986719131469727, Transition Loss 2.1083903312683105, Classifier Loss 0.3632144629955292, Total Loss 180.63687133789062\n",
      "26: Encoding Loss 17.258487701416016, Transition Loss 2.8556132316589355, Classifier Loss 0.3420484662055969, Total Loss 172.8438720703125\n",
      "26: Encoding Loss 18.855518341064453, Transition Loss 1.9010982513427734, Classifier Loss 0.3426823616027832, Total Loss 185.4925994873047\n",
      "26: Encoding Loss 18.482255935668945, Transition Loss 2.1786229610443115, Classifier Loss 0.33062809705734253, Total Loss 181.35658264160156\n",
      "26: Encoding Loss 18.408771514892578, Transition Loss 1.974395751953125, Classifier Loss 0.39478710293769836, Total Loss 187.14376831054688\n",
      "26: Encoding Loss 17.58991241455078, Transition Loss 2.807889461517334, Classifier Loss 0.33851754665374756, Total Loss 175.13262939453125\n",
      "26: Encoding Loss 18.09174156188965, Transition Loss 2.7451486587524414, Classifier Loss 0.3601972758769989, Total Loss 181.3026885986328\n",
      "26: Encoding Loss 17.656633377075195, Transition Loss 2.516646146774292, Classifier Loss 0.3594738841056824, Total Loss 177.7037811279297\n",
      "26: Encoding Loss 16.536073684692383, Transition Loss 3.1600704193115234, Classifier Loss 0.35524487495422363, Total Loss 168.44509887695312\n",
      "26: Encoding Loss 17.70799446105957, Transition Loss 2.416814088821411, Classifier Loss 0.39344310760498047, Total Loss 181.49163818359375\n",
      "26: Encoding Loss 18.240285873413086, Transition Loss 2.497551441192627, Classifier Loss 0.35332632064819336, Total Loss 181.75442504882812\n",
      "26: Encoding Loss 16.173574447631836, Transition Loss 2.646177291870117, Classifier Loss 0.3570418357849121, Total Loss 165.62200927734375\n",
      "26: Encoding Loss 18.083232879638672, Transition Loss 2.6787068843841553, Classifier Loss 0.3513789474964142, Total Loss 180.33950805664062\n",
      "26: Encoding Loss 18.02357292175293, Transition Loss 2.535327672958374, Classifier Loss 0.35606667399406433, Total Loss 180.3023223876953\n",
      "26: Encoding Loss 18.875457763671875, Transition Loss 2.383439302444458, Classifier Loss 0.3418668806552887, Total Loss 185.6670379638672\n",
      "26: Encoding Loss 17.71912956237793, Transition Loss 2.4628372192382812, Classifier Loss 0.363241970539093, Total Loss 178.56980895996094\n",
      "26: Encoding Loss 18.542156219482422, Transition Loss 2.0427589416503906, Classifier Loss 0.32275107502937317, Total Loss 181.0209197998047\n",
      "26: Encoding Loss 17.218524932861328, Transition Loss 2.1596264839172363, Classifier Loss 0.39935624599456787, Total Loss 178.11575317382812\n",
      "26: Encoding Loss 19.890201568603516, Transition Loss 2.5831966400146484, Classifier Loss 0.3607417047023773, Total Loss 195.71241760253906\n",
      "26: Encoding Loss 18.458370208740234, Transition Loss 2.1238925457000732, Classifier Loss 0.3665064871311188, Total Loss 184.74240112304688\n",
      "26: Encoding Loss 16.881988525390625, Transition Loss 2.286111831665039, Classifier Loss 0.342094361782074, Total Loss 169.7225799560547\n",
      "26: Encoding Loss 17.782291412353516, Transition Loss 2.4595794677734375, Classifier Loss 0.35060298442840576, Total Loss 177.810546875\n",
      "26: Encoding Loss 17.935876846313477, Transition Loss 2.185094118118286, Classifier Loss 0.37943923473358154, Total Loss 181.86795043945312\n",
      "26: Encoding Loss 17.28628158569336, Transition Loss 1.7318787574768066, Classifier Loss 0.3406480550765991, Total Loss 172.70144653320312\n",
      "26: Encoding Loss 17.2501220703125, Transition Loss 2.073652982711792, Classifier Loss 0.32804855704307556, Total Loss 171.22056579589844\n",
      "26: Encoding Loss 18.147268295288086, Transition Loss 2.5981407165527344, Classifier Loss 0.3701682686805725, Total Loss 182.714599609375\n",
      "26: Encoding Loss 18.08599281311035, Transition Loss 1.7682230472564697, Classifier Loss 0.34275999665260315, Total Loss 179.3175811767578\n",
      "26: Encoding Loss 18.74300193786621, Transition Loss 1.755906581878662, Classifier Loss 0.3568505048751831, Total Loss 185.98023986816406\n",
      "26: Encoding Loss 16.982200622558594, Transition Loss 2.671480417251587, Classifier Loss 0.3481142520904541, Total Loss 171.20333862304688\n",
      "26: Encoding Loss 17.569427490234375, Transition Loss 2.933070182800293, Classifier Loss 0.3530346751213074, Total Loss 176.44549560546875\n",
      "26: Encoding Loss 17.360130310058594, Transition Loss 2.565796375274658, Classifier Loss 0.3818516135215759, Total Loss 177.57936096191406\n",
      "26: Encoding Loss 18.03167724609375, Transition Loss 2.749208927154541, Classifier Loss 0.37407201528549194, Total Loss 182.21044921875\n",
      "26: Encoding Loss 17.50798988342285, Transition Loss 3.3739449977874756, Classifier Loss 0.3834705650806427, Total Loss 179.0857696533203\n",
      "26: Encoding Loss 18.594593048095703, Transition Loss 2.2355144023895264, Classifier Loss 0.33138713240623474, Total Loss 182.34255981445312\n",
      "26: Encoding Loss 18.4282169342041, Transition Loss 2.1698007583618164, Classifier Loss 0.32469773292541504, Total Loss 180.3294677734375\n",
      "26: Encoding Loss 17.185630798339844, Transition Loss 2.466843366622925, Classifier Loss 0.33961883187294006, Total Loss 171.94029235839844\n",
      "26: Encoding Loss 17.606014251708984, Transition Loss 2.189432144165039, Classifier Loss 0.36808693408966064, Total Loss 178.09471130371094\n",
      "26: Encoding Loss 17.447021484375, Transition Loss 2.521353244781494, Classifier Loss 0.3787688910961151, Total Loss 177.95733642578125\n",
      "26: Encoding Loss 17.673974990844727, Transition Loss 2.387964963912964, Classifier Loss 0.33699309825897217, Total Loss 175.56871032714844\n",
      "26: Encoding Loss 17.630489349365234, Transition Loss 2.111234188079834, Classifier Loss 0.3422452211380005, Total Loss 175.69068908691406\n",
      "26: Encoding Loss 17.70504379272461, Transition Loss 2.1771533489227295, Classifier Loss 0.3468053638935089, Total Loss 176.75633239746094\n",
      "26: Encoding Loss 16.218656539916992, Transition Loss 2.38112211227417, Classifier Loss 0.33072489500045776, Total Loss 163.2979736328125\n",
      "26: Encoding Loss 16.550249099731445, Transition Loss 2.5560405254364014, Classifier Loss 0.3507400453090668, Total Loss 167.98721313476562\n",
      "26: Encoding Loss 19.25861358642578, Transition Loss 2.0618083477020264, Classifier Loss 0.38140714168548584, Total Loss 192.6219940185547\n",
      "26: Encoding Loss 17.6800537109375, Transition Loss 2.923593521118164, Classifier Loss 0.3715136647224426, Total Loss 179.176513671875\n",
      "26: Encoding Loss 18.232248306274414, Transition Loss 2.310080051422119, Classifier Loss 0.3437846601009369, Total Loss 180.69847106933594\n",
      "26: Encoding Loss 18.529598236083984, Transition Loss 2.755579710006714, Classifier Loss 0.38606059551239014, Total Loss 187.39398193359375\n",
      "26: Encoding Loss 18.043546676635742, Transition Loss 2.326543092727661, Classifier Loss 0.3719390332698822, Total Loss 182.00758361816406\n",
      "26: Encoding Loss 17.050405502319336, Transition Loss 2.679123640060425, Classifier Loss 0.36668527126312256, Total Loss 173.60760498046875\n",
      "26: Encoding Loss 18.991743087768555, Transition Loss 1.6577153205871582, Classifier Loss 0.36063745617866516, Total Loss 188.32923889160156\n",
      "26: Encoding Loss 17.1580867767334, Transition Loss 1.8155698776245117, Classifier Loss 0.34061872959136963, Total Loss 171.68968200683594\n",
      "26: Encoding Loss 17.683563232421875, Transition Loss 2.2214627265930176, Classifier Loss 0.36439600586891174, Total Loss 178.35240173339844\n",
      "26: Encoding Loss 16.246681213378906, Transition Loss 2.590949058532715, Classifier Loss 0.3572889268398285, Total Loss 166.2205352783203\n",
      "26: Encoding Loss 16.8684024810791, Transition Loss 1.817915916442871, Classifier Loss 0.34174275398254395, Total Loss 169.48507690429688\n",
      "26: Encoding Loss 18.458833694458008, Transition Loss 3.0255661010742188, Classifier Loss 0.3802641034126282, Total Loss 186.3022003173828\n",
      "26: Encoding Loss 17.728059768676758, Transition Loss 2.0662553310394287, Classifier Loss 0.352824866771698, Total Loss 177.5202178955078\n",
      "26: Encoding Loss 19.72740364074707, Transition Loss 2.312485933303833, Classifier Loss 0.36550357937812805, Total Loss 194.8320770263672\n",
      "26: Encoding Loss 19.242206573486328, Transition Loss 2.404873847961426, Classifier Loss 0.3333245813846588, Total Loss 187.75108337402344\n",
      "26: Encoding Loss 18.319475173950195, Transition Loss 3.1276094913482666, Classifier Loss 0.35448694229125977, Total Loss 182.6300048828125\n",
      "26: Encoding Loss 16.079402923583984, Transition Loss 3.728205919265747, Classifier Loss 0.3657456338405609, Total Loss 165.9554443359375\n",
      "27: Encoding Loss 18.54388999938965, Transition Loss 2.230619430541992, Classifier Loss 0.33698123693466187, Total Loss 182.495361328125\n",
      "27: Encoding Loss 18.452367782592773, Transition Loss 2.0378835201263428, Classifier Loss 0.3439594805240631, Total Loss 182.42247009277344\n",
      "27: Encoding Loss 19.161922454833984, Transition Loss 2.28102707862854, Classifier Loss 0.3587733209133148, Total Loss 189.62893676757812\n",
      "27: Encoding Loss 17.909236907958984, Transition Loss 2.183016538619995, Classifier Loss 0.34966179728507996, Total Loss 178.6766815185547\n",
      "27: Encoding Loss 19.380069732666016, Transition Loss 2.0140695571899414, Classifier Loss 0.37633904814720154, Total Loss 193.07728576660156\n",
      "27: Encoding Loss 16.835744857788086, Transition Loss 2.7950849533081055, Classifier Loss 0.39850732684135437, Total Loss 175.095703125\n",
      "27: Encoding Loss 18.58578872680664, Transition Loss 1.9559160470962524, Classifier Loss 0.35567712783813477, Total Loss 184.6452178955078\n",
      "27: Encoding Loss 19.9655818939209, Transition Loss 1.906798243522644, Classifier Loss 0.39128372073173523, Total Loss 199.23439025878906\n",
      "27: Encoding Loss 17.901203155517578, Transition Loss 2.3030104637145996, Classifier Loss 0.42812204360961914, Total Loss 186.482421875\n",
      "27: Encoding Loss 19.838253021240234, Transition Loss 2.062713384628296, Classifier Loss 0.3813419044017792, Total Loss 197.2527618408203\n",
      "27: Encoding Loss 18.6400089263916, Transition Loss 2.0520331859588623, Classifier Loss 0.3455851078033447, Total Loss 184.0889892578125\n",
      "27: Encoding Loss 18.262725830078125, Transition Loss 2.1638689041137695, Classifier Loss 0.3438462018966675, Total Loss 180.919189453125\n",
      "27: Encoding Loss 17.446508407592773, Transition Loss 2.193291187286377, Classifier Loss 0.36663779616355896, Total Loss 176.67449951171875\n",
      "27: Encoding Loss 17.760427474975586, Transition Loss 2.337369918823242, Classifier Loss 0.35568687319755554, Total Loss 178.11956787109375\n",
      "27: Encoding Loss 17.15936851501465, Transition Loss 2.4350783824920654, Classifier Loss 0.37415850162506104, Total Loss 175.1778106689453\n",
      "27: Encoding Loss 18.05047607421875, Transition Loss 2.523833990097046, Classifier Loss 0.3499521315097809, Total Loss 179.90377807617188\n",
      "27: Encoding Loss 17.847232818603516, Transition Loss 2.3801941871643066, Classifier Loss 0.3613150417804718, Total Loss 179.38540649414062\n",
      "27: Encoding Loss 17.630308151245117, Transition Loss 2.3145558834075928, Classifier Loss 0.32974642515182495, Total Loss 174.48001098632812\n",
      "27: Encoding Loss 18.086870193481445, Transition Loss 2.2430858612060547, Classifier Loss 0.3126084804534912, Total Loss 176.40443420410156\n",
      "27: Encoding Loss 19.06209373474121, Transition Loss 2.255838632583618, Classifier Loss 0.3617936670780182, Total Loss 189.12728881835938\n",
      "27: Encoding Loss 17.965129852294922, Transition Loss 2.6192848682403564, Classifier Loss 0.36517468094825745, Total Loss 180.7623748779297\n",
      "27: Encoding Loss 17.55553436279297, Transition Loss 2.1214656829833984, Classifier Loss 0.3163386285305023, Total Loss 172.50242614746094\n",
      "27: Encoding Loss 17.09457778930664, Transition Loss 3.291233777999878, Classifier Loss 0.34540653228759766, Total Loss 171.9555206298828\n",
      "27: Encoding Loss 16.87120819091797, Transition Loss 2.1359643936157227, Classifier Loss 0.3508593440055847, Total Loss 170.48280334472656\n",
      "27: Encoding Loss 17.84856414794922, Transition Loss 1.9663418531417847, Classifier Loss 0.3448426127433777, Total Loss 177.66603088378906\n",
      "27: Encoding Loss 18.598609924316406, Transition Loss 2.381702184677124, Classifier Loss 0.3440096378326416, Total Loss 183.6661834716797\n",
      "27: Encoding Loss 18.81355857849121, Transition Loss 2.451707363128662, Classifier Loss 0.34822842478752136, Total Loss 185.8216552734375\n",
      "27: Encoding Loss 18.010107040405273, Transition Loss 2.5665781497955322, Classifier Loss 0.3249613046646118, Total Loss 177.09031677246094\n",
      "27: Encoding Loss 17.605119705200195, Transition Loss 2.52484130859375, Classifier Loss 0.3533504605293274, Total Loss 176.68096923828125\n",
      "27: Encoding Loss 17.327247619628906, Transition Loss 2.9803173542022705, Classifier Loss 0.3650273382663727, Total Loss 175.71678161621094\n",
      "27: Encoding Loss 17.91701316833496, Transition Loss 2.505500078201294, Classifier Loss 0.337395042181015, Total Loss 177.5767059326172\n",
      "27: Encoding Loss 16.675132751464844, Transition Loss 2.9641122817993164, Classifier Loss 0.34839680790901184, Total Loss 168.83355712890625\n",
      "27: Encoding Loss 18.628381729125977, Transition Loss 2.631971836090088, Classifier Loss 0.3698157072067261, Total Loss 186.53501892089844\n",
      "27: Encoding Loss 16.78325653076172, Transition Loss 1.957047939300537, Classifier Loss 0.3657580018043518, Total Loss 171.23326110839844\n",
      "27: Encoding Loss 17.215171813964844, Transition Loss 2.557018280029297, Classifier Loss 0.3506314754486084, Total Loss 173.29591369628906\n",
      "27: Encoding Loss 16.393238067626953, Transition Loss 2.122774362564087, Classifier Loss 0.3399394750595093, Total Loss 165.5644073486328\n",
      "27: Encoding Loss 18.14999008178711, Transition Loss 2.434537649154663, Classifier Loss 0.33834898471832275, Total Loss 179.52174377441406\n",
      "27: Encoding Loss 18.332578659057617, Transition Loss 3.2669007778167725, Classifier Loss 0.3733920454978943, Total Loss 184.65321350097656\n",
      "27: Encoding Loss 18.346799850463867, Transition Loss 2.1247591972351074, Classifier Loss 0.3441396653652191, Total Loss 181.6133270263672\n",
      "27: Encoding Loss 17.63888168334961, Transition Loss 2.785926580429077, Classifier Loss 0.3415190875530243, Total Loss 175.82015991210938\n",
      "27: Encoding Loss 17.403181076049805, Transition Loss 2.848480224609375, Classifier Loss 0.41531628370285034, Total Loss 181.3267822265625\n",
      "27: Encoding Loss 17.168701171875, Transition Loss 2.1746938228607178, Classifier Loss 0.36727282404899597, Total Loss 174.51182556152344\n",
      "27: Encoding Loss 17.158855438232422, Transition Loss 1.9982399940490723, Classifier Loss 0.31883347034454346, Total Loss 169.55384826660156\n",
      "27: Encoding Loss 16.80258560180664, Transition Loss 1.8390121459960938, Classifier Loss 0.3614789843559265, Total Loss 170.93638610839844\n",
      "27: Encoding Loss 19.024240493774414, Transition Loss 1.9750949144363403, Classifier Loss 0.32771801948547363, Total Loss 185.36074829101562\n",
      "27: Encoding Loss 18.717918395996094, Transition Loss 2.5563669204711914, Classifier Loss 0.38657692074775696, Total Loss 188.9123077392578\n",
      "27: Encoding Loss 17.992029190063477, Transition Loss 2.836272954940796, Classifier Loss 0.3887912929058075, Total Loss 183.38262939453125\n",
      "27: Encoding Loss 18.882505416870117, Transition Loss 2.234783411026001, Classifier Loss 0.36829614639282227, Total Loss 188.33660888671875\n",
      "27: Encoding Loss 18.393001556396484, Transition Loss 2.340360641479492, Classifier Loss 0.3475887179374695, Total Loss 182.3709716796875\n",
      "27: Encoding Loss 18.33712387084961, Transition Loss 2.5455729961395264, Classifier Loss 0.29717767238616943, Total Loss 176.92388916015625\n",
      "27: Encoding Loss 17.283544540405273, Transition Loss 1.9480146169662476, Classifier Loss 0.3295634090900421, Total Loss 171.6143035888672\n",
      "27: Encoding Loss 17.380901336669922, Transition Loss 2.1392574310302734, Classifier Loss 0.3407078683376312, Total Loss 173.54586791992188\n",
      "27: Encoding Loss 16.78392219543457, Transition Loss 2.0139951705932617, Classifier Loss 0.30806413292884827, Total Loss 165.4805908203125\n",
      "27: Encoding Loss 17.757755279541016, Transition Loss 2.6519861221313477, Classifier Loss 0.3457674980163574, Total Loss 177.169189453125\n",
      "27: Encoding Loss 17.20356559753418, Transition Loss 2.828080177307129, Classifier Loss 0.33034414052963257, Total Loss 171.22854614257812\n",
      "27: Encoding Loss 16.573366165161133, Transition Loss 2.7022624015808105, Classifier Loss 0.36440786719322205, Total Loss 169.5681610107422\n",
      "27: Encoding Loss 17.545753479003906, Transition Loss 2.322661876678467, Classifier Loss 0.3744049072265625, Total Loss 178.27105712890625\n",
      "27: Encoding Loss 17.11368751525879, Transition Loss 2.5097601413726807, Classifier Loss 0.3707091808319092, Total Loss 174.4823760986328\n",
      "27: Encoding Loss 17.162464141845703, Transition Loss 2.3012282848358154, Classifier Loss 0.3500085771083832, Total Loss 172.7608184814453\n",
      "27: Encoding Loss 17.064687728881836, Transition Loss 2.3110342025756836, Classifier Loss 0.33278948068618774, Total Loss 170.25865173339844\n",
      "27: Encoding Loss 18.70623016357422, Transition Loss 2.547271728515625, Classifier Loss 0.34166663885116577, Total Loss 184.32595825195312\n",
      "27: Encoding Loss 19.44021987915039, Transition Loss 2.19270658493042, Classifier Loss 0.3303844928741455, Total Loss 188.99874877929688\n",
      "27: Encoding Loss 17.95454216003418, Transition Loss 2.282548427581787, Classifier Loss 0.3491474688053131, Total Loss 179.00759887695312\n",
      "27: Encoding Loss 17.648252487182617, Transition Loss 2.278053045272827, Classifier Loss 0.3525433838367462, Total Loss 176.89596557617188\n",
      "27: Encoding Loss 16.903364181518555, Transition Loss 2.5026187896728516, Classifier Loss 0.3196018934249878, Total Loss 167.6876220703125\n",
      "27: Encoding Loss 16.584278106689453, Transition Loss 2.5043346881866455, Classifier Loss 0.32940512895584106, Total Loss 166.11561584472656\n",
      "27: Encoding Loss 17.809770584106445, Transition Loss 2.32820463180542, Classifier Loss 0.33490195870399475, Total Loss 176.43399047851562\n",
      "27: Encoding Loss 19.03141212463379, Transition Loss 2.1976726055145264, Classifier Loss 0.344312846660614, Total Loss 187.1221160888672\n",
      "27: Encoding Loss 18.865407943725586, Transition Loss 2.670997381210327, Classifier Loss 0.37973183393478394, Total Loss 189.43064880371094\n",
      "27: Encoding Loss 17.5308895111084, Transition Loss 2.525031566619873, Classifier Loss 0.3874090909957886, Total Loss 179.49302673339844\n",
      "27: Encoding Loss 18.53380012512207, Transition Loss 2.11116623878479, Classifier Loss 0.38545453548431396, Total Loss 187.2380828857422\n",
      "27: Encoding Loss 16.895009994506836, Transition Loss 2.672267436981201, Classifier Loss 0.31673339009284973, Total Loss 167.3678741455078\n",
      "27: Encoding Loss 17.201082229614258, Transition Loss 2.6447370052337646, Classifier Loss 0.3175433874130249, Total Loss 169.89193725585938\n",
      "27: Encoding Loss 18.42494773864746, Transition Loss 2.560333490371704, Classifier Loss 0.3349432945251465, Total Loss 181.40599060058594\n",
      "27: Encoding Loss 17.14842414855957, Transition Loss 3.4079108238220215, Classifier Loss 0.39100492000579834, Total Loss 176.96946716308594\n",
      "27: Encoding Loss 18.318355560302734, Transition Loss 2.4877326488494873, Classifier Loss 0.3242529034614563, Total Loss 179.46969604492188\n",
      "27: Encoding Loss 17.937597274780273, Transition Loss 2.909785747528076, Classifier Loss 0.33664053678512573, Total Loss 177.7467803955078\n",
      "27: Encoding Loss 17.941497802734375, Transition Loss 2.970736503601074, Classifier Loss 0.355634868144989, Total Loss 179.6896209716797\n",
      "27: Encoding Loss 17.25484275817871, Transition Loss 2.610976219177246, Classifier Loss 0.33320167660713196, Total Loss 171.88111877441406\n",
      "27: Encoding Loss 18.87413787841797, Transition Loss 2.2474894523620605, Classifier Loss 0.328378289937973, Total Loss 184.28042602539062\n",
      "27: Encoding Loss 17.253440856933594, Transition Loss 2.4259252548217773, Classifier Loss 0.3190092146396637, Total Loss 170.41363525390625\n",
      "27: Encoding Loss 17.988372802734375, Transition Loss 2.673624038696289, Classifier Loss 0.3587621748447418, Total Loss 180.31793212890625\n",
      "27: Encoding Loss 17.723949432373047, Transition Loss 2.471970558166504, Classifier Loss 0.3248485326766968, Total Loss 174.77085876464844\n",
      "27: Encoding Loss 18.375112533569336, Transition Loss 2.9760379791259766, Classifier Loss 0.35633984208106995, Total Loss 183.2301025390625\n",
      "27: Encoding Loss 18.676647186279297, Transition Loss 2.045053243637085, Classifier Loss 0.31910794973373413, Total Loss 181.73300170898438\n",
      "27: Encoding Loss 16.7387752532959, Transition Loss 2.6431360244750977, Classifier Loss 0.3335154056549072, Total Loss 167.79037475585938\n",
      "27: Encoding Loss 17.725177764892578, Transition Loss 2.3013463020324707, Classifier Loss 0.3765470087528229, Total Loss 179.9163818359375\n",
      "27: Encoding Loss 18.074365615844727, Transition Loss 3.1342597007751465, Classifier Loss 0.3693675398826599, Total Loss 182.1585235595703\n",
      "27: Encoding Loss 17.592464447021484, Transition Loss 2.4160830974578857, Classifier Loss 0.34237363934516907, Total Loss 175.46031188964844\n",
      "27: Encoding Loss 17.95492935180664, Transition Loss 2.2794790267944336, Classifier Loss 0.35826072096824646, Total Loss 179.92141723632812\n",
      "27: Encoding Loss 18.086519241333008, Transition Loss 2.222140312194824, Classifier Loss 0.33877378702163696, Total Loss 179.0139617919922\n",
      "27: Encoding Loss 18.485416412353516, Transition Loss 2.8389203548431396, Classifier Loss 0.3919079899787903, Total Loss 187.64190673828125\n",
      "27: Encoding Loss 17.804407119750977, Transition Loss 2.542595386505127, Classifier Loss 0.3211759328842163, Total Loss 175.06137084960938\n",
      "27: Encoding Loss 18.968530654907227, Transition Loss 2.1975722312927246, Classifier Loss 0.34564822912216187, Total Loss 186.75257873535156\n",
      "27: Encoding Loss 18.22600746154785, Transition Loss 2.426887273788452, Classifier Loss 0.3574334383010864, Total Loss 182.0367889404297\n",
      "27: Encoding Loss 16.725133895874023, Transition Loss 2.1678593158721924, Classifier Loss 0.3376515507698059, Total Loss 167.9998016357422\n",
      "27: Encoding Loss 18.181903839111328, Transition Loss 2.5327937602996826, Classifier Loss 0.3528148829936981, Total Loss 181.2432861328125\n",
      "27: Encoding Loss 17.641756057739258, Transition Loss 2.2551331520080566, Classifier Loss 0.38733020424842834, Total Loss 180.31808471679688\n",
      "27: Encoding Loss 16.859031677246094, Transition Loss 3.068021059036255, Classifier Loss 0.36886173486709595, Total Loss 172.3720245361328\n",
      "27: Encoding Loss 18.969816207885742, Transition Loss 2.0223653316497803, Classifier Loss 0.3166496753692627, Total Loss 183.82797241210938\n",
      "27: Encoding Loss 18.14192771911621, Transition Loss 2.328827381134033, Classifier Loss 0.34636956453323364, Total Loss 180.23812866210938\n",
      "27: Encoding Loss 18.144025802612305, Transition Loss 2.170032262802124, Classifier Loss 0.3740271031856537, Total Loss 182.98892211914062\n",
      "27: Encoding Loss 17.096925735473633, Transition Loss 3.0761170387268066, Classifier Loss 0.3326951265335083, Total Loss 170.66014099121094\n",
      "27: Encoding Loss 17.905961990356445, Transition Loss 2.969303846359253, Classifier Loss 0.33883121609687805, Total Loss 177.72467041015625\n",
      "27: Encoding Loss 17.24629783630371, Transition Loss 2.7117762565612793, Classifier Loss 0.3197849988937378, Total Loss 170.49124145507812\n",
      "27: Encoding Loss 16.125356674194336, Transition Loss 3.4171056747436523, Classifier Loss 0.3151285648345947, Total Loss 161.1991424560547\n",
      "27: Encoding Loss 17.599191665649414, Transition Loss 2.6062164306640625, Classifier Loss 0.32082560658454895, Total Loss 173.3973388671875\n",
      "27: Encoding Loss 17.81332778930664, Transition Loss 2.769353151321411, Classifier Loss 0.34647905826568604, Total Loss 177.70838928222656\n",
      "27: Encoding Loss 15.779354095458984, Transition Loss 2.786046266555786, Classifier Loss 0.334286093711853, Total Loss 160.2206573486328\n",
      "27: Encoding Loss 17.8472900390625, Transition Loss 2.9121766090393066, Classifier Loss 0.35513490438461304, Total Loss 178.87423706054688\n",
      "27: Encoding Loss 17.639060974121094, Transition Loss 2.7302470207214355, Classifier Loss 0.33796676993370056, Total Loss 175.45521545410156\n",
      "27: Encoding Loss 18.668209075927734, Transition Loss 2.6754860877990723, Classifier Loss 0.34608346223831177, Total Loss 184.4891357421875\n",
      "27: Encoding Loss 17.50918960571289, Transition Loss 2.6676549911499023, Classifier Loss 0.3628089129924774, Total Loss 176.88792419433594\n",
      "27: Encoding Loss 18.118181228637695, Transition Loss 2.2231502532958984, Classifier Loss 0.3218761682510376, Total Loss 177.57769775390625\n",
      "27: Encoding Loss 17.089717864990234, Transition Loss 2.266364336013794, Classifier Loss 0.34792017936706543, Total Loss 171.9630584716797\n",
      "27: Encoding Loss 19.519351959228516, Transition Loss 2.8690133094787598, Classifier Loss 0.3633106052875519, Total Loss 193.05967712402344\n",
      "27: Encoding Loss 18.019153594970703, Transition Loss 2.387718677520752, Classifier Loss 0.3437633812427521, Total Loss 179.00711059570312\n",
      "27: Encoding Loss 16.557483673095703, Transition Loss 2.4397571086883545, Classifier Loss 0.3306353688240051, Total Loss 166.0113525390625\n",
      "27: Encoding Loss 17.622831344604492, Transition Loss 2.6280078887939453, Classifier Loss 0.35404902696609497, Total Loss 176.9131622314453\n",
      "27: Encoding Loss 17.866466522216797, Transition Loss 2.381669282913208, Classifier Loss 0.344439834356308, Total Loss 177.85206604003906\n",
      "27: Encoding Loss 17.02817726135254, Transition Loss 1.8783090114593506, Classifier Loss 0.3355730473995209, Total Loss 170.1583709716797\n",
      "27: Encoding Loss 16.94515609741211, Transition Loss 2.223829507827759, Classifier Loss 0.3327710032463074, Total Loss 169.2831268310547\n",
      "27: Encoding Loss 17.96453857421875, Transition Loss 2.8851890563964844, Classifier Loss 0.3465985953807831, Total Loss 178.9532012939453\n",
      "27: Encoding Loss 17.81529426574707, Transition Loss 1.9963469505310059, Classifier Loss 0.3303850293159485, Total Loss 175.9601287841797\n",
      "27: Encoding Loss 18.32516098022461, Transition Loss 1.9745547771453857, Classifier Loss 0.3138619661331177, Total Loss 178.38241577148438\n",
      "27: Encoding Loss 16.77483558654785, Transition Loss 2.864724636077881, Classifier Loss 0.3411749303340912, Total Loss 168.88912963867188\n",
      "27: Encoding Loss 17.174755096435547, Transition Loss 3.1346700191497803, Classifier Loss 0.32881641387939453, Total Loss 170.90663146972656\n",
      "27: Encoding Loss 17.04563331604004, Transition Loss 2.727363109588623, Classifier Loss 0.3502102792263031, Total Loss 171.9315643310547\n",
      "27: Encoding Loss 17.598308563232422, Transition Loss 2.9137027263641357, Classifier Loss 0.36302459239959717, Total Loss 177.6716766357422\n",
      "27: Encoding Loss 17.171361923217773, Transition Loss 3.616903781890869, Classifier Loss 0.37101688981056213, Total Loss 175.19595336914062\n",
      "27: Encoding Loss 18.309240341186523, Transition Loss 2.4495937824249268, Classifier Loss 0.3533059358596802, Total Loss 182.29443359375\n",
      "27: Encoding Loss 18.124603271484375, Transition Loss 2.3857383728027344, Classifier Loss 0.30652469396591187, Total Loss 176.12643432617188\n",
      "27: Encoding Loss 16.846845626831055, Transition Loss 2.6291160583496094, Classifier Loss 0.33944249153137207, Total Loss 169.2448272705078\n",
      "27: Encoding Loss 17.221187591552734, Transition Loss 2.318687915802002, Classifier Loss 0.3482015132904053, Total Loss 173.05340576171875\n",
      "27: Encoding Loss 17.149770736694336, Transition Loss 2.6712706089019775, Classifier Loss 0.3636758327484131, Total Loss 174.10000610351562\n",
      "27: Encoding Loss 17.311199188232422, Transition Loss 2.497725486755371, Classifier Loss 0.3073292076587677, Total Loss 169.72207641601562\n",
      "27: Encoding Loss 17.401161193847656, Transition Loss 2.2543678283691406, Classifier Loss 0.35177814960479736, Total Loss 174.8379669189453\n",
      "27: Encoding Loss 17.40379524230957, Transition Loss 2.2932565212249756, Classifier Loss 0.3384358286857605, Total Loss 173.5325927734375\n",
      "27: Encoding Loss 15.972875595092773, Transition Loss 2.471278429031372, Classifier Loss 0.3137567639350891, Total Loss 159.6529541015625\n",
      "27: Encoding Loss 16.50040054321289, Transition Loss 2.5967352390289307, Classifier Loss 0.28427329659461975, Total Loss 160.94989013671875\n",
      "27: Encoding Loss 18.93662452697754, Transition Loss 2.2349047660827637, Classifier Loss 0.37792330980300903, Total Loss 189.7322998046875\n",
      "27: Encoding Loss 17.188276290893555, Transition Loss 3.0179407596588135, Classifier Loss 0.3740847110748291, Total Loss 175.5182647705078\n",
      "27: Encoding Loss 17.99555206298828, Transition Loss 2.470552444458008, Classifier Loss 0.34832578897476196, Total Loss 179.29110717773438\n",
      "27: Encoding Loss 18.0758056640625, Transition Loss 2.9042084217071533, Classifier Loss 0.35315069556236267, Total Loss 180.50234985351562\n",
      "27: Encoding Loss 17.716337203979492, Transition Loss 2.4885497093200684, Classifier Loss 0.3344239592552185, Total Loss 175.67080688476562\n",
      "27: Encoding Loss 16.758792877197266, Transition Loss 2.7819666862487793, Classifier Loss 0.3344387114048004, Total Loss 168.07061767578125\n",
      "27: Encoding Loss 18.777605056762695, Transition Loss 1.8129873275756836, Classifier Loss 0.34318265318870544, Total Loss 184.90170288085938\n",
      "27: Encoding Loss 17.01448631286621, Transition Loss 1.930666446685791, Classifier Loss 0.34481367468833923, Total Loss 170.9833984375\n",
      "27: Encoding Loss 17.29689598083496, Transition Loss 2.359133005142212, Classifier Loss 0.31351590156555176, Total Loss 170.19859313964844\n",
      "27: Encoding Loss 16.159191131591797, Transition Loss 2.6903209686279297, Classifier Loss 0.32101818919181824, Total Loss 161.91343688964844\n",
      "27: Encoding Loss 16.444366455078125, Transition Loss 1.8952269554138184, Classifier Loss 0.35607609152793884, Total Loss 167.5415802001953\n",
      "27: Encoding Loss 18.15262794494629, Transition Loss 3.1352009773254395, Classifier Loss 0.3199712634086609, Total Loss 177.84519958496094\n",
      "27: Encoding Loss 17.693735122680664, Transition Loss 2.1897459030151367, Classifier Loss 0.33862000703811646, Total Loss 175.84982299804688\n",
      "27: Encoding Loss 19.325822830200195, Transition Loss 2.5543787479400635, Classifier Loss 0.3484439551830292, Total Loss 189.96185302734375\n",
      "27: Encoding Loss 18.89568328857422, Transition Loss 2.598288059234619, Classifier Loss 0.35611140727996826, Total Loss 187.2962646484375\n",
      "27: Encoding Loss 17.913026809692383, Transition Loss 3.361125946044922, Classifier Loss 0.34477168321609497, Total Loss 178.45361328125\n",
      "27: Encoding Loss 16.06460189819336, Transition Loss 3.7134032249450684, Classifier Loss 0.41409340500831604, Total Loss 170.66885375976562\n",
      "28: Encoding Loss 18.18690299987793, Transition Loss 2.371127128601074, Classifier Loss 0.3414725959300995, Total Loss 180.11671447753906\n",
      "28: Encoding Loss 18.242341995239258, Transition Loss 2.143239974975586, Classifier Loss 0.3653746545314789, Total Loss 182.90484619140625\n",
      "28: Encoding Loss 18.9108829498291, Transition Loss 2.4029104709625244, Classifier Loss 0.3691917061805725, Total Loss 188.6868133544922\n",
      "28: Encoding Loss 17.632709503173828, Transition Loss 2.3087000846862793, Classifier Loss 0.3496341109275818, Total Loss 176.48683166503906\n",
      "28: Encoding Loss 19.079265594482422, Transition Loss 2.222797393798828, Classifier Loss 0.38910770416259766, Total Loss 191.98947143554688\n",
      "28: Encoding Loss 16.50041389465332, Transition Loss 2.8467752933502197, Classifier Loss 0.3807738423347473, Total Loss 170.65003967285156\n",
      "28: Encoding Loss 18.279619216918945, Transition Loss 2.165588617324829, Classifier Loss 0.3432053029537201, Total Loss 180.9906005859375\n",
      "28: Encoding Loss 19.74563980102539, Transition Loss 2.1488664150238037, Classifier Loss 0.3728521168231964, Total Loss 195.68011474609375\n",
      "28: Encoding Loss 17.56443214416504, Transition Loss 2.4561374187469482, Classifier Loss 0.38458824157714844, Total Loss 179.4654998779297\n",
      "28: Encoding Loss 19.47677993774414, Transition Loss 2.2705280780792236, Classifier Loss 0.3548869788646698, Total Loss 191.7570343017578\n",
      "28: Encoding Loss 18.328311920166016, Transition Loss 2.2284648418426514, Classifier Loss 0.3429925739765167, Total Loss 181.37144470214844\n",
      "28: Encoding Loss 17.74788475036621, Transition Loss 2.31955623626709, Classifier Loss 0.35718947649002075, Total Loss 178.1659393310547\n",
      "28: Encoding Loss 17.180484771728516, Transition Loss 2.3390069007873535, Classifier Loss 0.33118078112602234, Total Loss 171.02975463867188\n",
      "28: Encoding Loss 17.545772552490234, Transition Loss 2.4756696224212646, Classifier Loss 0.3560260534286499, Total Loss 176.46392822265625\n",
      "28: Encoding Loss 17.070396423339844, Transition Loss 2.5634891986846924, Classifier Loss 0.3783884048461914, Total Loss 174.91470336914062\n",
      "28: Encoding Loss 17.77228546142578, Transition Loss 2.7285726070404053, Classifier Loss 0.32138967514038086, Total Loss 174.86297607421875\n",
      "28: Encoding Loss 17.626502990722656, Transition Loss 2.600311040878296, Classifier Loss 0.3739684224128723, Total Loss 178.92893981933594\n",
      "28: Encoding Loss 17.527894973754883, Transition Loss 2.5020194053649902, Classifier Loss 0.3248644173145294, Total Loss 173.20999145507812\n",
      "28: Encoding Loss 17.850879669189453, Transition Loss 2.5227315425872803, Classifier Loss 0.31059038639068604, Total Loss 174.37062072753906\n",
      "28: Encoding Loss 18.634340286254883, Transition Loss 2.535336494445801, Classifier Loss 0.32172733545303345, Total Loss 181.7545166015625\n",
      "28: Encoding Loss 17.5390682220459, Transition Loss 2.9033970832824707, Classifier Loss 0.3460584878921509, Total Loss 175.4990692138672\n",
      "28: Encoding Loss 17.081249237060547, Transition Loss 2.300272226333618, Classifier Loss 0.3239935040473938, Total Loss 169.50941467285156\n",
      "28: Encoding Loss 16.66105079650879, Transition Loss 3.434295177459717, Classifier Loss 0.32888609170913696, Total Loss 166.86387634277344\n",
      "28: Encoding Loss 16.511507034301758, Transition Loss 2.289182186126709, Classifier Loss 0.34818246960639954, Total Loss 167.36814880371094\n",
      "28: Encoding Loss 17.376325607299805, Transition Loss 2.0733375549316406, Classifier Loss 0.30770444869995117, Total Loss 170.1957244873047\n",
      "28: Encoding Loss 18.16347312927246, Transition Loss 2.4791598320007324, Classifier Loss 0.35510140657424927, Total Loss 181.3137664794922\n",
      "28: Encoding Loss 18.44559669494629, Transition Loss 2.6069021224975586, Classifier Loss 0.33195680379867554, Total Loss 181.28182983398438\n",
      "28: Encoding Loss 17.52176284790039, Transition Loss 2.8024349212646484, Classifier Loss 0.3306618332862854, Total Loss 173.80078125\n",
      "28: Encoding Loss 17.18358612060547, Transition Loss 2.7160191535949707, Classifier Loss 0.3463315963745117, Total Loss 172.64505004882812\n",
      "28: Encoding Loss 17.021223068237305, Transition Loss 3.0982158184051514, Classifier Loss 0.3896957337856293, Total Loss 175.75900268554688\n",
      "28: Encoding Loss 17.477619171142578, Transition Loss 2.692750930786133, Classifier Loss 0.32291746139526367, Total Loss 172.6512451171875\n",
      "28: Encoding Loss 16.259361267089844, Transition Loss 3.1209959983825684, Classifier Loss 0.34386706352233887, Total Loss 165.08580017089844\n",
      "28: Encoding Loss 18.24616050720215, Transition Loss 2.8162612915039062, Classifier Loss 0.3487527668476105, Total Loss 181.40780639648438\n",
      "28: Encoding Loss 16.380762100219727, Transition Loss 2.0525553226470947, Classifier Loss 0.3361586630344391, Total Loss 165.0724639892578\n",
      "28: Encoding Loss 16.920217514038086, Transition Loss 2.6703054904937744, Classifier Loss 0.3609563410282135, Total Loss 171.99142456054688\n",
      "28: Encoding Loss 16.143985748291016, Transition Loss 2.224492311477661, Classifier Loss 0.34788069128990173, Total Loss 164.38485717773438\n",
      "28: Encoding Loss 17.725561141967773, Transition Loss 2.63896107673645, Classifier Loss 0.32894688844680786, Total Loss 175.22695922851562\n",
      "28: Encoding Loss 17.686115264892578, Transition Loss 3.5146830081939697, Classifier Loss 0.35206079483032227, Total Loss 177.39794921875\n",
      "28: Encoding Loss 17.993669509887695, Transition Loss 2.329105854034424, Classifier Loss 0.3042184114456177, Total Loss 174.83702087402344\n",
      "28: Encoding Loss 17.168455123901367, Transition Loss 2.935044050216675, Classifier Loss 0.354422390460968, Total Loss 173.37689208984375\n",
      "28: Encoding Loss 17.07218360900879, Transition Loss 3.002000570297241, Classifier Loss 0.37240347266197205, Total Loss 174.418212890625\n",
      "28: Encoding Loss 16.73931121826172, Transition Loss 2.2732391357421875, Classifier Loss 0.33803099393844604, Total Loss 168.1722412109375\n",
      "28: Encoding Loss 16.904502868652344, Transition Loss 2.1243369579315186, Classifier Loss 0.33438870310783386, Total Loss 169.09976196289062\n",
      "28: Encoding Loss 16.508522033691406, Transition Loss 1.9209702014923096, Classifier Loss 0.3353565037250519, Total Loss 165.98802185058594\n",
      "28: Encoding Loss 18.634143829345703, Transition Loss 2.1591100692749023, Classifier Loss 0.3416445553302765, Total Loss 183.66943359375\n",
      "28: Encoding Loss 18.433496475219727, Transition Loss 2.7144951820373535, Classifier Loss 0.37979891896247864, Total Loss 185.99075317382812\n",
      "28: Encoding Loss 17.57261848449707, Transition Loss 3.0075767040252686, Classifier Loss 0.34757834672927856, Total Loss 175.94029235839844\n",
      "28: Encoding Loss 18.472780227661133, Transition Loss 2.4995219707489014, Classifier Loss 0.3461543321609497, Total Loss 182.8975830078125\n",
      "28: Encoding Loss 18.149120330810547, Transition Loss 2.594698905944824, Classifier Loss 0.34878575801849365, Total Loss 180.59048461914062\n",
      "28: Encoding Loss 17.853008270263672, Transition Loss 2.8277926445007324, Classifier Loss 0.33366328477859497, Total Loss 176.75596618652344\n",
      "28: Encoding Loss 17.088045120239258, Transition Loss 2.1192588806152344, Classifier Loss 0.31145724654197693, Total Loss 168.27394104003906\n",
      "28: Encoding Loss 17.200908660888672, Transition Loss 2.2898294925689697, Classifier Loss 0.31125786900520325, Total Loss 169.1910400390625\n",
      "28: Encoding Loss 16.637178421020508, Transition Loss 2.1046500205993652, Classifier Loss 0.314969539642334, Total Loss 165.0153045654297\n",
      "28: Encoding Loss 17.474647521972656, Transition Loss 2.8565919399261475, Classifier Loss 0.3637888729572296, Total Loss 176.7473907470703\n",
      "28: Encoding Loss 16.77781867980957, Transition Loss 2.9258034229278564, Classifier Loss 0.3585997223854065, Total Loss 170.6676788330078\n",
      "28: Encoding Loss 16.114906311035156, Transition Loss 2.775113582611084, Classifier Loss 0.3290204405784607, Total Loss 162.37631225585938\n",
      "28: Encoding Loss 17.159912109375, Transition Loss 2.38181471824646, Classifier Loss 0.3497253656387329, Total Loss 172.7281951904297\n",
      "28: Encoding Loss 16.961801528930664, Transition Loss 2.6296873092651367, Classifier Loss 0.3444785475730896, Total Loss 170.668212890625\n",
      "28: Encoding Loss 16.92388153076172, Transition Loss 2.351720094680786, Classifier Loss 0.3248969316482544, Total Loss 168.35107421875\n",
      "28: Encoding Loss 16.706798553466797, Transition Loss 2.3963565826416016, Classifier Loss 0.33020997047424316, Total Loss 167.15467834472656\n",
      "28: Encoding Loss 18.314369201660156, Transition Loss 2.644714832305908, Classifier Loss 0.3847758173942566, Total Loss 185.521484375\n",
      "28: Encoding Loss 19.118892669677734, Transition Loss 2.4204788208007812, Classifier Loss 0.3217836916446686, Total Loss 185.61361694335938\n",
      "28: Encoding Loss 17.56584930419922, Transition Loss 2.361011505126953, Classifier Loss 0.3341435492038727, Total Loss 174.41334533691406\n",
      "28: Encoding Loss 17.342876434326172, Transition Loss 2.3431684970855713, Classifier Loss 0.3641849458217621, Total Loss 175.63015747070312\n",
      "28: Encoding Loss 16.61861801147461, Transition Loss 2.5741333961486816, Classifier Loss 0.33364230394363403, Total Loss 166.82801818847656\n",
      "28: Encoding Loss 16.22283363342285, Transition Loss 2.5340161323547363, Classifier Loss 0.3337869942188263, Total Loss 163.66818237304688\n",
      "28: Encoding Loss 17.384016036987305, Transition Loss 2.397737741470337, Classifier Loss 0.3052158057689667, Total Loss 170.07325744628906\n",
      "28: Encoding Loss 18.67328643798828, Transition Loss 2.3551979064941406, Classifier Loss 0.29767483472824097, Total Loss 179.62481689453125\n",
      "28: Encoding Loss 18.469200134277344, Transition Loss 2.8145835399627686, Classifier Loss 0.35788780450820923, Total Loss 184.1053009033203\n",
      "28: Encoding Loss 17.366796493530273, Transition Loss 2.6844382286071777, Classifier Loss 0.3771655559539795, Total Loss 177.18780517578125\n",
      "28: Encoding Loss 18.16055679321289, Transition Loss 2.264744758605957, Classifier Loss 0.3044010400772095, Total Loss 176.17750549316406\n",
      "28: Encoding Loss 16.619165420532227, Transition Loss 2.882425546646118, Classifier Loss 0.31997308135032654, Total Loss 165.52713012695312\n",
      "28: Encoding Loss 17.033475875854492, Transition Loss 2.7137653827667236, Classifier Loss 0.3203047215938568, Total Loss 168.84103393554688\n",
      "28: Encoding Loss 17.99637222290039, Transition Loss 2.8165571689605713, Classifier Loss 0.3004623055458069, Total Loss 174.5805206298828\n",
      "28: Encoding Loss 16.768230438232422, Transition Loss 3.4906234741210938, Classifier Loss 0.3840802311897278, Total Loss 173.2519989013672\n",
      "28: Encoding Loss 17.896753311157227, Transition Loss 2.69724178314209, Classifier Loss 0.3467797040939331, Total Loss 178.3914337158203\n",
      "28: Encoding Loss 17.573829650878906, Transition Loss 3.0701189041137695, Classifier Loss 0.3262028396129608, Total Loss 173.824951171875\n",
      "28: Encoding Loss 17.503103256225586, Transition Loss 3.2499966621398926, Classifier Loss 0.33418723940849304, Total Loss 174.09353637695312\n",
      "28: Encoding Loss 16.83732032775879, Transition Loss 2.8365585803985596, Classifier Loss 0.3310701847076416, Total Loss 168.3728790283203\n",
      "28: Encoding Loss 18.42698097229004, Transition Loss 2.555229663848877, Classifier Loss 0.32746389508247375, Total Loss 180.67327880859375\n",
      "28: Encoding Loss 16.838706970214844, Transition Loss 2.570678949356079, Classifier Loss 0.3488989472389221, Total Loss 170.11367797851562\n",
      "28: Encoding Loss 17.65915870666504, Transition Loss 2.862046718597412, Classifier Loss 0.35295286774635315, Total Loss 177.14096069335938\n",
      "28: Encoding Loss 17.301362991333008, Transition Loss 2.652894973754883, Classifier Loss 0.3398810625076294, Total Loss 172.92959594726562\n",
      "28: Encoding Loss 17.82233238220215, Transition Loss 3.1820664405822754, Classifier Loss 0.35838571190834045, Total Loss 179.05364990234375\n",
      "28: Encoding Loss 18.293500900268555, Transition Loss 2.3593246936798096, Classifier Loss 0.33870425820350647, Total Loss 180.69029235839844\n",
      "28: Encoding Loss 16.440391540527344, Transition Loss 2.7646219730377197, Classifier Loss 0.28588950634002686, Total Loss 160.66500854492188\n",
      "28: Encoding Loss 17.415475845336914, Transition Loss 2.453718423843384, Classifier Loss 0.33364132046699524, Total Loss 173.17868041992188\n",
      "28: Encoding Loss 17.676565170288086, Transition Loss 3.3566880226135254, Classifier Loss 0.3660825192928314, Total Loss 178.69212341308594\n",
      "28: Encoding Loss 17.2908878326416, Transition Loss 2.5911993980407715, Classifier Loss 0.33410757780075073, Total Loss 172.25608825683594\n",
      "28: Encoding Loss 17.61457633972168, Transition Loss 2.4716365337371826, Classifier Loss 0.3481539785861969, Total Loss 176.22633361816406\n",
      "28: Encoding Loss 17.81521987915039, Transition Loss 2.477883815765381, Classifier Loss 0.31099116802215576, Total Loss 174.116455078125\n",
      "28: Encoding Loss 17.983875274658203, Transition Loss 3.110968589782715, Classifier Loss 0.40092185139656067, Total Loss 184.58538818359375\n",
      "28: Encoding Loss 17.381900787353516, Transition Loss 2.786628246307373, Classifier Loss 0.3262118399143219, Total Loss 172.2337188720703\n",
      "28: Encoding Loss 18.542692184448242, Transition Loss 2.4613068103790283, Classifier Loss 0.3098222017288208, Total Loss 179.81602478027344\n",
      "28: Encoding Loss 17.891620635986328, Transition Loss 2.6329307556152344, Classifier Loss 0.3238348066806793, Total Loss 176.04302978515625\n",
      "28: Encoding Loss 16.39078712463379, Transition Loss 2.2757880687713623, Classifier Loss 0.30869829654693604, Total Loss 162.45127868652344\n",
      "28: Encoding Loss 17.697559356689453, Transition Loss 2.826673746109009, Classifier Loss 0.38212305307388306, Total Loss 180.35812377929688\n",
      "28: Encoding Loss 17.328927993774414, Transition Loss 2.4244158267974854, Classifier Loss 0.30024853348731995, Total Loss 169.1411590576172\n",
      "28: Encoding Loss 16.622390747070312, Transition Loss 3.2631022930145264, Classifier Loss 0.3484516143798828, Total Loss 168.47689819335938\n",
      "28: Encoding Loss 18.42853355407715, Transition Loss 2.24873685836792, Classifier Loss 0.3239938020706177, Total Loss 180.27740478515625\n",
      "28: Encoding Loss 17.715166091918945, Transition Loss 2.551759719848633, Classifier Loss 0.33142945170402527, Total Loss 175.37461853027344\n",
      "28: Encoding Loss 17.832799911499023, Transition Loss 2.4271280765533447, Classifier Loss 0.3348686695098877, Total Loss 176.6346893310547\n",
      "28: Encoding Loss 16.832252502441406, Transition Loss 3.366528034210205, Classifier Loss 0.3157154321670532, Total Loss 166.9028778076172\n",
      "28: Encoding Loss 17.35970687866211, Transition Loss 3.1341137886047363, Classifier Loss 0.32886770367622375, Total Loss 172.39125061035156\n",
      "28: Encoding Loss 17.021221160888672, Transition Loss 2.9124109745025635, Classifier Loss 0.33304479718208313, Total Loss 170.0567626953125\n",
      "28: Encoding Loss 15.837915420532227, Transition Loss 3.6038200855255127, Classifier Loss 0.3412402868270874, Total Loss 161.54811096191406\n",
      "28: Encoding Loss 17.22813606262207, Transition Loss 2.805957078933716, Classifier Loss 0.3232841491699219, Total Loss 170.71469116210938\n",
      "28: Encoding Loss 17.437463760375977, Transition Loss 2.9763309955596924, Classifier Loss 0.3429532051086426, Total Loss 174.39028930664062\n",
      "28: Encoding Loss 15.365458488464355, Transition Loss 2.8805935382843018, Classifier Loss 0.3255734145641327, Total Loss 156.05714416503906\n",
      "28: Encoding Loss 17.295026779174805, Transition Loss 3.0792672634124756, Classifier Loss 0.3478396236896515, Total Loss 173.76004028320312\n",
      "28: Encoding Loss 17.470510482788086, Transition Loss 2.9701101779937744, Classifier Loss 0.343668669462204, Total Loss 174.7249755859375\n",
      "28: Encoding Loss 18.08859634399414, Transition Loss 2.9629993438720703, Classifier Loss 0.31226134300231934, Total Loss 176.5275115966797\n",
      "28: Encoding Loss 17.119287490844727, Transition Loss 2.890587329864502, Classifier Loss 0.3167652189731598, Total Loss 169.20895385742188\n",
      "28: Encoding Loss 17.70555877685547, Transition Loss 2.448331356048584, Classifier Loss 0.31400400400161743, Total Loss 173.5345458984375\n",
      "28: Encoding Loss 16.79737091064453, Transition Loss 2.466627836227417, Classifier Loss 0.3503006100654602, Total Loss 169.90235900878906\n",
      "28: Encoding Loss 18.777009963989258, Transition Loss 3.196560859680176, Classifier Loss 0.3718207776546478, Total Loss 188.0374755859375\n",
      "28: Encoding Loss 17.654348373413086, Transition Loss 2.7435293197631836, Classifier Loss 0.3467647433280945, Total Loss 176.4599609375\n",
      "28: Encoding Loss 16.47027015686035, Transition Loss 2.6602792739868164, Classifier Loss 0.33880308270454407, Total Loss 166.17453002929688\n",
      "28: Encoding Loss 17.022720336914062, Transition Loss 2.859049081802368, Classifier Loss 0.32850411534309387, Total Loss 169.60397338867188\n",
      "28: Encoding Loss 17.369035720825195, Transition Loss 2.597270965576172, Classifier Loss 0.30638551712036133, Total Loss 170.11029052734375\n",
      "28: Encoding Loss 16.801067352294922, Transition Loss 2.01379656791687, Classifier Loss 0.3036925196647644, Total Loss 165.18055725097656\n",
      "28: Encoding Loss 16.823713302612305, Transition Loss 2.31720232963562, Classifier Loss 0.3188267946243286, Total Loss 166.93582153320312\n",
      "28: Encoding Loss 17.50372886657715, Transition Loss 3.0047454833984375, Classifier Loss 0.3299335241317749, Total Loss 173.62413024902344\n",
      "28: Encoding Loss 17.651084899902344, Transition Loss 2.1681106090545654, Classifier Loss 0.29260051250457764, Total Loss 170.90235900878906\n",
      "28: Encoding Loss 18.147294998168945, Transition Loss 2.1803879737854004, Classifier Loss 0.3428293466567993, Total Loss 179.8973846435547\n",
      "28: Encoding Loss 16.331743240356445, Transition Loss 3.0149247646331787, Classifier Loss 0.31129398941993713, Total Loss 162.38632202148438\n",
      "28: Encoding Loss 16.779701232910156, Transition Loss 3.329890727996826, Classifier Loss 0.34268489480018616, Total Loss 169.17208862304688\n",
      "28: Encoding Loss 16.649869918823242, Transition Loss 2.860830783843994, Classifier Loss 0.3447440564632416, Total Loss 168.2455291748047\n",
      "28: Encoding Loss 17.187522888183594, Transition Loss 3.1779017448425293, Classifier Loss 0.36680978536605835, Total Loss 174.8167266845703\n",
      "28: Encoding Loss 16.952795028686523, Transition Loss 3.875631809234619, Classifier Loss 0.3265698254108429, Total Loss 169.05447387695312\n",
      "28: Encoding Loss 17.988672256469727, Transition Loss 2.7572951316833496, Classifier Loss 0.31794339418411255, Total Loss 176.2551727294922\n",
      "28: Encoding Loss 17.81362533569336, Transition Loss 2.6336634159088135, Classifier Loss 0.3050832450389862, Total Loss 173.5440673828125\n",
      "28: Encoding Loss 16.594167709350586, Transition Loss 2.745952606201172, Classifier Loss 0.3325507640838623, Total Loss 166.5576171875\n",
      "28: Encoding Loss 16.813596725463867, Transition Loss 2.4806082248687744, Classifier Loss 0.314924418926239, Total Loss 166.49734497070312\n",
      "28: Encoding Loss 17.10436248779297, Transition Loss 2.8216476440429688, Classifier Loss 0.33717840909957886, Total Loss 171.1170654296875\n",
      "28: Encoding Loss 17.104272842407227, Transition Loss 2.7165334224700928, Classifier Loss 0.3273196816444397, Total Loss 170.1094512939453\n",
      "28: Encoding Loss 17.223711013793945, Transition Loss 2.448711633682251, Classifier Loss 0.3325655162334442, Total Loss 171.53598022460938\n",
      "28: Encoding Loss 17.091245651245117, Transition Loss 2.5530893802642822, Classifier Loss 0.3166278898715973, Total Loss 168.90338134765625\n",
      "28: Encoding Loss 15.924529075622559, Transition Loss 2.6040196418762207, Classifier Loss 0.3079145550727844, Total Loss 158.70848083496094\n",
      "28: Encoding Loss 16.380815505981445, Transition Loss 2.903092384338379, Classifier Loss 0.29331648349761963, Total Loss 160.9587860107422\n",
      "28: Encoding Loss 18.59916114807129, Transition Loss 2.5202298164367676, Classifier Loss 0.38226088881492615, Total Loss 187.52342224121094\n",
      "28: Encoding Loss 16.71115493774414, Transition Loss 3.392026901245117, Classifier Loss 0.35662031173706055, Total Loss 170.02967834472656\n",
      "28: Encoding Loss 17.513654708862305, Transition Loss 2.740283727645874, Classifier Loss 0.32734405994415283, Total Loss 173.39169311523438\n",
      "28: Encoding Loss 17.618921279907227, Transition Loss 3.260241985321045, Classifier Loss 0.3394322395324707, Total Loss 175.54664611816406\n",
      "28: Encoding Loss 17.36357879638672, Transition Loss 2.7977113723754883, Classifier Loss 0.3265606462955475, Total Loss 172.12423706054688\n",
      "28: Encoding Loss 16.436071395874023, Transition Loss 3.0691020488739014, Classifier Loss 0.3533731997013092, Total Loss 167.43971252441406\n",
      "28: Encoding Loss 18.38054084777832, Transition Loss 2.053313970565796, Classifier Loss 0.3052920997142792, Total Loss 177.98419189453125\n",
      "28: Encoding Loss 16.829692840576172, Transition Loss 2.1344192028045654, Classifier Loss 0.294772207736969, Total Loss 164.54165649414062\n",
      "28: Encoding Loss 17.02546501159668, Transition Loss 2.5668060779571533, Classifier Loss 0.29306623339653015, Total Loss 166.02371215820312\n",
      "28: Encoding Loss 15.8964204788208, Transition Loss 2.877002239227295, Classifier Loss 0.295504093170166, Total Loss 157.2971649169922\n",
      "28: Encoding Loss 16.4420108795166, Transition Loss 2.0344300270080566, Classifier Loss 0.3163428008556366, Total Loss 163.57725524902344\n",
      "28: Encoding Loss 17.603662490844727, Transition Loss 3.441154718399048, Classifier Loss 0.33885762095451355, Total Loss 175.40328979492188\n",
      "28: Encoding Loss 17.28956413269043, Transition Loss 2.3602871894836426, Classifier Loss 0.28775495290756226, Total Loss 167.56407165527344\n",
      "28: Encoding Loss 18.935630798339844, Transition Loss 2.83835506439209, Classifier Loss 0.3208805024623871, Total Loss 184.14076232910156\n",
      "28: Encoding Loss 18.456430435180664, Transition Loss 2.973252058029175, Classifier Loss 0.3088723421096802, Total Loss 179.13333129882812\n",
      "28: Encoding Loss 17.21444320678711, Transition Loss 3.7110676765441895, Classifier Loss 0.3205283582210541, Total Loss 170.5106201171875\n",
      "28: Encoding Loss 15.54613208770752, Transition Loss 4.145322322845459, Classifier Loss 0.39421898126602173, Total Loss 164.62002563476562\n",
      "29: Encoding Loss 17.939167022705078, Transition Loss 2.8010454177856445, Classifier Loss 0.3082192838191986, Total Loss 174.89547729492188\n",
      "29: Encoding Loss 17.839021682739258, Transition Loss 2.510349750518799, Classifier Loss 0.3231903314590454, Total Loss 175.5332794189453\n",
      "29: Encoding Loss 18.306108474731445, Transition Loss 2.8361635208129883, Classifier Loss 0.31479412317276, Total Loss 178.49551391601562\n",
      "29: Encoding Loss 17.20054817199707, Transition Loss 2.6985530853271484, Classifier Loss 0.3308505713939667, Total Loss 171.22914123535156\n",
      "29: Encoding Loss 18.610149383544922, Transition Loss 2.6981968879699707, Classifier Loss 0.30247482657432556, Total Loss 179.6683349609375\n",
      "29: Encoding Loss 16.1204833984375, Transition Loss 3.068241596221924, Classifier Loss 0.35181915760040283, Total Loss 164.75942993164062\n",
      "29: Encoding Loss 18.134428024291992, Transition Loss 2.496985912322998, Classifier Loss 0.3598382771015167, Total Loss 181.5586395263672\n",
      "29: Encoding Loss 19.214405059814453, Transition Loss 2.5314815044403076, Classifier Loss 0.351950466632843, Total Loss 189.4165802001953\n",
      "29: Encoding Loss 17.328838348388672, Transition Loss 2.8411312103271484, Classifier Loss 0.39403775334358215, Total Loss 178.60272216796875\n",
      "29: Encoding Loss 18.82648277282715, Transition Loss 2.7121880054473877, Classifier Loss 0.35098737478256226, Total Loss 186.25303649902344\n",
      "29: Encoding Loss 17.89236068725586, Transition Loss 2.586932420730591, Classifier Loss 0.31913211941719055, Total Loss 175.56948852539062\n",
      "29: Encoding Loss 17.309629440307617, Transition Loss 2.645296096801758, Classifier Loss 0.35246115922927856, Total Loss 174.252197265625\n",
      "29: Encoding Loss 16.776988983154297, Transition Loss 2.7168657779693604, Classifier Loss 0.33848732709884644, Total Loss 168.60804748535156\n",
      "29: Encoding Loss 16.94719696044922, Transition Loss 2.764951705932617, Classifier Loss 0.3159138858318329, Total Loss 167.72195434570312\n",
      "29: Encoding Loss 16.62664794921875, Transition Loss 2.9471325874328613, Classifier Loss 0.3196541965007782, Total Loss 165.5680389404297\n",
      "29: Encoding Loss 17.26708221435547, Transition Loss 3.2410595417022705, Classifier Loss 0.3411678671836853, Total Loss 172.9016571044922\n",
      "29: Encoding Loss 17.18231964111328, Transition Loss 2.9709081649780273, Classifier Loss 0.33091312646865845, Total Loss 171.14404296875\n",
      "29: Encoding Loss 16.999887466430664, Transition Loss 2.8321726322174072, Classifier Loss 0.30535081028938293, Total Loss 167.10061645507812\n",
      "29: Encoding Loss 17.438390731811523, Transition Loss 2.8930916786193848, Classifier Loss 0.3139893710613251, Total Loss 171.48468017578125\n",
      "29: Encoding Loss 18.06109046936035, Transition Loss 2.9480130672454834, Classifier Loss 0.3286973834037781, Total Loss 177.94805908203125\n",
      "29: Encoding Loss 17.118059158325195, Transition Loss 3.2649431228637695, Classifier Loss 0.33702996373176575, Total Loss 171.30044555664062\n",
      "29: Encoding Loss 16.89863395690918, Transition Loss 2.555485725402832, Classifier Loss 0.30270665884017944, Total Loss 165.9708251953125\n",
      "29: Encoding Loss 16.165842056274414, Transition Loss 3.8122856616973877, Classifier Loss 0.3429020643234253, Total Loss 164.37939453125\n",
      "29: Encoding Loss 16.3876895904541, Transition Loss 2.5541439056396484, Classifier Loss 0.3290501832962036, Total Loss 164.51736450195312\n",
      "29: Encoding Loss 17.035686492919922, Transition Loss 2.260176420211792, Classifier Loss 0.3005637228488922, Total Loss 166.79391479492188\n",
      "29: Encoding Loss 17.52019500732422, Transition Loss 2.7793073654174805, Classifier Loss 0.3221031129360199, Total Loss 172.927734375\n",
      "29: Encoding Loss 17.92513656616211, Transition Loss 3.025479793548584, Classifier Loss 0.3207542896270752, Total Loss 176.08163452148438\n",
      "29: Encoding Loss 17.467641830444336, Transition Loss 3.1836862564086914, Classifier Loss 0.31870317459106445, Total Loss 172.24818420410156\n",
      "29: Encoding Loss 16.835559844970703, Transition Loss 3.1312148571014404, Classifier Loss 0.33171579241752625, Total Loss 168.48228454589844\n",
      "29: Encoding Loss 16.56592559814453, Transition Loss 3.5040462017059326, Classifier Loss 0.34038519859313965, Total Loss 167.2667236328125\n",
      "29: Encoding Loss 17.419153213500977, Transition Loss 3.0195822715759277, Classifier Loss 0.35780495405197144, Total Loss 175.73764038085938\n",
      "29: Encoding Loss 16.009424209594727, Transition Loss 3.5993480682373047, Classifier Loss 0.31154268980026245, Total Loss 159.94952392578125\n",
      "29: Encoding Loss 17.578489303588867, Transition Loss 3.266071081161499, Classifier Loss 0.3435792326927185, Total Loss 175.63905334472656\n",
      "29: Encoding Loss 16.247669219970703, Transition Loss 2.1707377433776855, Classifier Loss 0.33594435453414917, Total Loss 164.0099334716797\n",
      "29: Encoding Loss 16.574735641479492, Transition Loss 2.979670763015747, Classifier Loss 0.347134530544281, Total Loss 167.9072723388672\n",
      "29: Encoding Loss 15.893391609191895, Transition Loss 2.399394989013672, Classifier Loss 0.33332359790802, Total Loss 160.95936584472656\n",
      "29: Encoding Loss 17.440387725830078, Transition Loss 2.976118564605713, Classifier Loss 0.2897392213344574, Total Loss 169.09225463867188\n",
      "29: Encoding Loss 17.143348693847656, Transition Loss 4.165785312652588, Classifier Loss 0.38097724318504333, Total Loss 176.07766723632812\n",
      "29: Encoding Loss 17.570537567138672, Transition Loss 2.7641420364379883, Classifier Loss 0.3169926404953003, Total Loss 172.81640625\n",
      "29: Encoding Loss 16.895212173461914, Transition Loss 3.3992984294891357, Classifier Loss 0.3119577169418335, Total Loss 167.03732299804688\n",
      "29: Encoding Loss 16.38365936279297, Transition Loss 3.293517827987671, Classifier Loss 0.35704657435417175, Total Loss 167.43263244628906\n",
      "29: Encoding Loss 16.623374938964844, Transition Loss 2.5555195808410645, Classifier Loss 0.3216124475002289, Total Loss 165.6593475341797\n",
      "29: Encoding Loss 16.562976837158203, Transition Loss 2.3072493076324463, Classifier Loss 0.31682705879211426, Total Loss 164.64797973632812\n",
      "29: Encoding Loss 16.48939323425293, Transition Loss 2.069951295852661, Classifier Loss 0.2960357367992401, Total Loss 161.93270874023438\n",
      "29: Encoding Loss 18.283050537109375, Transition Loss 2.4634041786193848, Classifier Loss 0.327698290348053, Total Loss 179.52691650390625\n",
      "29: Encoding Loss 17.889558792114258, Transition Loss 3.1955745220184326, Classifier Loss 0.34539732336997986, Total Loss 178.29531860351562\n",
      "29: Encoding Loss 17.08333396911621, Transition Loss 3.4207937717437744, Classifier Loss 0.34118127822875977, Total Loss 171.4689483642578\n",
      "29: Encoding Loss 18.087299346923828, Transition Loss 2.992249011993408, Classifier Loss 0.3303542733192444, Total Loss 178.332275390625\n",
      "29: Encoding Loss 17.51603889465332, Transition Loss 2.953383445739746, Classifier Loss 0.3207976520061493, Total Loss 172.7987518310547\n",
      "29: Encoding Loss 17.417713165283203, Transition Loss 3.1505463123321533, Classifier Loss 0.3057810962200165, Total Loss 170.5499267578125\n",
      "29: Encoding Loss 17.138139724731445, Transition Loss 2.2763593196868896, Classifier Loss 0.32105064392089844, Total Loss 169.6654510498047\n",
      "29: Encoding Loss 17.064878463745117, Transition Loss 2.5333919525146484, Classifier Loss 0.2981114685535431, Total Loss 166.83685302734375\n",
      "29: Encoding Loss 16.642784118652344, Transition Loss 2.2343573570251465, Classifier Loss 0.3231407701969147, Total Loss 165.90321350097656\n",
      "29: Encoding Loss 17.19927406311035, Transition Loss 3.1170694828033447, Classifier Loss 0.3137504458427429, Total Loss 169.5926513671875\n",
      "29: Encoding Loss 16.48211097717285, Transition Loss 3.1196913719177246, Classifier Loss 0.3303801417350769, Total Loss 165.51882934570312\n",
      "29: Encoding Loss 15.886699676513672, Transition Loss 2.914445638656616, Classifier Loss 0.33991241455078125, Total Loss 161.667724609375\n",
      "29: Encoding Loss 16.84364891052246, Transition Loss 2.640836715698242, Classifier Loss 0.3216572105884552, Total Loss 167.44308471679688\n",
      "29: Encoding Loss 16.496105194091797, Transition Loss 2.837113380432129, Classifier Loss 0.2766292691230774, Total Loss 160.19921875\n",
      "29: Encoding Loss 16.606721878051758, Transition Loss 2.5735256671905518, Classifier Loss 0.3241880238056183, Total Loss 165.78729248046875\n",
      "29: Encoding Loss 16.49199676513672, Transition Loss 2.6282105445861816, Classifier Loss 0.32339906692504883, Total Loss 164.801513671875\n",
      "29: Encoding Loss 17.859092712402344, Transition Loss 3.0540385246276855, Classifier Loss 0.32645222544670105, Total Loss 176.12876892089844\n",
      "29: Encoding Loss 18.584436416625977, Transition Loss 2.8091588020324707, Classifier Loss 0.3231995105743408, Total Loss 181.55726623535156\n",
      "29: Encoding Loss 17.063114166259766, Transition Loss 2.5640828609466553, Classifier Loss 0.3179038166999817, Total Loss 168.80810546875\n",
      "29: Encoding Loss 16.79678726196289, Transition Loss 2.5598244667053223, Classifier Loss 0.3207041025161743, Total Loss 166.9566650390625\n",
      "29: Encoding Loss 16.381235122680664, Transition Loss 2.8463332653045654, Classifier Loss 0.2794593870639801, Total Loss 159.56507873535156\n",
      "29: Encoding Loss 16.066057205200195, Transition Loss 2.7258827686309814, Classifier Loss 0.3073940575122833, Total Loss 159.81304931640625\n",
      "29: Encoding Loss 17.145626068115234, Transition Loss 2.635770082473755, Classifier Loss 0.31357645988464355, Total Loss 169.04983520507812\n",
      "29: Encoding Loss 18.082977294921875, Transition Loss 2.6772985458374023, Classifier Loss 0.2928686738014221, Total Loss 174.48614501953125\n",
      "29: Encoding Loss 17.746768951416016, Transition Loss 3.25361967086792, Classifier Loss 0.3421696126461029, Total Loss 176.8418426513672\n",
      "29: Encoding Loss 16.92094612121582, Transition Loss 2.922933340072632, Classifier Loss 0.330889493227005, Total Loss 169.0410919189453\n",
      "29: Encoding Loss 17.82250213623047, Transition Loss 2.7144734859466553, Classifier Loss 0.30823463201522827, Total Loss 173.9463653564453\n",
      "29: Encoding Loss 16.662378311157227, Transition Loss 3.164108991622925, Classifier Loss 0.3199082911014557, Total Loss 165.9226837158203\n",
      "29: Encoding Loss 16.606653213500977, Transition Loss 3.110626220703125, Classifier Loss 0.28165340423583984, Total Loss 161.6407012939453\n",
      "29: Encoding Loss 17.662607192993164, Transition Loss 3.14324688911438, Classifier Loss 0.2809302806854248, Total Loss 170.0225372314453\n",
      "29: Encoding Loss 16.303110122680664, Transition Loss 3.831273078918457, Classifier Loss 0.34526708722114563, Total Loss 165.7178497314453\n",
      "29: Encoding Loss 17.457090377807617, Transition Loss 2.962273359298706, Classifier Loss 0.3181663453578949, Total Loss 172.06581115722656\n",
      "29: Encoding Loss 16.948200225830078, Transition Loss 3.368711471557617, Classifier Loss 0.30802109837532043, Total Loss 167.0614471435547\n",
      "29: Encoding Loss 17.015098571777344, Transition Loss 3.4813005924224854, Classifier Loss 0.32325851917266846, Total Loss 169.1428985595703\n",
      "29: Encoding Loss 16.37611961364746, Transition Loss 3.132275342941284, Classifier Loss 0.3120732307434082, Total Loss 162.8427276611328\n",
      "29: Encoding Loss 18.1270751953125, Transition Loss 2.862680196762085, Classifier Loss 0.29578885436058044, Total Loss 175.16802978515625\n",
      "29: Encoding Loss 16.59700584411621, Transition Loss 2.8351924419403076, Classifier Loss 0.306806743144989, Total Loss 164.0237579345703\n",
      "29: Encoding Loss 17.31730842590332, Transition Loss 3.163330554962158, Classifier Loss 0.3475542664527893, Total Loss 173.9265594482422\n",
      "29: Encoding Loss 16.908349990844727, Transition Loss 2.9251723289489746, Classifier Loss 0.3104696571826935, Total Loss 166.8988037109375\n",
      "29: Encoding Loss 17.274837493896484, Transition Loss 3.6053459644317627, Classifier Loss 0.3248533010482788, Total Loss 171.40512084960938\n",
      "29: Encoding Loss 17.942859649658203, Transition Loss 2.6852450370788574, Classifier Loss 0.2953106760978699, Total Loss 173.61099243164062\n",
      "29: Encoding Loss 16.16446876525879, Transition Loss 3.0342671871185303, Classifier Loss 0.2854658365249634, Total Loss 158.4691925048828\n",
      "29: Encoding Loss 16.949867248535156, Transition Loss 2.662170886993408, Classifier Loss 0.34280523657798767, Total Loss 170.41189575195312\n",
      "29: Encoding Loss 17.10127067565918, Transition Loss 3.803995132446289, Classifier Loss 0.3292321264743805, Total Loss 170.4941864013672\n",
      "29: Encoding Loss 16.916048049926758, Transition Loss 2.8001506328582764, Classifier Loss 0.33372852206230164, Total Loss 169.26126098632812\n",
      "29: Encoding Loss 17.190641403198242, Transition Loss 2.7313671112060547, Classifier Loss 0.32514894008636475, Total Loss 170.5863037109375\n",
      "29: Encoding Loss 17.21741485595703, Transition Loss 2.824622631072998, Classifier Loss 0.2779359221458435, Total Loss 166.09783935546875\n",
      "29: Encoding Loss 17.29014778137207, Transition Loss 3.6195900440216064, Classifier Loss 0.3722652494907379, Total Loss 176.27162170410156\n",
      "29: Encoding Loss 17.09060287475586, Transition Loss 3.1891722679138184, Classifier Loss 0.3258574306964874, Total Loss 169.9484100341797\n",
      "29: Encoding Loss 17.993715286254883, Transition Loss 2.782594919204712, Classifier Loss 0.3077213764190674, Total Loss 175.27838134765625\n",
      "29: Encoding Loss 17.50235366821289, Transition Loss 2.970065116882324, Classifier Loss 0.3133232295513153, Total Loss 171.94515991210938\n",
      "29: Encoding Loss 16.31488609313965, Transition Loss 2.4249939918518066, Classifier Loss 0.3001464009284973, Total Loss 161.0187225341797\n",
      "29: Encoding Loss 17.15296173095703, Transition Loss 3.2127208709716797, Classifier Loss 0.33102715015411377, Total Loss 170.96896362304688\n",
      "29: Encoding Loss 17.02054214477539, Transition Loss 2.7067055702209473, Classifier Loss 0.3461011052131653, Total Loss 171.31578063964844\n",
      "29: Encoding Loss 16.28944206237793, Transition Loss 3.6507270336151123, Classifier Loss 0.344217449426651, Total Loss 165.46742248535156\n",
      "29: Encoding Loss 17.831417083740234, Transition Loss 2.4904489517211914, Classifier Loss 0.32029107213020325, Total Loss 175.1785430908203\n",
      "29: Encoding Loss 17.326589584350586, Transition Loss 2.841662645339966, Classifier Loss 0.31684303283691406, Total Loss 170.8653564453125\n",
      "29: Encoding Loss 17.50226593017578, Transition Loss 2.7516489028930664, Classifier Loss 0.34826117753982544, Total Loss 175.39456176757812\n",
      "29: Encoding Loss 16.63484001159668, Transition Loss 3.735440254211426, Classifier Loss 0.31845951080322266, Total Loss 165.6717529296875\n",
      "29: Encoding Loss 16.884937286376953, Transition Loss 3.448030471801758, Classifier Loss 0.3089068830013275, Total Loss 166.6597900390625\n",
      "29: Encoding Loss 16.757320404052734, Transition Loss 3.2701544761657715, Classifier Loss 0.29749637842178345, Total Loss 164.46224975585938\n",
      "29: Encoding Loss 15.618341445922852, Transition Loss 4.034418106079102, Classifier Loss 0.3008950650691986, Total Loss 155.84312438964844\n",
      "29: Encoding Loss 16.97597312927246, Transition Loss 3.056546926498413, Classifier Loss 0.3119191825389862, Total Loss 167.6110076904297\n",
      "29: Encoding Loss 16.902042388916016, Transition Loss 3.2795817852020264, Classifier Loss 0.3146383464336395, Total Loss 167.33609008789062\n",
      "29: Encoding Loss 15.318995475769043, Transition Loss 3.007884979248047, Classifier Loss 0.3117832541465759, Total Loss 154.3318634033203\n",
      "29: Encoding Loss 16.764135360717773, Transition Loss 3.3934717178344727, Classifier Loss 0.30534374713897705, Total Loss 165.32615661621094\n",
      "29: Encoding Loss 17.001298904418945, Transition Loss 3.3297669887542725, Classifier Loss 0.30628135800361633, Total Loss 167.30447387695312\n",
      "29: Encoding Loss 17.59517478942871, Transition Loss 3.348741292953491, Classifier Loss 0.31814801692962646, Total Loss 173.24595642089844\n",
      "29: Encoding Loss 16.698877334594727, Transition Loss 3.1359682083129883, Classifier Loss 0.3045796751976013, Total Loss 164.67617797851562\n",
      "29: Encoding Loss 17.29383659362793, Transition Loss 2.697377920150757, Classifier Loss 0.29550373554229736, Total Loss 168.44053649902344\n",
      "29: Encoding Loss 16.478668212890625, Transition Loss 2.609025716781616, Classifier Loss 0.35524633526802063, Total Loss 167.8757781982422\n",
      "29: Encoding Loss 18.000568389892578, Transition Loss 3.5552403926849365, Classifier Loss 0.3219892382621765, Total Loss 176.91452026367188\n",
      "29: Encoding Loss 17.4183349609375, Transition Loss 3.0622341632843018, Classifier Loss 0.31100204586982727, Total Loss 171.059326171875\n",
      "29: Encoding Loss 16.30459213256836, Transition Loss 2.9827792644500732, Classifier Loss 0.3031640648841858, Total Loss 161.34971618652344\n",
      "29: Encoding Loss 16.739599227905273, Transition Loss 3.154879570007324, Classifier Loss 0.2923829257488251, Total Loss 163.78607177734375\n",
      "29: Encoding Loss 17.09938621520996, Transition Loss 2.8727774620056152, Classifier Loss 0.31782546639442444, Total Loss 169.15219116210938\n",
      "29: Encoding Loss 16.83650016784668, Transition Loss 2.217026948928833, Classifier Loss 0.3085921108722687, Total Loss 165.99461364746094\n",
      "29: Encoding Loss 16.596364974975586, Transition Loss 2.5227737426757812, Classifier Loss 0.3209735155105591, Total Loss 165.37283325195312\n",
      "29: Encoding Loss 17.158700942993164, Transition Loss 3.339420795440674, Classifier Loss 0.3542007505893707, Total Loss 173.35755920410156\n",
      "29: Encoding Loss 17.50079345703125, Transition Loss 2.409142255783081, Classifier Loss 0.3043466806411743, Total Loss 170.92283630371094\n",
      "29: Encoding Loss 17.87711524963379, Transition Loss 2.473062038421631, Classifier Loss 0.3235766887664795, Total Loss 175.86920166015625\n",
      "29: Encoding Loss 16.24466896057129, Transition Loss 3.2698757648468018, Classifier Loss 0.3415968418121338, Total Loss 164.77101135253906\n",
      "29: Encoding Loss 16.20777130126953, Transition Loss 3.6487948894500732, Classifier Loss 0.31789639592170715, Total Loss 162.1815643310547\n",
      "29: Encoding Loss 16.319337844848633, Transition Loss 3.141777276992798, Classifier Loss 0.3462894558906555, Total Loss 165.81201171875\n",
      "29: Encoding Loss 16.69846534729004, Transition Loss 3.4721474647521973, Classifier Loss 0.34786689281463623, Total Loss 169.06884765625\n",
      "29: Encoding Loss 16.255144119262695, Transition Loss 4.377912521362305, Classifier Loss 0.319548100233078, Total Loss 162.8715362548828\n",
      "29: Encoding Loss 17.52789878845215, Transition Loss 3.0144243240356445, Classifier Loss 0.32268181443214417, Total Loss 173.09425354003906\n",
      "29: Encoding Loss 17.446260452270508, Transition Loss 2.8720543384552, Classifier Loss 0.2830091714859009, Total Loss 168.44541931152344\n",
      "29: Encoding Loss 16.15204429626465, Transition Loss 2.871091842651367, Classifier Loss 0.28562405705451965, Total Loss 158.3529815673828\n",
      "29: Encoding Loss 16.673158645629883, Transition Loss 2.6194212436676025, Classifier Loss 0.2927972078323364, Total Loss 163.18887329101562\n",
      "29: Encoding Loss 16.72396469116211, Transition Loss 2.9700446128845215, Classifier Loss 0.33016037940979004, Total Loss 167.4017791748047\n",
      "29: Encoding Loss 16.83351707458496, Transition Loss 2.9885406494140625, Classifier Loss 0.29631292819976807, Total Loss 164.89712524414062\n",
      "29: Encoding Loss 16.814355850219727, Transition Loss 2.6620967388153076, Classifier Loss 0.29944726824760437, Total Loss 164.99200439453125\n",
      "29: Encoding Loss 16.858272552490234, Transition Loss 2.776890993118286, Classifier Loss 0.3465004861354828, Total Loss 170.0716094970703\n",
      "29: Encoding Loss 15.827558517456055, Transition Loss 2.7036690711975098, Classifier Loss 0.2980039417743683, Total Loss 156.96160888671875\n",
      "29: Encoding Loss 16.21135711669922, Transition Loss 3.101195812225342, Classifier Loss 0.24861900508403778, Total Loss 155.17300415039062\n",
      "29: Encoding Loss 17.848011016845703, Transition Loss 2.6708686351776123, Classifier Loss 0.34940391778945923, Total Loss 178.2586669921875\n",
      "29: Encoding Loss 16.428037643432617, Transition Loss 3.62607479095459, Classifier Loss 0.3294835686683655, Total Loss 165.09786987304688\n",
      "29: Encoding Loss 17.119489669799805, Transition Loss 2.944622039794922, Classifier Loss 0.32532909512519836, Total Loss 170.0777587890625\n",
      "29: Encoding Loss 17.061466217041016, Transition Loss 3.4949986934661865, Classifier Loss 0.3379136919975281, Total Loss 170.9821014404297\n",
      "29: Encoding Loss 16.976472854614258, Transition Loss 2.989891529083252, Classifier Loss 0.3082022964954376, Total Loss 167.22999572753906\n",
      "29: Encoding Loss 16.233182907104492, Transition Loss 3.272042989730835, Classifier Loss 0.3146962523460388, Total Loss 161.98948669433594\n",
      "29: Encoding Loss 18.121736526489258, Transition Loss 2.282252311706543, Classifier Loss 0.31783586740493774, Total Loss 177.21392822265625\n",
      "29: Encoding Loss 16.85272216796875, Transition Loss 2.280437707901001, Classifier Loss 0.2968639135360718, Total Loss 164.96424865722656\n",
      "29: Encoding Loss 16.78243637084961, Transition Loss 2.7065041065216064, Classifier Loss 0.3127228915691376, Total Loss 166.07310485839844\n",
      "29: Encoding Loss 15.828339576721191, Transition Loss 3.0933432579040527, Classifier Loss 0.29075315594673157, Total Loss 156.32070922851562\n",
      "29: Encoding Loss 16.3099308013916, Transition Loss 2.153226375579834, Classifier Loss 0.3016369938850403, Total Loss 161.07379150390625\n",
      "29: Encoding Loss 17.329301834106445, Transition Loss 3.840200424194336, Classifier Loss 0.3258374333381653, Total Loss 171.98619079589844\n",
      "29: Encoding Loss 17.04141616821289, Transition Loss 2.654606819152832, Classifier Loss 0.2851659953594208, Total Loss 165.37884521484375\n",
      "29: Encoding Loss 18.23710060119629, Transition Loss 3.243036985397339, Classifier Loss 0.3098461925983429, Total Loss 177.530029296875\n",
      "29: Encoding Loss 17.714826583862305, Transition Loss 3.371060371398926, Classifier Loss 0.3053051233291626, Total Loss 172.92333984375\n",
      "29: Encoding Loss 16.719247817993164, Transition Loss 4.129459381103516, Classifier Loss 0.3098773956298828, Total Loss 165.567626953125\n",
      "29: Encoding Loss 15.036150932312012, Transition Loss 4.45816707611084, Classifier Loss 0.33278393745422363, Total Loss 154.459228515625\n",
      "30: Encoding Loss 17.60784149169922, Transition Loss 3.023798942565918, Classifier Loss 0.2775164246559143, Total Loss 169.21914672851562\n",
      "30: Encoding Loss 17.366939544677734, Transition Loss 2.719750165939331, Classifier Loss 0.33644774556159973, Total Loss 173.12425231933594\n",
      "30: Encoding Loss 17.77005958557129, Transition Loss 3.0977072715759277, Classifier Loss 0.3280269205570221, Total Loss 175.58270263671875\n",
      "30: Encoding Loss 16.881393432617188, Transition Loss 2.910428285598755, Classifier Loss 0.31287306547164917, Total Loss 166.92054748535156\n",
      "30: Encoding Loss 18.254528045654297, Transition Loss 3.082592725753784, Classifier Loss 0.3024589419364929, Total Loss 176.89865112304688\n",
      "30: Encoding Loss 15.492972373962402, Transition Loss 3.1976876258850098, Classifier Loss 0.3436993360519409, Total Loss 158.95326232910156\n",
      "30: Encoding Loss 17.69791603088379, Transition Loss 2.833479881286621, Classifier Loss 0.32477638125419617, Total Loss 174.62767028808594\n",
      "30: Encoding Loss 18.690471649169922, Transition Loss 2.861077308654785, Classifier Loss 0.3167487382888794, Total Loss 181.77088928222656\n",
      "30: Encoding Loss 16.931779861450195, Transition Loss 3.0198628902435303, Classifier Loss 0.36694979667663574, Total Loss 172.75318908691406\n",
      "30: Encoding Loss 18.18486213684082, Transition Loss 2.9155328273773193, Classifier Loss 0.3622671961784363, Total Loss 182.28871154785156\n",
      "30: Encoding Loss 17.43632698059082, Transition Loss 2.7245821952819824, Classifier Loss 0.3254064619541168, Total Loss 172.57618713378906\n",
      "30: Encoding Loss 16.90079116821289, Transition Loss 2.8445496559143066, Classifier Loss 0.31851908564567566, Total Loss 167.6271514892578\n",
      "30: Encoding Loss 16.481660842895508, Transition Loss 2.94091534614563, Classifier Loss 0.3295239508152008, Total Loss 165.39385986328125\n",
      "30: Encoding Loss 16.43901252746582, Transition Loss 2.9253251552581787, Classifier Loss 0.3079710900783539, Total Loss 162.89427185058594\n",
      "30: Encoding Loss 16.43393898010254, Transition Loss 3.1824848651885986, Classifier Loss 0.31976568698883057, Total Loss 164.08456420898438\n",
      "30: Encoding Loss 16.96947479248047, Transition Loss 3.544013738632202, Classifier Loss 0.2890644967556, Total Loss 165.3710479736328\n",
      "30: Encoding Loss 17.084787368774414, Transition Loss 3.164057731628418, Classifier Loss 0.31629449129104614, Total Loss 168.94056701660156\n",
      "30: Encoding Loss 16.594717025756836, Transition Loss 3.1746826171875, Classifier Loss 0.2871914505958557, Total Loss 162.11181640625\n",
      "30: Encoding Loss 17.265411376953125, Transition Loss 3.2641172409057617, Classifier Loss 0.2875808775424957, Total Loss 167.53419494628906\n",
      "30: Encoding Loss 17.52218246459961, Transition Loss 3.2704520225524902, Classifier Loss 0.2945100665092468, Total Loss 170.28256225585938\n",
      "30: Encoding Loss 16.77572250366211, Transition Loss 3.630342721939087, Classifier Loss 0.3228514492511749, Total Loss 167.21701049804688\n",
      "30: Encoding Loss 16.64173126220703, Transition Loss 2.76977276802063, Classifier Loss 0.2854256331920624, Total Loss 162.23036193847656\n",
      "30: Encoding Loss 15.7267484664917, Transition Loss 4.158201694488525, Classifier Loss 0.32596972584724426, Total Loss 159.2425994873047\n",
      "30: Encoding Loss 16.286298751831055, Transition Loss 2.7903473377227783, Classifier Loss 0.3473362326622009, Total Loss 165.58209228515625\n",
      "30: Encoding Loss 16.67856788635254, Transition Loss 2.326669216156006, Classifier Loss 0.30138951539993286, Total Loss 164.03282165527344\n",
      "30: Encoding Loss 17.05422019958496, Transition Loss 2.9293878078460693, Classifier Loss 0.30242040753364563, Total Loss 167.2616729736328\n",
      "30: Encoding Loss 17.336523056030273, Transition Loss 3.330944299697876, Classifier Loss 0.31341874599456787, Total Loss 170.7002410888672\n",
      "30: Encoding Loss 17.049068450927734, Transition Loss 3.504906415939331, Classifier Loss 0.3155341148376465, Total Loss 168.64695739746094\n",
      "30: Encoding Loss 16.69367027282715, Transition Loss 3.4656429290771484, Classifier Loss 0.3235672116279602, Total Loss 166.59921264648438\n",
      "30: Encoding Loss 16.072866439819336, Transition Loss 3.7102627754211426, Classifier Loss 0.33686313033103943, Total Loss 163.01129150390625\n",
      "30: Encoding Loss 16.94983673095703, Transition Loss 3.289750337600708, Classifier Loss 0.3061748743057251, Total Loss 166.87413024902344\n",
      "30: Encoding Loss 15.626107215881348, Transition Loss 3.922841787338257, Classifier Loss 0.2831529974937439, Total Loss 154.1087188720703\n",
      "30: Encoding Loss 17.159069061279297, Transition Loss 3.589186191558838, Classifier Loss 0.3348659873008728, Total Loss 171.4770050048828\n",
      "30: Encoding Loss 16.162145614624023, Transition Loss 2.1981921195983887, Classifier Loss 0.326163649559021, Total Loss 162.35316467285156\n",
      "30: Encoding Loss 16.028789520263672, Transition Loss 3.2848384380340576, Classifier Loss 0.3067372441291809, Total Loss 159.56101989746094\n",
      "30: Encoding Loss 15.747323036193848, Transition Loss 2.463222026824951, Classifier Loss 0.30651235580444336, Total Loss 157.12246704101562\n",
      "30: Encoding Loss 16.928119659423828, Transition Loss 3.297619104385376, Classifier Loss 0.3038502335548401, Total Loss 166.46951293945312\n",
      "30: Encoding Loss 16.764629364013672, Transition Loss 4.43808126449585, Classifier Loss 0.3326402008533478, Total Loss 168.26869201660156\n",
      "30: Encoding Loss 17.356388092041016, Transition Loss 3.029541015625, Classifier Loss 0.28538763523101807, Total Loss 167.9957733154297\n",
      "30: Encoding Loss 16.684492111206055, Transition Loss 3.6881401538848877, Classifier Loss 0.31536322832107544, Total Loss 165.74989318847656\n",
      "30: Encoding Loss 16.00397491455078, Transition Loss 3.5852084159851074, Classifier Loss 0.3481225371360779, Total Loss 163.56109619140625\n",
      "30: Encoding Loss 16.383092880249023, Transition Loss 2.7554726600646973, Classifier Loss 0.32388243079185486, Total Loss 164.00408935546875\n",
      "30: Encoding Loss 16.393217086791992, Transition Loss 2.523810863494873, Classifier Loss 0.2806892991065979, Total Loss 159.71942138671875\n",
      "30: Encoding Loss 16.458688735961914, Transition Loss 2.173945665359497, Classifier Loss 0.2912822663784027, Total Loss 161.23251342773438\n",
      "30: Encoding Loss 17.794456481933594, Transition Loss 2.7633628845214844, Classifier Loss 0.31967923045158386, Total Loss 174.87625122070312\n",
      "30: Encoding Loss 17.216989517211914, Transition Loss 3.535956382751465, Classifier Loss 0.3264521360397339, Total Loss 171.08831787109375\n",
      "30: Encoding Loss 16.645824432373047, Transition Loss 3.8259170055389404, Classifier Loss 0.34269076585769653, Total Loss 168.20086669921875\n",
      "30: Encoding Loss 17.555089950561523, Transition Loss 3.366450309753418, Classifier Loss 0.32286345958709717, Total Loss 173.40036010742188\n",
      "30: Encoding Loss 16.98434829711914, Transition Loss 3.2098793983459473, Classifier Loss 0.2937401533126831, Total Loss 165.8907928466797\n",
      "30: Encoding Loss 17.017414093017578, Transition Loss 3.507493495941162, Classifier Loss 0.2956386208534241, Total Loss 166.4046630859375\n",
      "30: Encoding Loss 17.126354217529297, Transition Loss 2.4899885654449463, Classifier Loss 0.28443974256515503, Total Loss 165.95281982421875\n",
      "30: Encoding Loss 16.944679260253906, Transition Loss 2.7669382095336914, Classifier Loss 0.2956523597240448, Total Loss 165.67605590820312\n",
      "30: Encoding Loss 16.521625518798828, Transition Loss 2.3702826499938965, Classifier Loss 0.29753926396369934, Total Loss 162.40098571777344\n",
      "30: Encoding Loss 17.14153289794922, Transition Loss 3.488474130630493, Classifier Loss 0.28133273124694824, Total Loss 165.96322631835938\n",
      "30: Encoding Loss 16.115243911743164, Transition Loss 3.542412757873535, Classifier Loss 0.32233864068984985, Total Loss 161.8643035888672\n",
      "30: Encoding Loss 15.938165664672852, Transition Loss 3.1618638038635254, Classifier Loss 0.32183945178985596, Total Loss 160.32164001464844\n",
      "30: Encoding Loss 16.69879722595215, Transition Loss 2.7977781295776367, Classifier Loss 0.2904508113861084, Total Loss 163.1950225830078\n",
      "30: Encoding Loss 16.133270263671875, Transition Loss 3.099167823791504, Classifier Loss 0.27308762073516846, Total Loss 156.9947509765625\n",
      "30: Encoding Loss 16.14720344543457, Transition Loss 2.677212953567505, Classifier Loss 0.2980872392654419, Total Loss 159.5218048095703\n",
      "30: Encoding Loss 16.33076286315918, Transition Loss 2.848637104034424, Classifier Loss 0.2832915782928467, Total Loss 159.5449981689453\n",
      "30: Encoding Loss 17.59959602355957, Transition Loss 3.3331730365753174, Classifier Loss 0.32289761304855347, Total Loss 173.753173828125\n",
      "30: Encoding Loss 18.162750244140625, Transition Loss 3.1738598346710205, Classifier Loss 0.2988869547843933, Total Loss 175.82546997070312\n",
      "30: Encoding Loss 16.632404327392578, Transition Loss 2.641096591949463, Classifier Loss 0.30246227979660034, Total Loss 163.83367919921875\n",
      "30: Encoding Loss 16.441001892089844, Transition Loss 2.6891672611236572, Classifier Loss 0.2941352128982544, Total Loss 161.47935485839844\n",
      "30: Encoding Loss 16.28921127319336, Transition Loss 3.1515581607818604, Classifier Loss 0.27051499485969543, Total Loss 157.99551391601562\n",
      "30: Encoding Loss 15.917540550231934, Transition Loss 2.9837260246276855, Classifier Loss 0.3130926489830017, Total Loss 159.246337890625\n",
      "30: Encoding Loss 16.717283248901367, Transition Loss 2.902601480484009, Classifier Loss 0.29476791620254517, Total Loss 163.7955780029297\n",
      "30: Encoding Loss 17.588146209716797, Transition Loss 2.9474070072174072, Classifier Loss 0.25551581382751465, Total Loss 166.84625244140625\n",
      "30: Encoding Loss 17.119117736816406, Transition Loss 3.5154201984405518, Classifier Loss 0.32764971256256104, Total Loss 170.42098999023438\n",
      "30: Encoding Loss 16.5644474029541, Transition Loss 3.2244760990142822, Classifier Loss 0.3334806561470032, Total Loss 166.508544921875\n",
      "30: Encoding Loss 17.424610137939453, Transition Loss 2.951178789138794, Classifier Loss 0.2997872233390808, Total Loss 169.96585083007812\n",
      "30: Encoding Loss 16.37941551208496, Transition Loss 3.4917538166046143, Classifier Loss 0.3009628355503082, Total Loss 161.8299560546875\n",
      "30: Encoding Loss 16.391836166381836, Transition Loss 3.258232831954956, Classifier Loss 0.2866849899291992, Total Loss 160.454833984375\n",
      "30: Encoding Loss 17.238590240478516, Transition Loss 3.5153560638427734, Classifier Loss 0.2811413109302521, Total Loss 166.72592163085938\n",
      "30: Encoding Loss 15.790502548217773, Transition Loss 4.124717712402344, Classifier Loss 0.35448959469795227, Total Loss 162.59793090820312\n",
      "30: Encoding Loss 17.058929443359375, Transition Loss 3.3025999069213867, Classifier Loss 0.271459698677063, Total Loss 164.27792358398438\n",
      "30: Encoding Loss 16.50670623779297, Transition Loss 3.6977269649505615, Classifier Loss 0.3065449297428131, Total Loss 163.44769287109375\n",
      "30: Encoding Loss 16.92696762084961, Transition Loss 3.8973097801208496, Classifier Loss 0.32824254035949707, Total Loss 169.01947021484375\n",
      "30: Encoding Loss 16.122142791748047, Transition Loss 3.4602506160736084, Classifier Loss 0.27181923389434814, Total Loss 156.85113525390625\n",
      "30: Encoding Loss 17.722450256347656, Transition Loss 3.2696337699890137, Classifier Loss 0.2810056805610657, Total Loss 170.5341033935547\n",
      "30: Encoding Loss 16.35325813293457, Transition Loss 3.050497055053711, Classifier Loss 0.3094249963760376, Total Loss 162.378662109375\n",
      "30: Encoding Loss 17.174821853637695, Transition Loss 3.462073802947998, Classifier Loss 0.3179238736629486, Total Loss 169.8833770751953\n",
      "30: Encoding Loss 16.642961502075195, Transition Loss 3.2978951930999756, Classifier Loss 0.31152766942977905, Total Loss 164.95603942871094\n",
      "30: Encoding Loss 16.794261932373047, Transition Loss 3.8586175441741943, Classifier Loss 0.3093021810054779, Total Loss 166.05606079101562\n",
      "30: Encoding Loss 17.51724624633789, Transition Loss 2.9944281578063965, Classifier Loss 0.2999807596206665, Total Loss 170.7349395751953\n",
      "30: Encoding Loss 16.033239364624023, Transition Loss 3.2294015884399414, Classifier Loss 0.2701685428619385, Total Loss 155.92864990234375\n",
      "30: Encoding Loss 16.714275360107422, Transition Loss 2.8166322708129883, Classifier Loss 0.29616042971611023, Total Loss 163.89358520507812\n",
      "30: Encoding Loss 16.62656021118164, Transition Loss 4.095296859741211, Classifier Loss 0.329181045293808, Total Loss 166.74964904785156\n",
      "30: Encoding Loss 16.308414459228516, Transition Loss 2.9531660079956055, Classifier Loss 0.3087064027786255, Total Loss 161.9285888671875\n",
      "30: Encoding Loss 16.72726821899414, Transition Loss 2.8398349285125732, Classifier Loss 0.30373790860176086, Total Loss 164.75990295410156\n",
      "30: Encoding Loss 16.95840072631836, Transition Loss 3.0522093772888184, Classifier Loss 0.30106788873672485, Total Loss 166.38446044921875\n",
      "30: Encoding Loss 16.9449520111084, Transition Loss 3.913439989089966, Classifier Loss 0.37569311261177063, Total Loss 173.91162109375\n",
      "30: Encoding Loss 16.744808197021484, Transition Loss 3.4603328704833984, Classifier Loss 0.2845824658870697, Total Loss 163.10879516601562\n",
      "30: Encoding Loss 17.637409210205078, Transition Loss 3.009082317352295, Classifier Loss 0.27228277921676636, Total Loss 168.9293670654297\n",
      "30: Encoding Loss 17.212778091430664, Transition Loss 3.2424681186676025, Classifier Loss 0.2751397490501404, Total Loss 165.8647003173828\n",
      "30: Encoding Loss 16.15353775024414, Transition Loss 2.5582141876220703, Classifier Loss 0.2775041162967682, Total Loss 157.4903564453125\n",
      "30: Encoding Loss 16.82288360595703, Transition Loss 3.5334458351135254, Classifier Loss 0.3241389989852905, Total Loss 167.7036590576172\n",
      "30: Encoding Loss 16.861141204833984, Transition Loss 2.8510828018188477, Classifier Loss 0.30558645725250244, Total Loss 166.01800537109375\n",
      "30: Encoding Loss 16.23406219482422, Transition Loss 3.8913967609405518, Classifier Loss 0.3277294635772705, Total Loss 163.42372131347656\n",
      "30: Encoding Loss 17.39797592163086, Transition Loss 2.599916696548462, Classifier Loss 0.2895409166812897, Total Loss 168.65789794921875\n",
      "30: Encoding Loss 16.889127731323242, Transition Loss 3.0051827430725098, Classifier Loss 0.304714173078537, Total Loss 166.18548583984375\n",
      "30: Encoding Loss 17.240053176879883, Transition Loss 2.920764446258545, Classifier Loss 0.34641578793525696, Total Loss 173.1461639404297\n",
      "30: Encoding Loss 16.410051345825195, Transition Loss 4.044519901275635, Classifier Loss 0.3062804937362671, Total Loss 162.7173614501953\n",
      "30: Encoding Loss 16.279178619384766, Transition Loss 3.614300012588501, Classifier Loss 0.31019166111946106, Total Loss 161.97544860839844\n",
      "30: Encoding Loss 16.390779495239258, Transition Loss 3.4270360469818115, Classifier Loss 0.29214712977409363, Total Loss 161.02635192871094\n",
      "30: Encoding Loss 15.398810386657715, Transition Loss 4.304518222808838, Classifier Loss 0.29092538356781006, Total Loss 153.1439208984375\n",
      "30: Encoding Loss 16.697084426879883, Transition Loss 3.2928924560546875, Classifier Loss 0.2835199236869812, Total Loss 162.58724975585938\n",
      "30: Encoding Loss 16.46226692199707, Transition Loss 3.567152976989746, Classifier Loss 0.27942976355552673, Total Loss 160.3545379638672\n",
      "30: Encoding Loss 15.170281410217285, Transition Loss 3.1828014850616455, Classifier Loss 0.3113642930984497, Total Loss 153.13525390625\n",
      "30: Encoding Loss 16.352537155151367, Transition Loss 3.608506202697754, Classifier Loss 0.29714226722717285, Total Loss 161.25621032714844\n",
      "30: Encoding Loss 16.749570846557617, Transition Loss 3.4926793575286865, Classifier Loss 0.32739365100860596, Total Loss 167.43446350097656\n",
      "30: Encoding Loss 17.243776321411133, Transition Loss 3.5573806762695312, Classifier Loss 0.287260502576828, Total Loss 167.38772583007812\n",
      "30: Encoding Loss 16.440244674682617, Transition Loss 3.321969747543335, Classifier Loss 0.3090605139732361, Total Loss 163.0924072265625\n",
      "30: Encoding Loss 16.924762725830078, Transition Loss 2.8559837341308594, Classifier Loss 0.29089224338531494, Total Loss 165.0585174560547\n",
      "30: Encoding Loss 16.24747085571289, Transition Loss 2.71101713180542, Classifier Loss 0.3103944659233093, Total Loss 161.56141662597656\n",
      "30: Encoding Loss 17.22385597229004, Transition Loss 3.7637369632720947, Classifier Loss 0.30394798517227173, Total Loss 168.93838500976562\n",
      "30: Encoding Loss 17.18667984008789, Transition Loss 3.3122406005859375, Classifier Loss 0.3115738034248352, Total Loss 169.31326293945312\n",
      "30: Encoding Loss 16.22896957397461, Transition Loss 3.202037811279297, Classifier Loss 0.32010236382484436, Total Loss 162.482421875\n",
      "30: Encoding Loss 16.589691162109375, Transition Loss 3.3911304473876953, Classifier Loss 0.29837778210639954, Total Loss 163.23353576660156\n",
      "30: Encoding Loss 16.751489639282227, Transition Loss 3.08740234375, Classifier Loss 0.302460640668869, Total Loss 164.87545776367188\n",
      "30: Encoding Loss 16.797657012939453, Transition Loss 2.397150754928589, Classifier Loss 0.2783995568752289, Total Loss 162.7006378173828\n",
      "30: Encoding Loss 16.574546813964844, Transition Loss 2.6627445220947266, Classifier Loss 0.2797769606113434, Total Loss 161.1066131591797\n",
      "30: Encoding Loss 16.961795806884766, Transition Loss 3.6336429119110107, Classifier Loss 0.3203882575035095, Total Loss 168.4599151611328\n",
      "30: Encoding Loss 17.49529457092285, Transition Loss 2.5931191444396973, Classifier Loss 0.30396923422813416, Total Loss 170.87791442871094\n",
      "30: Encoding Loss 17.68647003173828, Transition Loss 2.6869914531707764, Classifier Loss 0.3157625198364258, Total Loss 173.60540771484375\n",
      "30: Encoding Loss 15.803834915161133, Transition Loss 3.5434913635253906, Classifier Loss 0.27028322219848633, Total Loss 154.16769409179688\n",
      "30: Encoding Loss 15.891962051391602, Transition Loss 3.797523021697998, Classifier Loss 0.3182227909564972, Total Loss 159.7174835205078\n",
      "30: Encoding Loss 16.030956268310547, Transition Loss 3.381319761276245, Classifier Loss 0.34147417545318604, Total Loss 163.07135009765625\n",
      "30: Encoding Loss 16.29507064819336, Transition Loss 3.7377874851226807, Classifier Loss 0.3229612708091736, Total Loss 163.40426635742188\n",
      "30: Encoding Loss 15.90499496459961, Transition Loss 4.6528825759887695, Classifier Loss 0.31458351016044617, Total Loss 159.62889099121094\n",
      "30: Encoding Loss 17.29851531982422, Transition Loss 3.269045114517212, Classifier Loss 0.2836768627166748, Total Loss 167.4096221923828\n",
      "30: Encoding Loss 17.261016845703125, Transition Loss 3.0804038047790527, Classifier Loss 0.2777866721153259, Total Loss 166.48287963867188\n",
      "30: Encoding Loss 15.9504976272583, Transition Loss 3.03290057182312, Classifier Loss 0.29757237434387207, Total Loss 157.96780395507812\n",
      "30: Encoding Loss 16.317224502563477, Transition Loss 2.8114185333251953, Classifier Loss 0.2965940833091736, Total Loss 160.75949096679688\n",
      "30: Encoding Loss 16.34133529663086, Transition Loss 3.1994097232818604, Classifier Loss 0.2970239520072937, Total Loss 161.07296752929688\n",
      "30: Encoding Loss 16.599613189697266, Transition Loss 3.3240373134613037, Classifier Loss 0.2881903648376465, Total Loss 162.28074645996094\n",
      "30: Encoding Loss 16.71963119506836, Transition Loss 2.8862152099609375, Classifier Loss 0.2847711145877838, Total Loss 162.81141662597656\n",
      "30: Encoding Loss 16.573129653930664, Transition Loss 3.003326654434204, Classifier Loss 0.3098057508468628, Total Loss 164.16627502441406\n",
      "30: Encoding Loss 15.944148063659668, Transition Loss 2.81974458694458, Classifier Loss 0.2808631360530853, Total Loss 156.2034454345703\n",
      "30: Encoding Loss 16.203611373901367, Transition Loss 3.357145071029663, Classifier Loss 0.25016236305236816, Total Loss 155.31655883789062\n",
      "30: Encoding Loss 17.221986770629883, Transition Loss 2.856553316116333, Classifier Loss 0.3331410586833954, Total Loss 171.6613006591797\n",
      "30: Encoding Loss 15.991289138793945, Transition Loss 3.9300789833068848, Classifier Loss 0.31376150250434875, Total Loss 160.09246826171875\n",
      "30: Encoding Loss 16.903554916381836, Transition Loss 3.1726090908050537, Classifier Loss 0.30258941650390625, Total Loss 166.1219024658203\n",
      "30: Encoding Loss 16.825326919555664, Transition Loss 3.8104143142700195, Classifier Loss 0.29976266622543335, Total Loss 165.34097290039062\n",
      "30: Encoding Loss 16.75604248046875, Transition Loss 3.274280309677124, Classifier Loss 0.29138994216918945, Total Loss 163.84219360351562\n",
      "30: Encoding Loss 16.046342849731445, Transition Loss 3.537781238555908, Classifier Loss 0.27438807487487793, Total Loss 156.51710510253906\n",
      "30: Encoding Loss 17.746580123901367, Transition Loss 2.53265643119812, Classifier Loss 0.2941952347755432, Total Loss 171.89869689941406\n",
      "30: Encoding Loss 16.853694915771484, Transition Loss 2.42659854888916, Classifier Loss 0.2958656847476959, Total Loss 164.90145874023438\n",
      "30: Encoding Loss 16.461545944213867, Transition Loss 2.910609006881714, Classifier Loss 0.29440227150917053, Total Loss 161.7147216796875\n",
      "30: Encoding Loss 15.886530876159668, Transition Loss 3.3349881172180176, Classifier Loss 0.29830101132392883, Total Loss 157.58934020996094\n",
      "30: Encoding Loss 16.197317123413086, Transition Loss 2.288292407989502, Classifier Loss 0.2913288474082947, Total Loss 159.16908264160156\n",
      "30: Encoding Loss 16.762989044189453, Transition Loss 4.1349968910217285, Classifier Loss 0.2954334020614624, Total Loss 164.4742431640625\n",
      "30: Encoding Loss 16.979970932006836, Transition Loss 2.8930633068084717, Classifier Loss 0.2950100302696228, Total Loss 165.9193878173828\n",
      "30: Encoding Loss 17.87396240234375, Transition Loss 3.424405097961426, Classifier Loss 0.28410840034484863, Total Loss 172.08741760253906\n",
      "30: Encoding Loss 17.341827392578125, Transition Loss 3.781157970428467, Classifier Loss 0.296542227268219, Total Loss 169.1450653076172\n",
      "30: Encoding Loss 16.11534881591797, Transition Loss 4.407523155212402, Classifier Loss 0.2951579689979553, Total Loss 159.32008361816406\n",
      "30: Encoding Loss 14.55059814453125, Transition Loss 4.9771528244018555, Classifier Loss 0.3084981441497803, Total Loss 148.2500457763672\n",
      "31: Encoding Loss 17.499494552612305, Transition Loss 3.0289039611816406, Classifier Loss 0.28438395261764526, Total Loss 169.04013061523438\n",
      "31: Encoding Loss 17.01569366455078, Transition Loss 3.162304401397705, Classifier Loss 0.2953488230705261, Total Loss 166.2928924560547\n",
      "31: Encoding Loss 17.229780197143555, Transition Loss 3.181346893310547, Classifier Loss 0.3045409619808197, Total Loss 168.92861938476562\n",
      "31: Encoding Loss 16.6954288482666, Transition Loss 3.4070396423339844, Classifier Loss 0.30967462062835693, Total Loss 165.21231079101562\n",
      "31: Encoding Loss 18.0108699798584, Transition Loss 3.1220898628234863, Classifier Loss 0.28757959604263306, Total Loss 173.46934509277344\n",
      "31: Encoding Loss 15.164519309997559, Transition Loss 3.6491308212280273, Classifier Loss 0.3210831880569458, Total Loss 154.154296875\n",
      "31: Encoding Loss 17.518451690673828, Transition Loss 3.0011184215545654, Classifier Loss 0.3089691400527954, Total Loss 171.64474487304688\n",
      "31: Encoding Loss 18.11977767944336, Transition Loss 3.2372608184814453, Classifier Loss 0.30740147829055786, Total Loss 176.3458251953125\n",
      "31: Encoding Loss 16.58367156982422, Transition Loss 3.2843549251556396, Classifier Loss 0.39499425888061523, Total Loss 172.82566833496094\n",
      "31: Encoding Loss 17.723447799682617, Transition Loss 3.229645252227783, Classifier Loss 0.3266114592552185, Total Loss 175.09466552734375\n",
      "31: Encoding Loss 17.007043838500977, Transition Loss 2.9857773780822754, Classifier Loss 0.2810656428337097, Total Loss 164.76007080078125\n",
      "31: Encoding Loss 16.72310447692871, Transition Loss 2.9977316856384277, Classifier Loss 0.3310789167881012, Total Loss 167.49227905273438\n",
      "31: Encoding Loss 16.375173568725586, Transition Loss 3.3191545009613037, Classifier Loss 0.31144484877586365, Total Loss 162.80970764160156\n",
      "31: Encoding Loss 16.17875099182129, Transition Loss 2.9713306427001953, Classifier Loss 0.28374814987182617, Total Loss 158.3990936279297\n",
      "31: Encoding Loss 16.12474822998047, Transition Loss 3.5548884868621826, Classifier Loss 0.30245348811149597, Total Loss 159.95431518554688\n",
      "31: Encoding Loss 16.632177352905273, Transition Loss 3.6850223541259766, Classifier Loss 0.30428552627563477, Total Loss 164.2229766845703\n",
      "31: Encoding Loss 16.6366024017334, Transition Loss 3.4989805221557617, Classifier Loss 0.3024146556854248, Total Loss 164.03408813476562\n",
      "31: Encoding Loss 16.246307373046875, Transition Loss 3.306161403656006, Classifier Loss 0.2627370059490204, Total Loss 156.9053955078125\n",
      "31: Encoding Loss 16.95999526977539, Transition Loss 3.8474631309509277, Classifier Loss 0.26674169301986694, Total Loss 163.1236114501953\n",
      "31: Encoding Loss 17.048389434814453, Transition Loss 3.2781999111175537, Classifier Loss 0.2931925356388092, Total Loss 166.3620147705078\n",
      "31: Encoding Loss 16.45049476623535, Transition Loss 4.262404441833496, Classifier Loss 0.31895989179611206, Total Loss 164.3524169921875\n",
      "31: Encoding Loss 16.447643280029297, Transition Loss 2.880509853363037, Classifier Loss 0.292779803276062, Total Loss 161.43524169921875\n",
      "31: Encoding Loss 15.284268379211426, Transition Loss 4.739914417266846, Classifier Loss 0.3041030764579773, Total Loss 153.63243103027344\n",
      "31: Encoding Loss 16.210369110107422, Transition Loss 2.925386428833008, Classifier Loss 0.3237684965133667, Total Loss 162.6448974609375\n",
      "31: Encoding Loss 16.41023063659668, Transition Loss 2.465907573699951, Classifier Loss 0.27686792612075806, Total Loss 159.46182250976562\n",
      "31: Encoding Loss 16.48230743408203, Transition Loss 3.1824662685394287, Classifier Loss 0.2940119802951813, Total Loss 161.89614868164062\n",
      "31: Encoding Loss 16.830446243286133, Transition Loss 3.657292604446411, Classifier Loss 0.3056250810623169, Total Loss 165.9375457763672\n",
      "31: Encoding Loss 16.685361862182617, Transition Loss 3.830580711364746, Classifier Loss 0.2810842990875244, Total Loss 162.35743713378906\n",
      "31: Encoding Loss 16.5494441986084, Transition Loss 3.8253259658813477, Classifier Loss 0.3133392930030823, Total Loss 164.49453735351562\n",
      "31: Encoding Loss 15.691495895385742, Transition Loss 4.069594383239746, Classifier Loss 0.32281792163848877, Total Loss 158.627685546875\n",
      "31: Encoding Loss 16.69364356994629, Transition Loss 3.6012649536132812, Classifier Loss 0.28092485666275024, Total Loss 162.36187744140625\n",
      "31: Encoding Loss 15.4884672164917, Transition Loss 4.359612464904785, Classifier Loss 0.27293068170547485, Total Loss 152.07272338867188\n",
      "31: Encoding Loss 16.770532608032227, Transition Loss 3.907912254333496, Classifier Loss 0.31329792737960815, Total Loss 166.275634765625\n",
      "31: Encoding Loss 16.053546905517578, Transition Loss 2.2843782901763916, Classifier Loss 0.31449541449546814, Total Loss 160.3347930908203\n",
      "31: Encoding Loss 15.640061378479004, Transition Loss 3.6544995307922363, Classifier Loss 0.29860416054725647, Total Loss 155.71180725097656\n",
      "31: Encoding Loss 15.682517051696777, Transition Loss 2.5979552268981934, Classifier Loss 0.27675607800483704, Total Loss 153.65533447265625\n",
      "31: Encoding Loss 16.52838706970215, Transition Loss 3.750582456588745, Classifier Loss 0.30368858575820923, Total Loss 163.3460693359375\n",
      "31: Encoding Loss 16.193098068237305, Transition Loss 4.790923595428467, Classifier Loss 0.3238757252693176, Total Loss 162.8905487060547\n",
      "31: Encoding Loss 17.1378173828125, Transition Loss 3.3974082469940186, Classifier Loss 0.28677117824554443, Total Loss 166.4591522216797\n",
      "31: Encoding Loss 16.398271560668945, Transition Loss 4.000047206878662, Classifier Loss 0.2873755693435669, Total Loss 160.72373962402344\n",
      "31: Encoding Loss 15.628173828125, Transition Loss 4.020184516906738, Classifier Loss 0.30705371499061584, Total Loss 156.5347900390625\n",
      "31: Encoding Loss 16.318449020385742, Transition Loss 2.979973793029785, Classifier Loss 0.29801928997039795, Total Loss 160.9455108642578\n",
      "31: Encoding Loss 16.076231002807617, Transition Loss 2.786130428314209, Classifier Loss 0.29356658458709717, Total Loss 158.5237274169922\n",
      "31: Encoding Loss 16.330869674682617, Transition Loss 2.2503514289855957, Classifier Loss 0.2910311818122864, Total Loss 160.2001495361328\n",
      "31: Encoding Loss 17.585371017456055, Transition Loss 3.0877716541290283, Classifier Loss 0.282316654920578, Total Loss 169.5321807861328\n",
      "31: Encoding Loss 17.01642608642578, Transition Loss 3.9270637035369873, Classifier Loss 0.3147815763950348, Total Loss 168.3949737548828\n",
      "31: Encoding Loss 16.17515754699707, Transition Loss 4.4531779289245605, Classifier Loss 0.30886924266815186, Total Loss 161.17881774902344\n",
      "31: Encoding Loss 17.47211456298828, Transition Loss 3.756964921951294, Classifier Loss 0.2986636161804199, Total Loss 170.39466857910156\n",
      "31: Encoding Loss 16.723003387451172, Transition Loss 3.655529260635376, Classifier Loss 0.28225576877593994, Total Loss 162.74072265625\n",
      "31: Encoding Loss 16.771791458129883, Transition Loss 3.8214259147644043, Classifier Loss 0.25271695852279663, Total Loss 160.21031188964844\n",
      "31: Encoding Loss 17.168996810913086, Transition Loss 2.7850611209869385, Classifier Loss 0.28933778405189514, Total Loss 166.84275817871094\n",
      "31: Encoding Loss 16.87132453918457, Transition Loss 2.9616217613220215, Classifier Loss 0.26300719380378723, Total Loss 161.8636474609375\n",
      "31: Encoding Loss 16.517269134521484, Transition Loss 2.57175350189209, Classifier Loss 0.26833218336105347, Total Loss 159.48573303222656\n",
      "31: Encoding Loss 16.958812713623047, Transition Loss 3.8593063354492188, Classifier Loss 0.3001381754875183, Total Loss 166.45619201660156\n",
      "31: Encoding Loss 15.833179473876953, Transition Loss 3.928023338317871, Classifier Loss 0.27008968591690063, Total Loss 154.45999145507812\n",
      "31: Encoding Loss 15.804014205932617, Transition Loss 3.352574348449707, Classifier Loss 0.2994123101234436, Total Loss 157.0438690185547\n",
      "31: Encoding Loss 16.271127700805664, Transition Loss 3.120807647705078, Classifier Loss 0.25503188371658325, Total Loss 156.29637145996094\n",
      "31: Encoding Loss 15.836188316345215, Transition Loss 3.256877899169922, Classifier Loss 0.26703062653541565, Total Loss 154.0439453125\n",
      "31: Encoding Loss 15.809417724609375, Transition Loss 2.942953109741211, Classifier Loss 0.3076038062572479, Total Loss 157.82431030273438\n",
      "31: Encoding Loss 16.309675216674805, Transition Loss 3.0011372566223145, Classifier Loss 0.2722988724708557, Total Loss 158.30752563476562\n",
      "31: Encoding Loss 17.484567642211914, Transition Loss 3.7173402309417725, Classifier Loss 0.3037630021572113, Total Loss 170.99630737304688\n",
      "31: Encoding Loss 17.818464279174805, Transition Loss 3.4509098529815674, Classifier Loss 0.286172091960907, Total Loss 171.8551025390625\n",
      "31: Encoding Loss 16.239492416381836, Transition Loss 2.766075611114502, Classifier Loss 0.3075616657733917, Total Loss 161.22532653808594\n",
      "31: Encoding Loss 16.10965347290039, Transition Loss 2.8520257472991943, Classifier Loss 0.27952975034713745, Total Loss 157.40060424804688\n",
      "31: Encoding Loss 16.171274185180664, Transition Loss 3.5953712463378906, Classifier Loss 0.28508567810058594, Total Loss 158.5978240966797\n",
      "31: Encoding Loss 15.61938190460205, Transition Loss 3.321305751800537, Classifier Loss 0.2789537310600281, Total Loss 153.5146942138672\n",
      "31: Encoding Loss 16.77830696105957, Transition Loss 3.2607574462890625, Classifier Loss 0.2831226587295532, Total Loss 163.1908721923828\n",
      "31: Encoding Loss 17.313779830932617, Transition Loss 3.288212776184082, Classifier Loss 0.2596094608306885, Total Loss 165.1288299560547\n",
      "31: Encoding Loss 16.497411727905273, Transition Loss 4.020675182342529, Classifier Loss 0.305637925863266, Total Loss 163.34722900390625\n",
      "31: Encoding Loss 16.259395599365234, Transition Loss 3.497269630432129, Classifier Loss 0.3121410310268402, Total Loss 161.9887237548828\n",
      "31: Encoding Loss 17.33997917175293, Transition Loss 3.317708969116211, Classifier Loss 0.26950687170028687, Total Loss 166.3340606689453\n",
      "31: Encoding Loss 16.247285842895508, Transition Loss 3.847893714904785, Classifier Loss 0.29331323504447937, Total Loss 160.07919311523438\n",
      "31: Encoding Loss 16.35736083984375, Transition Loss 3.7535150051116943, Classifier Loss 0.25939249992370605, Total Loss 157.54884338378906\n",
      "31: Encoding Loss 17.135040283203125, Transition Loss 3.9884297847747803, Classifier Loss 0.2897097170352936, Total Loss 166.84898376464844\n",
      "31: Encoding Loss 15.449543952941895, Transition Loss 4.684206008911133, Classifier Loss 0.34458115696907043, Total Loss 158.99131774902344\n",
      "31: Encoding Loss 16.556196212768555, Transition Loss 3.63376522064209, Classifier Loss 0.2979336380958557, Total Loss 162.9696807861328\n",
      "31: Encoding Loss 16.151103973388672, Transition Loss 4.192437171936035, Classifier Loss 0.28339633345603943, Total Loss 158.386962890625\n",
      "31: Encoding Loss 16.423688888549805, Transition Loss 4.186303615570068, Classifier Loss 0.288777619600296, Total Loss 161.1045379638672\n",
      "31: Encoding Loss 15.943218231201172, Transition Loss 3.8439464569091797, Classifier Loss 0.2843286395072937, Total Loss 156.74740600585938\n",
      "31: Encoding Loss 17.450586318969727, Transition Loss 3.5770325660705566, Classifier Loss 0.2721790373325348, Total Loss 167.5380096435547\n",
      "31: Encoding Loss 16.14138412475586, Transition Loss 3.372544050216675, Classifier Loss 0.26895448565483093, Total Loss 156.7010498046875\n",
      "31: Encoding Loss 16.767351150512695, Transition Loss 3.797668218612671, Classifier Loss 0.29612329602241516, Total Loss 164.51068115234375\n",
      "31: Encoding Loss 16.48363494873047, Transition Loss 3.591270923614502, Classifier Loss 0.2986490726470947, Total Loss 162.45223999023438\n",
      "31: Encoding Loss 16.525711059570312, Transition Loss 4.241235256195068, Classifier Loss 0.27986910939216614, Total Loss 161.0408477783203\n",
      "31: Encoding Loss 17.177186965942383, Transition Loss 3.2565202713012695, Classifier Loss 0.2885996401309967, Total Loss 166.92877197265625\n",
      "31: Encoding Loss 15.90818977355957, Transition Loss 3.7038626670837402, Classifier Loss 0.25356775522232056, Total Loss 153.36306762695312\n",
      "31: Encoding Loss 16.38231658935547, Transition Loss 3.0790414810180664, Classifier Loss 0.29977095127105713, Total Loss 161.6514434814453\n",
      "31: Encoding Loss 16.26093101501465, Transition Loss 4.692669868469238, Classifier Loss 0.32742199301719666, Total Loss 163.7681884765625\n",
      "31: Encoding Loss 16.039716720581055, Transition Loss 3.2271344661712646, Classifier Loss 0.2788577377796173, Total Loss 156.84893798828125\n",
      "31: Encoding Loss 16.51587677001953, Transition Loss 2.9916985034942627, Classifier Loss 0.2785147428512573, Total Loss 160.5768280029297\n",
      "31: Encoding Loss 16.834156036376953, Transition Loss 3.37064266204834, Classifier Loss 0.24860884249210358, Total Loss 160.20826721191406\n",
      "31: Encoding Loss 16.528047561645508, Transition Loss 4.362758636474609, Classifier Loss 0.3631736636161804, Total Loss 169.414306640625\n",
      "31: Encoding Loss 16.556955337524414, Transition Loss 3.8437557220458984, Classifier Loss 0.2761665880680084, Total Loss 160.84104919433594\n",
      "31: Encoding Loss 17.3264102935791, Transition Loss 3.3529181480407715, Classifier Loss 0.25666677951812744, Total Loss 164.9485321044922\n",
      "31: Encoding Loss 17.05368423461914, Transition Loss 3.5869250297546387, Classifier Loss 0.2826971411705017, Total Loss 165.4165802001953\n",
      "31: Encoding Loss 16.025671005249023, Transition Loss 2.71718692779541, Classifier Loss 0.26457029581069946, Total Loss 155.20584106445312\n",
      "31: Encoding Loss 16.502437591552734, Transition Loss 4.0025739669799805, Classifier Loss 0.30829527974128723, Total Loss 163.64956665039062\n",
      "31: Encoding Loss 16.68076515197754, Transition Loss 3.146655559539795, Classifier Loss 0.29581746459007263, Total Loss 163.65719604492188\n",
      "31: Encoding Loss 16.049320220947266, Transition Loss 4.431840896606445, Classifier Loss 0.30220258235931396, Total Loss 159.50119018554688\n",
      "31: Encoding Loss 17.045942306518555, Transition Loss 2.8265318870544434, Classifier Loss 0.28619420528411865, Total Loss 165.55226135253906\n",
      "31: Encoding Loss 16.568937301635742, Transition Loss 3.3810648918151855, Classifier Loss 0.30213961005210876, Total Loss 163.44166564941406\n",
      "31: Encoding Loss 17.016969680786133, Transition Loss 3.394077777862549, Classifier Loss 0.3286741077899933, Total Loss 169.68199157714844\n",
      "31: Encoding Loss 16.216222763061523, Transition Loss 4.575263977050781, Classifier Loss 0.2855745553970337, Total Loss 159.20228576660156\n",
      "31: Encoding Loss 15.803235054016113, Transition Loss 4.02517032623291, Classifier Loss 0.28856122493743896, Total Loss 156.0870361328125\n",
      "31: Encoding Loss 16.193456649780273, Transition Loss 3.9170873165130615, Classifier Loss 0.28560248017311096, Total Loss 158.8913116455078\n",
      "31: Encoding Loss 15.222746849060059, Transition Loss 5.112757205963135, Classifier Loss 0.26424553990364075, Total Loss 149.2290802001953\n",
      "31: Encoding Loss 16.544208526611328, Transition Loss 3.7741663455963135, Classifier Loss 0.2715868651866913, Total Loss 160.26719665527344\n",
      "31: Encoding Loss 16.30160903930664, Transition Loss 4.183105945587158, Classifier Loss 0.2881486117839813, Total Loss 160.06436157226562\n",
      "31: Encoding Loss 14.896794319152832, Transition Loss 3.5950398445129395, Classifier Loss 0.27915191650390625, Total Loss 147.8085479736328\n",
      "31: Encoding Loss 15.88054084777832, Transition Loss 4.293213844299316, Classifier Loss 0.27974680066108704, Total Loss 155.87765502929688\n",
      "31: Encoding Loss 16.41314125061035, Transition Loss 4.02207088470459, Classifier Loss 0.26890796422958374, Total Loss 159.00033569335938\n",
      "31: Encoding Loss 16.959423065185547, Transition Loss 4.153082847595215, Classifier Loss 0.27183690667152405, Total Loss 163.689697265625\n",
      "31: Encoding Loss 16.15741729736328, Transition Loss 3.8030078411102295, Classifier Loss 0.30991891026496887, Total Loss 161.0118408203125\n",
      "31: Encoding Loss 16.662519454956055, Transition Loss 3.2087111473083496, Classifier Loss 0.25110912322998047, Total Loss 159.0528106689453\n",
      "31: Encoding Loss 16.05946922302246, Transition Loss 3.0635552406311035, Classifier Loss 0.326018750667572, Total Loss 161.6903533935547\n",
      "31: Encoding Loss 16.745878219604492, Transition Loss 4.3212199211120605, Classifier Loss 0.2797904908657074, Total Loss 162.81031799316406\n",
      "31: Encoding Loss 16.904956817626953, Transition Loss 3.840414524078369, Classifier Loss 0.2767234444618225, Total Loss 163.68008422851562\n",
      "31: Encoding Loss 16.11374855041504, Transition Loss 3.672476291656494, Classifier Loss 0.28316569328308105, Total Loss 157.9610595703125\n",
      "31: Encoding Loss 16.225261688232422, Transition Loss 3.8068549633026123, Classifier Loss 0.26653027534484863, Total Loss 157.2165069580078\n",
      "31: Encoding Loss 16.653446197509766, Transition Loss 3.452693462371826, Classifier Loss 0.2870945930480957, Total Loss 162.6275634765625\n",
      "31: Encoding Loss 16.88090705871582, Transition Loss 2.6447017192840576, Classifier Loss 0.2790045738220215, Total Loss 163.47665405273438\n",
      "31: Encoding Loss 16.54421615600586, Transition Loss 2.9353034496307373, Classifier Loss 0.2773086130619049, Total Loss 160.6716766357422\n",
      "31: Encoding Loss 16.648462295532227, Transition Loss 4.009028434753418, Classifier Loss 0.29319852590560913, Total Loss 163.30935668945312\n",
      "31: Encoding Loss 17.44803810119629, Transition Loss 2.8045084476470947, Classifier Loss 0.2864014804363251, Total Loss 168.7853546142578\n",
      "31: Encoding Loss 17.541927337646484, Transition Loss 2.9462461471557617, Classifier Loss 0.3016994297504425, Total Loss 171.09461975097656\n",
      "31: Encoding Loss 15.685215950012207, Transition Loss 3.9074602127075195, Classifier Loss 0.27072808146476746, Total Loss 153.33602905273438\n",
      "31: Encoding Loss 15.587509155273438, Transition Loss 4.251293182373047, Classifier Loss 0.27850642800331116, Total Loss 153.40098571777344\n",
      "31: Encoding Loss 15.831867218017578, Transition Loss 3.7946274280548096, Classifier Loss 0.3307591676712036, Total Loss 160.48977661132812\n",
      "31: Encoding Loss 15.814416885375977, Transition Loss 4.271404266357422, Classifier Loss 0.29553526639938354, Total Loss 156.9231414794922\n",
      "31: Encoding Loss 15.601532936096191, Transition Loss 5.296625137329102, Classifier Loss 0.305940717458725, Total Loss 156.46566772460938\n",
      "31: Encoding Loss 17.15508270263672, Transition Loss 3.716076374053955, Classifier Loss 0.309357613325119, Total Loss 168.9196319580078\n",
      "31: Encoding Loss 17.013864517211914, Transition Loss 3.4838857650756836, Classifier Loss 0.23565030097961426, Total Loss 160.3727264404297\n",
      "31: Encoding Loss 15.680991172790527, Transition Loss 3.303624153137207, Classifier Loss 0.26259806752204895, Total Loss 152.3684539794922\n",
      "31: Encoding Loss 16.057172775268555, Transition Loss 3.07561993598938, Classifier Loss 0.29089128971099854, Total Loss 158.16163635253906\n",
      "31: Encoding Loss 16.1633358001709, Transition Loss 3.5339436531066895, Classifier Loss 0.29712674021720886, Total Loss 159.7261505126953\n",
      "31: Encoding Loss 16.51629066467285, Transition Loss 3.8547158241271973, Classifier Loss 0.28446146845817566, Total Loss 161.34742736816406\n",
      "31: Encoding Loss 16.519058227539062, Transition Loss 3.2335143089294434, Classifier Loss 0.26413100957870483, Total Loss 159.21226501464844\n",
      "31: Encoding Loss 16.47113609313965, Transition Loss 3.492882251739502, Classifier Loss 0.3076300024986267, Total Loss 163.2306671142578\n",
      "31: Encoding Loss 15.670851707458496, Transition Loss 3.0770978927612305, Classifier Loss 0.25204378366470337, Total Loss 151.18661499023438\n",
      "31: Encoding Loss 16.218481063842773, Transition Loss 3.7784810066223145, Classifier Loss 0.2709156572818756, Total Loss 157.59510803222656\n",
      "31: Encoding Loss 17.000572204589844, Transition Loss 3.0264124870300293, Classifier Loss 0.3144337236881256, Total Loss 168.05323791503906\n",
      "31: Encoding Loss 15.76685619354248, Transition Loss 4.41251802444458, Classifier Loss 0.2954850494861603, Total Loss 156.56585693359375\n",
      "31: Encoding Loss 16.501699447631836, Transition Loss 3.510910749435425, Classifier Loss 0.28665149211883545, Total Loss 161.38092041015625\n",
      "31: Encoding Loss 16.60613250732422, Transition Loss 4.264519691467285, Classifier Loss 0.31873834133148193, Total Loss 165.5758056640625\n",
      "31: Encoding Loss 16.473682403564453, Transition Loss 3.635540723800659, Classifier Loss 0.2828572392463684, Total Loss 160.8022918701172\n",
      "31: Encoding Loss 15.918854713439941, Transition Loss 3.977574348449707, Classifier Loss 0.27299922704696655, Total Loss 155.4462890625\n",
      "31: Encoding Loss 17.53182029724121, Transition Loss 2.8369784355163574, Classifier Loss 0.2893758714199066, Total Loss 169.75955200195312\n",
      "31: Encoding Loss 16.822998046875, Transition Loss 2.674260377883911, Classifier Loss 0.30200091004371643, Total Loss 165.3189239501953\n",
      "31: Encoding Loss 16.353870391845703, Transition Loss 3.118745803833008, Classifier Loss 0.27716001868247986, Total Loss 159.17071533203125\n",
      "31: Encoding Loss 15.730319023132324, Transition Loss 3.7271018028259277, Classifier Loss 0.2678414285182953, Total Loss 153.3721160888672\n",
      "31: Encoding Loss 16.020166397094727, Transition Loss 2.4534106254577637, Classifier Loss 0.28597384691238403, Total Loss 157.2493896484375\n",
      "31: Encoding Loss 16.70557403564453, Transition Loss 4.709488868713379, Classifier Loss 0.28265058994293213, Total Loss 162.85154724121094\n",
      "31: Encoding Loss 16.83267593383789, Transition Loss 3.274817943572998, Classifier Loss 0.2805960774421692, Total Loss 163.3759765625\n",
      "31: Encoding Loss 17.510042190551758, Transition Loss 3.7569069862365723, Classifier Loss 0.2706572413444519, Total Loss 167.89744567871094\n",
      "31: Encoding Loss 17.09363555908203, Transition Loss 4.2294487953186035, Classifier Loss 0.2741454541683197, Total Loss 165.009521484375\n",
      "31: Encoding Loss 15.7960844039917, Transition Loss 4.8902082443237305, Classifier Loss 0.2952493131160736, Total Loss 156.87164306640625\n",
      "31: Encoding Loss 13.863465309143066, Transition Loss 5.763106346130371, Classifier Loss 0.3154861629009247, Total Loss 143.6089630126953\n",
      "32: Encoding Loss 17.408681869506836, Transition Loss 3.2100822925567627, Classifier Loss 0.27685126662254333, Total Loss 167.5966033935547\n",
      "32: Encoding Loss 16.928218841552734, Transition Loss 3.6202518939971924, Classifier Loss 0.29482802748680115, Total Loss 165.6326141357422\n",
      "32: Encoding Loss 16.950279235839844, Transition Loss 3.356848955154419, Classifier Loss 0.28785473108291626, Total Loss 165.05908203125\n",
      "32: Encoding Loss 16.424291610717773, Transition Loss 4.050976276397705, Classifier Loss 0.27883514761924744, Total Loss 160.08804321289062\n",
      "32: Encoding Loss 17.83136558532715, Transition Loss 3.253913640975952, Classifier Loss 0.28017792105674744, Total Loss 171.3195037841797\n",
      "32: Encoding Loss 14.817553520202637, Transition Loss 4.3296356201171875, Classifier Loss 0.2817634046077728, Total Loss 147.5826873779297\n",
      "32: Encoding Loss 17.39034652709961, Transition Loss 3.103397846221924, Classifier Loss 0.2993340492248535, Total Loss 169.6768798828125\n",
      "32: Encoding Loss 17.781810760498047, Transition Loss 3.9210431575775146, Classifier Loss 0.31372231245040894, Total Loss 174.41094970703125\n",
      "32: Encoding Loss 16.265836715698242, Transition Loss 3.40108585357666, Classifier Loss 0.34810149669647217, Total Loss 165.6170654296875\n",
      "32: Encoding Loss 17.247343063354492, Transition Loss 3.8963797092437744, Classifier Loss 0.30191776156425476, Total Loss 168.94979858398438\n",
      "32: Encoding Loss 16.782575607299805, Transition Loss 3.058499574661255, Classifier Loss 0.2767365872859955, Total Loss 162.54595947265625\n",
      "32: Encoding Loss 16.35529899597168, Transition Loss 3.5736560821533203, Classifier Loss 0.3097999393939972, Total Loss 162.53712463378906\n",
      "32: Encoding Loss 16.253541946411133, Transition Loss 3.515650749206543, Classifier Loss 0.31987646222114563, Total Loss 162.71910095214844\n",
      "32: Encoding Loss 15.824152946472168, Transition Loss 3.493657112121582, Classifier Loss 0.26266470551490784, Total Loss 153.5584259033203\n",
      "32: Encoding Loss 15.896915435791016, Transition Loss 3.6902828216552734, Classifier Loss 0.29188036918640137, Total Loss 157.10140991210938\n",
      "32: Encoding Loss 16.46550750732422, Transition Loss 4.487308979034424, Classifier Loss 0.27580690383911133, Total Loss 160.20220947265625\n",
      "32: Encoding Loss 16.543664932250977, Transition Loss 3.648270606994629, Classifier Loss 0.26847657561302185, Total Loss 159.9266357421875\n",
      "32: Encoding Loss 15.91324520111084, Transition Loss 4.103586196899414, Classifier Loss 0.2639414668083191, Total Loss 154.5208282470703\n",
      "32: Encoding Loss 16.916885375976562, Transition Loss 4.006782531738281, Classifier Loss 0.25857579708099365, Total Loss 161.9940185546875\n",
      "32: Encoding Loss 16.770912170410156, Transition Loss 4.187057971954346, Classifier Loss 0.27283260226249695, Total Loss 162.28797912597656\n",
      "32: Encoding Loss 16.2515926361084, Transition Loss 4.266507625579834, Classifier Loss 0.2845779359340668, Total Loss 159.32383728027344\n",
      "32: Encoding Loss 16.342960357666016, Transition Loss 3.360109329223633, Classifier Loss 0.2914115786552429, Total Loss 160.55686950683594\n",
      "32: Encoding Loss 15.03762435913086, Transition Loss 5.384281158447266, Classifier Loss 0.27258965373039246, Total Loss 148.63682556152344\n",
      "32: Encoding Loss 16.129213333129883, Transition Loss 3.394439220428467, Classifier Loss 0.2876395583152771, Total Loss 158.4765625\n",
      "32: Encoding Loss 16.318763732910156, Transition Loss 2.6512179374694824, Classifier Loss 0.25607961416244507, Total Loss 156.6883087158203\n",
      "32: Encoding Loss 16.00461196899414, Transition Loss 3.6016006469726562, Classifier Loss 0.2902032434940338, Total Loss 157.7775421142578\n",
      "32: Encoding Loss 16.5200138092041, Transition Loss 4.135688304901123, Classifier Loss 0.27938997745513916, Total Loss 160.92623901367188\n",
      "32: Encoding Loss 16.493722915649414, Transition Loss 4.3526153564453125, Classifier Loss 0.2745947241783142, Total Loss 160.27978515625\n",
      "32: Encoding Loss 16.365467071533203, Transition Loss 4.466024398803711, Classifier Loss 0.25884726643562317, Total Loss 157.70167541503906\n",
      "32: Encoding Loss 15.346894264221191, Transition Loss 4.7312822341918945, Classifier Loss 0.32253068685531616, Total Loss 155.9744873046875\n",
      "32: Encoding Loss 16.397573471069336, Transition Loss 4.049655437469482, Classifier Loss 0.2774387300014496, Total Loss 159.73440551757812\n",
      "32: Encoding Loss 15.322476387023926, Transition Loss 5.138020992279053, Classifier Loss 0.25858286023139954, Total Loss 149.4656982421875\n",
      "32: Encoding Loss 16.436790466308594, Transition Loss 4.396177291870117, Classifier Loss 0.2977888286113739, Total Loss 162.15245056152344\n",
      "32: Encoding Loss 15.782422065734863, Transition Loss 2.4903438091278076, Classifier Loss 0.29169732332229614, Total Loss 155.9271697998047\n",
      "32: Encoding Loss 15.286789894104004, Transition Loss 4.206889629364014, Classifier Loss 0.27225354313850403, Total Loss 150.36106872558594\n",
      "32: Encoding Loss 15.518921852111816, Transition Loss 3.0564217567443848, Classifier Loss 0.3061862587928772, Total Loss 155.38128662109375\n",
      "32: Encoding Loss 16.07486343383789, Transition Loss 4.1818647384643555, Classifier Loss 0.2825557589530945, Total Loss 157.69085693359375\n",
      "32: Encoding Loss 15.918242454528809, Transition Loss 6.158810615539551, Classifier Loss 0.3295874297618866, Total Loss 161.5364532470703\n",
      "32: Encoding Loss 16.972637176513672, Transition Loss 3.681429862976074, Classifier Loss 0.2721867561340332, Total Loss 163.73606872558594\n",
      "32: Encoding Loss 16.18855857849121, Transition Loss 4.800159931182861, Classifier Loss 0.2824355661869049, Total Loss 158.71206665039062\n",
      "32: Encoding Loss 15.239842414855957, Transition Loss 4.28953742980957, Classifier Loss 0.28569918870925903, Total Loss 151.34657287597656\n",
      "32: Encoding Loss 16.208189010620117, Transition Loss 3.4998724460601807, Classifier Loss 0.3286650776863098, Total Loss 163.23199462890625\n",
      "32: Encoding Loss 15.932846069335938, Transition Loss 3.0299386978149414, Classifier Loss 0.26543962955474854, Total Loss 154.6127166748047\n",
      "32: Encoding Loss 16.290775299072266, Transition Loss 2.54634952545166, Classifier Loss 0.2757025361061096, Total Loss 158.40573120117188\n",
      "32: Encoding Loss 17.294347763061523, Transition Loss 3.4118804931640625, Classifier Loss 0.276912659406662, Total Loss 166.72842407226562\n",
      "32: Encoding Loss 16.61844253540039, Transition Loss 4.536682605743408, Classifier Loss 0.27849477529525757, Total Loss 161.704345703125\n",
      "32: Encoding Loss 15.895181655883789, Transition Loss 5.029592990875244, Classifier Loss 0.26056787371635437, Total Loss 154.2241668701172\n",
      "32: Encoding Loss 17.022968292236328, Transition Loss 4.428379535675049, Classifier Loss 0.26715517044067383, Total Loss 163.78494262695312\n",
      "32: Encoding Loss 16.448543548583984, Transition Loss 4.092556953430176, Classifier Loss 0.27550289034843445, Total Loss 159.95716857910156\n",
      "32: Encoding Loss 16.564056396484375, Transition Loss 4.288630485534668, Classifier Loss 0.2677561640739441, Total Loss 160.1457977294922\n",
      "32: Encoding Loss 17.17733383178711, Transition Loss 3.0492184162139893, Classifier Loss 0.27300989627838135, Total Loss 165.32952880859375\n",
      "32: Encoding Loss 16.775188446044922, Transition Loss 3.386183738708496, Classifier Loss 0.25506824254989624, Total Loss 160.3855743408203\n",
      "32: Encoding Loss 16.379756927490234, Transition Loss 2.8577444553375244, Classifier Loss 0.2679513096809387, Total Loss 158.40475463867188\n",
      "32: Encoding Loss 16.88894271850586, Transition Loss 4.528676986694336, Classifier Loss 0.27719080448150635, Total Loss 163.7363739013672\n",
      "32: Encoding Loss 15.563997268676758, Transition Loss 4.462466239929199, Classifier Loss 0.2558571398258209, Total Loss 150.99017333984375\n",
      "32: Encoding Loss 15.552324295043945, Transition Loss 3.6921777725219727, Classifier Loss 0.2851606011390686, Total Loss 153.673095703125\n",
      "32: Encoding Loss 16.05736541748047, Transition Loss 3.604605197906494, Classifier Loss 0.2684907019138336, Total Loss 156.02891540527344\n",
      "32: Encoding Loss 15.687746047973633, Transition Loss 3.671494960784912, Classifier Loss 0.2619626522064209, Total Loss 152.4325408935547\n",
      "32: Encoding Loss 15.577616691589355, Transition Loss 3.337900400161743, Classifier Loss 0.25265586376190186, Total Loss 150.55410766601562\n",
      "32: Encoding Loss 16.09723472595215, Transition Loss 3.4097495079040527, Classifier Loss 0.24482908844947815, Total Loss 153.94273376464844\n",
      "32: Encoding Loss 17.14239501953125, Transition Loss 4.368710041046143, Classifier Loss 0.30488765239715576, Total Loss 168.50167846679688\n",
      "32: Encoding Loss 17.619434356689453, Transition Loss 3.9660468101501465, Classifier Loss 0.27862414717674255, Total Loss 169.61109924316406\n",
      "32: Encoding Loss 15.893611907958984, Transition Loss 3.025522470474243, Classifier Loss 0.2567213177680969, Total Loss 153.42613220214844\n",
      "32: Encoding Loss 15.832992553710938, Transition Loss 3.1259779930114746, Classifier Loss 0.2627721130847931, Total Loss 153.56634521484375\n",
      "32: Encoding Loss 16.059738159179688, Transition Loss 4.197766304016113, Classifier Loss 0.2748127579689026, Total Loss 156.79873657226562\n",
      "32: Encoding Loss 15.543868064880371, Transition Loss 3.7213878631591797, Classifier Loss 0.2714028060436249, Total Loss 152.23550415039062\n",
      "32: Encoding Loss 16.48966407775879, Transition Loss 3.666358470916748, Classifier Loss 0.2765248417854309, Total Loss 160.30307006835938\n",
      "32: Encoding Loss 16.89695930480957, Transition Loss 3.665090322494507, Classifier Loss 0.22889111936092377, Total Loss 158.7978057861328\n",
      "32: Encoding Loss 16.209129333496094, Transition Loss 4.5997443199157715, Classifier Loss 0.28555232286453247, Total Loss 159.14822387695312\n",
      "32: Encoding Loss 16.012493133544922, Transition Loss 3.9255473613739014, Classifier Loss 0.2924627661705017, Total Loss 158.13134765625\n",
      "32: Encoding Loss 17.187498092651367, Transition Loss 3.8447751998901367, Classifier Loss 0.29467839002609253, Total Loss 167.7367706298828\n",
      "32: Encoding Loss 16.18588638305664, Transition Loss 4.3108439445495605, Classifier Loss 0.27313461899757385, Total Loss 157.6627197265625\n",
      "32: Encoding Loss 16.036640167236328, Transition Loss 4.280288219451904, Classifier Loss 0.2587558627128601, Total Loss 155.02476501464844\n",
      "32: Encoding Loss 16.927919387817383, Transition Loss 4.510786056518555, Classifier Loss 0.2566958963871002, Total Loss 161.99510192871094\n",
      "32: Encoding Loss 15.155435562133789, Transition Loss 5.376830101013184, Classifier Loss 0.3011592626571655, Total Loss 152.4347686767578\n",
      "32: Encoding Loss 16.333826065063477, Transition Loss 4.07722282409668, Classifier Loss 0.2679644227027893, Total Loss 158.28250122070312\n",
      "32: Encoding Loss 15.864668846130371, Transition Loss 4.895266532897949, Classifier Loss 0.2647344470024109, Total Loss 154.36984252929688\n",
      "32: Encoding Loss 16.113317489624023, Transition Loss 4.815075874328613, Classifier Loss 0.2728768587112427, Total Loss 157.15724182128906\n",
      "32: Encoding Loss 15.735565185546875, Transition Loss 4.6944122314453125, Classifier Loss 0.24657601118087769, Total Loss 151.4810028076172\n",
      "32: Encoding Loss 17.293025970458984, Transition Loss 4.062352180480957, Classifier Loss 0.2694644033908844, Total Loss 166.10313415527344\n",
      "32: Encoding Loss 15.845576286315918, Transition Loss 4.039010047912598, Classifier Loss 0.2700869143009186, Total Loss 154.5811004638672\n",
      "32: Encoding Loss 16.674352645874023, Transition Loss 4.1760358810424805, Classifier Loss 0.2923559546470642, Total Loss 163.4656219482422\n",
      "32: Encoding Loss 16.24681282043457, Transition Loss 4.30047082901001, Classifier Loss 0.2810715138912201, Total Loss 158.94174194335938\n",
      "32: Encoding Loss 16.079010009765625, Transition Loss 4.868247985839844, Classifier Loss 0.29460299015045166, Total Loss 159.06602478027344\n",
      "32: Encoding Loss 17.049758911132812, Transition Loss 3.758342981338501, Classifier Loss 0.2537474036216736, Total Loss 162.52447509765625\n",
      "32: Encoding Loss 15.773713111877441, Transition Loss 4.397602081298828, Classifier Loss 0.2583805322647095, Total Loss 152.90728759765625\n",
      "32: Encoding Loss 16.146581649780273, Transition Loss 3.4046506881713867, Classifier Loss 0.29099199175834656, Total Loss 158.95277404785156\n",
      "32: Encoding Loss 15.907726287841797, Transition Loss 5.609002590179443, Classifier Loss 0.2787449061870575, Total Loss 156.2581024169922\n",
      "32: Encoding Loss 15.778239250183105, Transition Loss 3.6554274559020996, Classifier Loss 0.27512824535369873, Total Loss 154.46981811523438\n",
      "32: Encoding Loss 16.175636291503906, Transition Loss 3.469799518585205, Classifier Loss 0.2707177996635437, Total Loss 157.1708221435547\n",
      "32: Encoding Loss 16.580808639526367, Transition Loss 3.8873748779296875, Classifier Loss 0.23461289703845978, Total Loss 156.88523864746094\n",
      "32: Encoding Loss 16.272117614746094, Transition Loss 5.354061126708984, Classifier Loss 0.32049643993377686, Total Loss 163.2974090576172\n",
      "32: Encoding Loss 16.34148406982422, Transition Loss 4.6234130859375, Classifier Loss 0.26331627368927, Total Loss 157.98818969726562\n",
      "32: Encoding Loss 17.053680419921875, Transition Loss 3.9672598838806152, Classifier Loss 0.23488232493400574, Total Loss 160.7111358642578\n",
      "32: Encoding Loss 16.888050079345703, Transition Loss 4.166971206665039, Classifier Loss 0.2582343518733978, Total Loss 161.76123046875\n",
      "32: Encoding Loss 15.944852828979492, Transition Loss 3.1048812866210938, Classifier Loss 0.2515062689781189, Total Loss 153.33042907714844\n",
      "32: Encoding Loss 16.18773651123047, Transition Loss 4.805925369262695, Classifier Loss 0.3097294270992279, Total Loss 161.43601989746094\n",
      "32: Encoding Loss 16.602609634399414, Transition Loss 3.6994316577911377, Classifier Loss 0.2984878718852997, Total Loss 163.4095458984375\n",
      "32: Encoding Loss 15.734719276428223, Transition Loss 5.284564018249512, Classifier Loss 0.2869366705417633, Total Loss 155.6283416748047\n",
      "32: Encoding Loss 16.817407608032227, Transition Loss 3.1864354610443115, Classifier Loss 0.24388626217842102, Total Loss 159.56517028808594\n",
      "32: Encoding Loss 16.235074996948242, Transition Loss 3.8817059993743896, Classifier Loss 0.270252525806427, Total Loss 157.68218994140625\n",
      "32: Encoding Loss 16.820592880249023, Transition Loss 3.934123992919922, Classifier Loss 0.2934795618057251, Total Loss 164.69952392578125\n",
      "32: Encoding Loss 16.1086368560791, Transition Loss 5.395959854125977, Classifier Loss 0.27834755182266235, Total Loss 157.78305053710938\n",
      "32: Encoding Loss 15.458584785461426, Transition Loss 4.66722297668457, Classifier Loss 0.25303566455841064, Total Loss 149.9056854248047\n",
      "32: Encoding Loss 15.811124801635742, Transition Loss 4.594513893127441, Classifier Loss 0.27235132455825806, Total Loss 154.64303588867188\n",
      "32: Encoding Loss 15.124646186828613, Transition Loss 6.0035881996154785, Classifier Loss 0.25571173429489136, Total Loss 147.76905822753906\n",
      "32: Encoding Loss 16.341638565063477, Transition Loss 4.454638481140137, Classifier Loss 0.2555524706840515, Total Loss 157.17929077148438\n",
      "32: Encoding Loss 15.976324081420898, Transition Loss 4.869485855102539, Classifier Loss 0.26968899369239807, Total Loss 155.75338745117188\n",
      "32: Encoding Loss 14.726568222045898, Transition Loss 4.201858997344971, Classifier Loss 0.31831884384155273, Total Loss 150.4848175048828\n",
      "32: Encoding Loss 15.51081657409668, Transition Loss 4.816133975982666, Classifier Loss 0.279853492975235, Total Loss 153.0351104736328\n",
      "32: Encoding Loss 16.301067352294922, Transition Loss 4.593888282775879, Classifier Loss 0.26792633533477783, Total Loss 158.11996459960938\n",
      "32: Encoding Loss 16.84547996520996, Transition Loss 4.921465873718262, Classifier Loss 0.26883557438850403, Total Loss 162.63169860839844\n",
      "32: Encoding Loss 15.944070816040039, Transition Loss 4.544035911560059, Classifier Loss 0.29245200753211975, Total Loss 157.70657348632812\n",
      "32: Encoding Loss 16.3106689453125, Transition Loss 3.7355122566223145, Classifier Loss 0.23902034759521484, Total Loss 155.13449096679688\n",
      "32: Encoding Loss 15.845572471618652, Transition Loss 3.5332705974578857, Classifier Loss 0.3080325722694397, Total Loss 158.2744903564453\n",
      "32: Encoding Loss 16.241239547729492, Transition Loss 4.973097801208496, Classifier Loss 0.2718994617462158, Total Loss 158.11447143554688\n",
      "32: Encoding Loss 16.81749725341797, Transition Loss 4.548542022705078, Classifier Loss 0.28186094760894775, Total Loss 163.6357879638672\n",
      "32: Encoding Loss 16.031938552856445, Transition Loss 4.405083179473877, Classifier Loss 0.2603870630264282, Total Loss 155.17523193359375\n",
      "32: Encoding Loss 16.22909164428711, Transition Loss 4.586884498596191, Classifier Loss 0.2566571831703186, Total Loss 156.4158477783203\n",
      "32: Encoding Loss 16.412181854248047, Transition Loss 4.04337215423584, Classifier Loss 0.2806035876274109, Total Loss 160.16650390625\n",
      "32: Encoding Loss 16.736000061035156, Transition Loss 3.1522185802459717, Classifier Loss 0.25828617811203003, Total Loss 160.34706115722656\n",
      "32: Encoding Loss 16.393651962280273, Transition Loss 3.3779425621032715, Classifier Loss 0.2654271721839905, Total Loss 158.3675079345703\n",
      "32: Encoding Loss 16.499267578125, Transition Loss 4.729824542999268, Classifier Loss 0.2758398652076721, Total Loss 160.5240936279297\n",
      "32: Encoding Loss 17.28551483154297, Transition Loss 3.3034720420837402, Classifier Loss 0.2515445947647095, Total Loss 164.09927368164062\n",
      "32: Encoding Loss 17.508214950561523, Transition Loss 3.4353742599487305, Classifier Loss 0.30251434445381165, Total Loss 171.0042266845703\n",
      "32: Encoding Loss 15.399155616760254, Transition Loss 4.788898944854736, Classifier Loss 0.273607075214386, Total Loss 151.51173400878906\n",
      "32: Encoding Loss 15.178007125854492, Transition Loss 5.07582426071167, Classifier Loss 0.25877058506011963, Total Loss 148.3162841796875\n",
      "32: Encoding Loss 15.584320068359375, Transition Loss 4.4746856689453125, Classifier Loss 0.29475119709968567, Total Loss 155.0446319580078\n",
      "32: Encoding Loss 15.557831764221191, Transition Loss 5.251101493835449, Classifier Loss 0.271831214427948, Total Loss 152.6959991455078\n",
      "32: Encoding Loss 15.343714714050293, Transition Loss 6.443986892700195, Classifier Loss 0.28732848167419434, Total Loss 152.77137756347656\n",
      "32: Encoding Loss 16.985231399536133, Transition Loss 4.256515026092529, Classifier Loss 0.27405089139938354, Total Loss 164.13824462890625\n",
      "32: Encoding Loss 16.891841888427734, Transition Loss 4.0167741775512695, Classifier Loss 0.2331905961036682, Total Loss 159.25717163085938\n",
      "32: Encoding Loss 15.414097785949707, Transition Loss 3.7988054752349854, Classifier Loss 0.2605467736721039, Total Loss 150.12722778320312\n",
      "32: Encoding Loss 15.856730461120605, Transition Loss 3.621882677078247, Classifier Loss 0.2756144404411316, Total Loss 155.13966369628906\n",
      "32: Encoding Loss 15.982152938842773, Transition Loss 4.143155574798584, Classifier Loss 0.3018401861190796, Total Loss 158.869873046875\n",
      "32: Encoding Loss 16.246105194091797, Transition Loss 4.624359130859375, Classifier Loss 0.27563929557800293, Total Loss 158.45765686035156\n",
      "32: Encoding Loss 16.296085357666016, Transition Loss 3.778066635131836, Classifier Loss 0.2454625815153122, Total Loss 155.67056274414062\n",
      "32: Encoding Loss 16.339096069335938, Transition Loss 4.268651485443115, Classifier Loss 0.28430718183517456, Total Loss 159.99722290039062\n",
      "32: Encoding Loss 15.606358528137207, Transition Loss 3.4777064323425293, Classifier Loss 0.24248471856117249, Total Loss 149.7948760986328\n",
      "32: Encoding Loss 16.16733169555664, Transition Loss 4.593182563781738, Classifier Loss 0.25234681367874146, Total Loss 155.49197387695312\n",
      "32: Encoding Loss 16.62952423095703, Transition Loss 3.410149574279785, Classifier Loss 0.31756365299224854, Total Loss 165.47459411621094\n",
      "32: Encoding Loss 15.514501571655273, Transition Loss 5.41378927230835, Classifier Loss 0.2730371952056885, Total Loss 152.50250244140625\n",
      "32: Encoding Loss 16.292455673217773, Transition Loss 4.14779806137085, Classifier Loss 0.27513575553894043, Total Loss 158.6827850341797\n",
      "32: Encoding Loss 16.216899871826172, Transition Loss 5.164727687835693, Classifier Loss 0.2820059657096863, Total Loss 158.96875\n",
      "32: Encoding Loss 16.23844337463379, Transition Loss 4.274846076965332, Classifier Loss 0.25987833738327026, Total Loss 156.75035095214844\n",
      "32: Encoding Loss 15.702305793762207, Transition Loss 4.799515724182129, Classifier Loss 0.2802111506462097, Total Loss 154.59945678710938\n",
      "32: Encoding Loss 17.32236671447754, Transition Loss 3.360144853591919, Classifier Loss 0.27100273966789246, Total Loss 166.35122680664062\n",
      "32: Encoding Loss 16.769184112548828, Transition Loss 3.1767728328704834, Classifier Loss 0.27542492747306824, Total Loss 162.33132934570312\n",
      "32: Encoding Loss 16.16701316833496, Transition Loss 3.6294121742248535, Classifier Loss 0.2532165050506592, Total Loss 155.38363647460938\n",
      "32: Encoding Loss 15.647788047790527, Transition Loss 4.57038688659668, Classifier Loss 0.2541017234325409, Total Loss 151.5065460205078\n",
      "32: Encoding Loss 15.938521385192871, Transition Loss 2.8749709129333496, Classifier Loss 0.26649171113967896, Total Loss 154.7323455810547\n",
      "32: Encoding Loss 16.352916717529297, Transition Loss 5.808100700378418, Classifier Loss 0.2591263949871063, Total Loss 157.89761352539062\n",
      "32: Encoding Loss 16.724580764770508, Transition Loss 3.986217498779297, Classifier Loss 0.2579217553138733, Total Loss 160.3860626220703\n",
      "32: Encoding Loss 17.303800582885742, Transition Loss 4.610587120056152, Classifier Loss 0.25505220890045166, Total Loss 164.8577423095703\n",
      "32: Encoding Loss 16.87979507446289, Transition Loss 5.152709007263184, Classifier Loss 0.26847195625305176, Total Loss 162.91610717773438\n",
      "32: Encoding Loss 15.527048110961914, Transition Loss 6.266410827636719, Classifier Loss 0.27836883068084717, Total Loss 153.30654907226562\n",
      "32: Encoding Loss 13.492016792297363, Transition Loss 7.195366859436035, Classifier Loss 0.30798307061195374, Total Loss 140.1735076904297\n",
      "33: Encoding Loss 17.259790420532227, Transition Loss 3.765991687774658, Classifier Loss 0.2778779864311218, Total Loss 166.61932373046875\n",
      "33: Encoding Loss 16.643144607543945, Transition Loss 4.349214553833008, Classifier Loss 0.2598937451839447, Total Loss 160.00437927246094\n",
      "33: Encoding Loss 16.598514556884766, Transition Loss 4.0302653312683105, Classifier Loss 0.2575407028198242, Total Loss 159.34823608398438\n",
      "33: Encoding Loss 16.20429039001465, Transition Loss 4.956234455108643, Classifier Loss 0.26589953899383545, Total Loss 157.21551513671875\n",
      "33: Encoding Loss 17.69422149658203, Transition Loss 3.942615509033203, Classifier Loss 0.26522770524024963, Total Loss 168.8650665283203\n",
      "33: Encoding Loss 14.600174903869629, Transition Loss 5.191009521484375, Classifier Loss 0.27096009254455566, Total Loss 144.9356231689453\n",
      "33: Encoding Loss 17.20806312561035, Transition Loss 3.7236976623535156, Classifier Loss 0.28698641061782837, Total Loss 167.10787963867188\n",
      "33: Encoding Loss 17.499359130859375, Transition Loss 4.791994094848633, Classifier Loss 0.30655309557914734, Total Loss 171.60858154296875\n",
      "33: Encoding Loss 16.009445190429688, Transition Loss 4.160146236419678, Classifier Loss 0.34448179602622986, Total Loss 163.35577392578125\n",
      "33: Encoding Loss 16.94649887084961, Transition Loss 4.7778167724609375, Classifier Loss 0.2956749498844147, Total Loss 166.09506225585938\n",
      "33: Encoding Loss 16.532215118408203, Transition Loss 3.654984474182129, Classifier Loss 0.2453826665878296, Total Loss 157.52699279785156\n",
      "33: Encoding Loss 16.14705467224121, Transition Loss 4.347402095794678, Classifier Loss 0.27798548340797424, Total Loss 157.84446716308594\n",
      "33: Encoding Loss 15.995990753173828, Transition Loss 4.443759441375732, Classifier Loss 0.26366543769836426, Total Loss 155.22320556640625\n",
      "33: Encoding Loss 15.457773208618164, Transition Loss 4.107231616973877, Classifier Loss 0.26507285237312317, Total Loss 150.99090576171875\n",
      "33: Encoding Loss 15.59423542022705, Transition Loss 4.600092887878418, Classifier Loss 0.2702454924583435, Total Loss 152.6984405517578\n",
      "33: Encoding Loss 16.21269989013672, Transition Loss 5.5337114334106445, Classifier Loss 0.26906853914260864, Total Loss 157.71519470214844\n",
      "33: Encoding Loss 16.29152488708496, Transition Loss 4.467076301574707, Classifier Loss 0.26929426193237305, Total Loss 158.15504455566406\n",
      "33: Encoding Loss 15.659855842590332, Transition Loss 5.181182861328125, Classifier Loss 0.242482990026474, Total Loss 150.56338500976562\n",
      "33: Encoding Loss 16.706708908081055, Transition Loss 5.105270862579346, Classifier Loss 0.23711881041526794, Total Loss 158.38661193847656\n",
      "33: Encoding Loss 16.525636672973633, Transition Loss 5.246325492858887, Classifier Loss 0.24892830848693848, Total Loss 158.14720153808594\n",
      "33: Encoding Loss 16.018451690673828, Transition Loss 5.413382053375244, Classifier Loss 0.282842755317688, Total Loss 157.51455688476562\n",
      "33: Encoding Loss 16.124530792236328, Transition Loss 4.192863464355469, Classifier Loss 0.23430094122886658, Total Loss 153.26492309570312\n",
      "33: Encoding Loss 14.519512176513672, Transition Loss 7.1657209396362305, Classifier Loss 0.24652042984962463, Total Loss 142.24130249023438\n",
      "33: Encoding Loss 15.94431209564209, Transition Loss 4.180703163146973, Classifier Loss 0.27507302165031433, Total Loss 155.89793395996094\n",
      "33: Encoding Loss 16.053470611572266, Transition Loss 3.180305242538452, Classifier Loss 0.25668099522590637, Total Loss 154.73193359375\n",
      "33: Encoding Loss 15.733516693115234, Transition Loss 4.410679817199707, Classifier Loss 0.2546979486942291, Total Loss 152.22007751464844\n",
      "33: Encoding Loss 16.215633392333984, Transition Loss 5.209659576416016, Classifier Loss 0.2581894099712372, Total Loss 156.58595275878906\n",
      "33: Encoding Loss 16.333932876586914, Transition Loss 5.419448375701904, Classifier Loss 0.26030233502388, Total Loss 157.78558349609375\n",
      "33: Encoding Loss 16.236265182495117, Transition Loss 5.713642120361328, Classifier Loss 0.28230446577072144, Total Loss 159.2633056640625\n",
      "33: Encoding Loss 14.959125518798828, Transition Loss 5.947319984436035, Classifier Loss 0.2719621956348419, Total Loss 148.05868530273438\n",
      "33: Encoding Loss 16.14378547668457, Transition Loss 5.090986251831055, Classifier Loss 0.26599082350730896, Total Loss 156.76756286621094\n",
      "33: Encoding Loss 15.048745155334473, Transition Loss 6.714745044708252, Classifier Loss 0.24439311027526855, Total Loss 146.1722412109375\n",
      "33: Encoding Loss 16.069053649902344, Transition Loss 5.5975799560546875, Classifier Loss 0.2813214957714081, Total Loss 157.80409240722656\n",
      "33: Encoding Loss 15.521703720092773, Transition Loss 2.914529800415039, Classifier Loss 0.2848248779773712, Total Loss 153.23902893066406\n",
      "33: Encoding Loss 14.897761344909668, Transition Loss 5.512272834777832, Classifier Loss 0.26213279366493225, Total Loss 146.49781799316406\n",
      "33: Encoding Loss 15.299556732177734, Transition Loss 3.7146434783935547, Classifier Loss 0.25899308919906616, Total Loss 149.03871154785156\n",
      "33: Encoding Loss 15.817312240600586, Transition Loss 5.510526180267334, Classifier Loss 0.2571480870246887, Total Loss 153.3554229736328\n",
      "33: Encoding Loss 15.477307319641113, Transition Loss 7.799526214599609, Classifier Loss 0.2947268486022949, Total Loss 154.85104370117188\n",
      "33: Encoding Loss 16.747243881225586, Transition Loss 4.818206787109375, Classifier Loss 0.2529710829257965, Total Loss 160.2386932373047\n",
      "33: Encoding Loss 15.912897109985352, Transition Loss 6.281285285949707, Classifier Loss 0.2563759386539459, Total Loss 154.197021484375\n",
      "33: Encoding Loss 14.877753257751465, Transition Loss 5.483390808105469, Classifier Loss 0.27879229187965393, Total Loss 147.99794006347656\n",
      "33: Encoding Loss 16.09723663330078, Transition Loss 4.422920227050781, Classifier Loss 0.25620296597480774, Total Loss 155.28277587890625\n",
      "33: Encoding Loss 15.706795692443848, Transition Loss 3.7762320041656494, Classifier Loss 0.23927155137062073, Total Loss 150.33677673339844\n",
      "33: Encoding Loss 16.180665969848633, Transition Loss 3.149390697479248, Classifier Loss 0.2531691789627075, Total Loss 155.39212036132812\n",
      "33: Encoding Loss 17.040912628173828, Transition Loss 4.293794631958008, Classifier Loss 0.2712344229221344, Total Loss 164.30950927734375\n",
      "33: Encoding Loss 16.408092498779297, Transition Loss 5.938486576080322, Classifier Loss 0.2772197425365448, Total Loss 160.17442321777344\n",
      "33: Encoding Loss 15.604986190795898, Transition Loss 6.572072982788086, Classifier Loss 0.25644755363464355, Total Loss 151.79905700683594\n",
      "33: Encoding Loss 16.884172439575195, Transition Loss 5.768310070037842, Classifier Loss 0.26506415009498596, Total Loss 162.7334442138672\n",
      "33: Encoding Loss 16.230224609375, Transition Loss 5.207176685333252, Classifier Loss 0.266275554895401, Total Loss 157.51080322265625\n",
      "33: Encoding Loss 16.317007064819336, Transition Loss 5.482625484466553, Classifier Loss 0.23418515920639038, Total Loss 155.0511016845703\n",
      "33: Encoding Loss 17.108957290649414, Transition Loss 3.860186815261841, Classifier Loss 0.2666681706905365, Total Loss 164.31051635742188\n",
      "33: Encoding Loss 16.741518020629883, Transition Loss 4.312343597412109, Classifier Loss 0.2559228241443634, Total Loss 160.38690185546875\n",
      "33: Encoding Loss 16.25095558166504, Transition Loss 3.570895195007324, Classifier Loss 0.2510473132133484, Total Loss 155.82655334472656\n",
      "33: Encoding Loss 16.8310546875, Transition Loss 5.859951972961426, Classifier Loss 0.26233720779418945, Total Loss 162.0541534423828\n",
      "33: Encoding Loss 15.295074462890625, Transition Loss 5.810760974884033, Classifier Loss 0.23970162868499756, Total Loss 147.492919921875\n",
      "33: Encoding Loss 15.358688354492188, Transition Loss 4.67329216003418, Classifier Loss 0.28025779128074646, Total Loss 151.8299560546875\n",
      "33: Encoding Loss 15.77007007598877, Transition Loss 4.712268352508545, Classifier Loss 0.24280232191085815, Total Loss 151.3832550048828\n",
      "33: Encoding Loss 15.38006591796875, Transition Loss 4.74867582321167, Classifier Loss 0.23634836077690125, Total Loss 147.62510681152344\n",
      "33: Encoding Loss 15.171238899230957, Transition Loss 4.219621181488037, Classifier Loss 0.26140785217285156, Total Loss 148.3546142578125\n",
      "33: Encoding Loss 16.01084327697754, Transition Loss 4.2303667068481445, Classifier Loss 0.2340221107006073, Total Loss 152.33502197265625\n",
      "33: Encoding Loss 17.067485809326172, Transition Loss 5.582267761230469, Classifier Loss 0.2688679099082947, Total Loss 164.54315185546875\n",
      "33: Encoding Loss 17.446510314941406, Transition Loss 5.154257297515869, Classifier Loss 0.2560698986053467, Total Loss 166.20993041992188\n",
      "33: Encoding Loss 15.525477409362793, Transition Loss 3.642979621887207, Classifier Loss 0.2375757098197937, Total Loss 148.6899871826172\n",
      "33: Encoding Loss 15.349756240844727, Transition Loss 3.8869261741638184, Classifier Loss 0.25736725330352783, Total Loss 149.31216430664062\n",
      "33: Encoding Loss 15.918282508850098, Transition Loss 5.539115905761719, Classifier Loss 0.23927250504493713, Total Loss 152.38133239746094\n",
      "33: Encoding Loss 15.304646492004395, Transition Loss 4.954861640930176, Classifier Loss 0.23758119344711304, Total Loss 147.18626403808594\n",
      "33: Encoding Loss 16.282155990600586, Transition Loss 4.706797122955322, Classifier Loss 0.2532545328140259, Total Loss 156.52406311035156\n",
      "33: Encoding Loss 16.680133819580078, Transition Loss 4.8384599685668945, Classifier Loss 0.21515107154846191, Total Loss 155.9238739013672\n",
      "33: Encoding Loss 15.8073148727417, Transition Loss 5.914770603179932, Classifier Loss 0.2633458971977234, Total Loss 153.97605895996094\n",
      "33: Encoding Loss 15.741156578063965, Transition Loss 5.195633888244629, Classifier Loss 0.272499680519104, Total Loss 154.2183380126953\n",
      "33: Encoding Loss 17.00821876525879, Transition Loss 4.935727119445801, Classifier Loss 0.25121068954467773, Total Loss 162.17396545410156\n",
      "33: Encoding Loss 16.082223892211914, Transition Loss 5.700662612915039, Classifier Loss 0.25298917293548584, Total Loss 155.0968475341797\n",
      "33: Encoding Loss 15.891392707824707, Transition Loss 5.419261932373047, Classifier Loss 0.22462531924247742, Total Loss 150.67752075195312\n",
      "33: Encoding Loss 16.751659393310547, Transition Loss 5.978227138519287, Classifier Loss 0.24171099066734314, Total Loss 159.38003540039062\n",
      "33: Encoding Loss 14.861483573913574, Transition Loss 6.8798322677612305, Classifier Loss 0.290818452835083, Total Loss 149.34967041015625\n",
      "33: Encoding Loss 16.026779174804688, Transition Loss 5.342930793762207, Classifier Loss 0.24830032885074615, Total Loss 154.11285400390625\n",
      "33: Encoding Loss 15.512946128845215, Transition Loss 6.2428669929504395, Classifier Loss 0.24215450882911682, Total Loss 149.56759643554688\n",
      "33: Encoding Loss 15.874551773071289, Transition Loss 6.366397857666016, Classifier Loss 0.2638176679611206, Total Loss 154.65145874023438\n",
      "33: Encoding Loss 15.486055374145508, Transition Loss 6.245540142059326, Classifier Loss 0.2435915768146515, Total Loss 149.49671936035156\n",
      "33: Encoding Loss 17.146732330322266, Transition Loss 5.503469467163086, Classifier Loss 0.2504326403141022, Total Loss 163.31781005859375\n",
      "33: Encoding Loss 15.622700691223145, Transition Loss 5.107412338256836, Classifier Loss 0.24621248245239258, Total Loss 150.6243438720703\n",
      "33: Encoding Loss 16.473552703857422, Transition Loss 5.530354022979736, Classifier Loss 0.266284316778183, Total Loss 159.52293395996094\n",
      "33: Encoding Loss 16.01609992980957, Transition Loss 5.7084574699401855, Classifier Loss 0.25914111733436584, Total Loss 155.18460083007812\n",
      "33: Encoding Loss 15.766453742980957, Transition Loss 6.263758659362793, Classifier Loss 0.24394568800926208, Total Loss 151.77894592285156\n",
      "33: Encoding Loss 16.840518951416016, Transition Loss 4.958643913269043, Classifier Loss 0.23826895654201508, Total Loss 159.5427703857422\n",
      "33: Encoding Loss 15.543194770812988, Transition Loss 5.700846195220947, Classifier Loss 0.22718487679958344, Total Loss 148.20420837402344\n",
      "33: Encoding Loss 15.876352310180664, Transition Loss 4.473111152648926, Classifier Loss 0.26042062044143677, Total Loss 153.947509765625\n",
      "33: Encoding Loss 15.584725379943848, Transition Loss 7.161067962646484, Classifier Loss 0.269401490688324, Total Loss 153.05018615722656\n",
      "33: Encoding Loss 15.408787727355957, Transition Loss 4.714605331420898, Classifier Loss 0.24520714581012726, Total Loss 148.7339324951172\n",
      "33: Encoding Loss 15.802623748779297, Transition Loss 4.115842819213867, Classifier Loss 0.25246891379356384, Total Loss 152.49105834960938\n",
      "33: Encoding Loss 16.321197509765625, Transition Loss 5.245504856109619, Classifier Loss 0.21643701195716858, Total Loss 153.26239013671875\n",
      "33: Encoding Loss 15.926794052124023, Transition Loss 6.988420486450195, Classifier Loss 0.3009462356567383, Total Loss 158.90667724609375\n",
      "33: Encoding Loss 16.175273895263672, Transition Loss 6.264049053192139, Classifier Loss 0.24647238850593567, Total Loss 155.30224609375\n",
      "33: Encoding Loss 16.850669860839844, Transition Loss 5.160615921020508, Classifier Loss 0.22445738315582275, Total Loss 158.28321838378906\n",
      "33: Encoding Loss 16.711994171142578, Transition Loss 5.570376396179199, Classifier Loss 0.2384169101715088, Total Loss 158.65171813964844\n",
      "33: Encoding Loss 15.743155479431152, Transition Loss 3.90517258644104, Classifier Loss 0.22998693585395813, Total Loss 149.7249755859375\n",
      "33: Encoding Loss 15.98447322845459, Transition Loss 6.522294998168945, Classifier Loss 0.2719165086746216, Total Loss 156.3719024658203\n",
      "33: Encoding Loss 16.35269546508789, Transition Loss 4.78690242767334, Classifier Loss 0.26304152607917786, Total Loss 158.08309936523438\n",
      "33: Encoding Loss 15.72731876373291, Transition Loss 7.030045509338379, Classifier Loss 0.2656438946723938, Total Loss 153.78895568847656\n",
      "33: Encoding Loss 16.44630241394043, Transition Loss 4.129531383514404, Classifier Loss 0.24653100967407227, Total Loss 157.0494384765625\n",
      "33: Encoding Loss 15.986018180847168, Transition Loss 5.192589282989502, Classifier Loss 0.24609723687171936, Total Loss 153.536376953125\n",
      "33: Encoding Loss 16.66699981689453, Transition Loss 5.444640636444092, Classifier Loss 0.3043561279773712, Total Loss 164.86053466796875\n",
      "33: Encoding Loss 16.008995056152344, Transition Loss 7.333524227142334, Classifier Loss 0.2544020414352417, Total Loss 154.97886657714844\n",
      "33: Encoding Loss 14.971405029296875, Transition Loss 6.255340099334717, Classifier Loss 0.23618078231811523, Total Loss 144.640380859375\n",
      "33: Encoding Loss 15.58022403717041, Transition Loss 6.22452449798584, Classifier Loss 0.23896339535713196, Total Loss 149.78305053710938\n",
      "33: Encoding Loss 14.985671043395996, Transition Loss 8.365340232849121, Classifier Loss 0.2274750918149948, Total Loss 144.30593872070312\n",
      "33: Encoding Loss 16.22003746032715, Transition Loss 6.021177291870117, Classifier Loss 0.2626103162765503, Total Loss 157.22557067871094\n",
      "33: Encoding Loss 15.738750457763672, Transition Loss 6.763192176818848, Classifier Loss 0.23958902060985565, Total Loss 151.2215576171875\n",
      "33: Encoding Loss 14.487394332885742, Transition Loss 5.7006378173828125, Classifier Loss 0.2490905523300171, Total Loss 141.94833374023438\n",
      "33: Encoding Loss 15.209381103515625, Transition Loss 6.729014873504639, Classifier Loss 0.2436506599187851, Total Loss 147.38592529296875\n",
      "33: Encoding Loss 15.97340202331543, Transition Loss 6.276238441467285, Classifier Loss 0.24318939447402954, Total Loss 153.36141967773438\n",
      "33: Encoding Loss 16.442838668823242, Transition Loss 6.753117084503174, Classifier Loss 0.2527885437011719, Total Loss 158.17218017578125\n",
      "33: Encoding Loss 15.66588306427002, Transition Loss 6.275740146636963, Classifier Loss 0.235194131731987, Total Loss 150.10162353515625\n",
      "33: Encoding Loss 16.065044403076172, Transition Loss 5.112852096557617, Classifier Loss 0.2247031182050705, Total Loss 152.01324462890625\n",
      "33: Encoding Loss 15.544973373413086, Transition Loss 4.743366718292236, Classifier Loss 0.27983343601226807, Total Loss 153.29180908203125\n",
      "33: Encoding Loss 15.710094451904297, Transition Loss 6.554111003875732, Classifier Loss 0.23364122211933136, Total Loss 150.355712890625\n",
      "33: Encoding Loss 16.660730361938477, Transition Loss 6.197906017303467, Classifier Loss 0.24627439677715302, Total Loss 159.15286254882812\n",
      "33: Encoding Loss 15.949146270751953, Transition Loss 6.0465617179870605, Classifier Loss 0.25240176916122437, Total Loss 154.0426483154297\n",
      "33: Encoding Loss 15.982903480529785, Transition Loss 6.133768081665039, Classifier Loss 0.24574968218803406, Total Loss 153.6649627685547\n",
      "33: Encoding Loss 16.205297470092773, Transition Loss 5.447470188140869, Classifier Loss 0.24238750338554382, Total Loss 154.9706268310547\n",
      "33: Encoding Loss 16.834386825561523, Transition Loss 4.203031063079834, Classifier Loss 0.26188889145851135, Total Loss 161.70458984375\n",
      "33: Encoding Loss 16.220260620117188, Transition Loss 4.458469390869141, Classifier Loss 0.24489496648311615, Total Loss 155.14328002929688\n",
      "33: Encoding Loss 16.27363395690918, Transition Loss 6.25143575668335, Classifier Loss 0.247760608792305, Total Loss 156.21542358398438\n",
      "33: Encoding Loss 17.14615821838379, Transition Loss 4.363990783691406, Classifier Loss 0.25080177187919617, Total Loss 163.12225341796875\n",
      "33: Encoding Loss 17.420658111572266, Transition Loss 4.567234039306641, Classifier Loss 0.2751767337322235, Total Loss 167.79638671875\n",
      "33: Encoding Loss 15.185651779174805, Transition Loss 6.509071350097656, Classifier Loss 0.22927458584308624, Total Loss 145.71449279785156\n",
      "33: Encoding Loss 14.903761863708496, Transition Loss 6.695743560791016, Classifier Loss 0.2195461392402649, Total Loss 142.5238494873047\n",
      "33: Encoding Loss 15.34494400024414, Transition Loss 6.096222877502441, Classifier Loss 0.26852989196777344, Total Loss 150.83177185058594\n",
      "33: Encoding Loss 15.380255699157715, Transition Loss 6.996903419494629, Classifier Loss 0.2569430470466614, Total Loss 150.1357421875\n",
      "33: Encoding Loss 15.201662063598633, Transition Loss 8.605658531188965, Classifier Loss 0.2622392177581787, Total Loss 149.558349609375\n",
      "33: Encoding Loss 16.78319549560547, Transition Loss 5.741981506347656, Classifier Loss 0.2494327574968338, Total Loss 160.3572235107422\n",
      "33: Encoding Loss 16.77431869506836, Transition Loss 5.411952018737793, Classifier Loss 0.21781371533870697, Total Loss 157.05833435058594\n",
      "33: Encoding Loss 15.177358627319336, Transition Loss 4.984674453735352, Classifier Loss 0.23875853419303894, Total Loss 146.29165649414062\n",
      "33: Encoding Loss 15.635601997375488, Transition Loss 4.783644676208496, Classifier Loss 0.24146947264671326, Total Loss 150.18849182128906\n",
      "33: Encoding Loss 15.74612808227539, Transition Loss 5.350919723510742, Classifier Loss 0.2625301480293274, Total Loss 153.29222106933594\n",
      "33: Encoding Loss 16.194272994995117, Transition Loss 6.346389293670654, Classifier Loss 0.23068749904632568, Total Loss 153.8922119140625\n",
      "33: Encoding Loss 16.081132888793945, Transition Loss 5.018451690673828, Classifier Loss 0.23527690768241882, Total Loss 153.18045043945312\n",
      "33: Encoding Loss 16.12371253967285, Transition Loss 5.819350719451904, Classifier Loss 0.27306628227233887, Total Loss 157.46018981933594\n",
      "33: Encoding Loss 15.396053314208984, Transition Loss 4.452589988708496, Classifier Loss 0.2190602421760559, Total Loss 145.96498107910156\n",
      "33: Encoding Loss 16.117036819458008, Transition Loss 6.2510857582092285, Classifier Loss 0.22689665853977203, Total Loss 152.8761749267578\n",
      "33: Encoding Loss 16.1940975189209, Transition Loss 4.29146146774292, Classifier Loss 0.28631100058555603, Total Loss 159.04217529296875\n",
      "33: Encoding Loss 15.319987297058105, Transition Loss 7.315330982208252, Classifier Loss 0.25652390718460083, Total Loss 149.6753692626953\n",
      "33: Encoding Loss 16.01546287536621, Transition Loss 5.386340141296387, Classifier Loss 0.24410536885261536, Total Loss 153.61151123046875\n",
      "33: Encoding Loss 16.121891021728516, Transition Loss 6.842033863067627, Classifier Loss 0.2548048794269562, Total Loss 155.8240203857422\n",
      "33: Encoding Loss 16.027894973754883, Transition Loss 5.536343097686768, Classifier Loss 0.24389950931072235, Total Loss 153.7203826904297\n",
      "33: Encoding Loss 15.570462226867676, Transition Loss 6.279538154602051, Classifier Loss 0.24970278143882751, Total Loss 150.78988647460938\n",
      "33: Encoding Loss 17.200284957885742, Transition Loss 4.3526763916015625, Classifier Loss 0.25586211681365967, Total Loss 164.05902099609375\n",
      "33: Encoding Loss 16.623001098632812, Transition Loss 4.1403584480285645, Classifier Loss 0.24731868505477905, Total Loss 158.54396057128906\n",
      "33: Encoding Loss 15.792791366577148, Transition Loss 4.625600814819336, Classifier Loss 0.245865136384964, Total Loss 151.85397338867188\n",
      "33: Encoding Loss 15.555155754089355, Transition Loss 6.129723072052002, Classifier Loss 0.2242618203163147, Total Loss 148.0933837890625\n",
      "33: Encoding Loss 15.62677001953125, Transition Loss 3.719890594482422, Classifier Loss 0.2475724220275879, Total Loss 150.515380859375\n",
      "33: Encoding Loss 16.10231590270996, Transition Loss 7.70772123336792, Classifier Loss 0.2520902156829834, Total Loss 155.56910705566406\n",
      "33: Encoding Loss 16.52965545654297, Transition Loss 5.3381171226501465, Classifier Loss 0.23887568712234497, Total Loss 157.19244384765625\n",
      "33: Encoding Loss 17.17603874206543, Transition Loss 5.989081859588623, Classifier Loss 0.2520439028739929, Total Loss 163.81051635742188\n",
      "33: Encoding Loss 16.69612693786621, Transition Loss 6.9313836097717285, Classifier Loss 0.24163605272769928, Total Loss 159.118896484375\n",
      "33: Encoding Loss 15.322994232177734, Transition Loss 8.193902969360352, Classifier Loss 0.248281329870224, Total Loss 149.05087280273438\n",
      "33: Encoding Loss 13.255800247192383, Transition Loss 9.63280963897705, Classifier Loss 0.24532875418663025, Total Loss 132.50584411621094\n",
      "34: Encoding Loss 17.131799697875977, Transition Loss 4.631601333618164, Classifier Loss 0.2571023106575012, Total Loss 163.69094848632812\n",
      "34: Encoding Loss 16.46123695373535, Transition Loss 5.818643093109131, Classifier Loss 0.24671590328216553, Total Loss 157.52520751953125\n",
      "34: Encoding Loss 16.38022804260254, Transition Loss 5.162844657897949, Classifier Loss 0.2582606375217438, Total Loss 157.90045166015625\n",
      "34: Encoding Loss 15.942057609558105, Transition Loss 6.716738700866699, Classifier Loss 0.2522140145301819, Total Loss 154.10121154785156\n",
      "34: Encoding Loss 17.629972457885742, Transition Loss 5.142316818237305, Classifier Loss 0.2650485634803772, Total Loss 168.57308959960938\n",
      "34: Encoding Loss 14.226202011108398, Transition Loss 6.682439804077148, Classifier Loss 0.26278871297836304, Total Loss 141.42498779296875\n",
      "34: Encoding Loss 17.103740692138672, Transition Loss 4.990498065948486, Classifier Loss 0.2570192217826843, Total Loss 163.5299530029297\n",
      "34: Encoding Loss 17.285884857177734, Transition Loss 6.193126678466797, Classifier Loss 0.27322953939437866, Total Loss 166.8486785888672\n",
      "34: Encoding Loss 15.807443618774414, Transition Loss 5.517504692077637, Classifier Loss 0.3265882432460785, Total Loss 160.2218780517578\n",
      "34: Encoding Loss 16.666969299316406, Transition Loss 6.03036642074585, Classifier Loss 0.26687753200531006, Total Loss 161.22958374023438\n",
      "34: Encoding Loss 16.223840713500977, Transition Loss 4.764485836029053, Classifier Loss 0.24028649926185608, Total Loss 154.77227783203125\n",
      "34: Encoding Loss 15.91899585723877, Transition Loss 5.589503288269043, Classifier Loss 0.2689848244190216, Total Loss 155.3683624267578\n",
      "34: Encoding Loss 15.910965919494629, Transition Loss 5.9744462966918945, Classifier Loss 0.24547743797302246, Total Loss 153.03036499023438\n",
      "34: Encoding Loss 15.159987449645996, Transition Loss 5.210068702697754, Classifier Loss 0.23971515893936157, Total Loss 146.29342651367188\n",
      "34: Encoding Loss 15.320241928100586, Transition Loss 6.219296932220459, Classifier Loss 0.2705034613609314, Total Loss 150.8561553955078\n",
      "34: Encoding Loss 16.114160537719727, Transition Loss 7.231978416442871, Classifier Loss 0.23500680923461914, Total Loss 153.86036682128906\n",
      "34: Encoding Loss 16.072824478149414, Transition Loss 5.84047794342041, Classifier Loss 0.25215911865234375, Total Loss 154.9665985107422\n",
      "34: Encoding Loss 15.408838272094727, Transition Loss 6.9349365234375, Classifier Loss 0.2260618656873703, Total Loss 147.26388549804688\n",
      "34: Encoding Loss 16.665258407592773, Transition Loss 6.835580348968506, Classifier Loss 0.22422882914543152, Total Loss 157.112060546875\n",
      "34: Encoding Loss 16.303129196166992, Transition Loss 6.895902633666992, Classifier Loss 0.23335087299346924, Total Loss 155.13929748535156\n",
      "34: Encoding Loss 15.827013969421387, Transition Loss 7.104405403137207, Classifier Loss 0.25685644149780273, Total Loss 153.72264099121094\n",
      "34: Encoding Loss 15.96515941619873, Transition Loss 5.485500335693359, Classifier Loss 0.2330513298511505, Total Loss 152.12351989746094\n",
      "34: Encoding Loss 14.446898460388184, Transition Loss 9.512870788574219, Classifier Loss 0.238630473613739, Total Loss 141.34080505371094\n",
      "34: Encoding Loss 15.901248931884766, Transition Loss 5.385798454284668, Classifier Loss 0.25456684827804565, Total Loss 153.74383544921875\n",
      "34: Encoding Loss 15.827985763549805, Transition Loss 3.942563533782959, Classifier Loss 0.22005054354667664, Total Loss 149.41746520996094\n",
      "34: Encoding Loss 15.293204307556152, Transition Loss 5.656027793884277, Classifier Loss 0.2245294600725174, Total Loss 145.92979431152344\n",
      "34: Encoding Loss 15.926838874816895, Transition Loss 6.711706638336182, Classifier Loss 0.2356538325548172, Total Loss 152.32244873046875\n",
      "34: Encoding Loss 16.201169967651367, Transition Loss 6.9453558921813965, Classifier Loss 0.25110670924186707, Total Loss 156.10910034179688\n",
      "34: Encoding Loss 16.143959045410156, Transition Loss 7.33342981338501, Classifier Loss 0.24072180688381195, Total Loss 154.69053649902344\n",
      "34: Encoding Loss 14.745011329650879, Transition Loss 7.642987251281738, Classifier Loss 0.2614263594150543, Total Loss 145.63133239746094\n",
      "34: Encoding Loss 15.866898536682129, Transition Loss 6.683776378631592, Classifier Loss 0.23748482763767242, Total Loss 152.02044677734375\n",
      "34: Encoding Loss 15.00163745880127, Transition Loss 8.915399551391602, Classifier Loss 0.23504483699798584, Total Loss 145.3006591796875\n",
      "34: Encoding Loss 15.785369873046875, Transition Loss 7.3230743408203125, Classifier Loss 0.24652479588985443, Total Loss 152.40005493164062\n",
      "34: Encoding Loss 15.16198444366455, Transition Loss 3.6507158279418945, Classifier Loss 0.24146974086761475, Total Loss 146.17300415039062\n",
      "34: Encoding Loss 14.6619291305542, Transition Loss 7.350484371185303, Classifier Loss 0.23555731773376465, Total Loss 142.32125854492188\n",
      "34: Encoding Loss 14.952820777893066, Transition Loss 4.716192245483398, Classifier Loss 0.2504875063896179, Total Loss 145.61456298828125\n",
      "34: Encoding Loss 15.615569114685059, Transition Loss 7.371311187744141, Classifier Loss 0.22739845514297485, Total Loss 149.13865661621094\n",
      "34: Encoding Loss 15.266836166381836, Transition Loss 9.768485069274902, Classifier Loss 0.26354578137397766, Total Loss 150.44296264648438\n",
      "34: Encoding Loss 16.64766502380371, Transition Loss 6.336289405822754, Classifier Loss 0.2272658348083496, Total Loss 157.17515563964844\n",
      "34: Encoding Loss 15.814977645874023, Transition Loss 7.950054168701172, Classifier Loss 0.22630758583545685, Total Loss 150.7406005859375\n",
      "34: Encoding Loss 14.655963897705078, Transition Loss 7.176096439361572, Classifier Loss 0.2601085305213928, Total Loss 144.69378662109375\n",
      "34: Encoding Loss 15.968911170959473, Transition Loss 5.647445201873779, Classifier Loss 0.24480782449245453, Total Loss 153.361572265625\n",
      "34: Encoding Loss 15.531177520751953, Transition Loss 4.857384204864502, Classifier Loss 0.2317272424697876, Total Loss 148.39361572265625\n",
      "34: Encoding Loss 16.013830184936523, Transition Loss 3.8753890991210938, Classifier Loss 0.24266251921653748, Total Loss 153.1519775390625\n",
      "34: Encoding Loss 16.824310302734375, Transition Loss 5.339219570159912, Classifier Loss 0.25236207246780396, Total Loss 160.89852905273438\n",
      "34: Encoding Loss 16.237133026123047, Transition Loss 7.564826488494873, Classifier Loss 0.2560260593891144, Total Loss 157.0126495361328\n",
      "34: Encoding Loss 15.477723121643066, Transition Loss 8.435994148254395, Classifier Loss 0.2450634241104126, Total Loss 150.0153350830078\n",
      "34: Encoding Loss 16.785858154296875, Transition Loss 7.479612827301025, Classifier Loss 0.23943915963172913, Total Loss 159.72669982910156\n",
      "34: Encoding Loss 16.042802810668945, Transition Loss 6.710304260253906, Classifier Loss 0.23092997074127197, Total Loss 152.77748107910156\n",
      "34: Encoding Loss 16.175565719604492, Transition Loss 6.857422351837158, Classifier Loss 0.21452631056308746, Total Loss 152.22865295410156\n",
      "34: Encoding Loss 17.01861572265625, Transition Loss 4.881587982177734, Classifier Loss 0.25113803148269653, Total Loss 162.23904418945312\n",
      "34: Encoding Loss 16.67011070251465, Transition Loss 5.455836772918701, Classifier Loss 0.23806999623775482, Total Loss 158.25904846191406\n",
      "34: Encoding Loss 16.075647354125977, Transition Loss 4.503015041351318, Classifier Loss 0.24380061030387878, Total Loss 153.88584899902344\n",
      "34: Encoding Loss 16.783164978027344, Transition Loss 7.54925537109375, Classifier Loss 0.24582040309906006, Total Loss 160.3572235107422\n",
      "34: Encoding Loss 15.20693302154541, Transition Loss 7.355851173400879, Classifier Loss 0.22422440350055695, Total Loss 145.54908752441406\n",
      "34: Encoding Loss 15.202299118041992, Transition Loss 5.7693328857421875, Classifier Loss 0.24691422283649445, Total Loss 147.46368408203125\n",
      "34: Encoding Loss 15.515316009521484, Transition Loss 5.795490741729736, Classifier Loss 0.23132677376270294, Total Loss 148.41432189941406\n",
      "34: Encoding Loss 15.179268836975098, Transition Loss 5.992481708526611, Classifier Loss 0.22201015055179596, Total Loss 144.83367919921875\n",
      "34: Encoding Loss 15.025696754455566, Transition Loss 5.112634658813477, Classifier Loss 0.2214009016752243, Total Loss 143.36819458007812\n",
      "34: Encoding Loss 15.84603214263916, Transition Loss 5.475670337677002, Classifier Loss 0.21174843609333038, Total Loss 149.0382537841797\n",
      "34: Encoding Loss 16.91182518005371, Transition Loss 7.179032325744629, Classifier Loss 0.2537206709384918, Total Loss 162.10247802734375\n",
      "34: Encoding Loss 17.37122917175293, Transition Loss 6.546767234802246, Classifier Loss 0.2562348246574402, Total Loss 165.90267944335938\n",
      "34: Encoding Loss 15.260844230651855, Transition Loss 4.473799705505371, Classifier Loss 0.2324502319097519, Total Loss 146.22653198242188\n",
      "34: Encoding Loss 15.141912460327148, Transition Loss 4.896480083465576, Classifier Loss 0.2204558253288269, Total Loss 144.16018676757812\n",
      "34: Encoding Loss 15.784421920776367, Transition Loss 6.919599533081055, Classifier Loss 0.23173873126506805, Total Loss 150.8331756591797\n",
      "34: Encoding Loss 15.200539588928223, Transition Loss 6.17500114440918, Classifier Loss 0.23156487941741943, Total Loss 145.99581909179688\n",
      "34: Encoding Loss 16.200937271118164, Transition Loss 5.736169338226318, Classifier Loss 0.234793558716774, Total Loss 154.2340850830078\n",
      "34: Encoding Loss 16.463417053222656, Transition Loss 6.129764080047607, Classifier Loss 0.2100900262594223, Total Loss 153.94229125976562\n",
      "34: Encoding Loss 15.600031852722168, Transition Loss 7.157361030578613, Classifier Loss 0.24405235052108765, Total Loss 150.636962890625\n",
      "34: Encoding Loss 15.581729888916016, Transition Loss 6.700605869293213, Classifier Loss 0.24413573741912842, Total Loss 150.40753173828125\n",
      "34: Encoding Loss 16.966703414916992, Transition Loss 6.002062797546387, Classifier Loss 0.24069911241531372, Total Loss 161.0039520263672\n",
      "34: Encoding Loss 16.037992477416992, Transition Loss 7.307406425476074, Classifier Loss 0.2276090681552887, Total Loss 152.52633666992188\n",
      "34: Encoding Loss 15.753884315490723, Transition Loss 6.623405933380127, Classifier Loss 0.2160625159740448, Total Loss 148.96200561523438\n",
      "34: Encoding Loss 16.66469383239746, Transition Loss 7.578924179077148, Classifier Loss 0.2266855686903, Total Loss 157.50189208984375\n",
      "34: Encoding Loss 14.685189247131348, Transition Loss 8.510129928588867, Classifier Loss 0.2855779528617859, Total Loss 147.74134826660156\n",
      "34: Encoding Loss 15.825011253356934, Transition Loss 6.63447380065918, Classifier Loss 0.23326969146728516, Total Loss 151.2539520263672\n",
      "34: Encoding Loss 15.355504035949707, Transition Loss 7.793940544128418, Classifier Loss 0.22363097965717316, Total Loss 146.7659149169922\n",
      "34: Encoding Loss 15.744625091552734, Transition Loss 7.768682479858398, Classifier Loss 0.25277602672576904, Total Loss 152.78836059570312\n",
      "34: Encoding Loss 15.396052360534668, Transition Loss 7.686086177825928, Classifier Loss 0.2286163866519928, Total Loss 147.56727600097656\n",
      "34: Encoding Loss 17.069400787353516, Transition Loss 6.536979675292969, Classifier Loss 0.23401522636413574, Total Loss 161.26412963867188\n",
      "34: Encoding Loss 15.448684692382812, Transition Loss 6.351240634918213, Classifier Loss 0.22441890835762024, Total Loss 147.30162048339844\n",
      "34: Encoding Loss 16.35177230834961, Transition Loss 6.518733978271484, Classifier Loss 0.25086909532546997, Total Loss 157.20484924316406\n",
      "34: Encoding Loss 15.874471664428711, Transition Loss 7.079383850097656, Classifier Loss 0.23707398772239685, Total Loss 152.1190643310547\n",
      "34: Encoding Loss 15.637603759765625, Transition Loss 7.298915863037109, Classifier Loss 0.23239915072917938, Total Loss 149.80052185058594\n",
      "34: Encoding Loss 16.790315628051758, Transition Loss 5.959856986999512, Classifier Loss 0.23930957913398743, Total Loss 159.44544982910156\n",
      "34: Encoding Loss 15.499588966369629, Transition Loss 6.636061668395996, Classifier Loss 0.20889905095100403, Total Loss 146.21383666992188\n",
      "34: Encoding Loss 15.71979808807373, Transition Loss 5.300612449645996, Classifier Loss 0.25827720761299133, Total Loss 152.64622497558594\n",
      "34: Encoding Loss 15.511930465698242, Transition Loss 8.598455429077148, Classifier Loss 0.25396275520324707, Total Loss 151.21142578125\n",
      "34: Encoding Loss 15.214123725891113, Transition Loss 5.6205010414123535, Classifier Loss 0.22461062669754028, Total Loss 145.29815673828125\n",
      "34: Encoding Loss 15.593830108642578, Transition Loss 4.865865230560303, Classifier Loss 0.23965562880039215, Total Loss 149.6893768310547\n",
      "34: Encoding Loss 16.247459411621094, Transition Loss 6.1976318359375, Classifier Loss 0.21931493282318115, Total Loss 153.15069580078125\n",
      "34: Encoding Loss 15.774523735046387, Transition Loss 8.250334739685059, Classifier Loss 0.29459667205810547, Total Loss 157.30592346191406\n",
      "34: Encoding Loss 16.119741439819336, Transition Loss 7.605612277984619, Classifier Loss 0.2345762848854065, Total Loss 153.93667602539062\n",
      "34: Encoding Loss 16.821903228759766, Transition Loss 6.104766845703125, Classifier Loss 0.20837876200675964, Total Loss 156.63404846191406\n",
      "34: Encoding Loss 16.644794464111328, Transition Loss 6.536386013031006, Classifier Loss 0.23680457472801208, Total Loss 158.14608764648438\n",
      "34: Encoding Loss 15.612055778503418, Transition Loss 4.596096992492676, Classifier Loss 0.22394976019859314, Total Loss 148.2106475830078\n",
      "34: Encoding Loss 15.911967277526855, Transition Loss 7.762795925140381, Classifier Loss 0.25794216990470886, Total Loss 154.64251708984375\n",
      "34: Encoding Loss 16.238842010498047, Transition Loss 5.703427314758301, Classifier Loss 0.2540701627731323, Total Loss 156.4584503173828\n",
      "34: Encoding Loss 15.62368106842041, Transition Loss 8.286262512207031, Classifier Loss 0.25508150458335876, Total Loss 152.1548614501953\n",
      "34: Encoding Loss 16.327856063842773, Transition Loss 4.816457271575928, Classifier Loss 0.22792358696460724, Total Loss 154.3784942626953\n",
      "34: Encoding Loss 15.847835540771484, Transition Loss 6.023421287536621, Classifier Loss 0.24114453792572021, Total Loss 152.10182189941406\n",
      "34: Encoding Loss 16.61409568786621, Transition Loss 6.269998073577881, Classifier Loss 0.27380192279815674, Total Loss 161.5469512939453\n",
      "34: Encoding Loss 15.912399291992188, Transition Loss 8.52689266204834, Classifier Loss 0.23612478375434875, Total Loss 152.61705017089844\n",
      "34: Encoding Loss 14.839083671569824, Transition Loss 7.386240005493164, Classifier Loss 0.22847312688827515, Total Loss 143.0372314453125\n",
      "34: Encoding Loss 15.442639350891113, Transition Loss 7.346745491027832, Classifier Loss 0.22636514902114868, Total Loss 147.64697265625\n",
      "34: Encoding Loss 14.89612102508545, Transition Loss 9.709799766540527, Classifier Loss 0.21445202827453613, Total Loss 142.55612182617188\n",
      "34: Encoding Loss 16.074419021606445, Transition Loss 7.0451436042785645, Classifier Loss 0.23285779356956482, Total Loss 153.2901611328125\n",
      "34: Encoding Loss 15.709914207458496, Transition Loss 7.717670917510986, Classifier Loss 0.23894639313220978, Total Loss 151.11749267578125\n",
      "34: Encoding Loss 14.326708793640137, Transition Loss 6.55375862121582, Classifier Loss 0.24135057628154755, Total Loss 140.05947875976562\n",
      "34: Encoding Loss 15.051549911499023, Transition Loss 7.64792537689209, Classifier Loss 0.2289113700389862, Total Loss 144.8331298828125\n",
      "34: Encoding Loss 15.855375289916992, Transition Loss 7.226176738739014, Classifier Loss 0.23141969740390778, Total Loss 151.4302215576172\n",
      "34: Encoding Loss 16.358699798583984, Transition Loss 7.714755535125732, Classifier Loss 0.24777677655220032, Total Loss 157.19024658203125\n",
      "34: Encoding Loss 15.556947708129883, Transition Loss 7.276007175445557, Classifier Loss 0.23140636086463928, Total Loss 149.05142211914062\n",
      "34: Encoding Loss 15.927638053894043, Transition Loss 5.805146217346191, Classifier Loss 0.2133878618478775, Total Loss 149.92091369628906\n",
      "34: Encoding Loss 15.444149017333984, Transition Loss 5.660117149353027, Classifier Loss 0.25866442918777466, Total Loss 150.55166625976562\n",
      "34: Encoding Loss 15.58293342590332, Transition Loss 7.418152809143066, Classifier Loss 0.22494448721408844, Total Loss 148.64154052734375\n",
      "34: Encoding Loss 16.572696685791016, Transition Loss 7.389467716217041, Classifier Loss 0.24622845649719238, Total Loss 158.68231201171875\n",
      "34: Encoding Loss 15.88896656036377, Transition Loss 6.811805725097656, Classifier Loss 0.237680584192276, Total Loss 152.24215698242188\n",
      "34: Encoding Loss 15.919231414794922, Transition Loss 7.171586990356445, Classifier Loss 0.22429880499839783, Total Loss 151.21804809570312\n",
      "34: Encoding Loss 16.070310592651367, Transition Loss 6.196113586425781, Classifier Loss 0.23255020380020142, Total Loss 153.05673217773438\n",
      "34: Encoding Loss 16.700395584106445, Transition Loss 5.002473831176758, Classifier Loss 0.23657873272895813, Total Loss 158.2615203857422\n",
      "34: Encoding Loss 16.100311279296875, Transition Loss 5.133898735046387, Classifier Loss 0.21853557229042053, Total Loss 151.68283081054688\n",
      "34: Encoding Loss 16.159025192260742, Transition Loss 7.393826007843018, Classifier Loss 0.22529909014701843, Total Loss 153.28086853027344\n",
      "34: Encoding Loss 17.021711349487305, Transition Loss 5.139919281005859, Classifier Loss 0.23654986917972565, Total Loss 160.85665893554688\n",
      "34: Encoding Loss 17.352144241333008, Transition Loss 5.324845314025879, Classifier Loss 0.25464677810668945, Total Loss 165.3468017578125\n",
      "34: Encoding Loss 15.082184791564941, Transition Loss 7.515695571899414, Classifier Loss 0.22488418221473694, Total Loss 144.6490478515625\n",
      "34: Encoding Loss 14.802827835083008, Transition Loss 7.769444942474365, Classifier Loss 0.2122247815132141, Total Loss 141.19898986816406\n",
      "34: Encoding Loss 15.261430740356445, Transition Loss 6.966026782989502, Classifier Loss 0.25800415873527527, Total Loss 149.28506469726562\n",
      "34: Encoding Loss 15.277262687683105, Transition Loss 8.206931114196777, Classifier Loss 0.24585425853729248, Total Loss 148.44491577148438\n",
      "34: Encoding Loss 15.053114891052246, Transition Loss 9.952069282531738, Classifier Loss 0.24437066912651062, Total Loss 146.85240173339844\n",
      "34: Encoding Loss 16.786352157592773, Transition Loss 6.6449480056762695, Classifier Loss 0.24212732911109924, Total Loss 159.83255004882812\n",
      "34: Encoding Loss 16.729564666748047, Transition Loss 6.2915191650390625, Classifier Loss 0.21929208934307098, Total Loss 157.0240478515625\n",
      "34: Encoding Loss 15.040127754211426, Transition Loss 5.792329788208008, Classifier Loss 0.21670247614383698, Total Loss 143.1497344970703\n",
      "34: Encoding Loss 15.535283088684082, Transition Loss 5.579714775085449, Classifier Loss 0.24007847905158997, Total Loss 149.4060516357422\n",
      "34: Encoding Loss 15.568988800048828, Transition Loss 6.329808235168457, Classifier Loss 0.23542483150959015, Total Loss 149.3603515625\n",
      "34: Encoding Loss 16.07411003112793, Transition Loss 7.351215839385986, Classifier Loss 0.22825562953948975, Total Loss 152.88868713378906\n",
      "34: Encoding Loss 16.045978546142578, Transition Loss 5.704372882843018, Classifier Loss 0.22470690310001373, Total Loss 151.97938537597656\n",
      "34: Encoding Loss 16.061424255371094, Transition Loss 6.684087753295898, Classifier Loss 0.2537752389907837, Total Loss 155.2057342529297\n",
      "34: Encoding Loss 15.235466957092285, Transition Loss 5.083616256713867, Classifier Loss 0.20042507350444794, Total Loss 142.94297790527344\n",
      "34: Encoding Loss 16.100866317749023, Transition Loss 7.197463035583496, Classifier Loss 0.20402848720550537, Total Loss 150.64927673339844\n",
      "34: Encoding Loss 16.114839553833008, Transition Loss 4.7649431228637695, Classifier Loss 0.2595735788345337, Total Loss 155.8290557861328\n",
      "34: Encoding Loss 15.242130279541016, Transition Loss 8.394335746765137, Classifier Loss 0.24030649662017822, Total Loss 147.64654541015625\n",
      "34: Encoding Loss 15.926055908203125, Transition Loss 6.08319091796875, Classifier Loss 0.24255801737308502, Total Loss 152.88088989257812\n",
      "34: Encoding Loss 16.070253372192383, Transition Loss 7.906986713409424, Classifier Loss 0.2407115250825882, Total Loss 154.21456909179688\n",
      "34: Encoding Loss 15.938986778259277, Transition Loss 6.157702922821045, Classifier Loss 0.2374478280544281, Total Loss 152.48822021484375\n",
      "34: Encoding Loss 15.451823234558105, Transition Loss 7.327059268951416, Classifier Loss 0.23970182240009308, Total Loss 149.0501708984375\n",
      "34: Encoding Loss 17.09950828552246, Transition Loss 4.8532819747924805, Classifier Loss 0.2388203740119934, Total Loss 161.6487579345703\n",
      "34: Encoding Loss 16.49848175048828, Transition Loss 4.704649925231934, Classifier Loss 0.23172146081924438, Total Loss 156.10093688964844\n",
      "34: Encoding Loss 15.6392822265625, Transition Loss 5.244541168212891, Classifier Loss 0.22581398487091064, Total Loss 148.7445526123047\n",
      "34: Encoding Loss 15.384089469909668, Transition Loss 7.310030937194824, Classifier Loss 0.24326829612255096, Total Loss 148.86155700683594\n",
      "34: Encoding Loss 15.602892875671387, Transition Loss 3.9960694313049316, Classifier Loss 0.23757559061050415, Total Loss 149.37991333007812\n",
      "34: Encoding Loss 16.098175048828125, Transition Loss 9.155019760131836, Classifier Loss 0.23776060342788696, Total Loss 154.39247131347656\n",
      "34: Encoding Loss 16.464235305786133, Transition Loss 5.78245210647583, Classifier Loss 0.22696106135845184, Total Loss 155.5664825439453\n",
      "34: Encoding Loss 17.141197204589844, Transition Loss 7.329404354095459, Classifier Loss 0.23669160902500153, Total Loss 162.26461791992188\n",
      "34: Encoding Loss 16.64244270324707, Transition Loss 7.185513019561768, Classifier Loss 0.22953461110591888, Total Loss 157.5301055908203\n",
      "34: Encoding Loss 15.291183471679688, Transition Loss 10.374497413635254, Classifier Loss 0.22251510620117188, Total Loss 146.6558837890625\n",
      "34: Encoding Loss 13.110478401184082, Transition Loss 9.657100677490234, Classifier Loss 0.21425801515579224, Total Loss 128.24105834960938\n",
      "35: Encoding Loss 17.03636932373047, Transition Loss 6.22152042388916, Classifier Loss 0.22849181294441223, Total Loss 160.3844451904297\n",
      "35: Encoding Loss 16.374237060546875, Transition Loss 5.413432598114014, Classifier Loss 0.23283527791500092, Total Loss 155.360107421875\n",
      "35: Encoding Loss 16.363239288330078, Transition Loss 6.5778021812438965, Classifier Loss 0.24558620154857635, Total Loss 156.7801055908203\n",
      "35: Encoding Loss 15.89197826385498, Transition Loss 6.395179748535156, Classifier Loss 0.22810709476470947, Total Loss 151.22557067871094\n",
      "35: Encoding Loss 17.579072952270508, Transition Loss 6.773995399475098, Classifier Loss 0.24570848047733307, Total Loss 166.5582275390625\n",
      "35: Encoding Loss 14.166691780090332, Transition Loss 6.843128204345703, Classifier Loss 0.23514439165592194, Total Loss 138.2165985107422\n",
      "35: Encoding Loss 17.042987823486328, Transition Loss 5.847465991973877, Classifier Loss 0.2531728148460388, Total Loss 162.83067321777344\n",
      "35: Encoding Loss 17.230852127075195, Transition Loss 6.452719211578369, Classifier Loss 0.24524298310279846, Total Loss 163.66165161132812\n",
      "35: Encoding Loss 15.734338760375977, Transition Loss 6.2497711181640625, Classifier Loss 0.3073345720767975, Total Loss 157.85812377929688\n",
      "35: Encoding Loss 16.597288131713867, Transition Loss 6.458822727203369, Classifier Loss 0.25864866375923157, Total Loss 159.9349365234375\n",
      "35: Encoding Loss 16.154338836669922, Transition Loss 5.396660804748535, Classifier Loss 0.2189370095729828, Total Loss 152.207763671875\n",
      "35: Encoding Loss 15.850969314575195, Transition Loss 6.317647457122803, Classifier Loss 0.24725091457366943, Total Loss 152.79638671875\n",
      "35: Encoding Loss 15.837327003479004, Transition Loss 6.860299110412598, Classifier Loss 0.23578494787216187, Total Loss 151.649169921875\n",
      "35: Encoding Loss 15.014341354370117, Transition Loss 5.796938419342041, Classifier Loss 0.22600772976875305, Total Loss 143.87490844726562\n",
      "35: Encoding Loss 15.242685317993164, Transition Loss 6.992426872253418, Classifier Loss 0.24211189150810242, Total Loss 147.55116271972656\n",
      "35: Encoding Loss 16.02951431274414, Transition Loss 8.138920783996582, Classifier Loss 0.22735783457756042, Total Loss 152.59967041015625\n",
      "35: Encoding Loss 15.962469100952148, Transition Loss 6.577267646789551, Classifier Loss 0.22188034653663635, Total Loss 151.2032470703125\n",
      "35: Encoding Loss 15.344225883483887, Transition Loss 7.769825458526611, Classifier Loss 0.21413128077983856, Total Loss 145.7209014892578\n",
      "35: Encoding Loss 16.58668327331543, Transition Loss 7.617398262023926, Classifier Loss 0.21540208160877228, Total Loss 155.7571563720703\n",
      "35: Encoding Loss 16.287425994873047, Transition Loss 7.700497627258301, Classifier Loss 0.2196398377418518, Total Loss 153.8035125732422\n",
      "35: Encoding Loss 15.809381484985352, Transition Loss 7.96158504486084, Classifier Loss 0.24789643287658691, Total Loss 152.8570098876953\n",
      "35: Encoding Loss 15.887090682983398, Transition Loss 6.161800384521484, Classifier Loss 0.22064195573329926, Total Loss 150.39329528808594\n",
      "35: Encoding Loss 14.346816062927246, Transition Loss 10.667580604553223, Classifier Loss 0.22477275133132935, Total Loss 139.38531494140625\n",
      "35: Encoding Loss 15.771496772766113, Transition Loss 6.0003581047058105, Classifier Loss 0.238944411277771, Total Loss 151.26649475097656\n",
      "35: Encoding Loss 15.725937843322754, Transition Loss 4.517446994781494, Classifier Loss 0.21628153324127197, Total Loss 148.3391571044922\n",
      "35: Encoding Loss 15.229912757873535, Transition Loss 6.41317892074585, Classifier Loss 0.2165219783782959, Total Loss 144.77415466308594\n",
      "35: Encoding Loss 15.892590522766113, Transition Loss 7.483598709106445, Classifier Loss 0.22890889644622803, Total Loss 151.52833557128906\n",
      "35: Encoding Loss 16.126123428344727, Transition Loss 7.739377975463867, Classifier Loss 0.23768267035484314, Total Loss 154.32513427734375\n",
      "35: Encoding Loss 16.057966232299805, Transition Loss 8.182204246520996, Classifier Loss 0.2369135022163391, Total Loss 153.79151916503906\n",
      "35: Encoding Loss 14.613815307617188, Transition Loss 8.491061210632324, Classifier Loss 0.2434343695640564, Total Loss 142.95217895507812\n",
      "35: Encoding Loss 15.76430892944336, Transition Loss 7.286877632141113, Classifier Loss 0.2271079123020172, Total Loss 150.28265380859375\n",
      "35: Encoding Loss 14.910604476928711, Transition Loss 9.709100723266602, Classifier Loss 0.21666492521762848, Total Loss 142.89315795898438\n",
      "35: Encoding Loss 15.771142959594727, Transition Loss 7.94728946685791, Classifier Loss 0.2283918857574463, Total Loss 150.59779357910156\n",
      "35: Encoding Loss 15.03488540649414, Transition Loss 3.9556612968444824, Classifier Loss 0.2476458102464676, Total Loss 145.8347930908203\n",
      "35: Encoding Loss 14.615220069885254, Transition Loss 7.984801292419434, Classifier Loss 0.23211148381233215, Total Loss 141.72987365722656\n",
      "35: Encoding Loss 14.850713729858398, Transition Loss 5.116824626922607, Classifier Loss 0.24229273200035095, Total Loss 144.058349609375\n",
      "35: Encoding Loss 15.535847663879395, Transition Loss 8.268259048461914, Classifier Loss 0.22017087042331696, Total Loss 147.95751953125\n",
      "35: Encoding Loss 15.273381233215332, Transition Loss 10.43455696105957, Classifier Loss 0.24204222857952118, Total Loss 148.47817993164062\n",
      "35: Encoding Loss 16.592721939086914, Transition Loss 7.118524074554443, Classifier Loss 0.2285265475511551, Total Loss 157.0181427001953\n",
      "35: Encoding Loss 15.7258939743042, Transition Loss 8.674131393432617, Classifier Loss 0.229020357131958, Total Loss 150.4440155029297\n",
      "35: Encoding Loss 14.569104194641113, Transition Loss 8.182047843933105, Classifier Loss 0.25626611709594727, Total Loss 143.81585693359375\n",
      "35: Encoding Loss 15.86058521270752, Transition Loss 6.090950965881348, Classifier Loss 0.2389489859342575, Total Loss 151.99777221679688\n",
      "35: Encoding Loss 15.46009349822998, Transition Loss 5.499151229858398, Classifier Loss 0.22583626210689545, Total Loss 147.36419677734375\n",
      "35: Encoding Loss 15.911312103271484, Transition Loss 4.304515838623047, Classifier Loss 0.22744102776050568, Total Loss 150.8955078125\n",
      "35: Encoding Loss 16.775869369506836, Transition Loss 6.011593341827393, Classifier Loss 0.2350589632987976, Total Loss 158.9151611328125\n",
      "35: Encoding Loss 16.22109031677246, Transition Loss 8.215842247009277, Classifier Loss 0.24665915966033936, Total Loss 156.0778045654297\n",
      "35: Encoding Loss 15.456685066223145, Transition Loss 9.306607246398926, Classifier Loss 0.23893611133098602, Total Loss 149.40841674804688\n",
      "35: Encoding Loss 16.734580993652344, Transition Loss 7.955620765686035, Classifier Loss 0.22997108101844788, Total Loss 158.46487426757812\n",
      "35: Encoding Loss 15.993932723999023, Transition Loss 7.360544204711914, Classifier Loss 0.21819020807743073, Total Loss 151.2425994873047\n",
      "35: Encoding Loss 16.110742568969727, Transition Loss 7.3851823806762695, Classifier Loss 0.21592336893081665, Total Loss 151.95530700683594\n",
      "35: Encoding Loss 16.8960018157959, Transition Loss 5.3958740234375, Classifier Loss 0.2342788577079773, Total Loss 159.67507934570312\n",
      "35: Encoding Loss 16.61624526977539, Transition Loss 5.844881534576416, Classifier Loss 0.23530204594135284, Total Loss 157.629150390625\n",
      "35: Encoding Loss 15.961010932922363, Transition Loss 4.996913909912109, Classifier Loss 0.22196896374225616, Total Loss 150.88436889648438\n",
      "35: Encoding Loss 16.684791564941406, Transition Loss 8.169045448303223, Classifier Loss 0.23069171607494354, Total Loss 158.18130493164062\n",
      "35: Encoding Loss 15.160164833068848, Transition Loss 8.066259384155273, Classifier Loss 0.21818807721138, Total Loss 144.71337890625\n",
      "35: Encoding Loss 15.100380897521973, Transition Loss 6.215507507324219, Classifier Loss 0.2456725537776947, Total Loss 146.61341857910156\n",
      "35: Encoding Loss 15.446002006530762, Transition Loss 6.3217597007751465, Classifier Loss 0.22692564129829407, Total Loss 147.52493286132812\n",
      "35: Encoding Loss 15.091373443603516, Transition Loss 6.383627414703369, Classifier Loss 0.21482723951339722, Total Loss 143.49041748046875\n",
      "35: Encoding Loss 14.947037696838379, Transition Loss 5.5983076095581055, Classifier Loss 0.21463046967983246, Total Loss 142.1590118408203\n",
      "35: Encoding Loss 15.766157150268555, Transition Loss 5.859552383422852, Classifier Loss 0.21228884160518646, Total Loss 148.53005981445312\n",
      "35: Encoding Loss 16.873018264770508, Transition Loss 7.660652160644531, Classifier Loss 0.2497888058423996, Total Loss 161.4951629638672\n",
      "35: Encoding Loss 17.37572479248047, Transition Loss 6.994672775268555, Classifier Loss 0.24440884590148926, Total Loss 164.8456268310547\n",
      "35: Encoding Loss 15.171512603759766, Transition Loss 4.871723175048828, Classifier Loss 0.23297527432441711, Total Loss 145.6439666748047\n",
      "35: Encoding Loss 15.05392837524414, Transition Loss 5.369232654571533, Classifier Loss 0.22047631442546844, Total Loss 143.5529022216797\n",
      "35: Encoding Loss 15.710810661315918, Transition Loss 7.42868185043335, Classifier Loss 0.2276846468448639, Total Loss 149.94068908691406\n",
      "35: Encoding Loss 15.147799491882324, Transition Loss 6.7628374099731445, Classifier Loss 0.21437197923660278, Total Loss 143.97216796875\n",
      "35: Encoding Loss 16.094402313232422, Transition Loss 6.1176910400390625, Classifier Loss 0.234622523188591, Total Loss 153.44102478027344\n",
      "35: Encoding Loss 16.4477481842041, Transition Loss 6.852644920349121, Classifier Loss 0.2001391500234604, Total Loss 152.9664306640625\n",
      "35: Encoding Loss 15.607743263244629, Transition Loss 7.410480976104736, Classifier Loss 0.23776474595069885, Total Loss 150.1205291748047\n",
      "35: Encoding Loss 15.491205215454102, Transition Loss 7.440774917602539, Classifier Loss 0.24673031270503998, Total Loss 150.09083557128906\n",
      "35: Encoding Loss 16.931358337402344, Transition Loss 6.232173442840576, Classifier Loss 0.22749027609825134, Total Loss 159.44631958007812\n",
      "35: Encoding Loss 15.937036514282227, Transition Loss 8.172856330871582, Classifier Loss 0.218027263879776, Total Loss 150.93357849121094\n",
      "35: Encoding Loss 15.674274444580078, Transition Loss 7.206322193145752, Classifier Loss 0.21049724519252777, Total Loss 147.8851776123047\n",
      "35: Encoding Loss 16.644855499267578, Transition Loss 8.078542709350586, Classifier Loss 0.22070378065109253, Total Loss 156.8449249267578\n",
      "35: Encoding Loss 14.652474403381348, Transition Loss 9.08454418182373, Classifier Loss 0.2744443416595459, Total Loss 146.4811553955078\n",
      "35: Encoding Loss 15.820707321166992, Transition Loss 6.989720821380615, Classifier Loss 0.22575868666172028, Total Loss 150.53948974609375\n",
      "35: Encoding Loss 15.34814739227295, Transition Loss 8.407341957092285, Classifier Loss 0.21692082285881042, Total Loss 146.1587371826172\n",
      "35: Encoding Loss 15.692712783813477, Transition Loss 8.112592697143555, Classifier Loss 0.24782295525074005, Total Loss 151.94651794433594\n",
      "35: Encoding Loss 15.363387107849121, Transition Loss 8.319091796875, Classifier Loss 0.21755903959274292, Total Loss 146.32681274414062\n",
      "35: Encoding Loss 17.011629104614258, Transition Loss 6.826419830322266, Classifier Loss 0.23107904195785522, Total Loss 160.56622314453125\n",
      "35: Encoding Loss 15.383001327514648, Transition Loss 6.966200351715088, Classifier Loss 0.2184199094772339, Total Loss 146.2992401123047\n",
      "35: Encoding Loss 16.28350067138672, Transition Loss 6.69423770904541, Classifier Loss 0.2459539920091629, Total Loss 156.20225524902344\n",
      "35: Encoding Loss 15.822784423828125, Transition Loss 7.721821308135986, Classifier Loss 0.2376241236925125, Total Loss 151.88905334472656\n",
      "35: Encoding Loss 15.622014999389648, Transition Loss 7.642185211181641, Classifier Loss 0.22739702463150024, Total Loss 149.24427795410156\n",
      "35: Encoding Loss 16.73794174194336, Transition Loss 6.579123497009277, Classifier Loss 0.2212921380996704, Total Loss 157.3485870361328\n",
      "35: Encoding Loss 15.392674446105957, Transition Loss 7.173565864562988, Classifier Loss 0.20815008878707886, Total Loss 145.39111328125\n",
      "35: Encoding Loss 15.651102066040039, Transition Loss 5.849005222320557, Classifier Loss 0.23544764518737793, Total Loss 149.9233856201172\n",
      "35: Encoding Loss 15.482210159301758, Transition Loss 8.977535247802734, Classifier Loss 0.24827037751674652, Total Loss 150.480224609375\n",
      "35: Encoding Loss 15.178464889526367, Transition Loss 6.125951766967773, Classifier Loss 0.22640085220336914, Total Loss 145.29299926757812\n",
      "35: Encoding Loss 15.548490524291992, Transition Loss 5.25557279586792, Classifier Loss 0.22764481604099274, Total Loss 148.2035369873047\n",
      "35: Encoding Loss 16.1737117767334, Transition Loss 6.884836673736572, Classifier Loss 0.20065274834632874, Total Loss 150.83193969726562\n",
      "35: Encoding Loss 15.787294387817383, Transition Loss 8.907851219177246, Classifier Loss 0.2831985652446747, Total Loss 156.3997802734375\n",
      "35: Encoding Loss 16.02300262451172, Transition Loss 8.034261703491211, Classifier Loss 0.22925323247909546, Total Loss 152.71620178222656\n",
      "35: Encoding Loss 16.769895553588867, Transition Loss 6.39467716217041, Classifier Loss 0.20485050976276398, Total Loss 155.9231414794922\n",
      "35: Encoding Loss 16.597084045410156, Transition Loss 7.040790557861328, Classifier Loss 0.2143055647611618, Total Loss 155.61538696289062\n",
      "35: Encoding Loss 15.496891975402832, Transition Loss 4.8658833503723145, Classifier Loss 0.2214367389678955, Total Loss 147.09197998046875\n",
      "35: Encoding Loss 15.865914344787598, Transition Loss 8.288617134094238, Classifier Loss 0.2539405822753906, Total Loss 153.97911071777344\n",
      "35: Encoding Loss 16.18392562866211, Transition Loss 6.092119216918945, Classifier Loss 0.24475573003292084, Total Loss 155.16542053222656\n",
      "35: Encoding Loss 15.589349746704102, Transition Loss 9.014883041381836, Classifier Loss 0.23296856880187988, Total Loss 149.81463623046875\n",
      "35: Encoding Loss 16.25417709350586, Transition Loss 5.285102367401123, Classifier Loss 0.22091545164585114, Total Loss 153.18199157714844\n",
      "35: Encoding Loss 15.814142227172852, Transition Loss 6.498269557952881, Classifier Loss 0.2233702540397644, Total Loss 150.14981079101562\n",
      "35: Encoding Loss 16.53551483154297, Transition Loss 6.884127616882324, Classifier Loss 0.26541823148727417, Total Loss 160.20277404785156\n",
      "35: Encoding Loss 15.854948997497559, Transition Loss 9.2935152053833, Classifier Loss 0.22760418057441711, Total Loss 151.45870971679688\n",
      "35: Encoding Loss 14.762502670288086, Transition Loss 7.872010707855225, Classifier Loss 0.21114996075630188, Total Loss 140.7894287109375\n",
      "35: Encoding Loss 15.426895141601562, Transition Loss 7.7130446434021, Classifier Loss 0.22129902243614197, Total Loss 147.08766174316406\n",
      "35: Encoding Loss 14.822427749633789, Transition Loss 10.54491138458252, Classifier Loss 0.20541958510875702, Total Loss 141.23036193847656\n",
      "35: Encoding Loss 15.984705924987793, Transition Loss 7.491840362548828, Classifier Loss 0.22464147210121155, Total Loss 151.8401641845703\n",
      "35: Encoding Loss 15.662607192993164, Transition Loss 8.257457733154297, Classifier Loss 0.23263873159885406, Total Loss 150.21621704101562\n",
      "35: Encoding Loss 14.226298332214355, Transition Loss 7.031188488006592, Classifier Loss 0.22313430905342102, Total Loss 137.53004455566406\n",
      "35: Encoding Loss 15.007153511047363, Transition Loss 8.304336547851562, Classifier Loss 0.22161538898944855, Total Loss 143.879638671875\n",
      "35: Encoding Loss 15.84807300567627, Transition Loss 7.693176746368408, Classifier Loss 0.23885998129844666, Total Loss 152.20921325683594\n",
      "35: Encoding Loss 16.361522674560547, Transition Loss 8.22406005859375, Classifier Loss 0.24491730332374573, Total Loss 157.0287322998047\n",
      "35: Encoding Loss 15.536157608032227, Transition Loss 7.583226680755615, Classifier Loss 0.21698741614818573, Total Loss 147.50465393066406\n",
      "35: Encoding Loss 15.91341781616211, Transition Loss 6.216037750244141, Classifier Loss 0.21646615862846375, Total Loss 150.19717407226562\n",
      "35: Encoding Loss 15.367937088012695, Transition Loss 5.925269603729248, Classifier Loss 0.24688078463077545, Total Loss 148.81663513183594\n",
      "35: Encoding Loss 15.595693588256836, Transition Loss 7.886708736419678, Classifier Loss 0.21837209165096283, Total Loss 148.18011474609375\n",
      "35: Encoding Loss 16.51416015625, Transition Loss 7.714269161224365, Classifier Loss 0.22747103869915009, Total Loss 156.40322875976562\n",
      "35: Encoding Loss 15.831635475158691, Transition Loss 7.316810131072998, Classifier Loss 0.23758564889431, Total Loss 151.87501525878906\n",
      "35: Encoding Loss 15.825821876525879, Transition Loss 7.5794830322265625, Classifier Loss 0.22117507457733154, Total Loss 150.239990234375\n",
      "35: Encoding Loss 16.05170440673828, Transition Loss 6.5942511558532715, Classifier Loss 0.23032566905021667, Total Loss 152.76504516601562\n",
      "35: Encoding Loss 16.60109519958496, Transition Loss 5.299976348876953, Classifier Loss 0.22755280137062073, Total Loss 156.62403869628906\n",
      "35: Encoding Loss 16.063871383666992, Transition Loss 5.424493312835693, Classifier Loss 0.22920377552509308, Total Loss 152.51625061035156\n",
      "35: Encoding Loss 16.13055992126465, Transition Loss 7.6723432540893555, Classifier Loss 0.22093385457992554, Total Loss 152.67233276367188\n",
      "35: Encoding Loss 16.954431533813477, Transition Loss 5.270500183105469, Classifier Loss 0.2356034219264984, Total Loss 160.24990844726562\n",
      "35: Encoding Loss 17.28700828552246, Transition Loss 5.480406761169434, Classifier Loss 0.2461879551410675, Total Loss 164.0109405517578\n",
      "35: Encoding Loss 15.011090278625488, Transition Loss 7.801595687866211, Classifier Loss 0.222374826669693, Total Loss 143.8865203857422\n",
      "35: Encoding Loss 14.794880867004395, Transition Loss 8.048115730285645, Classifier Loss 0.21068435907363892, Total Loss 141.037109375\n",
      "35: Encoding Loss 15.23342227935791, Transition Loss 7.265334129333496, Classifier Loss 0.2434561550617218, Total Loss 147.6660614013672\n",
      "35: Encoding Loss 15.272419929504395, Transition Loss 8.523459434509277, Classifier Loss 0.23708173632621765, Total Loss 147.59222412109375\n",
      "35: Encoding Loss 15.057562828063965, Transition Loss 10.463103294372559, Classifier Loss 0.22934198379516602, Total Loss 145.48731994628906\n",
      "35: Encoding Loss 16.734895706176758, Transition Loss 7.006424903869629, Classifier Loss 0.23356816172599792, Total Loss 158.63726806640625\n",
      "35: Encoding Loss 16.679155349731445, Transition Loss 6.469117164611816, Classifier Loss 0.1983063817024231, Total Loss 154.55770874023438\n",
      "35: Encoding Loss 14.96713638305664, Transition Loss 6.017945289611816, Classifier Loss 0.21082909405231476, Total Loss 142.02357482910156\n",
      "35: Encoding Loss 15.461319923400879, Transition Loss 5.8069748878479, Classifier Loss 0.23791760206222534, Total Loss 148.6437225341797\n",
      "35: Encoding Loss 15.515045166015625, Transition Loss 6.694395065307617, Classifier Loss 0.22886672616004944, Total Loss 148.34591674804688\n",
      "35: Encoding Loss 15.993168830871582, Transition Loss 7.463260650634766, Classifier Loss 0.22133053839206696, Total Loss 151.571044921875\n",
      "35: Encoding Loss 15.926774024963379, Transition Loss 6.083491325378418, Classifier Loss 0.21497029066085815, Total Loss 150.1279296875\n",
      "35: Encoding Loss 15.986559867858887, Transition Loss 6.928407669067383, Classifier Loss 0.25601306557655334, Total Loss 154.8794708251953\n",
      "35: Encoding Loss 15.162580490112305, Transition Loss 5.473855972290039, Classifier Loss 0.19641795754432678, Total Loss 142.03721618652344\n",
      "35: Encoding Loss 15.997112274169922, Transition Loss 7.493583679199219, Classifier Loss 0.20079374313354492, Total Loss 149.55499267578125\n",
      "35: Encoding Loss 16.069856643676758, Transition Loss 5.145785331726074, Classifier Loss 0.2559313178062439, Total Loss 155.18113708496094\n",
      "35: Encoding Loss 15.209945678710938, Transition Loss 8.612354278564453, Classifier Loss 0.24918542802333832, Total Loss 148.32058715820312\n",
      "35: Encoding Loss 15.894072532653809, Transition Loss 6.485215663909912, Classifier Loss 0.2397993803024292, Total Loss 152.4295654296875\n",
      "35: Encoding Loss 16.040225982666016, Transition Loss 8.021985054016113, Classifier Loss 0.24577400088310242, Total Loss 154.50360107421875\n",
      "35: Encoding Loss 15.876806259155273, Transition Loss 6.5814290046691895, Classifier Loss 0.22931192815303802, Total Loss 151.26193237304688\n",
      "35: Encoding Loss 15.381771087646484, Transition Loss 7.491555690765381, Classifier Loss 0.22693482041358948, Total Loss 147.24595642089844\n",
      "35: Encoding Loss 17.072053909301758, Transition Loss 5.197616100311279, Classifier Loss 0.22879232466220856, Total Loss 160.49517822265625\n",
      "35: Encoding Loss 16.425710678100586, Transition Loss 4.9013824462890625, Classifier Loss 0.22710533440113068, Total Loss 155.0964813232422\n",
      "35: Encoding Loss 15.584651947021484, Transition Loss 5.676232814788818, Classifier Loss 0.21211674809455872, Total Loss 147.02415466308594\n",
      "35: Encoding Loss 15.326629638671875, Transition Loss 7.364138603210449, Classifier Loss 0.22666804492473602, Total Loss 146.75267028808594\n",
      "35: Encoding Loss 15.458056449890137, Transition Loss 4.458423614501953, Classifier Loss 0.22721190750598907, Total Loss 147.27732849121094\n",
      "35: Encoding Loss 16.05944061279297, Transition Loss 9.325977325439453, Classifier Loss 0.2374953031539917, Total Loss 154.09024047851562\n",
      "35: Encoding Loss 16.39238166809082, Transition Loss 6.377220153808594, Classifier Loss 0.2291521579027176, Total Loss 155.3297119140625\n",
      "35: Encoding Loss 17.102956771850586, Transition Loss 7.15139627456665, Classifier Loss 0.23538605868816376, Total Loss 161.79254150390625\n",
      "35: Encoding Loss 16.656007766723633, Transition Loss 8.168094635009766, Classifier Loss 0.2297818660736084, Total Loss 157.85987854003906\n",
      "35: Encoding Loss 15.284537315368652, Transition Loss 9.97522258758545, Classifier Loss 0.20805341005325317, Total Loss 145.07667541503906\n",
      "35: Encoding Loss 13.061625480651855, Transition Loss 11.482223510742188, Classifier Loss 0.221281498670578, Total Loss 128.9176025390625\n",
      "36: Encoding Loss 16.971338272094727, Transition Loss 5.561675071716309, Classifier Loss 0.21802739799022675, Total Loss 158.68577575683594\n",
      "36: Encoding Loss 16.313589096069336, Transition Loss 6.632500648498535, Classifier Loss 0.2263508141040802, Total Loss 154.47030639648438\n",
      "36: Encoding Loss 16.294885635375977, Transition Loss 6.019818305969238, Classifier Loss 0.22833286225795746, Total Loss 154.39633178710938\n",
      "36: Encoding Loss 15.845178604125977, Transition Loss 7.6534833908081055, Classifier Loss 0.2276475429534912, Total Loss 151.056884765625\n",
      "36: Encoding Loss 17.531944274902344, Transition Loss 5.9377593994140625, Classifier Loss 0.23629067838191986, Total Loss 165.07217407226562\n",
      "36: Encoding Loss 14.09185791015625, Transition Loss 7.85756778717041, Classifier Loss 0.24292299151420593, Total Loss 138.5986785888672\n",
      "36: Encoding Loss 16.98982048034668, Transition Loss 5.665891647338867, Classifier Loss 0.23706045746803284, Total Loss 160.75778198242188\n",
      "36: Encoding Loss 17.232440948486328, Transition Loss 7.252890110015869, Classifier Loss 0.24316151440143585, Total Loss 163.62625122070312\n",
      "36: Encoding Loss 15.731817245483398, Transition Loss 6.389912128448486, Classifier Loss 0.2989062964916229, Total Loss 157.02316284179688\n",
      "36: Encoding Loss 16.642436981201172, Transition Loss 7.003411293029785, Classifier Loss 0.24097198247909546, Total Loss 158.63739013671875\n",
      "36: Encoding Loss 16.099266052246094, Transition Loss 5.5441060066223145, Classifier Loss 0.2200859934091568, Total Loss 151.91156005859375\n",
      "36: Encoding Loss 15.800139427185059, Transition Loss 6.5485758781433105, Classifier Loss 0.2581685781478882, Total Loss 153.52767944335938\n",
      "36: Encoding Loss 15.73326301574707, Transition Loss 6.840102195739746, Classifier Loss 0.22278138995170593, Total Loss 149.51226806640625\n",
      "36: Encoding Loss 14.998149871826172, Transition Loss 6.043922424316406, Classifier Loss 0.21388274431228638, Total Loss 142.58226013183594\n",
      "36: Encoding Loss 15.168420791625977, Transition Loss 7.246033668518066, Classifier Loss 0.2392910122871399, Total Loss 146.72567749023438\n",
      "36: Encoding Loss 15.999540328979492, Transition Loss 8.46536636352539, Classifier Loss 0.22674912214279175, Total Loss 152.3643035888672\n",
      "36: Encoding Loss 15.893220901489258, Transition Loss 6.827522277832031, Classifier Loss 0.2305237054824829, Total Loss 151.5636444091797\n",
      "36: Encoding Loss 15.33108139038086, Transition Loss 8.01899528503418, Classifier Loss 0.22563664615154266, Total Loss 146.81613159179688\n",
      "36: Encoding Loss 16.50725555419922, Transition Loss 7.860401153564453, Classifier Loss 0.21154159307479858, Total Loss 154.78428649902344\n",
      "36: Encoding Loss 16.27454948425293, Transition Loss 7.900180816650391, Classifier Loss 0.22266897559165955, Total Loss 154.04331970214844\n",
      "36: Encoding Loss 15.727685928344727, Transition Loss 8.204244613647461, Classifier Loss 0.23936040699481964, Total Loss 151.39837646484375\n",
      "36: Encoding Loss 15.832379341125488, Transition Loss 6.3406982421875, Classifier Loss 0.20969119668006897, Total Loss 148.89630126953125\n",
      "36: Encoding Loss 14.347027778625488, Transition Loss 10.950775146484375, Classifier Loss 0.21715371310710907, Total Loss 138.68174743652344\n",
      "36: Encoding Loss 15.735581398010254, Transition Loss 6.221764087677002, Classifier Loss 0.24940451979637146, Total Loss 152.0694580078125\n",
      "36: Encoding Loss 15.688618659973145, Transition Loss 4.481357097625732, Classifier Loss 0.2110130935907364, Total Loss 147.50653076171875\n",
      "36: Encoding Loss 15.282867431640625, Transition Loss 6.555865287780762, Classifier Loss 0.2123715728521347, Total Loss 144.81126403808594\n",
      "36: Encoding Loss 15.871389389038086, Transition Loss 7.698153495788574, Classifier Loss 0.21480019390583038, Total Loss 149.9907684326172\n",
      "36: Encoding Loss 16.088886260986328, Transition Loss 8.015663146972656, Classifier Loss 0.22772914171218872, Total Loss 153.08714294433594\n",
      "36: Encoding Loss 16.011632919311523, Transition Loss 8.352437019348145, Classifier Loss 0.21589188277721405, Total Loss 151.3527374267578\n",
      "36: Encoding Loss 14.65738296508789, Transition Loss 8.722661972045898, Classifier Loss 0.23714850842952728, Total Loss 142.71844482421875\n",
      "36: Encoding Loss 15.749252319335938, Transition Loss 7.481401443481445, Classifier Loss 0.21994955837726593, Total Loss 149.48524475097656\n",
      "36: Encoding Loss 14.843657493591309, Transition Loss 9.984368324279785, Classifier Loss 0.22449056804180145, Total Loss 143.1951904296875\n",
      "36: Encoding Loss 15.770359992980957, Transition Loss 8.091818809509277, Classifier Loss 0.2247733175754547, Total Loss 150.25857543945312\n",
      "36: Encoding Loss 15.019965171813965, Transition Loss 4.125171661376953, Classifier Loss 0.24562621116638184, Total Loss 145.54737854003906\n",
      "36: Encoding Loss 14.57144546508789, Transition Loss 8.099807739257812, Classifier Loss 0.23482009768486023, Total Loss 141.67352294921875\n",
      "36: Encoding Loss 14.822566986083984, Transition Loss 5.405384540557861, Classifier Loss 0.24430757761001587, Total Loss 144.09237670898438\n",
      "36: Encoding Loss 15.535691261291504, Transition Loss 8.241022109985352, Classifier Loss 0.22128404676914215, Total Loss 148.06214904785156\n",
      "36: Encoding Loss 15.245481491088867, Transition Loss 10.945178985595703, Classifier Loss 0.2500576376914978, Total Loss 149.15866088867188\n",
      "36: Encoding Loss 16.54163932800293, Transition Loss 6.962181091308594, Classifier Loss 0.223250612616539, Total Loss 156.0506134033203\n",
      "36: Encoding Loss 15.686556816101074, Transition Loss 9.00306224822998, Classifier Loss 0.22071969509124756, Total Loss 149.3650360107422\n",
      "36: Encoding Loss 14.547989845275879, Transition Loss 7.963711738586426, Classifier Loss 0.2440347522497177, Total Loss 142.38014221191406\n",
      "36: Encoding Loss 15.792435646057129, Transition Loss 6.550025463104248, Classifier Loss 0.23868203163146973, Total Loss 151.5177001953125\n",
      "36: Encoding Loss 15.414660453796387, Transition Loss 5.484103202819824, Classifier Loss 0.21668532490730286, Total Loss 146.08262634277344\n",
      "36: Encoding Loss 15.85580062866211, Transition Loss 4.60560417175293, Classifier Loss 0.2234722077846527, Total Loss 150.11476135253906\n",
      "36: Encoding Loss 16.764446258544922, Transition Loss 5.986839294433594, Classifier Loss 0.23622143268585205, Total Loss 158.9351043701172\n",
      "36: Encoding Loss 16.181156158447266, Transition Loss 8.681519508361816, Classifier Loss 0.23981605470180511, Total Loss 155.16714477539062\n",
      "36: Encoding Loss 15.431653022766113, Transition Loss 9.395103454589844, Classifier Loss 0.22569085657596588, Total Loss 147.90133666992188\n",
      "36: Encoding Loss 16.731138229370117, Transition Loss 8.396450996398926, Classifier Loss 0.23482513427734375, Total Loss 159.0109100341797\n",
      "36: Encoding Loss 15.98426628112793, Transition Loss 7.449911117553711, Classifier Loss 0.21345508098602295, Total Loss 150.70962524414062\n",
      "36: Encoding Loss 16.092235565185547, Transition Loss 7.757227897644043, Classifier Loss 0.20759078860282898, Total Loss 151.04843139648438\n",
      "36: Encoding Loss 16.788047790527344, Transition Loss 5.494067192077637, Classifier Loss 0.23444625735282898, Total Loss 158.8478240966797\n",
      "36: Encoding Loss 16.540809631347656, Transition Loss 6.205137252807617, Classifier Loss 0.2169354110956192, Total Loss 155.26104736328125\n",
      "36: Encoding Loss 15.874114990234375, Transition Loss 5.168737411499023, Classifier Loss 0.2190409004688263, Total Loss 149.93075561523438\n",
      "36: Encoding Loss 16.637996673583984, Transition Loss 8.520736694335938, Classifier Loss 0.23600882291793823, Total Loss 158.4090118408203\n",
      "36: Encoding Loss 15.096505165100098, Transition Loss 8.245731353759766, Classifier Loss 0.21226570010185242, Total Loss 143.64776611328125\n",
      "36: Encoding Loss 15.050887107849121, Transition Loss 6.592833995819092, Classifier Loss 0.23554083704948425, Total Loss 145.27975463867188\n",
      "36: Encoding Loss 15.433411598205566, Transition Loss 6.499044418334961, Classifier Loss 0.2202206254005432, Total Loss 146.78916931152344\n",
      "36: Encoding Loss 15.053925514221191, Transition Loss 6.567897796630859, Classifier Loss 0.22038814425468445, Total Loss 143.7838134765625\n",
      "36: Encoding Loss 14.92041015625, Transition Loss 5.878143310546875, Classifier Loss 0.20422114431858063, Total Loss 140.96102905273438\n",
      "36: Encoding Loss 15.720044136047363, Transition Loss 6.138067245483398, Classifier Loss 0.21040280163288116, Total Loss 148.0282440185547\n",
      "36: Encoding Loss 16.81637954711914, Transition Loss 7.975127220153809, Classifier Loss 0.2422519028186798, Total Loss 160.35125732421875\n",
      "36: Encoding Loss 17.33316993713379, Transition Loss 7.174867630004883, Classifier Loss 0.23547780513763428, Total Loss 163.64810180664062\n",
      "36: Encoding Loss 15.185402870178223, Transition Loss 5.079662799835205, Classifier Loss 0.21639835834503174, Total Loss 144.1389923095703\n",
      "36: Encoding Loss 15.004776954650879, Transition Loss 5.461577415466309, Classifier Loss 0.21636095643043518, Total Loss 142.76663208007812\n",
      "36: Encoding Loss 15.705046653747559, Transition Loss 7.805971622467041, Classifier Loss 0.23124265670776367, Total Loss 150.32582092285156\n",
      "36: Encoding Loss 15.044721603393555, Transition Loss 6.79046630859375, Classifier Loss 0.2097899168729782, Total Loss 142.6948699951172\n",
      "36: Encoding Loss 16.0530948638916, Transition Loss 6.5009565353393555, Classifier Loss 0.2247103452682495, Total Loss 152.19598388671875\n",
      "36: Encoding Loss 16.431766510009766, Transition Loss 6.747347354888916, Classifier Loss 0.19406212866306305, Total Loss 152.20982360839844\n",
      "36: Encoding Loss 15.655973434448242, Transition Loss 8.024809837341309, Classifier Loss 0.22645005583763123, Total Loss 149.49777221679688\n",
      "36: Encoding Loss 15.479533195495605, Transition Loss 7.286772727966309, Classifier Loss 0.24359925091266632, Total Loss 149.65354919433594\n",
      "36: Encoding Loss 16.88107681274414, Transition Loss 6.751209259033203, Classifier Loss 0.22994334995746613, Total Loss 159.3931884765625\n",
      "36: Encoding Loss 15.873647689819336, Transition Loss 8.107338905334473, Classifier Loss 0.23587484657764435, Total Loss 152.19815063476562\n",
      "36: Encoding Loss 15.643911361694336, Transition Loss 7.501091480255127, Classifier Loss 0.20118246972560883, Total Loss 146.76976013183594\n",
      "36: Encoding Loss 16.600915908813477, Transition Loss 8.357665061950684, Classifier Loss 0.20983178913593292, Total Loss 155.46205139160156\n",
      "36: Encoding Loss 14.678848266601562, Transition Loss 9.432710647583008, Classifier Loss 0.2571336030960083, Total Loss 145.0306854248047\n",
      "36: Encoding Loss 15.782699584960938, Transition Loss 7.4945783615112305, Classifier Loss 0.2241324782371521, Total Loss 150.17376708984375\n",
      "36: Encoding Loss 15.336287498474121, Transition Loss 8.611027717590332, Classifier Loss 0.21942070126533508, Total Loss 146.3545684814453\n",
      "36: Encoding Loss 15.6627836227417, Transition Loss 8.855844497680664, Classifier Loss 0.22875812649726868, Total Loss 149.94924926757812\n",
      "36: Encoding Loss 15.290900230407715, Transition Loss 8.495305061340332, Classifier Loss 0.21434515714645386, Total Loss 145.46078491210938\n",
      "36: Encoding Loss 17.006441116333008, Transition Loss 7.562292098999023, Classifier Loss 0.2287714183330536, Total Loss 160.4411163330078\n",
      "36: Encoding Loss 15.365757942199707, Transition Loss 6.93972635269165, Classifier Loss 0.22121211886405945, Total Loss 146.43521118164062\n",
      "36: Encoding Loss 16.241455078125, Transition Loss 7.419185161590576, Classifier Loss 0.24022166430950165, Total Loss 155.43765258789062\n",
      "36: Encoding Loss 15.785831451416016, Transition Loss 7.601720809936523, Classifier Loss 0.22751492261886597, Total Loss 150.5584716796875\n",
      "36: Encoding Loss 15.611715316772461, Transition Loss 8.091373443603516, Classifier Loss 0.22358396649360657, Total Loss 148.87039184570312\n",
      "36: Encoding Loss 16.71697425842285, Transition Loss 6.495889186859131, Classifier Loss 0.2211102843284607, Total Loss 157.14599609375\n",
      "36: Encoding Loss 15.329192161560059, Transition Loss 7.577573776245117, Classifier Loss 0.20466642081737518, Total Loss 144.61569213867188\n",
      "36: Encoding Loss 15.59872817993164, Transition Loss 5.994714736938477, Classifier Loss 0.24281294643878937, Total Loss 150.27005004882812\n",
      "36: Encoding Loss 15.481966018676758, Transition Loss 9.435072898864746, Classifier Loss 0.24169635772705078, Total Loss 149.91236877441406\n",
      "36: Encoding Loss 15.145342826843262, Transition Loss 6.288712978363037, Classifier Loss 0.21687576174736023, Total Loss 144.10804748535156\n",
      "36: Encoding Loss 15.499632835388184, Transition Loss 5.413898468017578, Classifier Loss 0.22443507611751556, Total Loss 147.52334594726562\n",
      "36: Encoding Loss 16.156606674194336, Transition Loss 6.989167213439941, Classifier Loss 0.19638372957706451, Total Loss 150.28904724121094\n",
      "36: Encoding Loss 15.777819633483887, Transition Loss 9.15699577331543, Classifier Loss 0.28047874569892883, Total Loss 156.10183715820312\n",
      "36: Encoding Loss 16.013446807861328, Transition Loss 8.436699867248535, Classifier Loss 0.22235536575317383, Total Loss 152.03045654296875\n",
      "36: Encoding Loss 16.741544723510742, Transition Loss 6.64230489730835, Classifier Loss 0.1997433304786682, Total Loss 155.2351531982422\n",
      "36: Encoding Loss 16.556743621826172, Transition Loss 7.314493656158447, Classifier Loss 0.20785923302173615, Total Loss 154.70278930664062\n",
      "36: Encoding Loss 15.422749519348145, Transition Loss 5.05206298828125, Classifier Loss 0.21751834452152252, Total Loss 146.1442413330078\n",
      "36: Encoding Loss 15.867202758789062, Transition Loss 8.627694129943848, Classifier Loss 0.2523355484008789, Total Loss 153.89671325683594\n",
      "36: Encoding Loss 16.134321212768555, Transition Loss 6.417623996734619, Classifier Loss 0.24783502519130707, Total Loss 155.1416015625\n",
      "36: Encoding Loss 15.489798545837402, Transition Loss 9.298714637756348, Classifier Loss 0.23575648665428162, Total Loss 149.35377502441406\n",
      "36: Encoding Loss 16.270286560058594, Transition Loss 5.347091197967529, Classifier Loss 0.21804720163345337, Total Loss 153.0364227294922\n",
      "36: Encoding Loss 15.784293174743652, Transition Loss 6.538286209106445, Classifier Loss 0.21553467214107513, Total Loss 149.13548278808594\n",
      "36: Encoding Loss 16.52239418029785, Transition Loss 6.930850028991699, Classifier Loss 0.2587522268295288, Total Loss 159.4405517578125\n",
      "36: Encoding Loss 15.804673194885254, Transition Loss 9.317998886108398, Classifier Loss 0.2213919758796692, Total Loss 150.44020080566406\n",
      "36: Encoding Loss 14.804117202758789, Transition Loss 8.054624557495117, Classifier Loss 0.2109065055847168, Total Loss 141.134521484375\n",
      "36: Encoding Loss 15.405137062072754, Transition Loss 7.815028190612793, Classifier Loss 0.21509413421154022, Total Loss 146.3135223388672\n",
      "36: Encoding Loss 14.78531551361084, Transition Loss 10.830547332763672, Classifier Loss 0.20549148321151733, Total Loss 140.99777221679688\n",
      "36: Encoding Loss 15.950643539428711, Transition Loss 7.672596454620361, Classifier Loss 0.2299346625804901, Total Loss 152.1331329345703\n",
      "36: Encoding Loss 15.654160499572754, Transition Loss 8.478525161743164, Classifier Loss 0.21824640035629272, Total Loss 148.75364685058594\n",
      "36: Encoding Loss 14.159086227416992, Transition Loss 7.149395942687988, Classifier Loss 0.22592447698116302, Total Loss 137.29502868652344\n",
      "36: Encoding Loss 15.020489692687988, Transition Loss 8.642733573913574, Classifier Loss 0.2292245328426361, Total Loss 144.81491088867188\n",
      "36: Encoding Loss 15.800958633422852, Transition Loss 7.809197425842285, Classifier Loss 0.22318191826343536, Total Loss 150.28770446777344\n",
      "36: Encoding Loss 16.358915328979492, Transition Loss 8.560213088989258, Classifier Loss 0.23750099539756775, Total Loss 156.33346557617188\n",
      "36: Encoding Loss 15.505544662475586, Transition Loss 7.766547679901123, Classifier Loss 0.2181360125541687, Total Loss 147.41128540039062\n",
      "36: Encoding Loss 15.881345748901367, Transition Loss 6.472859859466553, Classifier Loss 0.21061591804027557, Total Loss 149.4069366455078\n",
      "36: Encoding Loss 15.346447944641113, Transition Loss 5.89790678024292, Classifier Loss 0.2580151855945587, Total Loss 149.752685546875\n",
      "36: Encoding Loss 15.649641036987305, Transition Loss 8.334681510925293, Classifier Loss 0.21062785387039185, Total Loss 147.92684936523438\n",
      "36: Encoding Loss 16.491785049438477, Transition Loss 7.5862321853637695, Classifier Loss 0.22468999028205872, Total Loss 155.92051696777344\n",
      "36: Encoding Loss 15.753128051757812, Transition Loss 7.64943265914917, Classifier Loss 0.23403294384479523, Total Loss 150.9582061767578\n",
      "36: Encoding Loss 15.7730073928833, Transition Loss 7.612760066986084, Classifier Loss 0.22311966121196747, Total Loss 150.01858520507812\n",
      "36: Encoding Loss 16.020326614379883, Transition Loss 6.857239723205566, Classifier Loss 0.22585156559944153, Total Loss 152.1192169189453\n",
      "36: Encoding Loss 16.570341110229492, Transition Loss 5.2993292808532715, Classifier Loss 0.22824017703533173, Total Loss 156.4466094970703\n",
      "36: Encoding Loss 15.987424850463867, Transition Loss 5.590198516845703, Classifier Loss 0.22478120028972626, Total Loss 151.49557495117188\n",
      "36: Encoding Loss 16.05541229248047, Transition Loss 7.73921012878418, Classifier Loss 0.21355795860290527, Total Loss 151.34693908691406\n",
      "36: Encoding Loss 16.877634048461914, Transition Loss 5.402608871459961, Classifier Loss 0.22522541880607605, Total Loss 158.62413024902344\n",
      "36: Encoding Loss 17.271364212036133, Transition Loss 5.491333484649658, Classifier Loss 0.2518923878669739, Total Loss 164.4584197998047\n",
      "36: Encoding Loss 15.000547409057617, Transition Loss 8.103387832641602, Classifier Loss 0.2192939966917038, Total Loss 143.55447387695312\n",
      "36: Encoding Loss 14.783174514770508, Transition Loss 8.277653694152832, Classifier Loss 0.2052372694015503, Total Loss 140.4446563720703\n",
      "36: Encoding Loss 15.187564849853516, Transition Loss 7.608275890350342, Classifier Loss 0.2503476142883301, Total Loss 148.0569305419922\n",
      "36: Encoding Loss 15.216097831726074, Transition Loss 8.690110206604004, Classifier Loss 0.24008356034755707, Total Loss 147.47515869140625\n",
      "36: Encoding Loss 15.012690544128418, Transition Loss 10.88641357421875, Classifier Loss 0.2282877266407013, Total Loss 145.10757446289062\n",
      "36: Encoding Loss 16.724477767944336, Transition Loss 7.028522491455078, Classifier Loss 0.2276899516582489, Total Loss 157.97052001953125\n",
      "36: Encoding Loss 16.63702392578125, Transition Loss 6.702538967132568, Classifier Loss 0.19716815650463104, Total Loss 154.1535186767578\n",
      "36: Encoding Loss 14.938328742980957, Transition Loss 6.167812824249268, Classifier Loss 0.21038834750652313, Total Loss 141.77903747558594\n",
      "36: Encoding Loss 15.44281005859375, Transition Loss 5.991085052490234, Classifier Loss 0.22487926483154297, Total Loss 147.22862243652344\n",
      "36: Encoding Loss 15.472123146057129, Transition Loss 6.773565769195557, Classifier Loss 0.21778400242328644, Total Loss 146.91009521484375\n",
      "36: Encoding Loss 15.967215538024902, Transition Loss 7.825286865234375, Classifier Loss 0.2234644740819931, Total Loss 151.64923095703125\n",
      "36: Encoding Loss 15.911458969116211, Transition Loss 6.03695011138916, Classifier Loss 0.21323180198669434, Total Loss 149.822265625\n",
      "36: Encoding Loss 15.959246635437012, Transition Loss 7.081258773803711, Classifier Loss 0.24270713329315186, Total Loss 153.36093139648438\n",
      "36: Encoding Loss 15.068230628967285, Transition Loss 5.49584436416626, Classifier Loss 0.20176070928573608, Total Loss 141.8210906982422\n",
      "36: Encoding Loss 15.899203300476074, Transition Loss 7.667184829711914, Classifier Loss 0.1947215050458908, Total Loss 148.19920349121094\n",
      "36: Encoding Loss 16.06839942932129, Transition Loss 5.094538688659668, Classifier Loss 0.2589859366416931, Total Loss 155.46469116210938\n",
      "36: Encoding Loss 15.229509353637695, Transition Loss 8.984537124633789, Classifier Loss 0.23817509412765503, Total Loss 147.4504852294922\n",
      "36: Encoding Loss 15.856398582458496, Transition Loss 6.461925506591797, Classifier Loss 0.23025552928447723, Total Loss 151.16912841796875\n",
      "36: Encoding Loss 16.03056526184082, Transition Loss 8.355618476867676, Classifier Loss 0.23479913175106049, Total Loss 153.39556884765625\n",
      "36: Encoding Loss 15.890547752380371, Transition Loss 6.569390773773193, Classifier Loss 0.21931639313697815, Total Loss 150.36988830566406\n",
      "36: Encoding Loss 15.330530166625977, Transition Loss 7.7074666023254395, Classifier Loss 0.22282478213310242, Total Loss 146.4682159423828\n",
      "36: Encoding Loss 17.07513999938965, Transition Loss 5.145229339599609, Classifier Loss 0.2315719574689865, Total Loss 160.78736877441406\n",
      "36: Encoding Loss 16.330766677856445, Transition Loss 5.027040004730225, Classifier Loss 0.22811219096183777, Total Loss 154.46275329589844\n",
      "36: Encoding Loss 15.550512313842773, Transition Loss 5.63632869720459, Classifier Loss 0.21447952091693878, Total Loss 146.97930908203125\n",
      "36: Encoding Loss 15.268136024475098, Transition Loss 7.619356632232666, Classifier Loss 0.21984054148197174, Total Loss 145.65301513671875\n",
      "36: Encoding Loss 15.39893627166748, Transition Loss 4.306314468383789, Classifier Loss 0.22384406626224518, Total Loss 146.43716430664062\n",
      "36: Encoding Loss 16.041240692138672, Transition Loss 9.858025550842285, Classifier Loss 0.23536163568496704, Total Loss 153.83770751953125\n",
      "36: Encoding Loss 16.354907989501953, Transition Loss 6.159936428070068, Classifier Loss 0.21758978068828583, Total Loss 153.83023071289062\n",
      "36: Encoding Loss 17.12204360961914, Transition Loss 7.919183731079102, Classifier Loss 0.22721458971500397, Total Loss 161.28164672851562\n",
      "36: Encoding Loss 16.654062271118164, Transition Loss 7.505969047546387, Classifier Loss 0.22174856066703796, Total Loss 156.90853881835938\n",
      "36: Encoding Loss 15.322035789489746, Transition Loss 11.303751945495605, Classifier Loss 0.21988996863365173, Total Loss 146.82603454589844\n",
      "36: Encoding Loss 13.096270561218262, Transition Loss 10.140237808227539, Classifier Loss 0.20660023391246796, Total Loss 127.45823669433594\n",
      "37: Encoding Loss 16.967836380004883, Transition Loss 6.70379638671875, Classifier Loss 0.22938348352909088, Total Loss 160.0218048095703\n",
      "37: Encoding Loss 16.30442237854004, Transition Loss 5.85205602645874, Classifier Loss 0.22031563520431519, Total Loss 153.63735961914062\n",
      "37: Encoding Loss 16.331317901611328, Transition Loss 6.987447738647461, Classifier Loss 0.2292289286851883, Total Loss 154.97093200683594\n",
      "37: Encoding Loss 15.814022064208984, Transition Loss 7.01677131652832, Classifier Loss 0.24608822166919708, Total Loss 152.52435302734375\n",
      "37: Encoding Loss 17.534963607788086, Transition Loss 6.941634178161621, Classifier Loss 0.22821234166622162, Total Loss 164.4892578125\n",
      "37: Encoding Loss 14.079094886779785, Transition Loss 7.438253402709961, Classifier Loss 0.24221104383468628, Total Loss 138.34152221679688\n",
      "37: Encoding Loss 16.95655632019043, Transition Loss 6.207846641540527, Classifier Loss 0.2349364012479782, Total Loss 160.3876495361328\n",
      "37: Encoding Loss 17.237117767333984, Transition Loss 7.031061172485352, Classifier Loss 0.23926670849323273, Total Loss 163.22984313964844\n",
      "37: Encoding Loss 15.662713050842285, Transition Loss 6.612208366394043, Classifier Loss 0.30858299136161804, Total Loss 157.48245239257812\n",
      "37: Encoding Loss 16.65766143798828, Transition Loss 6.665074348449707, Classifier Loss 0.23965483903884888, Total Loss 158.55978393554688\n",
      "37: Encoding Loss 16.088298797607422, Transition Loss 5.744832515716553, Classifier Loss 0.21449461579322815, Total Loss 151.30484008789062\n",
      "37: Encoding Loss 15.812878608703613, Transition Loss 6.592909812927246, Classifier Loss 0.24274323880672455, Total Loss 152.095947265625\n",
      "37: Encoding Loss 15.721883773803711, Transition Loss 7.1867780685424805, Classifier Loss 0.23223349452018738, Total Loss 150.435791015625\n",
      "37: Encoding Loss 15.010079383850098, Transition Loss 6.306400299072266, Classifier Loss 0.21033218502998352, Total Loss 142.37513732910156\n",
      "37: Encoding Loss 15.14337158203125, Transition Loss 7.577913761138916, Classifier Loss 0.22527951002120972, Total Loss 145.1905059814453\n",
      "37: Encoding Loss 15.974639892578125, Transition Loss 8.754806518554688, Classifier Loss 0.22616486251354218, Total Loss 152.16456604003906\n",
      "37: Encoding Loss 15.874128341674805, Transition Loss 7.110666751861572, Classifier Loss 0.2263013869524002, Total Loss 151.04530334472656\n",
      "37: Encoding Loss 15.313969612121582, Transition Loss 8.265893936157227, Classifier Loss 0.2110312283039093, Total Loss 145.26806640625\n",
      "37: Encoding Loss 16.50008773803711, Transition Loss 8.028564453125, Classifier Loss 0.2005559802055359, Total Loss 153.6620330810547\n",
      "37: Encoding Loss 16.30145835876465, Transition Loss 8.027060508728027, Classifier Loss 0.21827571094036102, Total Loss 153.8446502685547\n",
      "37: Encoding Loss 15.71663761138916, Transition Loss 8.354808807373047, Classifier Loss 0.23398452997207642, Total Loss 150.80252075195312\n",
      "37: Encoding Loss 15.784195899963379, Transition Loss 6.4895219802856445, Classifier Loss 0.21118764579296112, Total Loss 148.6902313232422\n",
      "37: Encoding Loss 14.322064399719238, Transition Loss 11.36507797241211, Classifier Loss 0.21812210977077484, Total Loss 138.6617431640625\n",
      "37: Encoding Loss 15.697019577026367, Transition Loss 6.3785576820373535, Classifier Loss 0.23524221777915955, Total Loss 150.3760986328125\n",
      "37: Encoding Loss 15.678635597229004, Transition Loss 4.692488193511963, Classifier Loss 0.21215496957302094, Total Loss 147.5830841064453\n",
      "37: Encoding Loss 15.301173210144043, Transition Loss 6.877662658691406, Classifier Loss 0.20152384042739868, Total Loss 143.9373016357422\n",
      "37: Encoding Loss 15.896714210510254, Transition Loss 7.804408550262451, Classifier Loss 0.21630319952964783, Total Loss 150.36492919921875\n",
      "37: Encoding Loss 16.037616729736328, Transition Loss 8.19565486907959, Classifier Loss 0.2202942818403244, Total Loss 151.96949768066406\n",
      "37: Encoding Loss 15.985838890075684, Transition Loss 8.581535339355469, Classifier Loss 0.2284432053565979, Total Loss 152.4473419189453\n",
      "37: Encoding Loss 14.625577926635742, Transition Loss 8.841548919677734, Classifier Loss 0.23100171983242035, Total Loss 141.87310791015625\n",
      "37: Encoding Loss 15.71388053894043, Transition Loss 7.740157604217529, Classifier Loss 0.21570292115211487, Total Loss 148.82937622070312\n",
      "37: Encoding Loss 14.823054313659668, Transition Loss 10.179603576660156, Classifier Loss 0.20943979918956757, Total Loss 141.5643310546875\n",
      "37: Encoding Loss 15.743325233459473, Transition Loss 8.405348777770996, Classifier Loss 0.22721877694129944, Total Loss 150.3495635986328\n",
      "37: Encoding Loss 14.99655532836914, Transition Loss 4.17253303527832, Classifier Loss 0.23384025692939758, Total Loss 144.1909637451172\n",
      "37: Encoding Loss 14.55302906036377, Transition Loss 8.401654243469238, Classifier Loss 0.22605450451374054, Total Loss 140.7100067138672\n",
      "37: Encoding Loss 14.784412384033203, Transition Loss 5.344234943389893, Classifier Loss 0.24178284406661987, Total Loss 143.52243041992188\n",
      "37: Encoding Loss 15.526867866516113, Transition Loss 8.646200180053711, Classifier Loss 0.211206316947937, Total Loss 147.06480407714844\n",
      "37: Encoding Loss 15.25826644897461, Transition Loss 10.65926742553711, Classifier Loss 0.2315552681684494, Total Loss 147.353515625\n",
      "37: Encoding Loss 16.50537109375, Transition Loss 7.398562908172607, Classifier Loss 0.21906758844852448, Total Loss 155.42942810058594\n",
      "37: Encoding Loss 15.633363723754883, Transition Loss 8.945286750793457, Classifier Loss 0.21514713764190674, Total Loss 148.3706817626953\n",
      "37: Encoding Loss 14.550394058227539, Transition Loss 8.627640724182129, Classifier Loss 0.2512357532978058, Total Loss 143.25225830078125\n",
      "37: Encoding Loss 15.749835014343262, Transition Loss 6.227303981781006, Classifier Loss 0.23210835456848145, Total Loss 150.45498657226562\n",
      "37: Encoding Loss 15.36807632446289, Transition Loss 5.736505508422852, Classifier Loss 0.2229437679052353, Total Loss 146.38629150390625\n",
      "37: Encoding Loss 15.748369216918945, Transition Loss 4.344553470611572, Classifier Loss 0.23094412684440613, Total Loss 149.9502716064453\n",
      "37: Encoding Loss 16.732053756713867, Transition Loss 6.350736618041992, Classifier Loss 0.22920896112918854, Total Loss 158.04747009277344\n",
      "37: Encoding Loss 16.190649032592773, Transition Loss 8.43337631225586, Classifier Loss 0.23762568831443787, Total Loss 154.9744415283203\n",
      "37: Encoding Loss 15.43424129486084, Transition Loss 9.780704498291016, Classifier Loss 0.21913427114486694, Total Loss 147.343505859375\n",
      "37: Encoding Loss 16.726518630981445, Transition Loss 8.285367965698242, Classifier Loss 0.22862960398197174, Total Loss 158.33218383789062\n",
      "37: Encoding Loss 16.002580642700195, Transition Loss 7.6998796463012695, Classifier Loss 0.21344654262065887, Total Loss 150.9052734375\n",
      "37: Encoding Loss 16.08829689025879, Transition Loss 7.7381510734558105, Classifier Loss 0.20153361558914185, Total Loss 150.40736389160156\n",
      "37: Encoding Loss 16.737720489501953, Transition Loss 5.652742385864258, Classifier Loss 0.2297322005033493, Total Loss 158.0055389404297\n",
      "37: Encoding Loss 16.466960906982422, Transition Loss 6.166513442993164, Classifier Loss 0.2219243049621582, Total Loss 155.16143798828125\n",
      "37: Encoding Loss 15.825762748718262, Transition Loss 5.326714515686035, Classifier Loss 0.21303491294384003, Total Loss 148.9749298095703\n",
      "37: Encoding Loss 16.614927291870117, Transition Loss 8.646952629089355, Classifier Loss 0.2279053032398224, Total Loss 157.4393310546875\n",
      "37: Encoding Loss 15.100512504577637, Transition Loss 8.694228172302246, Classifier Loss 0.20313385128974915, Total Loss 142.85633850097656\n",
      "37: Encoding Loss 14.984105110168457, Transition Loss 6.7407732009887695, Classifier Loss 0.24019306898117065, Total Loss 145.2403106689453\n",
      "37: Encoding Loss 15.402690887451172, Transition Loss 6.728953838348389, Classifier Loss 0.22047464549541473, Total Loss 146.6147918701172\n",
      "37: Encoding Loss 15.02790355682373, Transition Loss 6.6057610511779785, Classifier Loss 0.20195019245147705, Total Loss 141.73939514160156\n",
      "37: Encoding Loss 14.87241268157959, Transition Loss 6.172248363494873, Classifier Loss 0.210900217294693, Total Loss 141.30377197265625\n",
      "37: Encoding Loss 15.657503128051758, Transition Loss 6.085720539093018, Classifier Loss 0.19837376475334167, Total Loss 146.31454467773438\n",
      "37: Encoding Loss 16.811105728149414, Transition Loss 8.282733917236328, Classifier Loss 0.2334802895784378, Total Loss 159.493408203125\n",
      "37: Encoding Loss 17.336210250854492, Transition Loss 7.175046443939209, Classifier Loss 0.23437176644802094, Total Loss 163.56187438964844\n",
      "37: Encoding Loss 15.171143531799316, Transition Loss 5.27286434173584, Classifier Loss 0.22667467594146729, Total Loss 145.0911865234375\n",
      "37: Encoding Loss 14.987507820129395, Transition Loss 5.620026588439941, Classifier Loss 0.21392503380775452, Total Loss 142.4165802001953\n",
      "37: Encoding Loss 15.65646743774414, Transition Loss 8.141414642333984, Classifier Loss 0.2285590022802353, Total Loss 149.7359161376953\n",
      "37: Encoding Loss 15.049915313720703, Transition Loss 6.918918132781982, Classifier Loss 0.21017912030220032, Total Loss 142.80101013183594\n",
      "37: Encoding Loss 16.016094207763672, Transition Loss 6.633266448974609, Classifier Loss 0.22633960843086243, Total Loss 152.08938598632812\n",
      "37: Encoding Loss 16.432376861572266, Transition Loss 6.632214069366455, Classifier Loss 0.192649245262146, Total Loss 152.05038452148438\n",
      "37: Encoding Loss 15.629921913146973, Transition Loss 8.412820816040039, Classifier Loss 0.2328174114227295, Total Loss 150.00369262695312\n",
      "37: Encoding Loss 15.465752601623535, Transition Loss 7.093023777008057, Classifier Loss 0.23709580302238464, Total Loss 148.85421752929688\n",
      "37: Encoding Loss 16.8732852935791, Transition Loss 7.290949821472168, Classifier Loss 0.2261008322238922, Total Loss 159.05455017089844\n",
      "37: Encoding Loss 15.820040702819824, Transition Loss 7.891704559326172, Classifier Loss 0.21416300535202026, Total Loss 149.55496215820312\n",
      "37: Encoding Loss 15.597159385681152, Transition Loss 8.088335990905762, Classifier Loss 0.2015678733587265, Total Loss 146.55172729492188\n",
      "37: Encoding Loss 16.574695587158203, Transition Loss 8.145383834838867, Classifier Loss 0.20805802941322327, Total Loss 155.03244018554688\n",
      "37: Encoding Loss 14.638629913330078, Transition Loss 9.923467636108398, Classifier Loss 0.2601688504219055, Total Loss 145.11061096191406\n",
      "37: Encoding Loss 15.775012016296387, Transition Loss 7.115599155426025, Classifier Loss 0.2197594940662384, Total Loss 149.5991668701172\n",
      "37: Encoding Loss 15.360213279724121, Transition Loss 8.969101905822754, Classifier Loss 0.21640951931476593, Total Loss 146.3164825439453\n",
      "37: Encoding Loss 15.673296928405762, Transition Loss 8.229119300842285, Classifier Loss 0.2333422601222992, Total Loss 150.36642456054688\n",
      "37: Encoding Loss 15.279623031616211, Transition Loss 9.23129940032959, Classifier Loss 0.21595636010169983, Total Loss 145.6788787841797\n",
      "37: Encoding Loss 16.988252639770508, Transition Loss 6.412243843078613, Classifier Loss 0.23406733572483063, Total Loss 160.59521484375\n",
      "37: Encoding Loss 15.347737312316895, Transition Loss 8.44022274017334, Classifier Loss 0.22162717580795288, Total Loss 146.63265991210938\n",
      "37: Encoding Loss 16.234643936157227, Transition Loss 6.171856880187988, Classifier Loss 0.2364002913236618, Total Loss 154.75155639648438\n",
      "37: Encoding Loss 15.774080276489258, Transition Loss 9.516485214233398, Classifier Loss 0.22780314087867737, Total Loss 150.87625122070312\n",
      "37: Encoding Loss 15.62061595916748, Transition Loss 7.184020519256592, Classifier Loss 0.2154320329427719, Total Loss 147.94493103027344\n",
      "37: Encoding Loss 16.69341278076172, Transition Loss 7.380069255828857, Classifier Loss 0.21041004359722137, Total Loss 156.06431579589844\n",
      "37: Encoding Loss 15.290312767028809, Transition Loss 7.339883327484131, Classifier Loss 0.198722705245018, Total Loss 143.66275024414062\n",
      "37: Encoding Loss 15.61287784576416, Transition Loss 6.398659706115723, Classifier Loss 0.22877508401870728, Total Loss 149.06027221679688\n",
      "37: Encoding Loss 15.448039054870605, Transition Loss 9.186765670776367, Classifier Loss 0.24168112874031067, Total Loss 149.58978271484375\n",
      "37: Encoding Loss 15.143680572509766, Transition Loss 6.597052574157715, Classifier Loss 0.2115643471479416, Total Loss 143.6252899169922\n",
      "37: Encoding Loss 15.507109642028809, Transition Loss 5.38416051864624, Classifier Loss 0.22536015510559082, Total Loss 147.6697235107422\n",
      "37: Encoding Loss 16.13807487487793, Transition Loss 7.205549240112305, Classifier Loss 0.19620245695114136, Total Loss 150.16595458984375\n",
      "37: Encoding Loss 15.7742338180542, Transition Loss 9.087806701660156, Classifier Loss 0.2761084735393524, Total Loss 155.62228393554688\n",
      "37: Encoding Loss 16.010507583618164, Transition Loss 8.627448081970215, Classifier Loss 0.21898645162582397, Total Loss 151.7082061767578\n",
      "37: Encoding Loss 16.729238510131836, Transition Loss 6.780852794647217, Classifier Loss 0.19964797794818878, Total Loss 155.15487670898438\n",
      "37: Encoding Loss 16.513904571533203, Transition Loss 7.521289825439453, Classifier Loss 0.2054351568222046, Total Loss 154.1590118408203\n",
      "37: Encoding Loss 15.366442680358887, Transition Loss 5.16841983795166, Classifier Loss 0.21802982687950134, Total Loss 145.76821899414062\n",
      "37: Encoding Loss 15.849591255187988, Transition Loss 8.817208290100098, Classifier Loss 0.24376565217971802, Total Loss 152.93673706054688\n",
      "37: Encoding Loss 16.10663604736328, Transition Loss 6.459810733795166, Classifier Loss 0.23199772834777832, Total Loss 153.34481811523438\n",
      "37: Encoding Loss 15.450027465820312, Transition Loss 9.490323066711426, Classifier Loss 0.24426418542861938, Total Loss 149.92471313476562\n",
      "37: Encoding Loss 16.256961822509766, Transition Loss 5.22910213470459, Classifier Loss 0.2171923667192459, Total Loss 152.8207550048828\n",
      "37: Encoding Loss 15.784153938293457, Transition Loss 6.661627769470215, Classifier Loss 0.20885857939720154, Total Loss 148.4914093017578\n",
      "37: Encoding Loss 16.461198806762695, Transition Loss 7.045603275299072, Classifier Loss 0.2576330900192261, Total Loss 158.8620147705078\n",
      "37: Encoding Loss 15.796321868896484, Transition Loss 9.524627685546875, Classifier Loss 0.2171052098274231, Total Loss 149.98602294921875\n",
      "37: Encoding Loss 14.797845840454102, Transition Loss 8.239368438720703, Classifier Loss 0.20081953704357147, Total Loss 140.1125946044922\n",
      "37: Encoding Loss 15.360658645629883, Transition Loss 7.999880790710449, Classifier Loss 0.21467646956443787, Total Loss 145.95289611816406\n",
      "37: Encoding Loss 14.745121002197266, Transition Loss 10.959450721740723, Classifier Loss 0.20358100533485413, Total Loss 140.51095581054688\n",
      "37: Encoding Loss 15.916295051574707, Transition Loss 7.609710693359375, Classifier Loss 0.22228001058101654, Total Loss 151.08030700683594\n",
      "37: Encoding Loss 15.645121574401855, Transition Loss 8.404215812683105, Classifier Loss 0.21632981300354004, Total Loss 148.47479248046875\n",
      "37: Encoding Loss 14.150069236755371, Transition Loss 7.190646648406982, Classifier Loss 0.22360330820083618, Total Loss 136.99900817871094\n",
      "37: Encoding Loss 15.012665748596191, Transition Loss 8.629744529724121, Classifier Loss 0.2140698879957199, Total Loss 143.23426818847656\n",
      "37: Encoding Loss 15.79969596862793, Transition Loss 7.785201072692871, Classifier Loss 0.22066521644592285, Total Loss 150.02113342285156\n",
      "37: Encoding Loss 16.35664176940918, Transition Loss 8.6220064163208, Classifier Loss 0.23210391402244568, Total Loss 155.7879180908203\n",
      "37: Encoding Loss 15.4952974319458, Transition Loss 7.756346702575684, Classifier Loss 0.22818860411643982, Total Loss 148.33250427246094\n",
      "37: Encoding Loss 15.853968620300293, Transition Loss 6.394219398498535, Classifier Loss 0.20429344475269318, Total Loss 148.53993225097656\n",
      "37: Encoding Loss 15.301531791687012, Transition Loss 5.8873820304870605, Classifier Loss 0.23515036702156067, Total Loss 147.10476684570312\n",
      "37: Encoding Loss 15.673625946044922, Transition Loss 8.594468116760254, Classifier Loss 0.22280648350715637, Total Loss 149.3885498046875\n",
      "37: Encoding Loss 16.4565372467041, Transition Loss 7.677801609039307, Classifier Loss 0.22448286414146423, Total Loss 155.63613891601562\n",
      "37: Encoding Loss 15.724035263061523, Transition Loss 7.921635627746582, Classifier Loss 0.22691406309604645, Total Loss 150.06802368164062\n",
      "37: Encoding Loss 15.778154373168945, Transition Loss 7.678987503051758, Classifier Loss 0.21363712847232819, Total Loss 149.12474060058594\n",
      "37: Encoding Loss 15.988887786865234, Transition Loss 7.079630374908447, Classifier Loss 0.2318592667579651, Total Loss 152.51295471191406\n",
      "37: Encoding Loss 16.511070251464844, Transition Loss 5.23056697845459, Classifier Loss 0.2281946837902069, Total Loss 155.9541473388672\n",
      "37: Encoding Loss 15.965283393859863, Transition Loss 5.7272257804870605, Classifier Loss 0.22906729578971863, Total Loss 151.77444458007812\n",
      "37: Encoding Loss 16.051891326904297, Transition Loss 8.016073226928711, Classifier Loss 0.21904325485229492, Total Loss 151.9226837158203\n",
      "37: Encoding Loss 16.865026473999023, Transition Loss 5.57167387008667, Classifier Loss 0.2178305983543396, Total Loss 157.81761169433594\n",
      "37: Encoding Loss 17.221765518188477, Transition Loss 5.776721954345703, Classifier Loss 0.2500799298286438, Total Loss 163.93746948242188\n",
      "37: Encoding Loss 14.975675582885742, Transition Loss 8.330093383789062, Classifier Loss 0.21947790682315826, Total Loss 143.41921997070312\n",
      "37: Encoding Loss 14.774703979492188, Transition Loss 8.506858825683594, Classifier Loss 0.199252188205719, Total Loss 139.82421875\n",
      "37: Encoding Loss 15.174777030944824, Transition Loss 7.721851825714111, Classifier Loss 0.23663179576396942, Total Loss 146.60577392578125\n",
      "37: Encoding Loss 15.230302810668945, Transition Loss 8.783077239990234, Classifier Loss 0.23203037679195404, Total Loss 146.8020782470703\n",
      "37: Encoding Loss 14.9856538772583, Transition Loss 11.087629318237305, Classifier Loss 0.2230282872915268, Total Loss 144.4055938720703\n",
      "37: Encoding Loss 16.692962646484375, Transition Loss 7.202619552612305, Classifier Loss 0.22763663530349731, Total Loss 157.74789428710938\n",
      "37: Encoding Loss 16.609243392944336, Transition Loss 6.892683029174805, Classifier Loss 0.19153736531734467, Total Loss 153.40621948242188\n",
      "37: Encoding Loss 14.932207107543945, Transition Loss 6.310911178588867, Classifier Loss 0.2044014185667038, Total Loss 141.15997314453125\n",
      "37: Encoding Loss 15.41163158416748, Transition Loss 6.099129676818848, Classifier Loss 0.2215847671031952, Total Loss 146.67135620117188\n",
      "37: Encoding Loss 15.446078300476074, Transition Loss 6.867760181427002, Classifier Loss 0.22372278571128845, Total Loss 147.314453125\n",
      "37: Encoding Loss 15.93403434753418, Transition Loss 7.925629138946533, Classifier Loss 0.21240012347698212, Total Loss 150.29742431640625\n",
      "37: Encoding Loss 15.894338607788086, Transition Loss 6.112928867340088, Classifier Loss 0.21173861622810364, Total Loss 149.55116271972656\n",
      "37: Encoding Loss 15.942458152770996, Transition Loss 7.158435344696045, Classifier Loss 0.2489565759897232, Total Loss 153.86700439453125\n",
      "37: Encoding Loss 15.031691551208496, Transition Loss 5.577880382537842, Classifier Loss 0.20270641148090363, Total Loss 141.63973999023438\n",
      "37: Encoding Loss 15.86423110961914, Transition Loss 7.809737682342529, Classifier Loss 0.19031816720962524, Total Loss 147.5076141357422\n",
      "37: Encoding Loss 16.069093704223633, Transition Loss 5.065807342529297, Classifier Loss 0.25520244240760803, Total Loss 155.08616638183594\n",
      "37: Encoding Loss 15.218917846679688, Transition Loss 9.098451614379883, Classifier Loss 0.23156648874282837, Total Loss 146.72767639160156\n",
      "37: Encoding Loss 15.830035209655762, Transition Loss 6.48651123046875, Classifier Loss 0.22239640355110168, Total Loss 150.17723083496094\n",
      "37: Encoding Loss 16.01237678527832, Transition Loss 8.566240310668945, Classifier Loss 0.23827265202999115, Total Loss 153.6395263671875\n",
      "37: Encoding Loss 15.853748321533203, Transition Loss 6.565378189086914, Classifier Loss 0.21667508780956268, Total Loss 149.81056213378906\n",
      "37: Encoding Loss 15.301820755004883, Transition Loss 7.966547966003418, Classifier Loss 0.22406379878520966, Total Loss 146.4142608642578\n",
      "37: Encoding Loss 17.015329360961914, Transition Loss 5.083349227905273, Classifier Loss 0.23175360262393951, Total Loss 160.3146514892578\n",
      "37: Encoding Loss 16.293367385864258, Transition Loss 5.109457015991211, Classifier Loss 0.22506755590438843, Total Loss 153.87559509277344\n",
      "37: Encoding Loss 15.536858558654785, Transition Loss 5.7345757484436035, Classifier Loss 0.20952259004116058, Total Loss 146.39404296875\n",
      "37: Encoding Loss 15.22381591796875, Transition Loss 7.971791744232178, Classifier Loss 0.2248135507106781, Total Loss 145.86624145507812\n",
      "37: Encoding Loss 15.37902545928955, Transition Loss 4.300032615661621, Classifier Loss 0.22269593179225922, Total Loss 146.1617889404297\n",
      "37: Encoding Loss 16.041074752807617, Transition Loss 10.290899276733398, Classifier Loss 0.22751936316490173, Total Loss 153.1387176513672\n",
      "37: Encoding Loss 16.32071876525879, Transition Loss 6.148470401763916, Classifier Loss 0.21398736536502838, Total Loss 153.19418334960938\n",
      "37: Encoding Loss 17.099964141845703, Transition Loss 8.177168846130371, Classifier Loss 0.22289906442165375, Total Loss 160.7250518798828\n",
      "37: Encoding Loss 16.662023544311523, Transition Loss 7.5735344886779785, Classifier Loss 0.21980662643909454, Total Loss 156.79156494140625\n",
      "37: Encoding Loss 15.296666145324707, Transition Loss 11.341608047485352, Classifier Loss 0.21261052787303925, Total Loss 145.9027099609375\n",
      "37: Encoding Loss 13.131487846374512, Transition Loss 10.614413261413574, Classifier Loss 0.21123267710208893, Total Loss 128.2980499267578\n",
      "38: Encoding Loss 16.9390811920166, Transition Loss 6.61640739440918, Classifier Loss 0.21449433267116547, Total Loss 158.28536987304688\n",
      "38: Encoding Loss 16.28053855895996, Transition Loss 6.06859827041626, Classifier Loss 0.21498656272888184, Total Loss 152.95668029785156\n",
      "38: Encoding Loss 16.318429946899414, Transition Loss 6.942036151885986, Classifier Loss 0.21939094364643097, Total Loss 153.8749542236328\n",
      "38: Encoding Loss 15.822602272033691, Transition Loss 7.0635223388671875, Classifier Loss 0.22425676882266998, Total Loss 150.41920471191406\n",
      "38: Encoding Loss 17.531557083129883, Transition Loss 6.934503078460693, Classifier Loss 0.23433053493499756, Total Loss 165.07241821289062\n",
      "38: Encoding Loss 14.063823699951172, Transition Loss 7.524562835693359, Classifier Loss 0.2262256145477295, Total Loss 136.63807678222656\n",
      "38: Encoding Loss 16.932783126831055, Transition Loss 6.134456634521484, Classifier Loss 0.24352115392684937, Total Loss 161.04127502441406\n",
      "38: Encoding Loss 17.24701690673828, Transition Loss 6.992156028747559, Classifier Loss 0.24632178246974945, Total Loss 164.00674438476562\n",
      "38: Encoding Loss 15.677644729614258, Transition Loss 6.629819869995117, Classifier Loss 0.30449825525283813, Total Loss 157.1969451904297\n",
      "38: Encoding Loss 16.667490005493164, Transition Loss 6.683171272277832, Classifier Loss 0.24172671139240265, Total Loss 158.84922790527344\n",
      "38: Encoding Loss 16.128435134887695, Transition Loss 5.759467601776123, Classifier Loss 0.21937541663646698, Total Loss 152.11691284179688\n",
      "38: Encoding Loss 15.781787872314453, Transition Loss 6.654435634613037, Classifier Loss 0.23459497094154358, Total Loss 151.044677734375\n",
      "38: Encoding Loss 15.68609619140625, Transition Loss 7.315319061279297, Classifier Loss 0.22081951797008514, Total Loss 149.03378295898438\n",
      "38: Encoding Loss 14.98036003112793, Transition Loss 6.336724281311035, Classifier Loss 0.21186426281929016, Total Loss 142.29666137695312\n",
      "38: Encoding Loss 15.106512069702148, Transition Loss 7.584184169769287, Classifier Loss 0.22759045660495758, Total Loss 145.1279754638672\n",
      "38: Encoding Loss 15.967411041259766, Transition Loss 8.81342887878418, Classifier Loss 0.2234683334827423, Total Loss 151.8488006591797\n",
      "38: Encoding Loss 15.864141464233398, Transition Loss 7.167390823364258, Classifier Loss 0.217725932598114, Total Loss 150.11920166015625\n",
      "38: Encoding Loss 15.300869941711426, Transition Loss 8.30547046661377, Classifier Loss 0.21392026543617249, Total Loss 145.46006774902344\n",
      "38: Encoding Loss 16.497966766357422, Transition Loss 8.062662124633789, Classifier Loss 0.20640873908996582, Total Loss 154.23715209960938\n",
      "38: Encoding Loss 16.297733306884766, Transition Loss 8.138367652893066, Classifier Loss 0.2112634778022766, Total Loss 153.13587951660156\n",
      "38: Encoding Loss 15.704737663269043, Transition Loss 8.486922264099121, Classifier Loss 0.22985771298408508, Total Loss 150.32106018066406\n",
      "38: Encoding Loss 15.762832641601562, Transition Loss 6.513137340545654, Classifier Loss 0.20533815026283264, Total Loss 147.93910217285156\n",
      "38: Encoding Loss 14.321439743041992, Transition Loss 11.244405746459961, Classifier Loss 0.21991786360740662, Total Loss 138.81219482421875\n",
      "38: Encoding Loss 15.65009593963623, Transition Loss 6.436431407928467, Classifier Loss 0.23399649560451508, Total Loss 149.88771057128906\n",
      "38: Encoding Loss 15.663272857666016, Transition Loss 4.679884433746338, Classifier Loss 0.215290367603302, Total Loss 147.77117919921875\n",
      "38: Encoding Loss 15.286340713500977, Transition Loss 6.884058952331543, Classifier Loss 0.20036618411540985, Total Loss 143.70416259765625\n",
      "38: Encoding Loss 15.867486953735352, Transition Loss 7.876936435699463, Classifier Loss 0.22170566022396088, Total Loss 150.68585205078125\n",
      "38: Encoding Loss 16.002546310424805, Transition Loss 8.31969165802002, Classifier Loss 0.22664642333984375, Total Loss 152.3489532470703\n",
      "38: Encoding Loss 15.953836441040039, Transition Loss 8.646440505981445, Classifier Loss 0.21782329678535461, Total Loss 151.14231872558594\n",
      "38: Encoding Loss 14.605669021606445, Transition Loss 9.00869083404541, Classifier Loss 0.2324785590171814, Total Loss 141.8949432373047\n",
      "38: Encoding Loss 15.72926139831543, Transition Loss 7.708385467529297, Classifier Loss 0.21119017899036407, Total Loss 148.4947967529297\n",
      "38: Encoding Loss 14.792116165161133, Transition Loss 10.203003883361816, Classifier Loss 0.2082746922969818, Total Loss 141.2050018310547\n",
      "38: Encoding Loss 15.7381010055542, Transition Loss 8.484525680541992, Classifier Loss 0.22409456968307495, Total Loss 150.01116943359375\n",
      "38: Encoding Loss 14.930428504943848, Transition Loss 4.2287445068359375, Classifier Loss 0.23790167272090912, Total Loss 144.079345703125\n",
      "38: Encoding Loss 14.532635688781738, Transition Loss 8.520947456359863, Classifier Loss 0.22257210314273834, Total Loss 140.2224884033203\n",
      "38: Encoding Loss 14.719769477844238, Transition Loss 5.3787760734558105, Classifier Loss 0.23203372955322266, Total Loss 142.03729248046875\n",
      "38: Encoding Loss 15.498403549194336, Transition Loss 8.597597122192383, Classifier Loss 0.2115923911333084, Total Loss 146.86598205566406\n",
      "38: Encoding Loss 15.230883598327637, Transition Loss 10.661762237548828, Classifier Loss 0.26257631182670593, Total Loss 150.237060546875\n",
      "38: Encoding Loss 16.51542854309082, Transition Loss 7.073343276977539, Classifier Loss 0.20958246290683746, Total Loss 154.49635314941406\n",
      "38: Encoding Loss 15.629469871520996, Transition Loss 8.810608863830566, Classifier Loss 0.21358904242515564, Total Loss 148.1567840576172\n",
      "38: Encoding Loss 14.547467231750488, Transition Loss 8.479327201843262, Classifier Loss 0.23544470965862274, Total Loss 141.6200714111328\n",
      "38: Encoding Loss 15.717881202697754, Transition Loss 6.291909217834473, Classifier Loss 0.23752683401107788, Total Loss 150.75411987304688\n",
      "38: Encoding Loss 15.376508712768555, Transition Loss 5.72257137298584, Classifier Loss 0.21242326498031616, Total Loss 145.39892578125\n",
      "38: Encoding Loss 15.707844734191895, Transition Loss 4.456045627593994, Classifier Loss 0.21955102682113647, Total Loss 148.50906372070312\n",
      "38: Encoding Loss 16.729795455932617, Transition Loss 6.190448760986328, Classifier Loss 0.22466135025024414, Total Loss 157.54258728027344\n",
      "38: Encoding Loss 16.161706924438477, Transition Loss 8.614521026611328, Classifier Loss 0.23119094967842102, Total Loss 154.13565063476562\n",
      "38: Encoding Loss 15.42556381225586, Transition Loss 9.668201446533203, Classifier Loss 0.22889962792396545, Total Loss 148.22811889648438\n",
      "38: Encoding Loss 16.726320266723633, Transition Loss 8.367480278015137, Classifier Loss 0.23758110404014587, Total Loss 159.24215698242188\n",
      "38: Encoding Loss 15.96061897277832, Transition Loss 7.482414722442627, Classifier Loss 0.2128894031047821, Total Loss 150.4703826904297\n",
      "38: Encoding Loss 16.055084228515625, Transition Loss 7.639389514923096, Classifier Loss 0.19736993312835693, Total Loss 149.70555114746094\n",
      "38: Encoding Loss 16.704917907714844, Transition Loss 5.519377708435059, Classifier Loss 0.2228178232908249, Total Loss 157.02500915527344\n",
      "38: Encoding Loss 16.44536781311035, Transition Loss 6.13677978515625, Classifier Loss 0.20740684866905212, Total Loss 153.53097534179688\n",
      "38: Encoding Loss 15.80321979522705, Transition Loss 5.23192024230957, Classifier Loss 0.20853187143802643, Total Loss 148.32533264160156\n",
      "38: Encoding Loss 16.609867095947266, Transition Loss 8.675539016723633, Classifier Loss 0.23561927676200867, Total Loss 158.17596435546875\n",
      "38: Encoding Loss 15.080740928649902, Transition Loss 8.48501205444336, Classifier Loss 0.20646603405475616, Total Loss 142.98953247070312\n",
      "38: Encoding Loss 14.955629348754883, Transition Loss 6.813936233520508, Classifier Loss 0.2278822958469391, Total Loss 143.79605102539062\n",
      "38: Encoding Loss 15.384965896606445, Transition Loss 6.542026519775391, Classifier Loss 0.21936734020709991, Total Loss 146.3248748779297\n",
      "38: Encoding Loss 15.01573371887207, Transition Loss 6.936922073364258, Classifier Loss 0.2036106288433075, Total Loss 141.87432861328125\n",
      "38: Encoding Loss 14.881044387817383, Transition Loss 5.803436756134033, Classifier Loss 0.20807090401649475, Total Loss 141.01612854003906\n",
      "38: Encoding Loss 15.643507957458496, Transition Loss 6.400120735168457, Classifier Loss 0.20291993021965027, Total Loss 146.7200927734375\n",
      "38: Encoding Loss 16.808841705322266, Transition Loss 7.953152656555176, Classifier Loss 0.23849649727344513, Total Loss 159.91102600097656\n",
      "38: Encoding Loss 17.31020164489746, Transition Loss 7.508027076721191, Classifier Loss 0.2322976142168045, Total Loss 163.21298217773438\n",
      "38: Encoding Loss 15.141837120056152, Transition Loss 5.117306232452393, Classifier Loss 0.2284391224384308, Total Loss 145.0020751953125\n",
      "38: Encoding Loss 15.0048828125, Transition Loss 5.845330715179443, Classifier Loss 0.21435581147670746, Total Loss 142.64370727539062\n",
      "38: Encoding Loss 15.619889259338379, Transition Loss 7.89847993850708, Classifier Loss 0.22422799468040466, Total Loss 148.9616241455078\n",
      "38: Encoding Loss 14.995566368103027, Transition Loss 7.21094274520874, Classifier Loss 0.20273442566394806, Total Loss 141.68016052246094\n",
      "38: Encoding Loss 15.974553108215332, Transition Loss 6.4692864418029785, Classifier Loss 0.2225215882062912, Total Loss 151.34243774414062\n",
      "38: Encoding Loss 16.43419075012207, Transition Loss 7.136651039123535, Classifier Loss 0.18989244103431702, Total Loss 151.89010620117188\n",
      "38: Encoding Loss 15.667142868041992, Transition Loss 7.888695240020752, Classifier Loss 0.22902217507362366, Total Loss 149.81710815429688\n",
      "38: Encoding Loss 15.43234920501709, Transition Loss 7.774168968200684, Classifier Loss 0.24005281925201416, Total Loss 149.0189208984375\n",
      "38: Encoding Loss 16.854686737060547, Transition Loss 6.624269485473633, Classifier Loss 0.2201201617717743, Total Loss 158.17437744140625\n",
      "38: Encoding Loss 15.791696548461914, Transition Loss 8.519234657287598, Classifier Loss 0.20911936461925507, Total Loss 148.94935607910156\n",
      "38: Encoding Loss 15.595566749572754, Transition Loss 7.752147674560547, Classifier Loss 0.1997467577457428, Total Loss 146.28964233398438\n",
      "38: Encoding Loss 16.538516998291016, Transition Loss 8.518189430236816, Classifier Loss 0.21015416085720062, Total Loss 155.02719116210938\n",
      "38: Encoding Loss 14.635746002197266, Transition Loss 9.804694175720215, Classifier Loss 0.24366146326065063, Total Loss 143.4130401611328\n",
      "38: Encoding Loss 15.764089584350586, Transition Loss 7.4792399406433105, Classifier Loss 0.21445074677467346, Total Loss 149.05364990234375\n",
      "38: Encoding Loss 15.342024803161621, Transition Loss 8.853535652160645, Classifier Loss 0.21327272057533264, Total Loss 145.8341827392578\n",
      "38: Encoding Loss 15.645343780517578, Transition Loss 8.677915573120117, Classifier Loss 0.22303277254104614, Total Loss 149.20159912109375\n",
      "38: Encoding Loss 15.275830268859863, Transition Loss 8.844294548034668, Classifier Loss 0.2127857208251953, Total Loss 145.2540740966797\n",
      "38: Encoding Loss 16.97399139404297, Transition Loss 7.176567554473877, Classifier Loss 0.22132456302642822, Total Loss 159.35971069335938\n",
      "38: Encoding Loss 15.343786239624023, Transition Loss 7.5282745361328125, Classifier Loss 0.21205663681030273, Total Loss 145.4616241455078\n",
      "38: Encoding Loss 16.216955184936523, Transition Loss 7.233791351318359, Classifier Loss 0.23030981421470642, Total Loss 154.21337890625\n",
      "38: Encoding Loss 15.719542503356934, Transition Loss 8.148136138916016, Classifier Loss 0.23042967915534973, Total Loss 150.42893981933594\n",
      "38: Encoding Loss 15.597824096679688, Transition Loss 8.144847869873047, Classifier Loss 0.22517241537570953, Total Loss 148.92880249023438\n",
      "38: Encoding Loss 16.68245506286621, Transition Loss 6.788604736328125, Classifier Loss 0.2095719277858734, Total Loss 155.77456665039062\n",
      "38: Encoding Loss 15.291187286376953, Transition Loss 7.715097904205322, Classifier Loss 0.19654534757137299, Total Loss 143.52703857421875\n",
      "38: Encoding Loss 15.585982322692871, Transition Loss 6.165255069732666, Classifier Loss 0.23564857244491577, Total Loss 149.4857635498047\n",
      "38: Encoding Loss 15.47088623046875, Transition Loss 9.71871566772461, Classifier Loss 0.22906483709812164, Total Loss 148.6173095703125\n",
      "38: Encoding Loss 15.144363403320312, Transition Loss 6.492239952087402, Classifier Loss 0.20653019845485687, Total Loss 143.1063690185547\n",
      "38: Encoding Loss 15.504301071166992, Transition Loss 5.600547790527344, Classifier Loss 0.21604619920253754, Total Loss 146.7591552734375\n",
      "38: Encoding Loss 16.121793746948242, Transition Loss 7.088385581970215, Classifier Loss 0.20247405767440796, Total Loss 150.63943481445312\n",
      "38: Encoding Loss 15.80762767791748, Transition Loss 9.325199127197266, Classifier Loss 0.2766602039337158, Total Loss 155.99208068847656\n",
      "38: Encoding Loss 15.99179744720459, Transition Loss 8.626153945922852, Classifier Loss 0.2190728783607483, Total Loss 151.56690979003906\n",
      "38: Encoding Loss 16.723548889160156, Transition Loss 6.981922626495361, Classifier Loss 0.19567684829235077, Total Loss 154.75245666503906\n",
      "38: Encoding Loss 16.533037185668945, Transition Loss 7.540740966796875, Classifier Loss 0.20949333906173706, Total Loss 154.72177124023438\n",
      "38: Encoding Loss 15.350869178771973, Transition Loss 5.20967960357666, Classifier Loss 0.21071822941303253, Total Loss 144.92071533203125\n",
      "38: Encoding Loss 15.837414741516113, Transition Loss 8.856229782104492, Classifier Loss 0.24578562378883362, Total Loss 153.0491180419922\n",
      "38: Encoding Loss 16.100650787353516, Transition Loss 6.56721305847168, Classifier Loss 0.23317664861679077, Total Loss 153.43630981445312\n",
      "38: Encoding Loss 15.438323974609375, Transition Loss 9.597434043884277, Classifier Loss 0.21640139818191528, Total Loss 147.0662078857422\n",
      "38: Encoding Loss 16.244609832763672, Transition Loss 5.474794387817383, Classifier Loss 0.22091904282569885, Total Loss 153.1437530517578\n",
      "38: Encoding Loss 15.777368545532227, Transition Loss 6.791120529174805, Classifier Loss 0.22161231935024261, Total Loss 149.73841857910156\n",
      "38: Encoding Loss 16.459924697875977, Transition Loss 7.075990200042725, Classifier Loss 0.25915002822875977, Total Loss 159.0095977783203\n",
      "38: Encoding Loss 15.75173568725586, Transition Loss 9.62441635131836, Classifier Loss 0.22026249766349792, Total Loss 149.96502685546875\n",
      "38: Encoding Loss 14.807540893554688, Transition Loss 8.320399284362793, Classifier Loss 0.2002982199192047, Total Loss 140.1542205810547\n",
      "38: Encoding Loss 15.345333099365234, Transition Loss 7.9718146324157715, Classifier Loss 0.2117297351360321, Total Loss 145.52999877929688\n",
      "38: Encoding Loss 14.762374877929688, Transition Loss 11.15737247467041, Classifier Loss 0.2000299096107483, Total Loss 140.33346557617188\n",
      "38: Encoding Loss 15.910243034362793, Transition Loss 7.734086990356445, Classifier Loss 0.22063210606575012, Total Loss 150.8919677734375\n",
      "38: Encoding Loss 15.650527000427246, Transition Loss 8.544670104980469, Classifier Loss 0.22489291429519653, Total Loss 149.40245056152344\n",
      "38: Encoding Loss 14.1069917678833, Transition Loss 7.31401252746582, Classifier Loss 0.23480159044265747, Total Loss 137.79888916015625\n",
      "38: Encoding Loss 15.026338577270508, Transition Loss 8.78549575805664, Classifier Loss 0.21117320656776428, Total Loss 143.0851287841797\n",
      "38: Encoding Loss 15.786917686462402, Transition Loss 7.915045738220215, Classifier Loss 0.22290536761283875, Total Loss 150.16888427734375\n",
      "38: Encoding Loss 16.35605239868164, Transition Loss 8.540210723876953, Classifier Loss 0.22848808765411377, Total Loss 155.4052734375\n",
      "38: Encoding Loss 15.48629379272461, Transition Loss 7.879275798797607, Classifier Loss 0.21699628233909607, Total Loss 147.16583251953125\n",
      "38: Encoding Loss 15.865114212036133, Transition Loss 6.302080154418945, Classifier Loss 0.20575262606143951, Total Loss 148.756591796875\n",
      "38: Encoding Loss 15.273580551147461, Transition Loss 5.992220401763916, Classifier Loss 0.23975388705730438, Total Loss 147.36248779296875\n",
      "38: Encoding Loss 15.679961204528809, Transition Loss 8.332138061523438, Classifier Loss 0.21051497757434845, Total Loss 148.15760803222656\n",
      "38: Encoding Loss 16.454832077026367, Transition Loss 7.924924850463867, Classifier Loss 0.2280055433511734, Total Loss 156.02420043945312\n",
      "38: Encoding Loss 15.675654411315918, Transition Loss 7.582997798919678, Classifier Loss 0.22525654733181, Total Loss 149.44749450683594\n",
      "38: Encoding Loss 15.778023719787598, Transition Loss 7.717804431915283, Classifier Loss 0.20997056365013123, Total Loss 148.7648162841797\n",
      "38: Encoding Loss 15.978559494018555, Transition Loss 6.862964630126953, Classifier Loss 0.22012342512607574, Total Loss 151.21340942382812\n",
      "38: Encoding Loss 16.46720314025879, Transition Loss 5.417962074279785, Classifier Loss 0.22511154413223267, Total Loss 155.33236694335938\n",
      "38: Encoding Loss 15.940195083618164, Transition Loss 5.6187238693237305, Classifier Loss 0.20671840012073517, Total Loss 149.31715393066406\n",
      "38: Encoding Loss 16.025981903076172, Transition Loss 8.044845581054688, Classifier Loss 0.20642560720443726, Total Loss 150.4593963623047\n",
      "38: Encoding Loss 16.859006881713867, Transition Loss 5.450891971588135, Classifier Loss 0.21421536803245544, Total Loss 157.38377380371094\n",
      "38: Encoding Loss 17.234655380249023, Transition Loss 5.719349384307861, Classifier Loss 0.2402193546295166, Total Loss 163.0430450439453\n",
      "38: Encoding Loss 14.95172119140625, Transition Loss 8.31936264038086, Classifier Loss 0.20628678798675537, Total Loss 141.9063262939453\n",
      "38: Encoding Loss 14.735994338989258, Transition Loss 8.623983383178711, Classifier Loss 0.2004384994506836, Total Loss 139.65660095214844\n",
      "38: Encoding Loss 15.161174774169922, Transition Loss 7.84658145904541, Classifier Loss 0.23418985307216644, Total Loss 146.2777099609375\n",
      "38: Encoding Loss 15.225530624389648, Transition Loss 8.955586433410645, Classifier Loss 0.23690946400165558, Total Loss 147.28631591796875\n",
      "38: Encoding Loss 14.996953964233398, Transition Loss 11.272896766662598, Classifier Loss 0.22139795124530792, Total Loss 144.37001037597656\n",
      "38: Encoding Loss 16.69091033935547, Transition Loss 7.273353576660156, Classifier Loss 0.21672268211841583, Total Loss 156.6542205810547\n",
      "38: Encoding Loss 16.585844039916992, Transition Loss 6.946866512298584, Classifier Loss 0.19324776530265808, Total Loss 153.40090942382812\n",
      "38: Encoding Loss 14.858685493469238, Transition Loss 6.394567966461182, Classifier Loss 0.20514468848705292, Total Loss 140.66287231445312\n",
      "38: Encoding Loss 15.40249252319336, Transition Loss 6.109636306762695, Classifier Loss 0.22281628847122192, Total Loss 146.72349548339844\n",
      "38: Encoding Loss 15.415685653686523, Transition Loss 7.085082054138184, Classifier Loss 0.21623776853084564, Total Loss 146.3662872314453\n",
      "38: Encoding Loss 15.914470672607422, Transition Loss 7.844180107116699, Classifier Loss 0.21323494613170624, Total Loss 150.20809936523438\n",
      "38: Encoding Loss 15.886285781860352, Transition Loss 6.330456256866455, Classifier Loss 0.21226467192173004, Total Loss 149.58285522460938\n",
      "38: Encoding Loss 15.901000022888184, Transition Loss 7.125579833984375, Classifier Loss 0.24445192515850067, Total Loss 153.07830810546875\n",
      "38: Encoding Loss 15.000943183898926, Transition Loss 5.964947700500488, Classifier Loss 0.19019059836864471, Total Loss 140.2196044921875\n",
      "38: Encoding Loss 15.828972816467285, Transition Loss 7.907505512237549, Classifier Loss 0.1977757066488266, Total Loss 147.99085998535156\n",
      "38: Encoding Loss 16.082286834716797, Transition Loss 5.521610260009766, Classifier Loss 0.2421952337026596, Total Loss 153.98216247558594\n",
      "38: Encoding Loss 15.167726516723633, Transition Loss 8.80732536315918, Classifier Loss 0.22736895084381104, Total Loss 145.8401641845703\n",
      "38: Encoding Loss 15.830902099609375, Transition Loss 7.002120494842529, Classifier Loss 0.24029061198234558, Total Loss 152.07669067382812\n",
      "38: Encoding Loss 16.02105140686035, Transition Loss 8.144388198852539, Classifier Loss 0.24616128206253052, Total Loss 154.41342163085938\n",
      "38: Encoding Loss 15.822806358337402, Transition Loss 7.023481845855713, Classifier Loss 0.2196570783853531, Total Loss 149.95285034179688\n",
      "38: Encoding Loss 15.265653610229492, Transition Loss 7.665061950683594, Classifier Loss 0.21689900755882263, Total Loss 145.34814453125\n",
      "38: Encoding Loss 17.00169563293457, Transition Loss 5.325032711029053, Classifier Loss 0.22867554426193237, Total Loss 159.9461212158203\n",
      "38: Encoding Loss 16.266576766967773, Transition Loss 4.996236801147461, Classifier Loss 0.22573904693126678, Total Loss 153.7057647705078\n",
      "38: Encoding Loss 15.527099609375, Transition Loss 5.985741138458252, Classifier Loss 0.20587244629859924, Total Loss 146.00119018554688\n",
      "38: Encoding Loss 15.207698822021484, Transition Loss 7.684872150421143, Classifier Loss 0.21785685420036316, Total Loss 144.9842529296875\n",
      "38: Encoding Loss 15.316142082214355, Transition Loss 4.689414978027344, Classifier Loss 0.2209353744983673, Total Loss 145.56056213378906\n",
      "38: Encoding Loss 16.066293716430664, Transition Loss 9.753999710083008, Classifier Loss 0.23337207734584808, Total Loss 153.818359375\n",
      "38: Encoding Loss 16.29327964782715, Transition Loss 6.6285247802734375, Classifier Loss 0.21131256222724915, Total Loss 152.80319213867188\n",
      "38: Encoding Loss 17.108116149902344, Transition Loss 7.370089054107666, Classifier Loss 0.22485549747943878, Total Loss 160.82449340820312\n",
      "38: Encoding Loss 16.638961791992188, Transition Loss 8.53682804107666, Classifier Loss 0.21611249446868896, Total Loss 156.43031311035156\n",
      "38: Encoding Loss 15.287490844726562, Transition Loss 10.440835952758789, Classifier Loss 0.2044335901737213, Total Loss 144.83145141601562\n",
      "38: Encoding Loss 13.093841552734375, Transition Loss 12.06451416015625, Classifier Loss 0.20619285106658936, Total Loss 127.78292083740234\n",
      "39: Encoding Loss 16.94746208190918, Transition Loss 5.857487678527832, Classifier Loss 0.22081370651721954, Total Loss 158.8325653076172\n",
      "39: Encoding Loss 16.307355880737305, Transition Loss 6.944136142730713, Classifier Loss 0.2169858068227768, Total Loss 153.54624938964844\n",
      "39: Encoding Loss 16.30484962463379, Transition Loss 6.401641845703125, Classifier Loss 0.23020386695861816, Total Loss 154.73951721191406\n",
      "39: Encoding Loss 15.789007186889648, Transition Loss 7.820290565490723, Classifier Loss 0.2244108021259308, Total Loss 150.31719970703125\n",
      "39: Encoding Loss 17.509700775146484, Transition Loss 6.396076202392578, Classifier Loss 0.2264959216117859, Total Loss 164.00643920898438\n",
      "39: Encoding Loss 14.04934024810791, Transition Loss 8.203243255615234, Classifier Loss 0.2324393093585968, Total Loss 137.27931213378906\n",
      "39: Encoding Loss 16.92719841003418, Transition Loss 5.930996894836426, Classifier Loss 0.23959217965602875, Total Loss 160.56300354003906\n",
      "39: Encoding Loss 17.226200103759766, Transition Loss 7.438006401062012, Classifier Loss 0.23145556449890137, Total Loss 162.4427490234375\n",
      "39: Encoding Loss 15.619197845458984, Transition Loss 6.499697685241699, Classifier Loss 0.2876863479614258, Total Loss 155.0221710205078\n",
      "39: Encoding Loss 16.682588577270508, Transition Loss 6.916914939880371, Classifier Loss 0.23195143043994904, Total Loss 158.0392303466797\n",
      "39: Encoding Loss 16.093406677246094, Transition Loss 5.8173041343688965, Classifier Loss 0.2122362107038498, Total Loss 151.13433837890625\n",
      "39: Encoding Loss 15.76733684539795, Transition Loss 6.812432765960693, Classifier Loss 0.2454843521118164, Total Loss 152.04962158203125\n",
      "39: Encoding Loss 15.646607398986816, Transition Loss 7.313327789306641, Classifier Loss 0.2157324254512787, Total Loss 148.20877075195312\n",
      "39: Encoding Loss 14.966008186340332, Transition Loss 6.435539245605469, Classifier Loss 0.21046017110347748, Total Loss 142.06118774414062\n",
      "39: Encoding Loss 15.105528831481934, Transition Loss 7.666899681091309, Classifier Loss 0.22166448831558228, Total Loss 144.5440673828125\n",
      "39: Encoding Loss 15.962475776672363, Transition Loss 8.924098014831543, Classifier Loss 0.22306960821151733, Total Loss 151.79159545898438\n",
      "39: Encoding Loss 15.87507152557373, Transition Loss 7.197821140289307, Classifier Loss 0.22887538373470306, Total Loss 151.32766723632812\n",
      "39: Encoding Loss 15.29639720916748, Transition Loss 8.44765853881836, Classifier Loss 0.2024218887090683, Total Loss 144.3029022216797\n",
      "39: Encoding Loss 16.46540641784668, Transition Loss 8.118307113647461, Classifier Loss 0.20007921755313873, Total Loss 153.35482788085938\n",
      "39: Encoding Loss 16.30662727355957, Transition Loss 8.187919616699219, Classifier Loss 0.21056875586509705, Total Loss 153.14747619628906\n",
      "39: Encoding Loss 15.678802490234375, Transition Loss 8.548667907714844, Classifier Loss 0.22875794768333435, Total Loss 150.0159454345703\n",
      "39: Encoding Loss 15.752812385559082, Transition Loss 6.660022735595703, Classifier Loss 0.20799347758293152, Total Loss 148.15383911132812\n",
      "39: Encoding Loss 14.273829460144043, Transition Loss 11.269526481628418, Classifier Loss 0.2202351838350296, Total Loss 138.4680633544922\n",
      "39: Encoding Loss 15.636443138122559, Transition Loss 6.453191757202148, Classifier Loss 0.22787809371948242, Total Loss 149.16998291015625\n",
      "39: Encoding Loss 15.65245246887207, Transition Loss 4.7033867835998535, Classifier Loss 0.21405664086341858, Total Loss 147.5659637451172\n",
      "39: Encoding Loss 15.297618865966797, Transition Loss 7.0622429847717285, Classifier Loss 0.20380765199661255, Total Loss 144.17416381835938\n",
      "39: Encoding Loss 15.883994102478027, Transition Loss 7.8656415939331055, Classifier Loss 0.2174830436706543, Total Loss 150.3933868408203\n",
      "39: Encoding Loss 16.012937545776367, Transition Loss 8.51319408416748, Classifier Loss 0.21941477060317993, Total Loss 151.74761962890625\n",
      "39: Encoding Loss 15.955607414245605, Transition Loss 8.750104904174805, Classifier Loss 0.21595528721809387, Total Loss 150.9904022216797\n",
      "39: Encoding Loss 14.601765632629395, Transition Loss 9.167757034301758, Classifier Loss 0.22388844192028046, Total Loss 141.03652954101562\n",
      "39: Encoding Loss 15.68232250213623, Transition Loss 7.726271629333496, Classifier Loss 0.21046021580696106, Total Loss 148.04986572265625\n",
      "39: Encoding Loss 14.80307388305664, Transition Loss 10.404783248901367, Classifier Loss 0.20442809164524078, Total Loss 140.9483642578125\n",
      "39: Encoding Loss 15.72366714477539, Transition Loss 8.30858039855957, Classifier Loss 0.22113722562789917, Total Loss 149.56475830078125\n",
      "39: Encoding Loss 14.92491626739502, Transition Loss 4.325311660766602, Classifier Loss 0.23691421747207642, Total Loss 143.95582580566406\n",
      "39: Encoding Loss 14.517976760864258, Transition Loss 8.271891593933105, Classifier Loss 0.21939200162887573, Total Loss 139.7373809814453\n",
      "39: Encoding Loss 14.722893714904785, Transition Loss 5.593741416931152, Classifier Loss 0.23426374793052673, Total Loss 142.32827758789062\n",
      "39: Encoding Loss 15.493864059448242, Transition Loss 8.33010482788086, Classifier Loss 0.2067691534757614, Total Loss 146.29385375976562\n",
      "39: Encoding Loss 15.272948265075684, Transition Loss 11.532379150390625, Classifier Loss 0.23272383213043213, Total Loss 147.76243591308594\n",
      "39: Encoding Loss 16.480266571044922, Transition Loss 6.922795295715332, Classifier Loss 0.2192228138446808, Total Loss 155.14898681640625\n",
      "39: Encoding Loss 15.637251853942871, Transition Loss 9.436001777648926, Classifier Loss 0.20676064491271973, Total Loss 147.66128540039062\n",
      "39: Encoding Loss 14.568746566772461, Transition Loss 8.138561248779297, Classifier Loss 0.24490174651145935, Total Loss 142.66787719726562\n",
      "39: Encoding Loss 15.672314643859863, Transition Loss 6.737066745758057, Classifier Loss 0.22694046795368195, Total Loss 149.41998291015625\n",
      "39: Encoding Loss 15.328207969665527, Transition Loss 5.572077751159668, Classifier Loss 0.21000605821609497, Total Loss 144.7406768798828\n",
      "39: Encoding Loss 15.703303337097168, Transition Loss 4.600104331970215, Classifier Loss 0.21804603934288025, Total Loss 148.35104370117188\n",
      "39: Encoding Loss 16.73489761352539, Transition Loss 6.027647972106934, Classifier Loss 0.2223493754863739, Total Loss 157.3196563720703\n",
      "39: Encoding Loss 16.176189422607422, Transition Loss 8.748441696166992, Classifier Loss 0.22918282449245453, Total Loss 154.0775146484375\n",
      "39: Encoding Loss 15.423898696899414, Transition Loss 9.713432312011719, Classifier Loss 0.21952545642852783, Total Loss 147.2864227294922\n",
      "39: Encoding Loss 16.71463394165039, Transition Loss 8.381162643432617, Classifier Loss 0.22537493705749512, Total Loss 157.93080139160156\n",
      "39: Encoding Loss 15.95288372039795, Transition Loss 7.5815300941467285, Classifier Loss 0.2062501609325409, Total Loss 149.76438903808594\n",
      "39: Encoding Loss 16.05500030517578, Transition Loss 7.8827009201049805, Classifier Loss 0.1998419165611267, Total Loss 150.000732421875\n",
      "39: Encoding Loss 16.685911178588867, Transition Loss 5.6985764503479, Classifier Loss 0.2185625284910202, Total Loss 156.48324584960938\n",
      "39: Encoding Loss 16.45772361755371, Transition Loss 6.280966758728027, Classifier Loss 0.21473073959350586, Total Loss 154.39105224609375\n",
      "39: Encoding Loss 15.75823974609375, Transition Loss 5.285341262817383, Classifier Loss 0.21417301893234253, Total Loss 148.540283203125\n",
      "39: Encoding Loss 16.55487060546875, Transition Loss 8.558889389038086, Classifier Loss 0.2285047471523285, Total Loss 157.001220703125\n",
      "39: Encoding Loss 15.072327613830566, Transition Loss 8.351821899414062, Classifier Loss 0.2096613645553589, Total Loss 143.2151336669922\n",
      "39: Encoding Loss 14.931705474853516, Transition Loss 6.738009452819824, Classifier Loss 0.22926083207130432, Total Loss 143.72731018066406\n",
      "39: Encoding Loss 15.380570411682129, Transition Loss 6.636186122894287, Classifier Loss 0.21523678302764893, Total Loss 145.89549255371094\n",
      "39: Encoding Loss 14.993693351745605, Transition Loss 6.605691432952881, Classifier Loss 0.20088869333267212, Total Loss 141.35955810546875\n",
      "39: Encoding Loss 14.844268798828125, Transition Loss 6.149435043334961, Classifier Loss 0.20120997726917267, Total Loss 140.10504150390625\n",
      "39: Encoding Loss 15.617264747619629, Transition Loss 6.044408798217773, Classifier Loss 0.19959543645381927, Total Loss 146.10655212402344\n",
      "39: Encoding Loss 16.781843185424805, Transition Loss 8.25019645690918, Classifier Loss 0.22181476652622223, Total Loss 158.0862579345703\n",
      "39: Encoding Loss 17.325517654418945, Transition Loss 7.165422439575195, Classifier Loss 0.23476243019104004, Total Loss 163.5134735107422\n",
      "39: Encoding Loss 15.14621639251709, Transition Loss 5.308372974395752, Classifier Loss 0.22416119277477264, Total Loss 144.64752197265625\n",
      "39: Encoding Loss 14.998983383178711, Transition Loss 5.576817512512207, Classifier Loss 0.21112531423568726, Total Loss 142.21975708007812\n",
      "39: Encoding Loss 15.622899055480957, Transition Loss 8.185004234313965, Classifier Loss 0.2222406417131424, Total Loss 148.84425354003906\n",
      "39: Encoding Loss 14.97392463684082, Transition Loss 6.883415699005127, Classifier Loss 0.22034378349781036, Total Loss 143.20245361328125\n",
      "39: Encoding Loss 15.981996536254883, Transition Loss 6.668806552886963, Classifier Loss 0.22335466742515564, Total Loss 151.5251922607422\n",
      "39: Encoding Loss 16.449378967285156, Transition Loss 6.68490743637085, Classifier Loss 0.1908060610294342, Total Loss 152.0126190185547\n",
      "39: Encoding Loss 15.670319557189941, Transition Loss 8.507083892822266, Classifier Loss 0.2282482087612152, Total Loss 149.8887939453125\n",
      "39: Encoding Loss 15.449552536010742, Transition Loss 7.043557643890381, Classifier Loss 0.23025622963905334, Total Loss 148.03076171875\n",
      "39: Encoding Loss 16.846532821655273, Transition Loss 7.0926971435546875, Classifier Loss 0.2116180956363678, Total Loss 157.35260009765625\n",
      "39: Encoding Loss 15.77736759185791, Transition Loss 7.699807167053223, Classifier Loss 0.20327842235565186, Total Loss 148.0867462158203\n",
      "39: Encoding Loss 15.560651779174805, Transition Loss 8.021088600158691, Classifier Loss 0.19670608639717102, Total Loss 145.76004028320312\n",
      "39: Encoding Loss 16.549440383911133, Transition Loss 8.058738708496094, Classifier Loss 0.20684239268302917, Total Loss 154.69149780273438\n",
      "39: Encoding Loss 14.623560905456543, Transition Loss 9.946866989135742, Classifier Loss 0.25005674362182617, Total Loss 143.98353576660156\n",
      "39: Encoding Loss 15.74242115020752, Transition Loss 7.1439127922058105, Classifier Loss 0.21458134055137634, Total Loss 148.8262939453125\n",
      "39: Encoding Loss 15.360213279724121, Transition Loss 9.022385597229004, Classifier Loss 0.2112056016921997, Total Loss 145.80673217773438\n",
      "39: Encoding Loss 15.653300285339355, Transition Loss 8.260015487670898, Classifier Loss 0.23092839121818542, Total Loss 149.97125244140625\n",
      "39: Encoding Loss 15.252147674560547, Transition Loss 9.00007438659668, Classifier Loss 0.21472550928592682, Total Loss 145.28976440429688\n",
      "39: Encoding Loss 16.95557403564453, Transition Loss 6.565287113189697, Classifier Loss 0.21360604465007782, Total Loss 158.31826782226562\n",
      "39: Encoding Loss 15.320314407348633, Transition Loss 7.769222259521484, Classifier Loss 0.21234291791915894, Total Loss 145.3506622314453\n",
      "39: Encoding Loss 16.234737396240234, Transition Loss 6.528465270996094, Classifier Loss 0.23235681653022766, Total Loss 154.41929626464844\n",
      "39: Encoding Loss 15.713889122009277, Transition Loss 8.65522289276123, Classifier Loss 0.22570905089378357, Total Loss 150.0130615234375\n",
      "39: Encoding Loss 15.62516975402832, Transition Loss 7.322385311126709, Classifier Loss 0.2171972393989563, Total Loss 148.18556213378906\n",
      "39: Encoding Loss 16.699127197265625, Transition Loss 7.2107625007629395, Classifier Loss 0.21528083086013794, Total Loss 156.56324768066406\n",
      "39: Encoding Loss 15.252518653869629, Transition Loss 7.132355690002441, Classifier Loss 0.19308650493621826, Total Loss 142.75527954101562\n",
      "39: Encoding Loss 15.587278366088867, Transition Loss 6.576697826385498, Classifier Loss 0.2235892117023468, Total Loss 148.37249755859375\n",
      "39: Encoding Loss 15.464061737060547, Transition Loss 8.723227500915527, Classifier Loss 0.23184899985790253, Total Loss 148.6420440673828\n",
      "39: Encoding Loss 15.152087211608887, Transition Loss 6.851352214813232, Classifier Loss 0.21230125427246094, Total Loss 143.8170928955078\n",
      "39: Encoding Loss 15.515377044677734, Transition Loss 5.077312469482422, Classifier Loss 0.2152489423751831, Total Loss 146.6633758544922\n",
      "39: Encoding Loss 16.110740661621094, Transition Loss 7.408801078796387, Classifier Loss 0.19077648222446442, Total Loss 149.44534301757812\n",
      "39: Encoding Loss 15.773154258728027, Transition Loss 8.721166610717773, Classifier Loss 0.27493152022361755, Total Loss 155.42262268066406\n",
      "39: Encoding Loss 15.95391845703125, Transition Loss 8.647394180297852, Classifier Loss 0.21424733102321625, Total Loss 150.78555297851562\n",
      "39: Encoding Loss 16.726375579833984, Transition Loss 6.497735977172852, Classifier Loss 0.19867710769176483, Total Loss 154.978271484375\n",
      "39: Encoding Loss 16.501848220825195, Transition Loss 7.516469478607178, Classifier Loss 0.20805908739566803, Total Loss 154.32398986816406\n",
      "39: Encoding Loss 15.329789161682129, Transition Loss 4.959324836730957, Classifier Loss 0.2053973376750946, Total Loss 144.169921875\n",
      "39: Encoding Loss 15.830469131469727, Transition Loss 8.580077171325684, Classifier Loss 0.24529720842838287, Total Loss 152.88949584960938\n",
      "39: Encoding Loss 16.107044219970703, Transition Loss 6.3332600593566895, Classifier Loss 0.23589521646499634, Total Loss 153.7125244140625\n",
      "39: Encoding Loss 15.41278076171875, Transition Loss 9.486422538757324, Classifier Loss 0.2289039045572281, Total Loss 148.0899200439453\n",
      "39: Encoding Loss 16.240718841552734, Transition Loss 5.35021448135376, Classifier Loss 0.216765895485878, Total Loss 152.67239379882812\n",
      "39: Encoding Loss 15.769434928894043, Transition Loss 6.7320356369018555, Classifier Loss 0.2064245045185089, Total Loss 148.1443328857422\n",
      "39: Encoding Loss 16.451927185058594, Transition Loss 7.120021343231201, Classifier Loss 0.248228520154953, Total Loss 157.86227416992188\n",
      "39: Encoding Loss 15.766398429870605, Transition Loss 9.513006210327148, Classifier Loss 0.22433458268642426, Total Loss 150.46725463867188\n",
      "39: Encoding Loss 14.818617820739746, Transition Loss 8.207504272460938, Classifier Loss 0.20080143213272095, Total Loss 140.2705841064453\n",
      "39: Encoding Loss 15.327417373657227, Transition Loss 7.928316116333008, Classifier Loss 0.21749356389045715, Total Loss 145.95436096191406\n",
      "39: Encoding Loss 14.69936752319336, Transition Loss 10.84465503692627, Classifier Loss 0.1973724365234375, Total Loss 139.50111389160156\n",
      "39: Encoding Loss 15.901412010192871, Transition Loss 7.640376091003418, Classifier Loss 0.217157244682312, Total Loss 150.45509338378906\n",
      "39: Encoding Loss 15.62490463256836, Transition Loss 8.357022285461426, Classifier Loss 0.21658733487129211, Total Loss 148.32937622070312\n",
      "39: Encoding Loss 14.116460800170898, Transition Loss 7.211688041687012, Classifier Loss 0.22404572367668152, Total Loss 136.7786102294922\n",
      "39: Encoding Loss 15.038224220275879, Transition Loss 8.562384605407715, Classifier Loss 0.2154242992401123, Total Loss 143.5607147216797\n",
      "39: Encoding Loss 15.79365348815918, Transition Loss 7.816945552825928, Classifier Loss 0.2187906801700592, Total Loss 149.79168701171875\n",
      "39: Encoding Loss 16.36165428161621, Transition Loss 8.473132133483887, Classifier Loss 0.228617861866951, Total Loss 155.44964599609375\n",
      "39: Encoding Loss 15.476064682006836, Transition Loss 7.763425350189209, Classifier Loss 0.20821331441402435, Total Loss 146.1825408935547\n",
      "39: Encoding Loss 15.85361099243164, Transition Loss 6.331153869628906, Classifier Loss 0.20119567215442657, Total Loss 148.21469116210938\n",
      "39: Encoding Loss 15.267319679260254, Transition Loss 6.064553260803223, Classifier Loss 0.24064260721206665, Total Loss 147.4157257080078\n",
      "39: Encoding Loss 15.711625099182129, Transition Loss 8.352952003479004, Classifier Loss 0.20815005898475647, Total Loss 148.17860412597656\n",
      "39: Encoding Loss 16.441503524780273, Transition Loss 7.956269264221191, Classifier Loss 0.21974089741706848, Total Loss 155.0973663330078\n",
      "39: Encoding Loss 15.660261154174805, Transition Loss 7.733596324920654, Classifier Loss 0.22801755368709564, Total Loss 149.6305694580078\n",
      "39: Encoding Loss 15.740744590759277, Transition Loss 7.739202499389648, Classifier Loss 0.2093072086572647, Total Loss 148.40451049804688\n",
      "39: Encoding Loss 15.985491752624512, Transition Loss 6.901874542236328, Classifier Loss 0.2178206741809845, Total Loss 151.04637145996094\n",
      "39: Encoding Loss 16.42927360534668, Transition Loss 5.331335544586182, Classifier Loss 0.2196396291255951, Total Loss 154.46441650390625\n",
      "39: Encoding Loss 15.9102783203125, Transition Loss 5.622393608093262, Classifier Loss 0.21379560232162476, Total Loss 149.78627014160156\n",
      "39: Encoding Loss 16.04584312438965, Transition Loss 8.072833061218262, Classifier Loss 0.20801107585430145, Total Loss 150.78240966796875\n",
      "39: Encoding Loss 16.815649032592773, Transition Loss 5.510260105133057, Classifier Loss 0.21241968870162964, Total Loss 156.8692169189453\n",
      "39: Encoding Loss 17.228221893310547, Transition Loss 5.687542915344238, Classifier Loss 0.23627550899982452, Total Loss 162.59085083007812\n",
      "39: Encoding Loss 14.941511154174805, Transition Loss 8.21212100982666, Classifier Loss 0.20440350472927094, Total Loss 141.6148681640625\n",
      "39: Encoding Loss 14.73840618133545, Transition Loss 8.551546096801758, Classifier Loss 0.19807016849517822, Total Loss 139.42457580566406\n",
      "39: Encoding Loss 15.139325141906738, Transition Loss 7.780855178833008, Classifier Loss 0.23796866834163666, Total Loss 146.46763610839844\n",
      "39: Encoding Loss 15.234557151794434, Transition Loss 8.500224113464355, Classifier Loss 0.2337457835674286, Total Loss 146.95108032226562\n",
      "39: Encoding Loss 14.984272956848145, Transition Loss 11.03702163696289, Classifier Loss 0.2278466820716858, Total Loss 144.86624145507812\n",
      "39: Encoding Loss 16.684038162231445, Transition Loss 7.127170085906982, Classifier Loss 0.2279260903596878, Total Loss 157.69033813476562\n",
      "39: Encoding Loss 16.57761573791504, Transition Loss 6.810176372528076, Classifier Loss 0.1837097406387329, Total Loss 152.3539276123047\n",
      "39: Encoding Loss 14.87356948852539, Transition Loss 6.278162479400635, Classifier Loss 0.2009856104850769, Total Loss 140.34274291992188\n",
      "39: Encoding Loss 15.384748458862305, Transition Loss 6.0080413818359375, Classifier Loss 0.22265096008777618, Total Loss 146.54470825195312\n",
      "39: Encoding Loss 15.40782356262207, Transition Loss 6.8820085525512695, Classifier Loss 0.21808554232120514, Total Loss 146.44754028320312\n",
      "39: Encoding Loss 15.889002799987793, Transition Loss 7.70231294631958, Classifier Loss 0.21848417818546295, Total Loss 150.5009002685547\n",
      "39: Encoding Loss 15.864975929260254, Transition Loss 6.044553756713867, Classifier Loss 0.21332457661628723, Total Loss 149.461181640625\n",
      "39: Encoding Loss 15.922080039978027, Transition Loss 6.958709239959717, Classifier Loss 0.23564974963665009, Total Loss 152.33335876464844\n",
      "39: Encoding Loss 14.983960151672363, Transition Loss 5.750973701477051, Classifier Loss 0.19122472405433655, Total Loss 140.14434814453125\n",
      "39: Encoding Loss 15.793753623962402, Transition Loss 7.786645412445068, Classifier Loss 0.18605640530586243, Total Loss 146.51300048828125\n",
      "39: Encoding Loss 16.075857162475586, Transition Loss 5.274391174316406, Classifier Loss 0.24605652689933777, Total Loss 154.26739501953125\n",
      "39: Encoding Loss 15.150315284729004, Transition Loss 8.84827995300293, Classifier Loss 0.23677489161491394, Total Loss 146.64967346191406\n",
      "39: Encoding Loss 15.810583114624023, Transition Loss 6.592473983764648, Classifier Loss 0.22154280543327332, Total Loss 149.9574432373047\n",
      "39: Encoding Loss 16.018922805786133, Transition Loss 8.07483959197998, Classifier Loss 0.23333901166915894, Total Loss 153.1002655029297\n",
      "39: Encoding Loss 15.822320938110352, Transition Loss 6.79966926574707, Classifier Loss 0.21358489990234375, Total Loss 149.2969970703125\n",
      "39: Encoding Loss 15.253159523010254, Transition Loss 7.714366912841797, Classifier Loss 0.21642372012138367, Total Loss 145.21054077148438\n",
      "39: Encoding Loss 16.993000030517578, Transition Loss 5.179209232330322, Classifier Loss 0.22232912480831146, Total Loss 159.21275329589844\n",
      "39: Encoding Loss 16.216188430786133, Transition Loss 4.876016616821289, Classifier Loss 0.2126675546169281, Total Loss 151.97146606445312\n",
      "39: Encoding Loss 15.49411392211914, Transition Loss 5.989466667175293, Classifier Loss 0.20713762938976288, Total Loss 145.86456298828125\n",
      "39: Encoding Loss 15.193791389465332, Transition Loss 7.54795503616333, Classifier Loss 0.21852800250053406, Total Loss 144.91273498535156\n",
      "39: Encoding Loss 15.327693939208984, Transition Loss 4.669265270233154, Classifier Loss 0.21667951345443726, Total Loss 145.22335815429688\n",
      "39: Encoding Loss 16.107337951660156, Transition Loss 9.24628734588623, Classifier Loss 0.22134876251220703, Total Loss 152.84283447265625\n",
      "39: Encoding Loss 16.29357147216797, Transition Loss 6.639594078063965, Classifier Loss 0.21102403104305267, Total Loss 152.7788848876953\n",
      "39: Encoding Loss 17.097972869873047, Transition Loss 6.873125076293945, Classifier Loss 0.22690407931804657, Total Loss 160.8488311767578\n",
      "39: Encoding Loss 16.645559310913086, Transition Loss 8.854999542236328, Classifier Loss 0.21611981093883514, Total Loss 156.54745483398438\n",
      "39: Encoding Loss 15.329092979431152, Transition Loss 9.977616310119629, Classifier Loss 0.1994250863790512, Total Loss 144.57078552246094\n",
      "39: Encoding Loss 13.09196949005127, Transition Loss 12.32739543914795, Classifier Loss 0.20321351289749146, Total Loss 127.5225830078125\n",
      "40: Encoding Loss 16.90117645263672, Transition Loss 5.610036849975586, Classifier Loss 0.21830953657627106, Total Loss 158.16236877441406\n",
      "40: Encoding Loss 16.269725799560547, Transition Loss 6.866614818572998, Classifier Loss 0.21207576990127563, Total Loss 152.7387237548828\n",
      "40: Encoding Loss 16.299562454223633, Transition Loss 6.2142181396484375, Classifier Loss 0.2185797244310379, Total Loss 153.497314453125\n",
      "40: Encoding Loss 15.812827110290527, Transition Loss 7.532305717468262, Classifier Loss 0.22010807693004608, Total Loss 150.01988220214844\n",
      "40: Encoding Loss 17.51621437072754, Transition Loss 6.343649387359619, Classifier Loss 0.23494075238704681, Total Loss 164.89251708984375\n",
      "40: Encoding Loss 14.037195205688477, Transition Loss 7.656947135925293, Classifier Loss 0.2292698621749878, Total Loss 136.7559356689453\n",
      "40: Encoding Loss 16.94305992126465, Transition Loss 5.83287239074707, Classifier Loss 0.22872237861156464, Total Loss 159.5832977294922\n",
      "40: Encoding Loss 17.233667373657227, Transition Loss 6.948597431182861, Classifier Loss 0.2317749261856079, Total Loss 162.43655395507812\n",
      "40: Encoding Loss 15.618371963500977, Transition Loss 6.459070682525635, Classifier Loss 0.2680900990962982, Total Loss 153.04779052734375\n",
      "40: Encoding Loss 16.67901611328125, Transition Loss 6.696253299713135, Classifier Loss 0.22847388684749603, Total Loss 157.61875915527344\n",
      "40: Encoding Loss 16.095726013183594, Transition Loss 5.676907539367676, Classifier Loss 0.21600700914859772, Total Loss 151.50189208984375\n",
      "40: Encoding Loss 15.7901029586792, Transition Loss 6.484487056732178, Classifier Loss 0.23384782671928406, Total Loss 151.00250244140625\n",
      "40: Encoding Loss 15.673975944519043, Transition Loss 7.190094470977783, Classifier Loss 0.21623578667640686, Total Loss 148.45339965820312\n",
      "40: Encoding Loss 14.978909492492676, Transition Loss 6.2652740478515625, Classifier Loss 0.21109777688980103, Total Loss 142.19410705566406\n",
      "40: Encoding Loss 15.110093116760254, Transition Loss 7.54428768157959, Classifier Loss 0.2327640950679779, Total Loss 145.666015625\n",
      "40: Encoding Loss 15.9645357131958, Transition Loss 8.424089431762695, Classifier Loss 0.22111786901950836, Total Loss 151.5128936767578\n",
      "40: Encoding Loss 15.842843055725098, Transition Loss 6.9433722496032715, Classifier Loss 0.20872211456298828, Total Loss 149.00363159179688\n",
      "40: Encoding Loss 15.295279502868652, Transition Loss 8.131257057189941, Classifier Loss 0.2053975611925125, Total Loss 144.5282440185547\n",
      "40: Encoding Loss 16.446218490600586, Transition Loss 7.84758996963501, Classifier Loss 0.20420098304748535, Total Loss 153.55935668945312\n",
      "40: Encoding Loss 16.294301986694336, Transition Loss 7.954495906829834, Classifier Loss 0.21142174303531647, Total Loss 153.0874786376953\n",
      "40: Encoding Loss 15.678961753845215, Transition Loss 8.207528114318848, Classifier Loss 0.22420431673526764, Total Loss 149.49363708496094\n",
      "40: Encoding Loss 15.729473114013672, Transition Loss 6.37376070022583, Classifier Loss 0.2057512402534485, Total Loss 147.6856689453125\n",
      "40: Encoding Loss 14.28255558013916, Transition Loss 11.065038681030273, Classifier Loss 0.2073061168193817, Total Loss 137.20407104492188\n",
      "40: Encoding Loss 15.603550910949707, Transition Loss 6.423131465911865, Classifier Loss 0.2189885526895523, Total Loss 148.0118865966797\n",
      "40: Encoding Loss 15.642247200012207, Transition Loss 4.541243553161621, Classifier Loss 0.21455322206020355, Total Loss 147.5015411376953\n",
      "40: Encoding Loss 15.325844764709473, Transition Loss 6.8661065101623535, Classifier Loss 0.1999170184135437, Total Loss 143.9716796875\n",
      "40: Encoding Loss 15.877303123474121, Transition Loss 7.674532890319824, Classifier Loss 0.21031273901462555, Total Loss 149.58460998535156\n",
      "40: Encoding Loss 16.003009796142578, Transition Loss 8.287191390991211, Classifier Loss 0.21653778851032257, Total Loss 151.33529663085938\n",
      "40: Encoding Loss 15.934426307678223, Transition Loss 8.510231018066406, Classifier Loss 0.21204134821891785, Total Loss 150.381591796875\n",
      "40: Encoding Loss 14.582016944885254, Transition Loss 8.881129264831543, Classifier Loss 0.22589413821697235, Total Loss 141.02178955078125\n",
      "40: Encoding Loss 15.669054985046387, Transition Loss 7.675947189331055, Classifier Loss 0.2063312977552414, Total Loss 147.520751953125\n",
      "40: Encoding Loss 14.767325401306152, Transition Loss 10.159431457519531, Classifier Loss 0.2084638625383377, Total Loss 141.01687622070312\n",
      "40: Encoding Loss 15.737407684326172, Transition Loss 8.301034927368164, Classifier Loss 0.21921274065971375, Total Loss 149.48074340820312\n",
      "40: Encoding Loss 14.915009498596191, Transition Loss 4.174501419067383, Classifier Loss 0.22864514589309692, Total Loss 143.01950073242188\n",
      "40: Encoding Loss 14.5156888961792, Transition Loss 8.380209922790527, Classifier Loss 0.21349066495895386, Total Loss 139.15061950683594\n",
      "40: Encoding Loss 14.711040496826172, Transition Loss 5.380344867706299, Classifier Loss 0.22877877950668335, Total Loss 141.64227294921875\n",
      "40: Encoding Loss 15.510260581970215, Transition Loss 8.472538948059082, Classifier Loss 0.2057039439678192, Total Loss 146.34698486328125\n",
      "40: Encoding Loss 15.230249404907227, Transition Loss 10.533750534057617, Classifier Loss 0.2379504293203354, Total Loss 147.74378967285156\n",
      "40: Encoding Loss 16.47933006286621, Transition Loss 6.959263801574707, Classifier Loss 0.2086203545331955, Total Loss 154.08851623535156\n",
      "40: Encoding Loss 15.617853164672852, Transition Loss 8.809484481811523, Classifier Loss 0.2111871838569641, Total Loss 147.8234405517578\n",
      "40: Encoding Loss 14.536696434020996, Transition Loss 8.295019149780273, Classifier Loss 0.24374671280384064, Total Loss 142.32723999023438\n",
      "40: Encoding Loss 15.689821243286133, Transition Loss 6.153837203979492, Classifier Loss 0.2306416630744934, Total Loss 149.81350708007812\n",
      "40: Encoding Loss 15.329906463623047, Transition Loss 5.524385929107666, Classifier Loss 0.20115798711776733, Total Loss 143.8599395751953\n",
      "40: Encoding Loss 15.687967300415039, Transition Loss 4.49546480178833, Classifier Loss 0.21932269632816315, Total Loss 148.33509826660156\n",
      "40: Encoding Loss 16.736854553222656, Transition Loss 5.947081089019775, Classifier Loss 0.21714404225349426, Total Loss 156.7986602783203\n",
      "40: Encoding Loss 16.15611457824707, Transition Loss 8.610566139221191, Classifier Loss 0.22877095639705658, Total Loss 153.84811401367188\n",
      "40: Encoding Loss 15.417044639587402, Transition Loss 9.360130310058594, Classifier Loss 0.21992233395576477, Total Loss 147.2006072998047\n",
      "40: Encoding Loss 16.712139129638672, Transition Loss 8.28214168548584, Classifier Loss 0.21350060403347015, Total Loss 156.70361328125\n",
      "40: Encoding Loss 15.963836669921875, Transition Loss 7.190532684326172, Classifier Loss 0.20735548436641693, Total Loss 149.8843536376953\n",
      "40: Encoding Loss 16.046451568603516, Transition Loss 7.802495956420898, Classifier Loss 0.19560426473617554, Total Loss 149.49253845214844\n",
      "40: Encoding Loss 16.65460777282715, Transition Loss 5.430640697479248, Classifier Loss 0.21427476406097412, Total Loss 155.75045776367188\n",
      "40: Encoding Loss 16.410844802856445, Transition Loss 6.196920871734619, Classifier Loss 0.2071540504693985, Total Loss 153.24154663085938\n",
      "40: Encoding Loss 15.740701675415039, Transition Loss 5.129118919372559, Classifier Loss 0.20313502848148346, Total Loss 147.2649383544922\n",
      "40: Encoding Loss 16.54686164855957, Transition Loss 8.810623168945312, Classifier Loss 0.22808507084846497, Total Loss 156.94552612304688\n",
      "40: Encoding Loss 15.022510528564453, Transition Loss 8.115769386291504, Classifier Loss 0.20466788113117218, Total Loss 142.27001953125\n",
      "40: Encoding Loss 14.934650421142578, Transition Loss 6.980661392211914, Classifier Loss 0.22364331781864166, Total Loss 143.23765563964844\n",
      "40: Encoding Loss 15.376114845275879, Transition Loss 6.363835334777832, Classifier Loss 0.21051812171936035, Total Loss 145.33349609375\n",
      "40: Encoding Loss 15.013707160949707, Transition Loss 6.9613518714904785, Classifier Loss 0.2006390392780304, Total Loss 141.56582641601562\n",
      "40: Encoding Loss 14.838502883911133, Transition Loss 5.6095709800720215, Classifier Loss 0.19644084572792053, Total Loss 139.47402954101562\n",
      "40: Encoding Loss 15.616750717163086, Transition Loss 6.40193510055542, Classifier Loss 0.18975599110126495, Total Loss 145.1899871826172\n",
      "40: Encoding Loss 16.7806396484375, Transition Loss 7.933855056762695, Classifier Loss 0.22716256976127625, Total Loss 158.54815673828125\n",
      "40: Encoding Loss 17.302677154541016, Transition Loss 7.190079212188721, Classifier Loss 0.22778815031051636, Total Loss 162.63824462890625\n",
      "40: Encoding Loss 15.158233642578125, Transition Loss 5.104522705078125, Classifier Loss 0.2191172093153, Total Loss 144.198486328125\n",
      "40: Encoding Loss 14.98900318145752, Transition Loss 5.5864481925964355, Classifier Loss 0.20074176788330078, Total Loss 141.10350036621094\n",
      "40: Encoding Loss 15.590861320495605, Transition Loss 7.865571022033691, Classifier Loss 0.21228715777397156, Total Loss 147.5287322998047\n",
      "40: Encoding Loss 14.930031776428223, Transition Loss 6.8766889572143555, Classifier Loss 0.20147724449634552, Total Loss 140.96331787109375\n",
      "40: Encoding Loss 15.978714942932129, Transition Loss 6.49970817565918, Classifier Loss 0.21151115000247955, Total Loss 150.28077697753906\n",
      "40: Encoding Loss 16.443599700927734, Transition Loss 6.535754680633545, Classifier Loss 0.19005921483039856, Total Loss 151.86187744140625\n",
      "40: Encoding Loss 15.652101516723633, Transition Loss 8.107268333435059, Classifier Loss 0.22277820110321045, Total Loss 149.1160888671875\n",
      "40: Encoding Loss 15.420526504516602, Transition Loss 7.138387680053711, Classifier Loss 0.23776742815971375, Total Loss 148.56863403320312\n",
      "40: Encoding Loss 16.84136199951172, Transition Loss 6.992101669311523, Classifier Loss 0.21788577735424042, Total Loss 157.9178924560547\n",
      "40: Encoding Loss 15.742923736572266, Transition Loss 7.842552661895752, Classifier Loss 0.20644235610961914, Total Loss 148.1561279296875\n",
      "40: Encoding Loss 15.559531211853027, Transition Loss 7.745497703552246, Classifier Loss 0.1985747218132019, Total Loss 145.88282775878906\n",
      "40: Encoding Loss 16.543630599975586, Transition Loss 8.058745384216309, Classifier Loss 0.20685450732707977, Total Loss 154.64625549316406\n",
      "40: Encoding Loss 14.649693489074707, Transition Loss 9.716175079345703, Classifier Loss 0.2529579699039459, Total Loss 144.43658447265625\n",
      "40: Encoding Loss 15.768855094909668, Transition Loss 7.182230472564697, Classifier Loss 0.21160008013248444, Total Loss 148.74729919433594\n",
      "40: Encoding Loss 15.338272094726562, Transition Loss 8.783903121948242, Classifier Loss 0.20489853620529175, Total Loss 144.9528045654297\n",
      "40: Encoding Loss 15.666125297546387, Transition Loss 8.421331405639648, Classifier Loss 0.21872097253799438, Total Loss 148.88536071777344\n",
      "40: Encoding Loss 15.207880020141602, Transition Loss 8.9288330078125, Classifier Loss 0.21059386432170868, Total Loss 144.50819396972656\n",
      "40: Encoding Loss 16.966550827026367, Transition Loss 6.861084461212158, Classifier Loss 0.21950143575668335, Total Loss 159.05477905273438\n",
      "40: Encoding Loss 15.325136184692383, Transition Loss 7.380029201507568, Classifier Loss 0.2232426255941391, Total Loss 146.4013671875\n",
      "40: Encoding Loss 16.236825942993164, Transition Loss 6.977750301361084, Classifier Loss 0.22942256927490234, Total Loss 154.232421875\n",
      "40: Encoding Loss 15.707544326782227, Transition Loss 7.921867370605469, Classifier Loss 0.22334767878055573, Total Loss 149.57949829101562\n",
      "40: Encoding Loss 15.619004249572754, Transition Loss 7.976676940917969, Classifier Loss 0.2131979912519455, Total Loss 147.86717224121094\n",
      "40: Encoding Loss 16.66686248779297, Transition Loss 6.491159439086914, Classifier Loss 0.20412306487560272, Total Loss 155.04544067382812\n",
      "40: Encoding Loss 15.232892036437988, Transition Loss 7.654512405395508, Classifier Loss 0.19692562520503998, Total Loss 143.0865936279297\n",
      "40: Encoding Loss 15.589754104614258, Transition Loss 5.907129764556885, Classifier Loss 0.22251290082931519, Total Loss 148.1507568359375\n",
      "40: Encoding Loss 15.459061622619629, Transition Loss 9.750589370727539, Classifier Loss 0.22613944113254547, Total Loss 148.23655700683594\n",
      "40: Encoding Loss 15.13698959350586, Transition Loss 6.190066814422607, Classifier Loss 0.21140329539775848, Total Loss 143.47425842285156\n",
      "40: Encoding Loss 15.509262084960938, Transition Loss 5.731546401977539, Classifier Loss 0.21808433532714844, Total Loss 147.0288543701172\n",
      "40: Encoding Loss 16.084463119506836, Transition Loss 6.669963836669922, Classifier Loss 0.19575057923793793, Total Loss 149.5847625732422\n",
      "40: Encoding Loss 15.79220962524414, Transition Loss 9.439386367797852, Classifier Loss 0.27078261971473694, Total Loss 155.30380249023438\n",
      "40: Encoding Loss 15.94754695892334, Transition Loss 8.265239715576172, Classifier Loss 0.21510466933250427, Total Loss 150.743896484375\n",
      "40: Encoding Loss 16.741714477539062, Transition Loss 6.973292350769043, Classifier Loss 0.19352759420871735, Total Loss 154.68112182617188\n",
      "40: Encoding Loss 16.513166427612305, Transition Loss 7.226784706115723, Classifier Loss 0.20290401577949524, Total Loss 153.84109497070312\n",
      "40: Encoding Loss 15.277911186218262, Transition Loss 5.167516231536865, Classifier Loss 0.2026221752166748, Total Loss 143.51901245117188\n",
      "40: Encoding Loss 15.829365730285645, Transition Loss 8.558890342712402, Classifier Loss 0.24264201521873474, Total Loss 152.61090087890625\n",
      "40: Encoding Loss 16.069395065307617, Transition Loss 6.514908790588379, Classifier Loss 0.22954480350017548, Total Loss 152.8126220703125\n",
      "40: Encoding Loss 15.399712562561035, Transition Loss 9.474099159240723, Classifier Loss 0.22430852055549622, Total Loss 147.52337646484375\n",
      "40: Encoding Loss 16.248760223388672, Transition Loss 5.277760982513428, Classifier Loss 0.2135317176580429, Total Loss 152.39881896972656\n",
      "40: Encoding Loss 15.772945404052734, Transition Loss 6.534914016723633, Classifier Loss 0.20485900342464447, Total Loss 147.9764404296875\n",
      "40: Encoding Loss 16.441377639770508, Transition Loss 6.9863409996032715, Classifier Loss 0.24747851490974426, Total Loss 157.67613220214844\n",
      "40: Encoding Loss 15.72992992401123, Transition Loss 9.466065406799316, Classifier Loss 0.2226017564535141, Total Loss 149.99282836914062\n",
      "40: Encoding Loss 14.815311431884766, Transition Loss 8.225072860717773, Classifier Loss 0.20449954271316528, Total Loss 140.61746215820312\n",
      "40: Encoding Loss 15.342852592468262, Transition Loss 7.777303218841553, Classifier Loss 0.20875930786132812, Total Loss 145.17420959472656\n",
      "40: Encoding Loss 14.696317672729492, Transition Loss 10.90898609161377, Classifier Loss 0.19604571163654327, Total Loss 139.35691833496094\n",
      "40: Encoding Loss 15.877277374267578, Transition Loss 7.571223258972168, Classifier Loss 0.2179655134677887, Total Loss 150.32901000976562\n",
      "40: Encoding Loss 15.61330509185791, Transition Loss 8.365116119384766, Classifier Loss 0.21737605333328247, Total Loss 148.31707763671875\n",
      "40: Encoding Loss 14.073527336120605, Transition Loss 7.187343597412109, Classifier Loss 0.216699481010437, Total Loss 135.6956329345703\n",
      "40: Encoding Loss 15.015196800231934, Transition Loss 8.660723686218262, Classifier Loss 0.21041584014892578, Total Loss 142.8953094482422\n",
      "40: Encoding Loss 15.749465942382812, Transition Loss 7.690079689025879, Classifier Loss 0.22621408104896545, Total Loss 150.1551513671875\n",
      "40: Encoding Loss 16.35475730895996, Transition Loss 8.301563262939453, Classifier Loss 0.22567719221115112, Total Loss 155.0660858154297\n",
      "40: Encoding Loss 15.44698429107666, Transition Loss 7.60166072845459, Classifier Loss 0.20559129118919373, Total Loss 145.65533447265625\n",
      "40: Encoding Loss 15.838767051696777, Transition Loss 6.37459135055542, Classifier Loss 0.20018115639686584, Total Loss 148.003173828125\n",
      "40: Encoding Loss 15.264239311218262, Transition Loss 5.965886116027832, Classifier Loss 0.2314772605895996, Total Loss 146.45481872558594\n",
      "40: Encoding Loss 15.72158432006836, Transition Loss 8.391326904296875, Classifier Loss 0.20870690047740936, Total Loss 148.32164001464844\n",
      "40: Encoding Loss 16.447307586669922, Transition Loss 7.6324052810668945, Classifier Loss 0.22108682990074158, Total Loss 155.21363830566406\n",
      "40: Encoding Loss 15.638896942138672, Transition Loss 7.851844310760498, Classifier Loss 0.2207798808813095, Total Loss 148.75955200195312\n",
      "40: Encoding Loss 15.742295265197754, Transition Loss 7.483614444732666, Classifier Loss 0.20899580419063568, Total Loss 148.3346710205078\n",
      "40: Encoding Loss 15.976183891296387, Transition Loss 6.9455084800720215, Classifier Loss 0.21410800516605377, Total Loss 150.609375\n",
      "40: Encoding Loss 16.417858123779297, Transition Loss 5.230888843536377, Classifier Loss 0.21699833869934082, Total Loss 154.08888244628906\n",
      "40: Encoding Loss 15.901117324829102, Transition Loss 5.694723606109619, Classifier Loss 0.208608940243721, Total Loss 149.20877075195312\n",
      "40: Encoding Loss 16.027442932128906, Transition Loss 7.737654209136963, Classifier Loss 0.20544014871120453, Total Loss 150.3110809326172\n",
      "40: Encoding Loss 16.819324493408203, Transition Loss 5.350697040557861, Classifier Loss 0.21240603923797607, Total Loss 156.86534118652344\n",
      "40: Encoding Loss 17.19170570373535, Transition Loss 5.467627048492432, Classifier Loss 0.2356783151626587, Total Loss 162.1949920654297\n",
      "40: Encoding Loss 14.930814743041992, Transition Loss 8.275959968566895, Classifier Loss 0.2044624239206314, Total Loss 141.54795837402344\n",
      "40: Encoding Loss 14.744162559509277, Transition Loss 8.24972915649414, Classifier Loss 0.19812238216400146, Total Loss 139.4154815673828\n",
      "40: Encoding Loss 15.121400833129883, Transition Loss 7.7489824295043945, Classifier Loss 0.23437029123306274, Total Loss 145.95802307128906\n",
      "40: Encoding Loss 15.200961112976074, Transition Loss 8.286588668823242, Classifier Loss 0.24687670171260834, Total Loss 147.9526824951172\n",
      "40: Encoding Loss 14.969073295593262, Transition Loss 11.12217903137207, Classifier Loss 0.22994616627693176, Total Loss 144.97164916992188\n",
      "40: Encoding Loss 16.68760108947754, Transition Loss 6.942341327667236, Classifier Loss 0.21649999916553497, Total Loss 156.53927612304688\n",
      "40: Encoding Loss 16.573680877685547, Transition Loss 6.724406719207764, Classifier Loss 0.18381226062774658, Total Loss 152.31556701660156\n",
      "40: Encoding Loss 14.875316619873047, Transition Loss 6.120753765106201, Classifier Loss 0.2065722942352295, Total Loss 140.88392639160156\n",
      "40: Encoding Loss 15.347678184509277, Transition Loss 5.988783359527588, Classifier Loss 0.21631519496440887, Total Loss 145.61070251464844\n",
      "40: Encoding Loss 15.388975143432617, Transition Loss 6.672005653381348, Classifier Loss 0.211075097322464, Total Loss 145.5537109375\n",
      "40: Encoding Loss 15.902003288269043, Transition Loss 7.86992073059082, Classifier Loss 0.2061689794063568, Total Loss 149.4069061279297\n",
      "40: Encoding Loss 15.863720893859863, Transition Loss 5.825255870819092, Classifier Loss 0.21294358372688293, Total Loss 149.3691864013672\n",
      "40: Encoding Loss 15.893166542053223, Transition Loss 7.050109386444092, Classifier Loss 0.24525654315948486, Total Loss 153.0810089111328\n",
      "40: Encoding Loss 14.97848129272461, Transition Loss 5.321020603179932, Classifier Loss 0.19373732805252075, Total Loss 140.26580810546875\n",
      "40: Encoding Loss 15.78626823425293, Transition Loss 7.818877696990967, Classifier Loss 0.19098663330078125, Total Loss 146.95260620117188\n",
      "40: Encoding Loss 16.074790954589844, Transition Loss 4.7118000984191895, Classifier Loss 0.2381371408700943, Total Loss 153.3544158935547\n",
      "40: Encoding Loss 15.16930103302002, Transition Loss 9.354130744934082, Classifier Loss 0.2378671020269394, Total Loss 147.01193237304688\n",
      "40: Encoding Loss 15.811144828796387, Transition Loss 5.8928985595703125, Classifier Loss 0.22068701684474945, Total Loss 149.73643493652344\n",
      "40: Encoding Loss 16.029146194458008, Transition Loss 8.956379890441895, Classifier Loss 0.23396819829940796, Total Loss 153.4212646484375\n",
      "40: Encoding Loss 15.840028762817383, Transition Loss 6.075833320617676, Classifier Loss 0.22036856412887573, Total Loss 149.9722442626953\n",
      "40: Encoding Loss 15.22956371307373, Transition Loss 8.196301460266113, Classifier Loss 0.2176479995250702, Total Loss 145.24057006835938\n",
      "40: Encoding Loss 16.987628936767578, Transition Loss 4.7866997718811035, Classifier Loss 0.22348132729530334, Total Loss 159.2064971923828\n",
      "40: Encoding Loss 16.211387634277344, Transition Loss 5.14249849319458, Classifier Loss 0.21917687356472015, Total Loss 152.63729858398438\n",
      "40: Encoding Loss 15.507658004760742, Transition Loss 5.57178258895874, Classifier Loss 0.2070026397705078, Total Loss 145.87588500976562\n",
      "40: Encoding Loss 15.158336639404297, Transition Loss 8.203580856323242, Classifier Loss 0.22142523527145386, Total Loss 145.04994201660156\n",
      "40: Encoding Loss 15.30846118927002, Transition Loss 4.131799221038818, Classifier Loss 0.22058279812335968, Total Loss 145.35232543945312\n",
      "40: Encoding Loss 16.05157470703125, Transition Loss 10.407554626464844, Classifier Loss 0.2246389389038086, Total Loss 152.9580078125\n",
      "40: Encoding Loss 16.268308639526367, Transition Loss 6.057124137878418, Classifier Loss 0.20568417012691498, Total Loss 151.9263153076172\n",
      "40: Encoding Loss 17.114839553833008, Transition Loss 8.022237777709961, Classifier Loss 0.2197846621274948, Total Loss 160.5016326904297\n",
      "40: Encoding Loss 16.63431739807129, Transition Loss 7.620542049407959, Classifier Loss 0.217943474650383, Total Loss 156.3929901123047\n",
      "40: Encoding Loss 15.328721046447754, Transition Loss 11.123750686645508, Classifier Loss 0.2008957415819168, Total Loss 144.944091796875\n",
      "40: Encoding Loss 13.066869735717773, Transition Loss 10.858365058898926, Classifier Loss 0.211592897772789, Total Loss 127.86593627929688\n",
      "41: Encoding Loss 16.905176162719727, Transition Loss 6.058681011199951, Classifier Loss 0.22407911717891693, Total Loss 158.86105346679688\n",
      "41: Encoding Loss 16.30431365966797, Transition Loss 6.044750690460205, Classifier Loss 0.21770155429840088, Total Loss 153.4136199951172\n",
      "41: Encoding Loss 16.298707962036133, Transition Loss 6.542626857757568, Classifier Loss 0.2176160365343094, Total Loss 153.45980834960938\n",
      "41: Encoding Loss 15.799127578735352, Transition Loss 6.9983720779418945, Classifier Loss 0.21688315272331238, Total Loss 149.4810028076172\n",
      "41: Encoding Loss 17.503055572509766, Transition Loss 6.697431564331055, Classifier Loss 0.233839750289917, Total Loss 164.74790954589844\n",
      "41: Encoding Loss 14.044742584228516, Transition Loss 7.505521774291992, Classifier Loss 0.2263130247592926, Total Loss 136.49032592773438\n",
      "41: Encoding Loss 16.91026496887207, Transition Loss 6.044241905212402, Classifier Loss 0.22909016907215118, Total Loss 159.3999786376953\n",
      "41: Encoding Loss 17.23310089111328, Transition Loss 6.764695644378662, Classifier Loss 0.2353396862745285, Total Loss 162.751708984375\n",
      "41: Encoding Loss 15.608016014099121, Transition Loss 6.448740482330322, Classifier Loss 0.2680642902851105, Total Loss 152.96031188964844\n",
      "41: Encoding Loss 16.692665100097656, Transition Loss 6.576991081237793, Classifier Loss 0.2290632277727127, Total Loss 157.76304626464844\n",
      "41: Encoding Loss 16.081193923950195, Transition Loss 5.761712074279785, Classifier Loss 0.20424649119377136, Total Loss 150.22654724121094\n",
      "41: Encoding Loss 15.774883270263672, Transition Loss 6.611772537231445, Classifier Loss 0.2257884442806244, Total Loss 150.10028076171875\n",
      "41: Encoding Loss 15.64557933807373, Transition Loss 7.286742210388184, Classifier Loss 0.218228280544281, Total Loss 148.44480895996094\n",
      "41: Encoding Loss 14.961787223815918, Transition Loss 6.336585998535156, Classifier Loss 0.2092837691307068, Total Loss 141.88999938964844\n",
      "41: Encoding Loss 15.062932968139648, Transition Loss 7.655289649963379, Classifier Loss 0.2158595770597458, Total Loss 143.6204833984375\n",
      "41: Encoding Loss 15.95592975616455, Transition Loss 8.575057029724121, Classifier Loss 0.21683238446712494, Total Loss 151.04568481445312\n",
      "41: Encoding Loss 15.843995094299316, Transition Loss 7.02872896194458, Classifier Loss 0.21096310019493103, Total Loss 149.2540283203125\n",
      "41: Encoding Loss 15.255681991577148, Transition Loss 8.1719331741333, Classifier Loss 0.20100106298923492, Total Loss 143.7799530029297\n",
      "41: Encoding Loss 16.428707122802734, Transition Loss 7.889881134033203, Classifier Loss 0.1992977112531662, Total Loss 152.93740844726562\n",
      "41: Encoding Loss 16.299772262573242, Transition Loss 7.935945510864258, Classifier Loss 0.2082759290933609, Total Loss 152.81295776367188\n",
      "41: Encoding Loss 15.683704376220703, Transition Loss 8.284757614135742, Classifier Loss 0.22439518570899963, Total Loss 149.56610107421875\n",
      "41: Encoding Loss 15.719653129577637, Transition Loss 6.409686088562012, Classifier Loss 0.20521315932273865, Total Loss 147.5604705810547\n",
      "41: Encoding Loss 14.282267570495605, Transition Loss 11.009714126586914, Classifier Loss 0.20907621085643768, Total Loss 137.36770629882812\n",
      "41: Encoding Loss 15.576858520507812, Transition Loss 6.416478157043457, Classifier Loss 0.2137814164161682, Total Loss 147.27630615234375\n",
      "41: Encoding Loss 15.626275062561035, Transition Loss 4.6110076904296875, Classifier Loss 0.2034441977739334, Total Loss 146.27682495117188\n",
      "41: Encoding Loss 15.303383827209473, Transition Loss 6.981057643890381, Classifier Loss 0.19424644112586975, Total Loss 143.2479248046875\n",
      "41: Encoding Loss 15.888930320739746, Transition Loss 7.739263534545898, Classifier Loss 0.22129616141319275, Total Loss 150.78890991210938\n",
      "41: Encoding Loss 15.998857498168945, Transition Loss 8.324771881103516, Classifier Loss 0.2091245949268341, Total Loss 150.56826782226562\n",
      "41: Encoding Loss 15.90420913696289, Transition Loss 8.516493797302246, Classifier Loss 0.21420982480049133, Total Loss 150.35794067382812\n",
      "41: Encoding Loss 14.57505989074707, Transition Loss 8.847861289978027, Classifier Loss 0.2229841649532318, Total Loss 140.66847229003906\n",
      "41: Encoding Loss 15.666000366210938, Transition Loss 7.615957736968994, Classifier Loss 0.20873096585273743, Total Loss 147.7242889404297\n",
      "41: Encoding Loss 14.772686004638672, Transition Loss 10.142333984375, Classifier Loss 0.2055186778306961, Total Loss 140.7618408203125\n",
      "41: Encoding Loss 15.777704238891602, Transition Loss 8.319475173950195, Classifier Loss 0.216803640127182, Total Loss 149.56588745117188\n",
      "41: Encoding Loss 14.87808895111084, Transition Loss 4.200422286987305, Classifier Loss 0.2317913919687271, Total Loss 143.0439453125\n",
      "41: Encoding Loss 14.520617485046387, Transition Loss 8.259326934814453, Classifier Loss 0.21750685572624207, Total Loss 139.56748962402344\n",
      "41: Encoding Loss 14.69123363494873, Transition Loss 5.241641521453857, Classifier Loss 0.23433740437030792, Total Loss 142.01193237304688\n",
      "41: Encoding Loss 15.489092826843262, Transition Loss 8.471725463867188, Classifier Loss 0.20010410249233246, Total Loss 145.6175079345703\n",
      "41: Encoding Loss 15.25250244140625, Transition Loss 10.533873558044434, Classifier Loss 0.23677955567836761, Total Loss 147.80474853515625\n",
      "41: Encoding Loss 16.4676456451416, Transition Loss 7.061060905456543, Classifier Loss 0.20999369025230408, Total Loss 154.1527557373047\n",
      "41: Encoding Loss 15.617066383361816, Transition Loss 8.792786598205566, Classifier Loss 0.2109082043170929, Total Loss 147.78591918945312\n",
      "41: Encoding Loss 14.53232479095459, Transition Loss 8.192437171936035, Classifier Loss 0.24672457575798035, Total Loss 142.56954956054688\n",
      "41: Encoding Loss 15.66469955444336, Transition Loss 6.124253273010254, Classifier Loss 0.22684195637702942, Total Loss 149.22665405273438\n",
      "41: Encoding Loss 15.302033424377441, Transition Loss 5.545276165008545, Classifier Loss 0.21632564067840576, Total Loss 145.15789794921875\n",
      "41: Encoding Loss 15.685372352600098, Transition Loss 4.33939790725708, Classifier Loss 0.2193826586008072, Total Loss 148.28912353515625\n",
      "41: Encoding Loss 16.72273826599121, Transition Loss 6.004323959350586, Classifier Loss 0.22820314764976501, Total Loss 157.80308532714844\n",
      "41: Encoding Loss 16.16836166381836, Transition Loss 8.436976432800293, Classifier Loss 0.22527208924293518, Total Loss 153.56150817871094\n",
      "41: Encoding Loss 15.418225288391113, Transition Loss 9.327444076538086, Classifier Loss 0.21313369274139404, Total Loss 146.52467346191406\n",
      "41: Encoding Loss 16.710317611694336, Transition Loss 8.225663185119629, Classifier Loss 0.21938936412334442, Total Loss 157.2666015625\n",
      "41: Encoding Loss 15.973644256591797, Transition Loss 7.348651885986328, Classifier Loss 0.20874838531017303, Total Loss 150.13372802734375\n",
      "41: Encoding Loss 16.063793182373047, Transition Loss 7.744109153747559, Classifier Loss 0.18975535035133362, Total Loss 149.03472900390625\n",
      "41: Encoding Loss 16.637502670288086, Transition Loss 5.4701666831970215, Classifier Loss 0.23128314316272736, Total Loss 157.32237243652344\n",
      "41: Encoding Loss 16.411136627197266, Transition Loss 6.064135551452637, Classifier Loss 0.21279437839984894, Total Loss 153.78135681152344\n",
      "41: Encoding Loss 15.748841285705566, Transition Loss 5.1071553230285645, Classifier Loss 0.2058093547821045, Total Loss 147.59310913085938\n",
      "41: Encoding Loss 16.530824661254883, Transition Loss 8.69974136352539, Classifier Loss 0.23090365529060364, Total Loss 157.076904296875\n",
      "41: Encoding Loss 15.012269973754883, Transition Loss 8.028729438781738, Classifier Loss 0.19913390278816223, Total Loss 141.61729431152344\n",
      "41: Encoding Loss 14.898889541625977, Transition Loss 6.812790870666504, Classifier Loss 0.2190636694431305, Total Loss 142.46005249023438\n",
      "41: Encoding Loss 15.359923362731934, Transition Loss 6.262853145599365, Classifier Loss 0.22933082282543182, Total Loss 147.06503295898438\n",
      "41: Encoding Loss 14.995461463928223, Transition Loss 6.9110870361328125, Classifier Loss 0.19989590346813202, Total Loss 141.33551025390625\n",
      "41: Encoding Loss 14.856550216674805, Transition Loss 5.5150861740112305, Classifier Loss 0.20356443524360657, Total Loss 140.31185913085938\n",
      "41: Encoding Loss 15.597524642944336, Transition Loss 6.230006217956543, Classifier Loss 0.19848480820655823, Total Loss 145.87469482421875\n",
      "41: Encoding Loss 16.7895565032959, Transition Loss 7.56803035736084, Classifier Loss 0.22524115443229675, Total Loss 158.3541717529297\n",
      "41: Encoding Loss 17.321163177490234, Transition Loss 7.178893089294434, Classifier Loss 0.22638259828090668, Total Loss 162.6433563232422\n",
      "41: Encoding Loss 15.141640663146973, Transition Loss 5.062321662902832, Classifier Loss 0.21310067176818848, Total Loss 143.45567321777344\n",
      "41: Encoding Loss 14.984800338745117, Transition Loss 5.5919084548950195, Classifier Loss 0.20600658655166626, Total Loss 141.59744262695312\n",
      "41: Encoding Loss 15.600284576416016, Transition Loss 7.716646194458008, Classifier Loss 0.21993663907051086, Total Loss 148.33926391601562\n",
      "41: Encoding Loss 14.941222190856934, Transition Loss 6.819602012634277, Classifier Loss 0.19639402627944946, Total Loss 140.53309631347656\n",
      "41: Encoding Loss 15.963088989257812, Transition Loss 6.372358798980713, Classifier Loss 0.21741539239883423, Total Loss 150.72073364257812\n",
      "41: Encoding Loss 16.435293197631836, Transition Loss 6.6266584396362305, Classifier Loss 0.1850845366716385, Total Loss 151.31613159179688\n",
      "41: Encoding Loss 15.66705322265625, Transition Loss 8.05923843383789, Classifier Loss 0.2175571769475937, Total Loss 148.70399475097656\n",
      "41: Encoding Loss 15.419098854064941, Transition Loss 7.166784286499023, Classifier Loss 0.23392143845558167, Total Loss 148.17828369140625\n",
      "41: Encoding Loss 16.82017707824707, Transition Loss 6.627924919128418, Classifier Loss 0.2182365357875824, Total Loss 157.71066284179688\n",
      "41: Encoding Loss 15.719379425048828, Transition Loss 7.756056785583496, Classifier Loss 0.20369653403759003, Total Loss 147.67588806152344\n",
      "41: Encoding Loss 15.532980918884277, Transition Loss 7.5430097579956055, Classifier Loss 0.19554293155670166, Total Loss 145.32675170898438\n",
      "41: Encoding Loss 16.52088165283203, Transition Loss 7.967268943786621, Classifier Loss 0.20031264424324036, Total Loss 153.79177856445312\n",
      "41: Encoding Loss 14.591626167297363, Transition Loss 9.364356994628906, Classifier Loss 0.2562524378299713, Total Loss 144.23110961914062\n",
      "41: Encoding Loss 15.742355346679688, Transition Loss 7.1282124519348145, Classifier Loss 0.21176636219024658, Total Loss 148.54112243652344\n",
      "41: Encoding Loss 15.342008590698242, Transition Loss 8.57468032836914, Classifier Loss 0.20971894264221191, Total Loss 145.42291259765625\n",
      "41: Encoding Loss 15.636801719665527, Transition Loss 8.238556861877441, Classifier Loss 0.21720075607299805, Total Loss 148.46218872070312\n",
      "41: Encoding Loss 15.237667083740234, Transition Loss 8.533772468566895, Classifier Loss 0.2097647339105606, Total Loss 144.58457946777344\n",
      "41: Encoding Loss 16.956010818481445, Transition Loss 6.907279968261719, Classifier Loss 0.22049975395202637, Total Loss 159.0795135498047\n",
      "41: Encoding Loss 15.318095207214355, Transition Loss 7.2515106201171875, Classifier Loss 0.20683671534061432, Total Loss 144.67874145507812\n",
      "41: Encoding Loss 16.208118438720703, Transition Loss 6.988828182220459, Classifier Loss 0.22445879876613617, Total Loss 153.5085906982422\n",
      "41: Encoding Loss 15.691133499145508, Transition Loss 7.736465930938721, Classifier Loss 0.21365104615688324, Total Loss 148.4414520263672\n",
      "41: Encoding Loss 15.619115829467773, Transition Loss 7.83526611328125, Classifier Loss 0.2181050330400467, Total Loss 148.3304901123047\n",
      "41: Encoding Loss 16.682353973388672, Transition Loss 6.261218070983887, Classifier Loss 0.20701131224632263, Total Loss 155.41221618652344\n",
      "41: Encoding Loss 15.229291915893555, Transition Loss 7.433759689331055, Classifier Loss 0.196784108877182, Total Loss 142.99951171875\n",
      "41: Encoding Loss 15.607340812683105, Transition Loss 5.747174263000488, Classifier Loss 0.2224615067243576, Total Loss 148.25430297851562\n",
      "41: Encoding Loss 15.458409309387207, Transition Loss 9.546732902526855, Classifier Loss 0.22539761662483215, Total Loss 148.1163787841797\n",
      "41: Encoding Loss 15.120912551879883, Transition Loss 5.9774250984191895, Classifier Loss 0.20524030923843384, Total Loss 142.6868133544922\n",
      "41: Encoding Loss 15.49453353881836, Transition Loss 5.490178108215332, Classifier Loss 0.22403313219547272, Total Loss 147.45762634277344\n",
      "41: Encoding Loss 16.091732025146484, Transition Loss 6.574753761291504, Classifier Loss 0.19028232991695404, Total Loss 149.07705688476562\n",
      "41: Encoding Loss 15.797457695007324, Transition Loss 9.13585090637207, Classifier Loss 0.261959433555603, Total Loss 154.40277099609375\n",
      "41: Encoding Loss 15.922532081604004, Transition Loss 8.105030059814453, Classifier Loss 0.2105746567249298, Total Loss 150.05873107910156\n",
      "41: Encoding Loss 16.71178436279297, Transition Loss 6.778303623199463, Classifier Loss 0.19505707919597626, Total Loss 154.55564880371094\n",
      "41: Encoding Loss 16.496191024780273, Transition Loss 7.191070556640625, Classifier Loss 0.19981707632541656, Total Loss 153.3894500732422\n",
      "41: Encoding Loss 15.292962074279785, Transition Loss 5.119953632354736, Classifier Loss 0.1999078392982483, Total Loss 143.3584747314453\n",
      "41: Encoding Loss 15.82884693145752, Transition Loss 8.558309555053711, Classifier Loss 0.23317712545394897, Total Loss 151.66014099121094\n",
      "41: Encoding Loss 16.063199996948242, Transition Loss 6.410005569458008, Classifier Loss 0.2248096615076065, Total Loss 152.26856994628906\n",
      "41: Encoding Loss 15.37948989868164, Transition Loss 9.287747383117676, Classifier Loss 0.23070871829986572, Total Loss 147.96432495117188\n",
      "41: Encoding Loss 16.260839462280273, Transition Loss 5.1991143226623535, Classifier Loss 0.22222186625003815, Total Loss 153.34872436523438\n",
      "41: Encoding Loss 15.771673202514648, Transition Loss 6.531441688537598, Classifier Loss 0.21610622107982635, Total Loss 149.09030151367188\n",
      "41: Encoding Loss 16.421419143676758, Transition Loss 6.793524742126465, Classifier Loss 0.2510242164134979, Total Loss 157.8324737548828\n",
      "41: Encoding Loss 15.742880821228027, Transition Loss 9.303409576416016, Classifier Loss 0.21473458409309387, Total Loss 149.27719116210938\n",
      "41: Encoding Loss 14.811586380004883, Transition Loss 8.028716087341309, Classifier Loss 0.20521162450313568, Total Loss 140.61959838867188\n",
      "41: Encoding Loss 15.353157997131348, Transition Loss 7.5620012283325195, Classifier Loss 0.19997002184391022, Total Loss 144.3346710205078\n",
      "41: Encoding Loss 14.675026893615723, Transition Loss 10.846479415893555, Classifier Loss 0.20297199487686157, Total Loss 139.86671447753906\n",
      "41: Encoding Loss 15.862796783447266, Transition Loss 7.328929424285889, Classifier Loss 0.20911383628845215, Total Loss 149.279541015625\n",
      "41: Encoding Loss 15.635659217834473, Transition Loss 8.068058967590332, Classifier Loss 0.20857365429401398, Total Loss 147.55625915527344\n",
      "41: Encoding Loss 14.087308883666992, Transition Loss 6.919461250305176, Classifier Loss 0.2264661192893982, Total Loss 136.72898864746094\n",
      "41: Encoding Loss 15.007616996765137, Transition Loss 8.419929504394531, Classifier Loss 0.21073761582374573, Total Loss 142.81869506835938\n",
      "41: Encoding Loss 15.772249221801758, Transition Loss 7.462225914001465, Classifier Loss 0.2211034893989563, Total Loss 149.78079223632812\n",
      "41: Encoding Loss 16.345815658569336, Transition Loss 8.047468185424805, Classifier Loss 0.22643011808395386, Total Loss 155.01902770996094\n",
      "41: Encoding Loss 15.433147430419922, Transition Loss 7.369888782501221, Classifier Loss 0.21022826433181763, Total Loss 145.9619903564453\n",
      "41: Encoding Loss 15.846460342407227, Transition Loss 6.127708911895752, Classifier Loss 0.20775359869003296, Total Loss 148.7725830078125\n",
      "41: Encoding Loss 15.237211227416992, Transition Loss 5.621190071105957, Classifier Loss 0.24132069945335388, Total Loss 147.1540069580078\n",
      "41: Encoding Loss 15.754441261291504, Transition Loss 8.12370491027832, Classifier Loss 0.20264878869056702, Total Loss 147.92515563964844\n",
      "41: Encoding Loss 16.432218551635742, Transition Loss 7.445826053619385, Classifier Loss 0.21260099112987518, Total Loss 154.20701599121094\n",
      "41: Encoding Loss 15.6229248046875, Transition Loss 7.602537155151367, Classifier Loss 0.22618436813354492, Total Loss 149.12234497070312\n",
      "41: Encoding Loss 15.723201751708984, Transition Loss 7.27277946472168, Classifier Loss 0.20036491751670837, Total Loss 147.27667236328125\n",
      "41: Encoding Loss 15.959153175354004, Transition Loss 6.897158145904541, Classifier Loss 0.21549677848815918, Total Loss 150.6023406982422\n",
      "41: Encoding Loss 16.39356231689453, Transition Loss 5.038924694061279, Classifier Loss 0.21461349725723267, Total Loss 153.6176300048828\n",
      "41: Encoding Loss 15.896734237670898, Transition Loss 5.573276996612549, Classifier Loss 0.204159215092659, Total Loss 148.70445251464844\n",
      "41: Encoding Loss 16.01305389404297, Transition Loss 7.5605010986328125, Classifier Loss 0.20478931069374084, Total Loss 150.095458984375\n",
      "41: Encoding Loss 16.811792373657227, Transition Loss 5.209657192230225, Classifier Loss 0.21930673718452454, Total Loss 157.46694946289062\n",
      "41: Encoding Loss 17.20096778869629, Transition Loss 5.392625331878662, Classifier Loss 0.24239274859428406, Total Loss 162.925537109375\n",
      "41: Encoding Loss 14.901973724365234, Transition Loss 8.049795150756836, Classifier Loss 0.21145251393318176, Total Loss 141.97100830078125\n",
      "41: Encoding Loss 14.741315841674805, Transition Loss 8.144274711608887, Classifier Loss 0.19852834939956665, Total Loss 139.4122314453125\n",
      "41: Encoding Loss 15.120621681213379, Transition Loss 7.428206443786621, Classifier Loss 0.22481010854244232, Total Loss 144.93162536621094\n",
      "41: Encoding Loss 15.194818496704102, Transition Loss 7.985201835632324, Classifier Loss 0.23074910044670105, Total Loss 146.23049926757812\n",
      "41: Encoding Loss 14.978338241577148, Transition Loss 10.761934280395508, Classifier Loss 0.22128786146640778, Total Loss 144.10789489746094\n",
      "41: Encoding Loss 16.656740188598633, Transition Loss 6.716869831085205, Classifier Loss 0.21478529274463654, Total Loss 156.07582092285156\n",
      "41: Encoding Loss 16.558082580566406, Transition Loss 6.487887382507324, Classifier Loss 0.1867048740386963, Total Loss 152.43272399902344\n",
      "41: Encoding Loss 14.84087085723877, Transition Loss 5.932094097137451, Classifier Loss 0.2006741464138031, Total Loss 139.98080444335938\n",
      "41: Encoding Loss 15.363712310791016, Transition Loss 5.7709174156188965, Classifier Loss 0.21248991787433624, Total Loss 145.3128662109375\n",
      "41: Encoding Loss 15.40646743774414, Transition Loss 6.531702041625977, Classifier Loss 0.21564073860645294, Total Loss 146.1221466064453\n",
      "41: Encoding Loss 15.872014045715332, Transition Loss 7.446566581726074, Classifier Loss 0.20733776688575745, Total Loss 149.19920349121094\n",
      "41: Encoding Loss 15.875005722045898, Transition Loss 5.6706366539001465, Classifier Loss 0.20668751001358032, Total Loss 148.8029327392578\n",
      "41: Encoding Loss 15.882599830627441, Transition Loss 6.689429759979248, Classifier Loss 0.2344726175069809, Total Loss 151.84596252441406\n",
      "41: Encoding Loss 14.96997356414795, Transition Loss 5.337980270385742, Classifier Loss 0.18840526044368744, Total Loss 139.66790771484375\n",
      "41: Encoding Loss 15.78809642791748, Transition Loss 7.468652248382568, Classifier Loss 0.19211097061634064, Total Loss 147.0095977783203\n",
      "41: Encoding Loss 16.078847885131836, Transition Loss 4.853957176208496, Classifier Loss 0.24672119319438934, Total Loss 154.27369689941406\n",
      "41: Encoding Loss 15.143174171447754, Transition Loss 8.734359741210938, Classifier Loss 0.2260308712720871, Total Loss 145.495361328125\n",
      "41: Encoding Loss 15.823667526245117, Transition Loss 6.034816741943359, Classifier Loss 0.228081613779068, Total Loss 150.60447692871094\n",
      "41: Encoding Loss 16.00664710998535, Transition Loss 8.274856567382812, Classifier Loss 0.22807607054710388, Total Loss 152.5157470703125\n",
      "41: Encoding Loss 15.817188262939453, Transition Loss 6.300152778625488, Classifier Loss 0.21000099182128906, Total Loss 148.79762268066406\n",
      "41: Encoding Loss 15.257547378540039, Transition Loss 7.687203407287598, Classifier Loss 0.21422678232192993, Total Loss 145.0205078125\n",
      "41: Encoding Loss 16.980749130249023, Transition Loss 4.759371757507324, Classifier Loss 0.21632428467273712, Total Loss 158.4302978515625\n",
      "41: Encoding Loss 16.214174270629883, Transition Loss 4.850853443145752, Classifier Loss 0.21279701590538025, Total Loss 151.96327209472656\n",
      "41: Encoding Loss 15.517154693603516, Transition Loss 5.4311933517456055, Classifier Loss 0.2061120867729187, Total Loss 145.83468627929688\n",
      "41: Encoding Loss 15.19489860534668, Transition Loss 7.752537250518799, Classifier Loss 0.22097620368003845, Total Loss 145.2073211669922\n",
      "41: Encoding Loss 15.288287162780762, Transition Loss 3.9920639991760254, Classifier Loss 0.2163517326116562, Total Loss 144.73988342285156\n",
      "41: Encoding Loss 16.014766693115234, Transition Loss 10.163022994995117, Classifier Loss 0.2283669114112854, Total Loss 152.98744201660156\n",
      "41: Encoding Loss 16.256298065185547, Transition Loss 5.6738057136535645, Classifier Loss 0.20661962032318115, Total Loss 151.8471221923828\n",
      "41: Encoding Loss 17.11748504638672, Transition Loss 8.256962776184082, Classifier Loss 0.2133401781320572, Total Loss 159.92529296875\n",
      "41: Encoding Loss 16.634965896606445, Transition Loss 7.204801082611084, Classifier Loss 0.21503131091594696, Total Loss 156.02381896972656\n",
      "41: Encoding Loss 15.325504302978516, Transition Loss 10.717987060546875, Classifier Loss 0.20432114601135254, Total Loss 145.1797332763672\n",
      "41: Encoding Loss 13.041604042053223, Transition Loss 10.211015701293945, Classifier Loss 0.21502631902694702, Total Loss 127.87767791748047\n",
      "42: Encoding Loss 16.910633087158203, Transition Loss 6.198165416717529, Classifier Loss 0.22497400641441345, Total Loss 159.02210998535156\n",
      "42: Encoding Loss 16.263784408569336, Transition Loss 5.960679531097412, Classifier Loss 0.21085181832313538, Total Loss 152.38760375976562\n",
      "42: Encoding Loss 16.319019317626953, Transition Loss 6.414323329925537, Classifier Loss 0.23136383295059204, Total Loss 154.97140502929688\n",
      "42: Encoding Loss 15.778654098510742, Transition Loss 6.81428337097168, Classifier Loss 0.2192813754081726, Total Loss 149.52023315429688\n",
      "42: Encoding Loss 17.499216079711914, Transition Loss 6.393309593200684, Classifier Loss 0.21339768171310425, Total Loss 162.61215209960938\n",
      "42: Encoding Loss 14.026957511901855, Transition Loss 7.304369926452637, Classifier Loss 0.21493440866470337, Total Loss 135.16998291015625\n",
      "42: Encoding Loss 16.911605834960938, Transition Loss 5.759859085083008, Classifier Loss 0.22215145826339722, Total Loss 158.65997314453125\n",
      "42: Encoding Loss 17.223766326904297, Transition Loss 6.531834125518799, Classifier Loss 0.24382264912128448, Total Loss 163.47877502441406\n",
      "42: Encoding Loss 15.622397422790527, Transition Loss 6.054569721221924, Classifier Loss 0.27062100172042847, Total Loss 153.25218200683594\n",
      "42: Encoding Loss 16.713939666748047, Transition Loss 6.370607376098633, Classifier Loss 0.2249484658241272, Total Loss 157.48049926757812\n",
      "42: Encoding Loss 16.094375610351562, Transition Loss 5.590798854827881, Classifier Loss 0.21138067543506622, Total Loss 151.01123046875\n",
      "42: Encoding Loss 15.771538734436035, Transition Loss 6.307528018951416, Classifier Loss 0.2330663651227951, Total Loss 150.74046325683594\n",
      "42: Encoding Loss 15.642993927001953, Transition Loss 7.028592586517334, Classifier Loss 0.20874367654323578, Total Loss 147.4240264892578\n",
      "42: Encoding Loss 14.972442626953125, Transition Loss 6.0679168701171875, Classifier Loss 0.21005745232105255, Total Loss 141.9988555908203\n",
      "42: Encoding Loss 15.063464164733887, Transition Loss 7.261383056640625, Classifier Loss 0.21199479699134827, Total Loss 143.1594696044922\n",
      "42: Encoding Loss 15.957090377807617, Transition Loss 8.37105655670166, Classifier Loss 0.2184969037771225, Total Loss 151.18063354492188\n",
      "42: Encoding Loss 15.84646224975586, Transition Loss 6.822641849517822, Classifier Loss 0.21204498410224915, Total Loss 149.34072875976562\n",
      "42: Encoding Loss 15.267581939697266, Transition Loss 7.987395286560059, Classifier Loss 0.20306546986103058, Total Loss 144.04466247558594\n",
      "42: Encoding Loss 16.438709259033203, Transition Loss 7.647304534912109, Classifier Loss 0.19900350272655487, Total Loss 152.93948364257812\n",
      "42: Encoding Loss 16.294477462768555, Transition Loss 7.762847900390625, Classifier Loss 0.20834138989448547, Total Loss 152.74252319335938\n",
      "42: Encoding Loss 15.670987129211426, Transition Loss 8.08896541595459, Classifier Loss 0.2251037359237671, Total Loss 149.49606323242188\n",
      "42: Encoding Loss 15.697976112365723, Transition Loss 6.297428607940674, Classifier Loss 0.19909954071044922, Total Loss 146.75326538085938\n",
      "42: Encoding Loss 14.280569076538086, Transition Loss 11.02642822265625, Classifier Loss 0.21346327662467957, Total Loss 137.79617309570312\n",
      "42: Encoding Loss 15.58889102935791, Transition Loss 6.292994499206543, Classifier Loss 0.2222965955734253, Total Loss 148.19940185546875\n",
      "42: Encoding Loss 15.596134185791016, Transition Loss 4.465816020965576, Classifier Loss 0.20543135702610016, Total Loss 146.20535278320312\n",
      "42: Encoding Loss 15.316800117492676, Transition Loss 6.810329437255859, Classifier Loss 0.20096251368522644, Total Loss 143.99270629882812\n",
      "42: Encoding Loss 15.88681411743164, Transition Loss 7.520864009857178, Classifier Loss 0.21282094717025757, Total Loss 149.88076782226562\n",
      "42: Encoding Loss 16.00831413269043, Transition Loss 8.115659713745117, Classifier Loss 0.21038450300693512, Total Loss 150.7281036376953\n",
      "42: Encoding Loss 15.925613403320312, Transition Loss 8.331838607788086, Classifier Loss 0.21198615431785583, Total Loss 150.26988220214844\n",
      "42: Encoding Loss 14.58755111694336, Transition Loss 8.74155044555664, Classifier Loss 0.22694185376167297, Total Loss 141.14291381835938\n",
      "42: Encoding Loss 15.670316696166992, Transition Loss 7.555543422698975, Classifier Loss 0.2025555521249771, Total Loss 147.12921142578125\n",
      "42: Encoding Loss 14.752015113830566, Transition Loss 9.90451717376709, Classifier Loss 0.20232626795768738, Total Loss 140.22964477539062\n",
      "42: Encoding Loss 15.773419380187988, Transition Loss 8.094064712524414, Classifier Loss 0.2146528661251068, Total Loss 149.27146911621094\n",
      "42: Encoding Loss 14.8817720413208, Transition Loss 4.083630084991455, Classifier Loss 0.22567299008369446, Total Loss 142.43820190429688\n",
      "42: Encoding Loss 14.523351669311523, Transition Loss 8.127628326416016, Classifier Loss 0.21956564486026764, Total Loss 139.76890563964844\n",
      "42: Encoding Loss 14.687764167785645, Transition Loss 5.096465587615967, Classifier Loss 0.23333978652954102, Total Loss 141.85537719726562\n",
      "42: Encoding Loss 15.473410606384277, Transition Loss 8.392670631408691, Classifier Loss 0.206398606300354, Total Loss 146.1056671142578\n",
      "42: Encoding Loss 15.262274742126465, Transition Loss 10.414487838745117, Classifier Loss 0.24327075481414795, Total Loss 148.5081787109375\n",
      "42: Encoding Loss 16.462522506713867, Transition Loss 6.968385696411133, Classifier Loss 0.20605550706386566, Total Loss 153.69940185546875\n",
      "42: Encoding Loss 15.604264259338379, Transition Loss 8.435651779174805, Classifier Loss 0.21361543238162994, Total Loss 147.88279724121094\n",
      "42: Encoding Loss 14.532326698303223, Transition Loss 8.150092124938965, Classifier Loss 0.23908410966396332, Total Loss 141.79705810546875\n",
      "42: Encoding Loss 15.641427040100098, Transition Loss 5.838180065155029, Classifier Loss 0.22537539899349213, Total Loss 148.8365936279297\n",
      "42: Encoding Loss 15.282743453979492, Transition Loss 5.600184917449951, Classifier Loss 0.20277252793312073, Total Loss 143.6592559814453\n",
      "42: Encoding Loss 15.669530868530273, Transition Loss 4.261988162994385, Classifier Loss 0.21768911182880402, Total Loss 147.97756958007812\n",
      "42: Encoding Loss 16.712739944458008, Transition Loss 6.129812240600586, Classifier Loss 0.2225615680217743, Total Loss 157.1840362548828\n",
      "42: Encoding Loss 16.17064094543457, Transition Loss 8.138848304748535, Classifier Loss 0.22317306697368622, Total Loss 153.31021118164062\n",
      "42: Encoding Loss 15.408726692199707, Transition Loss 9.491670608520508, Classifier Loss 0.21288810670375824, Total Loss 146.4569549560547\n",
      "42: Encoding Loss 16.696016311645508, Transition Loss 7.881182670593262, Classifier Loss 0.23309358954429626, Total Loss 158.45372009277344\n",
      "42: Encoding Loss 15.990020751953125, Transition Loss 7.292863368988037, Classifier Loss 0.19992689788341522, Total Loss 149.37142944335938\n",
      "42: Encoding Loss 16.046119689941406, Transition Loss 7.236179351806641, Classifier Loss 0.18919578194618225, Total Loss 148.73577880859375\n",
      "42: Encoding Loss 16.633174896240234, Transition Loss 5.477537155151367, Classifier Loss 0.22365497052669525, Total Loss 156.5264129638672\n",
      "42: Encoding Loss 16.379426956176758, Transition Loss 5.893836975097656, Classifier Loss 0.2065221518278122, Total Loss 152.8664093017578\n",
      "42: Encoding Loss 15.704351425170898, Transition Loss 5.182318210601807, Classifier Loss 0.2052079141139984, Total Loss 147.19207763671875\n",
      "42: Encoding Loss 16.533626556396484, Transition Loss 8.197305679321289, Classifier Loss 0.22477176785469055, Total Loss 156.3856658935547\n",
      "42: Encoding Loss 15.037505149841309, Transition Loss 8.313066482543945, Classifier Loss 0.20171663165092468, Total Loss 142.1343231201172\n",
      "42: Encoding Loss 14.860940933227539, Transition Loss 6.418426990509033, Classifier Loss 0.225179985165596, Total Loss 142.689208984375\n",
      "42: Encoding Loss 15.356927871704102, Transition Loss 6.379796028137207, Classifier Loss 0.21698004007339478, Total Loss 145.82937622070312\n",
      "42: Encoding Loss 14.990252494812012, Transition Loss 6.188400745391846, Classifier Loss 0.19512267410755157, Total Loss 140.67198181152344\n",
      "42: Encoding Loss 14.808796882629395, Transition Loss 6.149530410766602, Classifier Loss 0.20076000690460205, Total Loss 139.77627563476562\n",
      "42: Encoding Loss 15.5820894241333, Transition Loss 5.690217971801758, Classifier Loss 0.1950438767671585, Total Loss 145.2991485595703\n",
      "42: Encoding Loss 16.769784927368164, Transition Loss 7.880897521972656, Classifier Loss 0.22185996174812317, Total Loss 157.92044067382812\n",
      "42: Encoding Loss 17.292694091796875, Transition Loss 6.56221866607666, Classifier Loss 0.2312050312757492, Total Loss 162.7744903564453\n",
      "42: Encoding Loss 15.136899948120117, Transition Loss 5.2336931228637695, Classifier Loss 0.22575479745864868, Total Loss 144.71742248535156\n",
      "42: Encoding Loss 14.9877290725708, Transition Loss 5.351558685302734, Classifier Loss 0.19637037813663483, Total Loss 140.6091766357422\n",
      "42: Encoding Loss 15.570795059204102, Transition Loss 8.260275840759277, Classifier Loss 0.2254362255334854, Total Loss 148.7620391845703\n",
      "42: Encoding Loss 14.914661407470703, Transition Loss 6.295294761657715, Classifier Loss 0.20467688143253326, Total Loss 141.04403686523438\n",
      "42: Encoding Loss 15.96133041381836, Transition Loss 6.810184478759766, Classifier Loss 0.21320728957653046, Total Loss 150.3734130859375\n",
      "42: Encoding Loss 16.42435646057129, Transition Loss 5.759545803070068, Classifier Loss 0.18593049049377441, Total Loss 151.1398162841797\n",
      "42: Encoding Loss 15.666831016540527, Transition Loss 8.942944526672363, Classifier Loss 0.21486777067184448, Total Loss 148.61001586914062\n",
      "42: Encoding Loss 15.404862403869629, Transition Loss 6.225919246673584, Classifier Loss 0.23131632804870605, Total Loss 147.61572265625\n",
      "42: Encoding Loss 16.840452194213867, Transition Loss 7.419129848480225, Classifier Loss 0.21858802437782288, Total Loss 158.06625366210938\n",
      "42: Encoding Loss 15.717615127563477, Transition Loss 7.152731895446777, Classifier Loss 0.20575770735740662, Total Loss 147.7472381591797\n",
      "42: Encoding Loss 15.534452438354492, Transition Loss 7.822459697723389, Classifier Loss 0.19210778176784515, Total Loss 145.0509033203125\n",
      "42: Encoding Loss 16.520177841186523, Transition Loss 7.640130996704102, Classifier Loss 0.2020757645368576, Total Loss 153.89703369140625\n",
      "42: Encoding Loss 14.617384910583496, Transition Loss 9.408003807067871, Classifier Loss 0.24939435720443726, Total Loss 143.76011657714844\n",
      "42: Encoding Loss 15.763395309448242, Transition Loss 6.894950866699219, Classifier Loss 0.21032477915287018, Total Loss 148.51864624023438\n",
      "42: Encoding Loss 15.352252960205078, Transition Loss 8.557487487792969, Classifier Loss 0.20843136310577393, Total Loss 145.37265014648438\n",
      "42: Encoding Loss 15.647774696350098, Transition Loss 7.994136810302734, Classifier Loss 0.22103649377822876, Total Loss 148.8846893310547\n",
      "42: Encoding Loss 15.230498313903809, Transition Loss 8.51987361907959, Classifier Loss 0.2041124552488327, Total Loss 143.95921325683594\n",
      "42: Encoding Loss 16.949079513549805, Transition Loss 6.656881809234619, Classifier Loss 0.21850678324699402, Total Loss 158.77468872070312\n",
      "42: Encoding Loss 15.317865371704102, Transition Loss 7.311075687408447, Classifier Loss 0.21027418971061707, Total Loss 145.03256225585938\n",
      "42: Encoding Loss 16.18951988220215, Transition Loss 6.71103048324585, Classifier Loss 0.22943000495433807, Total Loss 153.80137634277344\n",
      "42: Encoding Loss 15.696043014526367, Transition Loss 7.835014343261719, Classifier Loss 0.21678265929222107, Total Loss 148.81361389160156\n",
      "42: Encoding Loss 15.625024795532227, Transition Loss 7.431107997894287, Classifier Loss 0.2101835161447525, Total Loss 147.50477600097656\n",
      "42: Encoding Loss 16.684906005859375, Transition Loss 6.381218910217285, Classifier Loss 0.21016906201839447, Total Loss 155.77239990234375\n",
      "42: Encoding Loss 15.22260570526123, Transition Loss 7.043457508087158, Classifier Loss 0.1917111873626709, Total Loss 142.36065673828125\n",
      "42: Encoding Loss 15.579434394836426, Transition Loss 5.766800403594971, Classifier Loss 0.2420504093170166, Total Loss 149.99388122558594\n",
      "42: Encoding Loss 15.459216117858887, Transition Loss 8.595451354980469, Classifier Loss 0.22719517350196838, Total Loss 148.11233520507812\n",
      "42: Encoding Loss 15.122655868530273, Transition Loss 6.00474739074707, Classifier Loss 0.21372386813163757, Total Loss 143.55458068847656\n",
      "42: Encoding Loss 15.477705955505371, Transition Loss 5.098992347717285, Classifier Loss 0.20830808579921722, Total Loss 145.67225646972656\n",
      "42: Encoding Loss 16.092914581298828, Transition Loss 6.634737014770508, Classifier Loss 0.1884227991104126, Total Loss 148.9125518798828\n",
      "42: Encoding Loss 15.789713859558105, Transition Loss 8.678703308105469, Classifier Loss 0.25604093074798584, Total Loss 153.65753173828125\n",
      "42: Encoding Loss 15.953231811523438, Transition Loss 8.160455703735352, Classifier Loss 0.22358927130699158, Total Loss 151.61688232421875\n",
      "42: Encoding Loss 16.709287643432617, Transition Loss 6.3718976974487305, Classifier Loss 0.1932581514120102, Total Loss 154.27450561523438\n",
      "42: Encoding Loss 16.502880096435547, Transition Loss 7.06624698638916, Classifier Loss 0.20189431309700012, Total Loss 153.62574768066406\n",
      "42: Encoding Loss 15.28554916381836, Transition Loss 4.892953395843506, Classifier Loss 0.20686939358711243, Total Loss 143.94993591308594\n",
      "42: Encoding Loss 15.823519706726074, Transition Loss 8.143394470214844, Classifier Loss 0.23657000064849854, Total Loss 151.87384033203125\n",
      "42: Encoding Loss 16.07408332824707, Transition Loss 6.119250297546387, Classifier Loss 0.22646553814411163, Total Loss 152.46307373046875\n",
      "42: Encoding Loss 15.36756420135498, Transition Loss 9.029402732849121, Classifier Loss 0.23018601536750793, Total Loss 147.76499938964844\n",
      "42: Encoding Loss 16.23223114013672, Transition Loss 5.19498348236084, Classifier Loss 0.21004067361354828, Total Loss 151.90090942382812\n",
      "42: Encoding Loss 15.757705688476562, Transition Loss 6.339422225952148, Classifier Loss 0.19728320837020874, Total Loss 147.05784606933594\n",
      "42: Encoding Loss 16.401323318481445, Transition Loss 6.7989277839660645, Classifier Loss 0.24473994970321655, Total Loss 157.04437255859375\n",
      "42: Encoding Loss 15.731513023376465, Transition Loss 9.158775329589844, Classifier Loss 0.21507546305656433, Total Loss 149.19140625\n",
      "42: Encoding Loss 14.80804443359375, Transition Loss 7.915912628173828, Classifier Loss 0.18939723074436188, Total Loss 138.9872589111328\n",
      "42: Encoding Loss 15.317842483520508, Transition Loss 7.528883457183838, Classifier Loss 0.21045587956905365, Total Loss 145.0941162109375\n",
      "42: Encoding Loss 14.684270858764648, Transition Loss 10.452508926391602, Classifier Loss 0.1997630000114441, Total Loss 139.5409698486328\n",
      "42: Encoding Loss 15.865316390991211, Transition Loss 7.257744789123535, Classifier Loss 0.21229010820388794, Total Loss 149.6031036376953\n",
      "42: Encoding Loss 15.618356704711914, Transition Loss 7.946694374084473, Classifier Loss 0.2170090526342392, Total Loss 148.2371063232422\n",
      "42: Encoding Loss 14.085062026977539, Transition Loss 6.828245162963867, Classifier Loss 0.22067295014858246, Total Loss 136.11343383789062\n",
      "42: Encoding Loss 15.01203727722168, Transition Loss 8.279537200927734, Classifier Loss 0.20505189895629883, Total Loss 142.25741577148438\n",
      "42: Encoding Loss 15.772828102111816, Transition Loss 7.381346225738525, Classifier Loss 0.22028501331806183, Total Loss 149.68740844726562\n",
      "42: Encoding Loss 16.34974479675293, Transition Loss 8.056578636169434, Classifier Loss 0.2239360511302948, Total Loss 154.80287170410156\n",
      "42: Encoding Loss 15.4536771774292, Transition Loss 7.335648536682129, Classifier Loss 0.2174820452928543, Total Loss 146.84475708007812\n",
      "42: Encoding Loss 15.846529960632324, Transition Loss 5.86189079284668, Classifier Loss 0.2022411972284317, Total Loss 148.16873168945312\n",
      "42: Encoding Loss 15.244084358215332, Transition Loss 5.569419860839844, Classifier Loss 0.2352430522441864, Total Loss 146.59085083007812\n",
      "42: Encoding Loss 15.72533893585205, Transition Loss 7.872314929962158, Classifier Loss 0.20311607420444489, Total Loss 147.68878173828125\n",
      "42: Encoding Loss 16.446958541870117, Transition Loss 7.272531509399414, Classifier Loss 0.2228960394859314, Total Loss 155.3197784423828\n",
      "42: Encoding Loss 15.605914115905762, Transition Loss 7.267920970916748, Classifier Loss 0.22358450293540955, Total Loss 148.6593475341797\n",
      "42: Encoding Loss 15.729166984558105, Transition Loss 7.240055561065674, Classifier Loss 0.2015727013349533, Total Loss 147.43861389160156\n",
      "42: Encoding Loss 15.95751953125, Transition Loss 6.5691022872924805, Classifier Loss 0.2144903540611267, Total Loss 150.4230194091797\n",
      "42: Encoding Loss 16.38918113708496, Transition Loss 4.987923622131348, Classifier Loss 0.21895259618759155, Total Loss 154.0063018798828\n",
      "42: Encoding Loss 15.889252662658691, Transition Loss 5.346111297607422, Classifier Loss 0.2138918936252594, Total Loss 149.57244873046875\n",
      "42: Encoding Loss 16.020545959472656, Transition Loss 7.599367141723633, Classifier Loss 0.20665773749351501, Total Loss 150.35000610351562\n",
      "42: Encoding Loss 16.79792022705078, Transition Loss 5.1307220458984375, Classifier Loss 0.20860277116298676, Total Loss 156.269775390625\n",
      "42: Encoding Loss 17.20169448852539, Transition Loss 5.398984432220459, Classifier Loss 0.2369803488254547, Total Loss 162.39138793945312\n",
      "42: Encoding Loss 14.885408401489258, Transition Loss 7.72565221786499, Classifier Loss 0.20128636062145233, Total Loss 140.7570343017578\n",
      "42: Encoding Loss 14.734436988830566, Transition Loss 7.924126148223877, Classifier Loss 0.1931905448436737, Total Loss 138.7793731689453\n",
      "42: Encoding Loss 15.141371726989746, Transition Loss 7.253897666931152, Classifier Loss 0.22824469208717346, Total Loss 145.40621948242188\n",
      "42: Encoding Loss 15.18494987487793, Transition Loss 8.2212495803833, Classifier Loss 0.2287272959947586, Total Loss 145.99659729003906\n",
      "42: Encoding Loss 14.971630096435547, Transition Loss 10.528471946716309, Classifier Loss 0.20830777287483215, Total Loss 142.7095184326172\n",
      "42: Encoding Loss 16.67023277282715, Transition Loss 6.771890640258789, Classifier Loss 0.22037282586097717, Total Loss 156.75352478027344\n",
      "42: Encoding Loss 16.56551742553711, Transition Loss 6.395341396331787, Classifier Loss 0.18343865871429443, Total Loss 152.1470947265625\n",
      "42: Encoding Loss 14.84288215637207, Transition Loss 5.975693702697754, Classifier Loss 0.20045685768127441, Total Loss 139.98388671875\n",
      "42: Encoding Loss 15.364550590515137, Transition Loss 5.707005977630615, Classifier Loss 0.22440186142921448, Total Loss 146.4980010986328\n",
      "42: Encoding Loss 15.403489112854004, Transition Loss 6.537149906158447, Classifier Loss 0.2107401192188263, Total Loss 145.609375\n",
      "42: Encoding Loss 15.887259483337402, Transition Loss 7.2867431640625, Classifier Loss 0.20816358923912048, Total Loss 149.3717803955078\n",
      "42: Encoding Loss 15.841097831726074, Transition Loss 5.782449722290039, Classifier Loss 0.200306698679924, Total Loss 147.91595458984375\n",
      "42: Encoding Loss 15.881884574890137, Transition Loss 6.5170183181762695, Classifier Loss 0.2526162564754486, Total Loss 153.62010192871094\n",
      "42: Encoding Loss 14.945605278015137, Transition Loss 5.519618511199951, Classifier Loss 0.18771503865718842, Total Loss 139.44027709960938\n",
      "42: Encoding Loss 15.788793563842773, Transition Loss 7.340407848358154, Classifier Loss 0.1912851333618164, Total Loss 146.90695190429688\n",
      "42: Encoding Loss 16.07155418395996, Transition Loss 5.108872413635254, Classifier Loss 0.23782522976398468, Total Loss 153.37672424316406\n",
      "42: Encoding Loss 15.149672508239746, Transition Loss 8.272603034973145, Classifier Loss 0.2315269112586975, Total Loss 146.0045928955078\n",
      "42: Encoding Loss 15.811751365661621, Transition Loss 6.395119667053223, Classifier Loss 0.21499256789684296, Total Loss 149.2722930908203\n",
      "42: Encoding Loss 16.003223419189453, Transition Loss 7.492325305938721, Classifier Loss 0.22400140762329102, Total Loss 151.9243927001953\n",
      "42: Encoding Loss 15.814538955688477, Transition Loss 6.449230194091797, Classifier Loss 0.20842033624649048, Total Loss 148.64817810058594\n",
      "42: Encoding Loss 15.229357719421387, Transition Loss 7.150846481323242, Classifier Loss 0.20641596615314484, Total Loss 143.90663146972656\n",
      "42: Encoding Loss 16.983661651611328, Transition Loss 4.921518325805664, Classifier Loss 0.22779196500778198, Total Loss 159.63278198242188\n",
      "42: Encoding Loss 16.183425903320312, Transition Loss 4.61820125579834, Classifier Loss 0.213510200381279, Total Loss 151.7420654296875\n",
      "42: Encoding Loss 15.515336036682129, Transition Loss 5.72321891784668, Classifier Loss 0.19996626675128937, Total Loss 145.2639617919922\n",
      "42: Encoding Loss 15.175691604614258, Transition Loss 7.245009422302246, Classifier Loss 0.2236098349094391, Total Loss 145.21551513671875\n",
      "42: Encoding Loss 15.27864933013916, Transition Loss 4.411237716674805, Classifier Loss 0.2144576609134674, Total Loss 144.55722045898438\n",
      "42: Encoding Loss 16.06697654724121, Transition Loss 9.20889663696289, Classifier Loss 0.2243475615978241, Total Loss 152.81234741210938\n",
      "42: Encoding Loss 16.24020004272461, Transition Loss 6.305229187011719, Classifier Loss 0.20516854524612427, Total Loss 151.69952392578125\n",
      "42: Encoding Loss 17.109691619873047, Transition Loss 6.858767986297607, Classifier Loss 0.23446305096149445, Total Loss 161.6956024169922\n",
      "42: Encoding Loss 16.633054733276367, Transition Loss 8.196634292602539, Classifier Loss 0.20753854513168335, Total Loss 155.45762634277344\n",
      "42: Encoding Loss 15.31713581085205, Transition Loss 9.922513961791992, Classifier Loss 0.21825210750102997, Total Loss 146.3468017578125\n",
      "42: Encoding Loss 13.054628372192383, Transition Loss 11.387335777282715, Classifier Loss 0.196428582072258, Total Loss 126.35735321044922\n",
      "43: Encoding Loss 16.908702850341797, Transition Loss 5.518141746520996, Classifier Loss 0.21475352346897125, Total Loss 157.84861755371094\n",
      "43: Encoding Loss 16.271448135375977, Transition Loss 6.53709077835083, Classifier Loss 0.2049635946750641, Total Loss 151.97535705566406\n",
      "43: Encoding Loss 16.29257583618164, Transition Loss 6.007554531097412, Classifier Loss 0.22255860269069672, Total Loss 153.7979736328125\n",
      "43: Encoding Loss 15.792675971984863, Transition Loss 7.302495956420898, Classifier Loss 0.21585603058338165, Total Loss 149.38751220703125\n",
      "43: Encoding Loss 17.50475311279297, Transition Loss 6.102823734283447, Classifier Loss 0.21441620588302612, Total Loss 162.70021057128906\n",
      "43: Encoding Loss 14.032767295837402, Transition Loss 7.866210460662842, Classifier Loss 0.22348107397556305, Total Loss 136.18348693847656\n",
      "43: Encoding Loss 16.900463104248047, Transition Loss 5.642093658447266, Classifier Loss 0.2308790236711502, Total Loss 159.4200439453125\n",
      "43: Encoding Loss 17.228269577026367, Transition Loss 6.802561283111572, Classifier Loss 0.22607138752937317, Total Loss 161.79380798339844\n",
      "43: Encoding Loss 15.603363990783691, Transition Loss 6.161609649658203, Classifier Loss 0.2744956612586975, Total Loss 153.50880432128906\n",
      "43: Encoding Loss 16.691434860229492, Transition Loss 6.496444225311279, Classifier Loss 0.22257661819458008, Total Loss 157.0884246826172\n",
      "43: Encoding Loss 16.065675735473633, Transition Loss 5.514834880828857, Classifier Loss 0.20589031279087067, Total Loss 150.2174072265625\n",
      "43: Encoding Loss 15.765969276428223, Transition Loss 6.4292120933532715, Classifier Loss 0.23437273502349854, Total Loss 150.8508758544922\n",
      "43: Encoding Loss 15.636977195739746, Transition Loss 6.908182621002197, Classifier Loss 0.21123461425304413, Total Loss 147.60092163085938\n",
      "43: Encoding Loss 14.963277816772461, Transition Loss 6.114218711853027, Classifier Loss 0.2010982781648636, Total Loss 141.0388946533203\n",
      "43: Encoding Loss 15.059215545654297, Transition Loss 7.282719135284424, Classifier Loss 0.22051964700222015, Total Loss 143.98223876953125\n",
      "43: Encoding Loss 15.932000160217285, Transition Loss 8.370888710021973, Classifier Loss 0.21434512734413147, Total Loss 150.564697265625\n",
      "43: Encoding Loss 15.835519790649414, Transition Loss 6.766753673553467, Classifier Loss 0.21214334666728973, Total Loss 149.25184631347656\n",
      "43: Encoding Loss 15.277469635009766, Transition Loss 7.900673866271973, Classifier Loss 0.19614994525909424, Total Loss 143.41488647460938\n",
      "43: Encoding Loss 16.426897048950195, Transition Loss 7.563941955566406, Classifier Loss 0.19759903848171234, Total Loss 152.6878662109375\n",
      "43: Encoding Loss 16.29498863220215, Transition Loss 7.815065383911133, Classifier Loss 0.20610612630844116, Total Loss 152.53353881835938\n",
      "43: Encoding Loss 15.656792640686035, Transition Loss 7.9340291023254395, Classifier Loss 0.22283966839313507, Total Loss 149.1251220703125\n",
      "43: Encoding Loss 15.710351943969727, Transition Loss 6.306907653808594, Classifier Loss 0.1960635930299759, Total Loss 146.55055236816406\n",
      "43: Encoding Loss 14.26877212524414, Transition Loss 10.736673355102539, Classifier Loss 0.21076421439647675, Total Loss 137.37393188476562\n",
      "43: Encoding Loss 15.593401908874512, Transition Loss 6.318908214569092, Classifier Loss 0.222898930311203, Total Loss 148.30088806152344\n",
      "43: Encoding Loss 15.609929084777832, Transition Loss 4.49959659576416, Classifier Loss 0.20554663240909576, Total Loss 146.33401489257812\n",
      "43: Encoding Loss 15.314621925354004, Transition Loss 7.052450656890869, Classifier Loss 0.19281534850597382, Total Loss 143.20901489257812\n",
      "43: Encoding Loss 15.875073432922363, Transition Loss 7.52731466293335, Classifier Loss 0.20837397873401642, Total Loss 149.34344482421875\n",
      "43: Encoding Loss 16.001710891723633, Transition Loss 8.33000373840332, Classifier Loss 0.20816224813461304, Total Loss 150.49591064453125\n",
      "43: Encoding Loss 15.900290489196777, Transition Loss 8.326952934265137, Classifier Loss 0.21215809881687164, Total Loss 150.08352661132812\n",
      "43: Encoding Loss 14.578998565673828, Transition Loss 8.887659072875977, Classifier Loss 0.22357942163944244, Total Loss 140.7674560546875\n",
      "43: Encoding Loss 15.664143562316895, Transition Loss 7.52442741394043, Classifier Loss 0.21157094836235046, Total Loss 147.97512817382812\n",
      "43: Encoding Loss 14.730260848999023, Transition Loss 10.200264930725098, Classifier Loss 0.2013339102268219, Total Loss 140.01553344726562\n",
      "43: Encoding Loss 15.732112884521484, Transition Loss 8.04661750793457, Classifier Loss 0.20709557831287384, Total Loss 148.17579650878906\n",
      "43: Encoding Loss 14.8764009475708, Transition Loss 4.225295066833496, Classifier Loss 0.23176054656505585, Total Loss 143.03231811523438\n",
      "43: Encoding Loss 14.4946870803833, Transition Loss 8.06577205657959, Classifier Loss 0.20847585797309875, Total Loss 138.41824340820312\n",
      "43: Encoding Loss 14.682373046875, Transition Loss 5.3860063552856445, Classifier Loss 0.2278062403202057, Total Loss 141.31680297851562\n",
      "43: Encoding Loss 15.482332229614258, Transition Loss 7.980526924133301, Classifier Loss 0.20356722176074982, Total Loss 145.8114776611328\n",
      "43: Encoding Loss 15.252744674682617, Transition Loss 11.075944900512695, Classifier Loss 0.2278115451335907, Total Loss 147.018310546875\n",
      "43: Encoding Loss 16.458772659301758, Transition Loss 6.603724002838135, Classifier Loss 0.2070429027080536, Total Loss 153.69520568847656\n",
      "43: Encoding Loss 15.626482009887695, Transition Loss 8.97850513458252, Classifier Loss 0.20708829164505005, Total Loss 147.51638793945312\n",
      "43: Encoding Loss 14.57162857055664, Transition Loss 7.828118324279785, Classifier Loss 0.242083340883255, Total Loss 142.34698486328125\n",
      "43: Encoding Loss 15.648818016052246, Transition Loss 6.352103233337402, Classifier Loss 0.2169457972049713, Total Loss 148.15554809570312\n",
      "43: Encoding Loss 15.289538383483887, Transition Loss 5.373791217803955, Classifier Loss 0.20715990662574768, Total Loss 144.1070556640625\n",
      "43: Encoding Loss 15.674335479736328, Transition Loss 4.431269645690918, Classifier Loss 0.21470671892166138, Total Loss 147.75161743164062\n",
      "43: Encoding Loss 16.703474044799805, Transition Loss 5.832531452178955, Classifier Loss 0.21752740442752838, Total Loss 156.5470428466797\n",
      "43: Encoding Loss 16.162433624267578, Transition Loss 8.291154861450195, Classifier Loss 0.2223951667547226, Total Loss 153.1972198486328\n",
      "43: Encoding Loss 15.419180870056152, Transition Loss 9.161105155944824, Classifier Loss 0.21448355913162231, Total Loss 146.63401794433594\n",
      "43: Encoding Loss 16.690744400024414, Transition Loss 7.970619201660156, Classifier Loss 0.21597245335578918, Total Loss 156.71731567382812\n",
      "43: Encoding Loss 15.93687915802002, Transition Loss 7.200180530548096, Classifier Loss 0.20458117127418518, Total Loss 149.3931884765625\n",
      "43: Encoding Loss 16.070219039916992, Transition Loss 7.439229488372803, Classifier Loss 0.19736486673355103, Total Loss 149.78607177734375\n",
      "43: Encoding Loss 16.634458541870117, Transition Loss 5.324469089508057, Classifier Loss 0.21229547262191772, Total Loss 155.3701171875\n",
      "43: Encoding Loss 16.388072967529297, Transition Loss 5.890848159790039, Classifier Loss 0.20886656641960144, Total Loss 153.16943359375\n",
      "43: Encoding Loss 15.702446937561035, Transition Loss 5.094060897827148, Classifier Loss 0.20377588272094727, Total Loss 147.0159912109375\n",
      "43: Encoding Loss 16.552587509155273, Transition Loss 8.336027145385742, Classifier Loss 0.2298700511455536, Total Loss 157.0749053955078\n",
      "43: Encoding Loss 15.018583297729492, Transition Loss 8.107181549072266, Classifier Loss 0.20057161152362823, Total Loss 141.8272705078125\n",
      "43: Encoding Loss 14.891058921813965, Transition Loss 6.502964973449707, Classifier Loss 0.2110416293144226, Total Loss 141.53323364257812\n",
      "43: Encoding Loss 15.333410263061523, Transition Loss 6.439828395843506, Classifier Loss 0.21693941950798035, Total Loss 145.64920043945312\n",
      "43: Encoding Loss 14.965923309326172, Transition Loss 6.451667785644531, Classifier Loss 0.19670891761779785, Total Loss 140.68861389160156\n",
      "43: Encoding Loss 14.833296775817871, Transition Loss 5.974564552307129, Classifier Loss 0.2044721245765686, Total Loss 140.30850219726562\n",
      "43: Encoding Loss 15.582595825195312, Transition Loss 5.876191139221191, Classifier Loss 0.19215252995491028, Total Loss 145.05125427246094\n",
      "43: Encoding Loss 16.788944244384766, Transition Loss 7.858271598815918, Classifier Loss 0.22029730677604675, Total Loss 157.91293334960938\n",
      "43: Encoding Loss 17.29862403869629, Transition Loss 6.762223720550537, Classifier Loss 0.23117589950561523, Total Loss 162.85902404785156\n",
      "43: Encoding Loss 15.126761436462402, Transition Loss 5.132932662963867, Classifier Loss 0.2161610871553421, Total Loss 143.6567840576172\n",
      "43: Encoding Loss 14.95913028717041, Transition Loss 5.524980545043945, Classifier Loss 0.1968793272972107, Total Loss 140.46597290039062\n",
      "43: Encoding Loss 15.567370414733887, Transition Loss 7.843610763549805, Classifier Loss 0.21258893609046936, Total Loss 147.3665771484375\n",
      "43: Encoding Loss 14.940604209899902, Transition Loss 6.727123737335205, Classifier Loss 0.19732585549354553, Total Loss 140.60284423828125\n",
      "43: Encoding Loss 15.945418357849121, Transition Loss 6.482771396636963, Classifier Loss 0.2100740373134613, Total Loss 149.8673095703125\n",
      "43: Encoding Loss 16.42209243774414, Transition Loss 6.389194965362549, Classifier Loss 0.18346212804317474, Total Loss 151.0007781982422\n",
      "43: Encoding Loss 15.661327362060547, Transition Loss 8.207120895385742, Classifier Loss 0.2204352468252182, Total Loss 148.97557067871094\n",
      "43: Encoding Loss 15.419217109680176, Transition Loss 6.882131099700928, Classifier Loss 0.23040376603603363, Total Loss 147.77053833007812\n",
      "43: Encoding Loss 16.829429626464844, Transition Loss 6.920733451843262, Classifier Loss 0.2118832767009735, Total Loss 157.20790100097656\n",
      "43: Encoding Loss 15.739824295043945, Transition Loss 7.678452968597412, Classifier Loss 0.2051827609539032, Total Loss 147.97256469726562\n",
      "43: Encoding Loss 15.524272918701172, Transition Loss 7.9347357749938965, Classifier Loss 0.1896502673625946, Total Loss 144.74615478515625\n",
      "43: Encoding Loss 16.527799606323242, Transition Loss 7.95067024230957, Classifier Loss 0.199661985039711, Total Loss 153.7787322998047\n",
      "43: Encoding Loss 14.582762718200684, Transition Loss 9.732552528381348, Classifier Loss 0.2613896131515503, Total Loss 144.74757385253906\n",
      "43: Encoding Loss 15.742188453674316, Transition Loss 6.7735981941223145, Classifier Loss 0.20810645818710327, Total Loss 148.10289001464844\n",
      "43: Encoding Loss 15.326171875, Transition Loss 8.705039024353027, Classifier Loss 0.2051854282617569, Total Loss 144.86892700195312\n",
      "43: Encoding Loss 15.65507984161377, Transition Loss 8.054134368896484, Classifier Loss 0.2227642685174942, Total Loss 149.1278839111328\n",
      "43: Encoding Loss 15.210864067077637, Transition Loss 8.73597526550293, Classifier Loss 0.206917405128479, Total Loss 144.12583923339844\n",
      "43: Encoding Loss 16.952054977416992, Transition Loss 6.374578475952148, Classifier Loss 0.2148306518793106, Total Loss 158.37442016601562\n",
      "43: Encoding Loss 15.328413963317871, Transition Loss 7.598731517791748, Classifier Loss 0.21720555424690247, Total Loss 145.86761474609375\n",
      "43: Encoding Loss 16.194150924682617, Transition Loss 6.323853969573975, Classifier Loss 0.22427183389663696, Total Loss 153.2451629638672\n",
      "43: Encoding Loss 15.688369750976562, Transition Loss 8.455862045288086, Classifier Loss 0.21213027834892273, Total Loss 148.41116333007812\n",
      "43: Encoding Loss 15.597952842712402, Transition Loss 7.038893222808838, Classifier Loss 0.20904624462127686, Total Loss 147.0960235595703\n",
      "43: Encoding Loss 16.68311309814453, Transition Loss 6.938085556030273, Classifier Loss 0.2170449197292328, Total Loss 156.55702209472656\n",
      "43: Encoding Loss 15.20544147491455, Transition Loss 6.544808387756348, Classifier Loss 0.19881051778793335, Total Loss 142.8335418701172\n",
      "43: Encoding Loss 15.583975791931152, Transition Loss 6.072110652923584, Classifier Loss 0.22591343522071838, Total Loss 148.47756958007812\n",
      "43: Encoding Loss 15.46157169342041, Transition Loss 8.237763404846191, Classifier Loss 0.23772630095481873, Total Loss 149.11276245117188\n",
      "43: Encoding Loss 15.121597290039062, Transition Loss 6.477557182312012, Classifier Loss 0.20366142690181732, Total Loss 142.6344451904297\n",
      "43: Encoding Loss 15.48944091796875, Transition Loss 4.811639785766602, Classifier Loss 0.22129003703594208, Total Loss 147.00685119628906\n",
      "43: Encoding Loss 16.10055160522461, Transition Loss 7.003086090087891, Classifier Loss 0.1920607089996338, Total Loss 149.41111755371094\n",
      "43: Encoding Loss 15.798150062561035, Transition Loss 8.440414428710938, Classifier Loss 0.2542440593242645, Total Loss 153.49769592285156\n",
      "43: Encoding Loss 15.93198299407959, Transition Loss 8.399999618530273, Classifier Loss 0.2154294103384018, Total Loss 150.67880249023438\n",
      "43: Encoding Loss 16.705408096313477, Transition Loss 6.2436442375183105, Classifier Loss 0.18922622501850128, Total Loss 153.8146209716797\n",
      "43: Encoding Loss 16.481969833374023, Transition Loss 7.10857629776001, Classifier Loss 0.20274809002876282, Total Loss 153.5522918701172\n",
      "43: Encoding Loss 15.26248550415039, Transition Loss 4.843911170959473, Classifier Loss 0.2013079673051834, Total Loss 143.19944763183594\n",
      "43: Encoding Loss 15.818684577941895, Transition Loss 8.329019546508789, Classifier Loss 0.24116340279579163, Total Loss 152.3316192626953\n",
      "43: Encoding Loss 16.071138381958008, Transition Loss 6.242779731750488, Classifier Loss 0.2162747085094452, Total Loss 151.44512939453125\n",
      "43: Encoding Loss 15.380082130432129, Transition Loss 9.2410888671875, Classifier Loss 0.22100117802619934, Total Loss 146.98899841308594\n",
      "43: Encoding Loss 16.26866340637207, Transition Loss 5.188204288482666, Classifier Loss 0.22139479219913483, Total Loss 153.32643127441406\n",
      "43: Encoding Loss 15.755118370056152, Transition Loss 6.501917839050293, Classifier Loss 0.19704397022724152, Total Loss 147.0457305908203\n",
      "43: Encoding Loss 16.413501739501953, Transition Loss 6.869939804077148, Classifier Loss 0.2397533655166626, Total Loss 156.6573486328125\n",
      "43: Encoding Loss 15.734912872314453, Transition Loss 9.269176483154297, Classifier Loss 0.20929044485092163, Total Loss 148.66217041015625\n",
      "43: Encoding Loss 14.823455810546875, Transition Loss 8.065645217895508, Classifier Loss 0.19164754450321198, Total Loss 139.3655242919922\n",
      "43: Encoding Loss 15.337825775146484, Transition Loss 7.654160976409912, Classifier Loss 0.21437717974185944, Total Loss 145.67117309570312\n",
      "43: Encoding Loss 14.651192665100098, Transition Loss 10.686197280883789, Classifier Loss 0.19178041815757751, Total Loss 138.5248260498047\n",
      "43: Encoding Loss 15.867777824401855, Transition Loss 7.347501277923584, Classifier Loss 0.20983606576919556, Total Loss 149.39532470703125\n",
      "43: Encoding Loss 15.612329483032227, Transition Loss 7.994844436645508, Classifier Loss 0.20905181765556335, Total Loss 147.4027862548828\n",
      "43: Encoding Loss 14.088669776916504, Transition Loss 6.91272497177124, Classifier Loss 0.21360307931900024, Total Loss 135.45220947265625\n",
      "43: Encoding Loss 15.003579139709473, Transition Loss 8.279595375061035, Classifier Loss 0.20431356132030487, Total Loss 142.11590576171875\n",
      "43: Encoding Loss 15.765413284301758, Transition Loss 7.354275226593018, Classifier Loss 0.22154706716537476, Total Loss 149.74887084960938\n",
      "43: Encoding Loss 16.364686965942383, Transition Loss 7.863334655761719, Classifier Loss 0.22552791237831116, Total Loss 155.04295349121094\n",
      "43: Encoding Loss 15.43425178527832, Transition Loss 7.324043273925781, Classifier Loss 0.21030612289905548, Total Loss 145.9694366455078\n",
      "43: Encoding Loss 15.823959350585938, Transition Loss 6.083522796630859, Classifier Loss 0.20184829831123352, Total Loss 147.9932098388672\n",
      "43: Encoding Loss 15.248922348022461, Transition Loss 5.7215399742126465, Classifier Loss 0.24792082607746124, Total Loss 147.92776489257812\n",
      "43: Encoding Loss 15.713171005249023, Transition Loss 7.912501335144043, Classifier Loss 0.20531801879405975, Total Loss 147.81968688964844\n",
      "43: Encoding Loss 16.4337158203125, Transition Loss 7.296133995056152, Classifier Loss 0.22416475415229797, Total Loss 155.34542846679688\n",
      "43: Encoding Loss 15.622583389282227, Transition Loss 7.287771224975586, Classifier Loss 0.22220543026924133, Total Loss 148.65875244140625\n",
      "43: Encoding Loss 15.716373443603516, Transition Loss 7.226064682006836, Classifier Loss 0.20519913733005524, Total Loss 147.69610595703125\n",
      "43: Encoding Loss 15.953275680541992, Transition Loss 6.481705188751221, Classifier Loss 0.2279404252767563, Total Loss 151.7165985107422\n",
      "43: Encoding Loss 16.384899139404297, Transition Loss 4.876699924468994, Classifier Loss 0.21058006584644318, Total Loss 153.112548828125\n",
      "43: Encoding Loss 15.883783340454102, Transition Loss 5.276978492736816, Classifier Loss 0.2100737988948822, Total Loss 149.13304138183594\n",
      "43: Encoding Loss 16.0197696685791, Transition Loss 7.532627105712891, Classifier Loss 0.20314927399158478, Total Loss 149.9796142578125\n",
      "43: Encoding Loss 16.801862716674805, Transition Loss 5.0648603439331055, Classifier Loss 0.2089768648147583, Total Loss 156.3255615234375\n",
      "43: Encoding Loss 17.194398880004883, Transition Loss 5.220493793487549, Classifier Loss 0.23935392498970032, Total Loss 162.53468322753906\n",
      "43: Encoding Loss 14.900117874145508, Transition Loss 7.753209114074707, Classifier Loss 0.20516416430473328, Total Loss 141.26800537109375\n",
      "43: Encoding Loss 14.724430084228516, Transition Loss 8.020153999328613, Classifier Loss 0.1871383786201477, Total Loss 138.11331176757812\n",
      "43: Encoding Loss 15.139512062072754, Transition Loss 7.422027587890625, Classifier Loss 0.22650329768657684, Total Loss 145.25083923339844\n",
      "43: Encoding Loss 15.188384056091309, Transition Loss 8.175768852233887, Classifier Loss 0.24229681491851807, Total Loss 147.3719024658203\n",
      "43: Encoding Loss 14.955840110778809, Transition Loss 10.742895126342773, Classifier Loss 0.21606186032295227, Total Loss 143.40147399902344\n",
      "43: Encoding Loss 16.674989700317383, Transition Loss 6.712050437927246, Classifier Loss 0.21497543156147003, Total Loss 156.2398681640625\n",
      "43: Encoding Loss 16.58464813232422, Transition Loss 6.430931568145752, Classifier Loss 0.18468627333641052, Total Loss 152.4320068359375\n",
      "43: Encoding Loss 14.830333709716797, Transition Loss 5.928244590759277, Classifier Loss 0.2004770189523697, Total Loss 139.87603759765625\n",
      "43: Encoding Loss 15.347200393676758, Transition Loss 5.69478702545166, Classifier Loss 0.21687495708465576, Total Loss 145.60406494140625\n",
      "43: Encoding Loss 15.399763107299805, Transition Loss 6.636223316192627, Classifier Loss 0.20338861644268036, Total Loss 144.8642120361328\n",
      "43: Encoding Loss 15.855267524719238, Transition Loss 7.398350238800049, Classifier Loss 0.2064955234527588, Total Loss 148.97137451171875\n",
      "43: Encoding Loss 15.840983390808105, Transition Loss 5.657984733581543, Classifier Loss 0.2061227560043335, Total Loss 148.47174072265625\n",
      "43: Encoding Loss 15.891252517700195, Transition Loss 6.561921119689941, Classifier Loss 0.2246631383895874, Total Loss 150.90870666503906\n",
      "43: Encoding Loss 14.936192512512207, Transition Loss 5.502339839935303, Classifier Loss 0.18545682728290558, Total Loss 139.13568115234375\n",
      "43: Encoding Loss 15.785319328308105, Transition Loss 7.5361480712890625, Classifier Loss 0.1919466108083725, Total Loss 146.9844512939453\n",
      "43: Encoding Loss 16.08237075805664, Transition Loss 4.898972034454346, Classifier Loss 0.2404337227344513, Total Loss 153.68212890625\n",
      "43: Encoding Loss 15.122017860412598, Transition Loss 8.373085021972656, Classifier Loss 0.22651317715644836, Total Loss 145.30209350585938\n",
      "43: Encoding Loss 15.791613578796387, Transition Loss 6.195094585418701, Classifier Loss 0.20820625126361847, Total Loss 148.39254760742188\n",
      "43: Encoding Loss 15.975237846374512, Transition Loss 7.717327117919922, Classifier Loss 0.22476057708263397, Total Loss 151.82142639160156\n",
      "43: Encoding Loss 15.806376457214355, Transition Loss 6.3625807762146, Classifier Loss 0.20962513983249664, Total Loss 148.68605041503906\n",
      "43: Encoding Loss 15.233277320861816, Transition Loss 7.308446884155273, Classifier Loss 0.20610545575618744, Total Loss 143.93846130371094\n",
      "43: Encoding Loss 16.983976364135742, Transition Loss 4.973175525665283, Classifier Loss 0.21336497366428375, Total Loss 158.20294189453125\n",
      "43: Encoding Loss 16.208484649658203, Transition Loss 4.6025614738464355, Classifier Loss 0.21663428843021393, Total Loss 152.25181579589844\n",
      "43: Encoding Loss 15.514939308166504, Transition Loss 5.6876749992370605, Classifier Loss 0.2032455950975418, Total Loss 145.58160400390625\n",
      "43: Encoding Loss 15.15609359741211, Transition Loss 6.975642204284668, Classifier Loss 0.2177763432264328, Total Loss 144.42152404785156\n",
      "43: Encoding Loss 15.282368659973145, Transition Loss 4.453151702880859, Classifier Loss 0.21298012137413025, Total Loss 144.4475860595703\n",
      "43: Encoding Loss 16.030132293701172, Transition Loss 8.89001178741455, Classifier Loss 0.2212863564491272, Total Loss 152.147705078125\n",
      "43: Encoding Loss 16.253633499145508, Transition Loss 6.350823879241943, Classifier Loss 0.21056604385375977, Total Loss 152.35585021972656\n",
      "43: Encoding Loss 17.109758377075195, Transition Loss 6.378626823425293, Classifier Loss 0.22027355432510376, Total Loss 160.18115234375\n",
      "43: Encoding Loss 16.636388778686523, Transition Loss 8.290305137634277, Classifier Loss 0.21148712933063507, Total Loss 155.89788818359375\n",
      "43: Encoding Loss 15.313321113586426, Transition Loss 9.341572761535645, Classifier Loss 0.20396919548511505, Total Loss 144.7718048095703\n",
      "43: Encoding Loss 13.043542861938477, Transition Loss 11.708405494689941, Classifier Loss 0.19562794268131256, Total Loss 126.25281524658203\n",
      "44: Encoding Loss 16.91266632080078, Transition Loss 5.31727409362793, Classifier Loss 0.21306529641151428, Total Loss 157.67132568359375\n",
      "44: Encoding Loss 16.287599563598633, Transition Loss 6.38394832611084, Classifier Loss 0.21392008662223816, Total Loss 152.9696044921875\n",
      "44: Encoding Loss 16.301925659179688, Transition Loss 5.844223976135254, Classifier Loss 0.2095569670200348, Total Loss 152.53993225097656\n",
      "44: Encoding Loss 15.791719436645508, Transition Loss 7.010197162628174, Classifier Loss 0.2159225344657898, Total Loss 149.3280487060547\n",
      "44: Encoding Loss 17.504154205322266, Transition Loss 5.984489440917969, Classifier Loss 0.2137165069580078, Total Loss 162.60177612304688\n",
      "44: Encoding Loss 14.030818939208984, Transition Loss 7.382596969604492, Classifier Loss 0.2145293653011322, Total Loss 135.17601013183594\n",
      "44: Encoding Loss 16.914470672607422, Transition Loss 5.494376182556152, Classifier Loss 0.22368961572647095, Total Loss 158.7836151123047\n",
      "44: Encoding Loss 17.2264347076416, Transition Loss 6.4038238525390625, Classifier Loss 0.21239788830280304, Total Loss 160.33203125\n",
      "44: Encoding Loss 15.608247756958008, Transition Loss 5.955639839172363, Classifier Loss 0.275267630815506, Total Loss 153.58387756347656\n",
      "44: Encoding Loss 16.704570770263672, Transition Loss 6.230172634124756, Classifier Loss 0.2257145196199417, Total Loss 157.45407104492188\n",
      "44: Encoding Loss 16.089582443237305, Transition Loss 5.397430896759033, Classifier Loss 0.20862862467765808, Total Loss 150.6590118408203\n",
      "44: Encoding Loss 15.767427444458008, Transition Loss 6.184599876403809, Classifier Loss 0.22533833980560303, Total Loss 149.91017150878906\n",
      "44: Encoding Loss 15.621990203857422, Transition Loss 6.924030780792236, Classifier Loss 0.21154463291168213, Total Loss 147.51519775390625\n",
      "44: Encoding Loss 14.949909210205078, Transition Loss 5.974552631378174, Classifier Loss 0.1984063982963562, Total Loss 140.63482666015625\n",
      "44: Encoding Loss 15.077970504760742, Transition Loss 7.118045806884766, Classifier Loss 0.21363063156604767, Total Loss 143.4104461669922\n",
      "44: Encoding Loss 15.931321144104004, Transition Loss 8.119781494140625, Classifier Loss 0.21774733066558838, Total Loss 150.84927368164062\n",
      "44: Encoding Loss 15.839207649230957, Transition Loss 6.593701362609863, Classifier Loss 0.2105976939201355, Total Loss 149.09217834472656\n",
      "44: Encoding Loss 15.285933494567871, Transition Loss 7.753300189971924, Classifier Loss 0.1996738314628601, Total Loss 143.80551147460938\n",
      "44: Encoding Loss 16.40616226196289, Transition Loss 7.364516258239746, Classifier Loss 0.19813686609268188, Total Loss 152.535888671875\n",
      "44: Encoding Loss 16.30453109741211, Transition Loss 7.6045637130737305, Classifier Loss 0.2165912389755249, Total Loss 153.61630249023438\n",
      "44: Encoding Loss 15.677188873291016, Transition Loss 7.805255889892578, Classifier Loss 0.22804662585258484, Total Loss 149.78321838378906\n",
      "44: Encoding Loss 15.707291603088379, Transition Loss 6.186633110046387, Classifier Loss 0.1981104016304016, Total Loss 146.70669555664062\n",
      "44: Encoding Loss 14.283230781555176, Transition Loss 10.671404838562012, Classifier Loss 0.20414131879806519, Total Loss 136.8142547607422\n",
      "44: Encoding Loss 15.578707695007324, Transition Loss 6.197688579559326, Classifier Loss 0.23028388619422913, Total Loss 148.8975830078125\n",
      "44: Encoding Loss 15.611611366271973, Transition Loss 4.329244136810303, Classifier Loss 0.19869554042816162, Total Loss 145.6282958984375\n",
      "44: Encoding Loss 15.30770206451416, Transition Loss 6.628410816192627, Classifier Loss 0.19265128672122955, Total Loss 143.05242919921875\n",
      "44: Encoding Loss 15.877626419067383, Transition Loss 7.3377556800842285, Classifier Loss 0.20730873942375183, Total Loss 149.2194366455078\n",
      "44: Encoding Loss 15.983717918395996, Transition Loss 7.943246364593506, Classifier Loss 0.20404580235481262, Total Loss 149.86297607421875\n",
      "44: Encoding Loss 15.879363059997559, Transition Loss 7.9686737060546875, Classifier Loss 0.20266485214233398, Total Loss 148.89512634277344\n",
      "44: Encoding Loss 14.580639839172363, Transition Loss 8.475804328918457, Classifier Loss 0.22570538520812988, Total Loss 140.9108123779297\n",
      "44: Encoding Loss 15.668928146362305, Transition Loss 7.2784953117370605, Classifier Loss 0.20251122117042542, Total Loss 147.05825805664062\n",
      "44: Encoding Loss 14.723254203796387, Transition Loss 9.678112030029297, Classifier Loss 0.19921641051769257, Total Loss 139.64329528808594\n",
      "44: Encoding Loss 15.743036270141602, Transition Loss 7.933226108551025, Classifier Loss 0.20511746406555176, Total Loss 148.0426788330078\n",
      "44: Encoding Loss 14.88369369506836, Transition Loss 3.9368932247161865, Classifier Loss 0.22137212753295898, Total Loss 141.99415588378906\n",
      "44: Encoding Loss 14.497201919555664, Transition Loss 8.00472640991211, Classifier Loss 0.22057393193244934, Total Loss 139.63595581054688\n",
      "44: Encoding Loss 14.687256813049316, Transition Loss 5.031861305236816, Classifier Loss 0.23514367640018463, Total Loss 142.01881408691406\n",
      "44: Encoding Loss 15.483010292053223, Transition Loss 8.114011764526367, Classifier Loss 0.1973775327205658, Total Loss 145.22463989257812\n",
      "44: Encoding Loss 15.235039710998535, Transition Loss 10.094088554382324, Classifier Loss 0.22588758170604706, Total Loss 146.48789978027344\n",
      "44: Encoding Loss 16.45499038696289, Transition Loss 6.51693868637085, Classifier Loss 0.19605599343776703, Total Loss 152.5489044189453\n",
      "44: Encoding Loss 15.601495742797852, Transition Loss 8.334322929382324, Classifier Loss 0.210612952709198, Total Loss 147.54013061523438\n",
      "44: Encoding Loss 14.525898933410645, Transition Loss 7.861804008483887, Classifier Loss 0.23348866403102875, Total Loss 141.12841796875\n",
      "44: Encoding Loss 15.636503219604492, Transition Loss 5.867895126342773, Classifier Loss 0.20958492159843445, Total Loss 147.22410583496094\n",
      "44: Encoding Loss 15.295517921447754, Transition Loss 5.275089263916016, Classifier Loss 0.20552676916122437, Total Loss 143.9718475341797\n",
      "44: Encoding Loss 15.647807121276855, Transition Loss 4.190680027008057, Classifier Loss 0.2132081538438797, Total Loss 147.34140014648438\n",
      "44: Encoding Loss 16.705049514770508, Transition Loss 5.593447208404541, Classifier Loss 0.2289649099111557, Total Loss 157.6555633544922\n",
      "44: Encoding Loss 16.158344268798828, Transition Loss 7.950459957122803, Classifier Loss 0.22524043917655945, Total Loss 153.38088989257812\n",
      "44: Encoding Loss 15.404797554016113, Transition Loss 8.97552490234375, Classifier Loss 0.20999136567115784, Total Loss 146.03262329101562\n",
      "44: Encoding Loss 16.68976593017578, Transition Loss 7.735170364379883, Classifier Loss 0.21666483581066132, Total Loss 156.7316436767578\n",
      "44: Encoding Loss 15.971041679382324, Transition Loss 6.913380146026611, Classifier Loss 0.20478872954845428, Total Loss 149.6298828125\n",
      "44: Encoding Loss 16.052955627441406, Transition Loss 7.298483848571777, Classifier Loss 0.19038093090057373, Total Loss 148.9214324951172\n",
      "44: Encoding Loss 16.617116928100586, Transition Loss 5.185632705688477, Classifier Loss 0.21642521023750305, Total Loss 155.6165771484375\n",
      "44: Encoding Loss 16.363359451293945, Transition Loss 5.870739459991455, Classifier Loss 0.2071964293718338, Total Loss 152.80067443847656\n",
      "44: Encoding Loss 15.709921836853027, Transition Loss 4.9452314376831055, Classifier Loss 0.2036343812942505, Total Loss 147.0318603515625\n",
      "44: Encoding Loss 16.52157211303711, Transition Loss 8.28073501586914, Classifier Loss 0.22784924507141113, Total Loss 156.61366271972656\n",
      "44: Encoding Loss 15.018750190734863, Transition Loss 7.7465620040893555, Classifier Loss 0.20760990679264069, Total Loss 142.46031188964844\n",
      "44: Encoding Loss 14.868083953857422, Transition Loss 6.5376505851745605, Classifier Loss 0.22472798824310303, Total Loss 142.72500610351562\n",
      "44: Encoding Loss 15.356253623962402, Transition Loss 5.815890312194824, Classifier Loss 0.22494737803936005, Total Loss 146.50794982910156\n",
      "44: Encoding Loss 14.986727714538574, Transition Loss 6.543635368347168, Classifier Loss 0.19379842281341553, Total Loss 140.5823974609375\n",
      "44: Encoding Loss 14.816618919372559, Transition Loss 5.244159698486328, Classifier Loss 0.20131854712963104, Total Loss 139.71363830566406\n",
      "44: Encoding Loss 15.568071365356445, Transition Loss 5.905001640319824, Classifier Loss 0.19029191136360168, Total Loss 144.7547607421875\n",
      "44: Encoding Loss 16.77505874633789, Transition Loss 7.349108695983887, Classifier Loss 0.2233857810497284, Total Loss 158.0088653564453\n",
      "44: Encoding Loss 17.31394386291504, Transition Loss 6.723369598388672, Classifier Loss 0.2270699441432953, Total Loss 162.563232421875\n",
      "44: Encoding Loss 15.14219856262207, Transition Loss 4.778249263763428, Classifier Loss 0.22679078578948975, Total Loss 144.77230834960938\n",
      "44: Encoding Loss 14.976008415222168, Transition Loss 5.257371425628662, Classifier Loss 0.1952449083328247, Total Loss 140.38401794433594\n",
      "44: Encoding Loss 15.55915641784668, Transition Loss 7.269720554351807, Classifier Loss 0.21967028081417084, Total Loss 147.8942413330078\n",
      "44: Encoding Loss 14.897302627563477, Transition Loss 6.401384353637695, Classifier Loss 0.1981256902217865, Total Loss 140.27127075195312\n",
      "44: Encoding Loss 15.936244010925293, Transition Loss 6.03408670425415, Classifier Loss 0.2110936939716339, Total Loss 149.80613708496094\n",
      "44: Encoding Loss 16.423025131225586, Transition Loss 6.162388801574707, Classifier Loss 0.18236778676509857, Total Loss 150.85345458984375\n",
      "44: Encoding Loss 15.649579048156738, Transition Loss 7.559268474578857, Classifier Loss 0.21262003481388092, Total Loss 147.97048950195312\n",
      "44: Encoding Loss 15.412467002868652, Transition Loss 6.650713920593262, Classifier Loss 0.22544480860233307, Total Loss 147.1743621826172\n",
      "44: Encoding Loss 16.834196090698242, Transition Loss 6.5106048583984375, Classifier Loss 0.218502014875412, Total Loss 157.82589721679688\n",
      "44: Encoding Loss 15.708148956298828, Transition Loss 7.399130344390869, Classifier Loss 0.20769764482975006, Total Loss 147.91477966308594\n",
      "44: Encoding Loss 15.52995777130127, Transition Loss 7.211672306060791, Classifier Loss 0.1928057074546814, Total Loss 144.9625701904297\n",
      "44: Encoding Loss 16.530349731445312, Transition Loss 7.545189380645752, Classifier Loss 0.1988164484500885, Total Loss 153.63348388671875\n",
      "44: Encoding Loss 14.58779239654541, Transition Loss 9.024137496948242, Classifier Loss 0.24051976203918457, Total Loss 142.55914306640625\n",
      "44: Encoding Loss 15.729257583618164, Transition Loss 6.599869728088379, Classifier Loss 0.2118626832962036, Total Loss 148.34030151367188\n",
      "44: Encoding Loss 15.320545196533203, Transition Loss 8.17361831665039, Classifier Loss 0.20598194003105164, Total Loss 144.79727172851562\n",
      "44: Encoding Loss 15.657147407531738, Transition Loss 7.836940765380859, Classifier Loss 0.21295768022537231, Total Loss 148.12033081054688\n",
      "44: Encoding Loss 15.227988243103027, Transition Loss 8.165095329284668, Classifier Loss 0.19842378795146942, Total Loss 143.29931640625\n",
      "44: Encoding Loss 16.945030212402344, Transition Loss 6.4780402183532715, Classifier Loss 0.2243172973394394, Total Loss 159.28758239746094\n",
      "44: Encoding Loss 15.315564155578613, Transition Loss 6.961396217346191, Classifier Loss 0.2018592357635498, Total Loss 144.1027069091797\n",
      "44: Encoding Loss 16.201526641845703, Transition Loss 6.537050724029541, Classifier Loss 0.22602741420269012, Total Loss 153.52235412597656\n",
      "44: Encoding Loss 15.693015098571777, Transition Loss 7.436618804931641, Classifier Loss 0.2151699662208557, Total Loss 148.54843139648438\n",
      "44: Encoding Loss 15.600049018859863, Transition Loss 7.377815246582031, Classifier Loss 0.21831221878528595, Total Loss 148.107177734375\n",
      "44: Encoding Loss 16.67348289489746, Transition Loss 5.973349571228027, Classifier Loss 0.20745056867599487, Total Loss 155.3275909423828\n",
      "44: Encoding Loss 15.20625114440918, Transition Loss 7.043396949768066, Classifier Loss 0.1987132728099823, Total Loss 142.93002319335938\n",
      "44: Encoding Loss 15.579983711242676, Transition Loss 5.426105499267578, Classifier Loss 0.21993733942508698, Total Loss 147.7188262939453\n",
      "44: Encoding Loss 15.467756271362305, Transition Loss 8.922587394714355, Classifier Loss 0.22506184875965118, Total Loss 148.0327606201172\n",
      "44: Encoding Loss 15.121203422546387, Transition Loss 5.691590785980225, Classifier Loss 0.2097383290529251, Total Loss 143.081787109375\n",
      "44: Encoding Loss 15.48170280456543, Transition Loss 5.368795394897461, Classifier Loss 0.21442154049873352, Total Loss 146.36953735351562\n",
      "44: Encoding Loss 16.088773727416992, Transition Loss 6.284908294677734, Classifier Loss 0.18588674068450928, Total Loss 148.55584716796875\n",
      "44: Encoding Loss 15.787322998046875, Transition Loss 8.990989685058594, Classifier Loss 0.2574140429496765, Total Loss 153.83819580078125\n",
      "44: Encoding Loss 15.935193061828613, Transition Loss 7.5007829666137695, Classifier Loss 0.2095785290002823, Total Loss 149.93954467773438\n",
      "44: Encoding Loss 16.71889305114746, Transition Loss 6.424443244934082, Classifier Loss 0.19240592420101166, Total Loss 154.276611328125\n",
      "44: Encoding Loss 16.49810218811035, Transition Loss 6.757210731506348, Classifier Loss 0.20456138253211975, Total Loss 153.79238891601562\n",
      "44: Encoding Loss 15.261428833007812, Transition Loss 4.712831974029541, Classifier Loss 0.19748494029045105, Total Loss 142.78248596191406\n",
      "44: Encoding Loss 15.820775032043457, Transition Loss 7.825817584991455, Classifier Loss 0.23714348673820496, Total Loss 151.84571838378906\n",
      "44: Encoding Loss 16.047338485717773, Transition Loss 5.9663310050964355, Classifier Loss 0.2192898392677307, Total Loss 151.50096130371094\n",
      "44: Encoding Loss 15.366183280944824, Transition Loss 8.776915550231934, Classifier Loss 0.22727486491203308, Total Loss 147.41233825683594\n",
      "44: Encoding Loss 16.2593936920166, Transition Loss 4.81776762008667, Classifier Loss 0.21483533084392548, Total Loss 152.52223205566406\n",
      "44: Encoding Loss 15.75047779083252, Transition Loss 6.130762577056885, Classifier Loss 0.19492703676223755, Total Loss 146.72267150878906\n",
      "44: Encoding Loss 16.40753173828125, Transition Loss 6.52451229095459, Classifier Loss 0.24314194917678833, Total Loss 156.8793487548828\n",
      "44: Encoding Loss 15.7327880859375, Transition Loss 8.98475170135498, Classifier Loss 0.2106289118528366, Total Loss 148.72215270996094\n",
      "44: Encoding Loss 14.817785263061523, Transition Loss 7.79066276550293, Classifier Loss 0.1922941952943802, Total Loss 139.32984924316406\n",
      "44: Encoding Loss 15.34454345703125, Transition Loss 7.307192325592041, Classifier Loss 0.20820674300193787, Total Loss 145.03846740722656\n",
      "44: Encoding Loss 14.672467231750488, Transition Loss 10.31648063659668, Classifier Loss 0.20130299031734467, Total Loss 139.57333374023438\n",
      "44: Encoding Loss 15.848433494567871, Transition Loss 6.990817070007324, Classifier Loss 0.21155522763729095, Total Loss 149.34115600585938\n",
      "44: Encoding Loss 15.615727424621582, Transition Loss 7.8700361251831055, Classifier Loss 0.2129867970943451, Total Loss 147.7985076904297\n",
      "44: Encoding Loss 14.067825317382812, Transition Loss 6.677995681762695, Classifier Loss 0.208223357796669, Total Loss 134.70054626464844\n",
      "44: Encoding Loss 14.995257377624512, Transition Loss 8.386991500854492, Classifier Loss 0.20039710402488708, Total Loss 141.67916870117188\n",
      "44: Encoding Loss 15.757954597473145, Transition Loss 7.225497245788574, Classifier Loss 0.21152368187904358, Total Loss 148.66110229492188\n",
      "44: Encoding Loss 16.34996795654297, Transition Loss 8.019882202148438, Classifier Loss 0.22500179708003998, Total Loss 154.90390014648438\n",
      "44: Encoding Loss 15.442647933959961, Transition Loss 7.086863994598389, Classifier Loss 0.21208497881889343, Total Loss 146.1670684814453\n",
      "44: Encoding Loss 15.841347694396973, Transition Loss 5.854277610778809, Classifier Loss 0.1990455985069275, Total Loss 147.8061981201172\n",
      "44: Encoding Loss 15.223966598510742, Transition Loss 5.26869010925293, Classifier Loss 0.2235036939382553, Total Loss 145.1958465576172\n",
      "44: Encoding Loss 15.729976654052734, Transition Loss 8.008204460144043, Classifier Loss 0.19801568984985352, Total Loss 147.24302673339844\n",
      "44: Encoding Loss 16.429027557373047, Transition Loss 6.962234020233154, Classifier Loss 0.2145930826663971, Total Loss 154.2839813232422\n",
      "44: Encoding Loss 15.60181713104248, Transition Loss 7.287539005279541, Classifier Loss 0.2201334834098816, Total Loss 148.28538513183594\n",
      "44: Encoding Loss 15.72092342376709, Transition Loss 6.809784412384033, Classifier Loss 0.2017051726579666, Total Loss 147.2998504638672\n",
      "44: Encoding Loss 15.96076488494873, Transition Loss 6.550144195556641, Classifier Loss 0.2118634581565857, Total Loss 150.1824951171875\n",
      "44: Encoding Loss 16.38622283935547, Transition Loss 4.748342037200928, Classifier Loss 0.20993243157863617, Total Loss 153.03268432617188\n",
      "44: Encoding Loss 15.878266334533691, Transition Loss 5.342529296875, Classifier Loss 0.2105325162410736, Total Loss 149.1479034423828\n",
      "44: Encoding Loss 15.997445106506348, Transition Loss 7.158514976501465, Classifier Loss 0.20018057525157928, Total Loss 149.4293212890625\n",
      "44: Encoding Loss 16.798120498657227, Transition Loss 4.934475898742676, Classifier Loss 0.21105454862117767, Total Loss 156.47731018066406\n",
      "44: Encoding Loss 17.195528030395508, Transition Loss 5.018989562988281, Classifier Loss 0.23227013647556305, Total Loss 161.7950439453125\n",
      "44: Encoding Loss 14.878644943237305, Transition Loss 7.6147356033325195, Classifier Loss 0.21004539728164673, Total Loss 141.55665588378906\n",
      "44: Encoding Loss 14.733590126037598, Transition Loss 7.590337753295898, Classifier Loss 0.19319987297058105, Total Loss 138.706787109375\n",
      "44: Encoding Loss 15.134842872619629, Transition Loss 7.21736478805542, Classifier Loss 0.21663245558738708, Total Loss 144.1854705810547\n",
      "44: Encoding Loss 15.210260391235352, Transition Loss 7.796198844909668, Classifier Loss 0.22972893714904785, Total Loss 146.21420288085938\n",
      "44: Encoding Loss 14.972980499267578, Transition Loss 10.477859497070312, Classifier Loss 0.22207993268966675, Total Loss 144.08738708496094\n",
      "44: Encoding Loss 16.654830932617188, Transition Loss 6.459286212921143, Classifier Loss 0.21433010697364807, Total Loss 155.96351623535156\n",
      "44: Encoding Loss 16.554546356201172, Transition Loss 6.30518102645874, Classifier Loss 0.18486671149730682, Total Loss 152.18409729003906\n",
      "44: Encoding Loss 14.82638931274414, Transition Loss 5.800963878631592, Classifier Loss 0.19822633266448975, Total Loss 139.59393310546875\n",
      "44: Encoding Loss 15.34237003326416, Transition Loss 5.649816036224365, Classifier Loss 0.22430898249149323, Total Loss 146.29981994628906\n",
      "44: Encoding Loss 15.3871431350708, Transition Loss 6.340367317199707, Classifier Loss 0.20298218727111816, Total Loss 144.66342163085938\n",
      "44: Encoding Loss 15.853343963623047, Transition Loss 7.270020008087158, Classifier Loss 0.2219061255455017, Total Loss 150.47137451171875\n",
      "44: Encoding Loss 15.824141502380371, Transition Loss 5.447414398193359, Classifier Loss 0.21092785894870758, Total Loss 148.775390625\n",
      "44: Encoding Loss 15.895272254943848, Transition Loss 6.460784435272217, Classifier Loss 0.22047707438468933, Total Loss 150.50205993652344\n",
      "44: Encoding Loss 14.939359664916992, Transition Loss 5.190803050994873, Classifier Loss 0.1866280734539032, Total Loss 139.21585083007812\n",
      "44: Encoding Loss 15.756353378295898, Transition Loss 7.320833683013916, Classifier Loss 0.1919911503791809, Total Loss 146.71412658691406\n",
      "44: Encoding Loss 16.069812774658203, Transition Loss 4.6921000480651855, Classifier Loss 0.2422599494457245, Total Loss 153.72291564941406\n",
      "44: Encoding Loss 15.112253189086914, Transition Loss 8.563369750976562, Classifier Loss 0.2280965894460678, Total Loss 145.4203643798828\n",
      "44: Encoding Loss 15.807516098022461, Transition Loss 5.841004371643066, Classifier Loss 0.21780860424041748, Total Loss 149.40919494628906\n",
      "44: Encoding Loss 15.982845306396484, Transition Loss 7.752463340759277, Classifier Loss 0.21902450919151306, Total Loss 151.3157196044922\n",
      "44: Encoding Loss 15.809884071350098, Transition Loss 5.935717582702637, Classifier Loss 0.2126198410987854, Total Loss 148.92820739746094\n",
      "44: Encoding Loss 15.228147506713867, Transition Loss 7.300765037536621, Classifier Loss 0.2085781991481781, Total Loss 144.14317321777344\n",
      "44: Encoding Loss 16.954444885253906, Transition Loss 4.679125785827637, Classifier Loss 0.20600183308124542, Total Loss 157.17156982421875\n",
      "44: Encoding Loss 16.189373016357422, Transition Loss 4.601963996887207, Classifier Loss 0.2087310403585434, Total Loss 151.30850219726562\n",
      "44: Encoding Loss 15.510974884033203, Transition Loss 5.316722393035889, Classifier Loss 0.20225538313388824, Total Loss 145.3766632080078\n",
      "44: Encoding Loss 15.152910232543945, Transition Loss 7.282413005828857, Classifier Loss 0.20952555537223816, Total Loss 143.63232421875\n",
      "44: Encoding Loss 15.270771980285645, Transition Loss 3.945718288421631, Classifier Loss 0.20992432534694672, Total Loss 143.94773864746094\n",
      "44: Encoding Loss 16.02872657775879, Transition Loss 9.53395938873291, Classifier Loss 0.22161686420440674, Total Loss 152.2982940673828\n",
      "44: Encoding Loss 16.248451232910156, Transition Loss 5.632023811340332, Classifier Loss 0.2078949213027954, Total Loss 151.90350341796875\n",
      "44: Encoding Loss 17.107084274291992, Transition Loss 7.426993370056152, Classifier Loss 0.2158578336238861, Total Loss 159.9278564453125\n",
      "44: Encoding Loss 16.65792465209961, Transition Loss 6.953605651855469, Classifier Loss 0.20539052784442902, Total Loss 155.19317626953125\n",
      "44: Encoding Loss 15.304479598999023, Transition Loss 10.861862182617188, Classifier Loss 0.19864323735237122, Total Loss 144.47254943847656\n",
      "44: Encoding Loss 13.055604934692383, Transition Loss 9.713813781738281, Classifier Loss 0.19441266357898712, Total Loss 125.82887268066406\n",
      "45: Encoding Loss 16.909982681274414, Transition Loss 6.152348041534424, Classifier Loss 0.2084905356168747, Total Loss 157.35939025878906\n",
      "45: Encoding Loss 16.266101837158203, Transition Loss 5.530678749084473, Classifier Loss 0.20761911571025848, Total Loss 151.9968719482422\n",
      "45: Encoding Loss 16.284141540527344, Transition Loss 6.30852746963501, Classifier Loss 0.20707695186138153, Total Loss 152.24253845214844\n",
      "45: Encoding Loss 15.795527458190918, Transition Loss 6.371769428253174, Classifier Loss 0.21667492389678955, Total Loss 149.30606079101562\n",
      "45: Encoding Loss 17.509519577026367, Transition Loss 6.15328311920166, Classifier Loss 0.22231557965278625, Total Loss 163.53836059570312\n",
      "45: Encoding Loss 14.034375190734863, Transition Loss 6.805826663970947, Classifier Loss 0.21949860453605652, Total Loss 135.5860137939453\n",
      "45: Encoding Loss 16.902734756469727, Transition Loss 5.4699578285217285, Classifier Loss 0.2292247712612152, Total Loss 159.23834228515625\n",
      "45: Encoding Loss 17.215991973876953, Transition Loss 6.244311809539795, Classifier Loss 0.2243986427783966, Total Loss 161.41665649414062\n",
      "45: Encoding Loss 15.600604057312012, Transition Loss 5.797569751739502, Classifier Loss 0.25173449516296387, Total Loss 151.13780212402344\n",
      "45: Encoding Loss 16.707544326782227, Transition Loss 6.1195878982543945, Classifier Loss 0.22255820035934448, Total Loss 157.1400909423828\n",
      "45: Encoding Loss 16.076635360717773, Transition Loss 5.219477653503418, Classifier Loss 0.2012639045715332, Total Loss 149.7833709716797\n",
      "45: Encoding Loss 15.750106811523438, Transition Loss 6.059297561645508, Classifier Loss 0.22952523827552795, Total Loss 150.1652374267578\n",
      "45: Encoding Loss 15.604730606079102, Transition Loss 6.785201072692871, Classifier Loss 0.21779054403305054, Total Loss 147.97393798828125\n",
      "45: Encoding Loss 14.943862915039062, Transition Loss 5.939250946044922, Classifier Loss 0.20607469975948334, Total Loss 141.34622192382812\n",
      "45: Encoding Loss 15.09144401550293, Transition Loss 7.066969871520996, Classifier Loss 0.21738752722740173, Total Loss 143.88369750976562\n",
      "45: Encoding Loss 15.941201210021973, Transition Loss 8.04957389831543, Classifier Loss 0.21808844804763794, Total Loss 150.94837951660156\n",
      "45: Encoding Loss 15.83520221710205, Transition Loss 6.5381669998168945, Classifier Loss 0.2115238904953003, Total Loss 149.1416473388672\n",
      "45: Encoding Loss 15.247143745422363, Transition Loss 7.493124961853027, Classifier Loss 0.2015099972486496, Total Loss 143.62677001953125\n",
      "45: Encoding Loss 16.412256240844727, Transition Loss 7.174404621124268, Classifier Loss 0.19987499713897705, Total Loss 152.72042846679688\n",
      "45: Encoding Loss 16.294200897216797, Transition Loss 7.377896308898926, Classifier Loss 0.2056019902229309, Total Loss 152.389404296875\n",
      "45: Encoding Loss 15.646658897399902, Transition Loss 7.701468467712402, Classifier Loss 0.2223283350467682, Total Loss 148.94639587402344\n",
      "45: Encoding Loss 15.708220481872559, Transition Loss 5.975488662719727, Classifier Loss 0.20245088636875153, Total Loss 147.10595703125\n",
      "45: Encoding Loss 14.268298149108887, Transition Loss 10.325091361999512, Classifier Loss 0.2016690969467163, Total Loss 136.37831115722656\n",
      "45: Encoding Loss 15.584514617919922, Transition Loss 6.074707508087158, Classifier Loss 0.22429856657981873, Total Loss 148.3209228515625\n",
      "45: Encoding Loss 15.59560489654541, Transition Loss 4.1041035652160645, Classifier Loss 0.20026105642318726, Total Loss 145.6117706298828\n",
      "45: Encoding Loss 15.293862342834473, Transition Loss 6.418642044067383, Classifier Loss 0.20161941647529602, Total Loss 143.79656982421875\n",
      "45: Encoding Loss 15.868059158325195, Transition Loss 7.158629417419434, Classifier Loss 0.20040357112884521, Total Loss 148.41656494140625\n",
      "45: Encoding Loss 15.98214054107666, Transition Loss 7.753879070281982, Classifier Loss 0.21303066611289978, Total Loss 150.7109832763672\n",
      "45: Encoding Loss 15.87466049194336, Transition Loss 7.8854265213012695, Classifier Loss 0.20419806241989136, Total Loss 148.9941864013672\n",
      "45: Encoding Loss 14.573311805725098, Transition Loss 8.24209976196289, Classifier Loss 0.21767571568489075, Total Loss 140.00250244140625\n",
      "45: Encoding Loss 15.645814895629883, Transition Loss 7.133452892303467, Classifier Loss 0.20788292586803436, Total Loss 147.38150024414062\n",
      "45: Encoding Loss 14.721248626708984, Transition Loss 9.39698600769043, Classifier Loss 0.19629713892936707, Total Loss 139.2790985107422\n",
      "45: Encoding Loss 15.752091407775879, Transition Loss 7.7103071212768555, Classifier Loss 0.221197247505188, Total Loss 149.67852783203125\n",
      "45: Encoding Loss 14.84048080444336, Transition Loss 3.7216601371765137, Classifier Loss 0.22175759077072144, Total Loss 141.64395141601562\n",
      "45: Encoding Loss 14.503145217895508, Transition Loss 7.61264181137085, Classifier Loss 0.217209130525589, Total Loss 139.2686004638672\n",
      "45: Encoding Loss 14.673456192016602, Transition Loss 4.680271625518799, Classifier Loss 0.22729364037513733, Total Loss 141.05307006835938\n",
      "45: Encoding Loss 15.478440284729004, Transition Loss 7.801750183105469, Classifier Loss 0.2020317018032074, Total Loss 145.59104919433594\n",
      "45: Encoding Loss 15.244196891784668, Transition Loss 9.521892547607422, Classifier Loss 0.22162191569805145, Total Loss 146.0201416015625\n",
      "45: Encoding Loss 16.45313835144043, Transition Loss 6.455743312835693, Classifier Loss 0.20280282199382782, Total Loss 153.19654846191406\n",
      "45: Encoding Loss 15.601014137268066, Transition Loss 7.878974914550781, Classifier Loss 0.20205867290496826, Total Loss 146.58978271484375\n",
      "45: Encoding Loss 14.528298377990723, Transition Loss 7.765829086303711, Classifier Loss 0.23826012015342712, Total Loss 141.60556030273438\n",
      "45: Encoding Loss 15.633482933044434, Transition Loss 5.393946647644043, Classifier Loss 0.2204780876636505, Total Loss 148.19447326660156\n",
      "45: Encoding Loss 15.29956340789795, Transition Loss 5.236412048339844, Classifier Loss 0.20850519835948944, Total Loss 144.2943115234375\n",
      "45: Encoding Loss 15.668420791625977, Transition Loss 3.8358230590820312, Classifier Loss 0.22114156186580658, Total Loss 148.2286834716797\n",
      "45: Encoding Loss 16.69399642944336, Transition Loss 5.74442195892334, Classifier Loss 0.217871755361557, Total Loss 156.488037109375\n",
      "45: Encoding Loss 16.16786003112793, Transition Loss 7.556996822357178, Classifier Loss 0.22111280262470245, Total Loss 152.96556091308594\n",
      "45: Encoding Loss 15.418279647827148, Transition Loss 8.878700256347656, Classifier Loss 0.21032041311264038, Total Loss 146.15402221679688\n",
      "45: Encoding Loss 16.688186645507812, Transition Loss 7.235889434814453, Classifier Loss 0.22348390519618988, Total Loss 157.30105590820312\n",
      "45: Encoding Loss 15.951676368713379, Transition Loss 7.026284694671631, Classifier Loss 0.1972353309392929, Total Loss 148.74220275878906\n",
      "45: Encoding Loss 16.035701751708984, Transition Loss 6.873505592346191, Classifier Loss 0.18824294209480286, Total Loss 148.484619140625\n",
      "45: Encoding Loss 16.612512588500977, Transition Loss 5.246694564819336, Classifier Loss 0.21268808841705322, Total Loss 155.21824645996094\n",
      "45: Encoding Loss 16.35449981689453, Transition Loss 5.41005802154541, Classifier Loss 0.20070773363113403, Total Loss 151.98878479003906\n",
      "45: Encoding Loss 15.699971199035645, Transition Loss 4.86024284362793, Classifier Loss 0.19986999034881592, Total Loss 146.55880737304688\n",
      "45: Encoding Loss 16.522430419921875, Transition Loss 7.6708903312683105, Classifier Loss 0.22769278287887573, Total Loss 156.48289489746094\n",
      "45: Encoding Loss 15.004351615905762, Transition Loss 7.772448539733887, Classifier Loss 0.1956484317779541, Total Loss 141.15414428710938\n",
      "45: Encoding Loss 14.875516891479492, Transition Loss 5.912472248077393, Classifier Loss 0.2175402045249939, Total Loss 141.94065856933594\n",
      "45: Encoding Loss 15.362543106079102, Transition Loss 6.004176139831543, Classifier Loss 0.22306492924690247, Total Loss 146.4076690673828\n",
      "45: Encoding Loss 14.978973388671875, Transition Loss 5.76585578918457, Classifier Loss 0.19551169872283936, Total Loss 140.5361328125\n",
      "45: Encoding Loss 14.823805809020996, Transition Loss 5.9116010665893555, Classifier Loss 0.19425395131111145, Total Loss 139.1981658935547\n",
      "45: Encoding Loss 15.567891120910645, Transition Loss 5.251134395599365, Classifier Loss 0.19191209971904755, Total Loss 144.78457641601562\n",
      "45: Encoding Loss 16.765928268432617, Transition Loss 7.664996147155762, Classifier Loss 0.21407228708267212, Total Loss 157.06765747070312\n",
      "45: Encoding Loss 17.29009437561035, Transition Loss 6.064659118652344, Classifier Loss 0.2195272445678711, Total Loss 161.48641967773438\n",
      "45: Encoding Loss 15.138593673706055, Transition Loss 4.833954811096191, Classifier Loss 0.21339856088161469, Total Loss 143.4154052734375\n",
      "45: Encoding Loss 14.960546493530273, Transition Loss 4.779778480529785, Classifier Loss 0.18993009626865387, Total Loss 139.6333465576172\n",
      "45: Encoding Loss 15.565670013427734, Transition Loss 7.586220741271973, Classifier Loss 0.20476065576076508, Total Loss 146.5186767578125\n",
      "45: Encoding Loss 14.925421714782715, Transition Loss 5.893373489379883, Classifier Loss 0.2027154415845871, Total Loss 140.8535919189453\n",
      "45: Encoding Loss 15.948596954345703, Transition Loss 6.406661033630371, Classifier Loss 0.20783555507659912, Total Loss 149.65365600585938\n",
      "45: Encoding Loss 16.40299415588379, Transition Loss 5.571955680847168, Classifier Loss 0.18711413443088531, Total Loss 151.0497589111328\n",
      "45: Encoding Loss 15.643244743347168, Transition Loss 8.317869186401367, Classifier Loss 0.21972309052944183, Total Loss 148.78184509277344\n",
      "45: Encoding Loss 15.401122093200684, Transition Loss 6.022864818572998, Classifier Loss 0.22411605715751648, Total Loss 146.8251495361328\n",
      "45: Encoding Loss 16.813905715942383, Transition Loss 6.958049774169922, Classifier Loss 0.21789591014385223, Total Loss 157.6924591064453\n",
      "45: Encoding Loss 15.70898723602295, Transition Loss 6.884219646453857, Classifier Loss 0.19759057462215424, Total Loss 146.80780029296875\n",
      "45: Encoding Loss 15.530414581298828, Transition Loss 7.687692165374756, Classifier Loss 0.19302354753017426, Total Loss 145.0832061767578\n",
      "45: Encoding Loss 16.528284072875977, Transition Loss 7.347424507141113, Classifier Loss 0.2072473019361496, Total Loss 154.4204864501953\n",
      "45: Encoding Loss 14.617557525634766, Transition Loss 9.14793872833252, Classifier Loss 0.2429937720298767, Total Loss 143.06942749023438\n",
      "45: Encoding Loss 15.741345405578613, Transition Loss 6.4473724365234375, Classifier Loss 0.20615363121032715, Total Loss 147.83560180664062\n",
      "45: Encoding Loss 15.321099281311035, Transition Loss 8.170821189880371, Classifier Loss 0.20907115936279297, Total Loss 145.11009216308594\n",
      "45: Encoding Loss 15.66473388671875, Transition Loss 7.628533363342285, Classifier Loss 0.21561750769615173, Total Loss 148.40533447265625\n",
      "45: Encoding Loss 15.217974662780762, Transition Loss 8.244559288024902, Classifier Loss 0.20839780569076538, Total Loss 144.23248291015625\n",
      "45: Encoding Loss 16.935266494750977, Transition Loss 6.231977462768555, Classifier Loss 0.21062594652175903, Total Loss 157.79112243652344\n",
      "45: Encoding Loss 15.316003799438477, Transition Loss 7.063897132873535, Classifier Loss 0.20701591670513153, Total Loss 144.64239501953125\n",
      "45: Encoding Loss 16.208486557006836, Transition Loss 6.36379337310791, Classifier Loss 0.21833783388137817, Total Loss 152.77444458007812\n",
      "45: Encoding Loss 15.69015121459961, Transition Loss 7.658902168273926, Classifier Loss 0.2243153601884842, Total Loss 149.4845428466797\n",
      "45: Encoding Loss 15.617844581604004, Transition Loss 7.245073318481445, Classifier Loss 0.20997841656208038, Total Loss 147.38963317871094\n",
      "45: Encoding Loss 16.667041778564453, Transition Loss 6.183372497558594, Classifier Loss 0.19768217206001282, Total Loss 154.3412322998047\n",
      "45: Encoding Loss 15.201261520385742, Transition Loss 6.839254379272461, Classifier Loss 0.18738631904125214, Total Loss 141.71656799316406\n",
      "45: Encoding Loss 15.569733619689941, Transition Loss 5.653985500335693, Classifier Loss 0.22223827242851257, Total Loss 147.91250610351562\n",
      "45: Encoding Loss 15.450761795043945, Transition Loss 8.535650253295898, Classifier Loss 0.23736520111560822, Total Loss 149.04974365234375\n",
      "45: Encoding Loss 15.129037857055664, Transition Loss 5.734034061431885, Classifier Loss 0.2041381448507309, Total Loss 142.59292602539062\n",
      "45: Encoding Loss 15.479260444641113, Transition Loss 4.857707500457764, Classifier Loss 0.20955972373485565, Total Loss 145.7615966796875\n",
      "45: Encoding Loss 16.087583541870117, Transition Loss 6.348990440368652, Classifier Loss 0.206483855843544, Total Loss 150.6188507080078\n",
      "45: Encoding Loss 15.790555953979492, Transition Loss 8.38682746887207, Classifier Loss 0.25386300683021545, Total Loss 153.38812255859375\n",
      "45: Encoding Loss 15.940309524536133, Transition Loss 7.887933254241943, Classifier Loss 0.2098386585712433, Total Loss 150.0839385986328\n",
      "45: Encoding Loss 16.708093643188477, Transition Loss 6.199532985687256, Classifier Loss 0.19093357026576996, Total Loss 153.99801635742188\n",
      "45: Encoding Loss 16.49649429321289, Transition Loss 6.858946800231934, Classifier Loss 0.19349125027656555, Total Loss 152.69287109375\n",
      "45: Encoding Loss 15.248143196105957, Transition Loss 4.75204610824585, Classifier Loss 0.19965329766273499, Total Loss 142.90087890625\n",
      "45: Encoding Loss 15.816385269165039, Transition Loss 8.106042861938477, Classifier Loss 0.22952605783939362, Total Loss 151.1049041748047\n",
      "45: Encoding Loss 16.05650520324707, Transition Loss 6.006282806396484, Classifier Loss 0.22286316752433777, Total Loss 151.9396209716797\n",
      "45: Encoding Loss 15.365538597106934, Transition Loss 8.971294403076172, Classifier Loss 0.22071395814418793, Total Loss 146.7899627685547\n",
      "45: Encoding Loss 16.232038497924805, Transition Loss 4.908010482788086, Classifier Loss 0.21594572067260742, Total Loss 152.43247985839844\n",
      "45: Encoding Loss 15.769246101379395, Transition Loss 6.108470439910889, Classifier Loss 0.20123229920864105, Total Loss 147.49888610839844\n",
      "45: Encoding Loss 16.41533088684082, Transition Loss 6.556087493896484, Classifier Loss 0.2336089015007019, Total Loss 155.9947509765625\n",
      "45: Encoding Loss 15.735074996948242, Transition Loss 8.932731628417969, Classifier Loss 0.21055366098880768, Total Loss 148.72251892089844\n",
      "45: Encoding Loss 14.818517684936523, Transition Loss 7.64483642578125, Classifier Loss 0.18980316817760468, Total Loss 139.05743408203125\n",
      "45: Encoding Loss 15.320782661437988, Transition Loss 7.169474124908447, Classifier Loss 0.21087339520454407, Total Loss 145.08749389648438\n",
      "45: Encoding Loss 14.657331466674805, Transition Loss 10.078145027160645, Classifier Loss 0.1903095543384552, Total Loss 138.30523681640625\n",
      "45: Encoding Loss 15.857564926147461, Transition Loss 6.91311502456665, Classifier Loss 0.21518220007419586, Total Loss 149.76138305664062\n",
      "45: Encoding Loss 15.604924201965332, Transition Loss 7.692287445068359, Classifier Loss 0.2103482335805893, Total Loss 147.4126739501953\n",
      "45: Encoding Loss 14.066753387451172, Transition Loss 6.533203601837158, Classifier Loss 0.2169073522090912, Total Loss 135.53140258789062\n",
      "45: Encoding Loss 14.982985496520996, Transition Loss 8.015710830688477, Classifier Loss 0.2023184895515442, Total Loss 141.69888305664062\n",
      "45: Encoding Loss 15.75271224975586, Transition Loss 7.051835060119629, Classifier Loss 0.21644273400306702, Total Loss 149.07635498046875\n",
      "45: Encoding Loss 16.33522605895996, Transition Loss 7.614960193634033, Classifier Loss 0.21842272579669952, Total Loss 154.0470733642578\n",
      "45: Encoding Loss 15.435262680053711, Transition Loss 6.943803310394287, Classifier Loss 0.20635072886943817, Total Loss 145.50595092773438\n",
      "45: Encoding Loss 15.841336250305176, Transition Loss 5.663936614990234, Classifier Loss 0.2009676992893219, Total Loss 147.96023559570312\n",
      "45: Encoding Loss 15.229073524475098, Transition Loss 5.270902633666992, Classifier Loss 0.24073122441768646, Total Loss 146.95989990234375\n",
      "45: Encoding Loss 15.725361824035645, Transition Loss 7.548781394958496, Classifier Loss 0.20102407038211823, Total Loss 147.41505432128906\n",
      "45: Encoding Loss 16.43557357788086, Transition Loss 6.857967376708984, Classifier Loss 0.2121998816728592, Total Loss 154.07618713378906\n",
      "45: Encoding Loss 15.59318733215332, Transition Loss 7.108837604522705, Classifier Loss 0.22080253064632416, Total Loss 148.2475128173828\n",
      "45: Encoding Loss 15.715021133422852, Transition Loss 6.661112308502197, Classifier Loss 0.205205038189888, Total Loss 147.57290649414062\n",
      "45: Encoding Loss 15.96545124053955, Transition Loss 6.33526611328125, Classifier Loss 0.20562510192394257, Total Loss 149.5531768798828\n",
      "45: Encoding Loss 16.3963565826416, Transition Loss 4.62955379486084, Classifier Loss 0.21175530552864075, Total Loss 153.2722930908203\n",
      "45: Encoding Loss 15.880975723266602, Transition Loss 5.157193183898926, Classifier Loss 0.21313199400901794, Total Loss 149.39244079589844\n",
      "45: Encoding Loss 16.018003463745117, Transition Loss 7.081681251525879, Classifier Loss 0.20117409527301788, Total Loss 149.6777801513672\n",
      "45: Encoding Loss 16.819791793823242, Transition Loss 4.882167816162109, Classifier Loss 0.2007652223110199, Total Loss 155.61129760742188\n",
      "45: Encoding Loss 17.188199996948242, Transition Loss 5.005134105682373, Classifier Loss 0.22876757383346558, Total Loss 161.3833770751953\n",
      "45: Encoding Loss 14.882201194763184, Transition Loss 7.456482887268066, Classifier Loss 0.21383622288703918, Total Loss 141.9325408935547\n",
      "45: Encoding Loss 14.726601600646973, Transition Loss 7.367687225341797, Classifier Loss 0.18986311554908752, Total Loss 138.27267456054688\n",
      "45: Encoding Loss 15.123055458068848, Transition Loss 6.994754314422607, Classifier Loss 0.21870575845241547, Total Loss 144.2539825439453\n",
      "45: Encoding Loss 15.169970512390137, Transition Loss 7.47139310836792, Classifier Loss 0.22579210996627808, Total Loss 145.43325805664062\n",
      "45: Encoding Loss 14.977659225463867, Transition Loss 10.39583969116211, Classifier Loss 0.21871307492256165, Total Loss 143.77175903320312\n",
      "45: Encoding Loss 16.662738800048828, Transition Loss 6.207580089569092, Classifier Loss 0.21875230967998505, Total Loss 156.4186553955078\n",
      "45: Encoding Loss 16.543743133544922, Transition Loss 6.2134690284729, Classifier Loss 0.18191325664520264, Total Loss 151.7839813232422\n",
      "45: Encoding Loss 14.822308540344238, Transition Loss 5.496592998504639, Classifier Loss 0.19918781518936157, Total Loss 139.59657287597656\n",
      "45: Encoding Loss 15.348783493041992, Transition Loss 5.472285747528076, Classifier Loss 0.20586282014846802, Total Loss 144.47100830078125\n",
      "45: Encoding Loss 15.390463829040527, Transition Loss 6.011839866638184, Classifier Loss 0.2062600702047348, Total Loss 144.95208740234375\n",
      "45: Encoding Loss 15.839592933654785, Transition Loss 7.139352798461914, Classifier Loss 0.2163877785205841, Total Loss 149.7834014892578\n",
      "45: Encoding Loss 15.829094886779785, Transition Loss 5.042601585388184, Classifier Loss 0.20723408460617065, Total Loss 148.36468505859375\n",
      "45: Encoding Loss 15.86878776550293, Transition Loss 6.406012058258057, Classifier Loss 0.24108922481536865, Total Loss 152.34043884277344\n",
      "45: Encoding Loss 14.915427207946777, Transition Loss 4.652787208557129, Classifier Loss 0.19115076959133148, Total Loss 139.36904907226562\n",
      "45: Encoding Loss 15.771166801452637, Transition Loss 7.109314918518066, Classifier Loss 0.18319180607795715, Total Loss 145.91036987304688\n",
      "45: Encoding Loss 16.07676887512207, Transition Loss 4.118617534637451, Classifier Loss 0.2332773059606552, Total Loss 152.76560974121094\n",
      "45: Encoding Loss 15.128804206848145, Transition Loss 8.61832332611084, Classifier Loss 0.22331511974334717, Total Loss 145.08560180664062\n",
      "45: Encoding Loss 15.785764694213867, Transition Loss 5.259619235992432, Classifier Loss 0.21737179160118103, Total Loss 149.07522583007812\n",
      "45: Encoding Loss 15.981362342834473, Transition Loss 7.958134174346924, Classifier Loss 0.22065745294094086, Total Loss 151.50828552246094\n",
      "45: Encoding Loss 15.803620338439941, Transition Loss 5.568878173828125, Classifier Loss 0.20364026725292206, Total Loss 147.90676879882812\n",
      "45: Encoding Loss 15.207324028015137, Transition Loss 7.235808372497559, Classifier Loss 0.2057090401649475, Total Loss 143.67665100097656\n",
      "45: Encoding Loss 16.961824417114258, Transition Loss 4.388613224029541, Classifier Loss 0.21132220327854156, Total Loss 157.70452880859375\n",
      "45: Encoding Loss 16.191287994384766, Transition Loss 4.428508758544922, Classifier Loss 0.21696317195892334, Total Loss 152.11231994628906\n",
      "45: Encoding Loss 15.50476360321045, Transition Loss 5.0751423835754395, Classifier Loss 0.2010497897863388, Total Loss 145.15811157226562\n",
      "45: Encoding Loss 15.148571968078613, Transition Loss 6.974641799926758, Classifier Loss 0.22214828431606293, Total Loss 144.79833984375\n",
      "45: Encoding Loss 15.274773597717285, Transition Loss 3.7797341346740723, Classifier Loss 0.21690784394741058, Total Loss 144.64492797851562\n",
      "45: Encoding Loss 16.031084060668945, Transition Loss 8.77061653137207, Classifier Loss 0.2269168347120285, Total Loss 152.69447326660156\n",
      "45: Encoding Loss 16.245784759521484, Transition Loss 5.551729202270508, Classifier Loss 0.20563183724880219, Total Loss 151.63983154296875\n",
      "45: Encoding Loss 17.10396957397461, Transition Loss 6.817456245422363, Classifier Loss 0.2252831757068634, Total Loss 160.7235870361328\n",
      "45: Encoding Loss 16.652807235717773, Transition Loss 7.093161582946777, Classifier Loss 0.20038028061389923, Total Loss 154.67910766601562\n",
      "45: Encoding Loss 15.313419342041016, Transition Loss 9.923290252685547, Classifier Loss 0.2092815637588501, Total Loss 145.420166015625\n",
      "45: Encoding Loss 13.030317306518555, Transition Loss 9.894396781921387, Classifier Loss 0.1941085159778595, Total Loss 125.63227844238281\n",
      "46: Encoding Loss 16.896520614624023, Transition Loss 5.696108818054199, Classifier Loss 0.21049100160598755, Total Loss 157.36048889160156\n",
      "46: Encoding Loss 16.27052116394043, Transition Loss 5.600759029388428, Classifier Loss 0.1966044306755066, Total Loss 150.94476318359375\n",
      "46: Encoding Loss 16.294353485107422, Transition Loss 6.061074256896973, Classifier Loss 0.2118675261735916, Total Loss 152.75381469726562\n",
      "46: Encoding Loss 15.779046058654785, Transition Loss 6.215944290161133, Classifier Loss 0.21269617974758148, Total Loss 148.7451934814453\n",
      "46: Encoding Loss 17.498369216918945, Transition Loss 6.161001205444336, Classifier Loss 0.22346845269203186, Total Loss 163.5659942626953\n",
      "46: Encoding Loss 14.026487350463867, Transition Loss 6.794192314147949, Classifier Loss 0.21915912628173828, Total Loss 135.48666381835938\n",
      "46: Encoding Loss 16.90587615966797, Transition Loss 5.529380798339844, Classifier Loss 0.22203941643238068, Total Loss 158.5568389892578\n",
      "46: Encoding Loss 17.213226318359375, Transition Loss 6.0145087242126465, Classifier Loss 0.24076998233795166, Total Loss 162.98570251464844\n",
      "46: Encoding Loss 15.596175193786621, Transition Loss 5.860070705413818, Classifier Loss 0.2584589123725891, Total Loss 151.7873077392578\n",
      "46: Encoding Loss 16.688785552978516, Transition Loss 5.7823896408081055, Classifier Loss 0.2199370265007019, Total Loss 156.66046142578125\n",
      "46: Encoding Loss 16.101593017578125, Transition Loss 5.238752841949463, Classifier Loss 0.19824731349945068, Total Loss 149.6852264404297\n",
      "46: Encoding Loss 15.751861572265625, Transition Loss 5.922626495361328, Classifier Loss 0.22543233633041382, Total Loss 149.74264526367188\n",
      "46: Encoding Loss 15.60272216796875, Transition Loss 6.655796527862549, Classifier Loss 0.2093660980463028, Total Loss 147.0895538330078\n",
      "46: Encoding Loss 14.93342113494873, Transition Loss 5.659470558166504, Classifier Loss 0.201654314994812, Total Loss 140.7646942138672\n",
      "46: Encoding Loss 15.060994148254395, Transition Loss 6.71535587310791, Classifier Loss 0.21116158366203308, Total Loss 142.9471893310547\n",
      "46: Encoding Loss 15.93484878540039, Transition Loss 7.7452850341796875, Classifier Loss 0.2172185331583023, Total Loss 150.74969482421875\n",
      "46: Encoding Loss 15.83942699432373, Transition Loss 6.337698936462402, Classifier Loss 0.2123049944639206, Total Loss 149.2134552001953\n",
      "46: Encoding Loss 15.27038860321045, Transition Loss 7.228738784790039, Classifier Loss 0.19619733095169067, Total Loss 143.2285919189453\n",
      "46: Encoding Loss 16.401798248291016, Transition Loss 6.9547929763793945, Classifier Loss 0.19327256083488464, Total Loss 151.93260192871094\n",
      "46: Encoding Loss 16.287364959716797, Transition Loss 7.106044769287109, Classifier Loss 0.20186756551265717, Total Loss 151.90689086914062\n",
      "46: Encoding Loss 15.650129318237305, Transition Loss 7.411395072937012, Classifier Loss 0.22113962471485138, Total Loss 148.7972869873047\n",
      "46: Encoding Loss 15.678950309753418, Transition Loss 5.775455474853516, Classifier Loss 0.20088832080364227, Total Loss 146.67552185058594\n",
      "46: Encoding Loss 14.2628812789917, Transition Loss 9.940279960632324, Classifier Loss 0.2032647281885147, Total Loss 136.41757202148438\n",
      "46: Encoding Loss 15.552380561828613, Transition Loss 5.8479323387146, Classifier Loss 0.22450542449951172, Total Loss 148.03916931152344\n",
      "46: Encoding Loss 15.589255332946777, Transition Loss 4.070791721343994, Classifier Loss 0.1970512568950653, Total Loss 145.23333740234375\n",
      "46: Encoding Loss 15.299674034118652, Transition Loss 6.335415840148926, Classifier Loss 0.19540037214756012, Total Loss 143.2045135498047\n",
      "46: Encoding Loss 15.868219375610352, Transition Loss 6.9498515129089355, Classifier Loss 0.20522285997867584, Total Loss 148.85801696777344\n",
      "46: Encoding Loss 16.01980209350586, Transition Loss 7.698127746582031, Classifier Loss 0.2114430069923401, Total Loss 150.8423614501953\n",
      "46: Encoding Loss 15.875897407531738, Transition Loss 7.728619575500488, Classifier Loss 0.20254649221897125, Total Loss 148.80755615234375\n",
      "46: Encoding Loss 14.580519676208496, Transition Loss 8.296046257019043, Classifier Loss 0.2185070812702179, Total Loss 140.15408325195312\n",
      "46: Encoding Loss 15.667195320129395, Transition Loss 6.997849941253662, Classifier Loss 0.19958849251270294, Total Loss 146.69598388671875\n",
      "46: Encoding Loss 14.708230018615723, Transition Loss 9.463200569152832, Classifier Loss 0.20045292377471924, Total Loss 139.60377502441406\n",
      "46: Encoding Loss 15.749014854431152, Transition Loss 7.4973249435424805, Classifier Loss 0.20892399549484253, Total Loss 148.3839874267578\n",
      "46: Encoding Loss 14.856572151184082, Transition Loss 3.7623350620269775, Classifier Loss 0.22994953393936157, Total Loss 142.60000610351562\n",
      "46: Encoding Loss 14.494715690612793, Transition Loss 7.4905571937561035, Classifier Loss 0.20940043032169342, Total Loss 138.3958740234375\n",
      "46: Encoding Loss 14.67325210571289, Transition Loss 4.8659257888793945, Classifier Loss 0.2238006442785263, Total Loss 140.7392578125\n",
      "46: Encoding Loss 15.474675178527832, Transition Loss 7.548420429229736, Classifier Loss 0.19796931743621826, Total Loss 145.10401916503906\n",
      "46: Encoding Loss 15.256011009216309, Transition Loss 10.114194869995117, Classifier Loss 0.214320570230484, Total Loss 145.50299072265625\n",
      "46: Encoding Loss 16.46683692932129, Transition Loss 6.306200981140137, Classifier Loss 0.21302740275859833, Total Loss 154.29867553710938\n",
      "46: Encoding Loss 15.589978218078613, Transition Loss 8.411808013916016, Classifier Loss 0.21216368675231934, Total Loss 147.61854553222656\n",
      "46: Encoding Loss 14.53023624420166, Transition Loss 7.386876106262207, Classifier Loss 0.2404947131872177, Total Loss 141.76873779296875\n",
      "46: Encoding Loss 15.640292167663574, Transition Loss 5.794121265411377, Classifier Loss 0.22490756213665009, Total Loss 148.7719268798828\n",
      "46: Encoding Loss 15.269043922424316, Transition Loss 4.920496463775635, Classifier Loss 0.19751274585723877, Total Loss 142.88772583007812\n",
      "46: Encoding Loss 15.660590171813965, Transition Loss 4.0486602783203125, Classifier Loss 0.2088119387626648, Total Loss 146.97564697265625\n",
      "46: Encoding Loss 16.708070755004883, Transition Loss 5.250009059906006, Classifier Loss 0.2196623682975769, Total Loss 156.68080139160156\n",
      "46: Encoding Loss 16.147430419921875, Transition Loss 7.838018417358398, Classifier Loss 0.21764570474624634, Total Loss 152.51161193847656\n",
      "46: Encoding Loss 15.40298080444336, Transition Loss 8.525972366333008, Classifier Loss 0.2097054421901703, Total Loss 145.89959716796875\n",
      "46: Encoding Loss 16.68095588684082, Transition Loss 7.284802436828613, Classifier Loss 0.2171061635017395, Total Loss 156.61521911621094\n",
      "46: Encoding Loss 15.964816093444824, Transition Loss 6.6202392578125, Classifier Loss 0.1994418501853943, Total Loss 148.9867706298828\n",
      "46: Encoding Loss 16.04888343811035, Transition Loss 6.903090953826904, Classifier Loss 0.18944236636161804, Total Loss 148.71591186523438\n",
      "46: Encoding Loss 16.611629486083984, Transition Loss 4.945655822753906, Classifier Loss 0.21912823617458344, Total Loss 155.79501342773438\n",
      "46: Encoding Loss 16.36366081237793, Transition Loss 5.480313301086426, Classifier Loss 0.20624443888664246, Total Loss 152.62979125976562\n",
      "46: Encoding Loss 15.692082405090332, Transition Loss 4.651043891906738, Classifier Loss 0.20352640748023987, Total Loss 146.8195037841797\n",
      "46: Encoding Loss 16.52947235107422, Transition Loss 7.73260498046875, Classifier Loss 0.21712523698806763, Total Loss 155.4948272705078\n",
      "46: Encoding Loss 15.002543449401855, Transition Loss 7.619466781616211, Classifier Loss 0.20402070879936218, Total Loss 141.94631958007812\n",
      "46: Encoding Loss 14.866475105285645, Transition Loss 6.073375225067139, Classifier Loss 0.227659210562706, Total Loss 142.9123992919922\n",
      "46: Encoding Loss 15.351727485656738, Transition Loss 5.656737804412842, Classifier Loss 0.21049490571022034, Total Loss 144.99465942382812\n",
      "46: Encoding Loss 14.978612899780273, Transition Loss 5.919034957885742, Classifier Loss 0.191202774643898, Total Loss 140.13299560546875\n",
      "46: Encoding Loss 14.820112228393555, Transition Loss 5.38747501373291, Classifier Loss 0.19694119691848755, Total Loss 139.33251953125\n",
      "46: Encoding Loss 15.560544967651367, Transition Loss 5.513550281524658, Classifier Loss 0.19222821295261383, Total Loss 144.8098907470703\n",
      "46: Encoding Loss 16.763690948486328, Transition Loss 7.116305828094482, Classifier Loss 0.22457046806812286, Total Loss 157.98983764648438\n",
      "46: Encoding Loss 17.27230453491211, Transition Loss 6.36530065536499, Classifier Loss 0.2227189838886261, Total Loss 161.72340393066406\n",
      "46: Encoding Loss 15.153568267822266, Transition Loss 4.599411964416504, Classifier Loss 0.2155042290687561, Total Loss 143.69883728027344\n",
      "46: Encoding Loss 14.960372924804688, Transition Loss 5.056490421295166, Classifier Loss 0.19436059892177582, Total Loss 140.13034057617188\n",
      "46: Encoding Loss 15.555306434631348, Transition Loss 7.069726943969727, Classifier Loss 0.21625186502933502, Total Loss 147.48158264160156\n",
      "46: Encoding Loss 14.908985137939453, Transition Loss 6.080962181091309, Classifier Loss 0.19535106420516968, Total Loss 140.02316284179688\n",
      "46: Encoding Loss 15.922700881958008, Transition Loss 5.793035507202148, Classifier Loss 0.21434654295444489, Total Loss 149.974853515625\n",
      "46: Encoding Loss 16.423053741455078, Transition Loss 5.9624457359313965, Classifier Loss 0.1857374608516693, Total Loss 151.15066528320312\n",
      "46: Encoding Loss 15.648758888244629, Transition Loss 7.309655666351318, Classifier Loss 0.2108142375946045, Total Loss 147.73342895507812\n",
      "46: Encoding Loss 15.418920516967773, Transition Loss 6.516457557678223, Classifier Loss 0.2187141478061676, Total Loss 146.5260772705078\n",
      "46: Encoding Loss 16.81367301940918, Transition Loss 6.151981353759766, Classifier Loss 0.20586198568344116, Total Loss 156.3259735107422\n",
      "46: Encoding Loss 15.700324058532715, Transition Loss 7.341596603393555, Classifier Loss 0.20639410614967346, Total Loss 147.7103271484375\n",
      "46: Encoding Loss 15.530069351196289, Transition Loss 6.906839847564697, Classifier Loss 0.19179090857505798, Total Loss 144.80101013183594\n",
      "46: Encoding Loss 16.509418487548828, Transition Loss 7.490414142608643, Classifier Loss 0.19085946679115295, Total Loss 152.6593780517578\n",
      "46: Encoding Loss 14.604150772094727, Transition Loss 8.609284400939941, Classifier Loss 0.2410757690668106, Total Loss 142.6626434326172\n",
      "46: Encoding Loss 15.726812362670898, Transition Loss 6.543891429901123, Classifier Loss 0.20924250781536102, Total Loss 148.0475311279297\n",
      "46: Encoding Loss 15.328536987304688, Transition Loss 7.769643306732178, Classifier Loss 0.2060820460319519, Total Loss 144.79042053222656\n",
      "46: Encoding Loss 15.649317741394043, Transition Loss 7.885257720947266, Classifier Loss 0.21111859381198883, Total Loss 147.88345336914062\n",
      "46: Encoding Loss 15.228968620300293, Transition Loss 7.754098415374756, Classifier Loss 0.20183905959129333, Total Loss 143.5664825439453\n",
      "46: Encoding Loss 16.94040298461914, Transition Loss 6.599915504455566, Classifier Loss 0.2123355269432068, Total Loss 158.07675170898438\n",
      "46: Encoding Loss 15.310893058776855, Transition Loss 6.320854663848877, Classifier Loss 0.20415787398815155, Total Loss 144.16709899902344\n",
      "46: Encoding Loss 16.19373321533203, Transition Loss 6.7965474128723145, Classifier Loss 0.22338910400867462, Total Loss 153.2480926513672\n",
      "46: Encoding Loss 15.681718826293945, Transition Loss 6.853503227233887, Classifier Loss 0.2193513959646225, Total Loss 148.75958251953125\n",
      "46: Encoding Loss 15.59465503692627, Transition Loss 7.537988185882568, Classifier Loss 0.20709192752838135, Total Loss 146.97402954101562\n",
      "46: Encoding Loss 16.662622451782227, Transition Loss 5.629960060119629, Classifier Loss 0.20956139266490936, Total Loss 155.3831024169922\n",
      "46: Encoding Loss 15.197861671447754, Transition Loss 6.713994026184082, Classifier Loss 0.19938701391220093, Total Loss 142.86441040039062\n",
      "46: Encoding Loss 15.574347496032715, Transition Loss 5.175876617431641, Classifier Loss 0.22919857501983643, Total Loss 148.5498046875\n",
      "46: Encoding Loss 15.447295188903809, Transition Loss 8.490022659301758, Classifier Loss 0.2178671658039093, Total Loss 147.06307983398438\n",
      "46: Encoding Loss 15.11504077911377, Transition Loss 5.4729743003845215, Classifier Loss 0.2022523432970047, Total Loss 142.2401580810547\n",
      "46: Encoding Loss 15.486917495727539, Transition Loss 4.844860553741455, Classifier Loss 0.20458923280239105, Total Loss 145.3232421875\n",
      "46: Encoding Loss 16.083688735961914, Transition Loss 6.050759315490723, Classifier Loss 0.1881052404642105, Total Loss 148.69020080566406\n",
      "46: Encoding Loss 15.777112007141113, Transition Loss 8.022896766662598, Classifier Loss 0.24831323325634003, Total Loss 152.65280151367188\n",
      "46: Encoding Loss 15.912571907043457, Transition Loss 7.581816673278809, Classifier Loss 0.20841212570667267, Total Loss 149.65814208984375\n",
      "46: Encoding Loss 16.713695526123047, Transition Loss 6.034262180328369, Classifier Loss 0.18870580196380615, Total Loss 153.78700256347656\n",
      "46: Encoding Loss 16.494565963745117, Transition Loss 6.690296649932861, Classifier Loss 0.20680508017539978, Total Loss 153.97509765625\n",
      "46: Encoding Loss 15.25282096862793, Transition Loss 4.468271255493164, Classifier Loss 0.20484989881515503, Total Loss 143.40122985839844\n",
      "46: Encoding Loss 15.793682098388672, Transition Loss 7.760345458984375, Classifier Loss 0.23025628924369812, Total Loss 150.92715454101562\n",
      "46: Encoding Loss 16.048986434936523, Transition Loss 5.790696620941162, Classifier Loss 0.23106975853443146, Total Loss 152.65701293945312\n",
      "46: Encoding Loss 15.369903564453125, Transition Loss 8.60851001739502, Classifier Loss 0.2233656644821167, Total Loss 147.01748657226562\n",
      "46: Encoding Loss 16.242647171020508, Transition Loss 4.737008571624756, Classifier Loss 0.21188977360725403, Total Loss 152.0775604248047\n",
      "46: Encoding Loss 15.74742603302002, Transition Loss 5.9071044921875, Classifier Loss 0.19978341460227966, Total Loss 147.13917541503906\n",
      "46: Encoding Loss 16.402624130249023, Transition Loss 6.241475582122803, Classifier Loss 0.2470356673002243, Total Loss 157.1728515625\n",
      "46: Encoding Loss 15.733780860900879, Transition Loss 8.492383003234863, Classifier Loss 0.20758512616157532, Total Loss 148.32723999023438\n",
      "46: Encoding Loss 14.818761825561523, Transition Loss 7.442468643188477, Classifier Loss 0.19116638600826263, Total Loss 139.15524291992188\n",
      "46: Encoding Loss 15.322157859802246, Transition Loss 6.959851264953613, Classifier Loss 0.19585192203521729, Total Loss 143.55442810058594\n",
      "46: Encoding Loss 14.655324935913086, Transition Loss 9.93041706085205, Classifier Loss 0.20332476496696472, Total Loss 139.56117248535156\n",
      "46: Encoding Loss 15.847216606140137, Transition Loss 6.646431922912598, Classifier Loss 0.20517243444919586, Total Loss 148.624267578125\n",
      "46: Encoding Loss 15.604178428649902, Transition Loss 7.425717353820801, Classifier Loss 0.2060021609067917, Total Loss 146.9187774658203\n",
      "46: Encoding Loss 14.08266544342041, Transition Loss 6.345649719238281, Classifier Loss 0.21924050152301788, Total Loss 135.85452270507812\n",
      "46: Encoding Loss 14.994378089904785, Transition Loss 7.801311492919922, Classifier Loss 0.2036546915769577, Total Loss 141.88075256347656\n",
      "46: Encoding Loss 15.739572525024414, Transition Loss 6.86572265625, Classifier Loss 0.22007976472377777, Total Loss 149.29769897460938\n",
      "46: Encoding Loss 16.34088897705078, Transition Loss 7.310472011566162, Classifier Loss 0.22481520473957062, Total Loss 154.6707305908203\n",
      "46: Encoding Loss 15.42514705657959, Transition Loss 6.663880348205566, Classifier Loss 0.2057035118341446, Total Loss 145.30430603027344\n",
      "46: Encoding Loss 15.83554458618164, Transition Loss 5.47714900970459, Classifier Loss 0.1938478797674179, Total Loss 147.16456604003906\n",
      "46: Encoding Loss 15.221479415893555, Transition Loss 5.184200286865234, Classifier Loss 0.2280901074409485, Total Loss 145.61769104003906\n",
      "46: Encoding Loss 15.733716011047363, Transition Loss 7.463770389556885, Classifier Loss 0.19328610599040985, Total Loss 146.6910858154297\n",
      "46: Encoding Loss 16.438114166259766, Transition Loss 6.886213302612305, Classifier Loss 0.20923177897930145, Total Loss 153.80532836914062\n",
      "46: Encoding Loss 15.596212387084961, Transition Loss 6.831846714019775, Classifier Loss 0.22062647342681885, Total Loss 148.19871520996094\n",
      "46: Encoding Loss 15.722850799560547, Transition Loss 6.617654323577881, Classifier Loss 0.19642674922943115, Total Loss 146.7490234375\n",
      "46: Encoding Loss 15.939763069152832, Transition Loss 6.175926685333252, Classifier Loss 0.20638588070869446, Total Loss 149.39187622070312\n",
      "46: Encoding Loss 16.39972496032715, Transition Loss 4.671523094177246, Classifier Loss 0.21001121401786804, Total Loss 153.1332244873047\n",
      "46: Encoding Loss 15.900187492370605, Transition Loss 4.959432601928711, Classifier Loss 0.20105725526809692, Total Loss 148.29910278320312\n",
      "46: Encoding Loss 16.014413833618164, Transition Loss 7.020769119262695, Classifier Loss 0.19265194237232208, Total Loss 148.78466796875\n",
      "46: Encoding Loss 16.812969207763672, Transition Loss 4.726324081420898, Classifier Loss 0.21917836368083954, Total Loss 157.3668670654297\n",
      "46: Encoding Loss 17.193395614624023, Transition Loss 4.846365451812744, Classifier Loss 0.22523052990436554, Total Loss 161.03948974609375\n",
      "46: Encoding Loss 14.885088920593262, Transition Loss 7.20081090927124, Classifier Loss 0.2135651409626007, Total Loss 141.87738037109375\n",
      "46: Encoding Loss 14.733427047729492, Transition Loss 7.321875095367432, Classifier Loss 0.19376543164253235, Total Loss 138.70834350585938\n",
      "46: Encoding Loss 15.14537525177002, Transition Loss 6.8975300788879395, Classifier Loss 0.23120006918907166, Total Loss 145.66250610351562\n",
      "46: Encoding Loss 15.185182571411133, Transition Loss 7.652435779571533, Classifier Loss 0.22147828340530396, Total Loss 145.15977478027344\n",
      "46: Encoding Loss 14.965673446655273, Transition Loss 10.04546070098877, Classifier Loss 0.21652480959892273, Total Loss 143.386962890625\n",
      "46: Encoding Loss 16.65418815612793, Transition Loss 6.373003005981445, Classifier Loss 0.2023882269859314, Total Loss 154.74691772460938\n",
      "46: Encoding Loss 16.553382873535156, Transition Loss 6.039826393127441, Classifier Loss 0.18196582794189453, Total Loss 151.83160400390625\n",
      "46: Encoding Loss 14.8221435546875, Transition Loss 5.577820301055908, Classifier Loss 0.19482293725013733, Total Loss 139.17501831054688\n",
      "46: Encoding Loss 15.337064743041992, Transition Loss 5.318380355834961, Classifier Loss 0.21986863017082214, Total Loss 145.74705505371094\n",
      "46: Encoding Loss 15.408260345458984, Transition Loss 6.226339340209961, Classifier Loss 0.21418188512325287, Total Loss 145.92955017089844\n",
      "46: Encoding Loss 15.850484848022461, Transition Loss 6.9522175788879395, Classifier Loss 0.20017895102500916, Total Loss 148.21221923828125\n",
      "46: Encoding Loss 15.839981079101562, Transition Loss 5.375482082366943, Classifier Loss 0.19690972566604614, Total Loss 147.48593139648438\n",
      "46: Encoding Loss 15.852896690368652, Transition Loss 6.114724636077881, Classifier Loss 0.24008095264434814, Total Loss 152.05421447753906\n",
      "46: Encoding Loss 14.935585975646973, Transition Loss 5.083037376403809, Classifier Loss 0.18603986501693726, Total Loss 139.10528564453125\n",
      "46: Encoding Loss 15.757349014282227, Transition Loss 6.844930648803711, Classifier Loss 0.1818007230758667, Total Loss 145.60784912109375\n",
      "46: Encoding Loss 16.077796936035156, Transition Loss 4.544219493865967, Classifier Loss 0.24170149862766266, Total Loss 153.7013702392578\n",
      "46: Encoding Loss 15.121763229370117, Transition Loss 7.747932434082031, Classifier Loss 0.2190120965242386, Total Loss 144.42491149902344\n",
      "46: Encoding Loss 15.779322624206543, Transition Loss 5.696288585662842, Classifier Loss 0.2131257951259613, Total Loss 148.68641662597656\n",
      "46: Encoding Loss 15.97221851348877, Transition Loss 7.118995189666748, Classifier Loss 0.2266756296157837, Total Loss 151.86911010742188\n",
      "46: Encoding Loss 15.807915687561035, Transition Loss 5.836489677429199, Classifier Loss 0.2016400396823883, Total Loss 147.7946319580078\n",
      "46: Encoding Loss 15.215727806091309, Transition Loss 6.741431713104248, Classifier Loss 0.2000240981578827, Total Loss 143.07650756835938\n",
      "46: Encoding Loss 16.967056274414062, Transition Loss 4.571908473968506, Classifier Loss 0.2087097316980362, Total Loss 157.5218048095703\n",
      "46: Encoding Loss 16.19008445739746, Transition Loss 4.2550764083862305, Classifier Loss 0.21966350078582764, Total Loss 152.33804321289062\n",
      "46: Encoding Loss 15.50928783416748, Transition Loss 5.186177730560303, Classifier Loss 0.20225226879119873, Total Loss 145.33676147460938\n",
      "46: Encoding Loss 15.156009674072266, Transition Loss 6.3943867683410645, Classifier Loss 0.21235086023807526, Total Loss 143.7620391845703\n",
      "46: Encoding Loss 15.271851539611816, Transition Loss 3.96177339553833, Classifier Loss 0.21608814597129822, Total Loss 144.57598876953125\n",
      "46: Encoding Loss 15.999441146850586, Transition Loss 8.268239974975586, Classifier Loss 0.21665140986442566, Total Loss 151.31431579589844\n",
      "46: Encoding Loss 16.224735260009766, Transition Loss 5.765345096588135, Classifier Loss 0.20365656912326813, Total Loss 151.31661987304688\n",
      "46: Encoding Loss 17.10577392578125, Transition Loss 6.151906490325928, Classifier Loss 0.2164788842201233, Total Loss 159.72445678710938\n",
      "46: Encoding Loss 16.625411987304688, Transition Loss 7.698347568511963, Classifier Loss 0.201169952750206, Total Loss 154.6599578857422\n",
      "46: Encoding Loss 15.310281753540039, Transition Loss 8.80241870880127, Classifier Loss 0.1975720077753067, Total Loss 143.99993896484375\n",
      "46: Encoding Loss 13.078288078308105, Transition Loss 10.929099082946777, Classifier Loss 0.1957501769065857, Total Loss 126.38714599609375\n",
      "47: Encoding Loss 16.888511657714844, Transition Loss 4.99106502532959, Classifier Loss 0.21927662193775177, Total Loss 158.03396606445312\n",
      "47: Encoding Loss 16.262849807739258, Transition Loss 6.187762260437012, Classifier Loss 0.20897260308265686, Total Loss 152.23760986328125\n",
      "47: Encoding Loss 16.281902313232422, Transition Loss 5.468095302581787, Classifier Loss 0.20661409199237823, Total Loss 152.01025390625\n",
      "47: Encoding Loss 15.775137901306152, Transition Loss 6.640896797180176, Classifier Loss 0.21205922961235046, Total Loss 148.73521423339844\n",
      "47: Encoding Loss 17.493228912353516, Transition Loss 5.5743889808654785, Classifier Loss 0.20501433312892914, Total Loss 161.56214904785156\n",
      "47: Encoding Loss 14.033187866210938, Transition Loss 7.16068172454834, Classifier Loss 0.22675973176956177, Total Loss 136.3736114501953\n",
      "47: Encoding Loss 16.893518447875977, Transition Loss 5.284912586212158, Classifier Loss 0.23308657109737396, Total Loss 159.51377868652344\n",
      "47: Encoding Loss 17.218931198120117, Transition Loss 6.194985866546631, Classifier Loss 0.2213333249092102, Total Loss 161.123779296875\n",
      "47: Encoding Loss 15.602391242980957, Transition Loss 5.746675491333008, Classifier Loss 0.27841660380363464, Total Loss 153.8101348876953\n",
      "47: Encoding Loss 16.685396194458008, Transition Loss 5.660435676574707, Classifier Loss 0.22317755222320557, Total Loss 156.93299865722656\n",
      "47: Encoding Loss 16.072650909423828, Transition Loss 5.047905921936035, Classifier Loss 0.19919295608997345, Total Loss 149.5100860595703\n",
      "47: Encoding Loss 15.761896133422852, Transition Loss 5.8440961837768555, Classifier Loss 0.22551095485687256, Total Loss 149.81509399414062\n",
      "47: Encoding Loss 15.604939460754395, Transition Loss 6.511981964111328, Classifier Loss 0.20855209231376648, Total Loss 146.99713134765625\n",
      "47: Encoding Loss 14.938029289245605, Transition Loss 5.6996612548828125, Classifier Loss 0.19791555404663086, Total Loss 140.43572998046875\n",
      "47: Encoding Loss 15.058748245239258, Transition Loss 6.9144511222839355, Classifier Loss 0.20286384224891663, Total Loss 142.13926696777344\n",
      "47: Encoding Loss 15.934884071350098, Transition Loss 7.866399765014648, Classifier Loss 0.20981085300445557, Total Loss 150.03343200683594\n",
      "47: Encoding Loss 15.843667030334473, Transition Loss 6.400323390960693, Classifier Loss 0.21468305587768555, Total Loss 149.49771118164062\n",
      "47: Encoding Loss 15.250332832336426, Transition Loss 7.293172359466553, Classifier Loss 0.1982051432132721, Total Loss 143.2818145751953\n",
      "47: Encoding Loss 16.396343231201172, Transition Loss 6.960601329803467, Classifier Loss 0.196537047624588, Total Loss 152.21658325195312\n",
      "47: Encoding Loss 16.300622940063477, Transition Loss 7.14534854888916, Classifier Loss 0.20225472748279572, Total Loss 152.05953979492188\n",
      "47: Encoding Loss 15.656444549560547, Transition Loss 7.507471084594727, Classifier Loss 0.2239454984664917, Total Loss 149.14761352539062\n",
      "47: Encoding Loss 15.684760093688965, Transition Loss 5.891652584075928, Classifier Loss 0.20349106192588806, Total Loss 147.00550842285156\n",
      "47: Encoding Loss 14.270052909851074, Transition Loss 10.006986618041992, Classifier Loss 0.2010316252708435, Total Loss 136.26498413085938\n",
      "47: Encoding Loss 15.573198318481445, Transition Loss 5.924699306488037, Classifier Loss 0.2247915416955948, Total Loss 148.2496795654297\n",
      "47: Encoding Loss 15.59180736541748, Transition Loss 4.042147159576416, Classifier Loss 0.20871394872665405, Total Loss 146.41427612304688\n",
      "47: Encoding Loss 15.30608081817627, Transition Loss 6.393172264099121, Classifier Loss 0.197886124253273, Total Loss 143.51589965820312\n",
      "47: Encoding Loss 15.883828163146973, Transition Loss 7.002603054046631, Classifier Loss 0.2050652801990509, Total Loss 148.97769165039062\n",
      "47: Encoding Loss 15.986349105834961, Transition Loss 7.687914848327637, Classifier Loss 0.21504397690296173, Total Loss 150.9327850341797\n",
      "47: Encoding Loss 15.882598876953125, Transition Loss 7.767515659332275, Classifier Loss 0.20366957783699036, Total Loss 148.9812469482422\n",
      "47: Encoding Loss 14.571673393249512, Transition Loss 8.289084434509277, Classifier Loss 0.2253483086824417, Total Loss 140.7660369873047\n",
      "47: Encoding Loss 15.65424633026123, Transition Loss 6.833003044128418, Classifier Loss 0.20204952359199524, Total Loss 146.80552673339844\n",
      "47: Encoding Loss 14.73012924194336, Transition Loss 9.169002532958984, Classifier Loss 0.19778449833393097, Total Loss 139.4532928466797\n",
      "47: Encoding Loss 15.741021156311035, Transition Loss 7.468381404876709, Classifier Loss 0.20642143487930298, Total Loss 148.06399536132812\n",
      "47: Encoding Loss 14.842240333557129, Transition Loss 3.768611431121826, Classifier Loss 0.2230149656534195, Total Loss 141.79315185546875\n",
      "47: Encoding Loss 14.478070259094238, Transition Loss 7.295732498168945, Classifier Loss 0.21809092164039612, Total Loss 139.09280395507812\n",
      "47: Encoding Loss 14.67281723022461, Transition Loss 4.583795070648193, Classifier Loss 0.21781477332115173, Total Loss 140.08078002929688\n",
      "47: Encoding Loss 15.491863250732422, Transition Loss 7.393589496612549, Classifier Loss 0.20279189944267273, Total Loss 145.6928253173828\n",
      "47: Encoding Loss 15.243088722229004, Transition Loss 9.444538116455078, Classifier Loss 0.2174537181854248, Total Loss 145.57899475097656\n",
      "47: Encoding Loss 16.449180603027344, Transition Loss 6.165647029876709, Classifier Loss 0.19975987076759338, Total Loss 152.80255126953125\n",
      "47: Encoding Loss 15.60062026977539, Transition Loss 7.940102577209473, Classifier Loss 0.19987930357456207, Total Loss 146.3809051513672\n",
      "47: Encoding Loss 14.530542373657227, Transition Loss 7.334190845489502, Classifier Loss 0.2379380166530609, Total Loss 141.50498962402344\n",
      "47: Encoding Loss 15.6197509765625, Transition Loss 5.588861465454102, Classifier Loss 0.22160662710666656, Total Loss 148.23643493652344\n",
      "47: Encoding Loss 15.282713890075684, Transition Loss 4.96827507019043, Classifier Loss 0.20374929904937744, Total Loss 143.6302947998047\n",
      "47: Encoding Loss 15.66464614868164, Transition Loss 3.962437868118286, Classifier Loss 0.2075386941432953, Total Loss 146.86351013183594\n",
      "47: Encoding Loss 16.69729995727539, Transition Loss 5.212413787841797, Classifier Loss 0.21325674653053284, Total Loss 155.94654846191406\n",
      "47: Encoding Loss 16.16044807434082, Transition Loss 7.697897911071777, Classifier Loss 0.22095082700252533, Total Loss 152.91824340820312\n",
      "47: Encoding Loss 15.416656494140625, Transition Loss 8.454111099243164, Classifier Loss 0.20809268951416016, Total Loss 145.83334350585938\n",
      "47: Encoding Loss 16.699153900146484, Transition Loss 7.415169715881348, Classifier Loss 0.2195524275302887, Total Loss 157.03152465820312\n",
      "47: Encoding Loss 15.954662322998047, Transition Loss 6.441378116607666, Classifier Loss 0.19692397117614746, Total Loss 148.61798095703125\n",
      "47: Encoding Loss 16.040563583374023, Transition Loss 6.951963424682617, Classifier Loss 0.18556737899780273, Total Loss 148.27163696289062\n",
      "47: Encoding Loss 16.61086082458496, Transition Loss 4.905006408691406, Classifier Loss 0.21386684477329254, Total Loss 155.25457763671875\n",
      "47: Encoding Loss 16.362674713134766, Transition Loss 5.460104942321777, Classifier Loss 0.19987256824970245, Total Loss 151.98068237304688\n",
      "47: Encoding Loss 15.707444190979004, Transition Loss 4.578707218170166, Classifier Loss 0.20165039598941803, Total Loss 146.74034118652344\n",
      "47: Encoding Loss 16.517671585083008, Transition Loss 7.801408290863037, Classifier Loss 0.22011776268482208, Total Loss 155.71343994140625\n",
      "47: Encoding Loss 15.012746810913086, Transition Loss 7.3560686111450195, Classifier Loss 0.20159675180912018, Total Loss 141.7328643798828\n",
      "47: Encoding Loss 14.86605453491211, Transition Loss 6.018136501312256, Classifier Loss 0.2199791669845581, Total Loss 142.12998962402344\n",
      "47: Encoding Loss 15.341818809509277, Transition Loss 5.574538230895996, Classifier Loss 0.2147086262702942, Total Loss 145.32032775878906\n",
      "47: Encoding Loss 14.969732284545898, Transition Loss 6.019142150878906, Classifier Loss 0.191039577126503, Total Loss 140.06564331054688\n",
      "47: Encoding Loss 14.826315879821777, Transition Loss 5.091503620147705, Classifier Loss 0.19022156298160553, Total Loss 138.65098571777344\n",
      "47: Encoding Loss 15.542628288269043, Transition Loss 5.619716167449951, Classifier Loss 0.18530070781707764, Total Loss 143.9950408935547\n",
      "47: Encoding Loss 16.75999641418457, Transition Loss 7.106134414672852, Classifier Loss 0.21638284623622894, Total Loss 157.13949584960938\n",
      "47: Encoding Loss 17.27637481689453, Transition Loss 6.435062885284424, Classifier Loss 0.21326588094234467, Total Loss 160.82460021972656\n",
      "47: Encoding Loss 15.142838478088379, Transition Loss 4.538996696472168, Classifier Loss 0.20533666014671326, Total Loss 142.5841827392578\n",
      "47: Encoding Loss 14.964327812194824, Transition Loss 5.122002124786377, Classifier Loss 0.19203510880470276, Total Loss 139.94253540039062\n",
      "47: Encoding Loss 15.532506942749023, Transition Loss 7.031518459320068, Classifier Loss 0.2118898332118988, Total Loss 146.85536193847656\n",
      "47: Encoding Loss 14.919696807861328, Transition Loss 6.198145866394043, Classifier Loss 0.19311285018920898, Total Loss 139.90847778320312\n",
      "47: Encoding Loss 15.916071891784668, Transition Loss 5.750668525695801, Classifier Loss 0.20598021149635315, Total Loss 149.07672119140625\n",
      "47: Encoding Loss 16.428260803222656, Transition Loss 5.889655590057373, Classifier Loss 0.18032312393188477, Total Loss 150.63633728027344\n",
      "47: Encoding Loss 15.623074531555176, Transition Loss 7.123896598815918, Classifier Loss 0.21336928009986877, Total Loss 147.7462921142578\n",
      "47: Encoding Loss 15.399272918701172, Transition Loss 6.425737380981445, Classifier Loss 0.22713440656661987, Total Loss 147.19276428222656\n",
      "47: Encoding Loss 16.81031608581543, Transition Loss 6.131582736968994, Classifier Loss 0.21781697869300842, Total Loss 157.49053955078125\n",
      "47: Encoding Loss 15.692194938659668, Transition Loss 7.255073547363281, Classifier Loss 0.20784002542495728, Total Loss 147.7725830078125\n",
      "47: Encoding Loss 15.517001152038574, Transition Loss 6.710618495941162, Classifier Loss 0.1913476586341858, Total Loss 144.61289978027344\n",
      "47: Encoding Loss 16.510995864868164, Transition Loss 7.26207971572876, Classifier Loss 0.19475845992565155, Total Loss 153.0162353515625\n",
      "47: Encoding Loss 14.599043846130371, Transition Loss 8.485713005065918, Classifier Loss 0.23799675703048706, Total Loss 142.28916931152344\n",
      "47: Encoding Loss 15.73079776763916, Transition Loss 6.385436534881592, Classifier Loss 0.20085221529006958, Total Loss 147.2086944580078\n",
      "47: Encoding Loss 15.344982147216797, Transition Loss 7.828999042510986, Classifier Loss 0.19582748413085938, Total Loss 143.90841674804688\n",
      "47: Encoding Loss 15.63873291015625, Transition Loss 7.596486568450928, Classifier Loss 0.21371442079544067, Total Loss 148.0006103515625\n",
      "47: Encoding Loss 15.206750869750977, Transition Loss 7.776660442352295, Classifier Loss 0.20896558463573456, Total Loss 144.10589599609375\n",
      "47: Encoding Loss 16.933834075927734, Transition Loss 6.151507377624512, Classifier Loss 0.2056518793106079, Total Loss 157.26617431640625\n",
      "47: Encoding Loss 15.298999786376953, Transition Loss 6.448703289031982, Classifier Loss 0.2067987620830536, Total Loss 144.36160278320312\n",
      "47: Encoding Loss 16.203248977661133, Transition Loss 6.34335994720459, Classifier Loss 0.21825584769248962, Total Loss 152.72024536132812\n",
      "47: Encoding Loss 15.683249473571777, Transition Loss 6.925917625427246, Classifier Loss 0.21106061339378357, Total Loss 147.9572296142578\n",
      "47: Encoding Loss 15.602853775024414, Transition Loss 7.1773362159729, Classifier Loss 0.20614482462406158, Total Loss 146.87278747558594\n",
      "47: Encoding Loss 16.671146392822266, Transition Loss 5.550595283508301, Classifier Loss 0.19373466074466705, Total Loss 153.85276794433594\n",
      "47: Encoding Loss 15.195952415466309, Transition Loss 6.792434215545654, Classifier Loss 0.19375884532928467, Total Loss 142.302001953125\n",
      "47: Encoding Loss 15.564412117004395, Transition Loss 5.136550426483154, Classifier Loss 0.21523861587047577, Total Loss 147.06646728515625\n",
      "47: Encoding Loss 15.454468727111816, Transition Loss 8.734270095825195, Classifier Loss 0.22819897532463074, Total Loss 148.2025146484375\n",
      "47: Encoding Loss 15.115575790405273, Transition Loss 5.378948211669922, Classifier Loss 0.20306804776191711, Total Loss 142.3072052001953\n",
      "47: Encoding Loss 15.491583824157715, Transition Loss 4.997774124145508, Classifier Loss 0.20509053766727448, Total Loss 145.4412841796875\n",
      "47: Encoding Loss 16.09783172607422, Transition Loss 5.950434684753418, Classifier Loss 0.18355880677700043, Total Loss 148.32862854003906\n",
      "47: Encoding Loss 15.799602508544922, Transition Loss 8.429718971252441, Classifier Loss 0.24550767242908478, Total Loss 152.633544921875\n",
      "47: Encoding Loss 15.920101165771484, Transition Loss 7.542216777801514, Classifier Loss 0.2066981941461563, Total Loss 149.53907775878906\n",
      "47: Encoding Loss 16.708520889282227, Transition Loss 6.124692440032959, Classifier Loss 0.19114504754543304, Total Loss 154.0076141357422\n",
      "47: Encoding Loss 16.491567611694336, Transition Loss 6.521512508392334, Classifier Loss 0.1912066787481308, Total Loss 152.35751342773438\n",
      "47: Encoding Loss 15.237951278686523, Transition Loss 4.56913423538208, Classifier Loss 0.20003603398799896, Total Loss 142.82106018066406\n",
      "47: Encoding Loss 15.803498268127441, Transition Loss 7.704082489013672, Classifier Loss 0.22628185153007507, Total Loss 150.5970001220703\n",
      "47: Encoding Loss 16.047176361083984, Transition Loss 5.799630165100098, Classifier Loss 0.22697889804840088, Total Loss 152.23524475097656\n",
      "47: Encoding Loss 15.355090141296387, Transition Loss 8.634368896484375, Classifier Loss 0.22640107572078705, Total Loss 147.20770263671875\n",
      "47: Encoding Loss 16.232332229614258, Transition Loss 4.651392459869385, Classifier Loss 0.2193666249513626, Total Loss 152.72560119628906\n",
      "47: Encoding Loss 15.75841236114502, Transition Loss 5.917458534240723, Classifier Loss 0.20090985298156738, Total Loss 147.34176635742188\n",
      "47: Encoding Loss 16.392091751098633, Transition Loss 6.263426780700684, Classifier Loss 0.23437108099460602, Total Loss 155.82652282714844\n",
      "47: Encoding Loss 15.730497360229492, Transition Loss 8.358261108398438, Classifier Loss 0.2059708833694458, Total Loss 148.1127166748047\n",
      "47: Encoding Loss 14.828278541564941, Transition Loss 7.326864242553711, Classifier Loss 0.19433751702308655, Total Loss 139.52536010742188\n",
      "47: Encoding Loss 15.318406105041504, Transition Loss 6.770141124725342, Classifier Loss 0.19954156875610352, Total Loss 143.85545349121094\n",
      "47: Encoding Loss 14.649259567260742, Transition Loss 9.77153205871582, Classifier Loss 0.19246360659599304, Total Loss 138.39474487304688\n",
      "47: Encoding Loss 15.830973625183105, Transition Loss 6.646665573120117, Classifier Loss 0.20228196680545807, Total Loss 148.205322265625\n",
      "47: Encoding Loss 15.599275588989258, Transition Loss 7.307620525360107, Classifier Loss 0.21061232686042786, Total Loss 147.31695556640625\n",
      "47: Encoding Loss 14.056718826293945, Transition Loss 6.237938404083252, Classifier Loss 0.2069922536611557, Total Loss 134.4005584716797\n",
      "47: Encoding Loss 15.003365516662598, Transition Loss 7.746026992797852, Classifier Loss 0.2052726298570633, Total Loss 142.10340881347656\n",
      "47: Encoding Loss 15.752103805541992, Transition Loss 6.7870097160339355, Classifier Loss 0.207659512758255, Total Loss 148.14019775390625\n",
      "47: Encoding Loss 16.346046447753906, Transition Loss 7.377679347991943, Classifier Loss 0.21682533621788025, Total Loss 153.9264373779297\n",
      "47: Encoding Loss 15.429825782775879, Transition Loss 6.69451379776001, Classifier Loss 0.2089032232761383, Total Loss 145.66783142089844\n",
      "47: Encoding Loss 15.848583221435547, Transition Loss 5.451934337615967, Classifier Loss 0.20173214375972748, Total Loss 148.05227661132812\n",
      "47: Encoding Loss 15.213820457458496, Transition Loss 5.175748348236084, Classifier Loss 0.22329460084438324, Total Loss 145.07518005371094\n",
      "47: Encoding Loss 15.750754356384277, Transition Loss 7.452490329742432, Classifier Loss 0.19495831429958344, Total Loss 146.9923553466797\n",
      "47: Encoding Loss 16.431922912597656, Transition Loss 6.8606038093566895, Classifier Loss 0.2169426679611206, Total Loss 154.5217742919922\n",
      "47: Encoding Loss 15.585579872131348, Transition Loss 6.813259601593018, Classifier Loss 0.21520420908927917, Total Loss 147.56771850585938\n",
      "47: Encoding Loss 15.69261646270752, Transition Loss 6.701622486114502, Classifier Loss 0.20819738507270813, Total Loss 147.7010040283203\n",
      "47: Encoding Loss 15.958929061889648, Transition Loss 6.084136962890625, Classifier Loss 0.21042859554290771, Total Loss 149.93112182617188\n",
      "47: Encoding Loss 16.367935180664062, Transition Loss 4.582839488983154, Classifier Loss 0.21046200394630432, Total Loss 152.90625\n",
      "47: Encoding Loss 15.863574028015137, Transition Loss 4.905045509338379, Classifier Loss 0.2016264647245407, Total Loss 148.05224609375\n",
      "47: Encoding Loss 16.016014099121094, Transition Loss 7.006124496459961, Classifier Loss 0.203706294298172, Total Loss 149.8999786376953\n",
      "47: Encoding Loss 16.796672821044922, Transition Loss 4.758810997009277, Classifier Loss 0.21140053868293762, Total Loss 156.46522521972656\n",
      "47: Encoding Loss 17.18097686767578, Transition Loss 4.929102420806885, Classifier Loss 0.23169463872909546, Total Loss 161.6031036376953\n",
      "47: Encoding Loss 14.890087127685547, Transition Loss 7.260472774505615, Classifier Loss 0.20119264721870422, Total Loss 140.6920623779297\n",
      "47: Encoding Loss 14.742024421691895, Transition Loss 7.382054805755615, Classifier Loss 0.19767281413078308, Total Loss 139.1798858642578\n",
      "47: Encoding Loss 15.107267379760742, Transition Loss 6.785806655883789, Classifier Loss 0.21811288595199585, Total Loss 144.02659606933594\n",
      "47: Encoding Loss 15.185423851013184, Transition Loss 7.503360271453857, Classifier Loss 0.2210279107093811, Total Loss 145.08685302734375\n",
      "47: Encoding Loss 14.97096061706543, Transition Loss 9.670941352844238, Classifier Loss 0.2224562019109726, Total Loss 143.94749450683594\n",
      "47: Encoding Loss 16.64777946472168, Transition Loss 6.111642360687256, Classifier Loss 0.23482342064380646, Total Loss 157.8869171142578\n",
      "47: Encoding Loss 16.56170654296875, Transition Loss 5.7880964279174805, Classifier Loss 0.18046072125434875, Total Loss 151.6973419189453\n",
      "47: Encoding Loss 14.827875137329102, Transition Loss 5.432796478271484, Classifier Loss 0.1933477818965912, Total Loss 139.04434204101562\n",
      "47: Encoding Loss 15.332986831665039, Transition Loss 5.183595657348633, Classifier Loss 0.21586516499519348, Total Loss 145.28712463378906\n",
      "47: Encoding Loss 15.382726669311523, Transition Loss 6.0219902992248535, Classifier Loss 0.206452876329422, Total Loss 144.9114990234375\n",
      "47: Encoding Loss 15.847817420959473, Transition Loss 6.527641296386719, Classifier Loss 0.20185084640979767, Total Loss 148.27316284179688\n",
      "47: Encoding Loss 15.832676887512207, Transition Loss 5.260229110717773, Classifier Loss 0.20346589386463165, Total Loss 148.06005859375\n",
      "47: Encoding Loss 15.851523399353027, Transition Loss 5.817267417907715, Classifier Loss 0.2202060967683792, Total Loss 149.99624633789062\n",
      "47: Encoding Loss 14.918680191040039, Transition Loss 5.08927059173584, Classifier Loss 0.18080398440361023, Total Loss 138.44769287109375\n",
      "47: Encoding Loss 15.758265495300293, Transition Loss 6.589211940765381, Classifier Loss 0.18257322907447815, Total Loss 145.6412811279297\n",
      "47: Encoding Loss 16.081283569335938, Transition Loss 4.6882004737854, Classifier Loss 0.22895830869674683, Total Loss 152.48373413085938\n",
      "47: Encoding Loss 15.119620323181152, Transition Loss 7.331308841705322, Classifier Loss 0.22258657217025757, Total Loss 144.681884765625\n",
      "47: Encoding Loss 15.779511451721191, Transition Loss 5.827739715576172, Classifier Loss 0.21335656940937042, Total Loss 148.7373046875\n",
      "47: Encoding Loss 15.97468090057373, Transition Loss 6.739455223083496, Classifier Loss 0.2224266529083252, Total Loss 151.38800048828125\n",
      "47: Encoding Loss 15.804901123046875, Transition Loss 5.913450241088867, Classifier Loss 0.20367573201656342, Total Loss 147.98947143554688\n",
      "47: Encoding Loss 15.215689659118652, Transition Loss 6.641197204589844, Classifier Loss 0.20348386466503143, Total Loss 143.40213012695312\n",
      "47: Encoding Loss 16.967832565307617, Transition Loss 4.592744827270508, Classifier Loss 0.20230121910572052, Total Loss 156.89132690429688\n",
      "47: Encoding Loss 16.187856674194336, Transition Loss 4.217165946960449, Classifier Loss 0.20893138647079468, Total Loss 151.2394256591797\n",
      "47: Encoding Loss 15.504349708557129, Transition Loss 5.095105171203613, Classifier Loss 0.2041061967611313, Total Loss 145.46444702148438\n",
      "47: Encoding Loss 15.144083976745605, Transition Loss 6.503689765930176, Classifier Loss 0.21940520405769348, Total Loss 144.39393615722656\n",
      "47: Encoding Loss 15.261514663696289, Transition Loss 3.817185878753662, Classifier Loss 0.21284005045890808, Total Loss 144.1395721435547\n",
      "47: Encoding Loss 16.020998001098633, Transition Loss 8.432757377624512, Classifier Loss 0.20895321667194366, Total Loss 150.74986267089844\n",
      "47: Encoding Loss 16.226947784423828, Transition Loss 5.651109218597412, Classifier Loss 0.19900880753993988, Total Loss 150.8466796875\n",
      "47: Encoding Loss 17.11433982849121, Transition Loss 6.348854064941406, Classifier Loss 0.21431545913219452, Total Loss 159.6160430908203\n",
      "47: Encoding Loss 16.627477645874023, Transition Loss 7.247791290283203, Classifier Loss 0.2014322280883789, Total Loss 154.6125946044922\n",
      "47: Encoding Loss 15.316926956176758, Transition Loss 9.23461627960205, Classifier Loss 0.19647426903247833, Total Loss 144.02976989746094\n",
      "47: Encoding Loss 13.08935832977295, Transition Loss 10.302225112915039, Classifier Loss 0.19691145420074463, Total Loss 126.46646118164062\n",
      "48: Encoding Loss 16.8919620513916, Transition Loss 5.085705280303955, Classifier Loss 0.20387955009937286, Total Loss 156.54078674316406\n",
      "48: Encoding Loss 16.25935935974121, Transition Loss 5.771862030029297, Classifier Loss 0.20123043656349182, Total Loss 151.352294921875\n",
      "48: Encoding Loss 16.28530502319336, Transition Loss 5.566941261291504, Classifier Loss 0.2041912078857422, Total Loss 151.81495666503906\n",
      "48: Encoding Loss 15.779112815856934, Transition Loss 6.466042995452881, Classifier Loss 0.21746447682380676, Total Loss 149.27256774902344\n",
      "48: Encoding Loss 17.49664878845215, Transition Loss 5.412375450134277, Classifier Loss 0.2144143283367157, Total Loss 162.49710083007812\n",
      "48: Encoding Loss 14.006929397583008, Transition Loss 6.898987293243408, Classifier Loss 0.21997225284576416, Total Loss 135.4324493408203\n",
      "48: Encoding Loss 16.90965461730957, Transition Loss 5.062519550323486, Classifier Loss 0.2168715000152588, Total Loss 157.9768829345703\n",
      "48: Encoding Loss 17.218679428100586, Transition Loss 6.228718280792236, Classifier Loss 0.23421494662761688, Total Loss 162.4166717529297\n",
      "48: Encoding Loss 15.593131065368652, Transition Loss 5.417621612548828, Classifier Loss 0.25717854499816895, Total Loss 151.5464324951172\n",
      "48: Encoding Loss 16.677263259887695, Transition Loss 5.823802947998047, Classifier Loss 0.21827027201652527, Total Loss 156.40989685058594\n",
      "48: Encoding Loss 16.085926055908203, Transition Loss 4.938239097595215, Classifier Loss 0.1972253918647766, Total Loss 149.3975830078125\n",
      "48: Encoding Loss 15.750345230102539, Transition Loss 5.813906192779541, Classifier Loss 0.2266843467950821, Total Loss 149.833984375\n",
      "48: Encoding Loss 15.5914945602417, Transition Loss 6.217247009277344, Classifier Loss 0.2062576413154602, Total Loss 146.60118103027344\n",
      "48: Encoding Loss 14.950302124023438, Transition Loss 5.590277671813965, Classifier Loss 0.1999320089817047, Total Loss 140.7136688232422\n",
      "48: Encoding Loss 15.051568984985352, Transition Loss 6.4709391593933105, Classifier Loss 0.2087978571653366, Total Loss 142.58653259277344\n",
      "48: Encoding Loss 15.934572219848633, Transition Loss 7.569925785064697, Classifier Loss 0.20554772019386292, Total Loss 149.5453338623047\n",
      "48: Encoding Loss 15.840380668640137, Transition Loss 6.029098987579346, Classifier Loss 0.21040129661560059, Total Loss 148.96900939941406\n",
      "48: Encoding Loss 15.262048721313477, Transition Loss 7.2016754150390625, Classifier Loss 0.1974068433046341, Total Loss 143.27740478515625\n",
      "48: Encoding Loss 16.390949249267578, Transition Loss 6.619216442108154, Classifier Loss 0.19228185713291168, Total Loss 151.6796112060547\n",
      "48: Encoding Loss 16.29281997680664, Transition Loss 7.0843071937561035, Classifier Loss 0.2124176323413849, Total Loss 153.0011749267578\n",
      "48: Encoding Loss 15.64851188659668, Transition Loss 6.875882148742676, Classifier Loss 0.2193334549665451, Total Loss 148.4966278076172\n",
      "48: Encoding Loss 15.67850112915039, Transition Loss 5.716243743896484, Classifier Loss 0.2049647569656372, Total Loss 147.06771850585938\n",
      "48: Encoding Loss 14.252253532409668, Transition Loss 9.34709358215332, Classifier Loss 0.19859634339809418, Total Loss 135.7470703125\n",
      "48: Encoding Loss 15.55125904083252, Transition Loss 5.860186576843262, Classifier Loss 0.2091304361820221, Total Loss 146.4951629638672\n",
      "48: Encoding Loss 15.597908973693848, Transition Loss 3.8220856189727783, Classifier Loss 0.19933359324932098, Total Loss 145.48106384277344\n",
      "48: Encoding Loss 15.310884475708008, Transition Loss 6.433110237121582, Classifier Loss 0.1962779313325882, Total Loss 143.4014892578125\n",
      "48: Encoding Loss 15.868194580078125, Transition Loss 6.580877304077148, Classifier Loss 0.20324929058551788, Total Loss 148.58665466308594\n",
      "48: Encoding Loss 15.988741874694824, Transition Loss 7.649672031402588, Classifier Loss 0.217425137758255, Total Loss 151.18238830566406\n",
      "48: Encoding Loss 15.87780475616455, Transition Loss 7.446267127990723, Classifier Loss 0.19572660326957703, Total Loss 148.0843505859375\n",
      "48: Encoding Loss 14.570505142211914, Transition Loss 8.087563514709473, Classifier Loss 0.2225845754146576, Total Loss 140.44000244140625\n",
      "48: Encoding Loss 15.655278205871582, Transition Loss 6.742246627807617, Classifier Loss 0.20666563510894775, Total Loss 147.25723266601562\n",
      "48: Encoding Loss 14.714798927307129, Transition Loss 9.423023223876953, Classifier Loss 0.19922401010990143, Total Loss 139.52540588378906\n",
      "48: Encoding Loss 15.741756439208984, Transition Loss 7.34709358215332, Classifier Loss 0.21688604354858398, Total Loss 149.0920867919922\n",
      "48: Encoding Loss 14.85929012298584, Transition Loss 3.7077362537384033, Classifier Loss 0.21630370616912842, Total Loss 141.24624633789062\n",
      "48: Encoding Loss 14.490461349487305, Transition Loss 7.290750503540039, Classifier Loss 0.20446288585662842, Total Loss 137.82814025878906\n",
      "48: Encoding Loss 14.670162200927734, Transition Loss 4.747192859649658, Classifier Loss 0.21967777609825134, Total Loss 140.2785186767578\n",
      "48: Encoding Loss 15.483445167541504, Transition Loss 7.34616756439209, Classifier Loss 0.20033545792102814, Total Loss 145.370361328125\n",
      "48: Encoding Loss 15.257553100585938, Transition Loss 9.678394317626953, Classifier Loss 0.22366033494472504, Total Loss 146.3621368408203\n",
      "48: Encoding Loss 16.450748443603516, Transition Loss 6.04939079284668, Classifier Loss 0.2055811583995819, Total Loss 153.37399291992188\n",
      "48: Encoding Loss 15.606400489807129, Transition Loss 7.905832767486572, Classifier Loss 0.19490569829940796, Total Loss 145.92294311523438\n",
      "48: Encoding Loss 14.51757526397705, Transition Loss 7.232569694519043, Classifier Loss 0.23899565637111664, Total Loss 141.48667907714844\n",
      "48: Encoding Loss 15.613179206848145, Transition Loss 5.450094223022461, Classifier Loss 0.21176020801067352, Total Loss 147.1714630126953\n",
      "48: Encoding Loss 15.289040565490723, Transition Loss 4.817174911499023, Classifier Loss 0.197121724486351, Total Loss 142.98794555664062\n",
      "48: Encoding Loss 15.663061141967773, Transition Loss 3.8554787635803223, Classifier Loss 0.20755018293857574, Total Loss 146.8306121826172\n",
      "48: Encoding Loss 16.696914672851562, Transition Loss 5.200117111206055, Classifier Loss 0.21331313252449036, Total Loss 155.9466552734375\n",
      "48: Encoding Loss 16.143741607666016, Transition Loss 7.62033224105835, Classifier Loss 0.2070501744747162, Total Loss 151.37901306152344\n",
      "48: Encoding Loss 15.388297080993652, Transition Loss 8.416491508483887, Classifier Loss 0.2066572606563568, Total Loss 145.4553985595703\n",
      "48: Encoding Loss 16.67662811279297, Transition Loss 7.2952470779418945, Classifier Loss 0.21407800912857056, Total Loss 156.27987670898438\n",
      "48: Encoding Loss 15.957074165344238, Transition Loss 6.377950668334961, Classifier Loss 0.20395712554454803, Total Loss 149.32789611816406\n",
      "48: Encoding Loss 16.04639434814453, Transition Loss 6.678934574127197, Classifier Loss 0.19028706848621368, Total Loss 148.7356414794922\n",
      "48: Encoding Loss 16.619482040405273, Transition Loss 4.751065731048584, Classifier Loss 0.21527060866355896, Total Loss 155.43312072753906\n",
      "48: Encoding Loss 16.365673065185547, Transition Loss 5.349346160888672, Classifier Loss 0.20541098713874817, Total Loss 152.53636169433594\n",
      "48: Encoding Loss 15.680523872375488, Transition Loss 4.5103912353515625, Classifier Loss 0.20222993195056915, Total Loss 146.56927490234375\n",
      "48: Encoding Loss 16.52743148803711, Transition Loss 7.549679279327393, Classifier Loss 0.21115711331367493, Total Loss 154.84510803222656\n",
      "48: Encoding Loss 14.99728012084961, Transition Loss 7.281547546386719, Classifier Loss 0.20103555917739868, Total Loss 141.53811645507812\n",
      "48: Encoding Loss 14.871344566345215, Transition Loss 5.893998146057129, Classifier Loss 0.21958118677139282, Total Loss 142.10768127441406\n",
      "48: Encoding Loss 15.340118408203125, Transition Loss 5.45478630065918, Classifier Loss 0.2115936428308487, Total Loss 144.9712677001953\n",
      "48: Encoding Loss 14.957075119018555, Transition Loss 5.7157301902771, Classifier Loss 0.18859203159809113, Total Loss 139.65895080566406\n",
      "48: Encoding Loss 14.818116188049316, Transition Loss 5.0882954597473145, Classifier Loss 0.19130465388298035, Total Loss 138.69305419921875\n",
      "48: Encoding Loss 15.555543899536133, Transition Loss 5.343878746032715, Classifier Loss 0.186279758810997, Total Loss 144.14109802246094\n",
      "48: Encoding Loss 16.756343841552734, Transition Loss 6.949798107147217, Classifier Loss 0.20956216752529144, Total Loss 156.39694213867188\n",
      "48: Encoding Loss 17.27450180053711, Transition Loss 6.148697376251221, Classifier Loss 0.21887414157390594, Total Loss 161.3131866455078\n",
      "48: Encoding Loss 15.14875316619873, Transition Loss 4.4040656089782715, Classifier Loss 0.21597996354103088, Total Loss 143.66883850097656\n",
      "48: Encoding Loss 14.973454475402832, Transition Loss 4.889172077178955, Classifier Loss 0.19530349969863892, Total Loss 140.2958221435547\n",
      "48: Encoding Loss 15.552664756774902, Transition Loss 6.849542617797852, Classifier Loss 0.2110370248556137, Total Loss 146.89492797851562\n",
      "48: Encoding Loss 14.893511772155762, Transition Loss 6.043388366699219, Classifier Loss 0.19539199769496918, Total Loss 139.89596557617188\n",
      "48: Encoding Loss 15.910320281982422, Transition Loss 5.649587154388428, Classifier Loss 0.2113315463066101, Total Loss 149.54563903808594\n",
      "48: Encoding Loss 16.43950843811035, Transition Loss 5.807873725891113, Classifier Loss 0.180813267827034, Total Loss 150.75897216796875\n",
      "48: Encoding Loss 15.654296875, Transition Loss 6.994329452514648, Classifier Loss 0.21835875511169434, Total Loss 148.4691162109375\n",
      "48: Encoding Loss 15.415338516235352, Transition Loss 6.239256858825684, Classifier Loss 0.21802033483982086, Total Loss 146.37258911132812\n",
      "48: Encoding Loss 16.8275089263916, Transition Loss 5.928772449493408, Classifier Loss 0.2051125019788742, Total Loss 156.31707763671875\n",
      "48: Encoding Loss 15.696499824523926, Transition Loss 7.341002464294434, Classifier Loss 0.19759228825569153, Total Loss 146.79942321777344\n",
      "48: Encoding Loss 15.522374153137207, Transition Loss 6.786882400512695, Classifier Loss 0.18959523737430573, Total Loss 144.4958953857422\n",
      "48: Encoding Loss 16.517864227294922, Transition Loss 7.370900630950928, Classifier Loss 0.20689180493354797, Total Loss 154.30628967285156\n",
      "48: Encoding Loss 14.609161376953125, Transition Loss 8.543841361999512, Classifier Loss 0.2527979612350464, Total Loss 143.8618621826172\n",
      "48: Encoding Loss 15.727581024169922, Transition Loss 6.461435794830322, Classifier Loss 0.2039814591407776, Total Loss 147.51107788085938\n",
      "48: Encoding Loss 15.332916259765625, Transition Loss 7.683857440948486, Classifier Loss 0.1994420737028122, Total Loss 144.14431762695312\n",
      "48: Encoding Loss 15.638663291931152, Transition Loss 7.701141834259033, Classifier Loss 0.20692768692970276, Total Loss 147.34230041503906\n",
      "48: Encoding Loss 15.206002235412598, Transition Loss 7.559532165527344, Classifier Loss 0.20228466391563416, Total Loss 143.38839721679688\n",
      "48: Encoding Loss 16.93389320373535, Transition Loss 6.434170722961426, Classifier Loss 0.21417216956615448, Total Loss 158.17520141601562\n",
      "48: Encoding Loss 15.308247566223145, Transition Loss 6.297502040863037, Classifier Loss 0.20310121774673462, Total Loss 144.03561401367188\n",
      "48: Encoding Loss 16.188039779663086, Transition Loss 6.541996479034424, Classifier Loss 0.2277890145778656, Total Loss 153.59161376953125\n",
      "48: Encoding Loss 15.67298412322998, Transition Loss 6.733578681945801, Classifier Loss 0.2122463881969452, Total Loss 147.95521545410156\n",
      "48: Encoding Loss 15.596419334411621, Transition Loss 7.221591949462891, Classifier Loss 0.2087269425392151, Total Loss 147.08836364746094\n",
      "48: Encoding Loss 16.652645111083984, Transition Loss 5.498435020446777, Classifier Loss 0.20216074585914612, Total Loss 154.5369415283203\n",
      "48: Encoding Loss 15.188749313354492, Transition Loss 6.68256950378418, Classifier Loss 0.19195261597633362, Total Loss 142.04177856445312\n",
      "48: Encoding Loss 15.554560661315918, Transition Loss 5.149853706359863, Classifier Loss 0.21866127848625183, Total Loss 147.33258056640625\n",
      "48: Encoding Loss 15.435392379760742, Transition Loss 8.428349494934082, Classifier Loss 0.2287134975194931, Total Loss 148.0401611328125\n",
      "48: Encoding Loss 15.11772632598877, Transition Loss 5.369289398193359, Classifier Loss 0.2004813551902771, Total Loss 142.0637969970703\n",
      "48: Encoding Loss 15.466776847839355, Transition Loss 4.7587409019470215, Classifier Loss 0.20572271943092346, Total Loss 145.25823974609375\n",
      "48: Encoding Loss 16.089529037475586, Transition Loss 5.886202812194824, Classifier Loss 0.19524849951267242, Total Loss 149.4183349609375\n",
      "48: Encoding Loss 15.753141403198242, Transition Loss 7.989236831665039, Classifier Loss 0.24102994799613953, Total Loss 151.72598266601562\n",
      "48: Encoding Loss 15.914434432983398, Transition Loss 7.560111045837402, Classifier Loss 0.21872450411319733, Total Loss 150.699951171875\n",
      "48: Encoding Loss 16.71303939819336, Transition Loss 5.944399356842041, Classifier Loss 0.18498748540878296, Total Loss 153.39195251464844\n",
      "48: Encoding Loss 16.48524284362793, Transition Loss 6.495725154876709, Classifier Loss 0.19624066352844238, Total Loss 152.80516052246094\n",
      "48: Encoding Loss 15.25250244140625, Transition Loss 4.4491376876831055, Classifier Loss 0.2005194127559662, Total Loss 142.9617919921875\n",
      "48: Encoding Loss 15.802127838134766, Transition Loss 7.631170749664307, Classifier Loss 0.2303747832775116, Total Loss 150.98072814941406\n",
      "48: Encoding Loss 16.05071449279785, Transition Loss 5.685243606567383, Classifier Loss 0.2130770981311798, Total Loss 150.85047912597656\n",
      "48: Encoding Loss 15.358492851257324, Transition Loss 8.418707847595215, Classifier Loss 0.23972094058990479, Total Loss 148.52378845214844\n",
      "48: Encoding Loss 16.22984504699707, Transition Loss 4.486216068267822, Classifier Loss 0.21987265348434448, Total Loss 152.7232666015625\n",
      "48: Encoding Loss 15.744913101196289, Transition Loss 5.804368019104004, Classifier Loss 0.19906458258628845, Total Loss 147.02664184570312\n",
      "48: Encoding Loss 16.405595779418945, Transition Loss 6.116857528686523, Classifier Loss 0.22845007479190826, Total Loss 155.31314086914062\n",
      "48: Encoding Loss 15.730953216552734, Transition Loss 8.320999145507812, Classifier Loss 0.21601223945617676, Total Loss 149.11305236816406\n",
      "48: Encoding Loss 14.80240249633789, Transition Loss 7.128705978393555, Classifier Loss 0.19648778438568115, Total Loss 139.4937286376953\n",
      "48: Encoding Loss 15.331384658813477, Transition Loss 6.74072265625, Classifier Loss 0.202671080827713, Total Loss 144.26632690429688\n",
      "48: Encoding Loss 14.633133888244629, Transition Loss 9.370420455932617, Classifier Loss 0.19374297559261322, Total Loss 138.31346130371094\n",
      "48: Encoding Loss 15.837428092956543, Transition Loss 6.607542991638184, Classifier Loss 0.20836472511291504, Total Loss 148.85739135742188\n",
      "48: Encoding Loss 15.605239868164062, Transition Loss 7.251349449157715, Classifier Loss 0.21197804808616638, Total Loss 147.489990234375\n",
      "48: Encoding Loss 14.062760353088379, Transition Loss 6.0948357582092285, Classifier Loss 0.20386965572834015, Total Loss 134.10801696777344\n",
      "48: Encoding Loss 15.010828971862793, Transition Loss 7.576269626617432, Classifier Loss 0.19516976177692413, Total Loss 141.11886596679688\n",
      "48: Encoding Loss 15.731905937194824, Transition Loss 6.7039594650268555, Classifier Loss 0.2045927792787552, Total Loss 147.6553192138672\n",
      "48: Encoding Loss 16.332983016967773, Transition Loss 7.265588760375977, Classifier Loss 0.21225036680698395, Total Loss 153.34202575683594\n",
      "48: Encoding Loss 15.412965774536133, Transition Loss 6.634892463684082, Classifier Loss 0.20734672248363495, Total Loss 145.36537170410156\n",
      "48: Encoding Loss 15.826945304870605, Transition Loss 5.347649097442627, Classifier Loss 0.19067895412445068, Total Loss 146.75299072265625\n",
      "48: Encoding Loss 15.209485054016113, Transition Loss 5.084207057952881, Classifier Loss 0.2314947247505188, Total Loss 145.84219360351562\n",
      "48: Encoding Loss 15.72917652130127, Transition Loss 7.232504844665527, Classifier Loss 0.1906653493642807, Total Loss 146.34645080566406\n",
      "48: Encoding Loss 16.430265426635742, Transition Loss 6.775412559509277, Classifier Loss 0.2051551192998886, Total Loss 153.31272888183594\n",
      "48: Encoding Loss 15.571222305297852, Transition Loss 6.563053607940674, Classifier Loss 0.2144250124692917, Total Loss 147.32489013671875\n",
      "48: Encoding Loss 15.722210884094238, Transition Loss 6.5753397941589355, Classifier Loss 0.19804003834724426, Total Loss 146.89675903320312\n",
      "48: Encoding Loss 15.944637298583984, Transition Loss 5.901185035705566, Classifier Loss 0.20433077216148376, Total Loss 149.17042541503906\n",
      "48: Encoding Loss 16.378496170043945, Transition Loss 4.572823524475098, Classifier Loss 0.21076172590255737, Total Loss 153.01870727539062\n",
      "48: Encoding Loss 15.876395225524902, Transition Loss 4.856375217437744, Classifier Loss 0.20541706681251526, Total Loss 148.52413940429688\n",
      "48: Encoding Loss 16.00227928161621, Transition Loss 6.981741905212402, Classifier Loss 0.2007012665271759, Total Loss 149.48471069335938\n",
      "48: Encoding Loss 16.807119369506836, Transition Loss 4.634284019470215, Classifier Loss 0.20262274146080017, Total Loss 155.6460723876953\n",
      "48: Encoding Loss 17.186429977416992, Transition Loss 4.820840358734131, Classifier Loss 0.22322715818881989, Total Loss 160.7783203125\n",
      "48: Encoding Loss 14.874860763549805, Transition Loss 6.99923038482666, Classifier Loss 0.207527756690979, Total Loss 141.15150451660156\n",
      "48: Encoding Loss 14.73366928100586, Transition Loss 7.212557792663574, Classifier Loss 0.18689924478530884, Total Loss 138.0017852783203\n",
      "48: Encoding Loss 15.106560707092285, Transition Loss 6.525515079498291, Classifier Loss 0.21912944316864014, Total Loss 144.07054138183594\n",
      "48: Encoding Loss 15.188909530639648, Transition Loss 7.3374104499816895, Classifier Loss 0.2205234318971634, Total Loss 145.03111267089844\n",
      "48: Encoding Loss 14.967995643615723, Transition Loss 9.551254272460938, Classifier Loss 0.20717358589172363, Total Loss 142.37158203125\n",
      "48: Encoding Loss 16.658729553222656, Transition Loss 5.972884654998779, Classifier Loss 0.2133750319480896, Total Loss 155.8019256591797\n",
      "48: Encoding Loss 16.543134689331055, Transition Loss 5.828394889831543, Classifier Loss 0.18325676023960114, Total Loss 151.83642578125\n",
      "48: Encoding Loss 14.805188179016113, Transition Loss 5.455177307128906, Classifier Loss 0.19313745200634003, Total Loss 138.84628295898438\n",
      "48: Encoding Loss 15.339445114135742, Transition Loss 5.140446662902832, Classifier Loss 0.21272799372673035, Total Loss 145.01646423339844\n",
      "48: Encoding Loss 15.39783000946045, Transition Loss 5.9500412940979, Classifier Loss 0.19983842968940735, Total Loss 144.3564910888672\n",
      "48: Encoding Loss 15.841720581054688, Transition Loss 6.565160274505615, Classifier Loss 0.20599010586738586, Total Loss 148.64581298828125\n",
      "48: Encoding Loss 15.818426132202148, Transition Loss 5.182671070098877, Classifier Loss 0.2006607949733734, Total Loss 147.6500244140625\n",
      "48: Encoding Loss 15.853684425354004, Transition Loss 5.90873384475708, Classifier Loss 0.23981328308582306, Total Loss 151.9925537109375\n",
      "48: Encoding Loss 14.95365047454834, Transition Loss 4.856877326965332, Classifier Loss 0.19248124957084656, Total Loss 139.8487091064453\n",
      "48: Encoding Loss 15.776021957397461, Transition Loss 6.612578392028809, Classifier Loss 0.1875712126493454, Total Loss 146.28781127929688\n",
      "48: Encoding Loss 16.081636428833008, Transition Loss 4.405876159667969, Classifier Loss 0.23771102726459503, Total Loss 153.3053741455078\n",
      "48: Encoding Loss 15.119353294372559, Transition Loss 7.537334442138672, Classifier Loss 0.22599977254867554, Total Loss 145.06227111816406\n",
      "48: Encoding Loss 15.779471397399902, Transition Loss 5.568471908569336, Classifier Loss 0.21004927158355713, Total Loss 148.35438537597656\n",
      "48: Encoding Loss 15.956347465515137, Transition Loss 6.883206844329834, Classifier Loss 0.22265972197055817, Total Loss 151.29339599609375\n",
      "48: Encoding Loss 15.805523872375488, Transition Loss 5.694676399230957, Classifier Loss 0.2046402245759964, Total Loss 148.04714965820312\n",
      "48: Encoding Loss 15.216903686523438, Transition Loss 6.481815338134766, Classifier Loss 0.20252901315689087, Total Loss 143.28448486328125\n",
      "48: Encoding Loss 16.951614379882812, Transition Loss 4.479203701019287, Classifier Loss 0.21519947052001953, Total Loss 158.02870178222656\n",
      "48: Encoding Loss 16.175607681274414, Transition Loss 4.104832172393799, Classifier Loss 0.2077629417181015, Total Loss 151.0021209716797\n",
      "48: Encoding Loss 15.509137153625488, Transition Loss 5.160368919372559, Classifier Loss 0.20575329661369324, Total Loss 145.6804962158203\n",
      "48: Encoding Loss 15.160740852355957, Transition Loss 6.435893535614014, Classifier Loss 0.21117003262043, Total Loss 143.6901092529297\n",
      "48: Encoding Loss 15.265270233154297, Transition Loss 4.053360939025879, Classifier Loss 0.21075882017612457, Total Loss 144.00872802734375\n",
      "48: Encoding Loss 16.00430679321289, Transition Loss 8.097793579101562, Classifier Loss 0.220601886510849, Total Loss 151.7141876220703\n",
      "48: Encoding Loss 16.237834930419922, Transition Loss 5.698602676391602, Classifier Loss 0.20768693089485168, Total Loss 151.8111114501953\n",
      "48: Encoding Loss 17.109159469604492, Transition Loss 5.916119575500488, Classifier Loss 0.20852075517177582, Total Loss 158.90858459472656\n",
      "48: Encoding Loss 16.63143539428711, Transition Loss 7.479551792144775, Classifier Loss 0.19816088676452637, Total Loss 154.36349487304688\n",
      "48: Encoding Loss 15.307718276977539, Transition Loss 8.57307243347168, Classifier Loss 0.1969958245754242, Total Loss 143.87594604492188\n",
      "48: Encoding Loss 13.04460334777832, Transition Loss 10.353809356689453, Classifier Loss 0.1955443024635315, Total Loss 125.98201751708984\n",
      "49: Encoding Loss 16.88770294189453, Transition Loss 4.863546371459961, Classifier Loss 0.20279017090797424, Total Loss 156.3533477783203\n",
      "49: Encoding Loss 16.262100219726562, Transition Loss 5.842984676361084, Classifier Loss 0.1989242434501648, Total Loss 151.15782165527344\n",
      "49: Encoding Loss 16.269678115844727, Transition Loss 5.410739898681641, Classifier Loss 0.20645222067832947, Total Loss 151.88479614257812\n",
      "49: Encoding Loss 15.752205848693848, Transition Loss 6.382770538330078, Classifier Loss 0.20910225808620453, Total Loss 148.20443725585938\n",
      "49: Encoding Loss 17.47406005859375, Transition Loss 5.388804912567139, Classifier Loss 0.2128313034772873, Total Loss 162.1533660888672\n",
      "49: Encoding Loss 14.018378257751465, Transition Loss 6.730116367340088, Classifier Loss 0.21219931542873383, Total Loss 134.71298217773438\n",
      "49: Encoding Loss 16.887392044067383, Transition Loss 5.012589454650879, Classifier Loss 0.22225604951381683, Total Loss 158.32725524902344\n",
      "49: Encoding Loss 17.216413497924805, Transition Loss 5.953400611877441, Classifier Loss 0.2368386685848236, Total Loss 162.60585021972656\n",
      "49: Encoding Loss 15.56894302368164, Transition Loss 5.50011682510376, Classifier Loss 0.25073742866516113, Total Loss 150.7252960205078\n",
      "49: Encoding Loss 16.685483932495117, Transition Loss 5.6203742027282715, Classifier Loss 0.21524620056152344, Total Loss 156.13255310058594\n",
      "49: Encoding Loss 16.06706428527832, Transition Loss 5.01875114440918, Classifier Loss 0.20445072650909424, Total Loss 149.98533630371094\n",
      "49: Encoding Loss 15.736282348632812, Transition Loss 5.511045455932617, Classifier Loss 0.22781535983085632, Total Loss 149.7740020751953\n",
      "49: Encoding Loss 15.612024307250977, Transition Loss 6.36764669418335, Classifier Loss 0.21113023161888123, Total Loss 147.28274536132812\n",
      "49: Encoding Loss 14.93004035949707, Transition Loss 5.537449359893799, Classifier Loss 0.1932535022497177, Total Loss 139.87315368652344\n",
      "49: Encoding Loss 15.047178268432617, Transition Loss 6.7333984375, Classifier Loss 0.20675437152385712, Total Loss 142.39955139160156\n",
      "49: Encoding Loss 15.926316261291504, Transition Loss 7.44141149520874, Classifier Loss 0.21196208894252777, Total Loss 150.09503173828125\n",
      "49: Encoding Loss 15.824142456054688, Transition Loss 6.1034836769104, Classifier Loss 0.21144399046897888, Total Loss 148.95823669433594\n",
      "49: Encoding Loss 15.265215873718262, Transition Loss 6.992831707000732, Classifier Loss 0.1986893117427826, Total Loss 143.38922119140625\n",
      "49: Encoding Loss 16.39385414123535, Transition Loss 6.620258808135986, Classifier Loss 0.19477836787700653, Total Loss 151.9527130126953\n",
      "49: Encoding Loss 16.275033950805664, Transition Loss 6.758406639099121, Classifier Loss 0.21311630308628082, Total Loss 152.86358642578125\n",
      "49: Encoding Loss 15.658624649047852, Transition Loss 6.982270240783691, Classifier Loss 0.2118910253047943, Total Loss 147.85455322265625\n",
      "49: Encoding Loss 15.674840927124023, Transition Loss 5.511399745941162, Classifier Loss 0.19519729912281036, Total Loss 146.02073669433594\n",
      "49: Encoding Loss 14.274282455444336, Transition Loss 9.422209739685059, Classifier Loss 0.2053440660238266, Total Loss 136.6131134033203\n",
      "49: Encoding Loss 15.552949905395508, Transition Loss 5.42828893661499, Classifier Loss 0.2111356407403946, Total Loss 146.62283325195312\n",
      "49: Encoding Loss 15.604870796203613, Transition Loss 3.7928836345672607, Classifier Loss 0.19445015490055084, Total Loss 145.0425567626953\n",
      "49: Encoding Loss 15.294833183288574, Transition Loss 5.8400373458862305, Classifier Loss 0.1951090544462204, Total Loss 143.03758239746094\n",
      "49: Encoding Loss 15.869032859802246, Transition Loss 6.702868938446045, Classifier Loss 0.204820916056633, Total Loss 148.77493286132812\n",
      "49: Encoding Loss 15.986956596374512, Transition Loss 7.246857166290283, Classifier Loss 0.2068064957857132, Total Loss 150.02566528320312\n",
      "49: Encoding Loss 15.874105453491211, Transition Loss 7.389523029327393, Classifier Loss 0.20209583640098572, Total Loss 148.6803436279297\n",
      "49: Encoding Loss 14.564804077148438, Transition Loss 7.799020767211914, Classifier Loss 0.2100655436515808, Total Loss 139.0847930908203\n",
      "49: Encoding Loss 15.646028518676758, Transition Loss 6.68658447265625, Classifier Loss 0.19936630129814148, Total Loss 146.44216918945312\n",
      "49: Encoding Loss 14.716287612915039, Transition Loss 8.728093147277832, Classifier Loss 0.19534924626350403, Total Loss 139.01084899902344\n",
      "49: Encoding Loss 15.744147300720215, Transition Loss 7.28082275390625, Classifier Loss 0.19774553179740906, Total Loss 147.18389892578125\n",
      "49: Encoding Loss 14.816497802734375, Transition Loss 3.5796713829040527, Classifier Loss 0.24734671413898468, Total Loss 143.98257446289062\n",
      "49: Encoding Loss 14.478673934936523, Transition Loss 6.915687561035156, Classifier Loss 0.21309907734394073, Total Loss 138.52244567871094\n",
      "49: Encoding Loss 14.660388946533203, Transition Loss 4.400557041168213, Classifier Loss 0.2174793928861618, Total Loss 139.91114807128906\n",
      "49: Encoding Loss 15.467093467712402, Transition Loss 7.2111921310424805, Classifier Loss 0.19921588897705078, Total Loss 145.1005859375\n",
      "49: Encoding Loss 15.260675430297852, Transition Loss 9.098955154418945, Classifier Loss 0.22159302234649658, Total Loss 146.0644989013672\n",
      "49: Encoding Loss 16.44074058532715, Transition Loss 6.006256580352783, Classifier Loss 0.212600439786911, Total Loss 153.98721313476562\n",
      "49: Encoding Loss 15.601202011108398, Transition Loss 7.581436634063721, Classifier Loss 0.19660711288452148, Total Loss 145.9866180419922\n",
      "49: Encoding Loss 14.524747848510742, Transition Loss 7.1759161949157715, Classifier Loss 0.2329520881175995, Total Loss 140.92837524414062\n",
      "49: Encoding Loss 15.627509117126465, Transition Loss 5.149366855621338, Classifier Loss 0.2225298285484314, Total Loss 148.3029327392578\n",
      "49: Encoding Loss 15.265900611877441, Transition Loss 4.654536724090576, Classifier Loss 0.19804775714874268, Total Loss 142.86289978027344\n",
      "49: Encoding Loss 15.666994094848633, Transition Loss 3.770740509033203, Classifier Loss 0.20842348039150238, Total Loss 146.9324493408203\n",
      "49: Encoding Loss 16.699596405029297, Transition Loss 4.939501762390137, Classifier Loss 0.2154030203819275, Total Loss 156.12498474121094\n",
      "49: Encoding Loss 16.142969131469727, Transition Loss 7.386971950531006, Classifier Loss 0.22213798761367798, Total Loss 152.83494567871094\n",
      "49: Encoding Loss 15.400876998901367, Transition Loss 7.889560699462891, Classifier Loss 0.20096567273139954, Total Loss 144.88150024414062\n",
      "49: Encoding Loss 16.669519424438477, Transition Loss 7.140208721160889, Classifier Loss 0.2174617052078247, Total Loss 156.53036499023438\n",
      "49: Encoding Loss 15.943982124328613, Transition Loss 6.093913555145264, Classifier Loss 0.20295941829681396, Total Loss 149.0665740966797\n",
      "49: Encoding Loss 16.039960861206055, Transition Loss 6.666930198669434, Classifier Loss 0.1864910125732422, Total Loss 148.3021697998047\n",
      "49: Encoding Loss 16.594058990478516, Transition Loss 4.584377288818359, Classifier Loss 0.21762342751026154, Total Loss 155.4316864013672\n",
      "49: Encoding Loss 16.344606399536133, Transition Loss 5.3181986808776855, Classifier Loss 0.20430278778076172, Total Loss 152.2507781982422\n",
      "49: Encoding Loss 15.688895225524902, Transition Loss 4.363588809967041, Classifier Loss 0.1964554637670517, Total Loss 146.0294189453125\n",
      "49: Encoding Loss 16.5310115814209, Transition Loss 7.630407333374023, Classifier Loss 0.2226739376783371, Total Loss 156.04156494140625\n",
      "49: Encoding Loss 15.003484725952148, Transition Loss 6.758143424987793, Classifier Loss 0.19629980623722076, Total Loss 141.00949096679688\n",
      "49: Encoding Loss 14.870311737060547, Transition Loss 5.832920074462891, Classifier Loss 0.22318625450134277, Total Loss 142.4477081298828\n",
      "49: Encoding Loss 15.329072952270508, Transition Loss 5.189451217651367, Classifier Loss 0.21089036762714386, Total Loss 144.75950622558594\n",
      "49: Encoding Loss 14.959676742553711, Transition Loss 5.811464309692383, Classifier Loss 0.18958035111427307, Total Loss 139.79774475097656\n",
      "49: Encoding Loss 14.806371688842773, Transition Loss 4.805017948150635, Classifier Loss 0.19416582584381104, Total Loss 138.8285675048828\n",
      "49: Encoding Loss 15.562827110290527, Transition Loss 5.365862846374512, Classifier Loss 0.1899491846561432, Total Loss 144.57069396972656\n",
      "49: Encoding Loss 16.767305374145508, Transition Loss 6.772331237792969, Classifier Loss 0.21393075585365295, Total Loss 156.885986328125\n",
      "49: Encoding Loss 17.288949966430664, Transition Loss 6.098198890686035, Classifier Loss 0.22674573957920074, Total Loss 162.205810546875\n",
      "49: Encoding Loss 15.134761810302734, Transition Loss 4.270878314971924, Classifier Loss 0.2085760533809662, Total Loss 142.7898712158203\n",
      "49: Encoding Loss 14.970820426940918, Transition Loss 4.837260723114014, Classifier Loss 0.19654986262321472, Total Loss 140.38900756835938\n",
      "49: Encoding Loss 15.547442436218262, Transition Loss 6.761017799377441, Classifier Loss 0.21428465843200684, Total Loss 147.16021728515625\n",
      "49: Encoding Loss 14.896852493286133, Transition Loss 5.794323444366455, Classifier Loss 0.1898277848958969, Total Loss 139.3164520263672\n",
      "49: Encoding Loss 15.923009872436523, Transition Loss 5.5175395011901855, Classifier Loss 0.19922824203968048, Total Loss 148.41041564941406\n",
      "49: Encoding Loss 16.40043830871582, Transition Loss 5.595582008361816, Classifier Loss 0.18243244290351868, Total Loss 150.56585693359375\n",
      "49: Encoding Loss 15.650309562683105, Transition Loss 6.882773399353027, Classifier Loss 0.21571721136569977, Total Loss 148.1507568359375\n",
      "49: Encoding Loss 15.413681030273438, Transition Loss 5.982894420623779, Classifier Loss 0.221114844083786, Total Loss 146.6175079345703\n",
      "49: Encoding Loss 16.810468673706055, Transition Loss 6.004086017608643, Classifier Loss 0.20500850677490234, Total Loss 156.1854248046875\n",
      "49: Encoding Loss 15.702202796936035, Transition Loss 6.88209867477417, Classifier Loss 0.19749940931797028, Total Loss 146.74398803710938\n",
      "49: Encoding Loss 15.531825065612793, Transition Loss 6.8474273681640625, Classifier Loss 0.18711206316947937, Total Loss 144.33529663085938\n",
      "49: Encoding Loss 16.497739791870117, Transition Loss 7.016594886779785, Classifier Loss 0.19942083954811096, Total Loss 153.3273162841797\n",
      "49: Encoding Loss 14.589463233947754, Transition Loss 8.427669525146484, Classifier Loss 0.23580752313137054, Total Loss 141.98199462890625\n",
      "49: Encoding Loss 15.73189926147461, Transition Loss 6.284303665161133, Classifier Loss 0.20993375778198242, Total Loss 148.10543823242188\n",
      "49: Encoding Loss 15.337133407592773, Transition Loss 7.8365960121154785, Classifier Loss 0.20161327719688416, Total Loss 144.42572021484375\n",
      "49: Encoding Loss 15.637438774108887, Transition Loss 7.359419822692871, Classifier Loss 0.2077166736125946, Total Loss 147.3430633544922\n",
      "49: Encoding Loss 15.212725639343262, Transition Loss 7.775990009307861, Classifier Loss 0.19822292029857635, Total Loss 143.07928466796875\n",
      "49: Encoding Loss 16.93782615661621, Transition Loss 6.0101399421691895, Classifier Loss 0.20622305572032928, Total Loss 157.32693481445312\n",
      "49: Encoding Loss 15.28736400604248, Transition Loss 6.591502666473389, Classifier Loss 0.20289725065231323, Total Loss 143.9069366455078\n",
      "49: Encoding Loss 16.1898136138916, Transition Loss 6.081920146942139, Classifier Loss 0.22472251951694489, Total Loss 153.20713806152344\n",
      "49: Encoding Loss 15.685286521911621, Transition Loss 7.0301513671875, Classifier Loss 0.21311114728450775, Total Loss 148.19944763183594\n",
      "49: Encoding Loss 15.605724334716797, Transition Loss 6.673333168029785, Classifier Loss 0.19599385559558868, Total Loss 145.7798614501953\n",
      "49: Encoding Loss 16.670639038085938, Transition Loss 5.582873344421387, Classifier Loss 0.2028820812702179, Total Loss 154.7698974609375\n",
      "49: Encoding Loss 15.186910629272461, Transition Loss 6.238014221191406, Classifier Loss 0.1861967146396637, Total Loss 141.36256408691406\n",
      "49: Encoding Loss 15.555388450622559, Transition Loss 5.08062744140625, Classifier Loss 0.2217268943786621, Total Loss 147.63192749023438\n",
      "49: Encoding Loss 15.472993850708008, Transition Loss 7.843446731567383, Classifier Loss 0.2253521829843521, Total Loss 147.8878631591797\n",
      "49: Encoding Loss 15.115489959716797, Transition Loss 5.174818992614746, Classifier Loss 0.1991274356842041, Total Loss 141.8716278076172\n",
      "49: Encoding Loss 15.482783317565918, Transition Loss 4.508642673492432, Classifier Loss 0.20727166533470154, Total Loss 145.4911651611328\n",
      "49: Encoding Loss 16.08894920349121, Transition Loss 5.713845252990723, Classifier Loss 0.18246516585350037, Total Loss 148.10089111328125\n",
      "49: Encoding Loss 15.786076545715332, Transition Loss 7.727540969848633, Classifier Loss 0.24723948538303375, Total Loss 152.5580596923828\n",
      "49: Encoding Loss 15.920059204101562, Transition Loss 6.835560321807861, Classifier Loss 0.2082376927137375, Total Loss 149.5513458251953\n",
      "49: Encoding Loss 16.7056884765625, Transition Loss 5.88823127746582, Classifier Loss 0.19053193926811218, Total Loss 153.8763427734375\n",
      "49: Encoding Loss 16.489721298217773, Transition Loss 6.07012939453125, Classifier Loss 0.19286325573921204, Total Loss 152.41812133789062\n",
      "49: Encoding Loss 15.238232612609863, Transition Loss 4.399253845214844, Classifier Loss 0.20347322523593903, Total Loss 143.13304138183594\n",
      "49: Encoding Loss 15.804426193237305, Transition Loss 7.173128128051758, Classifier Loss 0.22573214769363403, Total Loss 150.44326782226562\n",
      "49: Encoding Loss 16.05292320251465, Transition Loss 5.609292984008789, Classifier Loss 0.2386431097984314, Total Loss 153.4095458984375\n",
      "49: Encoding Loss 15.366595268249512, Transition Loss 8.096942901611328, Classifier Loss 0.21265557408332825, Total Loss 145.8177032470703\n",
      "49: Encoding Loss 16.247739791870117, Transition Loss 4.578808784484863, Classifier Loss 0.20014266669750214, Total Loss 150.9119415283203\n",
      "49: Encoding Loss 15.752203941345215, Transition Loss 5.5546555519104, Classifier Loss 0.19125713407993317, Total Loss 146.2542724609375\n",
      "49: Encoding Loss 16.408344268798828, Transition Loss 6.0644211769104, Classifier Loss 0.2194855958223343, Total Loss 154.42820739746094\n",
      "49: Encoding Loss 15.72259521484375, Transition Loss 8.12114143371582, Classifier Loss 0.20586949586868286, Total Loss 147.99192810058594\n",
      "49: Encoding Loss 14.823444366455078, Transition Loss 7.117342472076416, Classifier Loss 0.19121935963630676, Total Loss 139.13294982910156\n",
      "49: Encoding Loss 15.305102348327637, Transition Loss 6.599945545196533, Classifier Loss 0.19387124478816986, Total Loss 143.14793395996094\n",
      "49: Encoding Loss 14.628849029541016, Transition Loss 9.667844772338867, Classifier Loss 0.1916300356388092, Total Loss 138.12734985351562\n",
      "49: Encoding Loss 15.839692115783691, Transition Loss 6.419432640075684, Classifier Loss 0.20547588169574738, Total Loss 148.5490264892578\n",
      "49: Encoding Loss 15.597973823547363, Transition Loss 7.246005535125732, Classifier Loss 0.20886921882629395, Total Loss 147.1199188232422\n",
      "49: Encoding Loss 14.045869827270508, Transition Loss 5.996303081512451, Classifier Loss 0.21458952128887177, Total Loss 135.02517700195312\n",
      "49: Encoding Loss 15.001078605651855, Transition Loss 8.020902633666992, Classifier Loss 0.1992860734462738, Total Loss 141.5414276123047\n",
      "49: Encoding Loss 15.745841026306152, Transition Loss 6.540337085723877, Classifier Loss 0.20658385753631592, Total Loss 147.93319702148438\n",
      "49: Encoding Loss 16.34559440612793, Transition Loss 7.532835006713867, Classifier Loss 0.2162138819694519, Total Loss 153.8927001953125\n",
      "49: Encoding Loss 15.4110746383667, Transition Loss 6.260377407073975, Classifier Loss 0.21305742859840393, Total Loss 145.84642028808594\n",
      "49: Encoding Loss 15.831439018249512, Transition Loss 5.563859939575195, Classifier Loss 0.1946289837360382, Total Loss 147.22718811035156\n",
      "49: Encoding Loss 15.209716796875, Transition Loss 4.489818572998047, Classifier Loss 0.2268759161233902, Total Loss 145.26329040527344\n",
      "49: Encoding Loss 15.738357543945312, Transition Loss 8.067283630371094, Classifier Loss 0.20091278851032257, Total Loss 147.61160278320312\n",
      "49: Encoding Loss 16.424007415771484, Transition Loss 6.0026655197143555, Classifier Loss 0.2132398784160614, Total Loss 153.91659545898438\n",
      "49: Encoding Loss 15.553194046020508, Transition Loss 6.9248247146606445, Classifier Loss 0.21847595274448395, Total Loss 147.65811157226562\n",
      "49: Encoding Loss 15.702445030212402, Transition Loss 6.36121129989624, Classifier Loss 0.1952587515115738, Total Loss 146.4176788330078\n",
      "49: Encoding Loss 15.935619354248047, Transition Loss 6.060542106628418, Classifier Loss 0.20411445200443268, Total Loss 149.1085205078125\n",
      "49: Encoding Loss 16.356271743774414, Transition Loss 4.520634174346924, Classifier Loss 0.20429545640945435, Total Loss 152.18385314941406\n",
      "49: Encoding Loss 15.861602783203125, Transition Loss 4.8184285163879395, Classifier Loss 0.19319500029087067, Total Loss 147.17601013183594\n",
      "49: Encoding Loss 16.002344131469727, Transition Loss 6.87161922454834, Classifier Loss 0.19983109831809998, Total Loss 149.37619018554688\n",
      "49: Encoding Loss 16.784194946289062, Transition Loss 4.589779376983643, Classifier Loss 0.21537773311138153, Total Loss 156.7292938232422\n",
      "49: Encoding Loss 17.185672760009766, Transition Loss 4.718341827392578, Classifier Loss 0.2270602136850357, Total Loss 161.13507080078125\n",
      "49: Encoding Loss 14.85802936553955, Transition Loss 6.975835800170898, Classifier Loss 0.20156355202198029, Total Loss 140.41575622558594\n",
      "49: Encoding Loss 14.713297843933105, Transition Loss 7.264030933380127, Classifier Loss 0.187647745013237, Total Loss 137.92396545410156\n",
      "49: Encoding Loss 15.124638557434082, Transition Loss 6.653112888336182, Classifier Loss 0.2124820500612259, Total Loss 143.57594299316406\n",
      "49: Encoding Loss 15.162960052490234, Transition Loss 7.326150894165039, Classifier Loss 0.21479888260364532, Total Loss 144.24879455566406\n",
      "49: Encoding Loss 14.96172046661377, Transition Loss 9.511906623840332, Classifier Loss 0.2120964676141739, Total Loss 142.8057861328125\n",
      "49: Encoding Loss 16.63779067993164, Transition Loss 6.032697677612305, Classifier Loss 0.2193644642829895, Total Loss 156.2453155517578\n",
      "49: Encoding Loss 16.5439395904541, Transition Loss 5.763916492462158, Classifier Loss 0.18270789086818695, Total Loss 151.77508544921875\n",
      "49: Encoding Loss 14.81298828125, Transition Loss 5.318894863128662, Classifier Loss 0.18799088895320892, Total Loss 138.3667755126953\n",
      "49: Encoding Loss 15.335925102233887, Transition Loss 5.123294353485107, Classifier Loss 0.20095142722129822, Total Loss 143.8072052001953\n",
      "49: Encoding Loss 15.3833589553833, Transition Loss 5.84433126449585, Classifier Loss 0.21691948175430298, Total Loss 145.9276885986328\n",
      "49: Encoding Loss 15.825133323669434, Transition Loss 6.606914520263672, Classifier Loss 0.20982569456100464, Total Loss 148.90501403808594\n",
      "49: Encoding Loss 15.834685325622559, Transition Loss 5.0211076736450195, Classifier Loss 0.20777477324008942, Total Loss 148.4591827392578\n",
      "49: Encoding Loss 15.867119789123535, Transition Loss 5.925571441650391, Classifier Loss 0.23255793750286102, Total Loss 151.3778839111328\n",
      "49: Encoding Loss 14.929808616638184, Transition Loss 4.847145080566406, Classifier Loss 0.1840367615222931, Total Loss 138.81158447265625\n",
      "49: Encoding Loss 15.749100685119629, Transition Loss 6.72305965423584, Classifier Loss 0.1748557835817337, Total Loss 144.822998046875\n",
      "49: Encoding Loss 16.095417022705078, Transition Loss 4.216085910797119, Classifier Loss 0.21843242645263672, Total Loss 151.44979858398438\n",
      "49: Encoding Loss 15.106115341186523, Transition Loss 7.568077087402344, Classifier Loss 0.22007721662521362, Total Loss 144.37026977539062\n",
      "49: Encoding Loss 15.792071342468262, Transition Loss 5.308160781860352, Classifier Loss 0.20327137410640717, Total Loss 147.725341796875\n",
      "49: Encoding Loss 15.952807426452637, Transition Loss 7.1634392738342285, Classifier Loss 0.2183915078639984, Total Loss 150.89430236816406\n",
      "49: Encoding Loss 15.796271324157715, Transition Loss 5.471455097198486, Classifier Loss 0.20633840560913086, Total Loss 148.09829711914062\n",
      "49: Encoding Loss 15.19725227355957, Transition Loss 6.654443740844727, Classifier Loss 0.20508208870887756, Total Loss 143.4171142578125\n",
      "49: Encoding Loss 16.954425811767578, Transition Loss 4.322724342346191, Classifier Loss 0.20501208305358887, Total Loss 157.00115966796875\n",
      "49: Encoding Loss 16.165996551513672, Transition Loss 4.1489362716674805, Classifier Loss 0.20714803040027618, Total Loss 150.87257385253906\n",
      "49: Encoding Loss 15.508382797241211, Transition Loss 4.911935329437256, Classifier Loss 0.20124119520187378, Total Loss 145.173583984375\n",
      "49: Encoding Loss 15.137475967407227, Transition Loss 6.415801048278809, Classifier Loss 0.2058952897787094, Total Loss 142.9724884033203\n",
      "49: Encoding Loss 15.268407821655273, Transition Loss 3.68796706199646, Classifier Loss 0.21127068996429443, Total Loss 144.01193237304688\n",
      "49: Encoding Loss 16.005531311035156, Transition Loss 8.312294960021973, Classifier Loss 0.22168904542922974, Total Loss 151.8756103515625\n",
      "49: Encoding Loss 16.223886489868164, Transition Loss 5.281466007232666, Classifier Loss 0.20840239524841309, Total Loss 151.6876220703125\n",
      "49: Encoding Loss 17.10407257080078, Transition Loss 6.407085418701172, Classifier Loss 0.2092529982328415, Total Loss 159.03929138183594\n",
      "49: Encoding Loss 16.655305862426758, Transition Loss 6.923438549041748, Classifier Loss 0.1964191198348999, Total Loss 154.26904296875\n",
      "49: Encoding Loss 15.302905082702637, Transition Loss 9.155450820922852, Classifier Loss 0.20231693983078003, Total Loss 144.48602294921875\n",
      "49: Encoding Loss 13.077263832092285, Transition Loss 9.689777374267578, Classifier Loss 0.19504882395267487, Total Loss 126.06095886230469\n",
      "50: Encoding Loss 16.889009475708008, Transition Loss 5.210721015930176, Classifier Loss 0.1987055391073227, Total Loss 156.0247802734375\n",
      "50: Encoding Loss 16.257431030273438, Transition Loss 5.43731164932251, Classifier Loss 0.2002837359905243, Total Loss 151.17529296875\n",
      "50: Encoding Loss 16.274259567260742, Transition Loss 5.5914626121521, Classifier Loss 0.20304140448570251, Total Loss 151.6165008544922\n",
      "50: Encoding Loss 15.762338638305664, Transition Loss 5.9484429359436035, Classifier Loss 0.20542532205581665, Total Loss 147.83091735839844\n",
      "50: Encoding Loss 17.49152374267578, Transition Loss 5.658466339111328, Classifier Loss 0.2256184220314026, Total Loss 163.625732421875\n",
      "50: Encoding Loss 14.01089096069336, Transition Loss 6.240264415740967, Classifier Loss 0.21125243604183197, Total Loss 134.46041870117188\n",
      "50: Encoding Loss 16.90045738220215, Transition Loss 5.015153884887695, Classifier Loss 0.21761611104011536, Total Loss 157.9683074951172\n",
      "50: Encoding Loss 17.212459564208984, Transition Loss 5.6150641441345215, Classifier Loss 0.20943666994571686, Total Loss 159.76637268066406\n",
      "50: Encoding Loss 15.588566780090332, Transition Loss 5.446758270263672, Classifier Loss 0.25067195296287537, Total Loss 150.86508178710938\n",
      "50: Encoding Loss 16.690248489379883, Transition Loss 5.462193965911865, Classifier Loss 0.21634070575237274, Total Loss 156.24850463867188\n",
      "50: Encoding Loss 16.071134567260742, Transition Loss 4.886335849761963, Classifier Loss 0.18986189365386963, Total Loss 148.53253173828125\n",
      "50: Encoding Loss 15.731979370117188, Transition Loss 5.544018268585205, Classifier Loss 0.22814559936523438, Total Loss 149.77920532226562\n",
      "50: Encoding Loss 15.595258712768555, Transition Loss 6.1559977531433105, Classifier Loss 0.2096710503101349, Total Loss 146.96038818359375\n",
      "50: Encoding Loss 14.932785987854004, Transition Loss 5.330661773681641, Classifier Loss 0.19882804155349731, Total Loss 140.41122436523438\n",
      "50: Encoding Loss 15.04165267944336, Transition Loss 6.40833854675293, Classifier Loss 0.21056219935417175, Total Loss 142.67111206054688\n",
      "50: Encoding Loss 15.93870735168457, Transition Loss 7.193904876708984, Classifier Loss 0.21284203231334686, Total Loss 150.23263549804688\n",
      "50: Encoding Loss 15.832408905029297, Transition Loss 5.9691081047058105, Classifier Loss 0.20238351821899414, Total Loss 148.09144592285156\n",
      "50: Encoding Loss 15.267287254333496, Transition Loss 6.821278095245361, Classifier Loss 0.19272050261497498, Total Loss 142.7746124267578\n",
      "50: Encoding Loss 16.382503509521484, Transition Loss 6.63230562210083, Classifier Loss 0.19842958450317383, Total Loss 152.22946166992188\n",
      "50: Encoding Loss 16.28506851196289, Transition Loss 6.786275386810303, Classifier Loss 0.20999568700790405, Total Loss 152.6373748779297\n",
      "50: Encoding Loss 15.653707504272461, Transition Loss 7.022201061248779, Classifier Loss 0.2174883335828781, Total Loss 148.3829345703125\n",
      "50: Encoding Loss 15.679532051086426, Transition Loss 5.344614028930664, Classifier Loss 0.20543339848518372, Total Loss 147.04852294921875\n",
      "50: Encoding Loss 14.246127128601074, Transition Loss 9.36379337310791, Classifier Loss 0.2038705050945282, Total Loss 136.22882080078125\n",
      "50: Encoding Loss 15.546637535095215, Transition Loss 5.439454078674316, Classifier Loss 0.22797971963882446, Total Loss 148.2589569091797\n",
      "50: Encoding Loss 15.586589813232422, Transition Loss 3.7576370239257812, Classifier Loss 0.20062586665153503, Total Loss 145.5068359375\n",
      "50: Encoding Loss 15.291831016540527, Transition Loss 5.871524333953857, Classifier Loss 0.1901271790266037, Total Loss 142.52166748046875\n",
      "50: Encoding Loss 15.87570571899414, Transition Loss 6.7240705490112305, Classifier Loss 0.1970221847295761, Total Loss 148.05267333984375\n",
      "50: Encoding Loss 15.979756355285645, Transition Loss 7.106998443603516, Classifier Loss 0.21100445091724396, Total Loss 150.35989379882812\n",
      "50: Encoding Loss 15.867321968078613, Transition Loss 7.374434471130371, Classifier Loss 0.19390293955802917, Total Loss 147.8037567138672\n",
      "50: Encoding Loss 14.570234298706055, Transition Loss 7.779472351074219, Classifier Loss 0.2138967663049698, Total Loss 139.5074462890625\n",
      "50: Encoding Loss 15.649259567260742, Transition Loss 6.70873498916626, Classifier Loss 0.20239703357219696, Total Loss 146.77554321289062\n",
      "50: Encoding Loss 14.717669486999512, Transition Loss 8.77277946472168, Classifier Loss 0.19504307210445404, Total Loss 139.00022888183594\n",
      "50: Encoding Loss 15.73789119720459, Transition Loss 7.270415306091309, Classifier Loss 0.1997443437576294, Total Loss 147.33164978027344\n",
      "50: Encoding Loss 14.823765754699707, Transition Loss 3.584045886993408, Classifier Loss 0.2151780128479004, Total Loss 140.82473754882812\n",
      "50: Encoding Loss 14.488951683044434, Transition Loss 7.279598712921143, Classifier Loss 0.21394768357276917, Total Loss 138.76229858398438\n",
      "50: Encoding Loss 14.647974967956543, Transition Loss 4.494507789611816, Classifier Loss 0.22794777154922485, Total Loss 140.87747192382812\n",
      "50: Encoding Loss 15.48121452331543, Transition Loss 7.115243434906006, Classifier Loss 0.1996966451406479, Total Loss 145.242431640625\n",
      "50: Encoding Loss 15.254040718078613, Transition Loss 9.470769882202148, Classifier Loss 0.22743819653987885, Total Loss 146.67030334472656\n",
      "50: Encoding Loss 16.448598861694336, Transition Loss 5.641213893890381, Classifier Loss 0.2016967535018921, Total Loss 152.88671875\n",
      "50: Encoding Loss 15.589319229125977, Transition Loss 7.844826698303223, Classifier Loss 0.19846601784229279, Total Loss 146.130126953125\n",
      "50: Encoding Loss 14.525572776794434, Transition Loss 6.5915985107421875, Classifier Loss 0.22800101339817047, Total Loss 140.322998046875\n",
      "50: Encoding Loss 15.61140251159668, Transition Loss 5.61081600189209, Classifier Loss 0.22160235047340393, Total Loss 148.1736297607422\n",
      "50: Encoding Loss 15.278670310974121, Transition Loss 4.340803146362305, Classifier Loss 0.20182140171527863, Total Loss 143.2796630859375\n",
      "50: Encoding Loss 15.643444061279297, Transition Loss 3.7426352500915527, Classifier Loss 0.2088654786348343, Total Loss 146.78262329101562\n",
      "50: Encoding Loss 16.69491195678711, Transition Loss 4.677818298339844, Classifier Loss 0.21565598249435425, Total Loss 156.0604705810547\n",
      "50: Encoding Loss 16.136180877685547, Transition Loss 7.308122158050537, Classifier Loss 0.21762797236442566, Total Loss 152.3138885498047\n",
      "50: Encoding Loss 15.409963607788086, Transition Loss 7.8760666847229, Classifier Loss 0.2048456370830536, Total Loss 145.33949279785156\n",
      "50: Encoding Loss 16.674787521362305, Transition Loss 6.996810436248779, Classifier Loss 0.21619164943695068, Total Loss 156.41683959960938\n",
      "50: Encoding Loss 15.954437255859375, Transition Loss 6.203041076660156, Classifier Loss 0.20198523998260498, Total Loss 149.07461547851562\n",
      "50: Encoding Loss 16.024927139282227, Transition Loss 6.494716644287109, Classifier Loss 0.18368744850158691, Total Loss 147.8671112060547\n",
      "50: Encoding Loss 16.610082626342773, Transition Loss 4.705272674560547, Classifier Loss 0.2086978703737259, Total Loss 154.69149780273438\n",
      "50: Encoding Loss 16.355466842651367, Transition Loss 5.114406585693359, Classifier Loss 0.2039162665605545, Total Loss 152.2582550048828\n",
      "50: Encoding Loss 15.695648193359375, Transition Loss 4.36098575592041, Classifier Loss 0.20643284916877747, Total Loss 147.08065795898438\n",
      "50: Encoding Loss 16.5212459564209, Transition Loss 7.271851539611816, Classifier Loss 0.21386787295341492, Total Loss 155.01113891601562\n",
      "50: Encoding Loss 14.987744331359863, Transition Loss 7.103799819946289, Classifier Loss 0.19296281039714813, Total Loss 140.61900329589844\n",
      "50: Encoding Loss 14.860307693481445, Transition Loss 5.577464580535889, Classifier Loss 0.21466565132141113, Total Loss 141.4645233154297\n",
      "50: Encoding Loss 15.330228805541992, Transition Loss 5.428289413452148, Classifier Loss 0.20301532745361328, Total Loss 144.02903747558594\n",
      "50: Encoding Loss 14.95600700378418, Transition Loss 5.351641654968262, Classifier Loss 0.19036470353603363, Total Loss 139.75486755371094\n",
      "50: Encoding Loss 14.822946548461914, Transition Loss 4.985884666442871, Classifier Loss 0.19554774463176727, Total Loss 139.13552856445312\n",
      "50: Encoding Loss 15.562396049499512, Transition Loss 4.921031951904297, Classifier Loss 0.19007830321788788, Total Loss 144.4912109375\n",
      "50: Encoding Loss 16.73979949951172, Transition Loss 6.719550609588623, Classifier Loss 0.22177404165267944, Total Loss 157.439697265625\n",
      "50: Encoding Loss 17.27909278869629, Transition Loss 5.828368663787842, Classifier Loss 0.2077462524175644, Total Loss 160.1730499267578\n",
      "50: Encoding Loss 15.13843059539795, Transition Loss 4.312382698059082, Classifier Loss 0.21038563549518585, Total Loss 143.00848388671875\n",
      "50: Encoding Loss 14.966119766235352, Transition Loss 4.624795436859131, Classifier Loss 0.20006030797958374, Total Loss 140.65994262695312\n",
      "50: Encoding Loss 15.550888061523438, Transition Loss 6.820387363433838, Classifier Loss 0.20285046100616455, Total Loss 146.0562286376953\n",
      "50: Encoding Loss 14.891027450561523, Transition Loss 5.629367828369141, Classifier Loss 0.19114495813846588, Total Loss 139.36859130859375\n",
      "50: Encoding Loss 15.9259672164917, Transition Loss 5.552917957305908, Classifier Loss 0.20747174322605133, Total Loss 149.26548767089844\n",
      "50: Encoding Loss 16.405925750732422, Transition Loss 5.357565402984619, Classifier Loss 0.18477685749530792, Total Loss 150.796630859375\n",
      "50: Encoding Loss 15.625168800354004, Transition Loss 7.035980224609375, Classifier Loss 0.21194075047969818, Total Loss 147.60263061523438\n",
      "50: Encoding Loss 15.399524688720703, Transition Loss 5.690119743347168, Classifier Loss 0.21838577091693878, Total Loss 146.17279052734375\n",
      "50: Encoding Loss 16.81124496459961, Transition Loss 6.024813175201416, Classifier Loss 0.21505101025104523, Total Loss 157.2000274658203\n",
      "50: Encoding Loss 15.704909324645996, Transition Loss 6.5703511238098145, Classifier Loss 0.1977275013923645, Total Loss 146.72608947753906\n",
      "50: Encoding Loss 15.523835182189941, Transition Loss 6.870532512664795, Classifier Loss 0.19331806898117065, Total Loss 144.89659118652344\n",
      "50: Encoding Loss 16.503658294677734, Transition Loss 6.736888408660889, Classifier Loss 0.19586557149887085, Total Loss 152.96322631835938\n",
      "50: Encoding Loss 14.610472679138184, Transition Loss 8.423810005187988, Classifier Loss 0.25430843234062195, Total Loss 143.9993896484375\n",
      "50: Encoding Loss 15.712909698486328, Transition Loss 5.910821437835693, Classifier Loss 0.2041708379983902, Total Loss 147.30252075195312\n",
      "50: Encoding Loss 15.322827339172363, Transition Loss 7.69102144241333, Classifier Loss 0.19546402990818024, Total Loss 143.667236328125\n",
      "50: Encoding Loss 15.629639625549316, Transition Loss 6.904080390930176, Classifier Loss 0.20578262209892273, Total Loss 146.99620056152344\n",
      "50: Encoding Loss 15.207901000976562, Transition Loss 7.830045700073242, Classifier Loss 0.1979677379131317, Total Loss 143.02598571777344\n",
      "50: Encoding Loss 16.92665672302246, Transition Loss 5.549734115600586, Classifier Loss 0.20017392933368683, Total Loss 156.54058837890625\n",
      "50: Encoding Loss 15.296468734741211, Transition Loss 6.818553924560547, Classifier Loss 0.21029454469680786, Total Loss 144.76492309570312\n",
      "50: Encoding Loss 16.200300216674805, Transition Loss 5.556434154510498, Classifier Loss 0.2235228419303894, Total Loss 153.0659637451172\n",
      "50: Encoding Loss 15.671990394592285, Transition Loss 7.504712104797363, Classifier Loss 0.2030206322669983, Total Loss 147.17893981933594\n",
      "50: Encoding Loss 15.605767250061035, Transition Loss 6.16152286529541, Classifier Loss 0.2134275883436203, Total Loss 147.42120361328125\n",
      "50: Encoding Loss 16.664533615112305, Transition Loss 5.9565019607543945, Classifier Loss 0.2079521268606186, Total Loss 155.3027801513672\n",
      "50: Encoding Loss 15.16767406463623, Transition Loss 5.85468864440918, Classifier Loss 0.19273920357227325, Total Loss 141.7862548828125\n",
      "50: Encoding Loss 15.561123847961426, Transition Loss 5.054101943969727, Classifier Loss 0.22800131142139435, Total Loss 148.29994201660156\n",
      "50: Encoding Loss 15.445273399353027, Transition Loss 7.4172468185424805, Classifier Loss 0.21748583018779755, Total Loss 146.79421997070312\n",
      "50: Encoding Loss 15.111472129821777, Transition Loss 5.348048210144043, Classifier Loss 0.19764643907546997, Total Loss 141.7260284423828\n",
      "50: Encoding Loss 15.489473342895508, Transition Loss 4.205787181854248, Classifier Loss 0.20072071254253387, Total Loss 144.82901000976562\n",
      "50: Encoding Loss 16.09392547607422, Transition Loss 5.914361953735352, Classifier Loss 0.18426965177059174, Total Loss 148.3612518310547\n",
      "50: Encoding Loss 15.767620086669922, Transition Loss 7.348155498504639, Classifier Loss 0.24646475911140442, Total Loss 152.257080078125\n",
      "50: Encoding Loss 15.916189193725586, Transition Loss 7.359206199645996, Classifier Loss 0.20664598047733307, Total Loss 149.46597290039062\n",
      "50: Encoding Loss 16.7015438079834, Transition Loss 5.53759241104126, Classifier Loss 0.19233529269695282, Total Loss 153.95339965820312\n",
      "50: Encoding Loss 16.48116111755371, Transition Loss 6.383094310760498, Classifier Loss 0.1922011524438858, Total Loss 152.34600830078125\n",
      "50: Encoding Loss 15.236597061157227, Transition Loss 4.285127639770508, Classifier Loss 0.20032477378845215, Total Loss 142.78228759765625\n",
      "50: Encoding Loss 15.803261756896973, Transition Loss 7.350358963012695, Classifier Loss 0.2228078544139862, Total Loss 150.1769561767578\n",
      "50: Encoding Loss 16.043798446655273, Transition Loss 5.462553977966309, Classifier Loss 0.2244444191455841, Total Loss 151.88734436035156\n",
      "50: Encoding Loss 15.356610298156738, Transition Loss 8.292271614074707, Classifier Loss 0.21722298860549927, Total Loss 146.23362731933594\n",
      "50: Encoding Loss 16.248836517333984, Transition Loss 4.476200580596924, Classifier Loss 0.2069718837738037, Total Loss 151.5831298828125\n",
      "50: Encoding Loss 15.743200302124023, Transition Loss 5.669919967651367, Classifier Loss 0.2179676592350006, Total Loss 148.87635803222656\n",
      "50: Encoding Loss 16.40107536315918, Transition Loss 5.943987846374512, Classifier Loss 0.2258981466293335, Total Loss 154.98721313476562\n",
      "50: Encoding Loss 15.738181114196777, Transition Loss 7.994342803955078, Classifier Loss 0.21314316987991333, Total Loss 148.81863403320312\n",
      "50: Encoding Loss 14.80699348449707, Transition Loss 6.796988010406494, Classifier Loss 0.18721331655979156, Total Loss 138.5366668701172\n",
      "50: Encoding Loss 15.312931060791016, Transition Loss 6.383153438568115, Classifier Loss 0.19292542338371277, Total Loss 143.07261657714844\n",
      "50: Encoding Loss 14.65081787109375, Transition Loss 9.244637489318848, Classifier Loss 0.18763214349746704, Total Loss 137.8186798095703\n",
      "50: Encoding Loss 15.83010196685791, Transition Loss 6.355652332305908, Classifier Loss 0.20655882358551025, Total Loss 148.56784057617188\n",
      "50: Encoding Loss 15.601911544799805, Transition Loss 6.887669086456299, Classifier Loss 0.20322875678539276, Total Loss 146.51571655273438\n",
      "50: Encoding Loss 14.049033164978027, Transition Loss 5.936198711395264, Classifier Loss 0.21440880000591278, Total Loss 135.0203857421875\n",
      "50: Encoding Loss 14.999988555908203, Transition Loss 7.354747772216797, Classifier Loss 0.2026900351047516, Total Loss 141.73985290527344\n",
      "50: Encoding Loss 15.73746395111084, Transition Loss 6.546126365661621, Classifier Loss 0.21634827554225922, Total Loss 148.84376525878906\n",
      "50: Encoding Loss 16.336652755737305, Transition Loss 6.941817283630371, Classifier Loss 0.2198478728532791, Total Loss 154.06637573242188\n",
      "50: Encoding Loss 15.419068336486816, Transition Loss 6.426112651824951, Classifier Loss 0.20235568284988403, Total Loss 144.8733367919922\n",
      "50: Encoding Loss 15.825498580932617, Transition Loss 5.0367431640625, Classifier Loss 0.19342613220214844, Total Loss 146.95396423339844\n",
      "50: Encoding Loss 15.211511611938477, Transition Loss 4.975695610046387, Classifier Loss 0.221234992146492, Total Loss 144.81072998046875\n",
      "50: Encoding Loss 15.732572555541992, Transition Loss 6.964718341827393, Classifier Loss 0.19127216935157776, Total Loss 146.38075256347656\n",
      "50: Encoding Loss 16.423471450805664, Transition Loss 6.695796489715576, Classifier Loss 0.21350575983524323, Total Loss 154.07749938964844\n",
      "50: Encoding Loss 15.577272415161133, Transition Loss 6.427155494689941, Classifier Loss 0.21256369352340698, Total Loss 147.15997314453125\n",
      "50: Encoding Loss 15.705927848815918, Transition Loss 6.6189351081848145, Classifier Loss 0.1982157826423645, Total Loss 146.79278564453125\n",
      "50: Encoding Loss 15.9384183883667, Transition Loss 5.740575790405273, Classifier Loss 0.2084769606590271, Total Loss 149.50315856933594\n",
      "50: Encoding Loss 16.37355613708496, Transition Loss 4.490481376647949, Classifier Loss 0.20569786429405212, Total Loss 152.45632934570312\n",
      "50: Encoding Loss 15.876336097717285, Transition Loss 4.6193976402282715, Classifier Loss 0.19851937890052795, Total Loss 147.78651428222656\n",
      "50: Encoding Loss 15.995254516601562, Transition Loss 6.911095142364502, Classifier Loss 0.19066816568374634, Total Loss 148.41107177734375\n",
      "50: Encoding Loss 16.792747497558594, Transition Loss 4.468292713165283, Classifier Loss 0.20120719075202942, Total Loss 155.3563690185547\n",
      "50: Encoding Loss 17.17510223388672, Transition Loss 4.684988975524902, Classifier Loss 0.2184804528951645, Total Loss 160.1858673095703\n",
      "50: Encoding Loss 14.866999626159668, Transition Loss 6.817384719848633, Classifier Loss 0.20253141224384308, Total Loss 140.5526123046875\n",
      "50: Encoding Loss 14.73380184173584, Transition Loss 7.138813018798828, Classifier Loss 0.19910813868045807, Total Loss 139.20899963378906\n",
      "50: Encoding Loss 15.125131607055664, Transition Loss 6.449570655822754, Classifier Loss 0.2290641814470291, Total Loss 145.1973876953125\n",
      "50: Encoding Loss 15.170609474182129, Transition Loss 7.0975141525268555, Classifier Loss 0.21545790135860443, Total Loss 144.33018493652344\n",
      "50: Encoding Loss 14.962773323059082, Transition Loss 9.235397338867188, Classifier Loss 0.20088866353034973, Total Loss 141.63812255859375\n",
      "50: Encoding Loss 16.644697189331055, Transition Loss 6.083311080932617, Classifier Loss 0.20567966997623444, Total Loss 154.94219970703125\n",
      "50: Encoding Loss 16.550872802734375, Transition Loss 5.626547336578369, Classifier Loss 0.17861522734165192, Total Loss 151.39381408691406\n",
      "50: Encoding Loss 14.807412147521973, Transition Loss 5.220655918121338, Classifier Loss 0.1905418336391449, Total Loss 138.5576171875\n",
      "50: Encoding Loss 15.342818260192871, Transition Loss 4.95158052444458, Classifier Loss 0.21060408651828766, Total Loss 144.7932586669922\n",
      "50: Encoding Loss 15.392114639282227, Transition Loss 5.8628621101379395, Classifier Loss 0.207126185297966, Total Loss 145.02210998535156\n",
      "50: Encoding Loss 15.825408935546875, Transition Loss 6.2678446769714355, Classifier Loss 0.20628634095191956, Total Loss 148.4854736328125\n",
      "50: Encoding Loss 15.826459884643555, Transition Loss 5.0015869140625, Classifier Loss 0.2045760154724121, Total Loss 148.06961059570312\n",
      "50: Encoding Loss 15.865729331970215, Transition Loss 5.564052104949951, Classifier Loss 0.22892946004867554, Total Loss 150.9315948486328\n",
      "50: Encoding Loss 14.927639961242676, Transition Loss 4.7550368309021, Classifier Loss 0.18516838550567627, Total Loss 138.8889617919922\n",
      "50: Encoding Loss 15.764766693115234, Transition Loss 6.139163970947266, Classifier Loss 0.18414363265037537, Total Loss 145.7603302001953\n",
      "50: Encoding Loss 16.095884323120117, Transition Loss 4.292860507965088, Classifier Loss 0.2320980578660965, Total Loss 152.83544921875\n",
      "50: Encoding Loss 15.09974193572998, Transition Loss 6.9632720947265625, Classifier Loss 0.23563584685325623, Total Loss 145.75418090820312\n",
      "50: Encoding Loss 15.78189754486084, Transition Loss 5.404432773590088, Classifier Loss 0.20919553935527802, Total Loss 148.255615234375\n",
      "50: Encoding Loss 15.946761131286621, Transition Loss 6.387731552124023, Classifier Loss 0.22122140228748322, Total Loss 150.97377014160156\n",
      "50: Encoding Loss 15.811033248901367, Transition Loss 5.5517425537109375, Classifier Loss 0.21139204502105713, Total Loss 148.73782348632812\n",
      "50: Encoding Loss 15.210783958435059, Transition Loss 6.142613410949707, Classifier Loss 0.19832560420036316, Total Loss 142.74734497070312\n",
      "50: Encoding Loss 16.9563045501709, Transition Loss 4.1690449714660645, Classifier Loss 0.20749713480472565, Total Loss 157.2339630126953\n",
      "50: Encoding Loss 16.171993255615234, Transition Loss 3.8080382347106934, Classifier Loss 0.21647146344184875, Total Loss 151.7847137451172\n",
      "50: Encoding Loss 15.509293556213379, Transition Loss 4.713311195373535, Classifier Loss 0.2041356861591339, Total Loss 145.4305877685547\n",
      "50: Encoding Loss 15.13703441619873, Transition Loss 6.047438621520996, Classifier Loss 0.21375760436058044, Total Loss 143.6815185546875\n",
      "50: Encoding Loss 15.271319389343262, Transition Loss 3.611717462539673, Classifier Loss 0.20941463112831116, Total Loss 143.83436584472656\n",
      "50: Encoding Loss 15.996617317199707, Transition Loss 7.80771017074585, Classifier Loss 0.22203956544399261, Total Loss 151.73843383789062\n",
      "50: Encoding Loss 16.221542358398438, Transition Loss 5.296394348144531, Classifier Loss 0.2008255571126938, Total Loss 150.91416931152344\n",
      "50: Encoding Loss 17.117223739624023, Transition Loss 5.828299522399902, Classifier Loss 0.20549920201301575, Total Loss 158.65338134765625\n",
      "50: Encoding Loss 16.638986587524414, Transition Loss 6.892270565032959, Classifier Loss 0.19680693745613098, Total Loss 154.17103576660156\n",
      "50: Encoding Loss 15.306427955627441, Transition Loss 8.48155403137207, Classifier Loss 0.19594597816467285, Total Loss 143.74232482910156\n",
      "50: Encoding Loss 13.055902481079102, Transition Loss 9.9009370803833, Classifier Loss 0.19615775346755981, Total Loss 126.04318237304688\n",
      "51: Encoding Loss 16.88709259033203, Transition Loss 4.769590854644775, Classifier Loss 0.21767576038837433, Total Loss 157.8182373046875\n",
      "51: Encoding Loss 16.265153884887695, Transition Loss 5.6407694816589355, Classifier Loss 0.1995132565498352, Total Loss 151.20071411132812\n",
      "51: Encoding Loss 16.288999557495117, Transition Loss 5.251721382141113, Classifier Loss 0.20566590130329132, Total Loss 151.92892456054688\n",
      "51: Encoding Loss 15.76974868774414, Transition Loss 6.246354579925537, Classifier Loss 0.2104240208864212, Total Loss 148.44964599609375\n",
      "51: Encoding Loss 17.49694061279297, Transition Loss 5.131181240081787, Classifier Loss 0.209398090839386, Total Loss 161.94155883789062\n",
      "51: Encoding Loss 14.006821632385254, Transition Loss 6.64182710647583, Classifier Loss 0.20589661598205566, Total Loss 133.9726104736328\n",
      "51: Encoding Loss 16.897611618041992, Transition Loss 4.814056396484375, Classifier Loss 0.21023987233638763, Total Loss 157.16769409179688\n",
      "51: Encoding Loss 17.199909210205078, Transition Loss 5.950738906860352, Classifier Loss 0.2090911865234375, Total Loss 159.69854736328125\n",
      "51: Encoding Loss 15.594407081604004, Transition Loss 5.298586368560791, Classifier Loss 0.24906349182128906, Total Loss 150.72134399414062\n",
      "51: Encoding Loss 16.676212310791016, Transition Loss 5.676624298095703, Classifier Loss 0.20941218733787537, Total Loss 155.4862518310547\n",
      "51: Encoding Loss 16.06922149658203, Transition Loss 4.805842399597168, Classifier Loss 0.19920119643211365, Total Loss 149.43505859375\n",
      "51: Encoding Loss 15.744882583618164, Transition Loss 5.466330528259277, Classifier Loss 0.23772621154785156, Total Loss 150.824951171875\n",
      "51: Encoding Loss 15.603028297424316, Transition Loss 6.1160993576049805, Classifier Loss 0.21013851463794708, Total Loss 147.06130981445312\n",
      "51: Encoding Loss 14.941925048828125, Transition Loss 5.4016571044921875, Classifier Loss 0.2008196860551834, Total Loss 140.6977081298828\n",
      "51: Encoding Loss 15.050312042236328, Transition Loss 6.40085506439209, Classifier Loss 0.21723084151744843, Total Loss 143.40574645996094\n",
      "51: Encoding Loss 15.930590629577637, Transition Loss 7.173635482788086, Classifier Loss 0.20729365944862366, Total Loss 149.6088104248047\n",
      "51: Encoding Loss 15.8279390335083, Transition Loss 5.8705220222473145, Classifier Loss 0.21258623898029327, Total Loss 149.05624389648438\n",
      "51: Encoding Loss 15.253632545471191, Transition Loss 6.804764270782471, Classifier Loss 0.1950216293334961, Total Loss 142.89218139648438\n",
      "51: Encoding Loss 16.39522361755371, Transition Loss 6.517056941986084, Classifier Loss 0.1971435248851776, Total Loss 152.17955017089844\n",
      "51: Encoding Loss 16.287601470947266, Transition Loss 6.775146484375, Classifier Loss 0.20969857275485992, Total Loss 152.62570190429688\n",
      "51: Encoding Loss 15.652896881103516, Transition Loss 6.8638596534729, Classifier Loss 0.2110515683889389, Total Loss 147.7010955810547\n",
      "51: Encoding Loss 15.687888145446777, Transition Loss 5.425442218780518, Classifier Loss 0.19591063261032104, Total Loss 146.1792449951172\n",
      "51: Encoding Loss 14.269927024841309, Transition Loss 9.457345008850098, Classifier Loss 0.19805416464805603, Total Loss 135.85629272460938\n",
      "51: Encoding Loss 15.55486011505127, Transition Loss 5.531475067138672, Classifier Loss 0.21996554732322693, Total Loss 147.54173278808594\n",
      "51: Encoding Loss 15.588349342346191, Transition Loss 3.712116241455078, Classifier Loss 0.20041030645370483, Total Loss 145.49024963378906\n",
      "51: Encoding Loss 15.290129661560059, Transition Loss 5.845513820648193, Classifier Loss 0.20151889324188232, Total Loss 143.64202880859375\n",
      "51: Encoding Loss 15.871976852416992, Transition Loss 6.36415958404541, Classifier Loss 0.20367078483104706, Total Loss 148.61572265625\n",
      "51: Encoding Loss 15.992941856384277, Transition Loss 7.065438270568848, Classifier Loss 0.20963448286056519, Total Loss 150.320068359375\n",
      "51: Encoding Loss 15.869603157043457, Transition Loss 7.167794227600098, Classifier Loss 0.1959104686975479, Total Loss 147.98143005371094\n",
      "51: Encoding Loss 14.568008422851562, Transition Loss 7.638991355895996, Classifier Loss 0.21011459827423096, Total Loss 139.0833282470703\n",
      "51: Encoding Loss 15.661097526550293, Transition Loss 6.349968433380127, Classifier Loss 0.19901737570762634, Total Loss 146.46051025390625\n",
      "51: Encoding Loss 14.722188949584961, Transition Loss 8.467248916625977, Classifier Loss 0.19097855687141418, Total Loss 138.56883239746094\n",
      "51: Encoding Loss 15.740124702453613, Transition Loss 6.8998823165893555, Classifier Loss 0.19722259044647217, Total Loss 147.02322387695312\n",
      "51: Encoding Loss 14.830782890319824, Transition Loss 3.4476468563079834, Classifier Loss 0.21973907947540283, Total Loss 141.30970764160156\n",
      "51: Encoding Loss 14.48019027709961, Transition Loss 6.7836809158325195, Classifier Loss 0.21638435125350952, Total Loss 138.83670043945312\n",
      "51: Encoding Loss 14.656843185424805, Transition Loss 4.236752033233643, Classifier Loss 0.20891247689723969, Total Loss 138.99334716796875\n",
      "51: Encoding Loss 15.479771614074707, Transition Loss 6.739840507507324, Classifier Loss 0.19856932759284973, Total Loss 145.04306030273438\n",
      "51: Encoding Loss 15.25107192993164, Transition Loss 8.905777931213379, Classifier Loss 0.212452232837677, Total Loss 145.03494262695312\n",
      "51: Encoding Loss 16.445247650146484, Transition Loss 5.6263580322265625, Classifier Loss 0.20643465220928192, Total Loss 153.3307342529297\n",
      "51: Encoding Loss 15.59975814819336, Transition Loss 7.573456287384033, Classifier Loss 0.20040394365787506, Total Loss 146.35316467285156\n",
      "51: Encoding Loss 14.516816139221191, Transition Loss 6.670854091644287, Classifier Loss 0.2255116105079651, Total Loss 140.01986694335938\n",
      "51: Encoding Loss 15.6146879196167, Transition Loss 5.28107213973999, Classifier Loss 0.21756631135940552, Total Loss 147.7303466796875\n",
      "51: Encoding Loss 15.274489402770996, Transition Loss 4.449214935302734, Classifier Loss 0.1936512142419815, Total Loss 142.4508819580078\n",
      "51: Encoding Loss 15.650537490844727, Transition Loss 3.7149465084075928, Classifier Loss 0.21388491988182068, Total Loss 147.33578491210938\n",
      "51: Encoding Loss 16.70471954345703, Transition Loss 4.734718322753906, Classifier Loss 0.213612362742424, Total Loss 155.94593811035156\n",
      "51: Encoding Loss 16.138795852661133, Transition Loss 7.210768699645996, Classifier Loss 0.22383898496627808, Total Loss 152.93641662597656\n",
      "51: Encoding Loss 15.399673461914062, Transition Loss 7.647222995758057, Classifier Loss 0.20244909822940826, Total Loss 144.97174072265625\n",
      "51: Encoding Loss 16.669448852539062, Transition Loss 6.920526504516602, Classifier Loss 0.20635473728179932, Total Loss 155.3751678466797\n",
      "51: Encoding Loss 15.95609188079834, Transition Loss 6.110838890075684, Classifier Loss 0.1946985423564911, Total Loss 148.34075927734375\n",
      "51: Encoding Loss 16.039730072021484, Transition Loss 6.53818941116333, Classifier Loss 0.18419773876667023, Total Loss 148.04525756835938\n",
      "51: Encoding Loss 16.598520278930664, Transition Loss 4.595792293548584, Classifier Loss 0.2001722753047943, Total Loss 153.72454833984375\n",
      "51: Encoding Loss 16.346675872802734, Transition Loss 5.091402530670166, Classifier Loss 0.1950182020664215, Total Loss 151.29351806640625\n",
      "51: Encoding Loss 15.689533233642578, Transition Loss 4.320796012878418, Classifier Loss 0.1987752467393875, Total Loss 146.25794982910156\n",
      "51: Encoding Loss 16.527973175048828, Transition Loss 7.344972610473633, Classifier Loss 0.21701940894126892, Total Loss 155.39471435546875\n",
      "51: Encoding Loss 14.9996337890625, Transition Loss 6.957535743713379, Classifier Loss 0.1948065459728241, Total Loss 140.86923217773438\n",
      "51: Encoding Loss 14.880636215209961, Transition Loss 5.591989517211914, Classifier Loss 0.22091367840766907, Total Loss 142.25485229492188\n",
      "51: Encoding Loss 15.33646297454834, Transition Loss 5.168631553649902, Classifier Loss 0.21207879483699799, Total Loss 144.9333038330078\n",
      "51: Encoding Loss 14.96099853515625, Transition Loss 5.407644748687744, Classifier Loss 0.19276206195354462, Total Loss 140.0457305908203\n",
      "51: Encoding Loss 14.813250541687012, Transition Loss 4.8858442306518555, Classifier Loss 0.19226203858852386, Total Loss 138.70938110351562\n",
      "51: Encoding Loss 15.539091110229492, Transition Loss 5.072943687438965, Classifier Loss 0.19248364865779877, Total Loss 144.57568359375\n",
      "51: Encoding Loss 16.74600601196289, Transition Loss 6.581129550933838, Classifier Loss 0.21018782258033752, Total Loss 156.3030548095703\n",
      "51: Encoding Loss 17.28143882751465, Transition Loss 5.798266410827637, Classifier Loss 0.21348807215690613, Total Loss 160.7599639892578\n",
      "51: Encoding Loss 15.13180160522461, Transition Loss 4.258519172668457, Classifier Loss 0.21565499901771545, Total Loss 143.47161865234375\n",
      "51: Encoding Loss 14.967777252197266, Transition Loss 4.747450351715088, Classifier Loss 0.19221973419189453, Total Loss 139.91368103027344\n",
      "51: Encoding Loss 15.549652099609375, Transition Loss 6.75428581237793, Classifier Loss 0.20526620745658875, Total Loss 146.2747039794922\n",
      "51: Encoding Loss 14.894883155822754, Transition Loss 5.7189154624938965, Classifier Loss 0.19927692413330078, Total Loss 140.2305450439453\n",
      "51: Encoding Loss 15.910154342651367, Transition Loss 5.414542198181152, Classifier Loss 0.19991128146648407, Total Loss 148.3552703857422\n",
      "51: Encoding Loss 16.406593322753906, Transition Loss 5.436652660369873, Classifier Loss 0.1871478259563446, Total Loss 151.0548553466797\n",
      "51: Encoding Loss 15.640753746032715, Transition Loss 6.934769630432129, Classifier Loss 0.20960402488708496, Total Loss 147.47337341308594\n",
      "51: Encoding Loss 15.413558006286621, Transition Loss 5.850969314575195, Classifier Loss 0.22111889719963074, Total Loss 146.59054565429688\n",
      "51: Encoding Loss 16.80200958251953, Transition Loss 5.8616228103637695, Classifier Loss 0.20639409124851227, Total Loss 156.22779846191406\n",
      "51: Encoding Loss 15.700121879577637, Transition Loss 6.659730434417725, Classifier Loss 0.20202142000198364, Total Loss 147.1350555419922\n",
      "51: Encoding Loss 15.516586303710938, Transition Loss 6.638965606689453, Classifier Loss 0.191890686750412, Total Loss 144.64955139160156\n",
      "51: Encoding Loss 16.512046813964844, Transition Loss 6.801027774810791, Classifier Loss 0.19038358330726624, Total Loss 152.49493408203125\n",
      "51: Encoding Loss 14.591233253479004, Transition Loss 8.289215087890625, Classifier Loss 0.25122103095054626, Total Loss 143.5098114013672\n",
      "51: Encoding Loss 15.712180137634277, Transition Loss 5.952435493469238, Classifier Loss 0.20282870531082153, Total Loss 147.17080688476562\n",
      "51: Encoding Loss 15.331341743469238, Transition Loss 7.5751471519470215, Classifier Loss 0.1935577690601349, Total Loss 143.52154541015625\n",
      "51: Encoding Loss 15.639415740966797, Transition Loss 7.041140079498291, Classifier Loss 0.20838162302970886, Total Loss 147.36172485351562\n",
      "51: Encoding Loss 15.213855743408203, Transition Loss 7.582720756530762, Classifier Loss 0.20501819252967834, Total Loss 143.7292022705078\n",
      "51: Encoding Loss 16.921749114990234, Transition Loss 5.6068434715271, Classifier Loss 0.21310806274414062, Total Loss 157.80618286132812\n",
      "51: Encoding Loss 15.2896146774292, Transition Loss 6.582111835479736, Classifier Loss 0.21450836956501007, Total Loss 145.0841827392578\n",
      "51: Encoding Loss 16.193674087524414, Transition Loss 5.544050693511963, Classifier Loss 0.22199197113513947, Total Loss 152.85740661621094\n",
      "51: Encoding Loss 15.682994842529297, Transition Loss 7.283961772918701, Classifier Loss 0.22475109994411469, Total Loss 149.39585876464844\n",
      "51: Encoding Loss 15.605671882629395, Transition Loss 5.958974361419678, Classifier Loss 0.2083093374967575, Total Loss 146.86810302734375\n",
      "51: Encoding Loss 16.674304962158203, Transition Loss 5.991426467895508, Classifier Loss 0.19577474892139435, Total Loss 154.17019653320312\n",
      "51: Encoding Loss 15.18033504486084, Transition Loss 5.756688117980957, Classifier Loss 0.18710963428020477, Total Loss 141.3049774169922\n",
      "51: Encoding Loss 15.553544998168945, Transition Loss 5.428420543670654, Classifier Loss 0.22635851800441742, Total Loss 148.14988708496094\n",
      "51: Encoding Loss 15.444035530090332, Transition Loss 7.114579677581787, Classifier Loss 0.22849342226982117, Total Loss 147.8245391845703\n",
      "51: Encoding Loss 15.105513572692871, Transition Loss 5.658298492431641, Classifier Loss 0.20062443614006042, Total Loss 142.0382080078125\n",
      "51: Encoding Loss 15.473956108093262, Transition Loss 4.104706764221191, Classifier Loss 0.20578888058662415, Total Loss 145.1914825439453\n",
      "51: Encoding Loss 16.094932556152344, Transition Loss 6.115022659301758, Classifier Loss 0.18850693106651306, Total Loss 148.83316040039062\n",
      "51: Encoding Loss 15.764252662658691, Transition Loss 7.187935829162598, Classifier Loss 0.24597711861133575, Total Loss 152.1493377685547\n",
      "51: Encoding Loss 15.925633430480957, Transition Loss 7.301381587982178, Classifier Loss 0.20831900835037231, Total Loss 149.69725036621094\n",
      "51: Encoding Loss 16.705013275146484, Transition Loss 5.36871862411499, Classifier Loss 0.18858923017978668, Total Loss 153.57278442382812\n",
      "51: Encoding Loss 16.492982864379883, Transition Loss 6.227268218994141, Classifier Loss 0.19057998061180115, Total Loss 152.247314453125\n",
      "51: Encoding Loss 15.23374080657959, Transition Loss 4.160443305969238, Classifier Loss 0.20014819502830505, Total Loss 142.7168426513672\n",
      "51: Encoding Loss 15.799530029296875, Transition Loss 7.235224723815918, Classifier Loss 0.23004667460918427, Total Loss 150.84796142578125\n",
      "51: Encoding Loss 16.04396629333496, Transition Loss 5.341277122497559, Classifier Loss 0.2218371033668518, Total Loss 151.60369873046875\n",
      "51: Encoding Loss 15.354151725769043, Transition Loss 8.202773094177246, Classifier Loss 0.215485617518425, Total Loss 146.02232360839844\n",
      "51: Encoding Loss 16.25537872314453, Transition Loss 4.433587074279785, Classifier Loss 0.20904715359210968, Total Loss 151.83445739746094\n",
      "51: Encoding Loss 15.741251945495605, Transition Loss 5.576072692871094, Classifier Loss 0.19969794154167175, Total Loss 147.01502990722656\n",
      "51: Encoding Loss 16.409223556518555, Transition Loss 6.017363548278809, Classifier Loss 0.22268721461296082, Total Loss 154.74598693847656\n",
      "51: Encoding Loss 15.729508399963379, Transition Loss 7.993686199188232, Classifier Loss 0.20827263593673706, Total Loss 148.26206970214844\n",
      "51: Encoding Loss 14.799176216125488, Transition Loss 6.867619514465332, Classifier Loss 0.18902158737182617, Total Loss 138.66908264160156\n",
      "51: Encoding Loss 15.300032615661621, Transition Loss 6.467367649078369, Classifier Loss 0.19539925456047058, Total Loss 143.23365783691406\n",
      "51: Encoding Loss 14.634721755981445, Transition Loss 9.19783878326416, Classifier Loss 0.19499117136001587, Total Loss 138.4164581298828\n",
      "51: Encoding Loss 15.837160110473633, Transition Loss 6.352166175842285, Classifier Loss 0.20564602315425873, Total Loss 148.53231811523438\n",
      "51: Encoding Loss 15.591561317443848, Transition Loss 6.89650821685791, Classifier Loss 0.2094878852367401, Total Loss 147.0605926513672\n",
      "51: Encoding Loss 14.041865348815918, Transition Loss 5.885726451873779, Classifier Loss 0.2143295705318451, Total Loss 134.9450225830078\n",
      "51: Encoding Loss 14.996479988098145, Transition Loss 7.172604560852051, Classifier Loss 0.1975417584180832, Total Loss 141.16053771972656\n",
      "51: Encoding Loss 15.744922637939453, Transition Loss 6.429845809936523, Classifier Loss 0.20149953663349152, Total Loss 147.39529418945312\n",
      "51: Encoding Loss 16.323984146118164, Transition Loss 6.988071441650391, Classifier Loss 0.2102278172969818, Total Loss 153.01226806640625\n",
      "51: Encoding Loss 15.4148588180542, Transition Loss 6.326680660247803, Classifier Loss 0.1987559050321579, Total Loss 144.4597930908203\n",
      "51: Encoding Loss 15.826457023620605, Transition Loss 5.0973687171936035, Classifier Loss 0.1919543445110321, Total Loss 146.82656860351562\n",
      "51: Encoding Loss 15.199300765991211, Transition Loss 4.923462390899658, Classifier Loss 0.22895511984825134, Total Loss 145.47462463378906\n",
      "51: Encoding Loss 15.73496150970459, Transition Loss 6.793496131896973, Classifier Loss 0.19441160559654236, Total Loss 146.67955017089844\n",
      "51: Encoding Loss 16.422916412353516, Transition Loss 6.387096405029297, Classifier Loss 0.20998746156692505, Total Loss 153.6595001220703\n",
      "51: Encoding Loss 15.573826789855957, Transition Loss 6.230721473693848, Classifier Loss 0.21297122538089752, Total Loss 147.13388061523438\n",
      "51: Encoding Loss 15.705755233764648, Transition Loss 6.260567665100098, Classifier Loss 0.20678843557834625, Total Loss 147.57701110839844\n",
      "51: Encoding Loss 15.937505722045898, Transition Loss 5.6139750480651855, Classifier Loss 0.21717464923858643, Total Loss 150.34030151367188\n",
      "51: Encoding Loss 16.370222091674805, Transition Loss 4.213846206665039, Classifier Loss 0.21083632111549377, Total Loss 152.88818359375\n",
      "51: Encoding Loss 15.864850044250488, Transition Loss 4.4947052001953125, Classifier Loss 0.20775538682937622, Total Loss 148.59327697753906\n",
      "51: Encoding Loss 15.990532875061035, Transition Loss 6.563319683074951, Classifier Loss 0.21475431323051453, Total Loss 150.71237182617188\n",
      "51: Encoding Loss 16.810964584350586, Transition Loss 4.406130313873291, Classifier Loss 0.20451664924621582, Total Loss 155.8206024169922\n",
      "51: Encoding Loss 17.194522857666016, Transition Loss 4.702923774719238, Classifier Loss 0.2273239940404892, Total Loss 161.22915649414062\n",
      "51: Encoding Loss 14.882054328918457, Transition Loss 6.706923961639404, Classifier Loss 0.199539452791214, Total Loss 140.3517608642578\n",
      "51: Encoding Loss 14.73016357421875, Transition Loss 6.840163230895996, Classifier Loss 0.18968577682971954, Total Loss 138.17791748046875\n",
      "51: Encoding Loss 15.114703178405762, Transition Loss 6.231320858001709, Classifier Loss 0.21085724234580994, Total Loss 143.24961853027344\n",
      "51: Encoding Loss 15.161832809448242, Transition Loss 7.009851932525635, Classifier Loss 0.21717709302902222, Total Loss 144.4143524169922\n",
      "51: Encoding Loss 14.979333877563477, Transition Loss 9.264485359191895, Classifier Loss 0.20308199524879456, Total Loss 141.99575805664062\n",
      "51: Encoding Loss 16.645427703857422, Transition Loss 5.957843780517578, Classifier Loss 0.2033926546573639, Total Loss 154.69427490234375\n",
      "51: Encoding Loss 16.562143325805664, Transition Loss 5.669185161590576, Classifier Loss 0.18047240376472473, Total Loss 151.67822265625\n",
      "51: Encoding Loss 14.803633689880371, Transition Loss 5.251179218292236, Classifier Loss 0.19588209688663483, Total Loss 139.0675048828125\n",
      "51: Encoding Loss 15.334979057312012, Transition Loss 5.004360675811768, Classifier Loss 0.20840692520141602, Total Loss 144.52139282226562\n",
      "51: Encoding Loss 15.37637710571289, Transition Loss 5.7981719970703125, Classifier Loss 0.19477006793022156, Total Loss 143.6476593017578\n",
      "51: Encoding Loss 15.824491500854492, Transition Loss 6.405608177185059, Classifier Loss 0.19514092803001404, Total Loss 147.3911590576172\n",
      "51: Encoding Loss 15.821549415588379, Transition Loss 4.954628944396973, Classifier Loss 0.19695089757442474, Total Loss 147.25840759277344\n",
      "51: Encoding Loss 15.87304401397705, Transition Loss 5.685660362243652, Classifier Loss 0.21625235676765442, Total Loss 149.74671936035156\n",
      "51: Encoding Loss 14.935730934143066, Transition Loss 4.74494743347168, Classifier Loss 0.1801946759223938, Total Loss 138.45431518554688\n",
      "51: Encoding Loss 15.758846282958984, Transition Loss 6.459561347961426, Classifier Loss 0.18233120441436768, Total Loss 145.59580993652344\n",
      "51: Encoding Loss 16.069120407104492, Transition Loss 4.24885892868042, Classifier Loss 0.22879019379615784, Total Loss 152.28175354003906\n",
      "51: Encoding Loss 15.081507682800293, Transition Loss 7.207845687866211, Classifier Loss 0.2172965109348297, Total Loss 143.8232879638672\n",
      "51: Encoding Loss 15.78528881072998, Transition Loss 5.329995632171631, Classifier Loss 0.21339304745197296, Total Loss 148.68760681152344\n",
      "51: Encoding Loss 15.954171180725098, Transition Loss 6.600042343139648, Classifier Loss 0.2328551709651947, Total Loss 152.23890686035156\n",
      "51: Encoding Loss 15.801891326904297, Transition Loss 5.495794773101807, Classifier Loss 0.1971069574356079, Total Loss 147.22499084472656\n",
      "51: Encoding Loss 15.204800605773926, Transition Loss 6.196651458740234, Classifier Loss 0.1928325742483139, Total Loss 142.16099548339844\n",
      "51: Encoding Loss 16.970199584960938, Transition Loss 4.258106231689453, Classifier Loss 0.20638473331928253, Total Loss 157.25169372558594\n",
      "51: Encoding Loss 16.192628860473633, Transition Loss 3.8316469192504883, Classifier Loss 0.21285541355609894, Total Loss 151.5928955078125\n",
      "51: Encoding Loss 15.501669883728027, Transition Loss 4.879678726196289, Classifier Loss 0.19540227949619293, Total Loss 144.52952575683594\n",
      "51: Encoding Loss 15.142976760864258, Transition Loss 6.038337230682373, Classifier Loss 0.2113325595855713, Total Loss 143.4847412109375\n",
      "51: Encoding Loss 15.273944854736328, Transition Loss 3.6897244453430176, Classifier Loss 0.21162474155426025, Total Loss 144.0919647216797\n",
      "51: Encoding Loss 15.990169525146484, Transition Loss 7.567661285400391, Classifier Loss 0.21590320765972137, Total Loss 151.0252227783203\n",
      "51: Encoding Loss 16.226041793823242, Transition Loss 5.448834419250488, Classifier Loss 0.1938035786151886, Total Loss 150.27845764160156\n",
      "51: Encoding Loss 17.097625732421875, Transition Loss 5.670325756072998, Classifier Loss 0.20958136022090912, Total Loss 158.87319946289062\n",
      "51: Encoding Loss 16.635356903076172, Transition Loss 7.153270244598389, Classifier Loss 0.19621561467647552, Total Loss 154.1350860595703\n",
      "51: Encoding Loss 15.289484977722168, Transition Loss 8.38296890258789, Classifier Loss 0.19966527819633484, Total Loss 143.95899963378906\n",
      "51: Encoding Loss 13.071663856506348, Transition Loss 10.086216926574707, Classifier Loss 0.19105994701385498, Total Loss 125.6965560913086\n",
      "52: Encoding Loss 16.878843307495117, Transition Loss 4.710687637329102, Classifier Loss 0.20623594522476196, Total Loss 156.5964813232422\n",
      "52: Encoding Loss 16.26831817626953, Transition Loss 5.6156134605407715, Classifier Loss 0.19630548357963562, Total Loss 150.9002227783203\n",
      "52: Encoding Loss 16.283641815185547, Transition Loss 5.151350021362305, Classifier Loss 0.20749564468860626, Total Loss 152.04898071289062\n",
      "52: Encoding Loss 15.776677131652832, Transition Loss 6.077381134033203, Classifier Loss 0.20864756405353546, Total Loss 148.29364013671875\n",
      "52: Encoding Loss 17.484148025512695, Transition Loss 5.164393901824951, Classifier Loss 0.21197830140590668, Total Loss 162.10389709472656\n",
      "52: Encoding Loss 14.016892433166504, Transition Loss 6.693145751953125, Classifier Loss 0.20808115601539612, Total Loss 134.28189086914062\n",
      "52: Encoding Loss 16.893314361572266, Transition Loss 4.850112438201904, Classifier Loss 0.21109488606452942, Total Loss 157.2260284423828\n",
      "52: Encoding Loss 17.218708038330078, Transition Loss 5.668252944946289, Classifier Loss 0.22191224992275238, Total Loss 161.0745391845703\n",
      "52: Encoding Loss 15.593862533569336, Transition Loss 5.281436443328857, Classifier Loss 0.26210445165634155, Total Loss 152.01763916015625\n",
      "52: Encoding Loss 16.699026107788086, Transition Loss 5.332535743713379, Classifier Loss 0.21292561292648315, Total Loss 155.95127868652344\n",
      "52: Encoding Loss 16.08151626586914, Transition Loss 4.688042640686035, Classifier Loss 0.1933300495147705, Total Loss 148.92274475097656\n",
      "52: Encoding Loss 15.747350692749023, Transition Loss 5.334913730621338, Classifier Loss 0.22691623866558075, Total Loss 149.7374267578125\n",
      "52: Encoding Loss 15.62307071685791, Transition Loss 6.100217342376709, Classifier Loss 0.20408585667610168, Total Loss 146.6132049560547\n",
      "52: Encoding Loss 14.938302993774414, Transition Loss 5.289866924285889, Classifier Loss 0.1977686733007431, Total Loss 140.3412628173828\n",
      "52: Encoding Loss 15.062252044677734, Transition Loss 6.279975414276123, Classifier Loss 0.22444327175617218, Total Loss 144.19834899902344\n",
      "52: Encoding Loss 15.93938159942627, Transition Loss 7.050530433654785, Classifier Loss 0.2144976556301117, Total Loss 150.3749237060547\n",
      "52: Encoding Loss 15.82957649230957, Transition Loss 5.7564215660095215, Classifier Loss 0.2057909518480301, Total Loss 148.36700439453125\n",
      "52: Encoding Loss 15.257716178894043, Transition Loss 6.558260917663574, Classifier Loss 0.19300325214862823, Total Loss 142.6737060546875\n",
      "52: Encoding Loss 16.39948272705078, Transition Loss 6.349093437194824, Classifier Loss 0.19147513806819916, Total Loss 151.6132049560547\n",
      "52: Encoding Loss 16.28346824645996, Transition Loss 6.542135238647461, Classifier Loss 0.19732490181922913, Total Loss 151.3086700439453\n",
      "52: Encoding Loss 15.647992134094238, Transition Loss 6.80669641494751, Classifier Loss 0.21106845140457153, Total Loss 147.65213012695312\n",
      "52: Encoding Loss 15.681597709655762, Transition Loss 5.335629463195801, Classifier Loss 0.19396434724330902, Total Loss 145.9163360595703\n",
      "52: Encoding Loss 14.249746322631836, Transition Loss 9.273004531860352, Classifier Loss 0.2008209228515625, Total Loss 135.93467712402344\n",
      "52: Encoding Loss 15.551348686218262, Transition Loss 5.354978561401367, Classifier Loss 0.21262332797050476, Total Loss 146.74412536621094\n",
      "52: Encoding Loss 15.590897560119629, Transition Loss 3.668212652206421, Classifier Loss 0.214030921459198, Total Loss 146.86392211914062\n",
      "52: Encoding Loss 15.286755561828613, Transition Loss 5.7906904220581055, Classifier Loss 0.1875491589307785, Total Loss 142.2071075439453\n",
      "52: Encoding Loss 15.870373725891113, Transition Loss 6.4271559715271, Classifier Loss 0.1971425712108612, Total Loss 147.96267700195312\n",
      "52: Encoding Loss 15.988665580749512, Transition Loss 7.010939598083496, Classifier Loss 0.19698020815849304, Total Loss 149.00953674316406\n",
      "52: Encoding Loss 15.864154815673828, Transition Loss 7.046191215515137, Classifier Loss 0.1958589255809784, Total Loss 147.90835571289062\n",
      "52: Encoding Loss 14.569716453552246, Transition Loss 7.481780052185059, Classifier Loss 0.2107715606689453, Total Loss 139.13124084472656\n",
      "52: Encoding Loss 15.65717601776123, Transition Loss 6.401278018951416, Classifier Loss 0.20019347965717316, Total Loss 146.55702209472656\n",
      "52: Encoding Loss 14.701922416687012, Transition Loss 8.457982063293457, Classifier Loss 0.194550022482872, Total Loss 138.761962890625\n",
      "52: Encoding Loss 15.744364738464355, Transition Loss 6.924158573150635, Classifier Loss 0.20296303927898407, Total Loss 147.63604736328125\n",
      "52: Encoding Loss 14.835001945495605, Transition Loss 3.3276727199554443, Classifier Loss 0.22209420800209045, Total Loss 141.55496215820312\n",
      "52: Encoding Loss 14.476029396057129, Transition Loss 6.851474761962891, Classifier Loss 0.2092730700969696, Total Loss 138.10585021972656\n",
      "52: Encoding Loss 14.666356086730957, Transition Loss 4.221778869628906, Classifier Loss 0.21155999600887299, Total Loss 139.33120727539062\n",
      "52: Encoding Loss 15.472611427307129, Transition Loss 6.8827924728393555, Classifier Loss 0.19533468782901764, Total Loss 144.69091796875\n",
      "52: Encoding Loss 15.244119644165039, Transition Loss 8.510313987731934, Classifier Loss 0.20767663419246674, Total Loss 144.4226837158203\n",
      "52: Encoding Loss 16.443326950073242, Transition Loss 5.753933906555176, Classifier Loss 0.20506080985069275, Total Loss 153.2034912109375\n",
      "52: Encoding Loss 15.593384742736816, Transition Loss 7.363083839416504, Classifier Loss 0.19641363620758057, Total Loss 145.86105346679688\n",
      "52: Encoding Loss 14.52080249786377, Transition Loss 7.048655033111572, Classifier Loss 0.22095340490341187, Total Loss 139.67149353027344\n",
      "52: Encoding Loss 15.610957145690918, Transition Loss 5.133853912353516, Classifier Loss 0.20941509306430817, Total Loss 146.85592651367188\n",
      "52: Encoding Loss 15.287737846374512, Transition Loss 4.657478332519531, Classifier Loss 0.19851039350032806, Total Loss 143.08444213867188\n",
      "52: Encoding Loss 15.63469123840332, Transition Loss 3.6069998741149902, Classifier Loss 0.20930913090705872, Total Loss 146.72984313964844\n",
      "52: Encoding Loss 16.694917678833008, Transition Loss 4.916351318359375, Classifier Loss 0.214497908949852, Total Loss 155.99240112304688\n",
      "52: Encoding Loss 16.161617279052734, Transition Loss 6.919651985168457, Classifier Loss 0.2143498659133911, Total Loss 152.1118621826172\n",
      "52: Encoding Loss 15.389585494995117, Transition Loss 7.794775485992432, Classifier Loss 0.20443092286586761, Total Loss 145.11874389648438\n",
      "52: Encoding Loss 16.664682388305664, Transition Loss 6.792703151702881, Classifier Loss 0.21835550665855408, Total Loss 156.5115509033203\n",
      "52: Encoding Loss 15.948055267333984, Transition Loss 5.985381603240967, Classifier Loss 0.1940343976020813, Total Loss 148.18496704101562\n",
      "52: Encoding Loss 16.035484313964844, Transition Loss 6.309754848480225, Classifier Loss 0.18311801552772522, Total Loss 147.8576202392578\n",
      "52: Encoding Loss 16.595661163330078, Transition Loss 4.50534725189209, Classifier Loss 0.20604893565177917, Total Loss 154.271240234375\n",
      "52: Encoding Loss 16.336944580078125, Transition Loss 5.052447319030762, Classifier Loss 0.19443173706531525, Total Loss 151.1492156982422\n",
      "52: Encoding Loss 15.673093795776367, Transition Loss 4.227128982543945, Classifier Loss 0.1959581971168518, Total Loss 145.8260040283203\n",
      "52: Encoding Loss 16.519346237182617, Transition Loss 7.226150989532471, Classifier Loss 0.2125585526227951, Total Loss 154.85586547851562\n",
      "52: Encoding Loss 14.992294311523438, Transition Loss 6.667459964752197, Classifier Loss 0.19238513708114624, Total Loss 140.51036071777344\n",
      "52: Encoding Loss 14.87293815612793, Transition Loss 5.711716651916504, Classifier Loss 0.2151818573474884, Total Loss 141.64404296875\n",
      "52: Encoding Loss 15.340645790100098, Transition Loss 5.024677276611328, Classifier Loss 0.21173734962940216, Total Loss 144.90383911132812\n",
      "52: Encoding Loss 14.969054222106934, Transition Loss 5.681334495544434, Classifier Loss 0.19113901257514954, Total Loss 140.00259399414062\n",
      "52: Encoding Loss 14.811856269836426, Transition Loss 4.490974426269531, Classifier Loss 0.19251874089241028, Total Loss 138.64491271972656\n",
      "52: Encoding Loss 15.567618370056152, Transition Loss 5.225903034210205, Classifier Loss 0.18518969416618347, Total Loss 144.1051025390625\n",
      "52: Encoding Loss 16.736841201782227, Transition Loss 6.362679958343506, Classifier Loss 0.20965783298015594, Total Loss 156.133056640625\n",
      "52: Encoding Loss 17.278398513793945, Transition Loss 5.9016571044921875, Classifier Loss 0.2296924591064453, Total Loss 162.37677001953125\n",
      "52: Encoding Loss 15.131436347961426, Transition Loss 4.087182998657227, Classifier Loss 0.20081675052642822, Total Loss 141.9506072998047\n",
      "52: Encoding Loss 14.963628768920898, Transition Loss 4.518646240234375, Classifier Loss 0.1885603666305542, Total Loss 139.46881103515625\n",
      "52: Encoding Loss 15.546148300170898, Transition Loss 6.349353313446045, Classifier Loss 0.21091356873512268, Total Loss 146.7304229736328\n",
      "52: Encoding Loss 14.888904571533203, Transition Loss 5.417331218719482, Classifier Loss 0.19289462268352509, Total Loss 139.48416137695312\n",
      "52: Encoding Loss 15.900928497314453, Transition Loss 5.161650657653809, Classifier Loss 0.20583438873291016, Total Loss 148.8231964111328\n",
      "52: Encoding Loss 16.420549392700195, Transition Loss 5.252931594848633, Classifier Loss 0.18590973317623138, Total Loss 151.00595092773438\n",
      "52: Encoding Loss 15.63662052154541, Transition Loss 6.518918037414551, Classifier Loss 0.20870479941368103, Total Loss 147.26724243164062\n",
      "52: Encoding Loss 15.390223503112793, Transition Loss 5.618403911590576, Classifier Loss 0.21405406296253204, Total Loss 145.65087890625\n",
      "52: Encoding Loss 16.806171417236328, Transition Loss 5.593382358551025, Classifier Loss 0.2058582305908203, Total Loss 156.15386962890625\n",
      "52: Encoding Loss 15.699532508850098, Transition Loss 6.543454647064209, Classifier Loss 0.1999165415763855, Total Loss 146.8966064453125\n",
      "52: Encoding Loss 15.5154390335083, Transition Loss 6.505378246307373, Classifier Loss 0.1881462037563324, Total Loss 144.2392120361328\n",
      "52: Encoding Loss 16.501741409301758, Transition Loss 6.703105926513672, Classifier Loss 0.19242441654205322, Total Loss 152.5970001220703\n",
      "52: Encoding Loss 14.573771476745605, Transition Loss 8.008930206298828, Classifier Loss 0.23701198399066925, Total Loss 141.89315795898438\n",
      "52: Encoding Loss 15.703191757202148, Transition Loss 5.983916282653809, Classifier Loss 0.20798912644386292, Total Loss 147.62123107910156\n",
      "52: Encoding Loss 15.322882652282715, Transition Loss 7.376282215118408, Classifier Loss 0.19473405182361603, Total Loss 143.53172302246094\n",
      "52: Encoding Loss 15.647947311401367, Transition Loss 6.938704490661621, Classifier Loss 0.20358049869537354, Total Loss 146.92938232421875\n",
      "52: Encoding Loss 15.201092720031738, Transition Loss 7.359524250030518, Classifier Loss 0.2017824947834015, Total Loss 143.25889587402344\n",
      "52: Encoding Loss 16.91810417175293, Transition Loss 5.658186435699463, Classifier Loss 0.20190732181072235, Total Loss 156.66720581054688\n",
      "52: Encoding Loss 15.273238182067871, Transition Loss 6.186468124389648, Classifier Loss 0.2146664708852768, Total Loss 144.8898468017578\n",
      "52: Encoding Loss 16.178699493408203, Transition Loss 5.751758098602295, Classifier Loss 0.23022884130477905, Total Loss 153.6028289794922\n",
      "52: Encoding Loss 15.667794227600098, Transition Loss 6.744034290313721, Classifier Loss 0.20391573011875153, Total Loss 147.08273315429688\n",
      "52: Encoding Loss 15.59158706665039, Transition Loss 6.163180351257324, Classifier Loss 0.20694148540496826, Total Loss 146.6594696044922\n",
      "52: Encoding Loss 16.656070709228516, Transition Loss 5.365603446960449, Classifier Loss 0.2134566456079483, Total Loss 155.6673583984375\n",
      "52: Encoding Loss 15.172860145568848, Transition Loss 5.802457809448242, Classifier Loss 0.18303678929805756, Total Loss 140.84706115722656\n",
      "52: Encoding Loss 15.552996635437012, Transition Loss 4.75196647644043, Classifier Loss 0.21617905795574188, Total Loss 146.99227905273438\n",
      "52: Encoding Loss 15.444523811340332, Transition Loss 7.343995094299316, Classifier Loss 0.22109434008598328, Total Loss 147.13441467285156\n",
      "52: Encoding Loss 15.10344123840332, Transition Loss 4.868808269500732, Classifier Loss 0.20576196908950806, Total Loss 142.3774871826172\n",
      "52: Encoding Loss 15.48647403717041, Transition Loss 4.210450172424316, Classifier Loss 0.2110372632741928, Total Loss 145.83761596679688\n",
      "52: Encoding Loss 16.10218620300293, Transition Loss 5.497203826904297, Classifier Loss 0.18683239817619324, Total Loss 148.6001739501953\n",
      "52: Encoding Loss 15.768914222717285, Transition Loss 7.381389141082764, Classifier Loss 0.23487883806228638, Total Loss 151.115478515625\n",
      "52: Encoding Loss 15.920689582824707, Transition Loss 6.881570816040039, Classifier Loss 0.21085114777088165, Total Loss 149.8269500732422\n",
      "52: Encoding Loss 16.711328506469727, Transition Loss 5.649198532104492, Classifier Loss 0.18215979635715485, Total Loss 153.0364532470703\n",
      "52: Encoding Loss 16.476991653442383, Transition Loss 5.984407901763916, Classifier Loss 0.19121260941028595, Total Loss 152.1340789794922\n",
      "52: Encoding Loss 15.246602058410645, Transition Loss 4.214099407196045, Classifier Loss 0.19935473799705505, Total Loss 142.75111389160156\n",
      "52: Encoding Loss 15.790485382080078, Transition Loss 6.9540276527404785, Classifier Loss 0.22385291755199432, Total Loss 150.0999755859375\n",
      "52: Encoding Loss 16.03963851928711, Transition Loss 5.378380298614502, Classifier Loss 0.21583224833011627, Total Loss 150.9760284423828\n",
      "52: Encoding Loss 15.364056587219238, Transition Loss 7.86228084564209, Classifier Loss 0.22056448459625244, Total Loss 146.54135131835938\n",
      "52: Encoding Loss 16.244718551635742, Transition Loss 4.277169227600098, Classifier Loss 0.20500776171684265, Total Loss 151.31396484375\n",
      "52: Encoding Loss 15.759980201721191, Transition Loss 5.305116653442383, Classifier Loss 0.19951306283473969, Total Loss 147.09217834472656\n",
      "52: Encoding Loss 16.391883850097656, Transition Loss 5.719056129455566, Classifier Loss 0.22617216408252716, Total Loss 154.89610290527344\n",
      "52: Encoding Loss 15.732471466064453, Transition Loss 7.6848835945129395, Classifier Loss 0.21203872561454773, Total Loss 148.60060119628906\n",
      "52: Encoding Loss 14.806525230407715, Transition Loss 6.837657928466797, Classifier Loss 0.19137223064899445, Total Loss 138.9569549560547\n",
      "52: Encoding Loss 15.30022144317627, Transition Loss 6.292990684509277, Classifier Loss 0.19042593240737915, Total Loss 142.7029571533203\n",
      "52: Encoding Loss 14.645035743713379, Transition Loss 9.194165229797363, Classifier Loss 0.20037588477134705, Total Loss 139.03671264648438\n",
      "52: Encoding Loss 15.835585594177246, Transition Loss 6.009346961975098, Classifier Loss 0.2006634771823883, Total Loss 147.95291137695312\n",
      "52: Encoding Loss 15.598419189453125, Transition Loss 6.852968215942383, Classifier Loss 0.20223063230514526, Total Loss 146.38101196289062\n",
      "52: Encoding Loss 14.066293716430664, Transition Loss 5.46842622756958, Classifier Loss 0.22005029022693634, Total Loss 135.6290740966797\n",
      "52: Encoding Loss 14.999709129333496, Transition Loss 7.315159320831299, Classifier Loss 0.19709138572216034, Total Loss 141.1698455810547\n",
      "52: Encoding Loss 15.733345985412598, Transition Loss 5.919276237487793, Classifier Loss 0.20658910274505615, Total Loss 147.70953369140625\n",
      "52: Encoding Loss 16.33365821838379, Transition Loss 7.163745880126953, Classifier Loss 0.21809405088424683, Total Loss 153.9114227294922\n",
      "52: Encoding Loss 15.41234016418457, Transition Loss 5.638888359069824, Classifier Loss 0.2036183476448059, Total Loss 144.788330078125\n",
      "52: Encoding Loss 15.829594612121582, Transition Loss 5.376647472381592, Classifier Loss 0.19304823875427246, Total Loss 147.0169219970703\n",
      "52: Encoding Loss 15.209120750427246, Transition Loss 4.121410846710205, Classifier Loss 0.23294098675251007, Total Loss 145.79135131835938\n",
      "52: Encoding Loss 15.743441581726074, Transition Loss 7.531500339508057, Classifier Loss 0.19665010273456573, Total Loss 147.1188507080078\n",
      "52: Encoding Loss 16.426837921142578, Transition Loss 5.794910430908203, Classifier Loss 0.21506787836551666, Total Loss 154.08047485351562\n",
      "52: Encoding Loss 15.57044792175293, Transition Loss 6.2059831619262695, Classifier Loss 0.2192661166191101, Total Loss 147.7313995361328\n",
      "52: Encoding Loss 15.700966835021973, Transition Loss 6.0609941482543945, Classifier Loss 0.192169189453125, Total Loss 146.036865234375\n",
      "52: Encoding Loss 15.928844451904297, Transition Loss 5.640023231506348, Classifier Loss 0.2186974287033081, Total Loss 150.4285125732422\n",
      "52: Encoding Loss 16.368022918701172, Transition Loss 4.19144344329834, Classifier Loss 0.20179520547389984, Total Loss 151.96200561523438\n",
      "52: Encoding Loss 15.861740112304688, Transition Loss 4.408238410949707, Classifier Loss 0.20762747526168823, Total Loss 148.53831481933594\n",
      "52: Encoding Loss 16.002742767333984, Transition Loss 6.406294345855713, Classifier Loss 0.20425361394882202, Total Loss 149.7285919189453\n",
      "52: Encoding Loss 16.78461456298828, Transition Loss 4.286851406097412, Classifier Loss 0.19367119669914246, Total Loss 154.5014190673828\n",
      "52: Encoding Loss 17.178401947021484, Transition Loss 4.486506938934326, Classifier Loss 0.2280563861131668, Total Loss 161.1301727294922\n",
      "52: Encoding Loss 14.864762306213379, Transition Loss 6.4719977378845215, Classifier Loss 0.20432592928409576, Total Loss 140.6450958251953\n",
      "52: Encoding Loss 14.730050086975098, Transition Loss 6.665972709655762, Classifier Loss 0.18624065816402435, Total Loss 137.79766845703125\n",
      "52: Encoding Loss 15.105500221252441, Transition Loss 6.119369983673096, Classifier Loss 0.21240234375, Total Loss 143.30812072753906\n",
      "52: Encoding Loss 15.174944877624512, Transition Loss 6.793293476104736, Classifier Loss 0.21469178795814514, Total Loss 144.22740173339844\n",
      "52: Encoding Loss 14.968384742736816, Transition Loss 8.95241928100586, Classifier Loss 0.2103421688079834, Total Loss 142.57179260253906\n",
      "52: Encoding Loss 16.636629104614258, Transition Loss 5.629001140594482, Classifier Loss 0.20771893858909607, Total Loss 154.99072265625\n",
      "52: Encoding Loss 16.550294876098633, Transition Loss 5.4119367599487305, Classifier Loss 0.17778314650058746, Total Loss 151.2630615234375\n",
      "52: Encoding Loss 14.800952911376953, Transition Loss 4.961655139923096, Classifier Loss 0.18608634173870087, Total Loss 138.00857543945312\n",
      "52: Encoding Loss 15.323866844177246, Transition Loss 4.786548137664795, Classifier Loss 0.20492704212665558, Total Loss 144.0409393310547\n",
      "52: Encoding Loss 15.383939743041992, Transition Loss 5.511847496032715, Classifier Loss 0.20644673705101013, Total Loss 144.81857299804688\n",
      "52: Encoding Loss 15.8103609085083, Transition Loss 6.081520080566406, Classifier Loss 0.20521292090415955, Total Loss 148.22048950195312\n",
      "52: Encoding Loss 15.81674575805664, Transition Loss 4.6867499351501465, Classifier Loss 0.18924906849861145, Total Loss 146.39620971679688\n",
      "52: Encoding Loss 15.851191520690918, Transition Loss 5.490012168884277, Classifier Loss 0.22286739945411682, Total Loss 150.19427490234375\n",
      "52: Encoding Loss 14.925054550170898, Transition Loss 4.431893348693848, Classifier Loss 0.18010741472244263, Total Loss 138.29757690429688\n",
      "52: Encoding Loss 15.754023551940918, Transition Loss 6.131382465362549, Classifier Loss 0.18288305401802063, Total Loss 145.54676818847656\n",
      "52: Encoding Loss 16.08028221130371, Transition Loss 3.9573748111724854, Classifier Loss 0.21613247692584991, Total Loss 151.04698181152344\n",
      "52: Encoding Loss 15.097854614257812, Transition Loss 7.108509540557861, Classifier Loss 0.2192363142967224, Total Loss 144.128173828125\n",
      "52: Encoding Loss 15.768621444702148, Transition Loss 4.872342109680176, Classifier Loss 0.21845030784606934, Total Loss 148.96849060058594\n",
      "52: Encoding Loss 15.950839042663574, Transition Loss 6.432640075683594, Classifier Loss 0.21537165343761444, Total Loss 150.43040466308594\n",
      "52: Encoding Loss 15.795701026916504, Transition Loss 5.024997711181641, Classifier Loss 0.20068258047103882, Total Loss 147.43887329101562\n",
      "52: Encoding Loss 15.204524040222168, Transition Loss 6.06339168548584, Classifier Loss 0.1983981430530548, Total Loss 142.68869018554688\n",
      "52: Encoding Loss 16.9506778717041, Transition Loss 3.9465696811676025, Classifier Loss 0.2002972960472107, Total Loss 156.42445373535156\n",
      "52: Encoding Loss 16.19362449645996, Transition Loss 3.747497797012329, Classifier Loss 0.21277497708797455, Total Loss 151.57598876953125\n",
      "52: Encoding Loss 15.504918098449707, Transition Loss 4.502701282501221, Classifier Loss 0.19826854765415192, Total Loss 144.76673889160156\n",
      "52: Encoding Loss 15.140897750854492, Transition Loss 5.960032939910889, Classifier Loss 0.20870362222194672, Total Loss 143.18955993652344\n",
      "52: Encoding Loss 15.2677001953125, Transition Loss 3.2965946197509766, Classifier Loss 0.20775273442268372, Total Loss 143.57618713378906\n",
      "52: Encoding Loss 15.981886863708496, Transition Loss 7.436967849731445, Classifier Loss 0.22183804214000702, Total Loss 151.5262908935547\n",
      "52: Encoding Loss 16.230274200439453, Transition Loss 4.810966968536377, Classifier Loss 0.20074217021465302, Total Loss 150.87860107421875\n",
      "52: Encoding Loss 17.113208770751953, Transition Loss 5.871941566467285, Classifier Loss 0.2052159607410431, Total Loss 158.60165405273438\n",
      "52: Encoding Loss 16.632282257080078, Transition Loss 6.446110248565674, Classifier Loss 0.20376598834991455, Total Loss 154.7240753173828\n",
      "52: Encoding Loss 15.301875114440918, Transition Loss 8.403331756591797, Classifier Loss 0.19580048322677612, Total Loss 143.67572021484375\n",
      "52: Encoding Loss 13.065185546875, Transition Loss 9.004878044128418, Classifier Loss 0.19752924144268036, Total Loss 126.07537841796875\n",
      "53: Encoding Loss 16.871904373168945, Transition Loss 4.879693984985352, Classifier Loss 0.20224279165267944, Total Loss 156.17544555664062\n",
      "53: Encoding Loss 16.250873565673828, Transition Loss 5.179422378540039, Classifier Loss 0.20024017989635468, Total Loss 151.06689453125\n",
      "53: Encoding Loss 16.27433967590332, Transition Loss 5.149143695831299, Classifier Loss 0.1969831883907318, Total Loss 150.92286682128906\n",
      "53: Encoding Loss 15.77924633026123, Transition Loss 5.64272928237915, Classifier Loss 0.20159019529819489, Total Loss 147.5215301513672\n",
      "53: Encoding Loss 17.47573471069336, Transition Loss 5.212876319885254, Classifier Loss 0.2035612165927887, Total Loss 161.20458984375\n",
      "53: Encoding Loss 13.998754501342773, Transition Loss 6.14777946472168, Classifier Loss 0.2141115367412567, Total Loss 134.63075256347656\n",
      "53: Encoding Loss 16.893749237060547, Transition Loss 4.77944803237915, Classifier Loss 0.2193196713924408, Total Loss 158.03785705566406\n",
      "53: Encoding Loss 17.217390060424805, Transition Loss 5.408496856689453, Classifier Loss 0.2199486941099167, Total Loss 160.81568908691406\n",
      "53: Encoding Loss 15.586377143859863, Transition Loss 5.244030475616455, Classifier Loss 0.24751220643520355, Total Loss 150.4910430908203\n",
      "53: Encoding Loss 16.68385124206543, Transition Loss 5.283930778503418, Classifier Loss 0.21651539206504822, Total Loss 156.17913818359375\n",
      "53: Encoding Loss 16.06879997253418, Transition Loss 4.596444129943848, Classifier Loss 0.20005884766578674, Total Loss 149.4755859375\n",
      "53: Encoding Loss 15.749236106872559, Transition Loss 5.114945888519287, Classifier Loss 0.2238735854625702, Total Loss 149.40423583984375\n",
      "53: Encoding Loss 15.58731460571289, Transition Loss 6.046084403991699, Classifier Loss 0.19909486174583435, Total Loss 145.8172149658203\n",
      "53: Encoding Loss 14.945687294006348, Transition Loss 5.078890800476074, Classifier Loss 0.1984158754348755, Total Loss 140.42286682128906\n",
      "53: Encoding Loss 15.04238510131836, Transition Loss 6.22774076461792, Classifier Loss 0.2082633376121521, Total Loss 142.4109649658203\n",
      "53: Encoding Loss 15.92278003692627, Transition Loss 6.848865985870361, Classifier Loss 0.21434888243675232, Total Loss 150.1868896484375\n",
      "53: Encoding Loss 15.81987476348877, Transition Loss 5.640139579772949, Classifier Loss 0.20347711443901062, Total Loss 148.03472900390625\n",
      "53: Encoding Loss 15.263707160949707, Transition Loss 6.406745910644531, Classifier Loss 0.19433832168579102, Total Loss 142.8248291015625\n",
      "53: Encoding Loss 16.38785171508789, Transition Loss 6.155117034912109, Classifier Loss 0.19443270564079285, Total Loss 151.777099609375\n",
      "53: Encoding Loss 16.27528953552246, Transition Loss 6.326061725616455, Classifier Loss 0.19706475734710693, Total Loss 151.17401123046875\n",
      "53: Encoding Loss 15.652909278869629, Transition Loss 6.78247594833374, Classifier Loss 0.21010537445545197, Total Loss 147.59031677246094\n",
      "53: Encoding Loss 15.677370071411133, Transition Loss 5.215137958526611, Classifier Loss 0.19951185584068298, Total Loss 146.41317749023438\n",
      "53: Encoding Loss 14.249897956848145, Transition Loss 9.090210914611816, Classifier Loss 0.20245137810707092, Total Loss 136.06236267089844\n",
      "53: Encoding Loss 15.527567863464355, Transition Loss 5.189157962799072, Classifier Loss 0.21302850544452667, Total Loss 146.56121826171875\n",
      "53: Encoding Loss 15.583353996276855, Transition Loss 3.5048797130584717, Classifier Loss 0.20122955739498138, Total Loss 145.4907684326172\n",
      "53: Encoding Loss 15.29893970489502, Transition Loss 5.690289497375488, Classifier Loss 0.19327053427696228, Total Loss 142.85662841796875\n",
      "53: Encoding Loss 15.889119148254395, Transition Loss 6.351670265197754, Classifier Loss 0.19742876291275024, Total Loss 148.1261749267578\n",
      "53: Encoding Loss 15.97675895690918, Transition Loss 6.931663513183594, Classifier Loss 0.21654640138149261, Total Loss 150.8550567626953\n",
      "53: Encoding Loss 15.87966537475586, Transition Loss 7.001900672912598, Classifier Loss 0.19567091763019562, Total Loss 148.00479125976562\n",
      "53: Encoding Loss 14.561408042907715, Transition Loss 7.476631164550781, Classifier Loss 0.205475851893425, Total Loss 138.5341796875\n",
      "53: Encoding Loss 15.664595603942871, Transition Loss 6.280109405517578, Classifier Loss 0.2006765902042389, Total Loss 146.6404571533203\n",
      "53: Encoding Loss 14.710637092590332, Transition Loss 8.447431564331055, Classifier Loss 0.19531908631324768, Total Loss 138.906494140625\n",
      "53: Encoding Loss 15.746487617492676, Transition Loss 6.681853771209717, Classifier Loss 0.20502933859825134, Total Loss 147.8112030029297\n",
      "53: Encoding Loss 14.828106880187988, Transition Loss 3.351184129714966, Classifier Loss 0.22783498466014862, Total Loss 142.07859802246094\n",
      "53: Encoding Loss 14.47901439666748, Transition Loss 6.557051658630371, Classifier Loss 0.20813997089862823, Total Loss 137.95753479003906\n",
      "53: Encoding Loss 14.6484375, Transition Loss 4.265865325927734, Classifier Loss 0.22151336073875427, Total Loss 140.1920166015625\n",
      "53: Encoding Loss 15.483553886413574, Transition Loss 6.30610990524292, Classifier Loss 0.19532620906829834, Total Loss 144.66226196289062\n",
      "53: Encoding Loss 15.247119903564453, Transition Loss 8.99199390411377, Classifier Loss 0.2131912112236023, Total Loss 145.09446716308594\n",
      "53: Encoding Loss 16.460952758789062, Transition Loss 5.274816989898682, Classifier Loss 0.21093301475048065, Total Loss 153.8358917236328\n",
      "53: Encoding Loss 15.59204387664795, Transition Loss 7.366189956665039, Classifier Loss 0.19282926619052887, Total Loss 145.4925079345703\n",
      "53: Encoding Loss 14.537028312683105, Transition Loss 6.3599138259887695, Classifier Loss 0.22346511483192444, Total Loss 139.9147186279297\n",
      "53: Encoding Loss 15.613870620727539, Transition Loss 5.142845153808594, Classifier Loss 0.21318048238754272, Total Loss 147.25758361816406\n",
      "53: Encoding Loss 15.278619766235352, Transition Loss 4.3165435791015625, Classifier Loss 0.1923791766166687, Total Loss 142.33018493652344\n",
      "53: Encoding Loss 15.639059066772461, Transition Loss 3.5468242168426514, Classifier Loss 0.20385177433490753, Total Loss 146.20701599121094\n",
      "53: Encoding Loss 16.70172119140625, Transition Loss 4.6011128425598145, Classifier Loss 0.20848600566387177, Total Loss 155.38259887695312\n",
      "53: Encoding Loss 16.14841079711914, Transition Loss 6.819118499755859, Classifier Loss 0.21943393349647522, Total Loss 152.4945068359375\n",
      "53: Encoding Loss 15.386617660522461, Transition Loss 7.572146415710449, Classifier Loss 0.2026536762714386, Total Loss 144.8727569580078\n",
      "53: Encoding Loss 16.671749114990234, Transition Loss 6.615310192108154, Classifier Loss 0.2228507548570633, Total Loss 156.98214721679688\n",
      "53: Encoding Loss 15.960062026977539, Transition Loss 5.892341613769531, Classifier Loss 0.19338467717170715, Total Loss 148.1974334716797\n",
      "53: Encoding Loss 16.03902816772461, Transition Loss 6.108358860015869, Classifier Loss 0.18836912512779236, Total Loss 148.37081909179688\n",
      "53: Encoding Loss 16.600605010986328, Transition Loss 4.454347133636475, Classifier Loss 0.2050599306821823, Total Loss 154.2017059326172\n",
      "53: Encoding Loss 16.344690322875977, Transition Loss 4.844830513000488, Classifier Loss 0.20244169235229492, Total Loss 151.9706573486328\n",
      "53: Encoding Loss 15.68315601348877, Transition Loss 4.229352951049805, Classifier Loss 0.19753071665763855, Total Loss 146.06419372558594\n",
      "53: Encoding Loss 16.53162384033203, Transition Loss 6.923830509185791, Classifier Loss 0.21483859419822693, Total Loss 155.12161254882812\n",
      "53: Encoding Loss 14.977429389953613, Transition Loss 6.803738117218018, Classifier Loss 0.19633422791957855, Total Loss 140.8135986328125\n",
      "53: Encoding Loss 14.850061416625977, Transition Loss 5.389697551727295, Classifier Loss 0.2159196436405182, Total Loss 141.47039794921875\n",
      "53: Encoding Loss 15.314939498901367, Transition Loss 5.077035903930664, Classifier Loss 0.2092055380344391, Total Loss 144.4554901123047\n",
      "53: Encoding Loss 14.955338478088379, Transition Loss 5.091113567352295, Classifier Loss 0.18775875866413116, Total Loss 139.4368133544922\n",
      "53: Encoding Loss 14.81913948059082, Transition Loss 4.836141586303711, Classifier Loss 0.19058626890182495, Total Loss 138.57896423339844\n",
      "53: Encoding Loss 15.540314674377441, Transition Loss 4.8260979652404785, Classifier Loss 0.17907799780368805, Total Loss 143.195556640625\n",
      "53: Encoding Loss 16.743688583374023, Transition Loss 6.56040096282959, Classifier Loss 0.21248352527618408, Total Loss 156.5099334716797\n",
      "53: Encoding Loss 17.274906158447266, Transition Loss 5.584690570831299, Classifier Loss 0.20893368124961853, Total Loss 160.20956420898438\n",
      "53: Encoding Loss 15.124499320983887, Transition Loss 4.091533184051514, Classifier Loss 0.20993416011333466, Total Loss 142.80772399902344\n",
      "53: Encoding Loss 14.977075576782227, Transition Loss 4.478697299957275, Classifier Loss 0.1902346909046173, Total Loss 139.73580932617188\n",
      "53: Encoding Loss 15.553644180297852, Transition Loss 6.5295729637146, Classifier Loss 0.20307207107543945, Total Loss 146.04226684570312\n",
      "53: Encoding Loss 14.891239166259766, Transition Loss 5.456938743591309, Classifier Loss 0.18967632949352264, Total Loss 139.1889190673828\n",
      "53: Encoding Loss 15.918417930603027, Transition Loss 5.28411865234375, Classifier Loss 0.2034657895565033, Total Loss 148.75074768066406\n",
      "53: Encoding Loss 16.41185188293457, Transition Loss 5.020223617553711, Classifier Loss 0.1834343820810318, Total Loss 150.64230346679688\n",
      "53: Encoding Loss 15.628617286682129, Transition Loss 6.639968395233154, Classifier Loss 0.208225816488266, Total Loss 147.1795196533203\n",
      "53: Encoding Loss 15.39188289642334, Transition Loss 5.471750736236572, Classifier Loss 0.21490541100502014, Total Loss 145.71995544433594\n",
      "53: Encoding Loss 16.80548095703125, Transition Loss 5.989230155944824, Classifier Loss 0.2028769701719284, Total Loss 155.92938232421875\n",
      "53: Encoding Loss 15.694564819335938, Transition Loss 6.4058122634887695, Classifier Loss 0.19962142407894135, Total Loss 146.79981994628906\n",
      "53: Encoding Loss 15.522602081298828, Transition Loss 6.895867347717285, Classifier Loss 0.18684256076812744, Total Loss 144.24424743652344\n",
      "53: Encoding Loss 16.496736526489258, Transition Loss 6.569897174835205, Classifier Loss 0.19220022857189178, Total Loss 152.5078887939453\n",
      "53: Encoding Loss 14.592394828796387, Transition Loss 8.335068702697754, Classifier Loss 0.23253609240055084, Total Loss 141.65977478027344\n",
      "53: Encoding Loss 15.725142478942871, Transition Loss 5.67002010345459, Classifier Loss 0.19965314865112305, Total Loss 146.90045166015625\n",
      "53: Encoding Loss 15.333541870117188, Transition Loss 7.5043110847473145, Classifier Loss 0.19556105136871338, Total Loss 143.72531127929688\n",
      "53: Encoding Loss 15.640506744384766, Transition Loss 6.6318206787109375, Classifier Loss 0.2039528787136078, Total Loss 146.845703125\n",
      "53: Encoding Loss 15.211145401000977, Transition Loss 7.597067832946777, Classifier Loss 0.19551149010658264, Total Loss 142.7597198486328\n",
      "53: Encoding Loss 16.926713943481445, Transition Loss 5.236677646636963, Classifier Loss 0.2037103921175003, Total Loss 156.8320770263672\n",
      "53: Encoding Loss 15.290884971618652, Transition Loss 6.607234954833984, Classifier Loss 0.21152107417583466, Total Loss 144.80062866210938\n",
      "53: Encoding Loss 16.182985305786133, Transition Loss 5.250951766967773, Classifier Loss 0.21291357278823853, Total Loss 151.805419921875\n",
      "53: Encoding Loss 15.671229362487793, Transition Loss 7.1959943771362305, Classifier Loss 0.19538414478302002, Total Loss 146.34744262695312\n",
      "53: Encoding Loss 15.60870361328125, Transition Loss 5.799259185791016, Classifier Loss 0.19597271084785461, Total Loss 145.6267547607422\n",
      "53: Encoding Loss 16.661083221435547, Transition Loss 5.572044372558594, Classifier Loss 0.19370949268341064, Total Loss 153.7740478515625\n",
      "53: Encoding Loss 15.185449600219727, Transition Loss 5.777552604675293, Classifier Loss 0.1891949623823166, Total Loss 141.55860900878906\n",
      "53: Encoding Loss 15.545980453491211, Transition Loss 5.064592361450195, Classifier Loss 0.20956122875213623, Total Loss 146.33689880371094\n",
      "53: Encoding Loss 15.453853607177734, Transition Loss 7.3093037605285645, Classifier Loss 0.21965518593788147, Total Loss 147.0582275390625\n",
      "53: Encoding Loss 15.111661911010742, Transition Loss 5.234844207763672, Classifier Loss 0.2024584412574768, Total Loss 142.1861114501953\n",
      "53: Encoding Loss 15.458864212036133, Transition Loss 4.0786662101745605, Classifier Loss 0.2006096988916397, Total Loss 144.54762268066406\n",
      "53: Encoding Loss 16.08816146850586, Transition Loss 5.719893455505371, Classifier Loss 0.18309056758880615, Total Loss 148.15834045410156\n",
      "53: Encoding Loss 15.759902000427246, Transition Loss 7.138811111450195, Classifier Loss 0.23408693075180054, Total Loss 150.91567993164062\n",
      "53: Encoding Loss 15.919299125671387, Transition Loss 7.229519844055176, Classifier Loss 0.22264184057712555, Total Loss 151.06448364257812\n",
      "53: Encoding Loss 16.70684242248535, Transition Loss 5.346599578857422, Classifier Loss 0.1816679984331131, Total Loss 152.89085388183594\n",
      "53: Encoding Loss 16.476499557495117, Transition Loss 6.109530925750732, Classifier Loss 0.19070149958133698, Total Loss 152.1040496826172\n",
      "53: Encoding Loss 15.23250675201416, Transition Loss 4.130993366241455, Classifier Loss 0.2090579867362976, Total Loss 143.59205627441406\n",
      "53: Encoding Loss 15.809797286987305, Transition Loss 7.276936054229736, Classifier Loss 0.22325393557548523, Total Loss 150.2591552734375\n",
      "53: Encoding Loss 16.03421974182129, Transition Loss 5.356423377990723, Classifier Loss 0.2217107117176056, Total Loss 151.51611328125\n",
      "53: Encoding Loss 15.354982376098633, Transition Loss 8.167353630065918, Classifier Loss 0.22742721438407898, Total Loss 147.21604919433594\n",
      "53: Encoding Loss 16.230079650878906, Transition Loss 4.226726531982422, Classifier Loss 0.2060362994670868, Total Loss 151.2896270751953\n",
      "53: Encoding Loss 15.740714073181152, Transition Loss 5.459410190582275, Classifier Loss 0.19214072823524475, Total Loss 146.23167419433594\n",
      "53: Encoding Loss 16.39878273010254, Transition Loss 5.960851192474365, Classifier Loss 0.22734417021274567, Total Loss 155.11685180664062\n",
      "53: Encoding Loss 15.734938621520996, Transition Loss 7.986128807067871, Classifier Loss 0.1999954879283905, Total Loss 147.47628784179688\n",
      "53: Encoding Loss 14.808956146240234, Transition Loss 6.9536237716674805, Classifier Loss 0.19026345014572144, Total Loss 138.88873291015625\n",
      "53: Encoding Loss 15.303552627563477, Transition Loss 6.4935712814331055, Classifier Loss 0.19267241656780243, Total Loss 142.994384765625\n",
      "53: Encoding Loss 14.645166397094727, Transition Loss 9.29440975189209, Classifier Loss 0.1904948651790619, Total Loss 138.0697021484375\n",
      "53: Encoding Loss 15.833601951599121, Transition Loss 6.264351844787598, Classifier Loss 0.197265625, Total Loss 147.64825439453125\n",
      "53: Encoding Loss 15.6097993850708, Transition Loss 6.8157854080200195, Classifier Loss 0.21174956858158112, Total Loss 147.41650390625\n",
      "53: Encoding Loss 14.036099433898926, Transition Loss 5.901327610015869, Classifier Loss 0.21111950278282166, Total Loss 134.5810089111328\n",
      "53: Encoding Loss 14.995759010314941, Transition Loss 7.228119850158691, Classifier Loss 0.19915929436683655, Total Loss 141.32762145996094\n",
      "53: Encoding Loss 15.729923248291016, Transition Loss 6.38418436050415, Classifier Loss 0.20537199079990387, Total Loss 147.65341186523438\n",
      "53: Encoding Loss 16.313148498535156, Transition Loss 6.865097522735596, Classifier Loss 0.2195737361907959, Total Loss 153.8355712890625\n",
      "53: Encoding Loss 15.415074348449707, Transition Loss 6.3166422843933105, Classifier Loss 0.19676274061203003, Total Loss 144.26019287109375\n",
      "53: Encoding Loss 15.814897537231445, Transition Loss 4.973749160766602, Classifier Loss 0.19896677136421204, Total Loss 147.41061401367188\n",
      "53: Encoding Loss 15.206952095031738, Transition Loss 4.676952838897705, Classifier Loss 0.23175305128097534, Total Loss 145.7663116455078\n",
      "53: Encoding Loss 15.738940238952637, Transition Loss 6.698361396789551, Classifier Loss 0.1915217638015747, Total Loss 146.4033660888672\n",
      "53: Encoding Loss 16.42569351196289, Transition Loss 6.257686614990234, Classifier Loss 0.21555033326148987, Total Loss 154.21212768554688\n",
      "53: Encoding Loss 15.571893692016602, Transition Loss 6.233841419219971, Classifier Loss 0.21033091843128204, Total Loss 146.85501098632812\n",
      "53: Encoding Loss 15.700918197631836, Transition Loss 6.196638584136963, Classifier Loss 0.20293766260147095, Total Loss 147.1404571533203\n",
      "53: Encoding Loss 15.929486274719238, Transition Loss 5.551461219787598, Classifier Loss 0.20782098174095154, Total Loss 149.32827758789062\n",
      "53: Encoding Loss 16.353900909423828, Transition Loss 4.31976318359375, Classifier Loss 0.2029692381620407, Total Loss 151.99208068847656\n",
      "53: Encoding Loss 15.867555618286133, Transition Loss 4.528804779052734, Classifier Loss 0.1984943151473999, Total Loss 147.6956329345703\n",
      "53: Encoding Loss 16.00311851501465, Transition Loss 6.449264049530029, Classifier Loss 0.2017926275730133, Total Loss 149.4940643310547\n",
      "53: Encoding Loss 16.781192779541016, Transition Loss 4.272324085235596, Classifier Loss 0.20065779983997345, Total Loss 155.16978454589844\n",
      "53: Encoding Loss 17.19268035888672, Transition Loss 4.371898174285889, Classifier Loss 0.2182217240333557, Total Loss 160.2379913330078\n",
      "53: Encoding Loss 14.881421089172363, Transition Loss 6.496458053588867, Classifier Loss 0.20107333362102509, Total Loss 140.45799255371094\n",
      "53: Encoding Loss 14.721650123596191, Transition Loss 6.766725540161133, Classifier Loss 0.19035404920578003, Total Loss 138.16195678710938\n",
      "53: Encoding Loss 15.115983009338379, Transition Loss 6.07157564163208, Classifier Loss 0.20695866644382477, Total Loss 142.83804321289062\n",
      "53: Encoding Loss 15.16663646697998, Transition Loss 6.790896892547607, Classifier Loss 0.2288583517074585, Total Loss 145.57711791992188\n",
      "53: Encoding Loss 14.969950675964355, Transition Loss 8.903972625732422, Classifier Loss 0.20170468091964722, Total Loss 141.7108612060547\n",
      "53: Encoding Loss 16.64462661743164, Transition Loss 5.694350242614746, Classifier Loss 0.207932710647583, Total Loss 155.0891571044922\n",
      "53: Encoding Loss 16.557130813598633, Transition Loss 5.457330703735352, Classifier Loss 0.17760615050792694, Total Loss 151.3091278076172\n",
      "53: Encoding Loss 14.810495376586914, Transition Loss 5.0456318855285645, Classifier Loss 0.18470212817192078, Total Loss 137.9633026123047\n",
      "53: Encoding Loss 15.328954696655273, Transition Loss 4.801621913909912, Classifier Loss 0.20594444870948792, Total Loss 144.18641662597656\n",
      "53: Encoding Loss 15.397701263427734, Transition Loss 5.623551368713379, Classifier Loss 0.19965721666812897, Total Loss 144.2720489501953\n",
      "53: Encoding Loss 15.836076736450195, Transition Loss 6.200175762176514, Classifier Loss 0.19967733323574066, Total Loss 147.89637756347656\n",
      "53: Encoding Loss 15.814412117004395, Transition Loss 4.833054065704346, Classifier Loss 0.20012541115283966, Total Loss 147.49444580078125\n",
      "53: Encoding Loss 15.854235649108887, Transition Loss 5.52763557434082, Classifier Loss 0.2204141765832901, Total Loss 149.9808349609375\n",
      "53: Encoding Loss 14.930749893188477, Transition Loss 4.741476058959961, Classifier Loss 0.18281792104244232, Total Loss 138.6760711669922\n",
      "53: Encoding Loss 15.757868766784668, Transition Loss 6.268707275390625, Classifier Loss 0.1797342449426651, Total Loss 145.2901153564453\n",
      "53: Encoding Loss 16.07025718688965, Transition Loss 4.173852443695068, Classifier Loss 0.2238149642944336, Total Loss 151.77833557128906\n",
      "53: Encoding Loss 15.10444164276123, Transition Loss 6.957040309906006, Classifier Loss 0.21610033512115479, Total Loss 143.83697509765625\n",
      "53: Encoding Loss 15.770593643188477, Transition Loss 5.296841621398926, Classifier Loss 0.21277311444282532, Total Loss 148.50143432617188\n",
      "53: Encoding Loss 15.942487716674805, Transition Loss 6.282736778259277, Classifier Loss 0.21122600138187408, Total Loss 149.91905212402344\n",
      "53: Encoding Loss 15.787109375, Transition Loss 5.38945198059082, Classifier Loss 0.2024892270565033, Total Loss 147.62368774414062\n",
      "53: Encoding Loss 15.21977710723877, Transition Loss 6.000683307647705, Classifier Loss 0.1977960765361786, Total Loss 142.7379608154297\n",
      "53: Encoding Loss 16.953439712524414, Transition Loss 4.096076965332031, Classifier Loss 0.21690306067466736, Total Loss 158.1370391845703\n",
      "53: Encoding Loss 16.16444969177246, Transition Loss 3.740593433380127, Classifier Loss 0.21071168780326843, Total Loss 151.1348876953125\n",
      "53: Encoding Loss 15.516082763671875, Transition Loss 4.7212138175964355, Classifier Loss 0.19788013398647308, Total Loss 144.8609161376953\n",
      "53: Encoding Loss 15.140198707580566, Transition Loss 5.795097351074219, Classifier Loss 0.20285917818546295, Total Loss 142.5665283203125\n",
      "53: Encoding Loss 15.256887435913086, Transition Loss 3.569613456726074, Classifier Loss 0.1966172158718109, Total Loss 142.43075561523438\n",
      "53: Encoding Loss 16.0030460357666, Transition Loss 7.452145099639893, Classifier Loss 0.22585347294807434, Total Loss 152.1001434326172\n",
      "53: Encoding Loss 16.215782165527344, Transition Loss 5.174373149871826, Classifier Loss 0.19309662282466888, Total Loss 150.07080078125\n",
      "53: Encoding Loss 17.107255935668945, Transition Loss 5.468629837036133, Classifier Loss 0.20992858707904816, Total Loss 158.9446258544922\n",
      "53: Encoding Loss 16.638460159301758, Transition Loss 6.859770774841309, Classifier Loss 0.19828782975673676, Total Loss 154.30841064453125\n",
      "53: Encoding Loss 15.309748649597168, Transition Loss 8.294317245483398, Classifier Loss 0.1996174156665802, Total Loss 144.0985870361328\n",
      "53: Encoding Loss 13.078734397888184, Transition Loss 9.584670066833496, Classifier Loss 0.18336772918701172, Total Loss 124.88357543945312\n",
      "54: Encoding Loss 16.884031295776367, Transition Loss 4.582813739776611, Classifier Loss 0.20223641395568848, Total Loss 156.21246337890625\n",
      "54: Encoding Loss 16.25991439819336, Transition Loss 5.387115955352783, Classifier Loss 0.2009197622537613, Total Loss 151.2487335205078\n",
      "54: Encoding Loss 16.272680282592773, Transition Loss 5.013310432434082, Classifier Loss 0.19537261128425598, Total Loss 150.7213592529297\n",
      "54: Encoding Loss 15.770783424377441, Transition Loss 5.76323127746582, Classifier Loss 0.20154759287834167, Total Loss 147.4736785888672\n",
      "54: Encoding Loss 17.479703903198242, Transition Loss 4.898262977600098, Classifier Loss 0.21166770160198212, Total Loss 161.9840545654297\n",
      "54: Encoding Loss 14.013136863708496, Transition Loss 6.116426944732666, Classifier Loss 0.22099629044532776, Total Loss 135.42800903320312\n",
      "54: Encoding Loss 16.89264488220215, Transition Loss 4.592582702636719, Classifier Loss 0.21434524655342102, Total Loss 157.49420166015625\n",
      "54: Encoding Loss 17.21061897277832, Transition Loss 5.434532165527344, Classifier Loss 0.23218713700771332, Total Loss 161.99057006835938\n",
      "54: Encoding Loss 15.570267677307129, Transition Loss 4.899580001831055, Classifier Loss 0.2441895604133606, Total Loss 149.96102905273438\n",
      "54: Encoding Loss 16.688873291015625, Transition Loss 5.113127708435059, Classifier Loss 0.220237597823143, Total Loss 156.557373046875\n",
      "54: Encoding Loss 16.07280731201172, Transition Loss 4.484681606292725, Classifier Loss 0.1915947049856186, Total Loss 148.6388702392578\n",
      "54: Encoding Loss 15.740918159484863, Transition Loss 5.092905521392822, Classifier Loss 0.2367379516363144, Total Loss 150.61972045898438\n",
      "54: Encoding Loss 15.58906078338623, Transition Loss 5.604913711547852, Classifier Loss 0.20172001421451569, Total Loss 146.00547790527344\n",
      "54: Encoding Loss 14.933221817016602, Transition Loss 4.891429424285889, Classifier Loss 0.1941421926021576, Total Loss 139.8582763671875\n",
      "54: Encoding Loss 15.03278636932373, Transition Loss 5.847334384918213, Classifier Loss 0.20108243823051453, Total Loss 141.5399932861328\n",
      "54: Encoding Loss 15.924094200134277, Transition Loss 6.788120746612549, Classifier Loss 0.2138507068157196, Total Loss 150.1354522705078\n",
      "54: Encoding Loss 15.839151382446289, Transition Loss 5.584704399108887, Classifier Loss 0.20562821626663208, Total Loss 148.39297485351562\n",
      "54: Encoding Loss 15.24654769897461, Transition Loss 6.420960903167725, Classifier Loss 0.19550563395023346, Total Loss 142.80714416503906\n",
      "54: Encoding Loss 16.39116859436035, Transition Loss 6.157700538635254, Classifier Loss 0.19136032462120056, Total Loss 151.49691772460938\n",
      "54: Encoding Loss 16.283349990844727, Transition Loss 6.357293128967285, Classifier Loss 0.20284557342529297, Total Loss 151.82281494140625\n",
      "54: Encoding Loss 15.652897834777832, Transition Loss 6.5730061531066895, Classifier Loss 0.21517913043498993, Total Loss 148.05569458007812\n",
      "54: Encoding Loss 15.668460845947266, Transition Loss 5.1104888916015625, Classifier Loss 0.2032065987586975, Total Loss 146.6904296875\n",
      "54: Encoding Loss 14.254156112670898, Transition Loss 8.917394638061523, Classifier Loss 0.2009536623954773, Total Loss 135.91209411621094\n",
      "54: Encoding Loss 15.54279899597168, Transition Loss 5.24410343170166, Classifier Loss 0.21906955540180206, Total Loss 147.29818725585938\n",
      "54: Encoding Loss 15.588933944702148, Transition Loss 3.520540952682495, Classifier Loss 0.19584941864013672, Total Loss 145.00051879882812\n",
      "54: Encoding Loss 15.287332534790039, Transition Loss 5.544878959655762, Classifier Loss 0.19237127900123596, Total Loss 142.64476013183594\n",
      "54: Encoding Loss 15.865671157836914, Transition Loss 6.23318338394165, Classifier Loss 0.20323458313941956, Total Loss 148.49546813964844\n",
      "54: Encoding Loss 15.992025375366211, Transition Loss 6.720573902130127, Classifier Loss 0.2058788239955902, Total Loss 149.8682098388672\n",
      "54: Encoding Loss 15.862393379211426, Transition Loss 6.85286808013916, Classifier Loss 0.2001170516014099, Total Loss 148.28143310546875\n",
      "54: Encoding Loss 14.55749797821045, Transition Loss 7.2906341552734375, Classifier Loss 0.20713269710540771, Total Loss 138.63137817382812\n",
      "54: Encoding Loss 15.648865699768066, Transition Loss 6.235788345336914, Classifier Loss 0.1933332085609436, Total Loss 145.77142333984375\n",
      "54: Encoding Loss 14.694156646728516, Transition Loss 8.158684730529785, Classifier Loss 0.19923163950443268, Total Loss 139.108154296875\n",
      "54: Encoding Loss 15.742548942565918, Transition Loss 6.766140937805176, Classifier Loss 0.20122313499450684, Total Loss 147.4159393310547\n",
      "54: Encoding Loss 14.82532024383545, Transition Loss 3.323850154876709, Classifier Loss 0.21539057791233063, Total Loss 140.80638122558594\n",
      "54: Encoding Loss 14.49747371673584, Transition Loss 6.61023473739624, Classifier Loss 0.20584455132484436, Total Loss 137.88629150390625\n",
      "54: Encoding Loss 14.637374877929688, Transition Loss 4.035549163818359, Classifier Loss 0.21793192625045776, Total Loss 139.69931030273438\n",
      "54: Encoding Loss 15.473193168640137, Transition Loss 6.641550540924072, Classifier Loss 0.1919916421175003, Total Loss 144.31301879882812\n",
      "54: Encoding Loss 15.246334075927734, Transition Loss 8.340092658996582, Classifier Loss 0.20591118931770325, Total Loss 144.2298126220703\n",
      "54: Encoding Loss 16.441247940063477, Transition Loss 5.5110321044921875, Classifier Loss 0.1960323601961136, Total Loss 152.2354278564453\n",
      "54: Encoding Loss 15.596110343933105, Transition Loss 6.9785919189453125, Classifier Loss 0.19712315499782562, Total Loss 145.87692260742188\n",
      "54: Encoding Loss 14.519327163696289, Transition Loss 6.501984596252441, Classifier Loss 0.22787749767303467, Total Loss 140.24276733398438\n",
      "54: Encoding Loss 15.604231834411621, Transition Loss 4.921494960784912, Classifier Loss 0.20601984858512878, Total Loss 146.42013549804688\n",
      "54: Encoding Loss 15.273852348327637, Transition Loss 4.296047210693359, Classifier Loss 0.1995851844549179, Total Loss 143.008544921875\n",
      "54: Encoding Loss 15.640726089477539, Transition Loss 3.432506561279297, Classifier Loss 0.21043506264686584, Total Loss 146.85581970214844\n",
      "54: Encoding Loss 16.693836212158203, Transition Loss 4.519832611083984, Classifier Loss 0.2098892480134964, Total Loss 155.44357299804688\n",
      "54: Encoding Loss 16.136564254760742, Transition Loss 6.977283954620361, Classifier Loss 0.21643832325935364, Total Loss 152.13180541992188\n",
      "54: Encoding Loss 15.383805274963379, Transition Loss 7.35496187210083, Classifier Loss 0.2046239674091339, Total Loss 145.00384521484375\n",
      "54: Encoding Loss 16.67316436767578, Transition Loss 6.691397666931152, Classifier Loss 0.22404548525810242, Total Loss 157.1281280517578\n",
      "54: Encoding Loss 15.954583168029785, Transition Loss 5.733409881591797, Classifier Loss 0.19354650378227234, Total Loss 148.13800048828125\n",
      "54: Encoding Loss 16.036657333374023, Transition Loss 6.369866371154785, Classifier Loss 0.1813933402299881, Total Loss 147.70655822753906\n",
      "54: Encoding Loss 16.587810516357422, Transition Loss 4.38464879989624, Classifier Loss 0.20608805119991302, Total Loss 154.188232421875\n",
      "54: Encoding Loss 16.335739135742188, Transition Loss 5.004117965698242, Classifier Loss 0.19002221524715424, Total Loss 150.68896484375\n",
      "54: Encoding Loss 15.663558006286621, Transition Loss 4.073596000671387, Classifier Loss 0.19792144000530243, Total Loss 145.9153289794922\n",
      "54: Encoding Loss 16.520450592041016, Transition Loss 7.112308025360107, Classifier Loss 0.22550082206726074, Total Loss 156.13613891601562\n",
      "54: Encoding Loss 14.990877151489258, Transition Loss 6.50955867767334, Classifier Loss 0.18772071599960327, Total Loss 140.00100708007812\n",
      "54: Encoding Loss 14.844868659973145, Transition Loss 5.484021186828613, Classifier Loss 0.21798044443130493, Total Loss 141.65379333496094\n",
      "54: Encoding Loss 15.327640533447266, Transition Loss 4.930835723876953, Classifier Loss 0.20350070297718048, Total Loss 143.9573516845703\n",
      "54: Encoding Loss 14.94594955444336, Transition Loss 5.370818614959717, Classifier Loss 0.19140248000621796, Total Loss 139.78201293945312\n",
      "54: Encoding Loss 14.798657417297363, Transition Loss 4.5253705978393555, Classifier Loss 0.1933775246143341, Total Loss 138.63209533691406\n",
      "54: Encoding Loss 15.54629135131836, Transition Loss 4.996455669403076, Classifier Loss 0.18156225979328156, Total Loss 143.52586364746094\n",
      "54: Encoding Loss 16.747825622558594, Transition Loss 6.3112263679504395, Classifier Loss 0.20771072804927826, Total Loss 156.01593017578125\n",
      "54: Encoding Loss 17.27037811279297, Transition Loss 5.6697282791137695, Classifier Loss 0.22872412204742432, Total Loss 162.16937255859375\n",
      "54: Encoding Loss 15.116229057312012, Transition Loss 4.004027366638184, Classifier Loss 0.21242471039295197, Total Loss 142.97311401367188\n",
      "54: Encoding Loss 14.958734512329102, Transition Loss 4.48354959487915, Classifier Loss 0.1878550946712494, Total Loss 139.3520965576172\n",
      "54: Encoding Loss 15.537738800048828, Transition Loss 6.376011848449707, Classifier Loss 0.22613009810447693, Total Loss 148.19012451171875\n",
      "54: Encoding Loss 14.899106979370117, Transition Loss 5.224864959716797, Classifier Loss 0.19767676293849945, Total Loss 140.00550842285156\n",
      "54: Encoding Loss 15.912458419799805, Transition Loss 4.913308143615723, Classifier Loss 0.20733021199703217, Total Loss 149.01536560058594\n",
      "54: Encoding Loss 16.402267456054688, Transition Loss 5.01561164855957, Classifier Loss 0.18326087296009064, Total Loss 150.54734802246094\n",
      "54: Encoding Loss 15.636958122253418, Transition Loss 6.348461151123047, Classifier Loss 0.21104702353477478, Total Loss 147.47006225585938\n",
      "54: Encoding Loss 15.401389122009277, Transition Loss 5.5637969970703125, Classifier Loss 0.21823228895664215, Total Loss 146.14710998535156\n",
      "54: Encoding Loss 16.808408737182617, Transition Loss 5.524205684661865, Classifier Loss 0.20127877593040466, Total Loss 155.6999969482422\n",
      "54: Encoding Loss 15.70532512664795, Transition Loss 6.382745742797852, Classifier Loss 0.20148833096027374, Total Loss 147.06797790527344\n",
      "54: Encoding Loss 15.510624885559082, Transition Loss 6.2099785804748535, Classifier Loss 0.18941225111484528, Total Loss 144.26821899414062\n",
      "54: Encoding Loss 16.50385284423828, Transition Loss 6.487565040588379, Classifier Loss 0.19612953066825867, Total Loss 152.9412841796875\n",
      "54: Encoding Loss 14.576969146728516, Transition Loss 7.789935111999512, Classifier Loss 0.23850233852863312, Total Loss 142.02395629882812\n",
      "54: Encoding Loss 15.724333763122559, Transition Loss 5.721141815185547, Classifier Loss 0.2090190201997757, Total Loss 147.84078979492188\n",
      "54: Encoding Loss 15.320798873901367, Transition Loss 7.114608287811279, Classifier Loss 0.19466355443000793, Total Loss 143.4556884765625\n",
      "54: Encoding Loss 15.649198532104492, Transition Loss 6.6672587394714355, Classifier Loss 0.20899611711502075, Total Loss 147.42665100097656\n",
      "54: Encoding Loss 15.205885887145996, Transition Loss 7.087708473205566, Classifier Loss 0.20011895895004272, Total Loss 143.07652282714844\n",
      "54: Encoding Loss 16.92763900756836, Transition Loss 5.474464416503906, Classifier Loss 0.2014434039592743, Total Loss 156.6603546142578\n",
      "54: Encoding Loss 15.29035472869873, Transition Loss 6.183773994445801, Classifier Loss 0.20167124271392822, Total Loss 143.72671508789062\n",
      "54: Encoding Loss 16.17736053466797, Transition Loss 5.611734390258789, Classifier Loss 0.20952108502388, Total Loss 151.4933319091797\n",
      "54: Encoding Loss 15.66118335723877, Transition Loss 6.591455459594727, Classifier Loss 0.2021755427122116, Total Loss 146.8253173828125\n",
      "54: Encoding Loss 15.60201644897461, Transition Loss 6.107481002807617, Classifier Loss 0.20305190980434418, Total Loss 146.34283447265625\n",
      "54: Encoding Loss 16.651390075683594, Transition Loss 5.357048511505127, Classifier Loss 0.19476759433746338, Total Loss 153.75929260253906\n",
      "54: Encoding Loss 15.184346199035645, Transition Loss 5.824671268463135, Classifier Loss 0.1883433759212494, Total Loss 141.4740447998047\n",
      "54: Encoding Loss 15.532876968383789, Transition Loss 4.874814033508301, Classifier Loss 0.2093782126903534, Total Loss 146.17579650878906\n",
      "54: Encoding Loss 15.429250717163086, Transition Loss 7.287420272827148, Classifier Loss 0.23046578466892242, Total Loss 147.93807983398438\n",
      "54: Encoding Loss 15.10167407989502, Transition Loss 5.060263633728027, Classifier Loss 0.20852093398571014, Total Loss 142.6775360107422\n",
      "54: Encoding Loss 15.471856117248535, Transition Loss 4.132591247558594, Classifier Loss 0.1977924257516861, Total Loss 144.38063049316406\n",
      "54: Encoding Loss 16.09343910217285, Transition Loss 5.592864036560059, Classifier Loss 0.1940329223871231, Total Loss 149.26937866210938\n",
      "54: Encoding Loss 15.757259368896484, Transition Loss 7.075530529022217, Classifier Loss 0.24680282175540924, Total Loss 152.15347290039062\n",
      "54: Encoding Loss 15.91290283203125, Transition Loss 7.012193202972412, Classifier Loss 0.20338653028011322, Total Loss 149.0443115234375\n",
      "54: Encoding Loss 16.696916580200195, Transition Loss 5.2765703201293945, Classifier Loss 0.18267127871513367, Total Loss 152.8977813720703\n",
      "54: Encoding Loss 16.488008499145508, Transition Loss 6.019917011260986, Classifier Loss 0.19545474648475647, Total Loss 152.6535186767578\n",
      "54: Encoding Loss 15.22716236114502, Transition Loss 3.993558883666992, Classifier Loss 0.20567956566810608, Total Loss 143.1839599609375\n",
      "54: Encoding Loss 15.798504829406738, Transition Loss 6.947972774505615, Classifier Loss 0.2196185141801834, Total Loss 149.73947143554688\n",
      "54: Encoding Loss 16.044963836669922, Transition Loss 5.069817543029785, Classifier Loss 0.20823487639427185, Total Loss 150.19717407226562\n",
      "54: Encoding Loss 15.355342864990234, Transition Loss 7.952461242675781, Classifier Loss 0.22073407471179962, Total Loss 146.50665283203125\n",
      "54: Encoding Loss 16.2219295501709, Transition Loss 4.153175354003906, Classifier Loss 0.20326580107212067, Total Loss 150.93264770507812\n",
      "54: Encoding Loss 15.741951942443848, Transition Loss 5.465336799621582, Classifier Loss 0.19766969978809357, Total Loss 146.795654296875\n",
      "54: Encoding Loss 16.392276763916016, Transition Loss 5.535529613494873, Classifier Loss 0.21612417697906494, Total Loss 153.85772705078125\n",
      "54: Encoding Loss 15.723769187927246, Transition Loss 7.918639659881592, Classifier Loss 0.20974186062812805, Total Loss 148.3480682373047\n",
      "54: Encoding Loss 14.806066513061523, Transition Loss 6.452091217041016, Classifier Loss 0.19359338283538818, Total Loss 139.09829711914062\n",
      "54: Encoding Loss 15.3038969039917, Transition Loss 6.537234306335449, Classifier Loss 0.19860795140266418, Total Loss 143.59942626953125\n",
      "54: Encoding Loss 14.645105361938477, Transition Loss 8.32863712310791, Classifier Loss 0.18845148384571075, Total Loss 137.67172241210938\n",
      "54: Encoding Loss 15.816506385803223, Transition Loss 6.627724647521973, Classifier Loss 0.21070080995559692, Total Loss 148.9276885986328\n",
      "54: Encoding Loss 15.588680267333984, Transition Loss 6.053341388702393, Classifier Loss 0.20575815439224243, Total Loss 146.4959259033203\n",
      "54: Encoding Loss 14.047460556030273, Transition Loss 6.065810203552246, Classifier Loss 0.20421910285949707, Total Loss 134.0147705078125\n",
      "54: Encoding Loss 14.991296768188477, Transition Loss 6.33836030960083, Classifier Loss 0.19569437205791473, Total Loss 140.76748657226562\n",
      "54: Encoding Loss 15.730359077453613, Transition Loss 6.6418023109436035, Classifier Loss 0.20616117119789124, Total Loss 147.78733825683594\n",
      "54: Encoding Loss 16.338991165161133, Transition Loss 6.2695417404174805, Classifier Loss 0.21561749279499054, Total Loss 153.527587890625\n",
      "54: Encoding Loss 15.412907600402832, Transition Loss 6.255422592163086, Classifier Loss 0.20040777325630188, Total Loss 144.59512329101562\n",
      "54: Encoding Loss 15.815862655639648, Transition Loss 4.726376533508301, Classifier Loss 0.19068944454193115, Total Loss 146.5411376953125\n",
      "54: Encoding Loss 15.199399948120117, Transition Loss 4.95837926864624, Classifier Loss 0.22549763321876526, Total Loss 145.1366424560547\n",
      "54: Encoding Loss 15.72944450378418, Transition Loss 6.4300103187561035, Classifier Loss 0.19381673634052277, Total Loss 146.50323486328125\n",
      "54: Encoding Loss 16.416963577270508, Transition Loss 6.3094024658203125, Classifier Loss 0.21148531138896942, Total Loss 153.74612426757812\n",
      "54: Encoding Loss 15.560870170593262, Transition Loss 6.016808032989502, Classifier Loss 0.20665833353996277, Total Loss 146.35617065429688\n",
      "54: Encoding Loss 15.696272850036621, Transition Loss 6.164496898651123, Classifier Loss 0.2049894630908966, Total Loss 147.30201721191406\n",
      "54: Encoding Loss 15.937365531921387, Transition Loss 5.464012622833252, Classifier Loss 0.20604337751865387, Total Loss 149.19606018066406\n",
      "54: Encoding Loss 16.346942901611328, Transition Loss 4.353830814361572, Classifier Loss 0.1991368532180786, Total Loss 151.55999755859375\n",
      "54: Encoding Loss 15.8531494140625, Transition Loss 4.471967697143555, Classifier Loss 0.20014937222003937, Total Loss 147.73452758789062\n",
      "54: Encoding Loss 15.995518684387207, Transition Loss 6.523129463195801, Classifier Loss 0.191568061709404, Total Loss 148.4255828857422\n",
      "54: Encoding Loss 16.779823303222656, Transition Loss 4.292093753814697, Classifier Loss 0.20856982469558716, Total Loss 155.9539794921875\n",
      "54: Encoding Loss 17.17948341369629, Transition Loss 4.378098011016846, Classifier Loss 0.2285138964653015, Total Loss 161.1628875732422\n",
      "54: Encoding Loss 14.863286972045898, Transition Loss 6.542381763458252, Classifier Loss 0.19680246710777283, Total Loss 139.89501953125\n",
      "54: Encoding Loss 14.730218887329102, Transition Loss 6.6204023361206055, Classifier Loss 0.1945982426404953, Total Loss 138.6256561279297\n",
      "54: Encoding Loss 15.121933937072754, Transition Loss 6.092416763305664, Classifier Loss 0.2195006161928177, Total Loss 144.14402770996094\n",
      "54: Encoding Loss 15.168102264404297, Transition Loss 6.619521141052246, Classifier Loss 0.23036105930805206, Total Loss 145.704833984375\n",
      "54: Encoding Loss 14.966861724853516, Transition Loss 8.839706420898438, Classifier Loss 0.2042795717716217, Total Loss 141.9307861328125\n",
      "54: Encoding Loss 16.6438045501709, Transition Loss 5.602261543273926, Classifier Loss 0.21349194645881653, Total Loss 155.62008666992188\n",
      "54: Encoding Loss 16.549192428588867, Transition Loss 5.238219261169434, Classifier Loss 0.18325047194957733, Total Loss 151.76622009277344\n",
      "54: Encoding Loss 14.812180519104004, Transition Loss 4.880261421203613, Classifier Loss 0.19135437905788422, Total Loss 138.60894775390625\n",
      "54: Encoding Loss 15.339280128479004, Transition Loss 4.662576675415039, Classifier Loss 0.21138527989387512, Total Loss 144.78529357910156\n",
      "54: Encoding Loss 15.375448226928711, Transition Loss 5.465583801269531, Classifier Loss 0.21144458651542664, Total Loss 145.24118041992188\n",
      "54: Encoding Loss 15.825835227966309, Transition Loss 6.184300899505615, Classifier Loss 0.20079748332500458, Total Loss 147.9232940673828\n",
      "54: Encoding Loss 15.812455177307129, Transition Loss 4.7017502784729, Classifier Loss 0.19978047907352448, Total Loss 147.4180450439453\n",
      "54: Encoding Loss 15.857171058654785, Transition Loss 5.473848819732666, Classifier Loss 0.21346208453178406, Total Loss 149.29835510253906\n",
      "54: Encoding Loss 14.928131103515625, Transition Loss 4.5806427001953125, Classifier Loss 0.17871931195259094, Total Loss 138.21310424804688\n",
      "54: Encoding Loss 15.770780563354492, Transition Loss 6.317221641540527, Classifier Loss 0.18559813499450684, Total Loss 145.98951721191406\n",
      "54: Encoding Loss 16.066530227661133, Transition Loss 4.055196762084961, Classifier Loss 0.22806626558303833, Total Loss 152.14990234375\n",
      "54: Encoding Loss 15.099291801452637, Transition Loss 7.218251705169678, Classifier Loss 0.21867425739765167, Total Loss 144.10540771484375\n",
      "54: Encoding Loss 15.764662742614746, Transition Loss 5.026646614074707, Classifier Loss 0.2100713849067688, Total Loss 148.1297607421875\n",
      "54: Encoding Loss 15.960552215576172, Transition Loss 6.604922294616699, Classifier Loss 0.21862876415252686, Total Loss 150.8682861328125\n",
      "54: Encoding Loss 15.7969331741333, Transition Loss 5.144043922424316, Classifier Loss 0.20739637315273285, Total Loss 148.14390563964844\n",
      "54: Encoding Loss 15.211833000183105, Transition Loss 6.1757941246032715, Classifier Loss 0.19431018829345703, Total Loss 142.36083984375\n",
      "54: Encoding Loss 16.937458038330078, Transition Loss 4.104753494262695, Classifier Loss 0.20180006325244904, Total Loss 156.50062561035156\n",
      "54: Encoding Loss 16.175683975219727, Transition Loss 3.888753890991211, Classifier Loss 0.20543771982192993, Total Loss 150.7270050048828\n",
      "54: Encoding Loss 15.497776985168457, Transition Loss 4.65744161605835, Classifier Loss 0.1981552541255951, Total Loss 144.72923278808594\n",
      "54: Encoding Loss 15.150331497192383, Transition Loss 6.242549419403076, Classifier Loss 0.20584940910339355, Total Loss 143.03610229492188\n",
      "54: Encoding Loss 15.258548736572266, Transition Loss 3.458960771560669, Classifier Loss 0.20584514737129211, Total Loss 143.3446807861328\n",
      "54: Encoding Loss 15.992419242858887, Transition Loss 7.98856258392334, Classifier Loss 0.2244226634502411, Total Loss 151.97933959960938\n",
      "54: Encoding Loss 16.241514205932617, Transition Loss 5.191398620605469, Classifier Loss 0.1932637095451355, Total Loss 150.29676818847656\n",
      "54: Encoding Loss 17.104320526123047, Transition Loss 5.994466304779053, Classifier Loss 0.2074326127767563, Total Loss 158.7767333984375\n",
      "54: Encoding Loss 16.632577896118164, Transition Loss 6.725102424621582, Classifier Loss 0.19785277545452118, Total Loss 154.19091796875\n",
      "54: Encoding Loss 15.278679847717285, Transition Loss 8.782873153686523, Classifier Loss 0.1923273652791977, Total Loss 143.21876525878906\n",
      "54: Encoding Loss 13.079116821289062, Transition Loss 9.479093551635742, Classifier Loss 0.18789249658584595, Total Loss 125.31800079345703\n",
      "55: Encoding Loss 16.886730194091797, Transition Loss 4.968533515930176, Classifier Loss 0.1967400312423706, Total Loss 155.76156616210938\n",
      "55: Encoding Loss 16.25319480895996, Transition Loss 5.334686279296875, Classifier Loss 0.19845934212207794, Total Loss 150.9384307861328\n",
      "55: Encoding Loss 16.271060943603516, Transition Loss 5.246232986450195, Classifier Loss 0.20238271355628967, Total Loss 151.45599365234375\n",
      "55: Encoding Loss 15.757608413696289, Transition Loss 5.788250923156738, Classifier Loss 0.1939101219177246, Total Loss 146.60952758789062\n",
      "55: Encoding Loss 17.48223114013672, Transition Loss 5.19375467300415, Classifier Loss 0.2203371375799179, Total Loss 162.93032836914062\n",
      "55: Encoding Loss 13.997014999389648, Transition Loss 6.035911560058594, Classifier Loss 0.20914599299430847, Total Loss 134.09791564941406\n",
      "55: Encoding Loss 16.900667190551758, Transition Loss 4.705109119415283, Classifier Loss 0.21015408635139465, Total Loss 157.16177368164062\n",
      "55: Encoding Loss 17.204065322875977, Transition Loss 5.41496467590332, Classifier Loss 0.2131088376045227, Total Loss 160.02639770507812\n",
      "55: Encoding Loss 15.587675094604492, Transition Loss 4.909639835357666, Classifier Loss 0.23646245896816254, Total Loss 149.32958984375\n",
      "55: Encoding Loss 16.6746826171875, Transition Loss 5.0384674072265625, Classifier Loss 0.22118136286735535, Total Loss 156.52328491210938\n",
      "55: Encoding Loss 16.060897827148438, Transition Loss 4.567699432373047, Classifier Loss 0.19152677059173584, Total Loss 148.55340576171875\n",
      "55: Encoding Loss 15.748476028442383, Transition Loss 5.14364767074585, Classifier Loss 0.22270289063453674, Total Loss 149.28683471679688\n",
      "55: Encoding Loss 15.604610443115234, Transition Loss 5.817352771759033, Classifier Loss 0.20324735343456268, Total Loss 146.32508850097656\n",
      "55: Encoding Loss 14.918599128723145, Transition Loss 5.046436309814453, Classifier Loss 0.19756940007209778, Total Loss 140.11502075195312\n",
      "55: Encoding Loss 15.04052448272705, Transition Loss 6.00307035446167, Classifier Loss 0.20023831725120544, Total Loss 141.5486297607422\n",
      "55: Encoding Loss 15.91528034210205, Transition Loss 6.893239974975586, Classifier Loss 0.21224910020828247, Total Loss 149.92579650878906\n",
      "55: Encoding Loss 15.836166381835938, Transition Loss 5.582472324371338, Classifier Loss 0.20289848744869232, Total Loss 148.09568786621094\n",
      "55: Encoding Loss 15.261594772338867, Transition Loss 6.486299514770508, Classifier Loss 0.1931982934474945, Total Loss 142.70985412597656\n",
      "55: Encoding Loss 16.39299964904785, Transition Loss 6.20352840423584, Classifier Loss 0.19183678925037384, Total Loss 151.56838989257812\n",
      "55: Encoding Loss 16.2744197845459, Transition Loss 6.370240688323975, Classifier Loss 0.1930529922246933, Total Loss 150.7747039794922\n",
      "55: Encoding Loss 15.658617973327637, Transition Loss 6.7127532958984375, Classifier Loss 0.2331867218017578, Total Loss 149.93016052246094\n",
      "55: Encoding Loss 15.674602508544922, Transition Loss 5.248665809631348, Classifier Loss 0.19778718054294586, Total Loss 146.22528076171875\n",
      "55: Encoding Loss 14.240795135498047, Transition Loss 8.86650562286377, Classifier Loss 0.20633625984191895, Total Loss 136.3332977294922\n",
      "55: Encoding Loss 15.539200782775879, Transition Loss 5.129604339599609, Classifier Loss 0.20122823119163513, Total Loss 145.4623565673828\n",
      "55: Encoding Loss 15.589322090148926, Transition Loss 3.518887996673584, Classifier Loss 0.19596923887729645, Total Loss 145.01527404785156\n",
      "55: Encoding Loss 15.286948204040527, Transition Loss 5.794909954071045, Classifier Loss 0.19041380286216736, Total Loss 142.49594116210938\n",
      "55: Encoding Loss 15.853729248046875, Transition Loss 6.045612335205078, Classifier Loss 0.19882245361804962, Total Loss 147.92120361328125\n",
      "55: Encoding Loss 15.989270210266113, Transition Loss 6.972021579742432, Classifier Loss 0.20612402260303497, Total Loss 149.9209747314453\n",
      "55: Encoding Loss 15.867472648620605, Transition Loss 6.794814586639404, Classifier Loss 0.19715073704719543, Total Loss 148.01382446289062\n",
      "55: Encoding Loss 14.555006980895996, Transition Loss 7.491049766540527, Classifier Loss 0.21701011061668396, Total Loss 139.6392822265625\n",
      "55: Encoding Loss 15.639538764953613, Transition Loss 6.107041835784912, Classifier Loss 0.2024974226951599, Total Loss 146.58746337890625\n",
      "55: Encoding Loss 14.715840339660645, Transition Loss 8.747528076171875, Classifier Loss 0.19285276532173157, Total Loss 138.76150512695312\n",
      "55: Encoding Loss 15.75063419342041, Transition Loss 6.429018497467041, Classifier Loss 0.20197388529777527, Total Loss 147.48826599121094\n",
      "55: Encoding Loss 14.812481880187988, Transition Loss 3.4228718280792236, Classifier Loss 0.22849556803703308, Total Loss 142.0339813232422\n",
      "55: Encoding Loss 14.48379898071289, Transition Loss 6.1218109130859375, Classifier Loss 0.20254892110824585, Total Loss 137.34963989257812\n",
      "55: Encoding Loss 14.63945484161377, Transition Loss 4.337085247039795, Classifier Loss 0.21712449193000793, Total Loss 139.6955108642578\n",
      "55: Encoding Loss 15.474246978759766, Transition Loss 6.151157855987549, Classifier Loss 0.19052283465862274, Total Loss 144.07647705078125\n",
      "55: Encoding Loss 15.234911918640137, Transition Loss 8.98263168334961, Classifier Loss 0.21140320599079132, Total Loss 144.81614685058594\n",
      "55: Encoding Loss 16.442584991455078, Transition Loss 5.439823627471924, Classifier Loss 0.1953260898590088, Total Loss 152.1612548828125\n",
      "55: Encoding Loss 15.588159561157227, Transition Loss 7.183131217956543, Classifier Loss 0.19721226394176483, Total Loss 145.86312866210938\n",
      "55: Encoding Loss 14.511686325073242, Transition Loss 6.504812240600586, Classifier Loss 0.22651049494743347, Total Loss 140.04551696777344\n",
      "55: Encoding Loss 15.604511260986328, Transition Loss 4.770701885223389, Classifier Loss 0.21189038455486298, Total Loss 146.97926330566406\n",
      "55: Encoding Loss 15.265436172485352, Transition Loss 4.444915771484375, Classifier Loss 0.19048625230789185, Total Loss 142.06109619140625\n",
      "55: Encoding Loss 15.637415885925293, Transition Loss 3.404161214828491, Classifier Loss 0.20711924135684967, Total Loss 146.49208068847656\n",
      "55: Encoding Loss 16.691987991333008, Transition Loss 4.774252414703369, Classifier Loss 0.20840536057949066, Total Loss 155.33128356933594\n",
      "55: Encoding Loss 16.134790420532227, Transition Loss 6.757019519805908, Classifier Loss 0.21447685360908508, Total Loss 151.87741088867188\n",
      "55: Encoding Loss 15.379921913146973, Transition Loss 7.774082183837891, Classifier Loss 0.19750462472438812, Total Loss 144.3446502685547\n",
      "55: Encoding Loss 16.668224334716797, Transition Loss 6.583364009857178, Classifier Loss 0.21976132690906525, Total Loss 156.63861083984375\n",
      "55: Encoding Loss 15.938549041748047, Transition Loss 5.759568214416504, Classifier Loss 0.1943110227584839, Total Loss 148.09141540527344\n",
      "55: Encoding Loss 16.028148651123047, Transition Loss 5.934200286865234, Classifier Loss 0.18194131553173065, Total Loss 147.60618591308594\n",
      "55: Encoding Loss 16.59457015991211, Transition Loss 4.412644863128662, Classifier Loss 0.2044016420841217, Total Loss 154.0792694091797\n",
      "55: Encoding Loss 16.336650848388672, Transition Loss 4.788059234619141, Classifier Loss 0.19829882681369781, Total Loss 151.480712890625\n",
      "55: Encoding Loss 15.677885055541992, Transition Loss 4.161102771759033, Classifier Loss 0.19657506048679352, Total Loss 145.91281127929688\n",
      "55: Encoding Loss 16.507652282714844, Transition Loss 6.970211505889893, Classifier Loss 0.2110098898410797, Total Loss 154.55624389648438\n",
      "55: Encoding Loss 14.983475685119629, Transition Loss 6.83966588973999, Classifier Loss 0.18626609444618225, Total Loss 139.8623504638672\n",
      "55: Encoding Loss 14.844303131103516, Transition Loss 5.396730422973633, Classifier Loss 0.21623045206069946, Total Loss 141.45680236816406\n",
      "55: Encoding Loss 15.317937850952148, Transition Loss 5.113569259643555, Classifier Loss 0.208302304148674, Total Loss 144.39646911621094\n",
      "55: Encoding Loss 14.958992958068848, Transition Loss 5.027173042297363, Classifier Loss 0.1944359391927719, Total Loss 140.1209716796875\n",
      "55: Encoding Loss 14.822229385375977, Transition Loss 4.839050769805908, Classifier Loss 0.19115005433559418, Total Loss 138.66064453125\n",
      "55: Encoding Loss 15.5584077835083, Transition Loss 4.845064163208008, Classifier Loss 0.19439032673835754, Total Loss 144.87530517578125\n",
      "55: Encoding Loss 16.73040008544922, Transition Loss 6.4004316329956055, Classifier Loss 0.20253199338912964, Total Loss 155.37649536132812\n",
      "55: Encoding Loss 17.273963928222656, Transition Loss 5.515193939208984, Classifier Loss 0.2384059876203537, Total Loss 163.13536071777344\n",
      "55: Encoding Loss 15.122416496276855, Transition Loss 4.039658069610596, Classifier Loss 0.20618733763694763, Total Loss 142.406005859375\n",
      "55: Encoding Loss 14.967637062072754, Transition Loss 4.218968868255615, Classifier Loss 0.1887042224407196, Total Loss 139.455322265625\n",
      "55: Encoding Loss 15.539613723754883, Transition Loss 6.317344665527344, Classifier Loss 0.2135925143957138, Total Loss 146.93963623046875\n",
      "55: Encoding Loss 14.883996963500977, Transition Loss 5.18067741394043, Classifier Loss 0.18826492130756378, Total Loss 138.93460083007812\n",
      "55: Encoding Loss 15.912501335144043, Transition Loss 5.177903175354004, Classifier Loss 0.19739554822444916, Total Loss 148.0751495361328\n",
      "55: Encoding Loss 16.405250549316406, Transition Loss 5.046064853668213, Classifier Loss 0.18265396356582642, Total Loss 150.51661682128906\n",
      "55: Encoding Loss 15.621685981750488, Transition Loss 6.636133193969727, Classifier Loss 0.20258073508739471, Total Loss 146.5587921142578\n",
      "55: Encoding Loss 15.38595199584961, Transition Loss 5.390172958374023, Classifier Loss 0.208273246884346, Total Loss 144.99298095703125\n",
      "55: Encoding Loss 16.80286979675293, Transition Loss 5.758577823638916, Classifier Loss 0.2080022543668747, Total Loss 156.37490844726562\n",
      "55: Encoding Loss 15.691083908081055, Transition Loss 6.145793437957764, Classifier Loss 0.19208726286888123, Total Loss 145.96656799316406\n",
      "55: Encoding Loss 15.510488510131836, Transition Loss 6.5491132736206055, Classifier Loss 0.18697699904441833, Total Loss 144.09144592285156\n",
      "55: Encoding Loss 16.499427795410156, Transition Loss 6.383816242218018, Classifier Loss 0.19375213980674744, Total Loss 152.64739990234375\n",
      "55: Encoding Loss 14.605252265930176, Transition Loss 7.913064002990723, Classifier Loss 0.25382643938064575, Total Loss 143.80728149414062\n",
      "55: Encoding Loss 15.719175338745117, Transition Loss 5.500323295593262, Classifier Loss 0.2100645899772644, Total Loss 147.8599395751953\n",
      "55: Encoding Loss 15.315106391906738, Transition Loss 7.2000226974487305, Classifier Loss 0.19517390429973602, Total Loss 143.47824096679688\n",
      "55: Encoding Loss 15.621841430664062, Transition Loss 6.446114540100098, Classifier Loss 0.19934329390525818, Total Loss 146.1982879638672\n",
      "55: Encoding Loss 15.193202018737793, Transition Loss 7.427789211273193, Classifier Loss 0.19836270809173584, Total Loss 142.86744689941406\n",
      "55: Encoding Loss 16.917205810546875, Transition Loss 5.240891456604004, Classifier Loss 0.20731382071971893, Total Loss 157.11720275878906\n",
      "55: Encoding Loss 15.282275199890137, Transition Loss 6.433505058288574, Classifier Loss 0.2061368077993393, Total Loss 144.15858459472656\n",
      "55: Encoding Loss 16.18828582763672, Transition Loss 5.29207706451416, Classifier Loss 0.21921591460704803, Total Loss 152.4862823486328\n",
      "55: Encoding Loss 15.669724464416504, Transition Loss 6.988548278808594, Classifier Loss 0.20295369625091553, Total Loss 147.05087280273438\n",
      "55: Encoding Loss 15.601461410522461, Transition Loss 5.861598014831543, Classifier Loss 0.20074822008609772, Total Loss 146.058837890625\n",
      "55: Encoding Loss 16.652090072631836, Transition Loss 5.597428321838379, Classifier Loss 0.19505837559700012, Total Loss 153.84205627441406\n",
      "55: Encoding Loss 15.183772087097168, Transition Loss 5.683215618133545, Classifier Loss 0.1867154836654663, Total Loss 141.2783660888672\n",
      "55: Encoding Loss 15.542028427124023, Transition Loss 4.986766815185547, Classifier Loss 0.21664875745773315, Total Loss 146.99847412109375\n",
      "55: Encoding Loss 15.43983268737793, Transition Loss 7.026391983032227, Classifier Loss 0.22745652496814728, Total Loss 147.6696014404297\n",
      "55: Encoding Loss 15.101339340209961, Transition Loss 4.998733997344971, Classifier Loss 0.19288498163223267, Total Loss 141.0989532470703\n",
      "55: Encoding Loss 15.456525802612305, Transition Loss 3.921828269958496, Classifier Loss 0.2039671242237091, Total Loss 144.83328247070312\n",
      "55: Encoding Loss 16.08884048461914, Transition Loss 5.457249641418457, Classifier Loss 0.1845073103904724, Total Loss 148.25289916992188\n",
      "55: Encoding Loss 15.750229835510254, Transition Loss 6.851174831390381, Classifier Loss 0.24637314677238464, Total Loss 152.0093994140625\n",
      "55: Encoding Loss 15.918417930603027, Transition Loss 6.770238876342773, Classifier Loss 0.20342786610126495, Total Loss 149.04417419433594\n",
      "55: Encoding Loss 16.69399642944336, Transition Loss 5.061125755310059, Classifier Loss 0.1867830455303192, Total Loss 153.2425079345703\n",
      "55: Encoding Loss 16.491090774536133, Transition Loss 5.837932109832764, Classifier Loss 0.19177979230880737, Total Loss 152.2742919921875\n",
      "55: Encoding Loss 15.238150596618652, Transition Loss 3.9425854682922363, Classifier Loss 0.1983809471130371, Total Loss 142.5318145751953\n",
      "55: Encoding Loss 15.793110847473145, Transition Loss 6.92517614364624, Classifier Loss 0.22514089941978455, Total Loss 150.2440185546875\n",
      "55: Encoding Loss 16.04279136657715, Transition Loss 5.215273857116699, Classifier Loss 0.21537765860557556, Total Loss 150.92315673828125\n",
      "55: Encoding Loss 15.35615348815918, Transition Loss 7.717208385467529, Classifier Loss 0.21540571749210358, Total Loss 145.93324279785156\n",
      "55: Encoding Loss 16.23098373413086, Transition Loss 4.157320499420166, Classifier Loss 0.2054326981306076, Total Loss 151.22262573242188\n",
      "55: Encoding Loss 15.73924732208252, Transition Loss 5.20631217956543, Classifier Loss 0.19236063957214355, Total Loss 146.19129943847656\n",
      "55: Encoding Loss 16.389612197875977, Transition Loss 5.705799579620361, Classifier Loss 0.2245340794324875, Total Loss 154.7114715576172\n",
      "55: Encoding Loss 15.728838920593262, Transition Loss 7.482892036437988, Classifier Loss 0.2016063630580902, Total Loss 147.48793029785156\n",
      "55: Encoding Loss 14.799543380737305, Transition Loss 6.545205116271973, Classifier Loss 0.1876770257949829, Total Loss 138.4730987548828\n",
      "55: Encoding Loss 15.291606903076172, Transition Loss 6.055474758148193, Classifier Loss 0.18622547388076782, Total Loss 142.16650390625\n",
      "55: Encoding Loss 14.642752647399902, Transition Loss 8.895689964294434, Classifier Loss 0.19264379143714905, Total Loss 138.185546875\n",
      "55: Encoding Loss 15.833043098449707, Transition Loss 6.040961265563965, Classifier Loss 0.19944587349891663, Total Loss 147.81712341308594\n",
      "55: Encoding Loss 15.59798812866211, Transition Loss 6.623084545135498, Classifier Loss 0.20745138823986053, Total Loss 146.85366821289062\n",
      "55: Encoding Loss 14.053994178771973, Transition Loss 5.598000526428223, Classifier Loss 0.20727619528770447, Total Loss 134.2791748046875\n",
      "55: Encoding Loss 14.993746757507324, Transition Loss 7.024354934692383, Classifier Loss 0.1924184113740921, Total Loss 140.59669494628906\n",
      "55: Encoding Loss 15.738492965698242, Transition Loss 6.107879161834717, Classifier Loss 0.19895270466804504, Total Loss 147.02479553222656\n",
      "55: Encoding Loss 16.325313568115234, Transition Loss 6.665140151977539, Classifier Loss 0.2161404937505722, Total Loss 153.54959106445312\n",
      "55: Encoding Loss 15.40369987487793, Transition Loss 6.030548095703125, Classifier Loss 0.20444321632385254, Total Loss 144.88003540039062\n",
      "55: Encoding Loss 15.822249412536621, Transition Loss 4.80893611907959, Classifier Loss 0.19643229246139526, Total Loss 147.18301391601562\n",
      "55: Encoding Loss 15.198958396911621, Transition Loss 4.506608009338379, Classifier Loss 0.2307528704404831, Total Loss 145.5682830810547\n",
      "55: Encoding Loss 15.72204303741455, Transition Loss 6.668395042419434, Classifier Loss 0.19159966707229614, Total Loss 146.26998901367188\n",
      "55: Encoding Loss 16.415681838989258, Transition Loss 6.1035590171813965, Classifier Loss 0.21884579956531525, Total Loss 154.43075561523438\n",
      "55: Encoding Loss 15.56863021850586, Transition Loss 6.115862846374512, Classifier Loss 0.207614928483963, Total Loss 146.53372192382812\n",
      "55: Encoding Loss 15.710787773132324, Transition Loss 5.934998035430908, Classifier Loss 0.19413354992866516, Total Loss 146.28665161132812\n",
      "55: Encoding Loss 15.939517974853516, Transition Loss 5.558223724365234, Classifier Loss 0.20448848605155945, Total Loss 149.07662963867188\n",
      "55: Encoding Loss 16.358226776123047, Transition Loss 4.203932762145996, Classifier Loss 0.19924667477607727, Total Loss 151.63128662109375\n",
      "55: Encoding Loss 15.860335350036621, Transition Loss 4.431727409362793, Classifier Loss 0.18905752897262573, Total Loss 146.67478942871094\n",
      "55: Encoding Loss 15.990252494812012, Transition Loss 6.256410598754883, Classifier Loss 0.19798146188259125, Total Loss 148.97145080566406\n",
      "55: Encoding Loss 16.786222457885742, Transition Loss 4.233788967132568, Classifier Loss 0.20578879117965698, Total Loss 155.7154083251953\n",
      "55: Encoding Loss 17.181324005126953, Transition Loss 4.267857551574707, Classifier Loss 0.21909093856811523, Total Loss 160.2132568359375\n",
      "55: Encoding Loss 14.870146751403809, Transition Loss 6.340153694152832, Classifier Loss 0.20499968528747559, Total Loss 140.72918701171875\n",
      "55: Encoding Loss 14.725671768188477, Transition Loss 6.422447681427002, Classifier Loss 0.18861201405525208, Total Loss 137.95106506347656\n",
      "55: Encoding Loss 15.111339569091797, Transition Loss 6.151584625244141, Classifier Loss 0.21400389075279236, Total Loss 143.52142333984375\n",
      "55: Encoding Loss 15.17670726776123, Transition Loss 6.495197772979736, Classifier Loss 0.22537842392921448, Total Loss 145.25054931640625\n",
      "55: Encoding Loss 14.96487045288086, Transition Loss 9.029898643493652, Classifier Loss 0.20377857983112335, Total Loss 141.90281677246094\n",
      "55: Encoding Loss 16.645906448364258, Transition Loss 5.49644660949707, Classifier Loss 0.2052244395017624, Total Loss 154.7889862060547\n",
      "55: Encoding Loss 16.553752899169922, Transition Loss 5.411121368408203, Classifier Loss 0.17832741141319275, Total Loss 151.3450164794922\n",
      "55: Encoding Loss 14.812332153320312, Transition Loss 4.912017345428467, Classifier Loss 0.1897827535867691, Total Loss 138.45933532714844\n",
      "55: Encoding Loss 15.341410636901855, Transition Loss 4.800714492797852, Classifier Loss 0.20807163417339325, Total Loss 144.49859619140625\n",
      "55: Encoding Loss 15.368474960327148, Transition Loss 5.43984842300415, Classifier Loss 0.1992519348859787, Total Loss 143.96096801757812\n",
      "55: Encoding Loss 15.829910278320312, Transition Loss 6.303325176239014, Classifier Loss 0.19805167615413666, Total Loss 147.7051239013672\n",
      "55: Encoding Loss 15.811614990234375, Transition Loss 4.54268217086792, Classifier Loss 0.19818897545337677, Total Loss 147.22035217285156\n",
      "55: Encoding Loss 15.863324165344238, Transition Loss 5.608847141265869, Classifier Loss 0.22856543958187103, Total Loss 150.88490295410156\n",
      "55: Encoding Loss 14.929754257202148, Transition Loss 4.269268035888672, Classifier Loss 0.1819053292274475, Total Loss 138.482421875\n",
      "55: Encoding Loss 15.752674102783203, Transition Loss 6.37333869934082, Classifier Loss 0.1877962350845337, Total Loss 146.07568359375\n",
      "55: Encoding Loss 16.05866241455078, Transition Loss 3.7581756114959717, Classifier Loss 0.22575673460960388, Total Loss 151.79660034179688\n",
      "55: Encoding Loss 15.085888862609863, Transition Loss 7.526335716247559, Classifier Loss 0.21549254655838013, Total Loss 143.7416229248047\n",
      "55: Encoding Loss 15.771093368530273, Transition Loss 4.696453094482422, Classifier Loss 0.20416182279586792, Total Loss 147.5242156982422\n",
      "55: Encoding Loss 15.943639755249023, Transition Loss 7.136972427368164, Classifier Loss 0.220189169049263, Total Loss 150.9954376220703\n",
      "55: Encoding Loss 15.79215145111084, Transition Loss 5.040124416351318, Classifier Loss 0.1996559053659439, Total Loss 147.31082153320312\n",
      "55: Encoding Loss 15.21213150024414, Transition Loss 6.369019985198975, Classifier Loss 0.19256415963172913, Total Loss 142.22726440429688\n",
      "55: Encoding Loss 16.945606231689453, Transition Loss 3.852806806564331, Classifier Loss 0.2046777307987213, Total Loss 156.80319213867188\n",
      "55: Encoding Loss 16.171594619750977, Transition Loss 3.952883243560791, Classifier Loss 0.20528452098369598, Total Loss 150.69178771972656\n",
      "55: Encoding Loss 15.501664161682129, Transition Loss 4.5054826736450195, Classifier Loss 0.19329415261745453, Total Loss 144.24383544921875\n",
      "55: Encoding Loss 15.141472816467285, Transition Loss 6.432300567626953, Classifier Loss 0.2110428810119629, Total Loss 143.5225372314453\n",
      "55: Encoding Loss 15.268223762512207, Transition Loss 3.331231117248535, Classifier Loss 0.21257434785366058, Total Loss 144.06947326660156\n",
      "55: Encoding Loss 15.975077629089355, Transition Loss 8.295473098754883, Classifier Loss 0.2177349030971527, Total Loss 151.2332000732422\n",
      "55: Encoding Loss 16.233745574951172, Transition Loss 4.841393947601318, Classifier Loss 0.19581148028373718, Total Loss 150.41940307617188\n",
      "55: Encoding Loss 17.093923568725586, Transition Loss 6.359555721282959, Classifier Loss 0.20068654417991638, Total Loss 158.09194946289062\n",
      "55: Encoding Loss 16.62492561340332, Transition Loss 6.06640625, Classifier Loss 0.18855738639831543, Total Loss 153.0684356689453\n",
      "55: Encoding Loss 15.290154457092285, Transition Loss 9.166028022766113, Classifier Loss 0.20465904474258423, Total Loss 144.62034606933594\n",
      "55: Encoding Loss 13.060080528259277, Transition Loss 8.524710655212402, Classifier Loss 0.18588218092918396, Total Loss 124.7738037109375\n",
      "56: Encoding Loss 16.891809463500977, Transition Loss 5.18212890625, Classifier Loss 0.1946565806865692, Total Loss 155.6365509033203\n",
      "56: Encoding Loss 16.250951766967773, Transition Loss 4.890699863433838, Classifier Loss 0.20355042815208435, Total Loss 151.34078979492188\n",
      "56: Encoding Loss 16.27165412902832, Transition Loss 5.290156364440918, Classifier Loss 0.20589879155158997, Total Loss 151.82113647460938\n",
      "56: Encoding Loss 15.75794792175293, Transition Loss 5.352865219116211, Classifier Loss 0.2023553103208542, Total Loss 147.36968994140625\n",
      "56: Encoding Loss 17.488229751586914, Transition Loss 5.259432315826416, Classifier Loss 0.2055530995130539, Total Loss 161.51303100585938\n",
      "56: Encoding Loss 14.011478424072266, Transition Loss 5.691918849945068, Classifier Loss 0.2074015587568283, Total Loss 133.97035217285156\n",
      "56: Encoding Loss 16.88709259033203, Transition Loss 4.671078681945801, Classifier Loss 0.21497482061386108, Total Loss 157.5284423828125\n",
      "56: Encoding Loss 17.20680809020996, Transition Loss 5.277474403381348, Classifier Loss 0.21152302622795105, Total Loss 159.8622589111328\n",
      "56: Encoding Loss 15.56628704071045, Transition Loss 4.988734722137451, Classifier Loss 0.2521277070045471, Total Loss 150.74081420898438\n",
      "56: Encoding Loss 16.68395233154297, Transition Loss 4.956965923309326, Classifier Loss 0.2117622196674347, Total Loss 155.6392364501953\n",
      "56: Encoding Loss 16.067289352416992, Transition Loss 4.472245216369629, Classifier Loss 0.19172587990760803, Total Loss 148.60536193847656\n",
      "56: Encoding Loss 15.736478805541992, Transition Loss 5.038857936859131, Classifier Loss 0.2252560555934906, Total Loss 149.4252166748047\n",
      "56: Encoding Loss 15.606014251708984, Transition Loss 5.767653465270996, Classifier Loss 0.2001733034849167, Total Loss 146.01898193359375\n",
      "56: Encoding Loss 14.933114051818848, Transition Loss 4.98493766784668, Classifier Loss 0.19588452577590942, Total Loss 140.0503692626953\n",
      "56: Encoding Loss 15.034699440002441, Transition Loss 5.9381608963012695, Classifier Loss 0.19459711015224457, Total Loss 140.92495727539062\n",
      "56: Encoding Loss 15.915681838989258, Transition Loss 6.781100749969482, Classifier Loss 0.21311315894126892, Total Loss 149.99298095703125\n",
      "56: Encoding Loss 15.828465461730957, Transition Loss 5.536158084869385, Classifier Loss 0.20959699153900146, Total Loss 148.6946563720703\n",
      "56: Encoding Loss 15.266695022583008, Transition Loss 6.4203338623046875, Classifier Loss 0.19003453850746155, Total Loss 142.42108154296875\n",
      "56: Encoding Loss 16.387662887573242, Transition Loss 6.181302070617676, Classifier Loss 0.1932579129934311, Total Loss 151.66336059570312\n",
      "56: Encoding Loss 16.27817153930664, Transition Loss 6.337777137756348, Classifier Loss 0.19692003726959229, Total Loss 151.1849365234375\n",
      "56: Encoding Loss 15.64920711517334, Transition Loss 6.709157943725586, Classifier Loss 0.21586069464683533, Total Loss 148.12155151367188\n",
      "56: Encoding Loss 15.664155960083008, Transition Loss 5.199680328369141, Classifier Loss 0.19002754986286163, Total Loss 145.35594177246094\n",
      "56: Encoding Loss 14.233868598937988, Transition Loss 8.961594581604004, Classifier Loss 0.19722536206245422, Total Loss 135.38580322265625\n",
      "56: Encoding Loss 15.546560287475586, Transition Loss 5.137919902801514, Classifier Loss 0.21864262223243713, Total Loss 147.26434326171875\n",
      "56: Encoding Loss 15.587189674377441, Transition Loss 3.502721071243286, Classifier Loss 0.18978041410446167, Total Loss 144.37611389160156\n",
      "56: Encoding Loss 15.292162895202637, Transition Loss 5.570306301116943, Classifier Loss 0.19246834516525269, Total Loss 142.6981964111328\n",
      "56: Encoding Loss 15.862788200378418, Transition Loss 6.178070545196533, Classifier Loss 0.19634728133678436, Total Loss 147.77264404296875\n",
      "56: Encoding Loss 15.98701286315918, Transition Loss 6.7861528396606445, Classifier Loss 0.22227492928504944, Total Loss 151.48081970214844\n",
      "56: Encoding Loss 15.853657722473145, Transition Loss 6.812513828277588, Classifier Loss 0.1960327923297882, Total Loss 147.7950439453125\n",
      "56: Encoding Loss 14.555792808532715, Transition Loss 7.233025074005127, Classifier Loss 0.21745553612709045, Total Loss 139.6385040283203\n",
      "56: Encoding Loss 15.65678596496582, Transition Loss 6.1812357902526855, Classifier Loss 0.20582471787929535, Total Loss 147.07301330566406\n",
      "56: Encoding Loss 14.703950881958008, Transition Loss 8.194132804870605, Classifier Loss 0.1960292011499405, Total Loss 138.87335205078125\n",
      "56: Encoding Loss 15.74022102355957, Transition Loss 6.725735187530518, Classifier Loss 0.19376125931739807, Total Loss 146.64305114746094\n",
      "56: Encoding Loss 14.821673393249512, Transition Loss 3.239483118057251, Classifier Loss 0.21961183845996857, Total Loss 141.18247985839844\n",
      "56: Encoding Loss 14.478078842163086, Transition Loss 6.647055625915527, Classifier Loss 0.20131967961788177, Total Loss 137.2860107421875\n",
      "56: Encoding Loss 14.638691902160645, Transition Loss 4.0808916091918945, Classifier Loss 0.2065378874540329, Total Loss 138.57949829101562\n",
      "56: Encoding Loss 15.469573974609375, Transition Loss 6.614020347595215, Classifier Loss 0.1937478482723236, Total Loss 144.4541778564453\n",
      "56: Encoding Loss 15.250929832458496, Transition Loss 8.112971305847168, Classifier Loss 0.21341104805469513, Total Loss 144.97113037109375\n",
      "56: Encoding Loss 16.450000762939453, Transition Loss 5.3976359367370605, Classifier Loss 0.1923617422580719, Total Loss 151.91571044921875\n",
      "56: Encoding Loss 15.59479808807373, Transition Loss 6.798915863037109, Classifier Loss 0.20223285257816315, Total Loss 146.34146118164062\n",
      "56: Encoding Loss 14.511351585388184, Transition Loss 6.316165924072266, Classifier Loss 0.24150903522968292, Total Loss 141.50494384765625\n",
      "56: Encoding Loss 15.58699893951416, Transition Loss 4.480010986328125, Classifier Loss 0.2359941005706787, Total Loss 149.19140625\n",
      "56: Encoding Loss 15.276987075805664, Transition Loss 4.122419357299805, Classifier Loss 0.20078691840171814, Total Loss 143.1190643310547\n",
      "56: Encoding Loss 15.637667655944824, Transition Loss 3.1706979274749756, Classifier Loss 0.20764067769050598, Total Loss 146.4995574951172\n",
      "56: Encoding Loss 16.705669403076172, Transition Loss 4.491871356964111, Classifier Loss 0.2105652540922165, Total Loss 155.60028076171875\n",
      "56: Encoding Loss 16.1428165435791, Transition Loss 6.421084403991699, Classifier Loss 0.22178825736045837, Total Loss 152.60556030273438\n",
      "56: Encoding Loss 15.376543045043945, Transition Loss 7.178854942321777, Classifier Loss 0.19964691996574402, Total Loss 144.41281127929688\n",
      "56: Encoding Loss 16.661928176879883, Transition Loss 6.37495231628418, Classifier Loss 0.2166064828634262, Total Loss 156.23106384277344\n",
      "56: Encoding Loss 15.954238891601562, Transition Loss 5.509429454803467, Classifier Loss 0.1893186867237091, Total Loss 147.66766357421875\n",
      "56: Encoding Loss 16.029794692993164, Transition Loss 5.937231540679932, Classifier Loss 0.1840745359659195, Total Loss 147.833251953125\n",
      "56: Encoding Loss 16.600242614746094, Transition Loss 4.167253017425537, Classifier Loss 0.21090979874134064, Total Loss 154.7263641357422\n",
      "56: Encoding Loss 16.34966468811035, Transition Loss 4.724185466766357, Classifier Loss 0.19767943024635315, Total Loss 151.51010131835938\n",
      "56: Encoding Loss 15.692352294921875, Transition Loss 3.891688108444214, Classifier Loss 0.19638508558273315, Total Loss 145.95565795898438\n",
      "56: Encoding Loss 16.52475357055664, Transition Loss 6.848688125610352, Classifier Loss 0.21268418431282043, Total Loss 154.836181640625\n",
      "56: Encoding Loss 14.984556198120117, Transition Loss 6.239152908325195, Classifier Loss 0.19445186853408813, Total Loss 140.56947326660156\n",
      "56: Encoding Loss 14.851325035095215, Transition Loss 5.329695701599121, Classifier Loss 0.21044966578483582, Total Loss 140.92149353027344\n",
      "56: Encoding Loss 15.309062004089355, Transition Loss 4.766981601715088, Classifier Loss 0.20787712931632996, Total Loss 144.21360778808594\n",
      "56: Encoding Loss 14.947917938232422, Transition Loss 5.337806224822998, Classifier Loss 0.18565437197685242, Total Loss 139.2163543701172\n",
      "56: Encoding Loss 14.812618255615234, Transition Loss 4.227658748626709, Classifier Loss 0.18421077728271484, Total Loss 137.76756286621094\n",
      "56: Encoding Loss 15.54507827758789, Transition Loss 4.920673370361328, Classifier Loss 0.18799015879631042, Total Loss 144.14376831054688\n",
      "56: Encoding Loss 16.75464630126953, Transition Loss 6.065303802490234, Classifier Loss 0.20018252730369568, Total Loss 155.2684783935547\n",
      "56: Encoding Loss 17.268388748168945, Transition Loss 5.550047874450684, Classifier Loss 0.22172178328037262, Total Loss 161.42930603027344\n",
      "56: Encoding Loss 15.125204086303711, Transition Loss 3.8738653659820557, Classifier Loss 0.20317907631397247, Total Loss 142.09432983398438\n",
      "56: Encoding Loss 14.977314949035645, Transition Loss 4.336410999298096, Classifier Loss 0.19595177471637726, Total Loss 140.28097534179688\n",
      "56: Encoding Loss 15.541336059570312, Transition Loss 6.122534275054932, Classifier Loss 0.2031794786453247, Total Loss 145.87313842773438\n",
      "56: Encoding Loss 14.886502265930176, Transition Loss 5.248405933380127, Classifier Loss 0.18693046271800995, Total Loss 138.83474731445312\n",
      "56: Encoding Loss 15.910662651062012, Transition Loss 5.030059337615967, Classifier Loss 0.2011367529630661, Total Loss 148.4049835205078\n",
      "56: Encoding Loss 16.40269660949707, Transition Loss 4.98884391784668, Classifier Loss 0.1845431625843048, Total Loss 150.6736602783203\n",
      "56: Encoding Loss 15.610162734985352, Transition Loss 6.350693225860596, Classifier Loss 0.20320814847946167, Total Loss 146.47225952148438\n",
      "56: Encoding Loss 15.395898818969727, Transition Loss 5.513609409332275, Classifier Loss 0.21099328994750977, Total Loss 145.36924743652344\n",
      "56: Encoding Loss 16.811159133911133, Transition Loss 5.5226006507873535, Classifier Loss 0.2103118896484375, Total Loss 156.62498474121094\n",
      "56: Encoding Loss 15.683963775634766, Transition Loss 6.231333255767822, Classifier Loss 0.2007545828819275, Total Loss 146.79342651367188\n",
      "56: Encoding Loss 15.513011932373047, Transition Loss 6.044355869293213, Classifier Loss 0.18843303620815277, Total Loss 144.15628051757812\n",
      "56: Encoding Loss 16.49213218688965, Transition Loss 6.334515571594238, Classifier Loss 0.19242766499519348, Total Loss 152.4467315673828\n",
      "56: Encoding Loss 14.5833158493042, Transition Loss 7.520277500152588, Classifier Loss 0.23167236149311066, Total Loss 141.33782958984375\n",
      "56: Encoding Loss 15.7291898727417, Transition Loss 5.483212947845459, Classifier Loss 0.21041825413703918, Total Loss 147.9720001220703\n",
      "56: Encoding Loss 15.31885051727295, Transition Loss 6.856532096862793, Classifier Loss 0.1894996613264084, Total Loss 142.8720703125\n",
      "56: Encoding Loss 15.6378173828125, Transition Loss 6.45297908782959, Classifier Loss 0.20480090379714966, Total Loss 146.8732147216797\n",
      "56: Encoding Loss 15.194474220275879, Transition Loss 6.698550701141357, Classifier Loss 0.19830772280693054, Total Loss 142.7262725830078\n",
      "56: Encoding Loss 16.914920806884766, Transition Loss 5.282917022705078, Classifier Loss 0.20671823620796204, Total Loss 157.0477752685547\n",
      "56: Encoding Loss 15.287737846374512, Transition Loss 5.784214973449707, Classifier Loss 0.2036711722612381, Total Loss 143.82586669921875\n",
      "56: Encoding Loss 16.17627716064453, Transition Loss 5.410231590270996, Classifier Loss 0.2259947955608368, Total Loss 153.0917510986328\n",
      "56: Encoding Loss 15.666723251342773, Transition Loss 6.334186553955078, Classifier Loss 0.20575205981731415, Total Loss 147.1758270263672\n",
      "56: Encoding Loss 15.607027053833008, Transition Loss 5.955907821655273, Classifier Loss 0.19783106446266174, Total Loss 145.83050537109375\n",
      "56: Encoding Loss 16.648242950439453, Transition Loss 5.031209468841553, Classifier Loss 0.21025453507900238, Total Loss 155.21763610839844\n",
      "56: Encoding Loss 15.174921035766602, Transition Loss 5.480177402496338, Classifier Loss 0.18624402582645416, Total Loss 141.11981201171875\n",
      "56: Encoding Loss 15.536016464233398, Transition Loss 4.522480010986328, Classifier Loss 0.21549178659915924, Total Loss 146.74180603027344\n",
      "56: Encoding Loss 15.450699806213379, Transition Loss 6.978208065032959, Classifier Loss 0.20889024436473846, Total Loss 145.89027404785156\n",
      "56: Encoding Loss 15.10356616973877, Transition Loss 4.671154499053955, Classifier Loss 0.20416846871376038, Total Loss 142.1796112060547\n",
      "56: Encoding Loss 15.471704483032227, Transition Loss 4.006196022033691, Classifier Loss 0.2064545750617981, Total Loss 145.2203369140625\n",
      "56: Encoding Loss 16.10088348388672, Transition Loss 5.220684051513672, Classifier Loss 0.1817312389612198, Total Loss 148.0243377685547\n",
      "56: Encoding Loss 15.761079788208008, Transition Loss 6.936875343322754, Classifier Loss 0.2334916740655899, Total Loss 150.82518005371094\n",
      "56: Encoding Loss 15.910988807678223, Transition Loss 6.522137641906738, Classifier Loss 0.20651045441627502, Total Loss 149.24339294433594\n",
      "56: Encoding Loss 16.702228546142578, Transition Loss 5.297882080078125, Classifier Loss 0.18642568588256836, Total Loss 153.31996154785156\n",
      "56: Encoding Loss 16.47512435913086, Transition Loss 5.742056846618652, Classifier Loss 0.19187863171100616, Total Loss 152.1372833251953\n",
      "56: Encoding Loss 15.230257034301758, Transition Loss 3.9213454723358154, Classifier Loss 0.1966213434934616, Total Loss 142.28846740722656\n",
      "56: Encoding Loss 15.799407005310059, Transition Loss 6.666388034820557, Classifier Loss 0.219923734664917, Total Loss 149.72091674804688\n",
      "56: Encoding Loss 16.03213119506836, Transition Loss 5.109741687774658, Classifier Loss 0.2120257169008255, Total Loss 150.48158264160156\n",
      "56: Encoding Loss 15.3506498336792, Transition Loss 7.518073558807373, Classifier Loss 0.23757877945899963, Total Loss 148.0666961669922\n",
      "56: Encoding Loss 16.2465763092041, Transition Loss 4.023377418518066, Classifier Loss 0.20858030021190643, Total Loss 151.63531494140625\n",
      "56: Encoding Loss 15.740325927734375, Transition Loss 5.060833930969238, Classifier Loss 0.1915585696697235, Total Loss 146.0906219482422\n",
      "56: Encoding Loss 16.38783073425293, Transition Loss 5.4156999588012695, Classifier Loss 0.21631044149398804, Total Loss 153.81683349609375\n",
      "56: Encoding Loss 15.730569839477539, Transition Loss 7.315424919128418, Classifier Loss 0.19949090480804443, Total Loss 147.25674438476562\n",
      "56: Encoding Loss 14.792242050170898, Transition Loss 6.315209865570068, Classifier Loss 0.1872219294309616, Total Loss 138.32318115234375\n",
      "56: Encoding Loss 15.298908233642578, Transition Loss 5.851981163024902, Classifier Loss 0.19588148593902588, Total Loss 143.14979553222656\n",
      "56: Encoding Loss 14.63471794128418, Transition Loss 8.270421028137207, Classifier Loss 0.1867390275001526, Total Loss 137.40573120117188\n",
      "56: Encoding Loss 15.837030410766602, Transition Loss 5.809474945068359, Classifier Loss 0.19690372049808502, Total Loss 147.5485076904297\n",
      "56: Encoding Loss 15.596962928771973, Transition Loss 6.2559003829956055, Classifier Loss 0.20218457281589508, Total Loss 146.24534606933594\n",
      "56: Encoding Loss 14.058053016662598, Transition Loss 5.399343013763428, Classifier Loss 0.2021697461605072, Total Loss 133.7612762451172\n",
      "56: Encoding Loss 14.998910903930664, Transition Loss 6.600475311279297, Classifier Loss 0.19453097879886627, Total Loss 140.7644805908203\n",
      "56: Encoding Loss 15.7269287109375, Transition Loss 5.891330718994141, Classifier Loss 0.21575626730918884, Total Loss 148.56932067871094\n",
      "56: Encoding Loss 16.325336456298828, Transition Loss 6.168865203857422, Classifier Loss 0.20950153470039368, Total Loss 152.78662109375\n",
      "56: Encoding Loss 15.408520698547363, Transition Loss 5.857171058654785, Classifier Loss 0.19469447433948517, Total Loss 143.90904235839844\n",
      "56: Encoding Loss 15.823375701904297, Transition Loss 4.460582256317139, Classifier Loss 0.19223357737064362, Total Loss 146.70248413085938\n",
      "56: Encoding Loss 15.203001022338867, Transition Loss 4.801115989685059, Classifier Loss 0.22362923622131348, Total Loss 144.94715881347656\n",
      "56: Encoding Loss 15.722187042236328, Transition Loss 5.799456596374512, Classifier Loss 0.1892259418964386, Total Loss 145.8599853515625\n",
      "56: Encoding Loss 16.421123504638672, Transition Loss 6.853654861450195, Classifier Loss 0.20453166961669922, Total Loss 153.19290161132812\n",
      "56: Encoding Loss 15.564316749572754, Transition Loss 5.348082542419434, Classifier Loss 0.2098255604505539, Total Loss 146.56671142578125\n",
      "56: Encoding Loss 15.70219898223877, Transition Loss 6.20310115814209, Classifier Loss 0.19224104285240173, Total Loss 146.08230590820312\n",
      "56: Encoding Loss 15.933035850524902, Transition Loss 4.938723087310791, Classifier Loss 0.20361021161079407, Total Loss 148.81304931640625\n",
      "56: Encoding Loss 16.363157272338867, Transition Loss 4.202265739440918, Classifier Loss 0.2010767012834549, Total Loss 151.85337829589844\n",
      "56: Encoding Loss 15.861907958984375, Transition Loss 4.123002052307129, Classifier Loss 0.19842180609703064, Total Loss 147.56204223632812\n",
      "56: Encoding Loss 15.996359825134277, Transition Loss 6.201597213745117, Classifier Loss 0.19912171363830566, Total Loss 149.12338256835938\n",
      "56: Encoding Loss 16.78849220275879, Transition Loss 4.055239200592041, Classifier Loss 0.20638398826122284, Total Loss 155.75738525390625\n",
      "56: Encoding Loss 17.178691864013672, Transition Loss 4.191569805145264, Classifier Loss 0.22211845219135284, Total Loss 160.47970581054688\n",
      "56: Encoding Loss 14.8668212890625, Transition Loss 6.103140354156494, Classifier Loss 0.19743101298809052, Total Loss 139.89830017089844\n",
      "56: Encoding Loss 14.731138229370117, Transition Loss 6.362165451049805, Classifier Loss 0.18367284536361694, Total Loss 137.48883056640625\n",
      "56: Encoding Loss 15.118910789489746, Transition Loss 5.849445343017578, Classifier Loss 0.21662847697734833, Total Loss 143.78402709960938\n",
      "56: Encoding Loss 15.172613143920898, Transition Loss 6.258517742156982, Classifier Loss 0.21283474564552307, Total Loss 143.9160919189453\n",
      "56: Encoding Loss 14.966669082641602, Transition Loss 8.365447998046875, Classifier Loss 0.2036084532737732, Total Loss 141.7672882080078\n",
      "56: Encoding Loss 16.644407272338867, Transition Loss 5.288665771484375, Classifier Loss 0.20622089505195618, Total Loss 154.8350830078125\n",
      "56: Encoding Loss 16.553508758544922, Transition Loss 5.052988052368164, Classifier Loss 0.17433682084083557, Total Loss 150.87237548828125\n",
      "56: Encoding Loss 14.803251266479492, Transition Loss 4.612345218658447, Classifier Loss 0.19034115970134735, Total Loss 138.38259887695312\n",
      "56: Encoding Loss 15.330194473266602, Transition Loss 4.397889137268066, Classifier Loss 0.20507538318634033, Total Loss 144.02867126464844\n",
      "56: Encoding Loss 15.374552726745605, Transition Loss 5.218400955200195, Classifier Loss 0.19349047541618347, Total Loss 143.38916015625\n",
      "56: Encoding Loss 15.814356803894043, Transition Loss 5.7651686668396, Classifier Loss 0.19859358668327332, Total Loss 147.52723693847656\n",
      "56: Encoding Loss 15.825699806213379, Transition Loss 4.379607677459717, Classifier Loss 0.20204897224903107, Total Loss 147.68641662597656\n",
      "56: Encoding Loss 15.851941108703613, Transition Loss 5.133594512939453, Classifier Loss 0.22053585946559906, Total Loss 149.8958282470703\n",
      "56: Encoding Loss 14.924493789672852, Transition Loss 4.2483930587768555, Classifier Loss 0.18010085821151733, Total Loss 138.25572204589844\n",
      "56: Encoding Loss 15.752236366271973, Transition Loss 5.878105163574219, Classifier Loss 0.18655070662498474, Total Loss 145.84860229492188\n",
      "56: Encoding Loss 16.08085823059082, Transition Loss 3.7924399375915527, Classifier Loss 0.21433386206626892, Total Loss 150.83872985839844\n",
      "56: Encoding Loss 15.083881378173828, Transition Loss 6.704792022705078, Classifier Loss 0.21883168816566467, Total Loss 143.89517211914062\n",
      "56: Encoding Loss 15.76571273803711, Transition Loss 4.710026741027832, Classifier Loss 0.2133113145828247, Total Loss 148.3988494873047\n",
      "56: Encoding Loss 15.947305679321289, Transition Loss 6.206830978393555, Classifier Loss 0.2083764374256134, Total Loss 149.65745544433594\n",
      "56: Encoding Loss 15.785202026367188, Transition Loss 4.930009841918945, Classifier Loss 0.19566243886947632, Total Loss 146.8338623046875\n",
      "56: Encoding Loss 15.220404624938965, Transition Loss 5.878418922424316, Classifier Loss 0.18521781265735626, Total Loss 141.46070861816406\n",
      "56: Encoding Loss 16.945484161376953, Transition Loss 3.8723673820495605, Classifier Loss 0.20444992184638977, Total Loss 156.78334045410156\n",
      "56: Encoding Loss 16.165740966796875, Transition Loss 3.707911252975464, Classifier Loss 0.21135544776916504, Total Loss 151.2030487060547\n",
      "56: Encoding Loss 15.49356460571289, Transition Loss 4.368303298950195, Classifier Loss 0.18758152425289154, Total Loss 143.580322265625\n",
      "56: Encoding Loss 15.1255464553833, Transition Loss 5.869499683380127, Classifier Loss 0.19842535257339478, Total Loss 142.02081298828125\n",
      "56: Encoding Loss 15.258306503295898, Transition Loss 3.3501806259155273, Classifier Loss 0.205119326710701, Total Loss 143.24842834472656\n",
      "56: Encoding Loss 15.986170768737793, Transition Loss 7.495116233825684, Classifier Loss 0.22787033021450043, Total Loss 152.1754150390625\n",
      "56: Encoding Loss 16.218852996826172, Transition Loss 4.951713562011719, Classifier Loss 0.19595104455947876, Total Loss 150.33628845214844\n",
      "56: Encoding Loss 17.11404800415039, Transition Loss 5.726300239562988, Classifier Loss 0.20768864452838898, Total Loss 158.82650756835938\n",
      "56: Encoding Loss 16.62333869934082, Transition Loss 6.468081951141357, Classifier Loss 0.19628247618675232, Total Loss 153.9085693359375\n",
      "56: Encoding Loss 15.320218086242676, Transition Loss 8.368168830871582, Classifier Loss 0.18914943933486938, Total Loss 143.1503143310547\n",
      "56: Encoding Loss 13.0742769241333, Transition Loss 9.103622436523438, Classifier Loss 0.19299322366714478, Total Loss 125.71426391601562\n",
      "57: Encoding Loss 16.877471923828125, Transition Loss 4.755073070526123, Classifier Loss 0.19992777705192566, Total Loss 155.9635772705078\n",
      "57: Encoding Loss 16.258466720581055, Transition Loss 5.241876125335693, Classifier Loss 0.1919577270746231, Total Loss 150.31187438964844\n",
      "57: Encoding Loss 16.266639709472656, Transition Loss 5.031896114349365, Classifier Loss 0.2052621841430664, Total Loss 151.66571044921875\n",
      "57: Encoding Loss 15.764595985412598, Transition Loss 5.667759418487549, Classifier Loss 0.2132994830608368, Total Loss 148.58026123046875\n",
      "57: Encoding Loss 17.477224349975586, Transition Loss 4.864317417144775, Classifier Loss 0.21286936104297638, Total Loss 162.07760620117188\n",
      "57: Encoding Loss 14.005322456359863, Transition Loss 5.769983291625977, Classifier Loss 0.21741899847984314, Total Loss 134.9384765625\n",
      "57: Encoding Loss 16.8787899017334, Transition Loss 4.401772499084473, Classifier Loss 0.22255000472068787, Total Loss 158.16567993164062\n",
      "57: Encoding Loss 17.208768844604492, Transition Loss 5.298938751220703, Classifier Loss 0.22099420428276062, Total Loss 160.82936096191406\n",
      "57: Encoding Loss 15.573378562927246, Transition Loss 4.595420837402344, Classifier Loss 0.2426217496395111, Total Loss 149.76828002929688\n",
      "57: Encoding Loss 16.665773391723633, Transition Loss 4.978649139404297, Classifier Loss 0.2128448784351349, Total Loss 155.6063995361328\n",
      "57: Encoding Loss 16.069265365600586, Transition Loss 4.188287734985352, Classifier Loss 0.1907631903886795, Total Loss 148.46810913085938\n",
      "57: Encoding Loss 15.73305892944336, Transition Loss 4.884087562561035, Classifier Loss 0.22027719020843506, Total Loss 148.8690185546875\n",
      "57: Encoding Loss 15.599838256835938, Transition Loss 5.399445533752441, Classifier Loss 0.20265990495681763, Total Loss 146.14459228515625\n",
      "57: Encoding Loss 14.929162979125977, Transition Loss 4.915283203125, Classifier Loss 0.19079270958900452, Total Loss 139.49563598632812\n",
      "57: Encoding Loss 15.048705101013184, Transition Loss 5.548096179962158, Classifier Loss 0.20569443702697754, Total Loss 142.06869506835938\n",
      "57: Encoding Loss 15.929093360900879, Transition Loss 6.599693775177002, Classifier Loss 0.21385355293750763, Total Loss 150.1380615234375\n",
      "57: Encoding Loss 15.836261749267578, Transition Loss 5.140522003173828, Classifier Loss 0.2101987898349762, Total Loss 148.73806762695312\n",
      "57: Encoding Loss 15.261122703552246, Transition Loss 6.156137943267822, Classifier Loss 0.19051674008369446, Total Loss 142.37188720703125\n",
      "57: Encoding Loss 16.39173698425293, Transition Loss 5.545180320739746, Classifier Loss 0.19253847002983093, Total Loss 151.4967803955078\n",
      "57: Encoding Loss 16.270357131958008, Transition Loss 6.295772552490234, Classifier Loss 0.20073387026786804, Total Loss 151.49539184570312\n",
      "57: Encoding Loss 15.641005516052246, Transition Loss 5.856093406677246, Classifier Loss 0.20963406562805176, Total Loss 147.26266479492188\n",
      "57: Encoding Loss 15.670363426208496, Transition Loss 5.151348114013672, Classifier Loss 0.1926465928554535, Total Loss 145.6578369140625\n",
      "57: Encoding Loss 14.247652053833008, Transition Loss 7.884398460388184, Classifier Loss 0.19504790008068085, Total Loss 135.06288146972656\n",
      "57: Encoding Loss 15.529746055603027, Transition Loss 5.304800987243652, Classifier Loss 0.2217566967010498, Total Loss 147.47459411621094\n",
      "57: Encoding Loss 15.578252792358398, Transition Loss 3.1941211223602295, Classifier Loss 0.191989466547966, Total Loss 144.46380615234375\n",
      "57: Encoding Loss 15.283529281616211, Transition Loss 5.451319694519043, Classifier Loss 0.19017751514911652, Total Loss 142.3762664794922\n",
      "57: Encoding Loss 15.867853164672852, Transition Loss 5.704502105712891, Classifier Loss 0.19211198389530182, Total Loss 147.294921875\n",
      "57: Encoding Loss 15.991174697875977, Transition Loss 6.48336124420166, Classifier Loss 0.21078360080718994, Total Loss 150.30442810058594\n",
      "57: Encoding Loss 15.854413986206055, Transition Loss 6.4975175857543945, Classifier Loss 0.18646958470344543, Total Loss 146.7817840576172\n",
      "57: Encoding Loss 14.561152458190918, Transition Loss 6.957574844360352, Classifier Loss 0.21313388645648956, Total Loss 139.19412231445312\n",
      "57: Encoding Loss 15.652067184448242, Transition Loss 5.945328235626221, Classifier Loss 0.20487883687019348, Total Loss 146.8935089111328\n",
      "57: Encoding Loss 14.699960708618164, Transition Loss 7.8001227378845215, Classifier Loss 0.19067828357219696, Total Loss 138.2275390625\n",
      "57: Encoding Loss 15.74436092376709, Transition Loss 6.433553218841553, Classifier Loss 0.19878336787223816, Total Loss 147.11993408203125\n",
      "57: Encoding Loss 14.817849159240723, Transition Loss 3.094257116317749, Classifier Loss 0.21423622965812683, Total Loss 140.5852813720703\n",
      "57: Encoding Loss 14.484137535095215, Transition Loss 6.217136859893799, Classifier Loss 0.2053280472755432, Total Loss 137.64932250976562\n",
      "57: Encoding Loss 14.664058685302734, Transition Loss 3.836005687713623, Classifier Loss 0.21552717685699463, Total Loss 139.63238525390625\n",
      "57: Encoding Loss 15.478681564331055, Transition Loss 6.311626434326172, Classifier Loss 0.19708964228630066, Total Loss 144.80075073242188\n",
      "57: Encoding Loss 15.246073722839355, Transition Loss 7.934931755065918, Classifier Loss 0.2160501629114151, Total Loss 145.1605987548828\n",
      "57: Encoding Loss 16.44503402709961, Transition Loss 5.310662269592285, Classifier Loss 0.19845189154148102, Total Loss 152.4676055908203\n",
      "57: Encoding Loss 15.576228141784668, Transition Loss 6.531373500823975, Classifier Loss 0.19149437546730042, Total Loss 145.06553649902344\n",
      "57: Encoding Loss 14.513153076171875, Transition Loss 6.420365333557129, Classifier Loss 0.21890892088413239, Total Loss 139.2801971435547\n",
      "57: Encoding Loss 15.596628189086914, Transition Loss 4.561509609222412, Classifier Loss 0.20940275490283966, Total Loss 146.6256103515625\n",
      "57: Encoding Loss 15.261646270751953, Transition Loss 4.3524250984191895, Classifier Loss 0.20078395307064056, Total Loss 143.0420379638672\n",
      "57: Encoding Loss 15.630997657775879, Transition Loss 3.2108402252197266, Classifier Loss 0.20844897627830505, Total Loss 146.53504943847656\n",
      "57: Encoding Loss 16.70003890991211, Transition Loss 4.597265720367432, Classifier Loss 0.20573927462100983, Total Loss 155.0937042236328\n",
      "57: Encoding Loss 16.13279151916504, Transition Loss 6.334517955780029, Classifier Loss 0.21405117213726044, Total Loss 151.73435974121094\n",
      "57: Encoding Loss 15.380205154418945, Transition Loss 7.374120712280273, Classifier Loss 0.19973570108413696, Total Loss 144.4900360107422\n",
      "57: Encoding Loss 16.663921356201172, Transition Loss 6.240026473999023, Classifier Loss 0.21440574526786804, Total Loss 155.9999542236328\n",
      "57: Encoding Loss 15.962185859680176, Transition Loss 5.680873870849609, Classifier Loss 0.19159430265426636, Total Loss 147.9930877685547\n",
      "57: Encoding Loss 16.023967742919922, Transition Loss 5.728281497955322, Classifier Loss 0.18210355937480927, Total Loss 147.5477752685547\n",
      "57: Encoding Loss 16.588682174682617, Transition Loss 4.3097968101501465, Classifier Loss 0.20844358205795288, Total Loss 154.415771484375\n",
      "57: Encoding Loss 16.338497161865234, Transition Loss 4.670374870300293, Classifier Loss 0.19383256137371063, Total Loss 151.02532958984375\n",
      "57: Encoding Loss 15.658595085144043, Transition Loss 4.105241298675537, Classifier Loss 0.19293729960918427, Total Loss 145.38352966308594\n",
      "57: Encoding Loss 16.524890899658203, Transition Loss 6.736687660217285, Classifier Loss 0.2173471301794052, Total Loss 155.2811737060547\n",
      "57: Encoding Loss 14.987454414367676, Transition Loss 6.502838134765625, Classifier Loss 0.1931024044752121, Total Loss 140.51043701171875\n",
      "57: Encoding Loss 14.863998413085938, Transition Loss 5.041421413421631, Classifier Loss 0.20343062281608582, Total Loss 140.26333618164062\n",
      "57: Encoding Loss 15.314393043518066, Transition Loss 5.020918846130371, Classifier Loss 0.20918098092079163, Total Loss 144.43743896484375\n",
      "57: Encoding Loss 14.958370208740234, Transition Loss 4.872560977935791, Classifier Loss 0.18577560782432556, Total Loss 139.21905517578125\n",
      "57: Encoding Loss 14.813429832458496, Transition Loss 4.6678571701049805, Classifier Loss 0.18530654907226562, Total Loss 137.97166442871094\n",
      "57: Encoding Loss 15.550655364990234, Transition Loss 4.562860488891602, Classifier Loss 0.18250547349452972, Total Loss 143.568359375\n",
      "57: Encoding Loss 16.7614688873291, Transition Loss 6.347031116485596, Classifier Loss 0.20360654592514038, Total Loss 155.72181701660156\n",
      "57: Encoding Loss 17.262556076049805, Transition Loss 5.3873796463012695, Classifier Loss 0.20961374044418335, Total Loss 160.13929748535156\n",
      "57: Encoding Loss 15.13471794128418, Transition Loss 3.9260060787200928, Classifier Loss 0.20335157215595245, Total Loss 142.19810485839844\n",
      "57: Encoding Loss 14.972609519958496, Transition Loss 4.21435022354126, Classifier Loss 0.1894317865371704, Total Loss 139.56692504882812\n",
      "57: Encoding Loss 15.537811279296875, Transition Loss 6.333144664764404, Classifier Loss 0.2020166665315628, Total Loss 145.77078247070312\n",
      "57: Encoding Loss 14.881717681884766, Transition Loss 5.125734806060791, Classifier Loss 0.18928401172161102, Total Loss 139.0072784423828\n",
      "57: Encoding Loss 15.907468795776367, Transition Loss 5.1589508056640625, Classifier Loss 0.20112761855125427, Total Loss 148.40431213378906\n",
      "57: Encoding Loss 16.403987884521484, Transition Loss 4.660301208496094, Classifier Loss 0.17808085680007935, Total Loss 149.97207641601562\n",
      "57: Encoding Loss 15.627392768859863, Transition Loss 6.744558334350586, Classifier Loss 0.2095959186553955, Total Loss 147.32763671875\n",
      "57: Encoding Loss 15.402525901794434, Transition Loss 4.896039009094238, Classifier Loss 0.2111556977033615, Total Loss 145.3149871826172\n",
      "57: Encoding Loss 16.813467025756836, Transition Loss 5.984424114227295, Classifier Loss 0.20230089128017426, Total Loss 155.93470764160156\n",
      "57: Encoding Loss 15.67785930633545, Transition Loss 5.576450347900391, Classifier Loss 0.1976722925901413, Total Loss 146.30540466308594\n",
      "57: Encoding Loss 15.503681182861328, Transition Loss 6.590352535247803, Classifier Loss 0.18698520958423615, Total Loss 144.04603576660156\n",
      "57: Encoding Loss 16.508647918701172, Transition Loss 6.007408618927002, Classifier Loss 0.19021330773830414, Total Loss 152.29200744628906\n",
      "57: Encoding Loss 14.570113182067871, Transition Loss 7.771955966949463, Classifier Loss 0.24606545269489288, Total Loss 142.7218475341797\n",
      "57: Encoding Loss 15.725139617919922, Transition Loss 5.330918312072754, Classifier Loss 0.2118331491947174, Total Loss 148.0506134033203\n",
      "57: Encoding Loss 15.325254440307617, Transition Loss 6.961641311645508, Classifier Loss 0.1961844563484192, Total Loss 143.61282348632812\n",
      "57: Encoding Loss 15.635357856750488, Transition Loss 6.299132347106934, Classifier Loss 0.20554041862487793, Total Loss 146.896728515625\n",
      "57: Encoding Loss 15.197168350219727, Transition Loss 6.888384819030762, Classifier Loss 0.19571220874786377, Total Loss 142.52622985839844\n",
      "57: Encoding Loss 16.927448272705078, Transition Loss 5.191465377807617, Classifier Loss 0.194183811545372, Total Loss 155.8762664794922\n",
      "57: Encoding Loss 15.272120475769043, Transition Loss 5.920935153961182, Classifier Loss 0.21122832596302032, Total Loss 144.48399353027344\n",
      "57: Encoding Loss 16.176036834716797, Transition Loss 5.212128162384033, Classifier Loss 0.21493563055992126, Total Loss 151.9442901611328\n",
      "57: Encoding Loss 15.66169261932373, Transition Loss 6.410233020782471, Classifier Loss 0.19735367596149445, Total Loss 146.31094360351562\n",
      "57: Encoding Loss 15.609240531921387, Transition Loss 5.637475967407227, Classifier Loss 0.1996883749961853, Total Loss 145.9702606201172\n",
      "57: Encoding Loss 16.645322799682617, Transition Loss 5.1445417404174805, Classifier Loss 0.21013639867305756, Total Loss 155.20513916015625\n",
      "57: Encoding Loss 15.172619819641113, Transition Loss 5.349987506866455, Classifier Loss 0.18185850977897644, Total Loss 140.6367950439453\n",
      "57: Encoding Loss 15.538787841796875, Transition Loss 4.537766456604004, Classifier Loss 0.20729081332683563, Total Loss 145.94692993164062\n",
      "57: Encoding Loss 15.445581436157227, Transition Loss 6.849560260772705, Classifier Loss 0.21157920360565186, Total Loss 146.0924835205078\n",
      "57: Encoding Loss 15.104312896728516, Transition Loss 4.72184944152832, Classifier Loss 0.2065664678812027, Total Loss 142.43551635742188\n",
      "57: Encoding Loss 15.483128547668457, Transition Loss 3.7928028106689453, Classifier Loss 0.20086133480072021, Total Loss 144.709716796875\n",
      "57: Encoding Loss 16.08561134338379, Transition Loss 5.218754768371582, Classifier Loss 0.18084973096847534, Total Loss 147.81361389160156\n",
      "57: Encoding Loss 15.747600555419922, Transition Loss 6.602421283721924, Classifier Loss 0.22837109863758087, Total Loss 150.13839721679688\n",
      "57: Encoding Loss 15.921303749084473, Transition Loss 6.53243350982666, Classifier Loss 0.2199515700340271, Total Loss 150.67208862304688\n",
      "57: Encoding Loss 16.70437240600586, Transition Loss 4.968812942504883, Classifier Loss 0.17806142568588257, Total Loss 152.4348907470703\n",
      "57: Encoding Loss 16.485536575317383, Transition Loss 5.61179256439209, Classifier Loss 0.19209231436252594, Total Loss 152.21588134765625\n",
      "57: Encoding Loss 15.233686447143555, Transition Loss 3.6674089431762695, Classifier Loss 0.1934976726770401, Total Loss 141.95274353027344\n",
      "57: Encoding Loss 15.79641056060791, Transition Loss 6.536945343017578, Classifier Loss 0.21527066826820374, Total Loss 149.20574951171875\n",
      "57: Encoding Loss 16.034610748291016, Transition Loss 4.8336920738220215, Classifier Loss 0.21459433436393738, Total Loss 150.7030487060547\n",
      "57: Encoding Loss 15.341509819030762, Transition Loss 7.387731075286865, Classifier Loss 0.20964211225509644, Total Loss 145.173828125\n",
      "57: Encoding Loss 16.240720748901367, Transition Loss 3.8698906898498535, Classifier Loss 0.19863323867321014, Total Loss 150.5630645751953\n",
      "57: Encoding Loss 15.74215316772461, Transition Loss 4.909335136413574, Classifier Loss 0.18398992717266083, Total Loss 145.31809997558594\n",
      "57: Encoding Loss 16.3917293548584, Transition Loss 5.323712348937988, Classifier Loss 0.22049769759178162, Total Loss 154.24835205078125\n",
      "57: Encoding Loss 15.713445663452148, Transition Loss 7.045928001403809, Classifier Loss 0.20326393842697144, Total Loss 147.44314575195312\n",
      "57: Encoding Loss 14.808886528015137, Transition Loss 6.1979265213012695, Classifier Loss 0.18753288686275482, Total Loss 138.46395874023438\n",
      "57: Encoding Loss 15.310057640075684, Transition Loss 5.778416633605957, Classifier Loss 0.19448556005954742, Total Loss 143.08470153808594\n",
      "57: Encoding Loss 14.636906623840332, Transition Loss 8.377362251281738, Classifier Loss 0.1913112848997116, Total Loss 137.90185546875\n",
      "57: Encoding Loss 15.82890510559082, Transition Loss 5.722194671630859, Classifier Loss 0.20647916197776794, Total Loss 148.42359924316406\n",
      "57: Encoding Loss 15.597087860107422, Transition Loss 6.173189163208008, Classifier Loss 0.20773360133171082, Total Loss 146.78469848632812\n",
      "57: Encoding Loss 14.036321640014648, Transition Loss 5.220573425292969, Classifier Loss 0.20238764584064484, Total Loss 133.57345581054688\n",
      "57: Encoding Loss 15.0048828125, Transition Loss 6.678197383880615, Classifier Loss 0.20235508680343628, Total Loss 141.61019897460938\n",
      "57: Encoding Loss 15.737993240356445, Transition Loss 5.8577165603637695, Classifier Loss 0.1983785182237625, Total Loss 146.913330078125\n",
      "57: Encoding Loss 16.33096694946289, Transition Loss 6.547580242156982, Classifier Loss 0.21932797133922577, Total Loss 153.89004516601562\n",
      "57: Encoding Loss 15.409590721130371, Transition Loss 5.8207292556762695, Classifier Loss 0.20673969388008118, Total Loss 145.11483764648438\n",
      "57: Encoding Loss 15.814967155456543, Transition Loss 4.749135971069336, Classifier Loss 0.1975310891866684, Total Loss 147.22267150878906\n",
      "57: Encoding Loss 15.19428539276123, Transition Loss 4.3433837890625, Classifier Loss 0.22543758153915405, Total Loss 144.9667205810547\n",
      "57: Encoding Loss 15.752090454101562, Transition Loss 6.545529842376709, Classifier Loss 0.19279474020004272, Total Loss 146.60531616210938\n",
      "57: Encoding Loss 16.420228958129883, Transition Loss 5.909797191619873, Classifier Loss 0.20530742406845093, Total Loss 153.0745391845703\n",
      "57: Encoding Loss 15.561306953430176, Transition Loss 6.063405513763428, Classifier Loss 0.21684953570365906, Total Loss 147.38809204101562\n",
      "57: Encoding Loss 15.706466674804688, Transition Loss 5.748322486877441, Classifier Loss 0.19792668521404266, Total Loss 146.59405517578125\n",
      "57: Encoding Loss 15.927770614624023, Transition Loss 5.4789910316467285, Classifier Loss 0.20153889060020447, Total Loss 148.67185974121094\n",
      "57: Encoding Loss 16.339622497558594, Transition Loss 4.00876522064209, Classifier Loss 0.2011953592300415, Total Loss 151.63827514648438\n",
      "57: Encoding Loss 15.85103988647461, Transition Loss 4.3764166831970215, Classifier Loss 0.19903138279914856, Total Loss 147.58676147460938\n",
      "57: Encoding Loss 16.00808334350586, Transition Loss 6.062633991241455, Classifier Loss 0.19236204028129578, Total Loss 148.51341247558594\n",
      "57: Encoding Loss 16.772356033325195, Transition Loss 4.134986400604248, Classifier Loss 0.20681728422641754, Total Loss 155.6875762939453\n",
      "57: Encoding Loss 17.170991897583008, Transition Loss 4.195339202880859, Classifier Loss 0.2202199399471283, Total Loss 160.22898864746094\n",
      "57: Encoding Loss 14.860279083251953, Transition Loss 6.189131736755371, Classifier Loss 0.20143236219882965, Total Loss 140.26329040527344\n",
      "57: Encoding Loss 14.717181205749512, Transition Loss 6.28692102432251, Classifier Loss 0.186113178730011, Total Loss 137.6061553955078\n",
      "57: Encoding Loss 15.105010986328125, Transition Loss 5.919240474700928, Classifier Loss 0.21540245413780212, Total Loss 143.56419372558594\n",
      "57: Encoding Loss 15.16528606414795, Transition Loss 6.151125907897949, Classifier Loss 0.2160056233406067, Total Loss 144.153076171875\n",
      "57: Encoding Loss 14.970582962036133, Transition Loss 8.511672019958496, Classifier Loss 0.2050192952156067, Total Loss 141.96893310546875\n",
      "57: Encoding Loss 16.649951934814453, Transition Loss 5.258478164672852, Classifier Loss 0.20261532068252563, Total Loss 154.51284790039062\n",
      "57: Encoding Loss 16.547168731689453, Transition Loss 5.162699222564697, Classifier Loss 0.1812736988067627, Total Loss 151.53726196289062\n",
      "57: Encoding Loss 14.791382789611816, Transition Loss 4.667619228363037, Classifier Loss 0.19117210805416107, Total Loss 138.3817901611328\n",
      "57: Encoding Loss 15.326480865478516, Transition Loss 4.551342964172363, Classifier Loss 0.19599664211273193, Total Loss 143.12176513671875\n",
      "57: Encoding Loss 15.381087303161621, Transition Loss 5.237186431884766, Classifier Loss 0.19685451686382294, Total Loss 143.78158569335938\n",
      "57: Encoding Loss 15.82879638671875, Transition Loss 5.802066326141357, Classifier Loss 0.19640108942985535, Total Loss 147.43089294433594\n",
      "57: Encoding Loss 15.810145378112793, Transition Loss 4.2679290771484375, Classifier Loss 0.19616875052452087, Total Loss 146.95162963867188\n",
      "57: Encoding Loss 15.85317325592041, Transition Loss 5.1744914054870605, Classifier Loss 0.22010537981987, Total Loss 149.87083435058594\n",
      "57: Encoding Loss 14.921433448791504, Transition Loss 4.091301441192627, Classifier Loss 0.17832235991954803, Total Loss 138.02197265625\n",
      "57: Encoding Loss 15.763239860534668, Transition Loss 5.924444198608398, Classifier Loss 0.18031416833400726, Total Loss 145.3222198486328\n",
      "57: Encoding Loss 16.062902450561523, Transition Loss 3.5938057899475098, Classifier Loss 0.2193806916475296, Total Loss 151.16004943847656\n",
      "57: Encoding Loss 15.10517692565918, Transition Loss 6.8217902183532715, Classifier Loss 0.20827868580818176, Total Loss 143.03366088867188\n",
      "57: Encoding Loss 15.76851749420166, Transition Loss 4.544177532196045, Classifier Loss 0.21155574917793274, Total Loss 148.21255493164062\n",
      "57: Encoding Loss 15.953839302062988, Transition Loss 6.398641586303711, Classifier Loss 0.21114884316921234, Total Loss 150.02532958984375\n",
      "57: Encoding Loss 15.796695709228516, Transition Loss 4.691981792449951, Classifier Loss 0.19917632639408112, Total Loss 147.22959899902344\n",
      "57: Encoding Loss 15.207569122314453, Transition Loss 5.947072982788086, Classifier Loss 0.20099756121635437, Total Loss 142.94970703125\n",
      "57: Encoding Loss 16.948524475097656, Transition Loss 3.666788101196289, Classifier Loss 0.2039775848388672, Total Loss 156.7193145751953\n",
      "57: Encoding Loss 16.161218643188477, Transition Loss 3.7010648250579834, Classifier Loss 0.19934391975402832, Total Loss 149.96435546875\n",
      "57: Encoding Loss 15.504740715026855, Transition Loss 4.193464756011963, Classifier Loss 0.1962086707353592, Total Loss 144.49749755859375\n",
      "57: Encoding Loss 15.123270034790039, Transition Loss 6.063089370727539, Classifier Loss 0.2087170034646988, Total Loss 143.0704803466797\n",
      "57: Encoding Loss 15.25517463684082, Transition Loss 2.9790306091308594, Classifier Loss 0.20244964957237244, Total Loss 142.88217163085938\n",
      "57: Encoding Loss 15.980193138122559, Transition Loss 7.891488075256348, Classifier Loss 0.21932172775268555, Total Loss 151.3520050048828\n",
      "57: Encoding Loss 16.2254695892334, Transition Loss 4.288788318634033, Classifier Loss 0.20058926939964294, Total Loss 150.72044372558594\n",
      "57: Encoding Loss 17.101884841918945, Transition Loss 6.144989013671875, Classifier Loss 0.2049010694026947, Total Loss 158.53419494628906\n",
      "57: Encoding Loss 16.629371643066406, Transition Loss 5.536311149597168, Classifier Loss 0.18831518292427063, Total Loss 152.9737548828125\n",
      "57: Encoding Loss 15.294100761413574, Transition Loss 8.773513793945312, Classifier Loss 0.1951010823249817, Total Loss 143.61761474609375\n",
      "57: Encoding Loss 13.105085372924805, Transition Loss 8.02333927154541, Classifier Loss 0.18537412583827972, Total Loss 124.98277282714844\n",
      "58: Encoding Loss 16.878450393676758, Transition Loss 5.048837184906006, Classifier Loss 0.1883082091808319, Total Loss 154.86819458007812\n",
      "58: Encoding Loss 16.244850158691406, Transition Loss 4.718764781951904, Classifier Loss 0.1953808069229126, Total Loss 150.44064331054688\n",
      "58: Encoding Loss 16.256975173950195, Transition Loss 5.155434608459473, Classifier Loss 0.20446500182151794, Total Loss 151.53338623046875\n",
      "58: Encoding Loss 15.7584810256958, Transition Loss 5.297741889953613, Classifier Loss 0.20290188491344452, Total Loss 147.41758728027344\n",
      "58: Encoding Loss 17.480815887451172, Transition Loss 5.0124335289001465, Classifier Loss 0.2175854593515396, Total Loss 162.60757446289062\n",
      "58: Encoding Loss 14.011286735534668, Transition Loss 5.664058208465576, Classifier Loss 0.2066042721271515, Total Loss 133.88352966308594\n",
      "58: Encoding Loss 16.89299774169922, Transition Loss 4.488112926483154, Classifier Loss 0.21364468336105347, Total Loss 157.4060821533203\n",
      "58: Encoding Loss 17.205198287963867, Transition Loss 5.035904884338379, Classifier Loss 0.2074645459651947, Total Loss 159.39523315429688\n",
      "58: Encoding Loss 15.568243980407715, Transition Loss 4.887966632843018, Classifier Loss 0.24438554048538208, Total Loss 149.9621124267578\n",
      "58: Encoding Loss 16.683496475219727, Transition Loss 4.823022365570068, Classifier Loss 0.21335560083389282, Total Loss 155.76812744140625\n",
      "58: Encoding Loss 16.073617935180664, Transition Loss 4.341002464294434, Classifier Loss 0.1896083950996399, Total Loss 148.41798400878906\n",
      "58: Encoding Loss 15.744569778442383, Transition Loss 4.859071254730225, Classifier Loss 0.2207755148410797, Total Loss 149.00592041015625\n",
      "58: Encoding Loss 15.59381103515625, Transition Loss 5.61317253112793, Classifier Loss 0.20365770161151886, Total Loss 146.2388916015625\n",
      "58: Encoding Loss 14.933107376098633, Transition Loss 4.803879737854004, Classifier Loss 0.19396932423114777, Total Loss 139.8225555419922\n",
      "58: Encoding Loss 15.031035423278809, Transition Loss 5.786015510559082, Classifier Loss 0.1968398243188858, Total Loss 141.08946228027344\n",
      "58: Encoding Loss 15.912230491638184, Transition Loss 6.511972427368164, Classifier Loss 0.20874227583408356, Total Loss 149.47447204589844\n",
      "58: Encoding Loss 15.830141067504883, Transition Loss 5.290738105773926, Classifier Loss 0.20058606564998627, Total Loss 147.7578887939453\n",
      "58: Encoding Loss 15.254866600036621, Transition Loss 6.1288580894470215, Classifier Loss 0.19162523746490479, Total Loss 142.42723083496094\n",
      "58: Encoding Loss 16.39850425720215, Transition Loss 5.903129577636719, Classifier Loss 0.18704523146152496, Total Loss 151.07318115234375\n",
      "58: Encoding Loss 16.270000457763672, Transition Loss 5.992908477783203, Classifier Loss 0.19472411274909973, Total Loss 150.8310089111328\n",
      "58: Encoding Loss 15.651183128356934, Transition Loss 6.453725814819336, Classifier Loss 0.2033790647983551, Total Loss 146.83810424804688\n",
      "58: Encoding Loss 15.658309936523438, Transition Loss 4.999829292297363, Classifier Loss 0.19853191077709198, Total Loss 146.11964416503906\n",
      "58: Encoding Loss 14.243179321289062, Transition Loss 8.646383285522461, Classifier Loss 0.19328798353672028, Total Loss 135.00350952148438\n",
      "58: Encoding Loss 15.539898872375488, Transition Loss 5.012655258178711, Classifier Loss 0.22284898161888123, Total Loss 147.60662841796875\n",
      "58: Encoding Loss 15.579036712646484, Transition Loss 3.373544931411743, Classifier Loss 0.2035692185163498, Total Loss 145.6639404296875\n",
      "58: Encoding Loss 15.296679496765137, Transition Loss 5.332358360290527, Classifier Loss 0.18836863338947296, Total Loss 142.27676391601562\n",
      "58: Encoding Loss 15.85628890991211, Transition Loss 6.061031341552734, Classifier Loss 0.19904214143753052, Total Loss 147.96673583984375\n",
      "58: Encoding Loss 15.9900541305542, Transition Loss 6.567614555358887, Classifier Loss 0.20084629952907562, Total Loss 149.31858825683594\n",
      "58: Encoding Loss 15.859461784362793, Transition Loss 6.644855976104736, Classifier Loss 0.19322535395622253, Total Loss 147.52719116210938\n",
      "58: Encoding Loss 14.555734634399414, Transition Loss 7.032202243804932, Classifier Loss 0.21931323409080505, Total Loss 139.78363037109375\n",
      "58: Encoding Loss 15.645520210266113, Transition Loss 5.7115702629089355, Classifier Loss 0.19148512184619904, Total Loss 145.45498657226562\n",
      "58: Encoding Loss 14.69882869720459, Transition Loss 7.674038410186768, Classifier Loss 0.19261756539344788, Total Loss 138.38719177246094\n",
      "58: Encoding Loss 15.750031471252441, Transition Loss 6.377363681793213, Classifier Loss 0.19492244720458984, Total Loss 146.76797485351562\n",
      "58: Encoding Loss 14.824692726135254, Transition Loss 3.0568771362304688, Classifier Loss 0.21739132702350616, Total Loss 140.94805908203125\n",
      "58: Encoding Loss 14.474672317504883, Transition Loss 6.323002815246582, Classifier Loss 0.20332248508930206, Total Loss 137.39422607421875\n",
      "58: Encoding Loss 14.649216651916504, Transition Loss 3.8687965869903564, Classifier Loss 0.20981915295124054, Total Loss 138.9494171142578\n",
      "58: Encoding Loss 15.46908187866211, Transition Loss 6.356847763061523, Classifier Loss 0.19315695762634277, Total Loss 144.3397216796875\n",
      "58: Encoding Loss 15.238142013549805, Transition Loss 7.830244541168213, Classifier Loss 0.20971795916557312, Total Loss 144.4429931640625\n",
      "58: Encoding Loss 16.434844970703125, Transition Loss 5.303628921508789, Classifier Loss 0.19748075306415558, Total Loss 152.28756713867188\n",
      "58: Encoding Loss 15.584046363830566, Transition Loss 6.650249004364014, Classifier Loss 0.19648657739162445, Total Loss 145.6510772705078\n",
      "58: Encoding Loss 14.510751724243164, Transition Loss 6.359860897064209, Classifier Loss 0.22519652545452118, Total Loss 139.8776397705078\n",
      "58: Encoding Loss 15.603995323181152, Transition Loss 4.606520652770996, Classifier Loss 0.20743587613105774, Total Loss 146.49685668945312\n",
      "58: Encoding Loss 15.26203441619873, Transition Loss 4.259556770324707, Classifier Loss 0.19779156148433685, Total Loss 142.7273406982422\n",
      "58: Encoding Loss 15.64094066619873, Transition Loss 3.1082820892333984, Classifier Loss 0.20730051398277283, Total Loss 146.47923278808594\n",
      "58: Encoding Loss 16.698753356933594, Transition Loss 4.522487640380859, Classifier Loss 0.21242862939834595, Total Loss 155.7373809814453\n",
      "58: Encoding Loss 16.13768196105957, Transition Loss 6.234334468841553, Classifier Loss 0.21555107831954956, Total Loss 151.90342712402344\n",
      "58: Encoding Loss 15.380023002624512, Transition Loss 7.375641345977783, Classifier Loss 0.2000950127840042, Total Loss 144.52481079101562\n",
      "58: Encoding Loss 16.663442611694336, Transition Loss 6.213688373565674, Classifier Loss 0.21377858519554138, Total Loss 155.92813110351562\n",
      "58: Encoding Loss 15.939172744750977, Transition Loss 5.542007923126221, Classifier Loss 0.18723250925540924, Total Loss 147.34503173828125\n",
      "58: Encoding Loss 16.02641487121582, Transition Loss 5.605154514312744, Classifier Loss 0.18354803323745728, Total Loss 147.68714904785156\n",
      "58: Encoding Loss 16.581809997558594, Transition Loss 4.256233215332031, Classifier Loss 0.19965310394763947, Total Loss 153.47103881835938\n",
      "58: Encoding Loss 16.328716278076172, Transition Loss 4.504210472106934, Classifier Loss 0.20453837513923645, Total Loss 151.98443603515625\n",
      "58: Encoding Loss 15.669121742248535, Transition Loss 3.959847927093506, Classifier Loss 0.19666838645935059, Total Loss 145.81179809570312\n",
      "58: Encoding Loss 16.51032066345215, Transition Loss 6.576326370239258, Classifier Loss 0.2153853327035904, Total Loss 154.9363555908203\n",
      "58: Encoding Loss 14.985220909118652, Transition Loss 6.602900505065918, Classifier Loss 0.1974395513534546, Total Loss 140.94630432128906\n",
      "58: Encoding Loss 14.85052490234375, Transition Loss 4.822788238525391, Classifier Loss 0.20427915453910828, Total Loss 140.19667053222656\n",
      "58: Encoding Loss 15.325517654418945, Transition Loss 4.883641719818115, Classifier Loss 0.22065061330795288, Total Loss 145.64593505859375\n",
      "58: Encoding Loss 14.951238632202148, Transition Loss 4.58834171295166, Classifier Loss 0.18274620175361633, Total Loss 138.8022003173828\n",
      "58: Encoding Loss 14.818906784057617, Transition Loss 4.6331892013549805, Classifier Loss 0.18490289151668549, Total Loss 137.9681854248047\n",
      "58: Encoding Loss 15.533238410949707, Transition Loss 4.242696285247803, Classifier Loss 0.17757070064544678, Total Loss 142.87152099609375\n",
      "58: Encoding Loss 16.73571014404297, Transition Loss 6.28383731842041, Classifier Loss 0.21252651512622833, Total Loss 156.39511108398438\n",
      "58: Encoding Loss 17.26993751525879, Transition Loss 4.958216190338135, Classifier Loss 0.2096278816461563, Total Loss 160.11392211914062\n",
      "58: Encoding Loss 15.12657356262207, Transition Loss 3.9603524208068848, Classifier Loss 0.20314958691596985, Total Loss 142.11961364746094\n",
      "58: Encoding Loss 14.978583335876465, Transition Loss 3.7663309574127197, Classifier Loss 0.19326826930046082, Total Loss 139.90875244140625\n",
      "58: Encoding Loss 15.525357246398926, Transition Loss 6.340829372406006, Classifier Loss 0.202727273106575, Total Loss 145.74375915527344\n",
      "58: Encoding Loss 14.876132011413574, Transition Loss 4.683242321014404, Classifier Loss 0.1880326271057129, Total Loss 138.74896240234375\n",
      "58: Encoding Loss 15.903963088989258, Transition Loss 5.1890459060668945, Classifier Loss 0.20325967669487, Total Loss 148.59548950195312\n",
      "58: Encoding Loss 16.405113220214844, Transition Loss 4.340194225311279, Classifier Loss 0.18022969365119934, Total Loss 150.1319122314453\n",
      "58: Encoding Loss 15.632769584655762, Transition Loss 6.753330707550049, Classifier Loss 0.20688149333000183, Total Loss 147.10096740722656\n",
      "58: Encoding Loss 15.394622802734375, Transition Loss 4.853038787841797, Classifier Loss 0.21240541338920593, Total Loss 145.36813354492188\n",
      "58: Encoding Loss 16.803646087646484, Transition Loss 5.745669364929199, Classifier Loss 0.20161528885364532, Total Loss 155.73985290527344\n",
      "58: Encoding Loss 15.677681922912598, Transition Loss 5.8324689865112305, Classifier Loss 0.19112390279769897, Total Loss 145.70034790039062\n",
      "58: Encoding Loss 15.494584083557129, Transition Loss 6.234788417816162, Classifier Loss 0.1827617734670639, Total Loss 143.4798126220703\n",
      "58: Encoding Loss 16.490522384643555, Transition Loss 6.069870471954346, Classifier Loss 0.19500385224819183, Total Loss 152.63853454589844\n",
      "58: Encoding Loss 14.58906078338623, Transition Loss 7.504245281219482, Classifier Loss 0.24699744582176208, Total Loss 142.9130859375\n",
      "58: Encoding Loss 15.708962440490723, Transition Loss 5.446413993835449, Classifier Loss 0.20112580060958862, Total Loss 146.87356567382812\n",
      "58: Encoding Loss 15.31560230255127, Transition Loss 6.871634006500244, Classifier Loss 0.19621631503105164, Total Loss 143.52078247070312\n",
      "58: Encoding Loss 15.650935173034668, Transition Loss 6.441232681274414, Classifier Loss 0.20658698678016663, Total Loss 147.15443420410156\n",
      "58: Encoding Loss 15.202912330627441, Transition Loss 6.816379070281982, Classifier Loss 0.20020806789398193, Total Loss 143.0074005126953\n",
      "58: Encoding Loss 16.91371726989746, Transition Loss 5.239158630371094, Classifier Loss 0.2014479786157608, Total Loss 156.5023651123047\n",
      "58: Encoding Loss 15.275232315063477, Transition Loss 5.753042221069336, Classifier Loss 0.20061850547790527, Total Loss 143.414306640625\n",
      "58: Encoding Loss 16.175065994262695, Transition Loss 5.348219394683838, Classifier Loss 0.21747751533985138, Total Loss 152.21792602539062\n",
      "58: Encoding Loss 15.666033744812012, Transition Loss 6.357219696044922, Classifier Loss 0.19681304693222046, Total Loss 146.28102111816406\n",
      "58: Encoding Loss 15.60473346710205, Transition Loss 5.831630706787109, Classifier Loss 0.19009925425052643, Total Loss 145.0141143798828\n",
      "58: Encoding Loss 16.642187118530273, Transition Loss 4.996486663818359, Classifier Loss 0.19967889785766602, Total Loss 154.1046905517578\n",
      "58: Encoding Loss 15.174894332885742, Transition Loss 5.4982008934021, Classifier Loss 0.18553581833839417, Total Loss 141.05238342285156\n",
      "58: Encoding Loss 15.537832260131836, Transition Loss 4.492996692657471, Classifier Loss 0.21039988100528717, Total Loss 146.2412567138672\n",
      "58: Encoding Loss 15.43630599975586, Transition Loss 6.973704814910889, Classifier Loss 0.21107986569404602, Total Loss 145.99319458007812\n",
      "58: Encoding Loss 15.107001304626465, Transition Loss 4.763250350952148, Classifier Loss 0.19304750859737396, Total Loss 141.11341857910156\n",
      "58: Encoding Loss 15.474089622497559, Transition Loss 3.8432583808898926, Classifier Loss 0.20099873840808868, Total Loss 144.66123962402344\n",
      "58: Encoding Loss 16.090600967407227, Transition Loss 5.215516090393066, Classifier Loss 0.18483075499534607, Total Loss 148.25099182128906\n",
      "58: Encoding Loss 15.751334190368652, Transition Loss 6.778726100921631, Classifier Loss 0.2297515869140625, Total Loss 150.34158325195312\n",
      "58: Encoding Loss 15.8956937789917, Transition Loss 6.859348773956299, Classifier Loss 0.2046322375535965, Total Loss 149.00064086914062\n",
      "58: Encoding Loss 16.697683334350586, Transition Loss 5.0815110206604, Classifier Loss 0.18795083463191986, Total Loss 153.39285278320312\n",
      "58: Encoding Loss 16.46817970275879, Transition Loss 5.690654754638672, Classifier Loss 0.19279184937477112, Total Loss 152.1627655029297\n",
      "58: Encoding Loss 15.221667289733887, Transition Loss 3.7197110652923584, Classifier Loss 0.19305747747421265, Total Loss 141.82302856445312\n",
      "58: Encoding Loss 15.800732612609863, Transition Loss 6.570782661437988, Classifier Loss 0.22717241942882538, Total Loss 150.43727111816406\n",
      "58: Encoding Loss 16.034204483032227, Transition Loss 4.974108695983887, Classifier Loss 0.22867992520332336, Total Loss 152.13645935058594\n",
      "58: Encoding Loss 15.35014533996582, Transition Loss 7.563716888427734, Classifier Loss 0.21940436959266663, Total Loss 146.2543487548828\n",
      "58: Encoding Loss 16.240211486816406, Transition Loss 3.8587961196899414, Classifier Loss 0.2121993750333786, Total Loss 151.91339111328125\n",
      "58: Encoding Loss 15.741914749145508, Transition Loss 4.939901351928711, Classifier Loss 0.19128786027431488, Total Loss 146.0520782470703\n",
      "58: Encoding Loss 16.385128021240234, Transition Loss 5.381872177124023, Classifier Loss 0.21801236271858215, Total Loss 153.95864868164062\n",
      "58: Encoding Loss 15.720505714416504, Transition Loss 7.249386787414551, Classifier Loss 0.19531536102294922, Total Loss 146.74546813964844\n",
      "58: Encoding Loss 14.81722354888916, Transition Loss 6.272003173828125, Classifier Loss 0.18969374895095825, Total Loss 138.76156616210938\n",
      "58: Encoding Loss 15.314269065856934, Transition Loss 5.89201021194458, Classifier Loss 0.19742335379123688, Total Loss 143.4348907470703\n",
      "58: Encoding Loss 14.626940727233887, Transition Loss 8.446844100952148, Classifier Loss 0.19232836365699768, Total Loss 137.93772888183594\n",
      "58: Encoding Loss 15.81894302368164, Transition Loss 5.737348556518555, Classifier Loss 0.20510007441043854, Total Loss 148.20901489257812\n",
      "58: Encoding Loss 15.592273712158203, Transition Loss 6.322670936584473, Classifier Loss 0.20898401737213135, Total Loss 146.901123046875\n",
      "58: Encoding Loss 14.031062126159668, Transition Loss 5.412738800048828, Classifier Loss 0.2033446729183197, Total Loss 133.66551208496094\n",
      "58: Encoding Loss 14.991667747497559, Transition Loss 6.744101524353027, Classifier Loss 0.18850308656692505, Total Loss 140.13246154785156\n",
      "58: Encoding Loss 15.715102195739746, Transition Loss 5.849341869354248, Classifier Loss 0.19963957369327545, Total Loss 146.85462951660156\n",
      "58: Encoding Loss 16.330387115478516, Transition Loss 6.340208530426025, Classifier Loss 0.21091362833976746, Total Loss 153.00250244140625\n",
      "58: Encoding Loss 15.397988319396973, Transition Loss 5.719700813293457, Classifier Loss 0.1976204812526703, Total Loss 144.08990478515625\n",
      "58: Encoding Loss 15.82571029663086, Transition Loss 4.411365032196045, Classifier Loss 0.19407407939434052, Total Loss 146.89537048339844\n",
      "58: Encoding Loss 15.187766075134277, Transition Loss 4.330541133880615, Classifier Loss 0.23203212022781372, Total Loss 145.57144165039062\n",
      "58: Encoding Loss 15.746392250061035, Transition Loss 6.1681342124938965, Classifier Loss 0.19069825112819672, Total Loss 146.27459716796875\n",
      "58: Encoding Loss 16.412540435791016, Transition Loss 5.924346923828125, Classifier Loss 0.21145576238632202, Total Loss 153.63076782226562\n",
      "58: Encoding Loss 15.553561210632324, Transition Loss 5.930218696594238, Classifier Loss 0.21332521736621857, Total Loss 146.9470672607422\n",
      "58: Encoding Loss 15.688302040100098, Transition Loss 5.826026916503906, Classifier Loss 0.1918385922908783, Total Loss 145.85548400878906\n",
      "58: Encoding Loss 15.92300796508789, Transition Loss 5.269330024719238, Classifier Loss 0.20285481214523315, Total Loss 148.72340393066406\n",
      "58: Encoding Loss 16.342960357666016, Transition Loss 4.053206443786621, Classifier Loss 0.18946325778961182, Total Loss 150.50064086914062\n",
      "58: Encoding Loss 15.848685264587402, Transition Loss 4.199758052825928, Classifier Loss 0.197187602519989, Total Loss 147.3481903076172\n",
      "58: Encoding Loss 16.00069236755371, Transition Loss 6.139148712158203, Classifier Loss 0.19112487137317657, Total Loss 148.34585571289062\n",
      "58: Encoding Loss 16.773683547973633, Transition Loss 4.034440517425537, Classifier Loss 0.2018192559480667, Total Loss 155.17828369140625\n",
      "58: Encoding Loss 17.177194595336914, Transition Loss 4.232189655303955, Classifier Loss 0.218006432056427, Total Loss 160.06463623046875\n",
      "58: Encoding Loss 14.864181518554688, Transition Loss 6.2631120681762695, Classifier Loss 0.19264915585517883, Total Loss 139.43099975585938\n",
      "58: Encoding Loss 14.721353530883789, Transition Loss 6.263658046722412, Classifier Loss 0.18130572140216827, Total Loss 137.1541290283203\n",
      "58: Encoding Loss 15.110031127929688, Transition Loss 5.78617000579834, Classifier Loss 0.21744754910469055, Total Loss 143.7822265625\n",
      "58: Encoding Loss 15.173800468444824, Transition Loss 6.1848297119140625, Classifier Loss 0.22226722538471222, Total Loss 144.85409545898438\n",
      "58: Encoding Loss 14.9660005569458, Transition Loss 8.367533683776855, Classifier Loss 0.20526178181171417, Total Loss 141.9276885986328\n",
      "58: Encoding Loss 16.636205673217773, Transition Loss 5.076962471008301, Classifier Loss 0.20850658416748047, Total Loss 154.95570373535156\n",
      "58: Encoding Loss 16.534860610961914, Transition Loss 4.953269958496094, Classifier Loss 0.178681880235672, Total Loss 151.13772583007812\n",
      "58: Encoding Loss 14.793294906616211, Transition Loss 4.516133785247803, Classifier Loss 0.19024039804935455, Total Loss 138.2736358642578\n",
      "58: Encoding Loss 15.327353477478027, Transition Loss 4.260670185089111, Classifier Loss 0.1945909857749939, Total Loss 142.9300537109375\n",
      "58: Encoding Loss 15.377073287963867, Transition Loss 5.046743869781494, Classifier Loss 0.18909762799739838, Total Loss 142.9357147216797\n",
      "58: Encoding Loss 15.808622360229492, Transition Loss 5.555377006530762, Classifier Loss 0.2023671567440033, Total Loss 147.8167724609375\n",
      "58: Encoding Loss 15.816557884216309, Transition Loss 4.319676876068115, Classifier Loss 0.19310368597507477, Total Loss 146.70677185058594\n",
      "58: Encoding Loss 15.855755805969238, Transition Loss 4.958874225616455, Classifier Loss 0.21594439446926117, Total Loss 149.43226623535156\n",
      "58: Encoding Loss 14.920130729675293, Transition Loss 4.162917137145996, Classifier Loss 0.17670458555221558, Total Loss 137.8640899658203\n",
      "58: Encoding Loss 15.747900009155273, Transition Loss 5.659364223480225, Classifier Loss 0.17584960162639618, Total Loss 144.70004272460938\n",
      "58: Encoding Loss 16.080612182617188, Transition Loss 3.723217487335205, Classifier Loss 0.21741066873073578, Total Loss 151.13059997558594\n",
      "58: Encoding Loss 15.103035926818848, Transition Loss 6.270484924316406, Classifier Loss 0.2250402569770813, Total Loss 144.58242797851562\n",
      "58: Encoding Loss 15.757987022399902, Transition Loss 4.731319427490234, Classifier Loss 0.19830331206321716, Total Loss 146.84048461914062\n",
      "58: Encoding Loss 15.945318222045898, Transition Loss 5.714024066925049, Classifier Loss 0.23175455629825592, Total Loss 151.8808135986328\n",
      "58: Encoding Loss 15.782402992248535, Transition Loss 5.016462326049805, Classifier Loss 0.1996513307094574, Total Loss 147.2276611328125\n",
      "58: Encoding Loss 15.208404541015625, Transition Loss 5.404561996459961, Classifier Loss 0.19379080832004547, Total Loss 142.1272430419922\n",
      "58: Encoding Loss 16.94021987915039, Transition Loss 3.930286407470703, Classifier Loss 0.19517281651496887, Total Loss 155.82510375976562\n",
      "58: Encoding Loss 16.176000595092773, Transition Loss 3.4227092266082764, Classifier Loss 0.19719326496124268, Total Loss 149.81187438964844\n",
      "58: Encoding Loss 15.49892807006836, Transition Loss 4.392345428466797, Classifier Loss 0.1997697651386261, Total Loss 144.8468780517578\n",
      "58: Encoding Loss 15.142830848693848, Transition Loss 5.352700233459473, Classifier Loss 0.19726447761058807, Total Loss 141.9396514892578\n",
      "58: Encoding Loss 15.274995803833008, Transition Loss 3.4769511222839355, Classifier Loss 0.2060985267162323, Total Loss 143.5052032470703\n",
      "58: Encoding Loss 15.974593162536621, Transition Loss 6.904867172241211, Classifier Loss 0.21070367097854614, Total Loss 150.24807739257812\n",
      "58: Encoding Loss 16.219694137573242, Transition Loss 5.029050350189209, Classifier Loss 0.18919320404529572, Total Loss 149.6826934814453\n",
      "58: Encoding Loss 17.096105575561523, Transition Loss 5.114398002624512, Classifier Loss 0.20457419753074646, Total Loss 158.24913024902344\n",
      "58: Encoding Loss 16.617145538330078, Transition Loss 6.469077110290527, Classifier Loss 0.19477233290672302, Total Loss 153.7082061767578\n",
      "58: Encoding Loss 15.291223526000977, Transition Loss 7.754097938537598, Classifier Loss 0.19094730913639069, Total Loss 142.975341796875\n",
      "58: Encoding Loss 13.087518692016602, Transition Loss 9.054644584655762, Classifier Loss 0.19662649929523468, Total Loss 126.1737289428711\n",
      "59: Encoding Loss 16.87784194946289, Transition Loss 4.42433500289917, Classifier Loss 0.21137596666812897, Total Loss 157.0452117919922\n",
      "59: Encoding Loss 16.256772994995117, Transition Loss 5.149862289428711, Classifier Loss 0.19468235969543457, Total Loss 150.55238342285156\n",
      "59: Encoding Loss 16.257604598999023, Transition Loss 4.8587422370910645, Classifier Loss 0.19304028153419495, Total Loss 150.3366241455078\n",
      "59: Encoding Loss 15.753520011901855, Transition Loss 5.512653350830078, Classifier Loss 0.20291641354560852, Total Loss 147.4223175048828\n",
      "59: Encoding Loss 17.4793643951416, Transition Loss 4.736466884613037, Classifier Loss 0.20711955428123474, Total Loss 161.49417114257812\n",
      "59: Encoding Loss 14.008423805236816, Transition Loss 5.983100414276123, Classifier Loss 0.21038861572742462, Total Loss 134.30288696289062\n",
      "59: Encoding Loss 16.890403747558594, Transition Loss 4.524690628051758, Classifier Loss 0.2209550142288208, Total Loss 158.12367248535156\n",
      "59: Encoding Loss 17.20719337463379, Transition Loss 5.150906085968018, Classifier Loss 0.22004736959934235, Total Loss 160.69247436523438\n",
      "59: Encoding Loss 15.573685646057129, Transition Loss 4.7207441329956055, Classifier Loss 0.23354510962963104, Total Loss 148.88815307617188\n",
      "59: Encoding Loss 16.675527572631836, Transition Loss 4.875370502471924, Classifier Loss 0.2041412889957428, Total Loss 154.7934112548828\n",
      "59: Encoding Loss 16.06253433227539, Transition Loss 4.297669410705566, Classifier Loss 0.1902521848678589, Total Loss 148.38502502441406\n",
      "59: Encoding Loss 15.723984718322754, Transition Loss 4.865933895111084, Classifier Loss 0.2254556119441986, Total Loss 149.31063842773438\n",
      "59: Encoding Loss 15.593952178955078, Transition Loss 5.526782035827637, Classifier Loss 0.19897088408470154, Total Loss 145.75405883789062\n",
      "59: Encoding Loss 14.923298835754395, Transition Loss 4.765608310699463, Classifier Loss 0.18793392181396484, Total Loss 139.13290405273438\n",
      "59: Encoding Loss 15.036599159240723, Transition Loss 5.69025993347168, Classifier Loss 0.19678843021392822, Total Loss 141.1096954345703\n",
      "59: Encoding Loss 15.912369728088379, Transition Loss 6.504549026489258, Classifier Loss 0.2118138074874878, Total Loss 149.78125\n",
      "59: Encoding Loss 15.843802452087402, Transition Loss 5.251805305480957, Classifier Loss 0.2068558782339096, Total Loss 148.48635864257812\n",
      "59: Encoding Loss 15.259309768676758, Transition Loss 6.020584583282471, Classifier Loss 0.1903356909751892, Total Loss 142.31216430664062\n",
      "59: Encoding Loss 16.389554977416992, Transition Loss 5.829161167144775, Classifier Loss 0.18782584369182587, Total Loss 151.06484985351562\n",
      "59: Encoding Loss 16.280738830566406, Transition Loss 5.877073764801025, Classifier Loss 0.1945773959159851, Total Loss 150.87905883789062\n",
      "59: Encoding Loss 15.642069816589355, Transition Loss 6.237276554107666, Classifier Loss 0.20826756954193115, Total Loss 147.2107696533203\n",
      "59: Encoding Loss 15.645442962646484, Transition Loss 4.809678077697754, Classifier Loss 0.2058607041835785, Total Loss 146.7115478515625\n",
      "59: Encoding Loss 14.253650665283203, Transition Loss 8.660276412963867, Classifier Loss 0.19143712520599365, Total Loss 134.90496826171875\n",
      "59: Encoding Loss 15.536162376403809, Transition Loss 4.907954216003418, Classifier Loss 0.22791877388954163, Total Loss 148.06277465820312\n",
      "59: Encoding Loss 15.588533401489258, Transition Loss 3.4336326122283936, Classifier Loss 0.1988808512687683, Total Loss 145.28306579589844\n",
      "59: Encoding Loss 15.296035766601562, Transition Loss 5.361428260803223, Classifier Loss 0.18568670749664307, Total Loss 142.0092315673828\n",
      "59: Encoding Loss 15.864728927612305, Transition Loss 6.011274337768555, Classifier Loss 0.19037620723247528, Total Loss 147.15771484375\n",
      "59: Encoding Loss 15.990523338317871, Transition Loss 6.468670845031738, Classifier Loss 0.20641076564788818, Total Loss 149.85899353027344\n",
      "59: Encoding Loss 15.862446784973145, Transition Loss 6.6876020431518555, Classifier Loss 0.1964622586965561, Total Loss 147.88333129882812\n",
      "59: Encoding Loss 14.552059173583984, Transition Loss 6.97210168838501, Classifier Loss 0.20743048191070557, Total Loss 138.553955078125\n",
      "59: Encoding Loss 15.637706756591797, Transition Loss 5.889970302581787, Classifier Loss 0.19723404943943024, Total Loss 146.00306701660156\n",
      "59: Encoding Loss 14.701147079467773, Transition Loss 7.774914741516113, Classifier Loss 0.1948135793209076, Total Loss 138.64552307128906\n",
      "59: Encoding Loss 15.74720287322998, Transition Loss 6.460790157318115, Classifier Loss 0.20198222994804382, Total Loss 147.46800231933594\n",
      "59: Encoding Loss 14.810408592224121, Transition Loss 3.0979373455047607, Classifier Loss 0.224233016371727, Total Loss 141.52615356445312\n",
      "59: Encoding Loss 14.474406242370605, Transition Loss 6.224735260009766, Classifier Loss 0.20504286885261536, Total Loss 137.5444793701172\n",
      "59: Encoding Loss 14.647845268249512, Transition Loss 3.832322835922241, Classifier Loss 0.21037064492702484, Total Loss 138.9862823486328\n",
      "59: Encoding Loss 15.4634370803833, Transition Loss 6.3652849197387695, Classifier Loss 0.19397203624248505, Total Loss 144.3777618408203\n",
      "59: Encoding Loss 15.239992141723633, Transition Loss 7.823503494262695, Classifier Loss 0.22071285545825958, Total Loss 145.55592346191406\n",
      "59: Encoding Loss 16.426767349243164, Transition Loss 5.226622581481934, Classifier Loss 0.18751917779445648, Total Loss 151.2113800048828\n",
      "59: Encoding Loss 15.596071243286133, Transition Loss 6.503084182739258, Classifier Loss 0.192727729678154, Total Loss 145.3419647216797\n",
      "59: Encoding Loss 14.513858795166016, Transition Loss 6.232398509979248, Classifier Loss 0.2165941745042801, Total Loss 139.01675415039062\n",
      "59: Encoding Loss 15.599312782287598, Transition Loss 4.409674644470215, Classifier Loss 0.20877636969089508, Total Loss 146.5540771484375\n",
      "59: Encoding Loss 15.255611419677734, Transition Loss 4.085455894470215, Classifier Loss 0.20109879970550537, Total Loss 142.9718780517578\n",
      "59: Encoding Loss 15.620344161987305, Transition Loss 3.1380200386047363, Classifier Loss 0.20807485282421112, Total Loss 146.39785766601562\n",
      "59: Encoding Loss 16.6988468170166, Transition Loss 4.390518665313721, Classifier Loss 0.2035452425479889, Total Loss 154.82339477539062\n",
      "59: Encoding Loss 16.13207244873047, Transition Loss 6.2292022705078125, Classifier Loss 0.2097894251346588, Total Loss 151.28135681152344\n",
      "59: Encoding Loss 15.391091346740723, Transition Loss 7.009120464324951, Classifier Loss 0.19551683962345123, Total Loss 144.08224487304688\n",
      "59: Encoding Loss 16.666767120361328, Transition Loss 6.099931716918945, Classifier Loss 0.212715283036232, Total Loss 155.82565307617188\n",
      "59: Encoding Loss 15.945942878723145, Transition Loss 5.45944881439209, Classifier Loss 0.1854032576084137, Total Loss 147.1997528076172\n",
      "59: Encoding Loss 16.03009605407715, Transition Loss 5.688554286956787, Classifier Loss 0.18636927008628845, Total Loss 148.01541137695312\n",
      "59: Encoding Loss 16.587289810180664, Transition Loss 4.118428707122803, Classifier Loss 0.19980911910533905, Total Loss 153.50291442871094\n",
      "59: Encoding Loss 16.339059829711914, Transition Loss 4.63058614730835, Classifier Loss 0.19535745680332184, Total Loss 151.17434692382812\n",
      "59: Encoding Loss 15.681647300720215, Transition Loss 3.862734794616699, Classifier Loss 0.19489005208015442, Total Loss 145.71473693847656\n",
      "59: Encoding Loss 16.505220413208008, Transition Loss 6.752925395965576, Classifier Loss 0.21209457516670227, Total Loss 154.601806640625\n",
      "59: Encoding Loss 14.971955299377441, Transition Loss 6.1466064453125, Classifier Loss 0.19634617865085602, Total Loss 140.63958740234375\n",
      "59: Encoding Loss 14.844666481018066, Transition Loss 5.03648567199707, Classifier Loss 0.22030901908874512, Total Loss 141.7955322265625\n",
      "59: Encoding Loss 15.32866096496582, Transition Loss 4.416723728179932, Classifier Loss 0.20367620885372162, Total Loss 143.8802490234375\n",
      "59: Encoding Loss 14.95079231262207, Transition Loss 5.081210613250732, Classifier Loss 0.1854100227355957, Total Loss 139.16357421875\n",
      "59: Encoding Loss 14.807822227478027, Transition Loss 3.9086227416992188, Classifier Loss 0.1990928053855896, Total Loss 139.15357971191406\n",
      "59: Encoding Loss 15.556939125061035, Transition Loss 4.770194053649902, Classifier Loss 0.18168285489082336, Total Loss 143.57785034179688\n",
      "59: Encoding Loss 16.739561080932617, Transition Loss 5.857509613037109, Classifier Loss 0.2102842777967453, Total Loss 156.11642456054688\n",
      "59: Encoding Loss 17.27090835571289, Transition Loss 5.26384973526001, Classifier Loss 0.21124786138534546, Total Loss 160.34481811523438\n",
      "59: Encoding Loss 15.104188919067383, Transition Loss 3.707831382751465, Classifier Loss 0.21041524410247803, Total Loss 142.61659240722656\n",
      "59: Encoding Loss 14.96976375579834, Transition Loss 4.136904716491699, Classifier Loss 0.18408320844173431, Total Loss 138.99380493164062\n",
      "59: Encoding Loss 15.545516014099121, Transition Loss 5.922128200531006, Classifier Loss 0.20607508718967438, Total Loss 146.15606689453125\n",
      "59: Encoding Loss 14.876994132995605, Transition Loss 5.0473856925964355, Classifier Loss 0.18766428530216217, Total Loss 138.79185485839844\n",
      "59: Encoding Loss 15.907729148864746, Transition Loss 4.80194091796875, Classifier Loss 0.20472145080566406, Total Loss 148.69436645507812\n",
      "59: Encoding Loss 16.394611358642578, Transition Loss 4.854925632476807, Classifier Loss 0.17988862097263336, Total Loss 150.11672973632812\n",
      "59: Encoding Loss 15.622635841369629, Transition Loss 6.082849502563477, Classifier Loss 0.20452214777469635, Total Loss 146.64987182617188\n",
      "59: Encoding Loss 15.377556800842285, Transition Loss 5.280494213104248, Classifier Loss 0.20653656125068665, Total Loss 144.73020935058594\n",
      "59: Encoding Loss 16.79518699645996, Transition Loss 5.293842792510986, Classifier Loss 0.202264204621315, Total Loss 155.6466827392578\n",
      "59: Encoding Loss 15.69416618347168, Transition Loss 6.130365371704102, Classifier Loss 0.190476655960083, Total Loss 145.8270721435547\n",
      "59: Encoding Loss 15.511448860168457, Transition Loss 6.153100967407227, Classifier Loss 0.18836525082588196, Total Loss 144.1587371826172\n",
      "59: Encoding Loss 16.50432586669922, Transition Loss 6.290597438812256, Classifier Loss 0.19390946626663208, Total Loss 152.6836700439453\n",
      "59: Encoding Loss 14.58207893371582, Transition Loss 7.619955062866211, Classifier Loss 0.2219848781824112, Total Loss 140.3791046142578\n",
      "59: Encoding Loss 15.702962875366211, Transition Loss 5.505598545074463, Classifier Loss 0.19796407222747803, Total Loss 146.521240234375\n",
      "59: Encoding Loss 15.303166389465332, Transition Loss 6.938224792480469, Classifier Loss 0.19374176859855652, Total Loss 143.18714904785156\n",
      "59: Encoding Loss 15.633028030395508, Transition Loss 6.575107574462891, Classifier Loss 0.2044455111026764, Total Loss 146.82379150390625\n",
      "59: Encoding Loss 15.191959381103516, Transition Loss 6.998012065887451, Classifier Loss 0.1935935914516449, Total Loss 142.29461669921875\n",
      "59: Encoding Loss 16.914112091064453, Transition Loss 5.374978065490723, Classifier Loss 0.20386065542697906, Total Loss 156.77395629882812\n",
      "59: Encoding Loss 15.277579307556152, Transition Loss 5.819444179534912, Classifier Loss 0.1995045393705368, Total Loss 143.33497619628906\n",
      "59: Encoding Loss 16.179582595825195, Transition Loss 5.469724655151367, Classifier Loss 0.21749313175678253, Total Loss 152.27992248535156\n",
      "59: Encoding Loss 15.668237686157227, Transition Loss 6.281816482543945, Classifier Loss 0.20352862775325775, Total Loss 146.9551239013672\n",
      "59: Encoding Loss 15.595585823059082, Transition Loss 5.909162998199463, Classifier Loss 0.1966591626405716, Total Loss 145.61244201660156\n",
      "59: Encoding Loss 16.649614334106445, Transition Loss 5.0093865394592285, Classifier Loss 0.20126597583293915, Total Loss 154.3253936767578\n",
      "59: Encoding Loss 15.172357559204102, Transition Loss 5.493072509765625, Classifier Loss 0.18762880563735962, Total Loss 141.2403564453125\n",
      "59: Encoding Loss 15.538607597351074, Transition Loss 4.4666571617126465, Classifier Loss 0.21311582624912262, Total Loss 146.51376342773438\n",
      "59: Encoding Loss 15.442152976989746, Transition Loss 7.127004623413086, Classifier Loss 0.2180718332529068, Total Loss 146.76980590820312\n",
      "59: Encoding Loss 15.09349250793457, Transition Loss 4.56535530090332, Classifier Loss 0.19862164556980133, Total Loss 141.52317810058594\n",
      "59: Encoding Loss 15.46298885345459, Transition Loss 3.933100700378418, Classifier Loss 0.19642913341522217, Total Loss 144.13345336914062\n",
      "59: Encoding Loss 16.080951690673828, Transition Loss 5.0482916831970215, Classifier Loss 0.1907089799642563, Total Loss 148.72816467285156\n",
      "59: Encoding Loss 15.753778457641602, Transition Loss 6.840099334716797, Classifier Loss 0.24607382714748383, Total Loss 152.00563049316406\n",
      "59: Encoding Loss 15.91214370727539, Transition Loss 6.2826714515686035, Classifier Loss 0.20545530319213867, Total Loss 149.0991973876953\n",
      "59: Encoding Loss 16.701995849609375, Transition Loss 5.122010707855225, Classifier Loss 0.18105262517929077, Total Loss 152.74562072753906\n",
      "59: Encoding Loss 16.48054313659668, Transition Loss 5.44796085357666, Classifier Loss 0.19586427509784698, Total Loss 152.52037048339844\n",
      "59: Encoding Loss 15.221566200256348, Transition Loss 3.8423423767089844, Classifier Loss 0.20308764278888702, Total Loss 142.84976196289062\n",
      "59: Encoding Loss 15.788846969604492, Transition Loss 6.574514865875244, Classifier Loss 0.2201005220413208, Total Loss 149.63572692871094\n",
      "59: Encoding Loss 16.042505264282227, Transition Loss 5.045208930969238, Classifier Loss 0.21044354140758514, Total Loss 150.39344787597656\n",
      "59: Encoding Loss 15.354660034179688, Transition Loss 7.345572471618652, Classifier Loss 0.2221737653017044, Total Loss 146.52377319335938\n",
      "59: Encoding Loss 16.232479095458984, Transition Loss 3.979755401611328, Classifier Loss 0.2040969878435135, Total Loss 151.06549072265625\n",
      "59: Encoding Loss 15.730019569396973, Transition Loss 4.981898307800293, Classifier Loss 0.18388935923576355, Total Loss 145.22547912597656\n",
      "59: Encoding Loss 16.385400772094727, Transition Loss 5.440383434295654, Classifier Loss 0.22496268153190613, Total Loss 154.66754150390625\n",
      "59: Encoding Loss 15.716753959655762, Transition Loss 7.273480415344238, Classifier Loss 0.20076680183410645, Total Loss 147.26541137695312\n",
      "59: Encoding Loss 14.805431365966797, Transition Loss 6.3211236000061035, Classifier Loss 0.1887817233800888, Total Loss 138.58584594726562\n",
      "59: Encoding Loss 15.3133544921875, Transition Loss 5.784420490264893, Classifier Loss 0.1925136148929596, Total Loss 142.9150848388672\n",
      "59: Encoding Loss 14.640380859375, Transition Loss 8.455367088317871, Classifier Loss 0.1895216405391693, Total Loss 137.7662811279297\n",
      "59: Encoding Loss 15.832219123840332, Transition Loss 5.682531833648682, Classifier Loss 0.20501641929149628, Total Loss 148.2958984375\n",
      "59: Encoding Loss 15.585176467895508, Transition Loss 6.353159427642822, Classifier Loss 0.20962688326835632, Total Loss 146.91473388671875\n",
      "59: Encoding Loss 14.028684616088867, Transition Loss 5.181589603424072, Classifier Loss 0.2004297971725464, Total Loss 133.30877685546875\n",
      "59: Encoding Loss 14.991512298583984, Transition Loss 6.902445316314697, Classifier Loss 0.193497896194458, Total Loss 140.66238403320312\n",
      "59: Encoding Loss 15.725728988647461, Transition Loss 5.644009113311768, Classifier Loss 0.1950232982635498, Total Loss 146.4369659423828\n",
      "59: Encoding Loss 16.305139541625977, Transition Loss 6.689617156982422, Classifier Loss 0.20453839004039764, Total Loss 152.23287963867188\n",
      "59: Encoding Loss 15.405489921569824, Transition Loss 5.412561893463135, Classifier Loss 0.19294099509716034, Total Loss 143.62054443359375\n",
      "59: Encoding Loss 15.821219444274902, Transition Loss 4.989162445068359, Classifier Loss 0.19382399320602417, Total Loss 146.94998168945312\n",
      "59: Encoding Loss 15.19275951385498, Transition Loss 3.9431779384613037, Classifier Loss 0.22098508477210999, Total Loss 144.42921447753906\n",
      "59: Encoding Loss 15.7274751663208, Transition Loss 7.25119686126709, Classifier Loss 0.19321446120738983, Total Loss 146.59149169921875\n",
      "59: Encoding Loss 16.40367889404297, Transition Loss 5.36775016784668, Classifier Loss 0.20397931337356567, Total Loss 152.70091247558594\n",
      "59: Encoding Loss 15.551657676696777, Transition Loss 6.038650989532471, Classifier Loss 0.2125057876110077, Total Loss 146.87156677246094\n",
      "59: Encoding Loss 15.705791473388672, Transition Loss 5.69283390045166, Classifier Loss 0.18578556180000305, Total Loss 145.36346435546875\n",
      "59: Encoding Loss 15.926374435424805, Transition Loss 5.34598445892334, Classifier Loss 0.19700515270233154, Total Loss 148.1807098388672\n",
      "59: Encoding Loss 16.34947967529297, Transition Loss 4.04704475402832, Classifier Loss 0.19940103590488434, Total Loss 151.54534912109375\n",
      "59: Encoding Loss 15.855904579162598, Transition Loss 4.2088093757629395, Classifier Loss 0.1939144879579544, Total Loss 147.08045959472656\n",
      "59: Encoding Loss 15.996429443359375, Transition Loss 6.157410621643066, Classifier Loss 0.1933552622795105, Total Loss 148.53843688964844\n",
      "59: Encoding Loss 16.775964736938477, Transition Loss 4.018829822540283, Classifier Loss 0.19963371753692627, Total Loss 154.974853515625\n",
      "59: Encoding Loss 17.17121124267578, Transition Loss 4.168039321899414, Classifier Loss 0.21211332082748413, Total Loss 159.4146270751953\n",
      "59: Encoding Loss 14.86191177368164, Transition Loss 6.052364349365234, Classifier Loss 0.1967780739068985, Total Loss 139.7835693359375\n",
      "59: Encoding Loss 14.7138671875, Transition Loss 6.371315956115723, Classifier Loss 0.18646690249443054, Total Loss 137.63189697265625\n",
      "59: Encoding Loss 15.10283374786377, Transition Loss 5.728058815002441, Classifier Loss 0.21196046471595764, Total Loss 143.16433715820312\n",
      "59: Encoding Loss 15.173150062561035, Transition Loss 6.348745346069336, Classifier Loss 0.21534056961536407, Total Loss 144.1890106201172\n",
      "59: Encoding Loss 14.978440284729004, Transition Loss 8.460162162780762, Classifier Loss 0.19858434796333313, Total Loss 141.37799072265625\n",
      "59: Encoding Loss 16.629301071166992, Transition Loss 5.398679733276367, Classifier Loss 0.19832195341587067, Total Loss 153.94635009765625\n",
      "59: Encoding Loss 16.535625457763672, Transition Loss 5.134374141693115, Classifier Loss 0.17567360401153564, Total Loss 150.87925720214844\n",
      "59: Encoding Loss 14.79692554473877, Transition Loss 4.700218200683594, Classifier Loss 0.19074968993663788, Total Loss 138.3904266357422\n",
      "59: Encoding Loss 15.330941200256348, Transition Loss 4.53134298324585, Classifier Loss 0.19805659353733063, Total Loss 143.35946655273438\n",
      "59: Encoding Loss 15.398062705993652, Transition Loss 5.2909698486328125, Classifier Loss 0.20469260215759277, Total Loss 144.7119598388672\n",
      "59: Encoding Loss 15.812284469604492, Transition Loss 5.7235260009765625, Classifier Loss 0.20655176043510437, Total Loss 148.29815673828125\n",
      "59: Encoding Loss 15.815112113952637, Transition Loss 4.368939399719238, Classifier Loss 0.1900327354669571, Total Loss 146.39796447753906\n",
      "59: Encoding Loss 15.858254432678223, Transition Loss 5.101905345916748, Classifier Loss 0.2171178162097931, Total Loss 149.59820556640625\n",
      "59: Encoding Loss 14.922772407531738, Transition Loss 4.146439075469971, Classifier Loss 0.17681556940078735, Total Loss 137.8930206298828\n",
      "59: Encoding Loss 15.76440143585205, Transition Loss 5.846160888671875, Classifier Loss 0.18360881507396698, Total Loss 145.64532470703125\n",
      "59: Encoding Loss 16.067773818969727, Transition Loss 3.6206068992614746, Classifier Loss 0.21860381960868835, Total Loss 151.12669372558594\n",
      "59: Encoding Loss 15.081689834594727, Transition Loss 6.637711524963379, Classifier Loss 0.22507905960083008, Total Loss 144.4889678955078\n",
      "59: Encoding Loss 15.749999046325684, Transition Loss 4.585395812988281, Classifier Loss 0.20365402102470398, Total Loss 147.282470703125\n",
      "59: Encoding Loss 15.947598457336426, Transition Loss 6.082696437835693, Classifier Loss 0.21365585923194885, Total Loss 150.1629180908203\n",
      "59: Encoding Loss 15.77441692352295, Transition Loss 4.766175746917725, Classifier Loss 0.19958630204200745, Total Loss 147.10719299316406\n",
      "59: Encoding Loss 15.203702926635742, Transition Loss 5.718079090118408, Classifier Loss 0.1893385350704193, Total Loss 141.70709228515625\n",
      "59: Encoding Loss 16.941808700561523, Transition Loss 3.7717583179473877, Classifier Loss 0.1960456669330597, Total Loss 155.8933868408203\n",
      "59: Encoding Loss 16.1676082611084, Transition Loss 3.537137746810913, Classifier Loss 0.2071945071220398, Total Loss 150.7677459716797\n",
      "59: Encoding Loss 15.50763988494873, Transition Loss 4.35516357421875, Classifier Loss 0.1902276575565338, Total Loss 143.95492553710938\n",
      "59: Encoding Loss 15.13172435760498, Transition Loss 5.790630340576172, Classifier Loss 0.20145846903324127, Total Loss 142.35777282714844\n",
      "59: Encoding Loss 15.267416954040527, Transition Loss 3.2267937660217285, Classifier Loss 0.20063745975494385, Total Loss 142.8484344482422\n",
      "59: Encoding Loss 15.977471351623535, Transition Loss 7.453356742858887, Classifier Loss 0.21098068356513977, Total Loss 150.4085235595703\n",
      "59: Encoding Loss 16.226388931274414, Transition Loss 4.770934581756592, Classifier Loss 0.1949707269668579, Total Loss 150.2623748779297\n",
      "59: Encoding Loss 17.085002899169922, Transition Loss 5.436382293701172, Classifier Loss 0.19971521198749542, Total Loss 157.7388458251953\n",
      "59: Encoding Loss 16.622425079345703, Transition Loss 6.036863803863525, Classifier Loss 0.20346561074256897, Total Loss 154.5333251953125\n",
      "59: Encoding Loss 15.28460693359375, Transition Loss 7.650371074676514, Classifier Loss 0.20105688273906708, Total Loss 143.91261291503906\n",
      "59: Encoding Loss 13.054071426391602, Transition Loss 8.240693092346191, Classifier Loss 0.1883143037557602, Total Loss 124.91213989257812\n"
     ]
    }
   ],
   "source": [
    "# Train Forward Loss\n",
    "training_loop(in_data, out_data, initiations_s, initiations_s_prime,  n_epochs, optimizer_forward, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate\n",
    "def plot_z_space(data):\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_actions):\n",
    "            i_s = data[i][2]\n",
    "            s = data[i][0][...,:obs_dim]\n",
    "            a = data[i][0][...,obs_dim:]\n",
    "            s_prime = data[i][1][..., :obs_dim]\n",
    "            z = encoder(s[i_s[...,i]==1])\n",
    "            z_prime_encoded = encoder(s_prime)\n",
    "            z_a = torch.cat((z[:, 0:-1], a[i_s[...,i]==1]), dim=-1)\n",
    "\n",
    "            z_prime = transition_model(linear_1(z_a) * linear_2(z_a))\n",
    "            s_prime = grounding_model(z[:, 0:-1])\n",
    "            # plt.scatter(z[:, 0], z[:, 1], label=f\"encoded action {i}\")\n",
    "            \n",
    "            plt.scatter(z_prime_encoded[:, 0], z_prime_encoded[:, 1], label=f\"encoded s prime {i}\")\n",
    "            plt.scatter(z_prime[:, 0], z_prime[:, 1], label=f\"prediction action {i}\")\n",
    "            # plt.scatter(s_prime[:, 0], s_prime[:, 1], label=f\"action {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_z_space(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = data[0][0][..., :obs_dim]\n",
    "    a = data[0][0][...,obs_dim:]\n",
    "    z = encoder(x)\n",
    "    logits = initiation_classifier(z[:, :latent_dim])\n",
    "    z_a = torch.cat((z[:, 0:-1], a), dim=-1)\n",
    "    z_prime = transition_model(linear_1(z_a) * linear_2(z_a))[..., :latent_dim]\n",
    "    logits_z_prime = initiation_classifier(z_prime)\n",
    "    init_masks = logits\n",
    "    \n",
    "    for i in range(n_actions):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.scatter(s[init_masks[...,i] > 0.8, 0], s[init_masks[...,i] > 0.8, 1], label=f\"initiation_set {i}\")\n",
    "        # plt.scatter(s[data[i][2][...,i] == 1, 0], s[data[i][2][...,i] == 1, 1], label=f\"initiation_set (truth) {i}\")\n",
    "        # plt.scatter(s_prime[0][I_s_prime[i,..., i] == 1, 0], s_prime[0][I_s_prime[i,...,i] == 1, 1])\n",
    "        # plt.scatter(s_prime[0][logits_z_prime[..., i] > 0.7, 0], s_prime[0][logits_z_prime[...,i] > 0.7, 1])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        \n",
    "        x = data[i][0][..., :obs_dim]\n",
    "        i_s = data[i][2][...,i]\n",
    "        a = data[i][0][i_s ==1, ...,obs_dim:]\n",
    "        z = encoder(x[i_s==1])\n",
    "        epsilon = torch.normal(0, 1, (z.size(0), latent_dim))\n",
    "        z_a = torch.cat((z[:, :latent_dim], a), dim=-1)\n",
    "        z_prime_prediction = transition_model(linear_1(z_a) * linear_2(z_a))\n",
    "        # z_prime_prediction = epsilon * z_prime_prediction[..., latent_dim:] + z_prime_prediction[..., :latent_dim]\n",
    "        s_prime = grounding_model(z_prime_prediction[..., :latent_dim])\n",
    "\n",
    "        epsilon = torch.normal(0, 1, (s_prime.size(0), obs_dim))\n",
    "        s_prime_sample = torch.exp(s_prime[..., obs_dim:]) * epsilon + s_prime[..., :obs_dim]\n",
    "        # s_prime_sample = s_prime[..., :obs_dim]\n",
    "        z_prime_effect = encoder(s_prime_sample)\n",
    "\n",
    "        z_prime_truth = encoder(data[i][1][i_s==1,..., :obs_dim])\n",
    "\n",
    "\n",
    "        logits = initiation_classifier(z[:, :latent_dim])\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(z_prime_truth[..., 0], z_prime_truth[..., 1], label=f\"S_prime Truth action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.scatter(z_prime_effect[..., 0], z_prime_effect[..., 1], label=f\"S_prime Effect prediction action{i}\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(z_prime_prediction[..., 0], z_prime_prediction[..., 1])\n",
    "        # plt.legend()\n",
    "        \n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
