experiment_cwd: exp_results/antmaze/egocentric/absmdps
experiment_name: test
exp_id: test
fabric:
  accelerator: gpu
  devices: 1
  strategy: auto

experiment:
  seed: 31
  gpu: 0
  demo: false
  eval_interval: 5000
  eval_n_runs: 10
  log_level: 20
  checkpoint_frequency: 10000 
  max_rollout_len:  # max length to rollout the ground env.
  log_tensorboard: true
  discounted: false
  steps: 250000
  train_every: 8
  max_episode_len: 100
  outdir: planning_ddqn
  explore_ground: false

world_model:
  ckpt: none
  data_path: none
  name: null
  reward_scale: ${planner.env.reward_scale}
  sample_transition: false
  model_success: false
  gamma: ${planner.env.gamma}
  fixed_modules: []
  
  model:
    model_config_dir: experiments/antmaze
    n_options: 3
    obs_dims: 29
    latent_dim: 16
    trajectory_len: 64
    encoder: ${loadcfg:${model.model_config_dir}/config/models/multiencoder.yaml}
    decoder: ${loadcfg:${model.model_config_dir}/config/models/decoder_mlp_critic.yaml}
    transition: ${loadcfg:${model.model_config_dir}/config/models/transition_gaussian.yaml}
    init_class: ${loadcfg:${model.model_config_dir}/config/models/init_class_mlp.yaml}
    reward: ${loadcfg:${model.model_config_dir}/config/models/reward_mlp.yaml}
    tau: ${loadcfg:${model.model_config_dir}/config/models/tau_mlp.yaml}
    goal_class: 
      type: 'mlp'
      hidden_dims: [256, 256]
      activation: silu
      output_dim: 1
      input_dim: 16
      reward_warmup_steps: 5000
    

  optimizer:
    type: adam
    params:
      lr: 1e-4
  scheduler:
    type: 'linear_scheduler'
    params:
      start_factor: 1.
      end_factor: 1.
      total_iters: 1.

  loss:
    grounding_const: 1.
    tpc_const: 1
    kl_const: 1
    transition_const: 1.
    reward_const: 1
    tau_const: 1.
    initset_const: 1
    n_samples: 1

  data:
    buffer_size: 100000
    batch_size: 2

planner:
  env:
    sample_abstract_transition: true
    render: false
    init_thresh: 0.5
    reward_scale: 0. #0.00016666666666666666
    gamma: 0.99995
    goal: 0
    abstract_goal_tol: 0.1
    envname: 'ant_maze_xl'

  agent:
    load: null
    final_exploration_steps: 0.4 # proportion of agent trainings steps to decay the epsilon
    final_epsilon: 0.1
    eval_epsilon: 0.001
    replay_start_size: 1000
    replay_buffer_size: 1000000
    target_update_interval: 5000
    update_interval: 5
    num_step_return: 1
    steps: 1000000
    lr: 1e-4
    eval_random_agent: false
    rollout_len: 100

  q_func: 
    type: 'mlp'
    n_actions: 3
    hidden_dims: [256, 256]
    activation: silu
    output_dim: ${world_model.model.n_options}


  q_func_rainbow:
    type: 'mlp'
    n_actions: 3
    hidden_dims: [256, 256]
    activation: relu
    output_dim: 1024