experiment_cwd: exp_results/antmaze/egocentric/absmdps
experiment_name: test
exp_id: test
fabric:
  accelerator: gpu
  devices: 1
  strategy: auto

experiment:
  seed: 31
  gpu: 0
  demo: false
  eval_interval: 5000
  eval_n_runs: 4
  log_level: 20
  checkpoint_frequency: 10000 
  max_rollout_len: 64 # max length to rollout the ground env.
  log_tensorboard: true
  discounted: false
  steps: 1000000
  train_every: 32
  max_episode_len: 50
  outdir: planning_ddqn
  explore_ground: false
  prefill: 512

world_model:
  ckpt: none
  data_path: none
  name: null
  reward_scale: ${planner.env.reward_scale}
  sample_transition: true
  model_success: false
  gamma: ${planner.env.gamma}
  fixed_modules: []
  
  model:
    model_config_dir: experiments/antmaze
    n_options: 3
    obs_dims: 29
    latent_dim: 32
    trajectory_len: 64
    encoder: 
      features:
        type: multicnnencoder
        n_cnns: 2
        outdim: ${model.latent_dim}
        cnn:
          type: doubling_residual
          in_width: 64
          in_height: 64
          color_channels: 3
          depth: 24
          cnn_blocks: 2
          min_resolution: 4
          mlp_layers: [1024,]
          mlp_activation: mish
          cnn_activation: mish
          outdim: 32
          obs_keys: 'image'
        mlp:
          type: mlp
          input_dim: 153
          hidden_dims:
          - 512
          - 512
          activation: mish
          output_dim: 32
          obs_keys: '.*'
        final_mlp:
          type: mlp
          output_dim: ${model.latent_dim}
          hidden_dims:
          - 512
          - 512
          activation: mish
          outact: none

    decoder:
      type: mlp
      input_dim: ${eval:${model.latent_dim}*2}
      output_dim: 1
      hidden_dims: [512, 512]
      activation: mish
    transition:
      features:
        type: dynamics_mlp
        latent_dim: ${model.latent_dim}
        n_options: ${model.n_options}
        hidden_dims: [512, 512]
        activation: mish
      dist:
        type: diag_gaussian
        output_dim: ${model.latent_dim}
        input_dim: ${model.latent_dim}
        min_std: 0.001
        max_std: 10
    # transition:
    #   features:
    #     type: dynamics_mlp
    #     latent_dim: ${model.latent_dim}
    #     n_options: ${model.n_options}
    #     hidden_dims: [512, 512]
    #     activation: mish
    #   dist:
    #     type: mixture_gaussian
    #     n_components: 8
    #     output_dim: ${model.latent_dim}
    #     input_dim: ${model.latent_dim}
    #     min_std: 0.001
    #     max_std: 100
    # transition:
    #   features:
    #     type: dynamics_mlp
    #     latent_dim: ${model.latent_dim}
    #     n_options: ${model.n_options}
    #     hidden_dims: [512, 512]
    #     activation: mish
    #   dist:
    #     type: deterministic
    init_class:
      type: mlp
      input_dim: ${model.latent_dim}
      output_dim: ${model.n_options}
      hidden_dims:
        - 256
        - 256
      activation: mish
    reward:
      type: mlp
      input_dim: ${eval:${model.latent_dim}*2+${model.n_options}}
      output_dim: 1
      hidden_dims:
        - 256
        - 256
      activation: mish
    tau:
      type: mlp
      input_dim: ${eval:${model.latent_dim}+${model.n_options}}
      output_dim: 1
      hidden_dims:
      - 256
      - 256
      activation: mish
    goal_class: 
      type: mlp
      hidden_dims: [256, 256]
      activation: mish
      output_dim: 1
      reward_warmup_steps: 60000
    
  optimizer:
    type: adam
    params:
      lr: 5e-4
  scheduler:
    type: 'linear_scheduler'
    params:
      start_factor: 1.
      end_factor: 1.
      total_iters: 1.

  loss:
    grounding_const: 1.
    tpc_const: 1.
    kl_const: 1
    transition_const: 1.
    reward_const: 1.
    tau_const: 0.001
    initset_const: 1.
    norm2_const: 0.
    goal_class_const: 1.
    n_samples: 1

  data:
    buffer_size: 100000
    batch_size: 16

planner:
  env:
    sample_abstract_transition: true
    render: false
    init_thresh: 0.5
    reward_scale: 0. #0.00016666666666666666
    gamma: 0.997 #0.9998
    goal: 0
    abstract_goal_tol: 0.1
    envname: 'ant_maze_xl'
    n_envs: 1

  agent:
    load: null
    final_exploration_steps: 0.4 # proportion of agent trainings steps to decay the epsilon
    final_epsilon: 0.1
    eval_epsilon: 0.001
    replay_start_size: 100
    replay_buffer_size: 100000
    target_update_interval: 5000
    update_interval: 5
    num_step_return: 1
    steps: 1000000
    lr: 3e-4
    eval_random_agent: false
    rollout_len: 400

  q_func: 
    type: 'mlp'
    n_actions: 3
    hidden_dims: [256, 256]
    activation: silu
    output_dim: ${world_model.model.n_options}

  q_func_rainbow:
    type: 'mlp'
    n_actions: 3
    hidden_dims: [256, 256]
    activation: tanh
    output_dim: 1024