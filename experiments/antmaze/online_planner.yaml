experiment_cwd: exp_results/antmaze/antmaze-umaze-v2/absmdps
experiment_name: test
exp_id: test
fabric:
  accelerator: cpu
  devices: 1
  strategy: auto

experiment:
  seed: 31
  gpu: -1
  demo: false
  eval_interval: 5000
  eval_n_runs: 10
  log_level: 20
  checkpoint_frequency: 10000 
  max_rollout_len: 64 # max length to rollout the ground env.
  log_tensorboard: true
  discounted: false
  steps: 250000
  train_every: 8
  max_episode_len: 100
  outdir: planning
  explore_ground: false
  prefill: 512
  gradient_steps: 1

world_model:
  ckpt: null #exp_results/antmaze/antmaze-umaze-v2/mdps/umaze_markov_1024_from_success_mixture_uniform_3__model.latentdim_16/phi_train/ckpts/last-v1.ckpt
  data_path: exp_results/antmaze/antmaze-umaze-v2/data/trajectories_128_uniform.zip
  name: null
  reward_scale: ${planner.env.reward_scale}
  sample_transition: true
  model_success: false
  gamma: ${planner.env.gamma}
  fixed_modules: [] #['encoder']
  train_every: ${experiment.train_every}
  
  
  model:
    model_config_dir: experiments/antmaze
    n_options: 8
    obs_dims: 29
    latent_dim: 8
    trajectory_len: 64
    encoder: ${loadcfg:${model.model_config_dir}/config/models/encoder_mlp.yaml}
    decoder: ${loadcfg:${model.model_config_dir}/config/models/decoder_mlp_critic.yaml}
    # transition: ${loadcfg:${model.model_config_dir}/config/models/transition_gaussian.yaml}
    transition:
      features:
        type: mlp
        input_dim: ${eval:${model.n_options} + ${model.latent_dim}}
        output_dim: 1024
        n_options: ${model.n_options}
        hidden_dims: [512, 512]
        activation: mish
        normalize: true
      dist:
        type: mixture_gaussian
        n_components: 2
        activation: mish
        output_dim: ${model.latent_dim}
        input_dim: ${model.transition.features.output_dim}
        min_std: 0.001
        max_std: 10
    termination:
      type: mlp
      input_dim: ${model.latent_dim}
      output_dim: 1
      hidden_dims:
      - 256
      - 256
      activation: mish
    init_class: ${loadcfg:${model.model_config_dir}/config/models/init_class_mlp.yaml}
    reward: ${loadcfg:${model.model_config_dir}/config/models/reward_mlp.yaml}
    tau: ${loadcfg:${model.model_config_dir}/config/models/tau_mlp.yaml}
    goal_class: 
      type: 'mlp'
      hidden_dims: [256, 256]
      activation: mish
      normalize: true
      output_dim: 1
      input_dim: ${model.latent_dim}
      reward_warmup_steps: 30000
    
  optimizer:
    type: adam
    params:
      lr: 1e-4
  scheduler:
    type: 'linear_scheduler'
    params:
      start_factor: 1.
      end_factor: 1.
      total_iters: 1.

  loss:
    grounding_const: 1.
    tpc_const: 1
    kl_const: 1
    transition_const: 1.
    reward_const: 1
    tau_const: 1.
    initset_const: 1
    goal_class_const: 0.01
    norm2_const: 1.
    termination_const: 1.
    n_samples: 1

  data:
    buffer_size: 100000
    batch_size: 16

planner:
  agent_id: ''
  train_every: ${experiment.train_every}
  env:
    sample_abstract_transition: true
    render: false
    init_thresh: 0.5
    reward_scale: 0. #0.00016666666666666666
    gamma: 0.995
    goal: 4
    abstract_goal_tol: 0.1
    envname: 'antmaze-medium-play-v2'
    n_envs: 2

  agent:
    load: null
    final_exploration_steps: 0.4 # proportion of agent trainings steps to decay the epsilon
    final_epsilon: 0.1
    eval_epsilon: 0.001
    replay_start_size: 1000
    replay_buffer_size: 100000
    target_update_interval: 5000
    update_interval: 5
    num_step_return: 3
    steps: 1000000
    lr: 1e-4
    eval_random_agent: false
    rollout_len: 100

  q_func: 
    type: 'mlp'
    n_actions: 8
    hidden_dims: [256, 256]
    activation: silu
    output_dim: ${world_model.model.n_options}


  q_func_rainbow:
    type: 'mlp'
    n_actions: 8
    hidden_dims: [256, 256]
    activation: relu
    output_dim: 1024

  ppo_func:
    type: 'mlp'
    n_actions: 8
    hidden_dims: [256, 256]
    activation: mish
    normalize: false
    output_dim: 1024