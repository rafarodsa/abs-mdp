experiment_cwd: exp_results/antmaze/antmaze-umaze-v2/absmdps
experiment_name: test
exp_id: test
fabric:
  accelerator: cpu
  devices: 1
  strategy: auto

experiment:
  seed: 31
  gpu: -1
  demo: false
  eval_interval: 5000
  eval_n_runs: 10
  log_level: 20
  checkpoint_frequency: 10000 
  max_rollout_len: 64 # max length to rollout the ground env.
  log_tensorboard: true
  discounted: false
  steps: 100000
  train_every: 8
  max_episode_len: 100
  outdir: planning_ddqn

world_model:
  ckpt: exp_results/antmaze/antmaze-umaze-v2/mdps/umaze_markov_1024_from_success_mixture_uniform_3__model.latentdim_16/phi_train/ckpts/last-v1.ckpt
  data_path: exp_results/antmaze/antmaze-umaze-v2/data/trajectories_128_uniform.zip
  name: null
  reward_scale: ${planner.env.reward_scale}
  sample_transition: false
  model_success: false
  gamma: ${planner.env.gamma}
  fixed_modules: ['encoder']
  
  model:
    model_config_dir: experiments/pb_obstacles/fullstate/
    n_options: 8
    obs_dims: 29
    latent_dim: 16
    trajectory_len: 64
    encoder: ${loadcfg:${model.model_config_dir}/config/models/encoder_mlp.yaml}
    decoder: ${loadcfg:${model.model_config_dir}/config/models/decoder_mlp_critic.yaml}
    transition: ${loadcfg:${model.model_config_dir}/config/models/transition_gaussian.yaml}
    init_class: ${loadcfg:${model.model_config_dir}/config/models/init_class_mlp.yaml}
    reward: ${loadcfg:${model.model_config_dir}/config/models/reward_mlp.yaml}
    tau: ${loadcfg:${model.model_config_dir}/config/models/tau_mlp.yaml}
    goal_class: 
      type: 'mlp'
      hidden_dims: [256, 256]
      activation: silu
      output_dim: 1
      input_dim: 16
      reward_warmup_steps: 5000
    

  optimizer:
    type: adam
    params:
      lr: 1e-4
  scheduler:
    type: 'linear_scheduler'
    params:
      start_factor: 1.
      end_factor: 1.
      total_iters: 1.

  loss:
    grounding_const: 1.
    tpc_const: 1
    kl_const: 1
    transition_const: 1.
    reward_const: 1
    tau_const: 1.
    initset_const: 1
    n_samples: 1

  data:
    buffer_size: 100000
    batch_size: 16

planner:
  env:
    sample_abstract_transition: true
    render: false
    init_thresh: 0.5
    reward_scale: 0. #0.00016666666666666666
    gamma: 0.995
    goal: 4
    envname: 'antmaze-umaze-v2'

  agent:
    load: null
    final_exploration_steps: 0.4 # proportion of agent trainings steps to decay the epsilon
    final_epsilon: 0.01
    eval_epsilon: 0.001
    replay_start_size: 1000
    replay_buffer_size: 500000
    target_update_interval: 5000
    update_interval: 5
    num_step_return: 1
    steps: 1000000
    lr: 1e-4
    eval_random_agent: false
    rollout_len: 100

  q_func: 
    type: 'mlp'
    n_actions: 8
    hidden_dims: [256, 256]
    activation: silu
    output_dim: ${world_model.model.n_options}


