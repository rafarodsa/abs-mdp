experiment_cwd: exp_results/pb_obstacles/pixel/absmdps
experiment_name: pinball_markov_1024_2__model.latentdim_16__seed_31
exp_id: test
fabric:
  accelerator: gpu
  devices: auto
  strategy: auto

experiment:
  seed: 31
  gpu: 0
  demo: false
  eval_interval: 1000
  eval_n_runs: 10
  log_level: 20
  checkpoint_frequency: 20000 
  max_rollout_len: 64 # max length to rollout the ground env.
  log_tensorboard: false
  discounted: false
  steps: 150000
  train_every: 5
  max_episode_len: 100
  outdir: planning_ddqn
  num_envs: 1
  explore_ground: false

world_model:
  ckpt: exp_results/pb_obstacles/pixel/mdps/pinball_markov_1024_2__model.latentdim_16__seed_31/phi_train/ckpts/last.ckpt
  data_path: exp_results/pb_obstacles/pixel/data/trajectories_cont_25_1024.zip
  name: null
  reward_scale: ${planner.env.reward_scale}
  sample_transition: true
  model_success: false
  gamma: ${planner.env.gamma}
  fixed_modules: ['encoder']
  model:
    model_config_dir: experiments/pb_obstacles/pixel/
    n_options: 4
    obs_dims: 4
    latent_dim: 2
    trajectory_len: 64
    encoder: ${loadcfg:${model_config_dir}/config/models/encoder_cnn.yaml}
    decoder: ${loadcfg:${model_config_dir}/config/models/decoder_cnn_critic.yaml}
    transition: ${loadcfg:${model_config_dir}/config/models/transition_rssm.yaml}
    init_class: ${loadcfg:${model_config_dir}/config/models/init_class_mlp.yaml}
    reward: ${loadcfg:${model_config_dir}/config/models/reward_mlp.yaml}
    tau: ${loadcfg:${model_config_dir}/config/models/tau_mlp.yaml}

  optimizer:
    type: adam
    params:
      lr: 2e-4
  scheduler:
    type: 'linear_scheduler'
    params:
      start_factor: 1.
      end_factor: 1.
      total_iters: 1.

  loss:
    grounding_const: 1.
    tpc_const: 1
    kl_const: 1
    transition_const: 1.
    reward_const: 1
    tau_const: 1.
    initset_const: 1
    n_samples: 1

  data:
    buffer_size: 1000000
    batch_size: 16

planner:
  env:
    sample_abstract_transition: true
    absgroundmdp: false
    render: false
    init_thresh: 0.5
    use_ground_init: false
    reward_scale: 0. #0.00016666666666666666
    gamma: 0.9997
    goal: 2
    pixels: true

  agent:
    load: null
    final_exploration_steps: 0.4 # proportion of agent trainings steps to decay the epsilon
    final_epsilon: 0.15
    eval_epsilon: 0.001
    replay_start_size: 1000
    replay_buffer_size: 100000
    target_update_interval: 10000
    update_interval: 5
    num_step_return: 1
    lr: 1e-4
    eval_random_agent: false
    rollout_len: 100
    pixels: true

  q_func: 
    type: 'mlp'
    n_actions: 4
    hidden_dims: [256, 256]
    activation: silu
    output_dim: ${world_model.model.n_options}


